[{"id": "2101.00008", "submitter": "Farah Shamout", "authors": "Munachiso Nwadike, Takumi Miyawaki, Esha Sarkar, Michail Maniatakos,\n  Farah Shamout", "title": "Explainability Matters: Backdoor Attacks on Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to be vulnerable to backdoor attacks,\nwhich could be easily introduced to the training set prior to model training.\nRecent work has focused on investigating backdoor attacks on natural images or\ntoy datasets. Consequently, the exact impact of backdoors is not yet fully\nunderstood in complex real-world applications, such as in medical imaging where\nmisdiagnosis can be very costly. In this paper, we explore the impact of\nbackdoor attacks on a multi-label disease classification task using chest\nradiography, with the assumption that the attacker can manipulate the training\ndataset to execute the attack. Extensive evaluation of a state-of-the-art\narchitecture demonstrates that by introducing images with few-pixel\nperturbations into the training set, an attacker can execute the backdoor\nsuccessfully without having to be involved with the training procedure. A\nsimple 3$\\times$3 pixel trigger can achieve up to 1.00 Area Under the Receiver\nOperating Characteristic (AUROC) curve on the set of infected images. In the\nset of clean images, the backdoored neural network could still achieve up to\n0.85 AUROC, highlighting the stealthiness of the attack. As the use of deep\nlearning based diagnostic systems proliferates in clinical practice, we also\nshow how explainability is indispensable in this context, as it can identify\nspatially localized backdoors in inference time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 09:41:19 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Nwadike", "Munachiso", ""], ["Miyawaki", "Takumi", ""], ["Sarkar", "Esha", ""], ["Maniatakos", "Michail", ""], ["Shamout", "Farah", ""]]}, {"id": "2101.00062", "submitter": "Zixiang Zhao", "authors": "Zixiang Zhao, Jiangshe Zhang, Shuang Xu, Kai Sun, Lu Huang, Junmin\n  Liu, Chunxia Zhang", "title": "FGF-GAN: A Lightweight Generative Adversarial Network for Pansharpening\n  via Fast Guided Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pansharpening is a widely used image enhancement technique for remote\nsensing. Its principle is to fuse the input high-resolution single-channel\npanchromatic (PAN) image and low-resolution multi-spectral image and to obtain\na high-resolution multi-spectral (HRMS) image. The existing deep learning\npansharpening method has two shortcomings. First, features of two input images\nneed to be concatenated along the channel dimension to reconstruct the HRMS\nimage, which makes the importance of PAN images not prominent, and also leads\nto high computational cost. Second, the implicit information of features is\ndifficult to extract through the manually designed loss function. To this end,\nwe propose a generative adversarial network via the fast guided filter (FGF)\nfor pansharpening. In generator, traditional channel concatenation is replaced\nby FGF to better retain the spatial information while reducing the number of\nparameters. Meanwhile, the fusion objects can be highlighted by the spatial\nattention module. In addition, the latent information of features can be\npreserved effectively through adversarial training. Numerous experiments\nillustrate that our network generates high-quality HRMS images that can surpass\nexisting methods, and with fewer parameters.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 20:27:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhao", "Zixiang", ""], ["Zhang", "Jiangshe", ""], ["Xu", "Shuang", ""], ["Sun", "Kai", ""], ["Huang", "Lu", ""], ["Liu", "Junmin", ""], ["Zhang", "Chunxia", ""]]}, {"id": "2101.00073", "submitter": "Nanchun Shi", "authors": "Zhifeng Yu, Nanchun Shi", "title": "A Multi-modal Deep Learning Model for Video Thumbnail Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thumbnail is the face of online videos. The explosive growth of videos both\nin number and variety underpins the importance of a good thumbnail because it\nsaves potential viewers time to choose videos and even entice them to click on\nthem. A good thumbnail should be a frame that best represents the content of a\nvideo while at the same time capturing viewers' attention. However, the\ntechniques and models in the past only focus on frames within a video, and we\nbelieve such narrowed focus leave out much useful information that are part of\na video. In this paper, we expand the definition of content to include title,\ndescription, and audio of a video and utilize information provided by these\nmodalities in our selection model. Specifically, our model will first sample\nframes uniformly in time and return the top 1,000 frames in this subset with\nthe highest aesthetic scores by a Double-column Convolutional Neural Network,\nto avoid the computational burden of processing all frames in downstream task.\nThen, the model incorporates frame features extracted from VGG16, text features\nfrom ELECTRA, and audio features from TRILL. These models were selected because\nof their results on popular datasets as well as their competitive performances.\nAfter feature extraction, the time-series features, frames and audio, will be\nfed into Transformer encoder layers to return a vector representing their\ncorresponding modality. Each of the four features (frames, title, description,\naudios) will pass through a context gating layer before concatenation. Finally,\nour model will generate a vector in the latent space and select the frame that\nis most similar to this vector in the latent space. To the best of our\nknowledge, we are the first to propose a multi-modal deep learning model to\nselect video thumbnail, which beats the result from the previous\nState-of-The-Art models.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:10:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yu", "Zhifeng", ""], ["Shi", "Nanchun", ""]]}, {"id": "2101.00122", "submitter": "Xiulong Yang", "authors": "Xiulong Yang, Hui Ye, Yang Ye, Xiang Li, Shihao Ji", "title": "Generative Max-Mahalanobis Classifiers for Image Classification,\n  Generation and More", "comments": "Accepted as a conference paper at ECML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard\nsoftmax classifier can be reinterpreted as an energy-based model (EBM) for the\njoint distribution p(x,y); the resulting model can be optimized to improve\ncalibration, robustness, and out-of-distribution detection, while generating\nsamples rivaling the quality of recent GAN-based approaches. However, the\nsoftmax classifier that JEM exploits is inherently discriminative and its\nlatent feature space is not well formulated as probabilistic distributions,\nwhich may hinder its potential for image generation and incur training\ninstability. We hypothesize that generative classifiers, such as Linear\nDiscriminant Analysis (LDA), might be more suitable for image generation since\ngenerative classifiers model the data generation process explicitly. This paper\ntherefore investigates an LDA classifier for image classification and\ngeneration. In particular, the Max-Mahalanobis Classifier (MMC), a special case\nof LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be\ntrained discriminatively, generatively, or jointly for image classification and\ngeneration. Extensive experiments on multiple datasets show that GMMC achieves\nstate-of-the-art discriminative and generative performances, while\noutperforming JEM in calibration, adversarial robustness, and\nout-of-distribution detection by a significant margin. Our source code is\navailable at https://github.com/sndnyang/GMMC.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 00:42:04 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 13:35:35 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 22:30:49 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 21:29:26 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Yang", "Xiulong", ""], ["Ye", "Hui", ""], ["Ye", "Yang", ""], ["Li", "Xiang", ""], ["Ji", "Shihao", ""]]}, {"id": "2101.00150", "submitter": "Pablo Navarrete Michelini", "authors": "Pablo Navarrete Michelini, Wenbin Chen, Hanwen Liu, Dan Zhu, Xingqun\n  Jiang", "title": "Multi-Grid Back-Projection Networks", "comments": "Accepted for publication in IEEE Journal of Selected Topics in Signal\n  Processing (J-STSP). arXiv admin note: text overlap with arXiv:1809.10711", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Multi-Grid Back-Projection (MGBP) is a fully-convolutional network\narchitecture that can learn to restore images and videos with upscaling\nartifacts. Using the same strategy of multi-grid partial differential equation\n(PDE) solvers this multiscale architecture scales computational complexity\nefficiently with increasing output resolutions. The basic processing block is\ninspired in the iterative back-projection (IBP) algorithm and constitutes a\ntype of cross-scale residual block with feedback from low resolution\nreferences. The architecture performs in par with state-of-the-arts\nalternatives for regression targets that aim to recover an exact copy of a high\nresolution image or video from which only a downscale image is known. A\nperceptual quality target aims to create more realistic outputs by introducing\nartificial changes that can be different from a high resolution original\ncontent as long as they are consistent with the low resolution input. For this\ntarget we propose a strategy using noise inputs in different resolution scales\nto control the amount of artificial details generated in the output. The noise\ninput controls the amount of innovation that the network uses to create\nartificial realistic details. The effectiveness of this strategy is shown in\nbenchmarks and it is explained as a particular strategy to traverse the\nperception-distortion plane.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:17:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Michelini", "Pablo Navarrete", ""], ["Chen", "Wenbin", ""], ["Liu", "Hanwen", ""], ["Zhu", "Dan", ""], ["Jiang", "Xingqun", ""]]}, {"id": "2101.00173", "submitter": "Kai Yi", "authors": "Mohamed Elhoseiny, Kai Yi, Mohamed Elfeki", "title": "CIZSL++: Creativity Inspired Generative Zero-Shot Learning", "comments": "This paper is an extended version of a paper published on the\n  International Conference on Computer Vision (ICCV), held in Seoul, Republic\n  of Korea, October 27-Nov 2nd, 2019 CIZSL-v2 code is available here\n  https://github.com/Vision-CAIR/CIZSLv2. arXiv admin note: substantial text\n  overlap with arXiv:1904.01109", "journal-ref": "https://openaccess.thecvf.com/content_ICCV_2019/papers/Elhoseiny_Creativity_Inspired_Zero-Shot_Learning_ICCV_2019_paper.pdf", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zero-shot learning (ZSL) aims at understanding unseen categories with no\ntraining examples from class-level descriptions. To improve the discriminative\npower of ZSL, we model the visual learning process of unseen categories with\ninspiration from the psychology of human creativity for producing novel art.\nFirst, we propose CIZSL-v1 as a creativity inspired model for generative ZSL.\nWe relate ZSL to human creativity by observing that ZSL is about recognizing\nthe unseen, and creativity is about creating a likable unseen. We introduce a\nlearning signal inspired by creativity literature that explores the unseen\nspace with hallucinated class-descriptions and encourages careful deviation of\ntheir visual feature generations from seen classes while allowing knowledge\ntransfer from seen to unseen classes. Second, CIZSL-v2 is proposed as an\nimproved version of CIZSL-v1 for generative zero-shot learning. CIZSL-v2\nconsists of an investigation of additional inductive losses for unseen classes\nalong with a semantic guided discriminator. Empirically, we show consistently\nthat CIZSL losses can improve generative ZSL models on the challenging task of\ngeneralized ZSL from a noisy text on CUB and NABirds datasets. We also show the\nadvantage of our approach to Attribute-based ZSL on AwA2, aPY, and SUN\ndatasets. We also show that CIZSL-v2 has improved performance compared to\nCIZSL-v1.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:47:57 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 09:08:51 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Yi", "Kai", ""], ["Elfeki", "Mohamed", ""]]}, {"id": "2101.00200", "submitter": "Chang Keun Paik", "authors": "Chang Keun Paik, Naeun Ko, Youngjoon Yoo", "title": "More than just an auxiliary loss: Anti-spoofing Backbone Training via\n  Adversarial Pseudo-depth Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a new method of training pipeline is discussed to achieve\nsignificant performance on the task of anti-spoofing with RGB image. We explore\nand highlight the impact of using pseudo-depth to pre-train a network that will\nbe used as the backbone to the final classifier. While the usage of\npseudo-depth for anti-spoofing task is not a new idea on its own, previous\nendeavours utilize pseudo-depth simply as another medium to extract features\nfor performing prediction, or as part of many auxiliary losses in aiding the\ntraining of the main classifier, normalizing the importance of pseudo-depth as\njust another semantic information. Through this work, we argue that there\nexists a significant advantage in training the final classifier can be gained\nby the pre-trained generator learning to predict the corresponding pseudo-depth\nof a given facial image, from a Generative Adversarial Network framework. Our\nexperimental results indicate that our method results in a much more adaptable\nsystem that can generalize beyond intra-dataset samples, but to inter-dataset\nsamples, which it has never seen before during training. Quantitatively, our\nmethod approaches the baseline performance of the current state of the art\nanti-spoofing models with 15.8x less parameters used. Moreover, experiments\nshowed that the introduced methodology performs well only using basic binary\nlabel without additional semantic information which indicates potential\nbenefits of this work in industrial and application based environment where\ntrade-off between additional labelling and resources are considered.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 09:00:17 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 04:57:42 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Paik", "Chang Keun", ""], ["Ko", "Naeun", ""], ["Yoo", "Youngjoon", ""]]}, {"id": "2101.00203", "submitter": "Anish Madan", "authors": "Anish Madan, Ranjitha Prasad", "title": "B-SMALL: A Bayesian Neural Network approach to Sparse Model-Agnostic\n  Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in the learning-to-learn paradigm, also known as\nmeta-learning, where models infer on new tasks using a few training examples.\nRecently, meta-learning based methods have been widely used in few-shot\nclassification, regression, reinforcement learning, and domain adaptation. The\nmodel-agnostic meta-learning (MAML) algorithm is a well-known algorithm that\nobtains model parameter initialization at meta-training phase. In the meta-test\nphase, this initialization is rapidly adapted to new tasks by using gradient\ndescent. However, meta-learning models are prone to overfitting since there are\ninsufficient training tasks resulting in over-parameterized models with poor\ngeneralization performance for unseen tasks. In this paper, we propose a\nBayesian neural network based MAML algorithm, which we refer to as the B-SMALL\nalgorithm. The proposed framework incorporates a sparse variational loss term\nalongside the loss function of MAML, which uses a sparsifying approximated KL\ndivergence as a regularizer. We demonstrate the performance of B-MAML using\nclassification and regression tasks, and highlight that training a sparsifying\nBNN using MAML indeed improves the parameter footprint of the model while\nperforming at par or even outperforming the MAML approach. We also illustrate\napplicability of our approach in distributed sensor networks, where sparsity\nand meta-learning can be beneficial.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 09:19:48 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Madan", "Anish", ""], ["Prasad", "Ranjitha", ""]]}, {"id": "2101.00214", "submitter": "Ginni Garg", "authors": "Ginni Garg, Dheeraj Kumar, ArvinderPal, Yash Sonker, Ritu Garg", "title": "A Hybrid MLP-SVM Model for Classification using Spatial-Spectral\n  Features on Hyper-Spectral Images", "comments": "9 pages, 5 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many challenges in the classification of hyper spectral images such\nas large dimensionality, scarcity of labeled data and spatial variability of\nspectral signatures. In this proposed method, we make a hybrid classifier\n(MLP-SVM) using multilayer perceptron (MLP) and support vector machine (SVM)\nwhich aimed to improve the various classification parameters such as accuracy,\nprecision, recall, f-score and to predict the region without ground truth. In\nproposed method, outputs from the last hidden layer of the neural net-ork\nbecome the input to the SVM, which finally classifies into various desired\nclasses. In the present study, we worked on Indian Pines, U. Pavia and Salinas\ndataset with 16, 9, 16 classes and 200, 103 and 204 reflectance bands\nrespectively, which is provided by AVIRIS and ROSIS sensor of NASA Jet\npropulsion laboratory. The proposed method significantly increases the accuracy\non testing dataset to 93.22%, 96.87%, 93.81% as compare to 86.97%, 88.58%,\n88.85% and 91.61%, 96.20%, 90.68% based on individual classifiers SVM and MLP\non Indian Pines, U. Pavia and Salinas datasets respectively.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 11:47:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Garg", "Ginni", ""], ["Kumar", "Dheeraj", ""], ["ArvinderPal", "", ""], ["Sonker", "Yash", ""], ["Garg", "Ritu", ""]]}, {"id": "2101.00215", "submitter": "Ginni Garg", "authors": "Ginni Garg and Mantosh Biswas", "title": "Improved Neural Network based Plant Diseases Identification", "comments": "12 pages, 7 Tables, 5 figures", "journal-ref": null, "doi": "10.1007/978-981-15-5341-7_6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The agriculture sector is essential for every country because it provides a\nbasic income to a large number of people and food as well, which is a\nfundamental requirement to survive on this planet. We see as time passes,\nsignificant changes come in the present era, which begins with Green\nRevolution. Due to improper knowledge of plant diseases, farmers use\nfertilizers in excess, which ultimately degrade the quality of food. Earlier\nfarmers use experts to determine the type of plant disease, which was expensive\nand time-consuming. In today time, Image processing is used to recognize and\ncatalog plant diseases using the lesion region of plant leaf, and there are\ndifferent modus-operandi for plant disease scent from leaf using Neural\nNetworks (NN), Support Vector Machine (SVM), and others. In this paper, we\nimproving the architecture of the Neural Networking by working on ten different\ntypes of training algorithms and the proper choice of neurons in the concealed\nlayer. Our proposed approach gives 98.30% accuracy on general plant leaf\ndisease and 100% accuracy on specific plant leaf disease based on Bayesian\nregularization, automation of cluster and without over-fitting on considered\nplant diseases over various other implemented methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 11:49:56 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Garg", "Ginni", ""], ["Biswas", "Mantosh", ""]]}, {"id": "2101.00216", "submitter": "Ginni Garg", "authors": "Ginni Garg, Ritu Garg", "title": "Brain Tumor Detection and Classification based on Hybrid Ensemble\n  Classifier", "comments": "18 Pages, 12 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve patient survival and treatment outcomes, early diagnosis of brain\ntumors is an essential task. It is a difficult task to evaluate the magnetic\nresonance imaging (MRI) images manually. Thus, there is a need for digital\nmethods for tumor diagnosis with better accuracy. However, it is still a very\nchallenging task in assessing their shape, volume, boundaries, tumor detection,\nsize, segmentation, and classification. In this proposed work, we propose a\nhybrid ensemble method using Random Forest (RF), K-Nearest Neighbour, and\nDecision Tree (DT) (KNN-RF-DT) based on Majority Voting Method. It aims to\ncalculate the area of the tumor region and classify brain tumors as benign and\nmalignant. In the beginning, segmentation is done by using Otsu's Threshold\nmethod. Feature Extraction is done by using Stationary Wavelet Transform (SWT),\nPrinciple Component Analysis (PCA), and Gray Level Co-occurrence Matrix (GLCM),\nwhich gives thirteen features for classification. The classification is done by\nhybrid ensemble classifier (KNN-RF-DT) based on the Majority Voting method.\nOverall it aimed at improving the performance by traditional classifiers\ninstead of going to deep learning. Traditional classifiers have an advantage\nover deep learning algorithms because they require small datasets for training\nand have low computational time complexity, low cost to the users, and can be\neasily adopted by less skilled people. Overall, our proposed method is tested\nupon dataset of 2556 images, which are used in 85:15 for training and testing\nrespectively and gives good accuracy of 97.305%.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 11:52:29 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Garg", "Ginni", ""], ["Garg", "Ritu", ""]]}, {"id": "2101.00221", "submitter": "Yu Luo", "authors": "Xin Ma and Zhicheng Zhang and Danfeng Wang and Yu Luo and Hui Yuan", "title": "Adaptive Deconvolution-based stereo matching Net for Local Stereo\n  Matching", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning-based local stereo matching methods, larger image patches\nusually bring better stereo matching accuracy. However, it is unrealistic to\nincrease the size of the image patch size without restriction. Arbitrarily\nextending the patch size will change the local stereo matching method into the\nglobal stereo matching method, and the matching accuracy will be saturated. We\nsimplified the existing Siamese convolutional network by reducing the number of\nnetwork parameters and propose an efficient CNN based structure, namely\nAdaptive Deconvolution-based disparity matching Net (ADSM net) by adding\ndeconvolution layers to learn how to enlarge the size of input feature map for\nthe following convolution layers. Experimental results on the KITTI 2012 and\n2015 datasets demonstrate that the proposed method can achieve a good trade-off\nbetween accuracy and complexity.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 12:18:53 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ma", "Xin", ""], ["Zhang", "Zhicheng", ""], ["Wang", "Danfeng", ""], ["Luo", "Yu", ""], ["Yuan", "Hui", ""]]}, {"id": "2101.00232", "submitter": "Jun Ma", "authors": "Jun Ma", "title": "Cutting-edge 3D Medical Image Segmentation Methods in 2020: Are Happy\n  Families All Alike?", "comments": "Not submitted for publishing. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Segmentation is one of the most important and popular tasks in medical image\nanalysis, which plays a critical role in disease diagnosis, surgical planning,\nand prognosis evaluation. During the past five years, on the one hand,\nthousands of medical image segmentation methods have been proposed for various\norgans and lesions in different medical images, which become more and more\nchallenging to fairly compare different methods. On the other hand,\ninternational segmentation challenges can provide a transparent platform to\nfairly evaluate and compare different methods. In this paper, we present a\ncomprehensive review of the top methods in ten 3D medical image segmentation\nchallenges during 2020, covering a variety of tasks and datasets. We also\nidentify the \"happy-families\" practices in the cutting-edge segmentation\nmethods, which are useful for developing powerful segmentation approaches.\nFinally, we discuss open research problems that should be addressed in the\nfuture. We also maintain a list of cutting-edge segmentation methods at\n\\url{https://github.com/JunMa11/SOTA-MedSeg}.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 13:39:26 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ma", "Jun", ""]]}, {"id": "2101.00245", "submitter": "Er-Dong Guo", "authors": "Erdong Guo and David Draper", "title": "The Bayesian Method of Tensor Networks", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning is a powerful learning framework which combines the\nexternal information of the data (background information) with the internal\ninformation (training data) in a logically consistent way in inference and\nprediction. By Bayes rule, the external information (prior distribution) and\nthe internal information (training data likelihood) are combined coherently,\nand the posterior distribution and the posterior predictive (marginal)\ndistribution obtained by Bayes rule summarize the total information needed in\nthe inference and prediction, respectively. In this paper, we study the\nBayesian framework of the Tensor Network from two perspective. First, we\nintroduce the prior distribution to the weights in the Tensor Network and\npredict the labels of the new observations by the posterior predictive\n(marginal) distribution. Since the intractability of the parameter integral in\nthe normalization constant computation, we approximate the posterior predictive\ndistribution by Laplace approximation and obtain the out-product approximation\nof the hessian matrix of the posterior distribution of the Tensor Network\nmodel. Second, to estimate the parameters of the stationary mode, we propose a\nstable initialization trick to accelerate the inference process by which the\nTensor Network can converge to the stationary path more efficiently and stably\nwith gradient descent method. We verify our work on the MNIST, Phishing Website\nand Breast Cancer data set. We study the Bayesian properties of the Bayesian\nTensor Network by visualizing the parameters of the model and the decision\nboundaries in the two dimensional synthetic data set. For a application\npurpose, our work can reduce the overfitting and improve the performance of\nnormal Tensor Network model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 14:59:15 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Guo", "Erdong", ""], ["Draper", "David", ""]]}, {"id": "2101.00265", "submitter": "Xiaopeng Lu", "authors": "Xiaopeng Lu, Tiancheng Zhao, Kyusong Lee", "title": "VisualSparta: An Embarrassingly Simple Approach to Large-scale\n  Text-to-Image Search with Weighted Bag-of-words", "comments": "Accepted to ACL2021 (10 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-image retrieval is an essential task in cross-modal information\nretrieval, i.e., retrieving relevant images from a large and unlabelled dataset\ngiven textual queries. In this paper, we propose VisualSparta, a novel\n(Visual-text Sparse Transformer Matching) model that shows significant\nimprovement in terms of both accuracy and efficiency. VisualSparta is capable\nof outperforming previous state-of-the-art scalable methods in MSCOCO and\nFlickr30K. We also show that it achieves substantial retrieving speed\nadvantages, i.e., for a 1 million image index, VisualSparta using CPU gets\n~391X speedup compared to CPU vector search and ~5.4X speedup compared to\nvector search with GPU acceleration. Experiments show that this speed advantage\neven gets bigger for larger datasets because VisualSparta can be efficiently\nimplemented as an inverted index. To the best of our knowledge, VisualSparta is\nthe first transformer-based text-to-image retrieval model that can achieve\nreal-time searching for large-scale datasets, with significant accuracy\nimprovement compared to previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 16:29:17 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 03:10:15 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Lu", "Xiaopeng", ""], ["Zhao", "Tiancheng", ""], ["Lee", "Kyusong", ""]]}, {"id": "2101.00295", "submitter": "Ali Tourani", "authors": "Ali Tourani, Sajjad Soroori, Asadollah Shahbahrami, and Alireza\n  Akoushideh", "title": "Iranis: A Large-scale Dataset of Farsi License Plate Characters", "comments": "9 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing huge amounts of data is a fundamental demand when dealing with Deep\nNeural Networks (DNNs). Employing these algorithms to solve computer vision\nproblems resulted in the advent of various image datasets to feed the most\ncommon visual imagery deep structures, known as Convolutional Neural Networks\n(CNNs). In this regard, some datasets can be found that contain hundreds or\neven thousands of images for license plate detection and optical character\nrecognition purposes. However, no publicly available image dataset provides\nsuch data for the recognition of Farsi characters used in car license plates.\nThe gap has to be filled due to the numerous advantages of developing accurate\ndeep learning-based systems for law enforcement and surveillance purposes. This\npaper introduces a large-scale dataset that includes images of numbers and\ncharacters used in Iranian car license plates. The dataset, named Iranis,\ncontains more than 83,000 images of Farsi numbers and letters collected from\nreal-world license plate images captured by various cameras. The variety of\ninstances in terms of camera shooting angle, illumination, resolution, and\ncontrast make the dataset a proper choice for training DNNs. Dataset images are\nmanually annotated for object detection and image classification. Finally, and\nto build a baseline for Farsi character recognition, the paper provides a\nperformance analysis using a YOLO v.3 object detector.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 18:54:44 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Tourani", "Ali", ""], ["Soroori", "Sajjad", ""], ["Shahbahrami", "Asadollah", ""], ["Akoushideh", "Alireza", ""]]}, {"id": "2101.00304", "submitter": "Shahabeddin Sotudian", "authors": "Shahabeddin Sotudian and Mohammad Hossein Fazel Zarandi", "title": "Interval Type-2 Enhanced Possibilistic Fuzzy C-Means Clustering for Gene\n  Expression Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Both FCM and PCM clustering methods have been widely applied to pattern\nrecognition and data clustering. Nevertheless, FCM is sensitive to noise and\nPCM occasionally generates coincident clusters. PFCM is an extension of the PCM\nmodel by combining FCM and PCM, but this method still suffers from the\nweaknesses of PCM and FCM. In the current paper, the weaknesses of the PFCM\nalgorithm are corrected and the enhanced possibilistic fuzzy c-means (EPFCM)\nclustering algorithm is presented. EPFCM can still be sensitive to noise.\nTherefore, we propose an interval type-2 enhanced possibilistic fuzzy c-means\n(IT2EPFCM) clustering method by utilizing two fuzzifiers $(m_1, m_2)$ for fuzzy\nmemberships and two fuzzifiers $({\\theta}_1, {\\theta}_2)$ for possibilistic\ntypicalities. Our computational results show the superiority of the proposed\napproaches compared with several state-of-the-art techniques in the literature.\nFinally, the proposed methods are implemented for analyzing microarray gene\nexpression data.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 19:29:24 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sotudian", "Shahabeddin", ""], ["Zarandi", "Mohammad Hossein Fazel", ""]]}, {"id": "2101.00316", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Bo Hu, Xiongchang Liu, Jun Lu, Jane You, Lingsheng Kong", "title": "Energy-constrained Self-training for Unsupervised Domain Adaptation", "comments": "Accepted to 25th International Conference on Pattern Recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to transfer the knowledge on a\nlabeled source domain distribution to perform well on an unlabeled target\ndomain. Recently, the deep self-training involves an iterative process of\npredicting on the target domain and then taking the confident predictions as\nhard pseudo-labels for retraining. However, the pseudo-labels are usually\nunreliable, and easily leading to deviated solutions with propagated errors. In\nthis paper, we resort to the energy-based model and constrain the training of\nthe unlabeled target sample with the energy function minimization objective. It\ncan be applied as a simple additional regularization. In this framework, it is\npossible to gain the benefits of the energy-based model, while retaining strong\ndiscriminative performance following a plug-and-play fashion. We deliver\nextensive experiments on the most popular and large scale UDA benchmarks of\nimage classification as well as semantic segmentation to demonstrate its\ngenerality and effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 21:02:18 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Hu", "Bo", ""], ["Liu", "Xiongchang", ""], ["Lu", "Jun", ""], ["You", "Jane", ""], ["Kong", "Lingsheng", ""]]}, {"id": "2101.00317", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Linghao Jin, Xu Han, Jun Lu, Jane You, Lingsheng Kong", "title": "Identity-aware Facial Expression Recognition in Compressed Video", "comments": "Accepted as the Oral paper at ICPR 2020 (<4.4%). arXiv admin note:\n  substantial text overlap with arXiv:2010.10637", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper targets to explore the inter-subject variations eliminated facial\nexpression representation in the compressed video domain. Most of the previous\nmethods process the RGB images of a sequence, while the off-the-shelf and\nvaluable expression-related muscle movement already embedded in the compression\nformat. In the up to two orders of magnitude compressed domain, we can\nexplicitly infer the expression from the residual frames and possible to\nextract identity factors from the I frame with a pre-trained face recognition\nnetwork. By enforcing the marginal independent of them, the expression feature\nis expected to be purer for the expression and be robust to identity shifts. We\ndo not need the identity label or multiple expression samples from the same\nperson for identity elimination. Moreover, when the apex frame is annotated in\nthe dataset, the complementary constraint can be further added to regularize\nthe feature-level game. In testing, only the compressed residual frames are\nrequired to achieve expression prediction. Our solution can achieve comparable\nor better performance than the recent decoded image based methods on the\ntypical FER benchmarks with about 3$\\times$ faster inference with compressed\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 21:03:13 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 23:46:22 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Jin", "Linghao", ""], ["Han", "Xu", ""], ["Lu", "Jun", ""], ["You", "Jane", ""], ["Kong", "Lingsheng", ""]]}, {"id": "2101.00318", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Xiongchang Liu, Bo Hu, Wenxuan Ji, Fangxu Xing, Jun Lu,\n  Jane You, C.-C. Jay Kuo, Georges El Fakhri, Jonghye Woo", "title": "Subtype-aware Unsupervised Domain Adaptation for Medical Diagnosis", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in unsupervised domain adaptation (UDA) show that\ntransferable prototypical learning presents a powerful means for class\nconditional alignment, which encourages the closeness of cross-domain class\ncentroids. However, the cross-domain inner-class compactness and the underlying\nfine-grained subtype structure remained largely underexplored. In this work, we\npropose to adaptively carry out the fine-grained subtype-aware alignment by\nexplicitly enforcing the class-wise separation and subtype-wise compactness\nwith intermediate pseudo labels. Our key insight is that the unlabeled subtypes\nof a class can be divergent to one another with different conditional and label\nshifts, while inheriting the local proximity within a subtype. The cases of\nwith or without the prior information on subtype numbers are investigated to\ndiscover the underlying subtype structure in an online fashion. The proposed\nsubtype-aware dynamic UDA achieves promising results on medical diagnosis\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 21:04:50 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 15:09:03 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Liu", "Xiongchang", ""], ["Hu", "Bo", ""], ["Ji", "Wenxuan", ""], ["Xing", "Fangxu", ""], ["Lu", "Jun", ""], ["You", "Jane", ""], ["Kuo", "C. -C. Jay", ""], ["Fakhri", "Georges El", ""], ["Woo", "Jonghye", ""]]}, {"id": "2101.00336", "submitter": "Hanxun Huang", "authors": "Hanxun Huang, Xingjun Ma, Sarah M. Erfani, James Bailey", "title": "Neural Architecture Search via Combinatorial Multi-Armed Bandit", "comments": "10 pages, 7 figures", "journal-ref": "International Joint Conference on Neural Networks (IJCNN) 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Neural Architecture Search (NAS) has gained significant popularity as an\neffective tool for designing high performance deep neural networks (DNNs). NAS\ncan be performed via policy gradient, evolutionary algorithms, differentiable\narchitecture search or tree-search methods. While significant progress has been\nmade for both policy gradient and differentiable architecture search,\ntree-search methods have so far failed to achieve comparable accuracy or search\nefficiency. In this paper, we formulate NAS as a Combinatorial Multi-Armed\nBandit (CMAB) problem (CMAB-NAS). This allows the decomposition of a large\nsearch space into smaller blocks where tree-search methods can be applied more\neffectively and efficiently. We further leverage a tree-based method called\nNested Monte-Carlo Search to tackle the CMAB-NAS problem. On CIFAR-10, our\napproach discovers a cell structure that achieves a low error rate that is\ncomparable to the state-of-the-art, using only 0.58 GPU days, which is 20 times\nfaster than current tree-search methods. Moreover, the discovered structure\ntransfers well to large-scale datasets such as ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 23:29:33 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 14:13:15 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Hanxun", ""], ["Ma", "Xingjun", ""], ["Erfani", "Sarah M.", ""], ["Bailey", "James", ""]]}, {"id": "2101.00337", "submitter": "Tobias Schlosser", "authors": "Tobias Schlosser, Frederik Beuth, and Danny Kowerko", "title": "Biologically Inspired Hexagonal Deep Learning for Hexagonal Image\n  Generation", "comments": "Accepted for: 2020 IEEE 27th International Conference on Image\n  Processing (ICIP); this article draws heavily from arXiv:1911.11251", "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9190995", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas conventional state-of-the-art image processing systems of recording\nand output devices almost exclusively utilize square arranged methods,\nbiological models, however, suggest an alternative, evolutionarily-based\nstructure. Inspired by the human visual perception system, hexagonal image\nprocessing in the context of machine learning offers a number of key advantages\nthat can benefit both researchers and users alike. The hexagonal deep learning\nframework Hexnet leveraged in this contribution serves therefore the generation\nof hexagonal images by utilizing hexagonal deep neural networks (H-DNN). As the\nresults of our created test environment show, the proposed models can surpass\ncurrent approaches of conventional image generation. While resulting in a\nreduction of the models' complexity in the form of trainable parameters, they\nfurthermore allow an increase of test rates in comparison to their square\ncounterparts.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 23:30:21 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 23:21:50 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Schlosser", "Tobias", ""], ["Beuth", "Frederik", ""], ["Kowerko", "Danny", ""]]}, {"id": "2101.00339", "submitter": "Angus Baird", "authors": "Angus Baird and Stefano Giani", "title": "An Artificial Intelligence System for Combined Fruit Detection and\n  Georeferencing, Using RTK-Based Perspective Projection in Drone Imagery", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents an Artificial Intelligence (AI) system, based on the\nFaster Region-Based Convolution Neural Network (Faster R-CNN) framework, which\ndetects and counts apples from oblique, aerial drone imagery of giant\ncommercial orchards. To reduce computational cost, a novel precursory stage to\nthe network is designed to preprocess raw imagery into cropped images of\nindividual trees. Unique geospatial identifiers are allocated to these using\nthe perspective projection model. This employs Real-Time Kinematic (RTK) data,\nDigital Terrain and Surface Models (DTM and DSM), as well as internal and\nexternal camera parameters. The bulk of experiments however focus on tuning\nhyperparameters in the detection network itself. Apples which are on trees and\napples which are on the ground are treated as separate classes. A mean Average\nPrecision (mAP) metric, calibrated by the size of the two classes, is devised\nto mitigate spurious results. Anchor box design is of key interest due to the\nscale of the apples. As such, a k-means clustering approach, never before seen\nin literature for Faster R-CNN, resulted in the most significant improvements\nto calibrated mAP. Other experiments showed that the maximum number of box\nproposals should be 225; the initial learning rate of 0.001 is best applied to\nthe adaptive RMS Prop optimiser; and ResNet 101 is the ideal base feature\nextractor when considering mAP and, to a lesser extent, inference time. The\namalgamation of the optimal hyperparameters leads to a model with a calibrated\nmAP of 0.7627.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 23:39:55 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Baird", "Angus", ""], ["Giani", "Stefano", ""]]}, {"id": "2101.00350", "submitter": "Abhishek Das", "authors": "Abhishek Das, Japsimar Singh Wahi, Mansi Anand, Yugant Rana", "title": "Multi-Image Steganography Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Steganography is the science of hiding a secret message within an ordinary\npublic message. Over the years, steganography has been used to encode a lower\nresolution image into a higher resolution image by simple methods like LSB\nmanipulation. We aim to utilize deep neural networks for the encoding and\ndecoding of multiple secret images inside a single cover image of the same\nresolution.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 01:51:38 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Das", "Abhishek", ""], ["Wahi", "Japsimar Singh", ""], ["Anand", "Mansi", ""], ["Rana", "Yugant", ""]]}, {"id": "2101.00359", "submitter": "Mingjian Zhu", "authors": "Mingjian Zhu, Chenrui Duan, Changbin Yu", "title": "Video Captioning in Compressed Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches in video captioning concentrate on exploring global frame\nfeatures in the uncompressed videos, while the free of charge and critical\nsaliency information already encoded in the compressed videos is generally\nneglected. We propose a video captioning method which operates directly on the\nstored compressed videos. To learn a discriminative visual representation for\nvideo captioning, we design a residuals-assisted encoder (RAE), which spots\nregions of interest in I-frames under the assistance of the residuals frames.\nFirst, we obtain the spatial attention weights by extracting features of\nresiduals as the saliency value of each location in I-frame and design a\nspatial attention module to refine the attention weights. We further propose a\ntemporal gate module to determine how much the attended features contribute to\nthe caption generation, which enables the model to resist the disturbance of\nsome noisy signals in the compressed videos. Finally, Long Short-Term Memory is\nutilized to decode the visual representations into descriptions. We evaluate\nour method on two benchmark datasets and demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 03:06:03 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhu", "Mingjian", ""], ["Duan", "Chenrui", ""], ["Yu", "Changbin", ""]]}, {"id": "2101.00364", "submitter": "KitIan Kou", "authors": "Jifei Miao and Kit Ian Kou", "title": "Quaternion higher-order singular value decomposition and its\n  applications in color image processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order singular value decomposition (HOSVD) is one of the most\nefficient tensor decomposition techniques. It has the salient ability to\nrepresent high_dimensional data and extract features. In more recent years, the\nquaternion has proven to be a very suitable tool for color pixel representation\nas it can well preserve cross-channel correlation of color channels. Motivated\nby the advantages of the HOSVD and the quaternion tool, in this paper, we\ngeneralize the HOSVD to the quaternion domain and define quaternion-based HOSVD\n(QHOSVD). Due to the non-commutability of quaternion multiplication, QHOSVD is\nnot a trivial extension of the HOSVD. They have similar but different\ncalculation procedures. The defined QHOSVD can be widely used in various visual\ndata processing with color pixels. In this paper, we present two applications\nof the defined QHOSVD in color image processing: multi_focus color image fusion\nand color image denoising. The experimental results on the two applications\nrespectively demonstrate the competitive performance of the proposed methods\nover some existing ones.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 03:54:56 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Miao", "Jifei", ""], ["Kou", "Kit Ian", ""]]}, {"id": "2101.00373", "submitter": "Siyuan Shen", "authors": "Siyuan Shen, Zi Wang, Ping Liu, Zhengqing Pan, Ruiqian Li, Tian Gao,\n  Shiying Li, and Jingyi Yu", "title": "Non-line-of-Sight Imaging via Neural Transient Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a neural modeling framework for Non-Line-of-Sight (NLOS) imaging.\nPrevious solutions have sought to explicitly recover the 3D geometry (e.g., as\npoint clouds) or voxel density (e.g., within a pre-defined volume) of the\nhidden scene. In contrast, inspired by the recent Neural Radiance Field (NeRF)\napproach, we use a multi-layer perceptron (MLP) to represent the neural\ntransient field or NeTF. However, NeTF measures the transient over spherical\nwavefronts rather than the radiance along lines. We therefore formulate a\nspherical volume NeTF reconstruction pipeline, applicable to both confocal and\nnon-confocal setups. Compared with NeRF, NeTF samples a much sparser set of\nviewpoints (scanning spots) and the sampling is highly uneven. We thus\nintroduce a Monte Carlo technique to improve the robustness in the\nreconstruction. Comprehensive experiments on synthetic and real datasets\ndemonstrate NeTF provides higher quality reconstruction and preserves fine\ndetails largely missing in the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 05:20:54 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 05:28:14 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Shen", "Siyuan", ""], ["Wang", "Zi", ""], ["Liu", "Ping", ""], ["Pan", "Zhengqing", ""], ["Li", "Ruiqian", ""], ["Gao", "Tian", ""], ["Li", "Shiying", ""], ["Yu", "Jingyi", ""]]}, {"id": "2101.00395", "submitter": "Masahiro Toyoura", "authors": "Siqiang Chen, Masahiro Toyoura, Takamasa Terada, Xiaoyang Mao, Gang Xu", "title": "Image-based Textile Decoding", "comments": null, "journal-ref": "Integrated Computer-Aided Engineering, Pre-press, pp. 1-14, 2020", "doi": "10.3233/ICA-200647", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A textile fabric consists of countless parallel vertical yarns (warps) and\nhorizontal yarns (wefts). While common looms can weave repetitive patterns,\nJacquard looms can weave the patterns without repetition restrictions. A\npattern in which the warps and wefts cross on a grid is defined in a binary\nmatrix. The binary matrix can define which warp and weft is on top at each grid\npoint of the Jacquard fabric. The process can be regarded as encoding from\npattern to textile. In this work, we propose a decoding method that generates a\nbinary pattern from a textile fabric that has been already woven. We could not\nuse a deep neural network to learn the process based solely on the training set\nof patterns and observed fabric images. The crossing points in the observed\nimage were not completely located on the grid points, so it was difficult to\ntake a direct correspondence between the fabric images and the pattern\nrepresented by the matrix in the framework of deep learning. Therefore, we\npropose a method that can apply the framework of deep learning via the\nintermediate representation of patterns and images. We show how to convert a\npattern into an intermediate representation and how to reconvert the output\ninto a pattern and confirm its effectiveness. In this experiment, we confirmed\nthat 93% of correct pattern was obtained by decoding the pattern from the\nactual fabric images and weaving them again.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 07:41:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chen", "Siqiang", ""], ["Toyoura", "Masahiro", ""], ["Terada", "Takamasa", ""], ["Mao", "Xiaoyang", ""], ["Xu", "Gang", ""]]}, {"id": "2101.00431", "submitter": "Matteo Poggi", "authors": "Matteo Poggi, Seungryong Kim, Fabio Tosi, Sunok Kim, Filippo Aleotti,\n  Dongbo Min, Kwanghoon Sohn, Stefano Mattoccia", "title": "On the confidence of stereo matching in a deep-learning era: a\n  quantitative evaluation", "comments": "TPAMI final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo matching is one of the most popular techniques to estimate dense depth\nmaps by finding the disparity between matching pixels on two, synchronized and\nrectified images. Alongside with the development of more accurate algorithms,\nthe research community focused on finding good strategies to estimate the\nreliability, i.e. the confidence, of estimated disparity maps. This information\nproves to be a powerful cue to naively find wrong matches as well as to improve\nthe overall effectiveness of a variety of stereo algorithms according to\ndifferent strategies. In this paper, we review more than ten years of\ndevelopments in the field of confidence estimation for stereo matching. We\nextensively discuss and evaluate existing confidence measures and their\nvariants, from hand-crafted ones to the most recent, state-of-the-art learning\nbased methods. We study the different behaviors of each measure when applied to\na pool of different stereo algorithms and, for the first time in literature,\nwhen paired with a state-of-the-art deep stereo network. Our experiments,\ncarried out on five different standard datasets, provide a comprehensive\noverview of the field, highlighting in particular both strengths and\nlimitations of learning-based strategies.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:40:17 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 10:01:18 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 19:15:54 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Poggi", "Matteo", ""], ["Kim", "Seungryong", ""], ["Tosi", "Fabio", ""], ["Kim", "Sunok", ""], ["Aleotti", "Filippo", ""], ["Min", "Dongbo", ""], ["Sohn", "Kwanghoon", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "2101.00440", "submitter": "Alexandros Stergiou MSc", "authors": "Alexandros Stergiou, Ronald Poppe, Grigorios Kalliatakis", "title": "Refining activation downsampling with SoftPool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) use pooling to decrease the size of\nactivation maps. This process is crucial to increase the receptive fields and\nto reduce computational requirements of subsequent convolutions. An important\nfeature of the pooling operation is the minimization of information loss, with\nrespect to the initial activation maps, without a significant impact on the\ncomputation and memory overhead. To meet these requirements, we propose\nSoftPool: a fast and efficient method for exponentially weighted activation\ndownsampling. Through experiments across a range of architectures and pooling\nmethods, we demonstrate that SoftPool can retain more information in the\nreduced activation maps. This refined downsampling leads to improvements in a\nCNN's classification accuracy. Experiments with pooling layer substitutions on\nImageNet1K show an increase in accuracy over both original architectures and\nother pooling methods. We also test SoftPool on video datasets for action\nrecognition. Again, through the direct replacement of pooling layers, we\nobserve consistent performance improvements while computational loads and\nmemory requirements remain limited.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 12:09:49 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 08:52:31 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 09:51:42 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Stergiou", "Alexandros", ""], ["Poppe", "Ronald", ""], ["Kalliatakis", "Grigorios", ""]]}, {"id": "2101.00442", "submitter": "Amirreza Mahbod", "authors": "Amirreza Mahbod, Gerald Schaefer, Benjamin Bancher, Christine L\\\"ow,\n  Georg Dorffner, Rupert Ecker, Isabella Ellinger", "title": "CryoNuSeg: A Dataset for Nuclei Instance Segmentation of Cryosectioned\n  H&E-Stained Histological Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.compbiomed.2021.104349", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclei instance segmentation plays an important role in the analysis of\nHematoxylin and Eosin (H&E)-stained images. While supervised deep learning\n(DL)-based approaches represent the state-of-the-art in automatic nuclei\ninstance segmentation, annotated datasets are required to train these models.\nThere are two main types of tissue processing protocols, namely formalin-fixed\nparaffin-embedded samples (FFPE) and frozen tissue samples (FS). Although\nFFPE-derived H&E stained tissue sections are the most widely used samples, H&E\nstaining on frozen sections derived from FS samples is a relevant method in\nintra-operative surgical sessions as it can be performed fast. Due to\ndifferences in the protocols of these two types of samples, the derived images\nand in particular the nuclei appearance may be different in the acquired whole\nslide images. Analysis of FS-derived H&E stained images can be more challenging\nas rapid preparation, staining, and scanning of FS sections may lead to\ndeterioration in image quality.\n  In this paper, we introduce CryoNuSeg, the first fully annotated FS-derived\ncryosectioned and H&E-stained nuclei instance segmentation dataset. The dataset\ncontains images from 10 human organs that were not exploited in other publicly\navailable datasets, and is provided with three manual mark-ups to allow\nmeasuring intra-observer and inter-observer variability. Moreover, we\ninvestigate the effects of tissue fixation/embedding protocol (i.e., FS or\nFFPE) on the automatic nuclei instance segmentation performance of one of the\nstate-of-the-art DL approaches. We also create a baseline segmentation\nbenchmark for the dataset that can be used in future research.\n  A step-by-step guide to generate the dataset as well as the full dataset and\nother detailed information are made available to fellow researchers at\nhttps://github.com/masih4/CryoNuSeg.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 12:34:06 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Mahbod", "Amirreza", ""], ["Schaefer", "Gerald", ""], ["Bancher", "Benjamin", ""], ["L\u00f6w", "Christine", ""], ["Dorffner", "Georg", ""], ["Ecker", "Rupert", ""], ["Ellinger", "Isabella", ""]]}, {"id": "2101.00443", "submitter": "Sourav Garg", "authors": "Sourav Garg, Niko S\\\"underhauf, Feras Dayoub, Douglas Morrison,\n  Akansel Cosgun, Gustavo Carneiro, Qi Wu, Tat-Jun Chin, Ian Reid, Stephen\n  Gould, Peter Corke, Michael Milford", "title": "Semantics for Robotic Mapping, Perception and Interaction: A Survey", "comments": "81 pages, 1 figure, published in Foundations and Trends in Robotics,\n  2020", "journal-ref": "Foundations and Trends in Robotics: Vol. 8: No. 1-2, pp 1-224\n  (2020)", "doi": "10.1561/2300000059", "report-no": null, "categories": "cs.RO cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots to navigate and interact more richly with the world around them,\nthey will likely require a deeper understanding of the world in which they\noperate. In robotics and related research fields, the study of understanding is\noften referred to as semantics, which dictates what does the world \"mean\" to a\nrobot, and is strongly tied to the question of how to represent that meaning.\nWith humans and robots increasingly operating in the same world, the prospects\nof human-robot interaction also bring semantics and ontology of natural\nlanguage into the picture. Driven by need, as well as by enablers like\nincreasing availability of training data and computational resources, semantics\nis a rapidly growing research area in robotics. The field has received\nsignificant attention in the research literature to date, but most reviews and\nsurveys have focused on particular aspects of the topic: the technical research\nissues regarding its use in specific robotic topics like mapping or\nsegmentation, or its relevance to one particular application domain like\nautonomous driving. A new treatment is therefore required, and is also timely\nbecause so much relevant research has occurred since many of the key surveys\nwere published. This survey therefore provides an overarching snapshot of where\nsemantics in robotics stands today. We establish a taxonomy for semantics\nresearch in or relevant to robotics, split into four broad categories of\nactivity, in which semantics are extracted, used, or both. Within these broad\ncategories we survey dozens of major topics including fundamentals from the\ncomputer vision field and key robotics research areas utilizing semantics,\nincluding mapping, navigation and interaction with the world. The survey also\ncovers key practical considerations, including enablers like increased data\navailability and improved computational hardware, and major application areas\nwhere...\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 12:34:39 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Garg", "Sourav", ""], ["S\u00fcnderhauf", "Niko", ""], ["Dayoub", "Feras", ""], ["Morrison", "Douglas", ""], ["Cosgun", "Akansel", ""], ["Carneiro", "Gustavo", ""], ["Wu", "Qi", ""], ["Chin", "Tat-Jun", ""], ["Reid", "Ian", ""], ["Gould", "Stephen", ""], ["Corke", "Peter", ""], ["Milford", "Michael", ""]]}, {"id": "2101.00468", "submitter": "Alina Roitberg", "authors": "Alina Roitberg, Monica Haurilet, Manuel Martinez and Rainer\n  Stiefelhagen", "title": "Uncertainty-sensitive Activity Recognition: a Reliability Benchmark and\n  the CARING Models", "comments": "Accepted as oral at ICPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond assigning the correct class, an activity recognition model should also\nbe able to determine, how certain it is in its predictions. We present the\nfirst study of how welthe confidence values of modern action recognition\narchitectures indeed reflect the probability of the correct outcome and propose\na learning-based approach for improving it. First, we extend two popular action\nrecognition datasets with a reliability benchmark in form of the expected\ncalibration error and reliability diagrams. Since our evaluation highlights\nthat confidence values of standard action recognition architectures do not\nrepresent the uncertainty well, we introduce a new approach which learns to\ntransform the model output into realistic confidence estimates through an\nadditional calibration network. The main idea of our Calibrated Action\nRecognition with Input Guidance (CARING) model is to learn an optimal scaling\nparameter depending on the video representation. We compare our model with the\nnative action recognition networks and the temperature scaling approach - a\nwide spread calibration method utilized in image classification. While\ntemperature scaling alone drastically improves the reliability of the\nconfidence values, our CARING method consistently leads to the best uncertainty\nestimates in all benchmark settings.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 15:41:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Roitberg", "Alina", ""], ["Haurilet", "Monica", ""], ["Martinez", "Manuel", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2101.00483", "submitter": "Junming Zhang", "authors": "Junming Zhang, Ming-Yuan Yu, Ram Vasudevan, Matthew Johnson-Roberson", "title": "Learning Rotation-Invariant Representations of Point Clouds Using\n  Aligned Edge Convolutional Neural Networks", "comments": "3D Vision Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis is an area of increasing interest due to the development\nof 3D sensors that are able to rapidly measure the depth of scenes accurately.\nUnfortunately, applying deep learning techniques to perform point cloud\nanalysis is non-trivial due to the inability of these methods to generalize to\nunseen rotations. To address this limitation, one usually has to augment the\ntraining data, which can lead to extra computation and require larger model\ncomplexity. This paper proposes a new neural network called the Aligned Edge\nConvolutional Neural Network (AECNN) that learns a feature representation of\npoint clouds relative to Local Reference Frames (LRFs) to ensure invariance to\nrotation. In particular, features are learned locally and aligned with respect\nto the LRF of an automatically computed reference point. The proposed approach\nis evaluated on point cloud classification and part segmentation tasks. This\npaper illustrates that the proposed technique outperforms a variety of state of\nthe art approaches (even those trained on augmented datasets) in terms of\nrobustness to rotation without requiring any additional data augmentation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 17:36:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Junming", ""], ["Yu", "Ming-Yuan", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "2101.00489", "submitter": "Adriano Pinto", "authors": "Adriano Pinto, S\\'ergio Pereira, Raphael Meier, Roland Wiest, Victor\n  Alves, Mauricio Reyes, Carlos A.Silva", "title": "Combining unsupervised and supervised learning for predicting the final\n  stroke lesion", "comments": "Accepted at Medical Image Analysis (MedIA)", "journal-ref": null, "doi": "10.1016/j.media.2020.101888", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Predicting the final ischaemic stroke lesion provides crucial information\nregarding the volume of salvageable hypoperfused tissue, which helps physicians\nin the difficult decision-making process of treatment planning and\nintervention. Treatment selection is influenced by clinical diagnosis, which\nrequires delineating the stroke lesion, as well as characterising cerebral\nblood flow dynamics using neuroimaging acquisitions. Nonetheless, predicting\nthe final stroke lesion is an intricate task, due to the variability in lesion\nsize, shape, location and the underlying cerebral haemodynamic processes that\noccur after the ischaemic stroke takes place. Moreover, since elapsed time\nbetween stroke and treatment is related to the loss of brain tissue, assessing\nand predicting the final stroke lesion needs to be performed in a short period\nof time, which makes the task even more complex. Therefore, there is a need for\nautomatic methods that predict the final stroke lesion and support physicians\nin the treatment decision process. We propose a fully automatic deep learning\nmethod based on unsupervised and supervised learning to predict the final\nstroke lesion after 90 days. Our aim is to predict the final stroke lesion\nlocation and extent, taking into account the underlying cerebral blood flow\ndynamics that can influence the prediction. To achieve this, we propose a\ntwo-branch Restricted Boltzmann Machine, which provides specialized data-driven\nfeatures from different sets of standard parametric Magnetic Resonance Imaging\nmaps. These data-driven feature maps are then combined with the parametric\nMagnetic Resonance Imaging maps, and fed to a Convolutional and Recurrent\nNeural Network architecture. We evaluated our proposal on the publicly\navailable ISLES 2017 testing dataset, reaching a Dice score of 0.38, Hausdorff\nDistance of 29.21 mm, and Average Symmetric Surface Distance of 5.52 mm.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 17:56:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Pinto", "Adriano", ""], ["Pereira", "S\u00e9rgio", ""], ["Meier", "Raphael", ""], ["Wiest", "Roland", ""], ["Alves", "Victor", ""], ["Reyes", "Mauricio", ""], ["Silva", "Carlos A.", ""]]}, {"id": "2101.00490", "submitter": "Adriano Pinto", "authors": "Carlos A. Silva, Adriano Pinto, S\\'ergio Pereira, and Ana Lopes", "title": "Multi-stage Deep Layer Aggregation for Brain Tumor Segmentation", "comments": "MICCAI 2020 BrainLes Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gliomas are among the most aggressive and deadly brain tumors. This paper\ndetails the proposed Deep Neural Network architecture for brain tumor\nsegmentation from Magnetic Resonance Images. The architecture consists of a\ncascade of three Deep Layer Aggregation neural networks, where each stage\nelaborates the response using the feature maps and the probabilities of the\nprevious stage, and the MRI channels as inputs. The neuroimaging data are part\nof the publicly available Brain Tumor Segmentation (BraTS) 2020 challenge\ndataset, where we evaluated our proposal in the BraTS 2020 Validation and Test\nsets. In the Test set, the experimental results achieved a Dice score of\n0.8858, 0.8297 and 0.7900, with an Hausdorff Distance of 5.32 mm, 22.32 mm and\n20.44 mm for the whole tumor, core tumor and enhanced tumor, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 17:59:30 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Silva", "Carlos A.", ""], ["Pinto", "Adriano", ""], ["Pereira", "S\u00e9rgio", ""], ["Lopes", "Ana", ""]]}, {"id": "2101.00522", "submitter": "Mohammad Rostami", "authors": "Serban Stan, Mohammad Rostami", "title": "Privacy Preserving Domain Adaptation for Semantic Segmentation of\n  Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have led to significant improvements in\ntasks involving semantic segmentation of images. CNNs are vulnerable in the\narea of biomedical image segmentation because of distributional gap between two\nsource and target domains with different data modalities which leads to domain\nshift. Domain shift makes data annotations in new modalities necessary because\nmodels must be retrained from scratch. Unsupervised domain adaptation (UDA) is\nproposed to adapt a model to new modalities using solely unlabeled target\ndomain data. Common UDA algorithms require access to data points in the source\ndomain which may not be feasible in medical imaging due to privacy concerns. In\nthis work, we develop an algorithm for UDA in a privacy-constrained setting,\nwhere the source domain data is inaccessible. Our idea is based on encoding the\ninformation from the source samples into a prototypical distribution that is\nused as an intermediate distribution for aligning the target domain\ndistribution with the source domain distribution. We demonstrate the\neffectiveness of our algorithm by comparing it to state-of-the-art medical\nimage semantic segmentation approaches on two medical image semantic\nsegmentation datasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 22:12:42 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 02:58:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Stan", "Serban", ""], ["Rostami", "Mohammad", ""]]}, {"id": "2101.00524", "submitter": "Sudipta Banerjee", "authors": "Sudipta Banerjee and Arun Ross", "title": "One-shot Representational Learning for Joint Biometric and Device\n  Authentication", "comments": "Accepted in 25th International Conference on Pattern Recognition\n  (ICPR), (Milan, Italy), January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a method to simultaneously perform (i) biometric\nrecognition (i.e., identify the individual), and (ii) device recognition,\n(i.e., identify the device) from a single biometric image, say, a face image,\nusing a one-shot schema. Such a joint recognition scheme can be useful in\ndevices such as smartphones for enhancing security as well as privacy. We\npropose to automatically learn a joint representation that encapsulates both\nbiometric-specific and sensor-specific features. We evaluate the proposed\napproach using iris, face and periocular images acquired using near-infrared\niris sensors and smartphone cameras. Experiments conducted using 14,451 images\nfrom 15 sensors resulted in a rank-1 identification accuracy of upto 99.81% and\na verification accuracy of upto 100% at a false match rate of 1%.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 22:29:29 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Banerjee", "Sudipta", ""], ["Ross", "Arun", ""]]}, {"id": "2101.00529", "submitter": "Pengchuan Zhang", "authors": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang,\n  Lijuan Wang, Yejin Choi, Jianfeng Gao", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a detailed study of improving visual representations for\nvision language (VL) tasks and develops an improved object detection model to\nprovide object-centric representations of images. Compared to the most widely\nused \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new\nmodel is bigger, better-designed for VL tasks, and pre-trained on much larger\ntraining corpora that combine multiple public annotated object detection\ndatasets. Therefore, it can generate representations of a richer collection of\nvisual objects and concepts. While previous VL research focuses mainly on\nimproving the vision-language fusion model and leaves the object detection\nmodel improvement untouched, we show that visual features matter significantly\nin VL models. In our experiments we feed the visual features generated by the\nnew object detection model into a Transformer-based VL fusion model \\oscar\n\\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the\nVL model and fine-tune it on a wide range of downstream VL tasks. Our results\nshow that the new visual features significantly improve the performance across\nall VL tasks, creating new state-of-the-art results on seven public benchmarks.\nWe will release the new object detection model to public.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 23:35:27 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 01:27:16 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Zhang", "Pengchuan", ""], ["Li", "Xiujun", ""], ["Hu", "Xiaowei", ""], ["Yang", "Jianwei", ""], ["Zhang", "Lei", ""], ["Wang", "Lijuan", ""], ["Choi", "Yejin", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2101.00535", "submitter": "Sharif Amit Kamran", "authors": "Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli,\n  Stewart Lee Zuckerbrod, Kenton M. Sanders, Salah A. Baker", "title": "RV-GAN: Segmenting Retinal Vascular Structure in Fundus Photographs\n  using a Novel Multi-scale Generative Adversarial Network", "comments": "Accepted to MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High fidelity segmentation of both macro and microvascular structure of the\nretina plays a pivotal role in determining degenerative retinal diseases, yet\nit is a difficult problem. Due to successive resolution loss in the encoding\nphase combined with the inability to recover this lost information in the\ndecoding phase, autoencoding based segmentation approaches are limited in their\nability to extract retinal microvascular structure. We propose RV-GAN, a new\nmulti-scale generative architecture for accurate retinal vessel segmentation to\nalleviate this. The proposed architecture uses two generators and two\nmulti-scale autoencoding discriminators for better microvessel localization and\nsegmentation. In order to avoid the loss of fidelity suffered by traditional\nGAN-based segmentation systems, we introduce a novel weighted feature matching\nloss. This new loss incorporates and prioritizes features from the\ndiscriminator's decoder over the encoder. Doing so combined with the fact that\nthe discriminator's decoder attempts to determine real or fake images at the\npixel level better preserves macro and microvascular structure. By combining\nreconstruction and weighted feature matching loss, the proposed architecture\nachieves an area under the curve (AUC) of 0.9887, 0.9914, and 0.9887 in\npixel-wise segmentation of retinal vasculature from three publicly available\ndatasets, namely DRIVE, CHASE-DB1, and STARE, respectively. Additionally,\nRV-GAN outperforms other architectures in two additional relevant metrics, mean\nintersection-over-union (Mean-IOU) and structural similarity measure (SSIM).\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 01:04:49 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 08:22:07 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Kamran", "Sharif Amit", ""], ["Hossain", "Khondker Fariha", ""], ["Tavakkoli", "Alireza", ""], ["Zuckerbrod", "Stewart Lee", ""], ["Sanders", "Kenton M.", ""], ["Baker", "Salah A.", ""]]}, {"id": "2101.00545", "submitter": "Ashraful Islam", "authors": "Ashraful Islam, Chengjiang Long, Richard Radke", "title": "A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action\n  Localization", "comments": "Extended version/preprint of a AAAI 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weakly supervised temporal action localization is a challenging vision task\ndue to the absence of ground-truth temporal locations of actions in the\ntraining videos. With only video-level supervision during training, most\nexisting methods rely on a Multiple Instance Learning (MIL) framework to\npredict the start and end frame of each action category in a video. However,\nthe existing MIL-based approach has a major limitation of only capturing the\nmost discriminative frames of an action, ignoring the full extent of an\nactivity. Moreover, these methods cannot model background activity effectively,\nwhich plays an important role in localizing foreground activities. In this\npaper, we present a novel framework named HAM-Net with a hybrid attention\nmechanism which includes temporal soft, semi-soft and hard attentions to\naddress these issues. Our temporal soft attention module, guided by an\nauxiliary background class in the classification module, models the background\nactivity by introducing an \"action-ness\" score for each video snippet.\nMoreover, our temporal semi-soft and hard attention modules, calculating two\nattention scores for each video snippet, help to focus on the less\ndiscriminative frames of an action to capture the full action boundary. Our\nproposed approach outperforms recent state-of-the-art methods by at least 2.2%\nmAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at\nIoU threshold 0.75 on the ActivityNet1.2 dataset. Code can be found at:\nhttps://github.com/asrafulashiq/hamnet.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 03:08:18 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 02:30:59 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 23:15:35 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Islam", "Ashraful", ""], ["Long", "Chengjiang", ""], ["Radke", "Richard", ""]]}, {"id": "2101.00561", "submitter": "Tianxiao Zhang", "authors": "Tianxiao Zhang, Wenchi Ma, Guanghui Wang", "title": "Six-channel Image Representation for Cross-domain Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep learning models are data-driven and the excellent performance is\nhighly dependent on the abundant and diverse datasets. However, it is very hard\nto obtain and label the datasets of some specific scenes or applications. If we\ntrain the detector using the data from one domain, it cannot perform well on\nthe data from another domain due to domain shift, which is one of the big\nchallenges of most object detection models. To address this issue, some\nimage-to-image translation techniques have been employed to generate some fake\ndata of some specific scenes to train the models. With the advent of Generative\nAdversarial Networks (GANs), we could realize unsupervised image-to-image\ntranslation in both directions from a source to a target domain and from the\ntarget to the source domain. In this study, we report a new approach to making\nuse of the generated images. We propose to concatenate the original 3-channel\nimages and their corresponding GAN-generated fake images to form 6-channel\nrepresentations of the dataset, hoping to address the domain shift problem\nwhile exploiting the success of available detection models. The idea of\naugmented data representation may inspire further study on object detection and\nother applications.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 04:50:03 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 21:03:25 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhang", "Tianxiao", ""], ["Ma", "Wenchi", ""], ["Wang", "Guanghui", ""]]}, {"id": "2101.00562", "submitter": "Arkabandhu Chowdhury", "authors": "Arkabandhu Chowdhury, Mingchao Jiang, Chris Jermaine", "title": "Few-shot Image Classification: Just Use a Library of Pre-trained Feature\n  Extractors and a Simple Classifier", "comments": "17 pages including appendix and references. 2 figures in the main\n  paper, 1 figure in appendix. Under submission at a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent papers have suggested that transfer learning can outperform\nsophisticated meta-learning methods for few-shot image classification. We take\nthis hypothesis to its logical conclusion, and suggest the use of an ensemble\nof high-quality, pre-trained feature extractors for few-shot image\nclassification. We show experimentally that a library of pre-trained feature\nextractors combined with a simple feed-forward network learned with an\nL2-regularizer can be an excellent option for solving cross-domain few-shot\nimage classification. Our experimental results suggest that this simpler\nsample-efficient approach far outperforms several well-established\nmeta-learning algorithms on a variety of few-shot tasks.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 05:30:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chowdhury", "Arkabandhu", ""], ["Jiang", "Mingchao", ""], ["Jermaine", "Chris", ""]]}, {"id": "2101.00567", "submitter": "Quan Liu", "authors": "Quan Liu, Isabella M. Gaeta, Mengyang Zhao, Ruining Deng, Aadarsh Jha,\n  Bryan A. Millis, Anita Mahadevan-Jansen, Matthew J. Tyska, Yuankai Huo", "title": "ASIST: Annotation-free Synthetic Instance Segmentation and Tracking by\n  Adversarial Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The quantitative analysis of microscope videos often requires\ninstance segmentation and tracking of cellular and subcellular objects. The\ntraditional method consists of two stages: (1) performing instance object\nsegmentation of each frame, and (2) associating objects frame-by-frame.\nRecently, pixel-embedding-based deep learning approaches these two steps\nsimultaneously as a single stage holistic solution. In computer vision,\nannotated training data with consistent segmentation and tracking is resource\nintensive, the severity of which is multiplied in microscopy imaging due to (1)\ndense objects (e.g., overlapping or touching), and (2) high dynamics (e.g.,\nirregular motion and mitosis). Adversarial simulations have provided successful\nsolutions to alleviate the lack of such annotations in dynamics scenes in\ncomputer vision, such as using simulated environments (e.g., computer games) to\ntrain real-world self-driving systems. Methods: In this paper, we propose an\nannotation-free synthetic instance segmentation and tracking (ASIST) method\nwith adversarial simulation and single-stage pixel-embedding based learning.\nContribution: The contribution of this paper is three-fold: (1) the proposed\nmethod aggregates adversarial simulations and single-stage pixel-embedding\nbased deep learning; (2) the method is assessed with both the cellular (i.e.,\nHeLa cells) and subcellular (i.e., microvilli) objects; and (3) to the best of\nour knowledge, this is the first study to explore annotation-free instance\nsegmentation and tracking study for microscope videos. Results: The ASIST\nmethod achieved an important step forward, when compared with fully supervised\napproaches: ASIST shows 7% to 11% higher segmentation, detection and tracking\nperformance on microvilli relative to fully supervised methods, and comparable\nperformance on Hela cell videos.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 07:04:13 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 22:32:33 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 20:09:20 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liu", "Quan", ""], ["Gaeta", "Isabella M.", ""], ["Zhao", "Mengyang", ""], ["Deng", "Ruining", ""], ["Jha", "Aadarsh", ""], ["Millis", "Bryan A.", ""], ["Mahadevan-Jansen", "Anita", ""], ["Tyska", "Matthew J.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2101.00588", "submitter": "Xin Jin", "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen", "title": "Style Normalization and Restitution for Domain Generalization and\n  Adaptation", "comments": "We have extended our SNR for domain generalization and adaptation to\n  various computer vision tasks, e.g., image classification, object detection,\n  semantic segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  For many practical computer vision applications, the learned models usually\nhave high performance on the datasets used for training but suffer from\nsignificant performance degradation when deployed in new environments, where\nthere are usually style differences between the training images and the testing\nimages. An effective domain generalizable model is expected to be able to learn\nfeature representations that are both generalizable and discriminative. In this\npaper, we design a novel Style Normalization and Restitution module (SNR) to\nsimultaneously ensure both high generalization and discrimination capability of\nthe networks. In the SNR module, particularly, we filter out the style\nvariations (e.g, illumination, color contrast) by performing Instance\nNormalization (IN) to obtain style normalized features, where the discrepancy\namong different samples and domains is reduced. However, such a process is\ntask-ignorant and inevitably removes some task-relevant discriminative\ninformation, which could hurt the performance. To remedy this, we propose to\ndistill task-relevant discriminative features from the residual (i.e, the\ndifference between the original feature and the style normalized feature) and\nadd them back to the network to ensure high discrimination. Moreover, for\nbetter disentanglement, we enforce a dual causality loss constraint in the\nrestitution step to encourage the better separation of task-relevant and\ntask-irrelevant features. We validate the effectiveness of our SNR on different\ncomputer vision tasks, including classification, semantic segmentation, and\nobject detection. Experiments demonstrate that our SNR module is capable of\nimproving the performance of networks for domain generalization (DG) and\nunsupervised domain adaptation (UDA) on many tasks. Code are available at\nhttps://github.com/microsoft/SNR.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 09:01:39 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 11:33:36 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jin", "Xin", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""]]}, {"id": "2101.00590", "submitter": "Jing Xu", "authors": "Jing Xu, Yu Pan, Xinglin Pan, Steven Hoi, Zhang Yi, Zenglin Xu", "title": "RegNet: Self-Regulated Network for Image Classification", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ResNet and its variants have achieved remarkable successes in various\ncomputer vision tasks. Despite its success in making gradient flow through\nbuilding blocks, the simple shortcut connection mechanism limits the ability of\nre-exploring new potentially complementary features due to the additive\nfunction. To address this issue, in this paper, we propose to introduce a\nregulator module as a memory mechanism to extract complementary features, which\nare further fed to the ResNet. In particular, the regulator module is composed\nof convolutional RNNs (e.g., Convolutional LSTMs or Convolutional GRUs), which\nare shown to be good at extracting Spatio-temporal information. We named the\nnew regulated networks as RegNet. The regulator module can be easily\nimplemented and appended to any ResNet architecture. We also apply the\nregulator module for improving the Squeeze-and-Excitation ResNet to show the\ngeneralization ability of our method. Experimental results on three image\nclassification datasets have demonstrated the promising performance of the\nproposed architecture compared with the standard ResNet, SE-ResNet, and other\nstate-of-the-art architectures.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 09:06:25 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Xu", "Jing", ""], ["Pan", "Yu", ""], ["Pan", "Xinglin", ""], ["Hoi", "Steven", ""], ["Yi", "Zhang", ""], ["Xu", "Zenglin", ""]]}, {"id": "2101.00591", "submitter": "Chen Zhao", "authors": "Chen Zhao, Yixiao Ge, Feng Zhu, Rui Zhao, Hongsheng Li, Mathieu\n  Salzmann", "title": "Progressive Correspondence Pruning by Consensus Learning", "comments": "Accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Correspondence selection aims to correctly select the consistent matches\n(inliers) from an initial set of putative correspondences. The selection is\nchallenging since putative matches are typically extremely unbalanced, largely\ndominated by outliers, and the random distribution of such outliers further\ncomplicates the learning process for learning-based methods. To address this\nissue, we propose to progressively prune the correspondences via a\nlocal-to-global consensus learning procedure. We introduce a ``pruning'' block\nthat lets us identify reliable candidates among the initial matches according\nto consensus scores estimated using local-to-global dynamic graphs. We then\nachieve progressive pruning by stacking multiple pruning blocks sequentially.\nOur method outperforms state-of-the-arts on robust line fitting, camera pose\nestimation and retrieval-based image localization benchmarks by significant\nmargins and shows promising generalization ability to different datasets and\ndetector/descriptor combinations.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 09:10:00 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 15:23:03 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhao", "Chen", ""], ["Ge", "Yixiao", ""], ["Zhu", "Feng", ""], ["Zhao", "Rui", ""], ["Li", "Hongsheng", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2101.00599", "submitter": "Yulong Liu", "authors": "Zhongxing Sun, Wei Cui, and Yulong Liu", "title": "Phase Transitions in Recovery of Structured Signals from Corrupted\n  Measurements", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with the problem of recovering a structured signal\nfrom a relatively small number of corrupted random measurements. Sharp phase\ntransitions have been numerically observed in practice when different convex\nprogramming procedures are used to solve this problem. This paper is devoted to\npresenting theoretical explanations for these phenomenons by employing some\nbasic tools from Gaussian process theory. Specifically, we identify the precise\nlocations of the phase transitions for both constrained and penalized recovery\nprocedures. Our theoretical results show that these phase transitions are\ndetermined by some geometric measures of structure, e.g., the spherical\nGaussian width of a tangent cone and the Gaussian (squared) distance to a\nscaled subdifferential. By utilizing the established phase transition theory,\nwe further investigate the relationship between these two kinds of recovery\nprocedures, which also reveals an optimal strategy (in the sense of Lagrange\ntheory) for choosing the tradeoff parameter in the penalized recovery\nprocedure. Numerical experiments are provided to verify our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 10:11:49 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sun", "Zhongxing", ""], ["Cui", "Wei", ""], ["Liu", "Yulong", ""]]}, {"id": "2101.00603", "submitter": "Haotian Li", "authors": "Zhuqing Jiang, Haotian Li, Liangjie Liu, Aidong Men, Haiying Wang", "title": "A Switched View of Retinex: Deep Self-Regularized Low-Light Image\n  Enhancement", "comments": null, "journal-ref": "Neurocomputing 454 (2021): 361-372", "doi": "10.1016/j.neucom.2021.05.025", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-regularized low-light image enhancement does not require any\nnormal-light image in training, thereby freeing from the chains on paired or\nunpaired low-/normal-images. However, existing methods suffer color deviation\nand fail to generalize to various lighting conditions. This paper presents a\nnovel self-regularized method based on Retinex, which, inspired by HSV,\npreserves all colors (Hue, Saturation) and only integrates Retinex theory into\nbrightness (Value). We build a reflectance estimation network by restricting\nthe consistency of reflectances embedded in both the original and a novel\nrandom disturbed form of the brightness of the same scene. The generated\nreflectance, which is assumed to be irrelevant of illumination by Retinex, is\ntreated as enhanced brightness. Our method is efficient as a low-light image is\ndecoupled into two subspaces, color and brightness, for better preservation and\nenhancement. Extensive experiments demonstrate that our method outperforms\nmultiple state-of-the-art algorithms qualitatively and quantitatively and\nadapts to more lighting conditions.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 10:40:31 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Jiang", "Zhuqing", ""], ["Li", "Haotian", ""], ["Liu", "Liangjie", ""], ["Men", "Aidong", ""], ["Wang", "Haiying", ""]]}, {"id": "2101.00604", "submitter": "Jizhe Zhou", "authors": "Jizhe Zhou, Chi-Man Pun, Yu Tong", "title": "Privacy-sensitive Objects Pixelation for Live Video Streaming", "comments": null, "journal-ref": null, "doi": "10.1145/3394171.3413972", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the prevailing of live video streaming, establishing an online\npixelation method for privacy-sensitive objects is an urgency. Caused by the\ninaccurate detection of privacy-sensitive objects, simply migrating the\ntracking-by-detection structure into the online form will incur problems in\ntarget initialization, drifting, and over-pixelation. To cope with the\ninevitable but impacting detection issue, we propose a novel Privacy-sensitive\nObjects Pixelation (PsOP) framework for automatic personal privacy filtering\nduring live video streaming. Leveraging pre-trained detection networks, our\nPsOP is extendable to any potential privacy-sensitive objects pixelation.\nEmploying the embedding networks and the proposed Positioned Incremental\nAffinity Propagation (PIAP) clustering algorithm as the backbone, our PsOP\nunifies the pixelation of discriminating and indiscriminating pixelation\nobjects through trajectories generation. In addition to the pixelation accuracy\nboosting, experiments on the streaming video data we built show that the\nproposed PsOP can significantly reduce the over-pixelation ratio in\nprivacy-sensitive object pixelation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 11:07:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhou", "Jizhe", ""], ["Pun", "Chi-Man", ""], ["Tong", "Yu", ""]]}, {"id": "2101.00606", "submitter": "Jizhe Zhou", "authors": "Jizhe Zhou, Chi-Man Pun, Yu Tong", "title": "News Image Steganography: A Novel Architecture Facilitates the Fake News\n  Identification", "comments": null, "journal-ref": null, "doi": "10.1109/VCIP49819.2020.9301846", "report-no": null, "categories": "cs.CV cs.CR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A larger portion of fake news quotes untampered images from other sources\nwith ulterior motives rather than conducting image forgery. Such elaborate\nengraftments keep the inconsistency between images and text reports stealthy,\nthereby, palm off the spurious for the genuine. This paper proposes an\narchitecture named News Image Steganography (NIS) to reveal the aforementioned\ninconsistency through image steganography based on GAN. Extractive\nsummarization about a news image is generated based on its source texts, and a\nlearned steganographic algorithm encodes and decodes the summarization of the\nimage in a manner that approaches perceptual invisibility. Once an encoded\nimage is quoted, its source summarization can be decoded and further presented\nas the ground truth to verify the quoting news. The pairwise encoder and\ndecoder endow images of the capability to carry along their imperceptible\nsummarization. Our NIS reveals the underlying inconsistency, thereby, according\nto our experiments and investigations, contributes to the identification\naccuracy of fake news that engrafts untampered images.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 11:12:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhou", "Jizhe", ""], ["Pun", "Chi-Man", ""], ["Tong", "Yu", ""]]}, {"id": "2101.00652", "submitter": "Hardik Uppal", "authors": "Hardik Uppal, Alireza Sepas-Moghaddam, Michael Greenspan and Ali\n  Etemad", "title": "Depth as Attention for Face Representation Learning", "comments": "16 pages, 11 figures, Accepted to IEEE Transactions on Information\n  Forensics and Security 2021", "journal-ref": null, "doi": "10.1109/TIFS.2021.3053458", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face representation learning solutions have recently achieved great success\nfor various applications such as verification and identification. However, face\nrecognition approaches that are based purely on RGB images rely solely on\nintensity information, and therefore are more sensitive to facial variations,\nnotably pose, occlusions, and environmental changes such as illumination and\nbackground. A novel depth-guided attention mechanism is proposed for deep\nmulti-modal face recognition using low-cost RGB-D sensors. Our novel attention\nmechanism directs the deep network \"where to look\" for visual features in the\nRGB image by focusing the attention of the network using depth features\nextracted by a Convolution Neural Network (CNN). The depth features help the\nnetwork focus on regions of the face in the RGB image that contains more\nprominent person-specific information. Our attention mechanism then uses this\ncorrelation to generate an attention map for RGB images from the depth features\nextracted by CNN. We test our network on four public datasets, showing that the\nfeatures obtained by our proposed solution yield better results on the\nLock3DFace, CurtinFaces, IIIT-D RGB-D, and KaspAROV datasets which include\nchallenging variations in pose, occlusion, illumination, expression, and\ntime-lapse. Our solution achieves average (increased) accuracies of 87.3\\%\n(+5.0\\%), 99.1\\% (+0.9\\%), 99.7\\% (+0.6\\%) and 95.3\\%(+0.5\\%) for the four\ndatasets respectively, thereby improving the state-of-the-art. We also perform\nadditional experiments with thermal images, instead of depth images, showing\nthe high generalization ability of our solution when adopting other modalities\nfor guiding the attention mechanism instead of depth information\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 16:19:34 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 08:56:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Uppal", "Hardik", ""], ["Sepas-Moghaddam", "Alireza", ""], ["Greenspan", "Michael", ""], ["Etemad", "Ali", ""]]}, {"id": "2101.00667", "submitter": "Idoia Ruiz", "authors": "Idoia Ruiz, Lorenzo Porzi, Samuel Rota Bul\\`o, Peter Kontschieder,\n  Joan Serrat", "title": "Weakly Supervised Multi-Object Tracking and Segmentation", "comments": "Accepted at Autonomous Vehicle Vision WACV 2021 Workshop", "journal-ref": "Proceedings of the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV) 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of weakly supervised Multi-Object Tracking and\nSegmentation, i.e. joint weakly supervised instance segmentation and\nmulti-object tracking, in which we do not provide any kind of mask annotation.\nTo address it, we design a novel synergistic training strategy by taking\nadvantage of multi-task learning, i.e. classification and tracking tasks guide\nthe training of the unsupervised instance segmentation. For that purpose, we\nextract weak foreground localization information, provided by Grad-CAM\nheatmaps, to generate a partial ground truth to learn from. Additionally, RGB\nimage level information is employed to refine the mask prediction at the edges\nof the objects. We evaluate our method on KITTI MOTS, the most representative\nbenchmark for this task, reducing the performance gap on the MOTSP metric\nbetween the fully supervised and weakly supervised approach to just 12% and\n12.7% for cars and pedestrians, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 17:06:43 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ruiz", "Idoia", ""], ["Porzi", "Lorenzo", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Kontschieder", "Peter", ""], ["Serrat", "Joan", ""]]}, {"id": "2101.00676", "submitter": "Muhammad Usama", "authors": "Bilal Yousaf, Muhammad Usama, Waqas Sultani, Arif Mahmood, Junaid\n  Qadir", "title": "Fake Visual Content Detection Using Two-Stream Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid progress in adversarial learning has enabled the generation of\nrealistic-looking fake visual content. To distinguish between fake and real\nvisual content, several detection techniques have been proposed. The\nperformance of most of these techniques however drops off significantly if the\ntest and the training data are sampled from different distributions. This\nmotivates efforts towards improving the generalization of fake detectors. Since\ncurrent fake content generation techniques do not accurately model the\nfrequency spectrum of the natural images, we observe that the frequency\nspectrum of the fake visual data contains discriminative characteristics that\ncan be used to detect fake content. We also observe that the information\ncaptured in the frequency spectrum is different from that of the spatial\ndomain. Using these insights, we propose to complement frequency and spatial\ndomain features using a two-stream convolutional neural network architecture\ncalled TwoStreamNet. We demonstrate the improved generalization of the proposed\ntwo-stream network to several unseen generation architectures, datasets, and\ntechniques. The proposed detector has demonstrated significant performance\nimprovement compared to the current state-of-the-art fake content detectors and\nfusing the frequency and spatial domain streams has also improved\ngeneralization of the detector.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 18:05:07 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yousaf", "Bilal", ""], ["Usama", "Muhammad", ""], ["Sultani", "Waqas", ""], ["Mahmood", "Arif", ""], ["Qadir", "Junaid", ""]]}, {"id": "2101.00686", "submitter": "M. F. Mridha", "authors": "Md. Mohsin Kabir, Abu Quwsar Ohi, Md. Saifur Rahman, M. F. Mridha", "title": "An Evolution of CNN Object Classifiers on Low-Resolution Images", "comments": "Accepted in IEEE Honet 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object classification is a significant task in computer vision. It has become\nan effective research area as an important aspect of image processing and the\nbuilding block of image localization, detection, and scene parsing. Object\nclassification from low-quality images is difficult for the variance of object\ncolors, aspect ratios, and cluttered backgrounds. The field of object\nclassification has seen remarkable advancements, with the development of deep\nconvolutional neural networks (DCNNs). Deep neural networks have been\ndemonstrated as very powerful systems for facing the challenge of object\nclassification from high-resolution images, but deploying such object\nclassification networks on the embedded device remains challenging due to the\nhigh computational and memory requirements. Using high-quality images often\ncauses high computational and memory complexity, whereas low-quality images can\nsolve this issue. Hence, in this paper, we investigate an optimal architecture\nthat accurately classifies low-quality images using DCNNs architectures. To\nvalidate different baselines on lowquality images, we perform experiments using\nwebcam captured image datasets of 10 different objects. In this research work,\nwe evaluate the proposed architecture by implementing popular CNN\narchitectures. The experimental results validate that the MobileNet\narchitecture delivers better than most of the available CNN architectures for\nlow-resolution webcam image datasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 18:44:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kabir", "Md. Mohsin", ""], ["Ohi", "Abu Quwsar", ""], ["Rahman", "Md. Saifur", ""], ["Mridha", "M. F.", ""]]}, {"id": "2101.00691", "submitter": "Tanvir Mahmud", "authors": "Tanvir Mahmud, Md. Jahin Alam, Sakib Chowdhury, Shams Nafisa Ali, Md\n  Maisoon Rahman, Shaikh Anowarul Fattah, Mohammad Saquib", "title": "CovTANet: A Hybrid Tri-level Attention Based Network for Lesion\n  Segmentation, Diagnosis, and Severity Prediction of COVID-19 Chest CT Scans", "comments": "10 Pages, 8 figures. This article has been published in IEEE\n  Transactions on Industrial Informatics", "journal-ref": null, "doi": "10.1109/TII.2020.3048391", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid and precise diagnosis of COVID-19 is one of the major challenges faced\nby the global community to control the spread of this overgrowing pandemic. In\nthis paper, a hybrid neural network is proposed, named CovTANet, to provide an\nend-to-end clinical diagnostic tool for early diagnosis, lesion segmentation,\nand severity prediction of COVID-19 utilizing chest computer tomography (CT)\nscans. A multi-phase optimization strategy is introduced for solving the\nchallenges of complicated diagnosis at a very early stage of infection, where\nan efficient lesion segmentation network is optimized initially which is later\nintegrated into a joint optimization framework for the diagnosis and severity\nprediction tasks providing feature enhancement of the infected regions.\nMoreover, for overcoming the challenges with diffused, blurred, and varying\nshaped edges of COVID lesions with novel and diverse characteristics, a novel\nsegmentation network is introduced, namely Tri-level Attention-based\nSegmentation Network (TA-SegNet). This network has significantly reduced\nsemantic gaps in subsequent encoding decoding stages, with immense\nparallelization of multi-scale features for faster convergence providing\nconsiderable performance improvement over traditional networks. Furthermore, a\nnovel tri-level attention mechanism has been introduced, which is repeatedly\nutilized over the network, combining channel, spatial, and pixel attention\nschemes for faster and efficient generalization of contextual information\nembedded in the feature map through feature re-calibration and enhancement\noperations. Outstanding performances have been achieved in all three-tasks\nthrough extensive experimentation on a large publicly available dataset\ncontaining 1110 chest CT-volumes that signifies the effectiveness of the\nproposed scheme at the current stage of the pandemic.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 19:24:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mahmud", "Tanvir", ""], ["Alam", "Md. Jahin", ""], ["Chowdhury", "Sakib", ""], ["Ali", "Shams Nafisa", ""], ["Rahman", "Md Maisoon", ""], ["Fattah", "Shaikh Anowarul", ""], ["Saquib", "Mohammad", ""]]}, {"id": "2101.00703", "submitter": "Samit Chakraborty", "authors": "Samit Chakraborty, Marguerite Moore, Lisa Parrillo-Chapman", "title": "Automatic Defect Detection of Print Fabric Using Convolutional Neural\n  Network", "comments": "8 pages, 4 figures, Conference", "journal-ref": "Digital Fashion Innovation e-Symposium, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatic defect detection is a challenging task because of the variability\nin texture and type of fabric defects. An effective defect detection system\nenables manufacturers to improve the quality of processes and products.\nAutomation across the textile manufacturing systems would reduce fabric wastage\nand increase profitability by saving cost and resources. There are different\ncontemporary research on automatic defect detection systems using image\nprocessing and machine learning techniques. These techniques differ from each\nother based on the manufacturing processes and defect types. Researchers have\nalso been able to establish real-time defect detection system during weaving.\nAlthough, there has been research on patterned fabric defect detection, these\ndefects are related to weaving faults such as holes, and warp and weft defects.\nBut, there has not been any research that is designed to detect defects that\narise during such as spot and print mismatch. This research has fulfilled this\ngap by developing a print fabric database and implementing deep convolutional\nneural network (CNN).\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 20:56:56 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chakraborty", "Samit", ""], ["Moore", "Marguerite", ""], ["Parrillo-Chapman", "Lisa", ""]]}, {"id": "2101.00732", "submitter": "Muhtadyuzzaman Syed", "authors": "Muhtadyuzzaman Syed and Arvind Akpuram Srinivasan", "title": "Generalized Latency Performance Estimation for Once-For-All Neural\n  Architecture Search", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has enabled the possibility of automated\nmachine learning by streamlining the manual development of deep neural network\narchitectures defining a search space, search strategy, and performance\nestimation strategy. To solve the need for multi-platform deployment of\nConvolutional Neural Network (CNN) models, Once-For-All (OFA) proposed to\ndecouple Training and Search to deliver a one-shot model of sub-networks that\nare constrained to various accuracy-latency tradeoffs. We find that the\nperformance estimation strategy for OFA's search severely lacks\ngeneralizability of different hardware deployment platforms due to single\nhardware latency lookup tables that require significant amount of time and\nmanual effort to build beforehand. In this work, we demonstrate the framework\nfor building latency predictors for neural network architectures to address the\nneed for heterogeneous hardware support and reduce the overhead of lookup\ntables altogether. We introduce two generalizability strategies which include\nfine-tuning using a base model trained on a specific hardware and NAS search\nspace, and GPU-generalization which trains a model on GPU hardware parameters\nsuch as Number of Cores, RAM Size, and Memory Bandwidth. With this, we provide\na family of latency prediction models that achieve over 50% lower RMSE loss as\ncompared to with ProxylessNAS. We also show that the use of these latency\npredictors match the NAS performance of the lookup table baseline approach if\nnot exceeding it in certain cases.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 00:48:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Syed", "Muhtadyuzzaman", ""], ["Srinivasan", "Arvind Akpuram", ""]]}, {"id": "2101.00734", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "title": "Factor Analysis, Probabilistic Principal Component Analysis, Variational\n  Inference, and Variational Autoencoder: Tutorial and Survey", "comments": "To appear as a part of an upcoming textbook on dimensionality\n  reduction and manifold learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a tutorial and survey paper on factor analysis, probabilistic\nPrincipal Component Analysis (PCA), variational inference, and Variational\nAutoencoder (VAE). These methods, which are tightly related, are dimensionality\nreduction and generative models. They asssume that every data point is\ngenerated from or caused by a low-dimensional latent factor. By learning the\nparameters of distribution of latent space, the corresponding low-dimensional\nfactors are found for the sake of dimensionality reduction. For their\nstochastic and generative behaviour, these models can also be used for\ngeneration of new data points in the data space. In this paper, we first start\nwith variational inference where we derive the Evidence Lower Bound (ELBO) and\nExpectation Maximization (EM) for learning the parameters. Then, we introduce\nfactor analysis, derive its joint and marginal distributions, and work out its\nEM steps. Probabilistic PCA is then explained, as a special case of factor\nanalysis, and its closed-form solutions are derived. Finally, VAE is explained\nwhere the encoder, decoder and sampling from the latent space are introduced.\nTraining VAE using both EM and backpropagation are explained.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 01:29:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2101.00745", "submitter": "Yuke Wang", "authors": "Yuke Wang, Boyuan Feng, Yufei Ding", "title": "DSXplore: Optimizing Convolutional Neural Networks via Sliding-Channel\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the key advancement of the convolutional neural networks (CNNs), depthwise\nseparable convolutions (DSCs) are becoming one of the most popular techniques\nto reduce the computations and parameters size of CNNs meanwhile maintaining\nthe model accuracy. It also brings profound impact to improve the applicability\nof the compute- and memory-intensive CNNs to a broad range of applications,\nsuch as mobile devices, which are generally short of computation power and\nmemory. However, previous research in DSCs are largely focusing on compositing\nthe limited existing DSC designs, thus, missing the opportunities to explore\nmore potential designs that can achieve better accuracy and higher\ncomputation/parameter reduction. Besides, the off-the-shelf convolution\nimplementations offer limited computing schemes, therefore, lacking support for\nDSCs with different convolution patterns.\n  To this end, we introduce, DSXplore, the first optimized design for exploring\nDSCs on CNNs. Specifically, at the algorithm level, DSXplore incorporates a\nnovel factorized kernel -- sliding-channel convolution (SCC), featured with\ninput-channel overlapping to balance the accuracy performance and the reduction\nof computation and memory cost. SCC also offers enormous space for design\nexploration by introducing adjustable kernel parameters. Further, at the\nimplementation level, we carry out an optimized GPU-implementation tailored for\nSCC by leveraging several key techniques, such as the input-centric backward\ndesign and the channel-cyclic optimization. Intensive experiments on different\ndatasets across mainstream CNNs show the advantages of DSXplore in balancing\naccuracy and computation/parameter reduction over the standard convolution and\nthe existing DSCs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 02:59:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Yuke", ""], ["Feng", "Boyuan", ""], ["Ding", "Yufei", ""]]}, {"id": "2101.00784", "submitter": "Zekun Wang", "authors": "Zekun Wang, Pengwei Wang, Peter C. Louis, Lee E. Wheless, Yuankai Huo", "title": "WearMask: Fast In-browser Face Mask Detection with Serverless Edge\n  Computing for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 epidemic has been a significant healthcare challenge in the\nUnited States. According to the Centers for Disease Control and Prevention\n(CDC), COVID-19 infection is transmitted predominately by respiratory droplets\ngenerated when people breathe, talk, cough, or sneeze. Wearing a mask is the\nprimary, effective, and convenient method of blocking 80% of all respiratory\ninfections. Therefore, many face mask detection and monitoring systems have\nbeen developed to provide effective supervision for hospitals, airports,\npublication transportation, sports venues, and retail locations. However, the\ncurrent commercial face mask detection systems are typically bundled with\nspecific software or hardware, impeding public accessibility. In this paper, we\npropose an in-browser serverless edge-computing based face mask detection\nsolution, called Web-based efficient AI recognition of masks (WearMask), which\ncan be deployed on any common devices (e.g., cell phones, tablets, computers)\nthat have internet connections using web browsers, without installing any\nsoftware. The serverless edge-computing design minimizes the extra hardware\ncosts (e.g., specific devices or cloud computing servers). The contribution of\nthe proposed method is to provide a holistic edge-computing framework of\nintegrating (1) deep learning models (YOLO), (2) high-performance neural\nnetwork inference computing framework (NCNN), and (3) a stack-based virtual\nmachine (WebAssembly). For end-users, our web-based solution has advantages of\n(1) serverless edge-computing design with minimal device limitation and privacy\nrisk, (2) installation free deployment, (3) low computing requirements, and (4)\nhigh detection speed. Our WearMask application has been launched with public\naccess at facemask-detection.com.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 05:50:48 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Zekun", ""], ["Wang", "Pengwei", ""], ["Louis", "Peter C.", ""], ["Wheless", "Lee E.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2101.00793", "submitter": "Karthik E", "authors": "Karthik E", "title": "A Framework for Fast Scalable BNN Inference using Googlenet and Transfer\n  Learning", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and accurate object detection in video and image analysis is one of\nthe major beneficiaries of the advancement in computer vision systems with the\nhelp of deep learning. With the aid of deep learning, more powerful tools\nevolved, which are capable to learn high-level and deeper features and thus can\novercome the existing problems in traditional architectures of object detection\nalgorithms. The work in this thesis aims to achieve high accuracy in object\ndetection with good real-time performance.\n  In the area of computer vision, a lot of research is going into the area of\ndetection and processing of visual information, by improving the existing\nalgorithms. The binarized neural network has shown high performance in various\nvision tasks such as image classification, object detection, and semantic\nsegmentation. The Modified National Institute of Standards and Technology\ndatabase (MNIST), Canadian Institute for Advanced Research (CIFAR), and Street\nView House Numbers (SVHN) datasets are used which is implemented using a\npre-trained convolutional neural network (CNN) that is 22 layers deep.\nSupervised learning is used in the work, which classifies the particular\ndataset with the proper structure of the model. In still images, to improve\naccuracy, Googlenet is used. The final layer of the Googlenet is replaced with\nthe transfer learning to improve the accuracy of the Googlenet. At the same\ntime, the accuracy in moving images can be maintained by transfer learning\ntechniques. Hardware is the main backbone for any model to obtain faster\nresults with a large number of datasets. Here, Nvidia Jetson Nano is used which\nis a graphics processing unit (GPU), that can handle a large number of\ncomputations in the process of object detection. Results show that the accuracy\nof objects detected by the transfer learning method is more when compared to\nthe existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 06:16:52 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 07:28:38 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["E", "Karthik", ""]]}, {"id": "2101.00809", "submitter": "Chao Wang", "authors": "Chao Wang, Min Tao, Chen-Nee Chuah, James Nagy, Yifei Lou", "title": "Minimizing L1 over L2 norms on the gradient", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the L1/L2 minimization on the gradient for imaging\napplications. Several recent works have demonstrated that L1/L2 is better than\nthe L1 norm when approximating the L0 norm to promote sparsity. Consequently,\nwe postulate that applying L1/L2 on the gradient is better than the classic\ntotal variation (the L1 norm on the gradient) to enforce the sparsity of the\nimage gradient. To verify our hypothesis, we consider a constrained formulation\nto reveal empirical evidence on the superiority of L1/L2 over L1 when\nrecovering piecewise constant signals from low-frequency measurements.\nNumerically, we design a specific splitting scheme, under which we can prove\nsubsequential and global convergence for the alternating direction method of\nmultipliers (ADMM) under certain conditions. Experimentally, we demonstrate\nvisible improvements of L1/L2 over L1 and other nonconvex regularizations for\nimage recovery from low-frequency measurements and two medical applications of\nMRI and CT reconstruction. All the numerical results show the efficiency of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:19:27 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 05:07:38 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wang", "Chao", ""], ["Tao", "Min", ""], ["Chuah", "Chen-Nee", ""], ["Nagy", "James", ""], ["Lou", "Yifei", ""]]}, {"id": "2101.00813", "submitter": "Ya'nan Wang", "authors": "Ya'nan Wang, Zhuqing Jiang, Chang Liu, Kai Li, Aidong Men, Haiying\n  Wang", "title": "Shed Various Lights on a Low-Light Image: Multi-Level Enhancement Guided\n  by Arbitrary References", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is suggested that low-light image enhancement realizes one-to-many mapping\nsince we have different definitions of NORMAL-light given application scenarios\nor users' aesthetic. However, most existing methods ignore subjectivity of the\ntask, and simply produce one result with fixed brightness. This paper proposes\na neural network for multi-level low-light image enhancement, which is\nuser-friendly to meet various requirements by selecting different images as\nbrightness reference. Inspired by style transfer, our method decomposes an\nimage into two low-coupling feature components in the latent space, which\nallows the concatenation feasibility of the content components from low-light\nimages and the luminance components from reference images. In such a way, the\nnetwork learns to extract scene-invariant and brightness-specific information\nfrom a set of image pairs instead of learning brightness differences. Moreover,\ninformation except for the brightness is preserved to the greatest extent to\nalleviate color distortion. Extensive results show strong capacity and\nsuperiority of our network against existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:38:51 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Ya'nan", ""], ["Jiang", "Zhuqing", ""], ["Liu", "Chang", ""], ["Li", "Kai", ""], ["Men", "Aidong", ""], ["Wang", "Haiying", ""]]}, {"id": "2101.00814", "submitter": "Chenxing Wang", "authors": "Fanzhou Wang, Chenxing Wang, Qingze Guan", "title": "Single-shot fringe projection profilometry based on Deep Learning and\n  Computer Graphics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple works have applied deep learning to fringe projection profilometry\n(FPP) in recent years. However, to obtain a large amount of data from actual\nsystems for training is still a tricky problem, and moreover, the network\ndesign and optimization still worth exploring. In this paper, we introduce\ncomputer graphics to build virtual FPP systems in order to generate the desired\ndatasets conveniently and simply. The way of constructing a virtual FPP system\nis described in detail firstly, and then some key factors to set the virtual\nFPP system much close to the reality are analyzed. With the aim of accurately\nestimating the depth image from only one fringe image, we also design a new\nloss function to enhance the quality of the overall and detailed information\nrestored. And two representative networks, U-Net and pix2pix, are compared in\nmultiple aspects. The real experiments prove the good accuracy and\ngeneralization of the network trained by the data from our virtual systems and\nthe designed loss, implying the potential of our method for applications.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:42:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Fanzhou", ""], ["Wang", "Chenxing", ""], ["Guan", "Qingze", ""]]}, {"id": "2101.00820", "submitter": "Yang Liu", "authors": "Yang Liu, Keze Wang, Haoyuan Lan, Liang Lin", "title": "Temporal Contrastive Graph Learning for Video Action Recognition and\n  Retrieval", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempt to fully discover the temporal diversity and chronological\ncharacteristics for self-supervised video representation learning, this work\ntakes advantage of the temporal dependencies within videos and further proposes\na novel self-supervised method named Temporal Contrastive Graph Learning\n(TCGL). In contrast to the existing methods that ignore modeling elaborate\ntemporal dependencies, our TCGL roots in a hybrid graph contrastive learning\nstrategy to jointly regard the inter-snippet and intra-snippet temporal\ndependencies as self-supervision signals for temporal representation learning.\nTo model multi-scale temporal dependencies, our TCGL integrates the prior\nknowledge about the frame and snippet orders into graph structures, i.e., the\nintra-/inter- snippet temporal contrastive graphs. By randomly removing edges\nand masking nodes of the intra-snippet graphs or inter-snippet graphs, our TCGL\ncan generate different correlated graph views. Then, specific contrastive\nlearning modules are designed to maximize the agreement between nodes in\ndifferent views. To adaptively learn the global context representation and\nrecalibrate the channel-wise features, we introduce an adaptive video snippet\norder prediction module, which leverages the relational knowledge among video\nsnippets to predict the actual snippet orders. Experimental results demonstrate\nthe superiority of our TCGL over the state-of-the-art methods on large-scale\naction recognition and video retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 08:11:39 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 03:33:28 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 08:51:36 GMT"}, {"version": "v4", "created": "Mon, 1 Feb 2021 03:43:47 GMT"}, {"version": "v5", "created": "Mon, 1 Mar 2021 08:51:25 GMT"}, {"version": "v6", "created": "Tue, 2 Mar 2021 08:34:02 GMT"}, {"version": "v7", "created": "Thu, 4 Mar 2021 13:08:41 GMT"}, {"version": "v8", "created": "Wed, 17 Mar 2021 03:32:52 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Yang", ""], ["Wang", "Keze", ""], ["Lan", "Haoyuan", ""], ["Lin", "Liang", ""]]}, {"id": "2101.00850", "submitter": "Syed Waqas Zamir", "authors": "Aditya Arora, Muhammad Haris, Syed Waqas Zamir, Munawar Hayat, Fahad\n  Shahbaz Khan, Ling Shao, Ming-Hsuan Yang", "title": "Low Light Image Enhancement via Global and Local Context Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Images captured under low-light conditions manifest poor visibility, lack\ncontrast and color vividness. Compared to conventional approaches, deep\nconvolutional neural networks (CNNs) perform well in enhancing images. However,\nbeing solely reliant on confined fixed primitives to model dependencies,\nexisting data-driven deep models do not exploit the contexts at various spatial\nscales to address low-light image enhancement. These contexts can be crucial\ntowards inferring several image enhancement tasks, e.g., local and global\ncontrast, brightness and color corrections; which requires cues from both local\nand global spatial extent. To this end, we introduce a context-aware deep\nnetwork for low-light image enhancement. First, it features a global context\nmodule that models spatial correlations to find complementary cues over full\nspatial domain. Second, it introduces a dense residual block that captures\nlocal context with a relatively large receptive field. We evaluate the proposed\napproach using three challenging datasets: MIT-Adobe FiveK, LoL, and SID. On\nall these datasets, our method performs favorably against the state-of-the-arts\nin terms of standard image fidelity metrics. In particular, compared to the\nbest performing method on the MIT-Adobe FiveK dataset, our algorithm improves\nPSNR from 23.04 dB to 24.45 dB.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 09:40:54 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Arora", "Aditya", ""], ["Haris", "Muhammad", ""], ["Zamir", "Syed Waqas", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2101.00858", "submitter": "Sinem Aslan", "authors": "Sinem Aslan, Luc Steels", "title": "Identifying centres of interest in paintings using alignment and edge\n  detection: Case studies on works by Luc Tuymans", "comments": "Accepted to International Workshop on Fine Art Pattern Extraction and\n  Recognition of 25th International Conference on Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  What is the creative process through which an artist goes from an original\nimage to a painting? Can we examine this process using techniques from computer\nvision and pattern recognition? Here we set the first preliminary steps to\nalgorithmically deconstruct some of the transformations that an artist applies\nto an original image in order to establish centres of interest, which are focal\nareas of a painting that carry meaning. We introduce a comparative methodology\nthat first cuts out the minimal segment from the original image on which the\npainting is based, then aligns the painting with this source, investigates\nmicro-differences to identify centres of interest and attempts to understand\ntheir role. In this paper we focus exclusively on micro-differences with\nrespect to edges. We believe that research into where and how artists create\ncentres of interest in paintings is valuable for curators, art historians,\nviewers, and art educators, and might even help artists to understand and\nrefine their own artistic method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 10:04:19 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Aslan", "Sinem", ""], ["Steels", "Luc", ""]]}, {"id": "2101.00910", "submitter": "Shang-Hua Gao", "authors": "Shang-Hua Gao, Qi Han, Zhong-Yu Li, Pai Peng, Liang Wang, Ming-Ming\n  Cheng", "title": "Global2Local: Efficient Structure Search for Video Action Segmentation", "comments": "Accepted by CVPR 2021. Source code:\n  https://github.com/ShangHua-Gao/G2L-search", "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal receptive fields of models play an important role in action\nsegmentation. Large receptive fields facilitate the long-term relations among\nvideo clips while small receptive fields help capture the local details.\nExisting methods construct models with hand-designed receptive fields in\nlayers. Can we effectively search for receptive field combinations to replace\nhand-designed patterns? To answer this question, we propose to find better\nreceptive field combinations through a global-to-local search scheme. Our\nsearch scheme exploits both global search to find the coarse combinations and\nlocal search to get the refined receptive field combination patterns further.\nThe global search finds possible coarse combinations other than human-designed\npatterns. On top of the global search, we propose an expectation guided\niterative local search scheme to refine combinations effectively. Our\nglobal-to-local search can be plugged into existing action segmentation methods\nto achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:06:03 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 02:51:47 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Gao", "Shang-Hua", ""], ["Han", "Qi", ""], ["Li", "Zhong-Yu", ""], ["Peng", "Pai", ""], ["Wang", "Liang", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2101.00932", "submitter": "Xin Tan", "authors": "Xiaoyang Zheng, Xin Tan, Jie Zhou, Lizhuang Ma, Rynson W.H. Lau", "title": "Weakly-Supervised Saliency Detection via Salient Object Subitizing", "comments": "This paper is accepted to IEEE Trans. on Circuits and Systems for\n  Video Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Salient object detection aims at detecting the most visually distinct objects\nand producing the corresponding masks. As the cost of pixel-level annotations\nis high, image tags are usually used as weak supervisions. However, an image\ntag can only be used to annotate one class of objects. In this paper, we\nintroduce saliency subitizing as the weak supervision since it is\nclass-agnostic. This allows the supervision to be aligned with the property of\nsaliency detection, where the salient objects of an image could be from more\nthan one class. To this end, we propose a model with two modules, Saliency\nSubitizing Module (SSM) and Saliency Updating Module (SUM). While SSM learns to\ngenerate the initial saliency masks using the subitizing information, without\nthe need for any unsupervised methods or some random seeds, SUM helps\niteratively refine the generated saliency masks. We conduct extensive\nexperiments on five benchmark datasets. The experimental results show that our\nmethod outperforms other weakly-supervised methods and even performs comparably\nto some fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:51:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zheng", "Xiaoyang", ""], ["Tan", "Xin", ""], ["Zhou", "Jie", ""], ["Ma", "Lizhuang", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "2101.00948", "submitter": "Morteza Hosseinioun", "authors": "Niloofar Akhavan Javan, Ali Jebreili, Babak Mozafari, Morteza\n  Hosseinioun", "title": "Classification and Segmentation of Pulmonary Lesions in CT images using\n  a combined VGG-XGBoost method, and an integrated Fuzzy Clustering-Level Set\n  technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Given that lung cancer is one of the deadliest diseases, and many die from\nthe disease every year, early detection and diagnosis of this disease are\nvaluable, preventing cancer from growing and spreading. So if cancer is\ndiagnosed in the early stage, the patient's life will be saved. However, the\ncurrent pulmonary disease diagnosis is made by human resources, which is\ntime-consuming and requires a specialist in this field. Also, there is a high\nlevel of errors in human diagnosis. Our goal is to develop a system that can\ndetect and classify lung lesions with high accuracy and segment them in CT-scan\nimages. In the proposed method, first, features are extracted automatically\nfrom the CT-scan image; then, the extracted features are classified by Ensemble\nGradient Boosting methods. Finally, if there is a lesion in the CT-scan image,\nusing a hybrid method based on [1], including Fuzzy Clustering and Level Set,\nthe lesion is segmented. We collected a dataset, including CT-scan images of\npulmonary lesions. The target community was the patients in Mashhad. The\ncollected samples were then tagged by a specialist. We used this dataset for\ntraining and testing our models. Finally, we were able to achieve an accuracy\nof 96% for this dataset. This system can help physicians to diagnose pulmonary\nlesions and prevent possible mistakes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:25:13 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Javan", "Niloofar Akhavan", ""], ["Jebreili", "Ali", ""], ["Mozafari", "Babak", ""], ["Hosseinioun", "Morteza", ""]]}, {"id": "2101.00973", "submitter": "Adil Karjauv", "authors": "Chaoning Zhang, Adil Karjauv, Philipp Benz, In So Kweon", "title": "Towards Robust Data Hiding Against (JPEG) Compression: A\n  Pseudo-Differentiable Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data hiding is one widely used approach for protecting authentication and\nownership. Most multimedia content like images and videos are transmitted or\nsaved in the compressed form. This kind of lossy compression, such as JPEG, can\ndestroy the hidden data, which raises the need of robust data hiding. It is\nstill an open challenge to achieve the goal of data hiding that can be against\nthese compressions. Recently, deep learning has shown large success in data\nhiding, while non-differentiability of JPEG makes it challenging to train a\ndeep pipeline for improving robustness against lossy compression. The existing\nSOTA approaches replace the non-differentiable parts with differentiable\nmodules that perform similar operations. Multiple limitations exist: (a) large\nengineering effort; (b) requiring a white-box knowledge of compression attacks;\n(c) only works for simple compression like JPEG. In this work, we propose a\nsimple yet effective approach to address all the above limitations at once.\nBeyond JPEG, our approach has been shown to improve robustness against various\nimage and video lossy compression algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 12:30:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Chaoning", ""], ["Karjauv", "Adil", ""], ["Benz", "Philipp", ""], ["Kweon", "In So", ""]]}, {"id": "2101.00989", "submitter": "Yanghao Zhang", "authors": "Yanghao Zhang, Fu Wang and Wenjie Ruan", "title": "Fooling Object Detectors: Adversarial Attacks by Half-Neighbor Masks", "comments": "To appear in the Proceedings of the CIKM 2020 Workshops published by\n  CEUR-WS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there are a great number of adversarial attacks on deep learning\nbased classifiers, how to attack object detection systems has been rarely\nstudied. In this paper, we propose a Half-Neighbor Masked Projected Gradient\nDescent (HNM-PGD) based attack, which can generate strong perturbation to fool\ndifferent kinds of detectors under strict constraints. We also applied the\nproposed HNM-PGD attack in the CIKM 2020 AnalytiCup Competition, which was\nranked within the top 1% on the leaderboard. We release the code at\nhttps://github.com/YanghaoZYH/HNM-PGD.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 14:03:22 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Yanghao", ""], ["Wang", "Fu", ""], ["Ruan", "Wenjie", ""]]}, {"id": "2101.00990", "submitter": "Alejandro Gonz\\'alez Alzate", "authors": "Manel Mateos, Alejandro Gonz\\'alez, Xavier Sevillano", "title": "Guiding GANs: How to control non-conditional pre-trained GANs for\n  conditional image generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) are an arrange of two neural networks\n-- the generator and the discriminator -- that are jointly trained to generate\nartificial data, such as images, from random inputs. The quality of these\ngenerated images has recently reached such levels that can often lead both\nmachines and humans into mistaking fake for real examples. However, the process\nperformed by the generator of the GAN has some limitations when we want to\ncondition the network to generate images from subcategories of a specific\nclass. Some recent approaches tackle this \\textit{conditional generation} by\nintroducing extra information prior to the training process, such as image\nsemantic segmentation or textual descriptions. While successful, these\ntechniques still require defining beforehand the desired subcategories and\ncollecting large labeled image datasets representing them to train the GAN from\nscratch. In this paper we present a novel and alternative method for guiding\ngeneric non-conditional GANs to behave as conditional GANs. Instead of\nre-training the GAN, our approach adds into the mix an encoder network to\ngenerate the high-dimensional random input vectors that are fed to the\ngenerator network of a non-conditional GAN to make it generate images from a\nspecific subcategory. In our experiments, when compared to training a\nconditional GAN from scratch, our guided GAN is able to generate artificial\nimages of perceived quality comparable to that of non-conditional GANs after\ntraining the encoder on just a few hundreds of images, which substantially\naccelerates the process and enables adding new subcategories seamlessly.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 14:03:32 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mateos", "Manel", ""], ["Gonz\u00e1lez", "Alejandro", ""], ["Sevillano", "Xavier", ""]]}, {"id": "2101.01032", "submitter": "Tao Xiang", "authors": "Tao Xiang, Hangcheng Liu, Shangwei Guo, Tianwei Zhang, Xiaofeng Liao", "title": "Local Black-box Adversarial Attacks: A Query Efficient Approach", "comments": "This work has been submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Adversarial attacks have threatened the application of deep neural networks\nin security-sensitive scenarios. Most existing black-box attacks fool the\ntarget model by interacting with it many times and producing global\nperturbations. However, global perturbations change the smooth and\ninsignificant background, which not only makes the perturbation more easily be\nperceived but also increases the query overhead. In this paper, we propose a\nnovel framework to perturb the discriminative areas of clean examples only\nwithin limited queries in black-box attacks. Our framework is constructed based\non two types of transferability. The first one is the transferability of model\ninterpretations. Based on this property, we identify the discriminative areas\nof a given clean example easily for local perturbations. The second is the\ntransferability of adversarial examples. It helps us to produce a local\npre-perturbation for improving query efficiency. After identifying the\ndiscriminative areas and pre-perturbing, we generate the final adversarial\nexamples from the pre-perturbed example by querying the targeted model with two\nkinds of black-box attack techniques, i.e., gradient estimation and random\nsearch. We conduct extensive experiments to show that our framework can\nsignificantly improve the query efficiency during black-box perturbing with a\nhigh attack success rate. Experimental results show that our attacks outperform\nstate-of-the-art black-box attacks under various system settings.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 15:32:16 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Xiang", "Tao", ""], ["Liu", "Hangcheng", ""], ["Guo", "Shangwei", ""], ["Zhang", "Tianwei", ""], ["Liao", "Xiaofeng", ""]]}, {"id": "2101.01035", "submitter": "Andrew Hoopes", "authors": "Andrew Hoopes, Malte Hoffmann, Bruce Fischl, John Guttag, Adrian V.\n  Dalca", "title": "HyperMorph: Amortized Hyperparameter Learning for Image Registration", "comments": "IPMI 2021: Information Processing in Medical Imaging. Keywords:\n  Deformable Image Registration, Hyperparameter Search, Deep Learning,\n  Hypernetworks, and Amortized Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HyperMorph, a learning-based strategy for deformable image\nregistration that removes the need to tune important registration\nhyperparameters during training. Classical registration methods solve an\noptimization problem to find a set of spatial correspondences between two\nimages, while learning-based methods leverage a training dataset to learn a\nfunction that generates these correspondences. The quality of the results for\nboth types of techniques depends greatly on the choice of hyperparameters.\nUnfortunately, hyperparameter tuning is time-consuming and typically involves\ntraining many separate models with various hyperparameter values, potentially\nleading to suboptimal results. To address this inefficiency, we introduce\namortized hyperparameter learning for image registration, a novel strategy to\nlearn the effects of hyperparameters on deformation fields. The proposed\nframework learns a hypernetwork that takes in an input hyperparameter and\nmodulates a registration network to produce the optimal deformation field for\nthat hyperparameter value. In effect, this strategy trains a single, rich model\nthat enables rapid, fine-grained discovery of hyperparameter values from a\ncontinuous interval at test-time. We demonstrate that this approach can be used\nto optimize multiple hyperparameters considerably faster than existing search\nstrategies, leading to a reduced computational and human burden as well as\nincreased flexibility. We also show several important benefits, including\nincreased robustness to initialization and the ability to rapidly identify\noptimal hyperparameter values specific to a registration task, dataset, or even\na single anatomical region, all without retraining the HyperMorph model. Our\ncode is publicly available at http://voxelmorph.mit.edu.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 15:39:16 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 18:56:08 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Hoopes", "Andrew", ""], ["Hoffmann", "Malte", ""], ["Fischl", "Bruce", ""], ["Guttag", "John", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "2101.01036", "submitter": "Jian Chen", "authors": "Jian Chen and Meng Ling and Rui Li and Petra Isenberg and Tobias\n  Isenberg and Michael Sedlmair and Torsten M\\\"oller and Robert S. Laramee and\n  Han-Wei Shen and Katharina W\\\"unsche and Qiru Wang", "title": "VIS30K: A Collection of Figures and Tables from IEEE Visualization\n  Conference Publications", "comments": "12 pages", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 27\n  January 2021", "doi": "10.1109/TVCG.2021.3054916", "report-no": "33502982", "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the VIS30K dataset, a collection of 29,689 images that represents\n30 years of figures and tables from each track of the IEEE Visualization\nconference series (Vis, SciVis, InfoVis, VAST). VIS30K's comprehensive coverage\nof the scientific literature in visualization not only reflects the progress of\nthe field but also enables researchers to study the evolution of the\nstate-of-the-art and to find relevant work based on graphical content. We\ndescribe the dataset and our semi-automatic collection process, which couples\nconvolutional neural networks (CNN) with curation. Extracting figures and\ntables semi-automatically allows us to verify that no images are overlooked or\nextracted erroneously. To improve quality further, we engaged in a peer-search\nprocess for high-quality figures from early IEEE Visualization papers. With the\nresulting data, we also contribute VISImageNavigator (VIN,\nvisimagenavigator.github.io), a web-based tool that facilitates searching and\nexploring VIS30K by author names, paper keywords, title and abstract, and\nyears.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 19:56:29 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 03:05:49 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 11:50:39 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Jian", ""], ["Ling", "Meng", ""], ["Li", "Rui", ""], ["Isenberg", "Petra", ""], ["Isenberg", "Tobias", ""], ["Sedlmair", "Michael", ""], ["M\u00f6ller", "Torsten", ""], ["Laramee", "Robert S.", ""], ["Shen", "Han-Wei", ""], ["W\u00fcnsche", "Katharina", ""], ["Wang", "Qiru", ""]]}, {"id": "2101.01042", "submitter": "Gustavo de Rosa", "authors": "Gustavo H. de Rosa, Mateus Roder, Jo\\~ao P. Papa", "title": "Fast Ensemble Learning Using Adversarially-Generated Restricted\n  Boltzmann Machines", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning has been applied in a wide range of tasks throughout the\nlast years, ranging from image classification to autonomous driving and natural\nlanguage processing. Restricted Boltzmann Machine (RBM) has received recent\nattention and relies on an energy-based structure to model data probability\ndistributions. Notwithstanding, such a technique is susceptible to adversarial\nmanipulation, i.e., slightly or profoundly modified data. An alternative to\novercome the adversarial problem lies in the Generative Adversarial Networks\n(GAN), capable of modeling data distributions and generating adversarial data\nthat resemble the original ones. Therefore, this work proposes to artificially\ngenerate RBMs using Adversarial Learning, where pre-trained weight matrices\nserve as the GAN inputs. Furthermore, it proposes to sample copious amounts of\nmatrices and combine them into ensembles, alleviating the burden of training\nnew models'. Experimental results demonstrate the suitability of the proposed\napproach under image reconstruction and image classification tasks, and\ndescribe how artificial-based ensembles are alternatives to pre-training vast\namounts of RBMs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:00:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["de Rosa", "Gustavo H.", ""], ["Roder", "Mateus", ""], ["Papa", "Jo\u00e3o P.", ""]]}, {"id": "2101.01054", "submitter": "Sagar Gubbi Venkatesh", "authors": "Sagar Gubbi and Bharadwaj Amrutur", "title": "Scene Text Detection for Augmented Reality -- Character Bigram Approach\n  to reduce False Positive Rate", "comments": null, "journal-ref": null, "doi": "10.1007/s40012-018-0203-2", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Natural scene text detection is an important aspect of scene understanding\nand could be a useful tool in building engaging augmented reality applications.\nIn this work, we address the problem of false positives in text spotting. We\npropose improving the performace of sliding window text spotters by looking for\ncharacter pairs (bigrams) rather than single characters. An efficient\nconvolutional neural network is designed and trained to detect bigrams. The\nproposed detector reduces false positive rate by 28.16% on the ICDAR 2015\ndataset. We demonstrate that detecting bigrams is a computationally inexpensive\nway to improve sliding window text spotters.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 08:56:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Gubbi", "Sagar", ""], ["Amrutur", "Bharadwaj", ""]]}, {"id": "2101.01060", "submitter": "Jizhe Zhou", "authors": "Jizhe Zhou, Chi-Man Pun", "title": "Personal Privacy Protection via Irrelevant Faces Tracking and Pixelation\n  in Video Live Streaming", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2020.3029913", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To date, the privacy-protection intended pixelation tasks are still\nlabor-intensive and yet to be studied. With the prevailing of video live\nstreaming, establishing an online face pixelation mechanism during streaming is\nan urgency. In this paper, we develop a new method called Face Pixelation in\nVideo Live Streaming (FPVLS) to generate automatic personal privacy filtering\nduring unconstrained streaming activities. Simply applying multi-face trackers\nwill encounter problems in target drifting, computing efficiency, and\nover-pixelation. Therefore, for fast and accurate pixelation of irrelevant\npeople's faces, FPVLS is organized in a frame-to-video structure of two core\nstages. On individual frames, FPVLS utilizes image-based face detection and\nembedding networks to yield face vectors. In the raw trajectories generation\nstage, the proposed Positioned Incremental Affinity Propagation (PIAP)\nclustering algorithm leverages face vectors and positioned information to\nquickly associate the same person's faces across frames. Such frame-wise\naccumulated raw trajectories are likely to be intermittent and unreliable on\nvideo level. Hence, we further introduce the trajectory refinement stage that\nmerges a proposal network with the two-sample test based on the Empirical\nLikelihood Ratio (ELR) statistic to refine the raw trajectories. A Gaussian\nfilter is laid on the refined trajectories for final pixelation. On the video\nlive streaming dataset we collected, FPVLS obtains satisfying accuracy,\nreal-time efficiency, and contains the over-pixelation problems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:18:26 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 14:01:09 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zhou", "Jizhe", ""], ["Pun", "Chi-Man", ""]]}, {"id": "2101.01073", "submitter": "Ramna Maqsood", "authors": "R. Maqsood, UI. Bajwa, G. Saleem, Rana H. Raza, MW. Anwar", "title": "Anomaly Recognition from surveillance videos using 3D Convolutional\n  Neural Networks", "comments": "25 pages, 7, figures, 8 Tables, Under Review in Multimedia Tools and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalous activity recognition deals with identifying the patterns and events\nthat vary from the normal stream. In a surveillance paradigm, these events\nrange from abuse to fighting and road accidents to snatching, etc. Due to the\nsparse occurrence of anomalous events, anomalous activity recognition from\nsurveillance videos is a challenging research task. The approaches reported can\nbe generally categorized as handcrafted and deep learning-based. Most of the\nreported studies address binary classification i.e. anomaly detection from\nsurveillance videos. But these reported approaches did not address other\nanomalous events e.g. abuse, fight, road accidents, shooting, stealing,\nvandalism, and robbery, etc. from surveillance videos. Therefore, this paper\naims to provide an effective framework for the recognition of different\nreal-world anomalies from videos. This study provides a simple, yet effective\napproach for learning spatiotemporal features using deep 3-dimensional\nconvolutional networks (3D ConvNets) trained on the University of Central\nFlorida (UCF) Crime video dataset. Firstly, the frame-level labels of the UCF\nCrime dataset are provided, and then to extract anomalous spatiotemporal\nfeatures more efficiently a fine-tuned 3D ConvNets is proposed. Findings of the\nproposed study are twofold 1)There exist specific, detectable, and quantifiable\nfeatures in UCF Crime video feed that associate with each other 2) Multiclass\nlearning can improve generalizing competencies of the 3D ConvNets by\neffectively learning frame-level information of dataset and can be leveraged in\nterms of better results by applying spatial augmentation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:32:48 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Maqsood", "R.", ""], ["Bajwa", "UI.", ""], ["Saleem", "G.", ""], ["Raza", "Rana H.", ""], ["Anwar", "MW.", ""]]}, {"id": "2101.01097", "submitter": "Junyong You", "authors": "Junyong You, Jari Korhonen", "title": "Transformer for Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer has become the new standard method in natural language processing\n(NLP), and it also attracts research interests in computer vision area. In this\npaper we investigate the application of Transformer in Image Quality (TRIQ)\nassessment. Following the original Transformer encoder employed in Vision\nTransformer (ViT), we propose an architecture of using a shallow Transformer\nencoder on the top of a feature map extracted by convolution neural networks\n(CNN). Adaptive positional embedding is employed in the Transformer encoder to\nhandle images with arbitrary resolutions. Different settings of Transformer\narchitectures have been investigated on publicly available image quality\ndatabases. We have found that the proposed TRIQ architecture achieves\noutstanding performance. The implementation of TRIQ is published on Github\n(https://github.com/junyongyou/triq).\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 18:43:11 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 12:12:32 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["You", "Junyong", ""], ["Korhonen", "Jari", ""]]}, {"id": "2101.01104", "submitter": "Zhen Fang", "authors": "Li Zhong, Zhen Fang, Feng Liu, Jie Lu, Bo Yuan, Guangquan Zhang", "title": "How does the Combined Risk Affect the Performance of Unsupervised Domain\n  Adaptation Approaches?", "comments": "9 pages, 3 figures, Accepted by Association for the Advancement of\n  Artificial Intelligence 2021 (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to train a target classifier with\nlabeled samples from the source domain and unlabeled samples from the target\ndomain. Classical UDA learning bounds show that target risk is upper bounded by\nthree terms: source risk, distribution discrepancy, and combined risk. Based on\nthe assumption that the combined risk is a small fixed value, methods based on\nthis bound train a target classifier by only minimizing estimators of the\nsource risk and the distribution discrepancy. However, the combined risk may\nincrease when minimizing both estimators, which makes the target risk\nuncontrollable. Hence the target classifier cannot achieve ideal performance if\nwe fail to control the combined risk. To control the combined risk, the key\nchallenge takes root in the unavailability of the labeled samples in the target\ndomain. To address this key challenge, we propose a method named E-MixNet.\nE-MixNet employs enhanced mixup, a generic vicinal distribution, on the labeled\nsource samples and pseudo-labeled target samples to calculate a proxy of the\ncombined risk. Experiments show that the proxy can effectively curb the\nincrease of the combined risk when minimizing the source risk and distribution\ndiscrepancy. Furthermore, we show that if the proxy of the combined risk is\nadded into loss functions of four representative UDA methods, their performance\nis also improved.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 00:46:57 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhong", "Li", ""], ["Fang", "Zhen", ""], ["Liu", "Feng", ""], ["Lu", "Jie", ""], ["Yuan", "Bo", ""], ["Zhang", "Guangquan", ""]]}, {"id": "2101.01115", "submitter": "Yunus Camg\\\"ozl\\\"u", "authors": "Yunus Camg\\\"ozl\\\"u, Yakup Kutlu", "title": "Analysis of Filter Size Effect In Deep Learning", "comments": "10 Pages, 9 Figures, Journal of Artificial Intelligence with\n  Applications, published", "journal-ref": "Journal of Artificial Intelligence with Applications, 1(1), 20-29,\n  2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the use of deep learning in many areas, how to improve this technology\nor how to develop the structure used more effectively and in a shorter time is\nan issue that is of interest to many people working in this field. Many studies\nare carried out on this subject, it is aimed to reduce the duration of the\noperation and the processing power required, except to obtain the best result\nwith the changes made in the variables, functions and data in the models used.\nIn this study, in the leaf classification made using Mendeley data set\nconsisting of leaf images with a fixed background, all other variables such as\nlayer number, iteration, number of layers in the model and pooling process were\nkept constant, except for the filter dimensions of the convolution layers in\nthe determined model. Convolution layers in 3 different filter sizes and in\naddition to this, many results obtained in 2 different structures, increasing\nand decreasing, and 3 different image sizes were examined. In the literature,\nit is seen that different uses of pooling layers, changes due to increase or\ndecrease in the number of layers, the difference in the size of the data used,\nand the results of many functions used with different parameters are evaluated.\nIn the leaf classification of the determined data set with CNN, the change in\nthe filter size of the convolution layer together with the change in different\nfilter combinations and in different sized images was focused. Using the data\nset and data reproduction methods, it was aimed to make the differences in\nfilter sizes and image sizes more distinct. Using the fixed number of\niterations, model and data set, the effect of different filter sizes has been\nobserved.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 11:05:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Camg\u00f6zl\u00fc", "Yunus", ""], ["Kutlu", "Yakup", ""]]}, {"id": "2101.01133", "submitter": "Max Allan", "authors": "Max Allan and Jonathan Mcleod and Congcong Wang and Jean Claude\n  Rosenthal and Zhenglei Hu and Niklas Gard and Peter Eisert and Ke Xue Fu and\n  Trevor Zeffiro and Wenyao Xia and Zhanshi Zhu and Huoling Luo and Fucang Jia\n  and Xiran Zhang and Xiaohong Li and Lalith Sharan and Tom Kurmann and\n  Sebastian Schmid and Raphael Sznitman and Dimitris Psychogyios and Mahdi\n  Azizian and Danail Stoyanov and Lena Maier-Hein and Stefanie Speidel", "title": "Stereo Correspondence and Reconstruction of Endoscopic Data Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The stereo correspondence and reconstruction of endoscopic data sub-challenge\nwas organized during the Endovis challenge at MICCAI 2019 in Shenzhen, China.\nThe task was to perform dense depth estimation using 7 training datasets and 2\ntest sets of structured light data captured using porcine cadavers. These were\nprovided by a team at Intuitive Surgical. 10 teams participated in the\nchallenge day. This paper contains 3 additional methods which were submitted\nafter the challenge finished as well as a supplemental section from these teams\non issues they found with the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 17:58:48 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 19:36:32 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 00:27:21 GMT"}, {"version": "v4", "created": "Thu, 28 Jan 2021 20:09:37 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Allan", "Max", ""], ["Mcleod", "Jonathan", ""], ["Wang", "Congcong", ""], ["Rosenthal", "Jean Claude", ""], ["Hu", "Zhenglei", ""], ["Gard", "Niklas", ""], ["Eisert", "Peter", ""], ["Fu", "Ke Xue", ""], ["Zeffiro", "Trevor", ""], ["Xia", "Wenyao", ""], ["Zhu", "Zhanshi", ""], ["Luo", "Huoling", ""], ["Jia", "Fucang", ""], ["Zhang", "Xiran", ""], ["Li", "Xiaohong", ""], ["Sharan", "Lalith", ""], ["Kurmann", "Tom", ""], ["Schmid", "Sebastian", ""], ["Sznitman", "Raphael", ""], ["Psychogyios", "Dimitris", ""], ["Azizian", "Mahdi", ""], ["Stoyanov", "Danail", ""], ["Maier-Hein", "Lena", ""], ["Speidel", "Stefanie", ""]]}, {"id": "2101.01154", "submitter": "Nikolay Malkin", "authors": "Nikolay Malkin, Caleb Robinson, Nebojsa Jojic", "title": "High-resolution land cover change from low-resolution labels: Simple\n  baselines for the 2021 IEEE GRSS Data Fusion Contest", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present simple algorithms for land cover change detection in the 2021 IEEE\nGRSS Data Fusion Contest. The task of the contest is to create high-resolution\n(1m / pixel) land cover change maps of a study area in Maryland, USA, given\nmulti-resolution imagery and label data. We study several baseline models for\nthis task and discuss directions for further research.\n  See https://dfc2021.blob.core.windows.net/competition-data/dfc2021_index.txt\nfor the data and https://github.com/calebrob6/dfc2021-msd-baseline for an\nimplementation of these baselines.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:33:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Malkin", "Nikolay", ""], ["Robinson", "Caleb", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "2101.01165", "submitter": "Ilke Demir", "authors": "Ilke Demir and Umur A. Ciftci", "title": "Where Do Deep Fakes Look? Synthetic Face Detection via Gaze Tracking", "comments": "To appear in the proceedings of ACM ETRA 2021", "journal-ref": null, "doi": "10.1145/3448017.3457387", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the recent initiatives for the democratization of AI, deep fake\ngenerators have become increasingly popular and accessible, causing dystopian\nscenarios towards social erosion of trust. A particular domain, such as\nbiological signals, attracted attention towards detection methods that are\ncapable of exploiting authenticity signatures in real videos that are not yet\nfaked by generative approaches. In this paper, we first propose several\nprominent eye and gaze features that deep fakes exhibit differently. Second, we\ncompile those features into signatures and analyze and compare those of real\nand fake videos, formulating geometric, visual, metric, temporal, and spectral\nvariations. Third, we generalize this formulation to the deep fake detection\nproblem by a deep neural network, to classify any video in the wild as fake or\nreal. We evaluate our approach on several deep fake datasets, achieving 92.48%\naccuracy on FaceForensics++, 80.0% on Deep Fakes (in the wild), 88.35% on\nCelebDF, and 99.27% on DeeperForensics datasets. Our approach outperforms most\ndeep and biological fake detectors with complex network architectures without\nthe proposed gaze signatures. We conduct ablation studies involving different\nfeatures, architectures, sequence durations, and post-processing artifacts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:54:46 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 17:59:20 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Demir", "Ilke", ""], ["Ciftci", "Umur A.", ""]]}, {"id": "2101.01169", "submitter": "Salman Khan Dr.", "authors": "Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad\n  Shahbaz Khan, Mubarak Shah", "title": "Transformers in Vision: A Survey", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Astounding results from Transformer models on natural language tasks have\nintrigued the vision community to study their application to computer vision\nproblems. Among their salient benefits, Transformers enable modeling long\ndependencies between input sequence elements and support parallel processing of\nsequence as compared to recurrent networks e.g., Long short-term memory (LSTM).\nDifferent from convolutional networks, Transformers require minimal inductive\nbiases for their design and are naturally suited as set-functions. Furthermore,\nthe straightforward design of Transformers allows processing multiple\nmodalities (e.g., images, videos, text and speech) using similar processing\nblocks and demonstrates excellent scalability to very large capacity networks\nand huge datasets. These strengths have led to exciting progress on a number of\nvision tasks using Transformer networks. This survey aims to provide a\ncomprehensive overview of the Transformer models in the computer vision\ndiscipline. We start with an introduction to fundamental concepts behind the\nsuccess of Transformers i.e., self-attention, large-scale pre-training, and\nbidirectional encoding. We then cover extensive applications of transformers in\nvision including popular recognition tasks (e.g., image classification, object\ndetection, action recognition, and segmentation), generative modeling,\nmulti-modal tasks (e.g., visual-question answering, visual reasoning, and\nvisual grounding), video processing (e.g., activity recognition, video\nforecasting), low-level vision (e.g., image super-resolution, image\nenhancement, and colorization) and 3D analysis (e.g., point cloud\nclassification and segmentation). We compare the respective advantages and\nlimitations of popular techniques both in terms of architectural design and\ntheir experimental value. Finally, we provide an analysis on open research\ndirections and possible future works.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:57:24 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 11:40:11 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Khan", "Salman", ""], ["Naseer", "Muzammal", ""], ["Hayat", "Munawar", ""], ["Zamir", "Syed Waqas", ""], ["Khan", "Fahad Shahbaz", ""], ["Shah", "Mubarak", ""]]}, {"id": "2101.01178", "submitter": "Jeffrey Ede BSc MPhys", "authors": "Jeffrey M. Ede", "title": "Advances in Electron Microscopy with Deep Learning", "comments": "295 pages, phd thesis, 100 figures + 12 tables, papers are compressed", "journal-ref": null, "doi": "10.5281/zenodo.4399748", "report-no": null, "categories": "eess.IV cond-mat.mtrl-sci cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This doctoral thesis covers some of my advances in electron microscopy with\ndeep learning. Highlights include a comprehensive review of deep learning in\nelectron microscopy; large new electron microscopy datasets for machine\nlearning, dataset search engines based on variational autoencoders, and\nautomatic data clustering by t-distributed stochastic neighbour embedding;\nadaptive learning rate clipping to stabilize learning; generative adversarial\nnetworks for compressed sensing with spiral, uniformly spaced and other fixed\nsparse scan paths; recurrent neural networks trained to piecewise adapt sparse\nscan paths to specimens by reinforcement learning; improving signal-to-noise;\nand conditional generative adversarial networks for exit wavefunction\nreconstruction from single transmission electron micrographs. This thesis adds\nto my publications by presenting their relationships, reflections, and holistic\nconclusions. This version of my thesis is typeset for online dissemination to\nimprove readability, whereas the thesis submitted to the University of Warwick\nin support of my application for the degree of Doctor of Philosophy in Physics\nis typeset for physical printing and binding.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:49:37 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 17:30:04 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 12:06:00 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 14:53:24 GMT"}, {"version": "v5", "created": "Thu, 11 Mar 2021 17:25:33 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ede", "Jeffrey M.", ""]]}, {"id": "2101.01207", "submitter": "Peter He", "authors": "Peter He, Raksha Jain, J\\'er\\^ome Chambost, C\\'eline Jacques, Cristina\n  Hickman", "title": "Semantic Video Segmentation for Intracytoplasmic Sperm Injection\n  Procedures", "comments": "Accepted at the 'Medical Imaging meets NeurIPS Workshop' at the 34th\n  Conference on Neural Information Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first deep learning model for the analysis of intracytoplasmic\nsperm injection (ICSI) procedures. Using a dataset of ICSI procedure videos, we\ntrain a deep neural network to segment key objects in the videos achieving a\nmean IoU of 0.962, and to localize the needle tip achieving a mean pixel error\nof 3.793 pixels at 14 FPS on a single GPU. We further analyze the variation\nbetween the dataset's human annotators and find the model's performance to be\ncomparable to human experts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 19:33:12 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["He", "Peter", ""], ["Jain", "Raksha", ""], ["Chambost", "J\u00e9r\u00f4me", ""], ["Jacques", "C\u00e9line", ""], ["Hickman", "Cristina", ""]]}, {"id": "2101.01214", "submitter": "Eric Guzman", "authors": "Eric Guzman and Joel Meyers", "title": "Reconstructing Patchy Reionization with Deep Learning", "comments": "14 pages, 9 figures. Code available from\n  https://github.com/EEmGuzman/resunet-cmb", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The precision anticipated from next-generation cosmic microwave background\n(CMB) surveys will create opportunities for characteristically new insights\ninto cosmology. Secondary anisotropies of the CMB will have an increased\nimportance in forthcoming surveys, due both to the cosmological information\nthey encode and the role they play in obscuring our view of the primary\nfluctuations. Quadratic estimators have become the standard tools for\nreconstructing the fields that distort the primary CMB and produce secondary\nanisotropies. While successful for lensing reconstruction with current data,\nquadratic estimators will be sub-optimal for the reconstruction of lensing and\nother effects at the expected sensitivity of the upcoming CMB surveys. In this\npaper we describe a convolutional neural network, ResUNet-CMB, that is capable\nof the simultaneous reconstruction of two sources of secondary CMB\nanisotropies, gravitational lensing and patchy reionization. We show that the\nResUNet-CMB network significantly outperforms the quadratic estimator at low\nnoise levels and is not subject to the lensing-induced bias on the patchy\nreionization reconstruction that would be present with a straightforward\napplication of the quadratic estimator.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 19:58:28 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Guzman", "Eric", ""], ["Meyers", "Joel", ""]]}, {"id": "2101.01215", "submitter": "Teofilo de Campos", "authors": "Tiago de C. G. Pereira and Teofilo E. de Campos", "title": "Learn by Guessing: Multi-Step Pseudo-Label Refinement for Person\n  Re-Identification", "comments": "11 pages, 2 fitures, 48 references. Submitted to a computer vision\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) methods for person Re-Identification\n(Re-ID) rely on target domain samples to model the marginal distribution of the\ndata. To deal with the lack of target domain labels, UDA methods leverage\ninformation from labeled source samples and unlabeled target samples. A\npromising approach relies on the use of unsupervised learning as part of the\npipeline, such as clustering methods. The quality of the clusters clearly plays\na major role in methods performance, but this point has been overlooked. In\nthis work, we propose a multi-step pseudo-label refinement method to select the\nbest possible clusters and keep improving them so that these clusters become\ncloser to the class divisions without knowledge of the class labels. Our\nrefinement method includes a cluster selection strategy and a camera-based\nnormalization method which reduces the within-domain variations caused by the\nuse of multiple cameras in person Re-ID. This allows our method to reach\nstate-of-the-art UDA results on DukeMTMC-Market1501 (source-target). We surpass\nstate-of-the-art for UDA Re-ID by 3.4% on Market1501-DukeMTMC datasets, which\nis a more challenging adaptation setup because the target domain (DukeMTMC) has\neight distinct cameras. Furthermore, the camera-based normalization method\ncauses a significant reduction in the number of iterations required for\ntraining convergence.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 20:00:33 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Pereira", "Tiago de C. G.", ""], ["de Campos", "Teofilo E.", ""]]}, {"id": "2101.01260", "submitter": "Keren Ye", "authors": "Keren Ye, Adriana Kovashka, Mark Sandler, Menglong Zhu, Andrew Howard,\n  Marco Fornoni", "title": "SpotPatch: Parameter-Efficient Transfer Learning for Mobile Object\n  Detection", "comments": "Accepted by the ACCV2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based object detectors are commonly deployed on mobile devices\nto solve a variety of tasks. For maximum accuracy, each detector is usually\ntrained to solve one single specific task, and comes with a completely\nindependent set of parameters. While this guarantees high performance, it is\nalso highly inefficient, as each model has to be separately downloaded and\nstored. In this paper we address the question: can task-specific detectors be\ntrained and represented as a shared set of weights, plus a very small set of\nadditional weights for each task? The main contributions of this paper are the\nfollowing: 1) we perform the first systematic study of parameter-efficient\ntransfer learning techniques for object detection problems; 2) we propose a\ntechnique to learn a model patch with a size that is dependent on the\ndifficulty of the task to be learned, and validate our approach on 10 different\nobject detection tasks. Our approach achieves similar accuracy as previously\nproposed approaches, while being significantly more compact.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 22:24:06 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Ye", "Keren", ""], ["Kovashka", "Adriana", ""], ["Sandler", "Mark", ""], ["Zhu", "Menglong", ""], ["Howard", "Andrew", ""], ["Fornoni", "Marco", ""]]}, {"id": "2101.01308", "submitter": "Guankai Li", "authors": "Chi Zhang, Guankai Li, Guosheng Lin, Qingyao Wu, Rui Yao", "title": "CycleSegNet: Object Co-segmentation with Cycle Refinement and Region\n  Correspondence", "comments": "Accept to TIP", "journal-ref": null, "doi": "10.1109/TIP.2021.3087401", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image co-segmentation is an active computer vision task that aims to segment\nthe common objects from a set of images. Recently, researchers design various\nlearning-based algorithms to undertake the co-segmentation task. The main\ndifficulty in this task is how to effectively transfer information between\nimages to make conditional predictions. In this paper, we present CycleSegNet,\na novel framework for the co-segmentation task. Our network design has two key\ncomponents: a region correspondence module which is the basic operation for\nexchanging information between local image regions, and a cycle refinement\nmodule, which utilizes ConvLSTMs to progressively update image representations\nand exchange information in a cycle and iterative manner. Extensive experiments\ndemonstrate that our proposed method significantly outperforms the\nstate-of-the-art methods on four popular benchmark datasets -- PASCAL VOC\ndataset, MSRC dataset, Internet dataset, and iCoseg dataset, by 2.6%, 7.7%,\n2.2%, and 2.9%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 01:35:19 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 08:00:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhang", "Chi", ""], ["Li", "Guankai", ""], ["Lin", "Guosheng", ""], ["Wu", "Qingyao", ""], ["Yao", "Rui", ""]]}, {"id": "2101.01310", "submitter": "Haiyu Wu", "authors": "Fukang Tian, Haiyu Wu, Bo Xu", "title": "Research on Fast Text Recognition Method for Financial Ticket Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, deep learning methods have been widely applied in and thus\npromoted the development of different fields. In the financial accounting\nfield, the rapid increase in the number of financial tickets dramatically\nincreases labor costs; hence, using a deep learning method to relieve the\npressure on accounting is necessary. At present, a few works have applied deep\nlearning methods to financial ticket recognition. However, first, their\napproaches only cover a few types of tickets. In addition, the precision and\nspeed of their recognition models cannot meet the requirements of practical\nfinancial accounting systems. Moreover, none of the methods provides a detailed\nanalysis of both the types and content of tickets. Therefore, this paper first\nanalyzes the different features of 482 kinds of financial tickets, divides all\nkinds of financial tickets into three categories and proposes different\nrecognition patterns for each category. These recognition patterns can meet\nalmost all types of financial ticket recognition needs. Second, regarding the\nfixed format types of financial tickets (accounting for 68.27\\% of the total\ntypes of tickets), we propose a simple yet efficient network named the\nFinancial Ticket Faster Detection network (FTFDNet) based on a Faster RCNN.\nFurthermore, according to the characteristics of the financial ticket text, in\norder to obtain higher recognition accuracy, the loss function, Region Proposal\nNetwork (RPN), and Non-Maximum Suppression (NMS) are improved to make FTFDNet\nfocus more on text. Finally, we perform a comparison with the best ticket\nrecognition model from the ICDAR2019 invoice competition. The experimental\nresults illustrate that FTFDNet increases the processing speed by 50\\% while\nmaintaining similar precision.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 01:42:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Tian", "Fukang", ""], ["Wu", "Haiyu", ""], ["Xu", "Bo", ""]]}, {"id": "2101.01322", "submitter": "Bin Lee", "authors": "Bin Li and Mu Hu and Shuling Wang and Lianghao Wang and Xiaojin Gong", "title": "Self-supervised Visual-LiDAR Odometry with Flip Consistency", "comments": "9 pages, 5 figures, 2 tables, WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most learning-based methods estimate ego-motion by utilizing visual sensors,\nwhich suffer from dramatic lighting variations and textureless scenarios. In\nthis paper, we incorporate sparse but accurate depth measurements obtained from\nlidars to overcome the limitation of visual methods. To this end, we design a\nself-supervised visual-lidar odometry (Self-VLO) framework. It takes both\nmonocular images and sparse depth maps projected from 3D lidar points as input,\nand produces pose and depth estimations in an end-to-end learning manner,\nwithout using any ground truth labels. To effectively fuse two modalities, we\ndesign a two-pathway encoder to extract features from visual and depth images\nand fuse the encoded features with those in decoders at multiple scales by our\nfusion module. We also adopt a siamese architecture and design an adaptively\nweighted flip consistency loss to facilitate the self-supervised learning of\nour VLO. Experiments on the KITTI odometry benchmark show that the proposed\napproach outperforms all self-supervised visual or lidar odometries. It also\nperforms better than fully supervised VOs, demonstrating the power of fusion.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 02:42:59 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Li", "Bin", ""], ["Hu", "Mu", ""], ["Wang", "Shuling", ""], ["Wang", "Lianghao", ""], ["Gong", "Xiaojin", ""]]}, {"id": "2101.01355", "submitter": "Eva Agapaki", "authors": "Eva Agapaki and Ioannis Brilakis", "title": "CLOI: An Automated Benchmark Framework For Generating Geometric Digital\n  Twins Of Industrial Facilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper devises, implements and benchmarks a novel framework, named CLOI,\nthat can accurately generate individual labelled point clusters of the most\nimportant shapes of existing industrial facilities with minimal manual effort\nin a generic point-level format. CLOI employs a combination of deep learning\nand geometric methods to segment the points into classes and individual\ninstances. The current geometric digital twin generation from point cloud data\nin commercial software is a tedious, manual process. Experiments with our CLOI\nframework reveal that the method can reliably segment complex and incomplete\npoint clouds of industrial facilities, yielding 82% class segmentation\naccuracy. Compared to the current state-of-practice, the proposed framework can\nrealize estimated time-savings of 30% on average. CLOI is the first framework\nof its kind to have achieved geometric digital twinning for the most important\nobjects of industrial factories. It provides the foundation for further\nresearch on the generation of semantically enriched digital twins of the built\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 05:47:08 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Agapaki", "Eva", ""], ["Brilakis", "Ioannis", ""]]}, {"id": "2101.01362", "submitter": "Ma Le", "authors": "Le Ma, Xiaoyue Wu, Zhiwei Li", "title": "High Precision Medicine Bottles Vision Online Inspection System and\n  Classification Based on Multi-Features and Ensemble Learning via Independence\n  Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To address the problem of online automatic inspection of drug liquid bottles\nin production line, an implantable visual inspection system is designed and the\nensemble learning algorithm for detection is proposed based on multi-features\nfusion. A tunnel structure is designed for visual inspection system, which\nallows bottles inspection to be automated without changing original\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 06:19:27 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 06:10:36 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ma", "Le", ""], ["Wu", "Xiaoyue", ""], ["Li", "Zhiwei", ""]]}, {"id": "2101.01368", "submitter": "Haiwen Diao", "authors": "Haiwen Diao, Ying Zhang, Lin Ma, Huchuan Lu", "title": "Similarity Reasoning and Filtration for Image-Text Matching", "comments": "14 pages, 8 figures, Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-text matching plays a critical role in bridging the vision and\nlanguage, and great progress has been made by exploiting the global alignment\nbetween image and sentence, or local alignments between regions and words.\nHowever, how to make the most of these alignments to infer more accurate\nmatching scores is still underexplored. In this paper, we propose a novel\nSimilarity Graph Reasoning and Attention Filtration (SGRAF) network for\nimage-text matching. Specifically, the vector-based similarity representations\nare firstly learned to characterize the local and global alignments in a more\ncomprehensive manner, and then the Similarity Graph Reasoning (SGR) module\nrelying on one graph convolutional neural network is introduced to infer\nrelation-aware similarities with both the local and global alignments. The\nSimilarity Attention Filtration (SAF) module is further developed to integrate\nthese alignments effectively by selectively attending on the significant and\nrepresentative alignments and meanwhile casting aside the interferences of\nnon-meaningful alignments. We demonstrate the superiority of the proposed\nmethod with achieving state-of-the-art performances on the Flickr30K and MSCOCO\ndatasets, and the good interpretability of SGR and SAF modules with extensive\nqualitative experiments and analyses.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 06:29:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Diao", "Haiwen", ""], ["Zhang", "Ying", ""], ["Ma", "Lin", ""], ["Lu", "Huchuan", ""]]}, {"id": "2101.01373", "submitter": "Moein Razavi", "authors": "Moein Razavi, Hamed Alikhani, Vahid Janfaza, Benyamin Sadeghi, Ehsan\n  Alikhani", "title": "An Automatic System to Monitor the Physical Distance and Face Mask\n  Wearing of Construction Workers in COVID-19 Pandemic", "comments": "7 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The COVID-19 pandemic has caused many shutdowns in different industries\naround the world. Sectors such as infrastructure construction and maintenance\nprojects have not been suspended due to their significant effect on people's\nroutine life. In such projects, workers work close together that makes a high\nrisk of infection. The World Health Organization recommends wearing a face mask\nand practicing physical distancing to mitigate the virus's spread. This paper\ndeveloped a computer vision system to automatically detect the violation of\nface mask wearing and physical distancing among construction workers to assure\ntheir safety on infrastructure projects during the pandemic. For the face mask\ndetection, the paper collected and annotated 1,000 images, including different\ntypes of face mask wearing, and added them to a pre-existing face mask dataset\nto develop a dataset of 1,853 images. Then trained and tested multiple\nTensorflow state-of-the-art object detection models on the face mask dataset\nand chose the Faster R-CNN Inception ResNet V2 network that yielded the\naccuracy of 99.8%. For physical distance detection, the paper employed the\nFaster R-CNN Inception V2 to detect people. A transformation matrix was used to\neliminate the camera angle's effect on the object distances on the image. The\nEuclidian distance used the pixels of the transformed image to compute the\nactual distance between people. A threshold of six feet was considered to\ncapture physical distance violation. The paper also used transfer learning for\ntraining the model. The final model was applied on four videos of road\nmaintenance projects in Houston, TX, that effectively detected the face mask\nand physical distance. We recommend that construction owners use the proposed\nsystem to enhance construction workers' safety in the pandemic situation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 06:53:41 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 02:08:46 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Razavi", "Moein", ""], ["Alikhani", "Hamed", ""], ["Janfaza", "Vahid", ""], ["Sadeghi", "Benyamin", ""], ["Alikhani", "Ehsan", ""]]}, {"id": "2101.01386", "submitter": "Shuyue Guan", "authors": "Shuyue Guan, Murray Loew", "title": "Understanding the Ability of Deep Neural Networks to Count Connected\n  Components in Images", "comments": "7 pages, 12 figures. Accepted by IEEE AIPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can count very fast by subitizing, but slow substantially as the\nnumber of objects increases. Previous studies have shown a trained deep neural\nnetwork (DNN) detector can count the number of objects in an amount of time\nthat increases slowly with the number of objects. Such a phenomenon suggests\nthe subitizing ability of DNNs, and unlike humans, it works equally well for\nlarge numbers. Many existing studies have successfully applied DNNs to object\ncounting, but few studies have studied the subitizing ability of DNNs and its\ninterpretation. In this paper, we found DNNs do not have the ability to\ngenerally count connected components. We provided experiments to support our\nconclusions and explanations to understand the results and phenomena of these\nexperiments. We proposed three ML-learnable characteristics to verify learnable\nproblems for ML models, such as DNNs, and explain why DNNs work for specific\ncounting problems but cannot generally count connected components.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 07:28:34 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Guan", "Shuyue", ""], ["Loew", "Murray", ""]]}, {"id": "2101.01394", "submitter": "Xi Li", "authors": "Huanzhang Dou, Wenhu Zhang, Pengyi Zhang, Yuhan Zhao, Songyuan Li,\n  Zequn Qin, Fei Wu, Lin Dong, Xi Li", "title": "VersatileGait: A Large-Scale Synthetic Gait Dataset with\n  Fine-GrainedAttributes and Complicated Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the motivation of practical gait recognition applications, we propose to\nautomatically create a large-scale synthetic gait dataset (called\nVersatileGait) by a game engine, which consists of around one million\nsilhouette sequences of 11,000 subjects with fine-grained attributes in various\ncomplicated scenarios. Compared with existing real gait datasets with limited\nsamples and simple scenarios, the proposed VersatileGait dataset possesses\nseveral nice properties, including huge dataset size, high sample diversity,\nhigh-quality annotations, multi-pitch angles, small domain gap with the real\none, etc. Furthermore, we investigate the effectiveness of our dataset (e.g.,\ndomain transfer after pretraining). Then, we use the fine-grained attributes\nfrom VersatileGait to promote gait recognition in both accuracy and speed, and\nmeanwhile justify the gait recognition performance under multi-pitch angle\nsettings. Additionally, we explore a variety of potential applications for\nresearch.Extensive experiments demonstrate the value and effective-ness of the\nproposed VersatileGait in gait recognition along with its associated\napplications. We will release both VersatileGait and its corresponding data\ngeneration toolkit for further studies.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 07:45:12 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Dou", "Huanzhang", ""], ["Zhang", "Wenhu", ""], ["Zhang", "Pengyi", ""], ["Zhao", "Yuhan", ""], ["Li", "Songyuan", ""], ["Qin", "Zequn", ""], ["Wu", "Fei", ""], ["Dong", "Lin", ""], ["Li", "Xi", ""]]}, {"id": "2101.01400", "submitter": "Chongxuan Li", "authors": "Qijun Luo, Zhili Liu, Lanqing Hong, Chongxuan Li, Kuo Yang, Liyuan\n  Wang, Fengwei Zhou, Guilin Li, Zhenguo Li, Jun Zhu", "title": "Relaxed Conditional Image Transfer for Semi-supervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised domain adaptation (SSDA), which aims to learn models in a\npartially labeled target domain with the assistance of the fully labeled source\ndomain, attracts increasing attention in recent years. To explicitly leverage\nthe labeled data in both domains, we naturally introduce a conditional GAN\nframework to transfer images without changing the semantics in SSDA. However,\nwe identify a label-domination problem in such an approach. In fact, the\ngenerator tends to overlook the input source image and only memorizes\nprototypes of each class, which results in unsatisfactory adaptation\nperformance. To this end, we propose a simple yet effective Relaxed conditional\nGAN (Relaxed cGAN) framework. Specifically, we feed the image without its label\nto our generator. In this way, the generator has to infer the semantic\ninformation of input data. We formally prove that its equilibrium is desirable\nand empirically validate its practical convergence and effectiveness in image\ntransfer. Additionally, we propose several techniques to make use of unlabeled\ndata in the target domain, enhancing the model in SSDA settings. We validate\nour method on the well-adopted datasets: Digits, DomainNet, and Office-Home. We\nachieve state-of-the-art performance on DomainNet, Office-Home and most digit\nbenchmarks in low-resource and high-resource settings.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:15:48 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Luo", "Qijun", ""], ["Liu", "Zhili", ""], ["Hong", "Lanqing", ""], ["Li", "Chongxuan", ""], ["Yang", "Kuo", ""], ["Wang", "Liyuan", ""], ["Zhou", "Fengwei", ""], ["Li", "Guilin", ""], ["Li", "Zhenguo", ""], ["Zhu", "Jun", ""]]}, {"id": "2101.01418", "submitter": "Petros Spachos", "authors": "Lili Zhu, Petros Spachos", "title": "Support Vector Machine and YOLO for a Mobile Food Grading System", "comments": null, "journal-ref": null, "doi": "10.1016/j.iot.2021.100359", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Food quality and safety are of great concern to society since it is an\nessential guarantee not only for human health but also for social development,\nand stability. Ensuring food quality and safety is a complex process. All food\nprocessing stages should be considered, from cultivating, harvesting and\nstorage to preparation and consumption. Grading is one of the essential\nprocesses to control food quality. This paper proposed a mobile visual-based\nsystem to evaluate food grading. Specifically, the proposed system acquires\nimages of bananas when they are on moving conveyors. A two-layer image\nprocessing system based on machine learning is used to grade bananas, and these\ntwo layers are allocated on edge devices and cloud servers, respectively.\nSupport Vector Machine (SVM) is the first layer to classify bananas based on an\nextracted feature vector composed of color and texture features. Then, the a\nYou Only Look Once (YOLO) v3 model further locating the peel's defected area\nand determining if the inputs belong to the mid-ripened or well-ripened class.\nAccording to experimental results, the first layer's performance achieved an\naccuracy of 98.5% while the accuracy of the second layer is 85.7%, and the\noverall accuracy is 96.4%.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 09:01:06 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhu", "Lili", ""], ["Spachos", "Petros", ""]]}, {"id": "2101.01444", "submitter": "Henry Krumb", "authors": "Henry Krumb and Dhritimaan Das and Romol Chadda and Anirban\n  Mukhopadhyay", "title": "CycleGAN for Interpretable Online EMT Compensation", "comments": "Conditionally accepted for publication in IJCARS & presentation at\n  IPCAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: Electromagnetic Tracking (EMT) can partially replace X-ray guidance\nin minimally invasive procedures, reducing radiation in the OR. However, in\nthis hybrid setting, EMT is disturbed by metallic distortion caused by the\nX-ray device. We plan to make hybrid navigation clinical reality to reduce\nradiation exposure for patients and surgeons, by compensating EMT error.\n  Methods: Our online compensation strategy exploits cycle-consistent\ngenerative adversarial neural networks (CycleGAN). 3D positions are translated\nfrom various bedside environments to their bench equivalents. Domain-translated\npoints are fine-tuned to reduce error in the bench domain. We evaluate our\ncompensation approach in a phantom experiment.\n  Results: Since the domain-translation approach maps distorted points to their\nlab equivalents, predictions are consistent among different C-arm environments.\nError is successfully reduced in all evaluation environments. Our qualitative\nphantom experiment demonstrates that our approach generalizes well to an unseen\nC-arm environment.\n  Conclusion: Adversarial, cycle-consistent training is an explicable,\nconsistent and thus interpretable approach for online error compensation.\nQualitative assessment of EMT error compensation gives a glimpse to the\npotential of our method for rotational error compensation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 10:34:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Krumb", "Henry", ""], ["Das", "Dhritimaan", ""], ["Chadda", "Romol", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "2101.01445", "submitter": "Christof Bertram", "authors": "Christof A. Bertram, Taryn A. Donovan, Marco Tecilla, Florian\n  Bartenschlager, Marco Fragoso, Frauke Wilm, Christian Marzahl, Katharina\n  Breininger, Andreas Maier, Robert Klopfleisch, Marc Aubreville", "title": "Dataset on Bi- and Multi-Nucleated Tumor Cells in Canine Cutaneous Mast\n  Cell Tumors", "comments": "Accepted at BVM workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor cells with two nuclei (binucleated cells, BiNC) or more nuclei\n(multinucleated cells, MuNC) indicate an increased amount of cellular genetic\nmaterial which is thought to facilitate oncogenesis, tumor progression and\ntreatment resistance. In canine cutaneous mast cell tumors (ccMCT),\nbinucleation and multinucleation are parameters used in cytologic and\nhistologic grading schemes (respectively) which correlate with poor patient\noutcome. For this study, we created the first open source data-set with 19,983\nannotations of BiNC and 1,416 annotations of MuNC in 32 histological whole\nslide images of ccMCT. Labels were created by a pathologist and an\nalgorithmic-aided labeling approach with expert review of each generated\ncandidate. A state-of-the-art deep learning-based model yielded an $F_1$ score\nof 0.675 for BiNC and 0.623 for MuNC on 11 test whole slide images. In regions\nof interest ($2.37 mm^2$) extracted from these test images, 6 pathologists had\nan object detection performance between 0.270 - 0.526 for BiNC and 0.316 -\n0.622 for MuNC, while our model archived an $F_1$ score of 0.667 for BiNC and\n0.685 for MuNC. This open dataset can facilitate development of automated image\nanalysis for this task and may thereby help to promote standardization of this\nfacet of histologic tumor prognostication.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 10:35:41 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Bertram", "Christof A.", ""], ["Donovan", "Taryn A.", ""], ["Tecilla", "Marco", ""], ["Bartenschlager", "Florian", ""], ["Fragoso", "Marco", ""], ["Wilm", "Frauke", ""], ["Marzahl", "Christian", ""], ["Breininger", "Katharina", ""], ["Maier", "Andreas", ""], ["Klopfleisch", "Robert", ""], ["Aubreville", "Marc", ""]]}, {"id": "2101.01447", "submitter": "Hung-Ting Su", "authors": "Hung-Ting Su, Chen-Hsi Chang, Po-Wei Shen, Yu-Siang Wang, Ya-Liang\n  Chang, Yu-Cheng Chang, Pu-Jen Cheng and Winston H. Hsu", "title": "End-to-End Video Question-Answer Generation with Generator-Pretester\n  Network", "comments": "Accepted to TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel task, Video Question-Answer Generation (VQAG), for\nchallenging Video Question Answering (Video QA) task in multimedia. Due to\nexpensive data annotation costs, many widely used, large-scale Video QA\ndatasets such as Video-QA, MSVD-QA and MSRVTT-QA are automatically annotated\nusing Caption Question Generation (CapQG) which inputs captions instead of the\nvideo itself. As captions neither fully represent a video, nor are they always\npractically available, it is crucial to generate question-answer pairs based on\na video via Video Question-Answer Generation (VQAG). Existing video-to-text\n(V2T) approaches, despite taking a video as the input, only generate a question\nalone. In this work, we propose a novel model Generator-Pretester Network that\nfocuses on two components: (1) The Joint Question-Answer Generator (JQAG) which\ngenerates a question with its corresponding answer to allow Video Question\n\"Answering\" training. (2) The Pretester (PT) verifies a generated question by\ntrying to answer it and checks the pretested answer with both the model's\nproposed answer and the ground truth answer. We evaluate our system with the\nonly two available large-scale human-annotated Video QA datasets and achieves\nstate-of-the-art question generation performances. Furthermore, using our\ngenerated QA pairs only on the Video QA task, we can surpass some supervised\nbaselines. We apply our generated questions to Video QA applications and\nsurpasses some supervised baselines using generated questions only. As a\npre-training strategy, we outperform both CapQG and transfer learning\napproaches when employing semi-supervised (20%) or fully supervised learning\nwith annotated data. These experimental results suggest the novel perspectives\nfor Video QA training.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 10:46:06 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Su", "Hung-Ting", ""], ["Chang", "Chen-Hsi", ""], ["Shen", "Po-Wei", ""], ["Wang", "Yu-Siang", ""], ["Chang", "Ya-Liang", ""], ["Chang", "Yu-Cheng", ""], ["Cheng", "Pu-Jen", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2101.01456", "submitter": "Bojia Zi", "authors": "Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, Yu-Gang Jiang", "title": "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, the abuse of a face swap technique called deepfake Deepfake\nhas raised enormous public concerns. So far, a large number of deepfake videos\n(known as \"deepfakes\") have been crafted and uploaded to the internet, calling\nfor effective countermeasures. One promising countermeasure against deepfakes\nis deepfake detection. Several deepfake datasets have been released to support\nthe training and testing of deepfake detectors, such as DeepfakeDetection and\nFaceForensics++. While this has greatly advanced deepfake detection, most of\nthe real videos in these datasets are filmed with a few volunteer actors in\nlimited scenes, and the fake videos are crafted by researchers using a few\npopular deepfake softwares. Detectors developed on these datasets may become\nless effective against real-world deepfakes on the internet. To better support\ndetection against real-world deepfakes, in this paper, we introduce a new\ndataset WildDeepfake, which consists of 7,314 face sequences extracted from 707\ndeepfake videos collected completely from the internet. WildDeepfake is a small\ndataset that can be used, in addition to existing datasets, to develop and test\nthe effectiveness of deepfake detectors against real-world deepfakes. We\nconduct a systematic evaluation of a set of baseline detection networks on both\nexisting and our WildDeepfake datasets, and show that WildDeepfake is indeed a\nmore challenging dataset, where the detection performance can decrease\ndrastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake\nDetection Networks (ADDNets) to leverage the attention masks on real/fake faces\nfor improved detection. We empirically verify the effectiveness of ADDNets on\nboth existing datasets and WildDeepfake. The dataset is available\nat:https://github.com/deepfakeinthewild/deepfake-in-the-wild.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 11:10:32 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zi", "Bojia", ""], ["Chang", "Minghao", ""], ["Chen", "Jingjing", ""], ["Ma", "Xingjun", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2101.01461", "submitter": "Jinlai Zhang", "authors": "Jinlai Zhang, Lyujie Chen, Bo Ouyang, Binbin Liu, Jihong Zhu, Yujing\n  Chen, Yanmei Meng, Danfeng Wu", "title": "PointCutMix: Regularization Strategy for Point Cloud Classification", "comments": "8 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As 3D point cloud analysis has received increasing attention, the\ninsufficient scale of point cloud datasets and the weak generalization ability\nof networks become prominent. In this paper, we propose a simple and effective\naugmentation method for the point cloud data, named PointCutMix, to alleviate\nthose problems. It finds the optimal assignment between two point clouds and\ngenerates new training data by replacing the points in one sample with their\noptimal assigned pairs. Two replacement strategies are proposed to adapt to the\naccuracy or robustness requirement for different tasks, one of which is to\nrandomly select all replacing points while the other one is to select k nearest\nneighbors of a single random point. Both strategies consistently and\nsignificantly improve the performance of various models on point cloud\nclassification problems. By introducing the saliency maps to guide the\nselection of replacing points, the performance further improves. Moreover,\nPointCutMix is validated to enhance the model robustness against the point\nattack. It is worth noting that when using as a defense method, our method\noutperforms the state-of-the-art defense algorithms. The code is available\nat:https://github.com/cuge1995/PointCutMix\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 11:39:06 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 08:03:01 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Zhang", "Jinlai", ""], ["Chen", "Lyujie", ""], ["Ouyang", "Bo", ""], ["Liu", "Binbin", ""], ["Zhu", "Jihong", ""], ["Chen", "Yujing", ""], ["Meng", "Yanmei", ""], ["Wu", "Danfeng", ""]]}, {"id": "2101.01479", "submitter": "Qiaosi Yi", "authors": "Qiaosi Yi, Yunxing Liu, Aiwen Jiang, Juncheng Li, Kangfu Mei, and\n  Mingwen Wang", "title": "Scale-Aware Network with Regional and Semantic Attentions for Crowd\n  Counting under Cluttered Background", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowd counting is an important task that shown great application value in\npublic safety-related fields, which has attracted increasing attention in\nrecent years. In the current research, the accuracy of counting numbers and\ncrowd density estimation are the main concerns. Although the emergence of deep\nlearning has greatly promoted the development of this field, crowd counting\nunder cluttered background is still a serious challenge. In order to solve this\nproblem, we propose a ScaleAware Crowd Counting Network (SACCN) with regional\nand semantic attentions. The proposed SACCN distinguishes crowd and background\nby applying regional and semantic self-attention mechanisms on the shallow\nlayers and deep layers, respectively. Moreover, the asymmetric multi-scale\nmodule (AMM) is proposed to deal with the problem of scale diversity, and\nregional attention based dense connections and skip connections are designed to\nalleviate the variations on crowd scales. Extensive experimental results on\nmultiple public benchmarks demonstrate that our proposed SACCN achieves\nsatisfied superior performances and outperform most state-of-the-art methods.\nAll codes and pretrained models will be released soon.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 12:20:10 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 11:55:12 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Yi", "Qiaosi", ""], ["Liu", "Yunxing", ""], ["Jiang", "Aiwen", ""], ["Li", "Juncheng", ""], ["Mei", "Kangfu", ""], ["Wang", "Mingwen", ""]]}, {"id": "2101.01480", "submitter": "Yann Lifchitz", "authors": "Yann Lifchitz, Yannis Avrithis, Sylvaine Picard", "title": "Local Propagation for Few-Shot Learning", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The challenge in few-shot learning is that available data is not enough to\ncapture the underlying distribution. To mitigate this, two emerging directions\nare (a) using local image representations, essentially multiplying the amount\nof data by a constant factor, and (b) using more unlabeled data, for instance\nby transductive inference, jointly on a number of queries. In this work, we\nbring these two ideas together, introducing \\emph{local propagation}. We treat\nlocal image features as independent examples, we build a graph on them and we\nuse it to propagate both the features themselves and the labels, known and\nunknown. Interestingly, since there is a number of features per image, even a\nsingle query gives rise to transductive inference. As a result, we provide a\nuniversally safe choice for few-shot inference under both non-transductive and\ntransductive settings, improving accuracy over corresponding methods. This is\nin contrast to existing solutions, where one needs to choose the method\ndepending on the quantity of available data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 12:26:23 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Lifchitz", "Yann", ""], ["Avrithis", "Yannis", ""], ["Picard", "Sylvaine", ""]]}, {"id": "2101.01513", "submitter": "Jingkun Chen", "authors": "Jingkun Chen, Wenqi Li, Hongwei Li, Jianguo Zhang", "title": "Deep Class-Specific Affinity-Guided Convolutional Network for Multimodal\n  Unpaired Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-modal medical image segmentation plays an essential role in clinical\ndiagnosis. It remains challenging as the input modalities are often not\nwell-aligned spatially. Existing learning-based methods mainly consider sharing\ntrainable layers across modalities and minimizing visual feature discrepancies.\nWhile the problem is often formulated as joint supervised feature learning,\nmultiple-scale features and class-specific representation have not yet been\nexplored. In this paper, we propose an affinity-guided fully convolutional\nnetwork for multimodal image segmentation. To learn effective representations,\nwe design class-specific affinity matrices to encode the knowledge of\nhierarchical feature reasoning, together with the shared convolutional layers\nto ensure the cross-modality generalization. Our affinity matrix does not\ndepend on spatial alignments of the visual features and thus allows us to train\nwith unpaired, multimodal inputs. We extensively evaluated our method on two\npublic multimodal benchmark datasets and outperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:56:51 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Chen", "Jingkun", ""], ["Li", "Wenqi", ""], ["Li", "Hongwei", ""], ["Zhang", "Jianguo", ""]]}, {"id": "2101.01533", "submitter": "John Tsotsos", "authors": "John K. Tsotsos, Omar Abid, Iuliia Kotseruba, Markus D. Solbach", "title": "On the Control of Attentional Processes in Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The study of attentional processing in vision has a long and deep history.\nRecently, several papers have presented insightful perspectives into how the\ncoordination of multiple attentional functions in the brain might occur. These\nbegin with experimental observations and the authors propose structures,\nprocesses, and computations that might explain those observations. Here, we\nconsider a perspective that past works have not, as a complementary approach to\nthe experimentally-grounded ones. We approach the same problem as past authors\nbut from the other end of the computational spectrum, from the problem nature,\nas Marr's Computational Level would prescribe. What problem must the brain\nsolve when orchestrating attentional processes in order to successfully\ncomplete one of the myriad possible visuospatial tasks at which we as humans\nexcel? The hope, of course, is for the approaches to eventually meet and thus\nform a complete theory, but this is likely not soon. We make the first steps\ntowards this by addressing the necessity of attentional control, examining the\nbreadth and computational difficulty of the visuospatial and attentional tasks\nseen in human behavior, and suggesting a sketch of how attentional control\nmight arise in the brain. The key conclusions of this paper are that an\nexecutive controller is necessary for human attentional function in vision, and\nthat there is a 'first principles' computational approach to its understanding\nthat is complementary to the previous approaches that focus on modelling or\nlearning from experimental observations directly.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:24:20 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Tsotsos", "John K.", ""], ["Abid", "Omar", ""], ["Kotseruba", "Iuliia", ""], ["Solbach", "Markus D.", ""]]}, {"id": "2101.01543", "submitter": "Rachel Sterneck", "authors": "Rachel Sterneck, Abhishek Moitra, Priyadarshini Panda", "title": "Noise Sensitivity-Based Energy Efficient and Robust Adversary Detection\n  in Neural Networks", "comments": "Accepted in IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems (TCAD), 2021", "journal-ref": null, "doi": "10.1109/TCAD.2021.3091436", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks have achieved remarkable performance in computer vision,\nhowever they are vulnerable to adversarial examples. Adversarial examples are\ninputs that have been carefully perturbed to fool classifier networks, while\nappearing unchanged to humans. Based on prior works on detecting adversaries,\nwe propose a structured methodology of augmenting a deep neural network (DNN)\nwith a detector subnetwork. We use $\\textit{Adversarial Noise Sensitivity}$\n(ANS), a novel metric for measuring the adversarial gradient contribution of\ndifferent intermediate layers of a network. Based on the ANS value, we append a\ndetector to the most sensitive layer. In prior works, more complex detectors\nwere added to a DNN, increasing the inference computational cost of the model.\nIn contrast, our structured and strategic addition of a detector to a DNN\nreduces the complexity of the model while making the overall network\nadversarially resilient. Through comprehensive white-box and black-box\nexperiments on MNIST, CIFAR-10, and CIFAR-100, we show that our method improves\nstate-of-the-art detector robustness against adversarial examples. Furthermore,\nwe validate the energy efficiency of our proposed adversarial detection\nmethodology through an extensive energy analysis on various hardware scalable\nCMOS accelerator platforms. We also demonstrate the effects of quantization on\nour detector-appended networks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:31:53 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 17:08:28 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Sterneck", "Rachel", ""], ["Moitra", "Abhishek", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2101.01546", "submitter": "Vikas Kumar Anand", "authors": "Vikas Kumar Anand, Sanjeev Grampurohit, Pranav Aurangabadkar, Avinash\n  Kori, Mahendra Khened, Raghavendra S Bhat, Ganapathy Krishnamurthi", "title": "Brain Tumor Segmentation and Survival Prediction using Automatic Hard\n  mining in 3D CNN Architecture", "comments": "11 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize 3-D fully convolutional neural networks (CNN) to segment gliomas\nand its constituents from multimodal Magnetic Resonance Images (MRI). The\narchitecture uses dense connectivity patterns to reduce the number of weights\nand residual connections and is initialized with weights obtained from training\nthis model with BraTS 2018 dataset. Hard mining is done during training to\ntrain for the difficult cases of segmentation tasks by increasing the dice\nsimilarity coefficient (DSC) threshold to choose the hard cases as epoch\nincreases. On the BraTS2020 validation data (n = 125), this architecture\nachieved a tumor core, whole tumor, and active tumor dice of 0.744, 0.876,\n0.714,respectively. On the test dataset, we get an increment in DSC of tumor\ncore and active tumor by approximately 7%. In terms of DSC, our network\nperformances on the BraTS 2020 test data are 0.775, 0.815, and 0.85 for\nenhancing tumor, tumor core, and whole tumor, respectively. Overall survival of\na subject is determined using conventional machine learning from rediomics\nfeatures obtained using a generated segmentation mask. Our approach has\nachieved 0.448 and 0.452 as the accuracy on the validation and test dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:34:16 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Anand", "Vikas Kumar", ""], ["Grampurohit", "Sanjeev", ""], ["Aurangabadkar", "Pranav", ""], ["Kori", "Avinash", ""], ["Khened", "Mahendra", ""], ["Bhat", "Raghavendra S", ""], ["Krishnamurthi", "Ganapathy", ""]]}, {"id": "2101.01570", "submitter": "Zaccharie Ramzi", "authors": "Zaccharie Ramzi, Jean-Luc Starck, Philippe Ciuciu", "title": "Density Compensated Unrolled Networks for Non-Cartesian MRI\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have recently been thoroughly investigated as a powerful\ntool for MRI reconstruction. There is a lack of research, however, regarding\ntheir use for a specific setting of MRI, namely non-Cartesian acquisitions. In\nthis work, we introduce a novel kind of deep neural networks to tackle this\nproblem, namely density compensated unrolled neural networks, which rely on\nDensity Compensation to correct the uneven weighting of the k-space. We assess\ntheir efficiency on the publicly available fastMRI dataset, and perform a small\nablation study. Our results show that the density-compensated unrolled neural\nnetworks outperform the different baselines, and that all parts of the design\nare needed. We also open source our code, in particular a Non-Uniform Fast\nFourier transform for TensorFlow.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:03:38 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 10:52:24 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ramzi", "Zaccharie", ""], ["Starck", "Jean-Luc", ""], ["Ciuciu", "Philippe", ""]]}, {"id": "2101.01597", "submitter": "Nantheera Anantrasirichai", "authors": "N. Anantrasirichai and David Bull", "title": "Contextual colorization and denoising for low-light ultra high\n  resolution sequences", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-light image sequences generally suffer from spatio-temporal incoherent\nnoise, flicker and blurring of moving objects. These artefacts significantly\nreduce visual quality and, in most cases, post-processing is needed in order to\ngenerate acceptable quality. Most state-of-the-art enhancement methods based on\nmachine learning require ground truth data but this is not usually available\nfor naturally captured low light sequences. We tackle these problems with an\nunpaired-learning method that offers simultaneous colorization and denoising.\nOur approach is an adaptation of the CycleGAN structure. To overcome the\nexcessive memory limitations associated with ultra high resolution content, we\npropose a multiscale patch-based framework, capturing both local and contextual\nfeatures. Additionally, an adaptive temporal smoothing technique is employed to\nremove flickering artefacts. Experimental results show that our method\noutperforms existing approaches in terms of subjective quality and that it is\nrobust to variations in brightness levels and noise.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:35:29 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Anantrasirichai", "N.", ""], ["Bull", "David", ""]]}, {"id": "2101.01600", "submitter": "Didac Sur\\'is Coll-Vinent", "authors": "D\\'idac Sur\\'is, Ruoshi Liu, Carl Vondrick", "title": "Learning the Predictability of the Future", "comments": "Website: https://hyperfuture.cs.columbia.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for learning from unlabeled video what is\npredictable in the future. Instead of committing up front to features to\npredict, our approach learns from data which features are predictable. Based on\nthe observation that hyperbolic geometry naturally and compactly encodes\nhierarchical structure, we propose a predictive model in hyperbolic space. When\nthe model is most confident, it will predict at a concrete level of the\nhierarchy, but when the model is not confident, it learns to automatically\nselect a higher level of abstraction. Experiments on two established datasets\nshow the key role of hierarchical representations for action prediction.\nAlthough our representation is trained with unlabeled video, visualizations\nshow that action hierarchies emerge in the representation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 18:58:36 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Sur\u00eds", "D\u00eddac", ""], ["Liu", "Ruoshi", ""], ["Vondrick", "Carl", ""]]}, {"id": "2101.01601", "submitter": "Yuhua Xu", "authors": "Bin Xu, Yuhua Xu, Xiaoli Yang, Wei Jia, Yulan Guo", "title": "Bilateral Grid Learning for Stereo Matching Networks", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time performance of stereo matching networks is important for many\napplications, such as automatic driving, robot navigation and augmented reality\n(AR). Although significant progress has been made in stereo matching networks\nin recent years, it is still challenging to balance real-time performance and\naccuracy. In this paper, we present a novel edge-preserving cost volume\nupsampling module based on the slicing operation in the learned bilateral grid.\nThe slicing layer is parameter-free, which allows us to obtain a high quality\ncost volume of high resolution from a low-resolution cost volume under the\nguide of the learned guidance map efficiently. The proposed cost volume\nupsampling module can be seamlessly embedded into many existing stereo matching\nnetworks, such as GCNet, PSMNet, and GANet. The resulting networks are\naccelerated several times while maintaining comparable accuracy. Furthermore,\nwe design a real-time network (named BGNet) based on this module, which\noutperforms existing published real-time deep stereo matching networks, as well\nas some complex networks on the KITTI stereo datasets. The code is available at\nhttps://github.com/YuhuaXu/BGNet.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 09:08:01 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 02:43:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Xu", "Bin", ""], ["Xu", "Yuhua", ""], ["Yang", "Xiaoli", ""], ["Jia", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "2101.01602", "submitter": "Wentao Yuan", "authors": "Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, Steven Lovegrove", "title": "STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in\n  Motion with Neural Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present STaR, a novel method that performs Self-supervised Tracking and\nReconstruction of dynamic scenes with rigid motion from multi-view RGB videos\nwithout any manual annotation. Recent work has shown that neural networks are\nsurprisingly effective at the task of compressing many views of a scene into a\nlearned function which maps from a viewing ray to an observed radiance value\nvia volume rendering. Unfortunately, these methods lose all their predictive\npower once any object in the scene has moved. In this work, we explicitly model\nrigid motion of objects in the context of neural representations of radiance\nfields. We show that without any additional human specified supervision, we can\nreconstruct a dynamic scene with a single rigid object in motion by\nsimultaneously decomposing it into its two constituent parts and encoding each\nwith its own neural representation. We achieve this by jointly optimizing the\nparameters of two neural radiance fields and a set of rigid poses which align\nthe two fields at each frame. On both synthetic and real world datasets, we\ndemonstrate that our method can render photorealistic novel views, where\nnovelty is measured on both spatial and temporal axes. Our factored\nrepresentation furthermore enables animation of unseen object motion.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 23:45:28 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Yuan", "Wentao", ""], ["Lv", "Zhaoyang", ""], ["Schmidt", "Tanner", ""], ["Lovegrove", "Steven", ""]]}, {"id": "2101.01611", "submitter": "Mengmi Zhang", "authors": "Mengmi Zhang, Will Xiao, Olivia Rose, Katarina Bendtz, Margaret\n  Livingstone, Carlos Ponce, Gabriel Kreiman", "title": "Look Twice: A Computational Model of Return Fixations across Tasks and\n  Species", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Saccadic eye movements allow animals to bring different parts of an image\ninto high-resolution. During free viewing, inhibition of return incentivizes\nexploration by discouraging previously visited locations. Despite this\ninhibition, here we show that subjects make frequent return fixations. We\nsystematically studied a total of 44,328 return fixations out of 217,440\nfixations across different tasks, in monkeys and humans, and in static images\nor egocentric videos. The ubiquitous return fixations were consistent across\nsubjects, tended to occur within short offsets, and were characterized by\nlonger duration than non-return fixations. The locations of return fixations\ncorresponded to image areas of higher saliency and higher similarity to the\nsought target during visual search tasks. We propose a biologically-inspired\ncomputational model that capitalizes on a deep convolutional neural network for\nobject recognition to predict a sequence of fixations. Given an input image,\nthe model computes four maps that constrain the location of the next saccade: a\nsaliency map, a target similarity map, a saccade size map, and a memory map.\nThe model exhibits frequent return fixations and approximates the properties of\nreturn fixations across tasks and species. The model provides initial steps\ntowards capturing the trade-off between exploitation of informative image\nlocations combined with exploration of novel image locations during scene\nviewing.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:53:39 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zhang", "Mengmi", ""], ["Xiao", "Will", ""], ["Rose", "Olivia", ""], ["Bendtz", "Katarina", ""], ["Livingstone", "Margaret", ""], ["Ponce", "Carlos", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "2101.01619", "submitter": "Yuxin Hou", "authors": "Yuxin Hou, Arno Solin, Juho Kannala", "title": "Novel View Synthesis via Depth-guided Skip Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a principled approach for synthesizing new views of a scene\ngiven a single source image. Previous methods for novel view synthesis can be\ndivided into image-based rendering methods (e.g. flow prediction) or pixel\ngeneration methods. Flow predictions enable the target view to re-use pixels\ndirectly, but can easily lead to distorted results. Directly regressing pixels\ncan produce structurally consistent results but generally suffer from the lack\nof low-level details. In this paper, we utilize an encoder-decoder architecture\nto regress pixels of a target view. In order to maintain details, we couple the\ndecoder aligned feature maps with skip connections, where the alignment is\nguided by predicted depth map of the target view. Our experimental results show\nthat our method does not suffer from distortions and successfully preserves\ntexture details with aligned skip connections.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 16:10:40 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Hou", "Yuxin", ""], ["Solin", "Arno", ""], ["Kannala", "Juho", ""]]}, {"id": "2101.01659", "submitter": "Stefan Stevsic", "authors": "Stefan Stevsic, Otmar Hilliges", "title": "Spatial Attention Improves Iterative 6D Object Pose Estimation", "comments": "Accepted to 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of estimating the 6D pose of an object from RGB images can be broken\ndown into two main steps: an initial pose estimation step, followed by a\nrefinement procedure to correctly register the object and its observation. In\nthis paper, we propose a new method for 6D pose estimation refinement from RGB\nimages. To achieve high accuracy of the final estimate, the observation and a\nrendered model need to be aligned. Our main insight is that after the initial\npose estimate, it is important to pay attention to distinct spatial features of\nthe object in order to improve the estimation accuracy during alignment.\nFurthermore, parts of the object that are occluded in the image should be given\nless weight during the alignment process. Most state-of-the-art refinement\napproaches do not allow for this fine-grained reasoning and can not fully\nleverage the structure of the problem. In contrast, we propose a novel neural\nnetwork architecture built around a spatial attention mechanism that identifies\nand leverages information about spatial details during pose refinement. We\nexperimentally show that this approach learns to attend to salient spatial\nfeatures and learns to ignore occluded parts of the object, leading to better\npose estimation across datasets. We conduct experiments on standard benchmark\ndatasets for 6D pose estimation (LineMOD and Occlusion LineMOD) and outperform\nprevious state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 17:18:52 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Stevsic", "Stefan", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2101.01665", "submitter": "Rana Mostafa AbdElMohsen AbdElMolla", "authors": "Reem Abdel-Salam, Rana Mostafa and Mayada Hadhood", "title": "Human Activity Recognition using Wearable Sensors: Review, Challenges,\n  Evaluation Benchmark", "comments": "Accepted at 2ND International Workshop on Deep Learning for Human\n  Activity Recognition, Held in conjunction with IJCAI-PRICAI 2020, January\n  2021, Japan and published at Springer Communications in Computer and\n  Information Science (CCIS) proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognizing human activity plays a significant role in the advancements of\nhuman-interaction applications in healthcare, personal fitness, and smart\ndevices. Many papers presented various techniques for human activity\nrepresentation that resulted in distinguishable progress. In this study, we\nconduct an extensive literature review on recent, top-performing techniques in\nhuman activity recognition based on wearable sensors. Due to the lack of\nstandardized evaluation and to assess and ensure a fair comparison between the\nstate-of-the-art techniques, we applied a standardized evaluation benchmark on\nthe state-of-the-art techniques using six publicly available data-sets:\nMHealth, USCHAD, UTD-MHAD, WISDM, WHARF, and OPPORTUNITY. Also, we propose an\nexperimental, improved approach that is a hybrid of enhanced handcrafted\nfeatures and a neural network architecture which outperformed top-performing\ntechniques with the same standardized evaluation benchmark applied concerning\nMHealth, USCHAD, UTD-MHAD data-sets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 17:33:04 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 09:19:21 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Abdel-Salam", "Reem", ""], ["Mostafa", "Rana", ""], ["Hadhood", "Mayada", ""]]}, {"id": "2101.01666", "submitter": "Muhammad Uzair Zahid", "authors": "Muhammad Uzair Zahid, Serkan Kiranyaz, Turker Ince, Ozer Can\n  Devecioglu, Muhammad E. H. Chowdhury, Amith Khandakar, Anas Tahir and Moncef\n  Gabbouj", "title": "Robust R-Peak Detection in Low-Quality Holter ECGs using 1D\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Noise and low quality of ECG signals acquired from Holter or wearable devices\ndeteriorate the accuracy and robustness of R-peak detection algorithms. This\npaper presents a generic and robust system for R-peak detection in Holter ECG\nsignals. While many proposed algorithms have successfully addressed the problem\nof ECG R-peak detection, there is still a notable gap in the performance of\nthese detectors on such low-quality ECG records. Therefore, in this study, a\nnovel implementation of the 1D Convolutional Neural Network (CNN) is used\nintegrated with a verification model to reduce the number of false alarms. This\nCNN architecture consists of an encoder block and a corresponding decoder block\nfollowed by a sample-wise classification layer to construct the 1D segmentation\nmap of R- peaks from the input ECG signal. Once the proposed model has been\ntrained, it can solely be used to detect R-peaks possibly in a single channel\nECG data stream quickly and accurately, or alternatively, such a solution can\nbe conveniently employed for real-time monitoring on a lightweight portable\ndevice. The model is tested on two open-access ECG databases: The China\nPhysiological Signal Challenge (2020) database (CPSC-DB) with more than one\nmillion beats, and the commonly used MIT-BIH Arrhythmia Database (MIT-DB).\nExperimental results demonstrate that the proposed systematic approach achieves\n99.30% F1-score, 99.69% recall, and 98.91% precision in CPSC-DB, which is the\nbest R-peak detection performance ever achieved. Compared to all competing\nmethods, the proposed approach can reduce the false-positives and\nfalse-negatives in Holter ECG signals by more than 54% and 82%, respectively.\nResults also demonstrate similar or better performance than most competing\nalgorithms on MIT-DB with 99.83% F1-score, 99.85% recall, and 99.82% precision.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 21:10:54 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:04:19 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zahid", "Muhammad Uzair", ""], ["Kiranyaz", "Serkan", ""], ["Ince", "Turker", ""], ["Devecioglu", "Ozer Can", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Khandakar", "Amith", ""], ["Tahir", "Anas", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2101.01677", "submitter": "Vitor Guizilini", "authors": "Rares Ambrus, Vitor Guizilini, Naveen Kuppuswamy, Andrew Beaulieu,\n  Adrien Gaidon, Alex Alspach", "title": "Monocular Depth Estimation for Soft Visuotactile Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluid-filled soft visuotactile sensors such as the Soft-bubbles alleviate key\nchallenges for robust manipulation, as they enable reliable grasps along with\nthe ability to obtain high-resolution sensory feedback on contact geometry and\nforces. Although they are simple in construction, their utility has been\nlimited due to size constraints introduced by enclosed custom IR/depth imaging\nsensors to directly measure surface deformations. Towards mitigating this\nlimitation, we investigate the application of state-of-the-art monocular depth\nestimation to infer dense internal (tactile) depth maps directly from the\ninternal single small IR imaging sensor. Through real-world experiments, we\nshow that deep networks typically used for long-range depth estimation (1-100m)\ncan be effectively trained for precise predictions at a much shorter range\n(1-100mm) inside a mostly textureless deformable fluid-filled sensor. We\npropose a simple supervised learning process to train an object-agnostic\nnetwork requiring less than 10 random poses in contact for less than 10 seconds\nfor a small set of diverse objects (mug, wine glass, box, and fingers in our\nexperiments). We show that our approach is sample-efficient, accurate, and\ngeneralizes across different objects and sensor configurations unseen at\ntraining time. Finally, we discuss the implications of our approach for the\ndesign of soft visuotactile sensors and grippers.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 17:51:11 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Ambrus", "Rares", ""], ["Guizilini", "Vitor", ""], ["Kuppuswamy", "Naveen", ""], ["Beaulieu", "Andrew", ""], ["Gaidon", "Adrien", ""], ["Alspach", "Alex", ""]]}, {"id": "2101.01710", "submitter": "Prune Truong", "authors": "Prune Truong and Martin Danelljan and Luc Van Gool and Radu Timofte", "title": "Learning Accurate Dense Correspondences and When to Trust Them", "comments": "CVPR 2021 ORAL Code: https://github.com/PruneTruong/PDCNet\n  Website:https://prunetruong.com/research/pdcnet", "journal-ref": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  2021, CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing dense correspondences between a pair of images is an important\nand general problem. However, dense flow estimation is often inaccurate in the\ncase of large displacements or homogeneous regions. For most applications and\ndown-stream tasks, such as pose estimation, image manipulation, or 3D\nreconstruction, it is crucial to know when and where to trust the estimated\nmatches.\n  In this work, we aim to estimate a dense flow field relating two images,\ncoupled with a robust pixel-wise confidence map indicating the reliability and\naccuracy of the prediction. We develop a flexible probabilistic approach that\njointly learns the flow prediction and its uncertainty. In particular, we\nparametrize the predictive distribution as a constrained mixture model,\nensuring better modelling of both accurate flow predictions and outliers.\nMoreover, we develop an architecture and training strategy tailored for robust\nand generalizable uncertainty prediction in the context of self-supervised\ntraining. Our approach obtains state-of-the-art results on multiple challenging\ngeometric matching and optical flow datasets. We further validate the\nusefulness of our probabilistic confidence estimation for the task of pose\nestimation. Code and models are available at\nhttps://github.com/PruneTruong/PDCNet.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:54:11 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 16:57:01 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Truong", "Prune", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2101.01713", "submitter": "Naoto Inoue", "authors": "Naoto Inoue, Toshihiko Yamasaki", "title": "Learning from Synthetic Shadows for Shadow Detection and Removal", "comments": "Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT), v2: fixed typos", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3047977", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadow removal is an essential task in computer vision and computer graphics.\nRecent shadow removal approaches all train convolutional neural networks (CNN)\non real paired shadow/shadow-free or shadow/shadow-free/mask image datasets.\nHowever, obtaining a large-scale, diverse, and accurate dataset has been a big\nchallenge, and it limits the performance of the learned models on shadow images\nwith unseen shapes/intensities. To overcome this challenge, we present\nSynShadow, a novel large-scale synthetic shadow/shadow-free/matte image\ntriplets dataset and a pipeline to synthesize it. We extend a\nphysically-grounded shadow illumination model and synthesize a shadow image\ngiven an arbitrary combination of a shadow-free image, a matte image, and\nshadow attenuation parameters. Owing to the diversity, quantity, and quality of\nSynShadow, we demonstrate that shadow removal models trained on SynShadow\nperform well in removing shadows with diverse shapes and intensities on some\nchallenging benchmarks. Furthermore, we show that merely fine-tuning from a\nSynShadow-pre-trained model improves existing shadow detection and removal\nmodels. Codes are publicly available at https://github.com/naoto0804/SynShadow.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:56:34 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 06:40:05 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Inoue", "Naoto", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2101.01715", "submitter": "Matthieu Paul", "authors": "Matthieu Paul, Martin Danelljan, Luc Van Gool, Radu Timofte", "title": "Local Memory Attention for Fast Video Semantic Segmentation", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural network module that transforms an existing\nsingle-frame semantic segmentation model into a video semantic segmentation\npipeline. In contrast to prior works, we strive towards a simple and general\nmodule that can be integrated into virtually any single-frame architecture. Our\napproach aggregates a rich representation of the semantic information in past\nframes into a memory module. Information stored in the memory is then accessed\nthrough an attention mechanism. This provides temporal appearance cues from\nprior frames, which are then fused with an encoding of the current frame\nthrough a second attention-based module. The segmentation decoder processes the\nfused representation to predict the final semantic segmentation. We integrate\nour approach into two popular semantic segmentation networks: ERFNet and\nPSPNet. We observe an improvement in segmentation performance on Cityscapes by\n1.7% and 2.1% in mIoU respectively, while increasing inference time of ERFNet\nby only 1.5ms.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:57:09 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Paul", "Matthieu", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2101.01761", "submitter": "Hieu Pham", "authors": "Hieu Pham, Quoc V. Le", "title": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks are often over-parameterized and hence benefit from\naggressive regularization. Conventional regularization methods, such as Dropout\nor weight decay, do not leverage the structures of the network's inputs and\nhidden states. As a result, these conventional methods are less effective than\nmethods that leverage the structures, such as SpatialDropout and DropBlock,\nwhich randomly drop the values at certain contiguous areas in the hidden states\nand setting them to zero. Although the locations of dropout areas random, the\npatterns of SpatialDropout and DropBlock are manually designed and fixed. Here\nwe propose to learn the dropout patterns. In our method, a controller learns to\ngenerate a dropout pattern at every channel and layer of a target network, such\nas a ConvNet or a Transformer. The target network is then trained with the\ndropout pattern, and its resulting validation performance is used as a signal\nfor the controller to learn from. We show that this method works well for both\nimage recognition on CIFAR-10 and ImageNet, as well as language modeling on\nPenn Treebank and WikiText-2. The learned dropout patterns also transfers to\ndifferent tasks and datasets, such as from language model on Penn Treebank to\nEngligh-French translation on WMT 2014. Our code will be available.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 19:54:22 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Pham", "Hieu", ""], ["Le", "Quoc V.", ""]]}, {"id": "2101.01781", "submitter": "Kuo Yang", "authors": "Kuo Yang, Emad A. Mohammed", "title": "A Review of Artificial Intelligence Technologies for Early Prediction of\n  Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Alzheimer's Disease (AD) is a severe brain disorder, destroying memories and\nbrain functions. AD causes chronically, progressively, and irreversibly\ncognitive declination and brain damages. The reliable and effective evaluation\nof early dementia has become essential research with medical imaging\ntechnologies and computer-aided algorithms. This trend has moved to modern\nArtificial Intelligence (AI) technologies motivated by deeplearning success in\nimage classification and natural language processing. The purpose of this\nreview is to provide an overview of the latest research involving deep-learning\nalgorithms in evaluating the process of dementia, diagnosing the early stage of\nAD, and discussing an outlook for this research. This review introduces various\napplications of modern AI algorithms in AD diagnosis, including Convolutional\nNeural Network (CNN), Recurrent Neural Network (RNN), Automatic Image\nSegmentation, Autoencoder, Graph CNN (GCN), Ensemble Learning, and Transfer\nLearning. The advantages and disadvantages of the proposed methods and their\nperformance are discussed. The conclusion section summarizes the primary\ncontributions and medical imaging preprocessing techniques applied in the\nreviewed research. Finally, we discuss the limitations and future outlooks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 01:05:34 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Yang", "Kuo", ""], ["Mohammed", "Emad A.", ""]]}, {"id": "2101.01841", "submitter": "Yingfu Xu", "authors": "Yingfu Xu, Guido C. H. E. de Croon", "title": "CNN-based Ego-Motion Estimation for Fast MAV Maneuvers", "comments": "10 pages, 10 figures, 7 tables. Accepted by ICRA 2021 (without the\n  Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of visual ego-motion estimation for Micro Air Vehicles (MAVs),\nfast maneuvers stay challenging mainly because of the big visual disparity and\nmotion blur. In the pursuit of higher robustness, we study convolutional neural\nnetworks (CNNs) that predict the relative pose between subsequent images from a\nfast-moving monocular camera facing a planar scene. Aided by the Inertial\nMeasurement Unit (IMU), we mainly focus on translational motion. The networks\nwe study have similar small model sizes (around 1.35MB) and high inference\nspeeds (around 10 milliseconds on a mobile GPU). Images for training and\ntesting have realistic motion blur. Departing from a network framework that\niteratively warps the first image to match the second with cascaded network\nblocks, we study different network architectures and training strategies.\nSimulated datasets and a self-collected MAV flight dataset are used for\nevaluation. The proposed setup shows better accuracy over existing networks and\ntraditional feature-point-based methods during fast maneuvers. Moreover,\nself-supervised learning outperforms supervised learning. Videos and\nopen-sourced code are available at https://github.com/tudelft/PoseNet_Planar\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 01:20:29 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 00:06:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Yingfu", ""], ["de Croon", "Guido C. H. E.", ""]]}, {"id": "2101.01843", "submitter": "Zhenyi Liu", "authors": "Zhenyi Liu, Joyce Farrell, Brian Wandell", "title": "ISETAuto: Detecting vehicles with depth and radiance information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous driving applications use two types of sensor systems to identify\nvehicles - depth sensing LiDAR and radiance sensing cameras. We compare the\nperformance (average precision) of a ResNet for vehicle detection in complex,\ndaytime, driving scenes when the input is a depth map (D = d(x,y)), a radiance\nimage (L = r(x,y)), or both [D,L]. (1) When the spatial sampling resolution of\nthe depth map and radiance image are equal to typical camera resolutions, a\nResNet detects vehicles at higher average precision from depth than radiance.\n(2) As the spatial sampling of the depth map declines to the range of current\nLiDAR devices, the ResNet average precision is higher for radiance than depth.\n(3) For a hybrid system that combines a depth map and radiance image, the\naverage precision is higher than using depth or radiance alone. We established\nthese observations in simulation and then confirmed them using realworld data.\nThe advantage of combining depth and radiance can be explained by noting that\nthe two type of information have complementary weaknesses. The radiance data\nare limited by dynamic range and motion blur. The LiDAR data have relatively\nlow spatial resolution. The ResNet combines the two data sources effectively to\nimprove overall vehicle detection.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 01:37:43 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 02:25:02 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Liu", "Zhenyi", ""], ["Farrell", "Joyce", ""], ["Wandell", "Brian", ""]]}, {"id": "2101.01844", "submitter": "Qiaojun Feng", "authors": "Qiaojun Feng, Nikolay Atanasov", "title": "Mesh Reconstruction from Aerial Images for Outdoor Terrain Mapping Using\n  Joint 2D-3D Learning", "comments": "7 pages, 7 figures. Accepted at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses outdoor terrain mapping using overhead images obtained\nfrom an unmanned aerial vehicle. Dense depth estimation from aerial images\nduring flight is challenging. While feature-based localization and mapping\ntechniques can deliver real-time odometry and sparse points reconstruction, a\ndense environment model is generally recovered offline with significant\ncomputation and storage. This paper develops a joint 2D-3D learning approach to\nreconstruct local meshes at each camera keyframe, which can be assembled into a\nglobal environment model. Each local mesh is initialized from sparse depth\nmeasurements. We associate image features with the mesh vertices through camera\nprojection and apply graph convolution to refine the mesh vertices based on\njoint 2-D reprojected depth and 3-D mesh supervision. Quantitative and\nqualitative evaluations using real aerial images show the potential of our\nmethod to support environmental monitoring and surveillance applications.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 02:09:03 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 20:45:33 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Feng", "Qiaojun", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "2101.01861", "submitter": "JIe Zhang", "authors": "Jie Zhang", "title": "TGCN: Time Domain Graph Convolutional Network for Multiple Objects\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple object tracking is to give each object an id in the video. The\ndifficulty is how to match the predicted objects and detected objects in same\nframes. Matching features include appearance features, location features, etc.\nThese features of the predicted object are basically based on some previous\nframes. However, few papers describe the relationship in the time domain\nbetween the previous frame features and the current frame features.In this\npaper, we proposed a time domain graph convolutional network for multiple\nobjects tracking.The model is mainly divided into two parts, we first use\nconvolutional neural network (CNN) to extract pedestrian appearance feature,\nwhich is a normal operation processing image in deep learning, then we use GCN\nto model some past frames' appearance feature to get the prediction appearance\nfeature of the current frame. Due to this extension, we can get the pose\nfeatures of the current frame according to the relationship between some frames\nin the past. Experimental evaluation shows that our extensions improve the MOTA\nby 1.3 on the MOT16, achieving overall competitive performance at high frame\nrates.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 04:11:25 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Zhang", "Jie", ""]]}, {"id": "2101.01872", "submitter": "Wenxue Cui", "authors": "Wenxue Cui, Shaohui Liu, Feng Jiang, Yongliang Liu, Debin Zhao", "title": "Multi-Stage Residual Hiding for Image-into-Audio Steganography", "comments": "ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The widespread application of audio communication technologies has speeded up\naudio data flowing across the Internet, which made it a popular carrier for\ncovert communication. In this paper, we present a cross-modal steganography\nmethod for hiding image content into audio carriers while preserving the\nperceptual fidelity of the cover audio. In our framework, two multi-stage\nnetworks are designed: the first network encodes the decreasing multilevel\nresidual errors inside different audio subsequences with the corresponding\nstage sub-networks, while the second network decodes the residual errors from\nthe modified carrier with the corresponding stage sub-networks to produce the\nfinal revealed results. The multi-stage design of proposed framework not only\nmake the controlling of payload capacity more flexible, but also make hiding\neasier because of the gradual sparse characteristic of residual errors.\nQualitative experiments suggest that modifications to the carrier are\nunnoticeable by human listeners and that the decoded images are highly\nintelligible.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:01:45 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Cui", "Wenxue", ""], ["Liu", "Shaohui", ""], ["Jiang", "Feng", ""], ["Liu", "Yongliang", ""], ["Zhao", "Debin", ""]]}, {"id": "2101.01874", "submitter": "Majid Harouni", "authors": "Mina Mohammadi Dashti, Majid Harouni", "title": "Smile and Laugh Expressions Detection Based on Local Minimum Key Points", "comments": "20 pages, in Farsi, 11 figures, 7 tables, 21 equations, journal, 2\n  authors", "journal-ref": null, "doi": "10.29252/jsdp.15.2.69", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, a smile and laugh facial expression is presented based on\ndimension reduction and description process of the key points. The paper has\ntwo main objectives; the first is to extract the local critical points in terms\nof their apparent features, and the second is to reduce the system's dependence\non training inputs. To achieve these objectives, three different scenarios on\nextracting the features are proposed. First of all, the discrete parts of a\nface are detected by local binary pattern method that is used to extract a set\nof global feature vectors for texture classification considering various\nregions of an input-image face. Then, in the first scenario and with respect to\nthe correlation changes of adjacent pixels on the texture of a mouth area, a\nset of local key points are extracted using the Harris corner detector. In the\nsecond scenario, the dimension reduction of the extracted points of first\nscenario provided by principal component analysis algorithm leading to\nreduction in computational costs and overall complexity without loss of\nperformance and flexibility, etc.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:13:11 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Dashti", "Mina Mohammadi", ""], ["Harouni", "Majid", ""]]}, {"id": "2101.01880", "submitter": "Sugam Garg", "authors": "Sugam Garg, Harichandana and Sumit Kumar", "title": "On-Device Document Classification using multimodal features", "comments": "8th ACM IKDD CODS and 26th COMAD 2-4 January 2021", "journal-ref": null, "doi": "10.1145/3430984.3431030", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  From small screenshots to large videos, documents take up a bulk of space in\na modern smartphone. Documents in a phone can accumulate from various sources,\nand with the high storage capacity of mobiles, hundreds of documents are\naccumulated in a short period. However, searching or managing documents remains\nan onerous task, since most search methods depend on meta-information or only\ntext in a document. In this paper, we showcase that a single modality is\ninsufficient for classification and present a novel pipeline to classify\ndocuments on-device, thus preventing any private user data transfer to server.\nFor this task, we integrate an open-source library for Optical Character\nRecognition (OCR) and our novel model architecture in the pipeline. We optimise\nthe model for size, a necessary metric for on-device inference. We benchmark\nour classification model with a standard multimodal dataset FOOD-101 and\nshowcase competitive results with the previous State of the Art with 30% model\ncompression.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:36:58 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Garg", "Sugam", ""], ["Harichandana", "", ""], ["Kumar", "Sumit", ""]]}, {"id": "2101.01881", "submitter": "Woojeong Jin", "authors": "Woojeong Jin, Maziar Sanjabi, Shaoliang Nie, Liang Tan, Xiang Ren,\n  Hamed Firooz", "title": "Modality-specific Distillation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large neural networks are impractical to deploy on mobile devices due to\ntheir heavy computational cost and slow inference. Knowledge distillation (KD)\nis a technique to reduce the model size while retaining performance by\ntransferring knowledge from a large \"teacher\" model to a smaller \"student\"\nmodel. However, KD on multimodal datasets such as vision-language datasets is\nrelatively unexplored and digesting such multimodal information is challenging\nsince different modalities present different types of information. In this\npaper, we propose modality-specific distillation (MSD) to effectively transfer\nknowledge from a teacher on multimodal datasets. Existing KD approaches can be\napplied to multimodal setup, but a student doesn't have access to\nmodality-specific predictions. Our idea aims at mimicking a teacher's\nmodality-specific predictions by introducing an auxiliary loss term for each\nmodality. Because each modality has different importance for predictions, we\nalso propose weighting approaches for the auxiliary losses; a meta-learning\napproach to learn the optimal weights on these loss terms. In our experiments,\nwe demonstrate the effectiveness of our MSD and the weighting scheme and show\nthat it achieves better performance than KD.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:45:07 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Jin", "Woojeong", ""], ["Sanjabi", "Maziar", ""], ["Nie", "Shaoliang", ""], ["Tan", "Liang", ""], ["Ren", "Xiang", ""], ["Firooz", "Hamed", ""]]}, {"id": "2101.01886", "submitter": "Wei Wang", "authors": "Wei Wang, Xiang-Gen Xia, Chuanjiang He, Zemin Ren, Jian Lu, Tianfu\n  Wang and Baiying Lei", "title": "A New Weighting Scheme for Fan-beam and Circle Cone-beam CT\n  Reconstructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first present an arc based algorithm for fan-beam computed\ntomography (CT) reconstruction via applying Katsevich's helical CT formula to\n2D fan-beam CT reconstruction. Then, we propose a new weighting function to\ndeal with the redundant projection data. By extending the weighted arc based\nfan-beam algorithm to circle cone-beam geometry, we also obtain a new\nFDK-similar algorithm for circle cone-beam CT reconstruction. Experiments show\nthat our methods can obtain higher PSNR and SSIM compared to the\nParker-weighted conventional fan-beam algorithm and the FDK algorithm for\nsuper-short-scan trajectories.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 06:06:04 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wang", "Wei", ""], ["Xia", "Xiang-Gen", ""], ["He", "Chuanjiang", ""], ["Ren", "Zemin", ""], ["Lu", "Jian", ""], ["Wang", "Tianfu", ""], ["Lei", "Baiying", ""]]}, {"id": "2101.01909", "submitter": "Yifan Xu", "authors": "Yifan Xu, Weijian Xu, David Cheung and Zhuowen Tu", "title": "Line Segment Detection Using Transformers without Edges", "comments": "Accepted to CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a joint end-to-end line segment detection algorithm\nusing Transformers that is post-processing and heuristics-guided intermediate\nprocessing (edge/junction/region detection) free. Our method, named LinE\nsegment TRansformers (LETR), takes advantages of having integrated tokenized\nqueries, a self-attention mechanism, and an encoding-decoding strategy within\nTransformers by skipping standard heuristic designs for the edge element\ndetection and perceptual grouping processes. We equip Transformers with a\nmulti-scale encoder/decoder strategy to perform fine-grained line segment\ndetection under a direct endpoint distance loss. This loss term is particularly\nsuitable for detecting geometric structures such as line segments that are not\nconveniently represented by the standard bounding box representations. The\nTransformers learn to gradually refine line segments through layers of\nself-attention. In our experiments, we show state-of-the-art results on\nWireframe and YorkUrban benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 08:00:18 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 17:34:55 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Xu", "Yifan", ""], ["Xu", "Weijian", ""], ["Cheung", "David", ""], ["Tu", "Zhuowen", ""]]}, {"id": "2101.01975", "submitter": "Suwei Yang", "authors": "Suwei Yang, Massimo Lupascu, Kuldeep S. Meel", "title": "Predicting Forest Fire Using Remote Sensing Data And Machine Learning", "comments": "8 pages, 3 figures, to be published in the Thirty-Fifth AAAI\n  Conference on Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, deforestation and climate change have caused\nincreasing number of forest fires. In Southeast Asia, Indonesia has been the\nmost affected country by tropical peatland forest fires. These fires have a\nsignificant impact on the climate resulting in extensive health, social and\neconomic issues. Existing forest fire prediction systems, such as the Canadian\nForest Fire Danger Rating System, are based on handcrafted features and require\ninstallation and maintenance of expensive instruments on the ground, which can\nbe a challenge for developing countries such as Indonesia. We propose a novel,\ncost-effective, machine-learning based approach that uses remote sensing data\nto predict forest fires in Indonesia. Our prediction model achieves more than\n0.81 area under the receiver operator characteristic (ROC) curve, performing\nsignificantly better than the baseline approach which never exceeds 0.70 area\nunder ROC curve on the same tasks. Our model's performance remained above 0.81\narea under ROC curve even when evaluated with reduced data. The results support\nour claim that machine-learning based approaches can lead to reliable and\ncost-effective forest fire prediction systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 11:22:55 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Yang", "Suwei", ""], ["Lupascu", "Massimo", ""], ["Meel", "Kuldeep S.", ""]]}, {"id": "2101.01976", "submitter": "Zhongheng Li", "authors": "Rong Wang, Wei Feng, Qianrong Zhang, Feiping Nie, Zhen Wang, and\n  Xuelong Li", "title": "Ensemble and Random Collaborative Representation-Based Anomaly Detector\n  for Hyperspectral Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, hyperspectral anomaly detection (HAD) has become an active\ntopic and plays a significant role in military and civilian fields. As a\nclassic HAD method, the collaboration representation-based detector (CRD) has\nattracted extensive attention and in-depth research. Despite the good\nperformance of CRD method, its computational cost is too high for the widely\ndemanded real-time applications. To alleviate this problem, a novel ensemble\nand random collaborative representation-based detector (ERCRD) is proposed for\nHAD. This approach comprises two main steps. Firstly, we propose a random\nbackground modeling to replace the sliding dual window strategy used in the\noriginal CRD method. Secondly, we can obtain multiple detection results through\nmultiple random background modeling, and these results are further refined to\nfinal detection result through ensemble learning. Experiments on four real\nhyperspectral datasets exhibit the accuracy and efficiency of this proposed\nERCRD method compared with ten state-of-the-art HAD methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 11:23:51 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wang", "Rong", ""], ["Feng", "Wei", ""], ["Zhang", "Qianrong", ""], ["Nie", "Feiping", ""], ["Wang", "Zhen", ""], ["Li", "Xuelong", ""]]}, {"id": "2101.01984", "submitter": "Fan Wang", "authors": "Fan Wang, Lei Luo, En Zhu, Siwei Wang, Jun Long", "title": "Multi-object Tracking with a Hierarchical Single-branch Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent Multiple Object Tracking (MOT) methods have gradually attempted to\nintegrate object detection and instance re-identification (Re-ID) into a united\nnetwork to form a one-stage solution. Typically, these methods use two\nseparated branches within a single network to accomplish detection and Re-ID\nrespectively without studying the inter-relationship between them, which\ninevitably impedes the tracking performance. In this paper, we propose an\nonline multi-object tracking framework based on a hierarchical single-branch\nnetwork to solve this problem. Specifically, the proposed single-branch network\nutilizes an improved Hierarchical Online In-stance Matching (iHOIM) loss to\nexplicitly model the inter-relationship between object detection and Re-ID. Our\nnovel iHOIM loss function unifies the objectives of the two sub-tasks and\nencourages better detection performance and feature learning even in extremely\ncrowded scenes. Moreover, we propose to introduce the object positions,\npredicted by a motion model, as region proposals for subsequent object\ndetection, where the intuition is that detection results and motion predictions\ncan complement each other in different scenarios. Experimental results on MOT16\nand MOT20 datasets show that we can achieve state-of-the-art tracking\nperformance, and the ablation study verifies the effectiveness of each proposed\ncomponent.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 12:14:58 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wang", "Fan", ""], ["Luo", "Lei", ""], ["Zhu", "En", ""], ["Wang", "Siwei", ""], ["Long", "Jun", ""]]}, {"id": "2101.02000", "submitter": "Jialiang Zhang", "authors": "Jialiang Zhang, Lixiang Lin, Jianke Zhu, Steven C.H. Hoi", "title": "Weakly-Supervised Multi-Face 3D Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D face reconstruction plays a very important role in many real-world\nmultimedia applications, including digital entertainment, social media,\naffection analysis, and person identification. The de-facto pipeline for\nestimating the parametric face model from an image requires to firstly detect\nthe facial regions with landmarks, and then crop each face to feed the deep\nlearning-based regressor. Comparing to the conventional methods performing\nforward inference for each detected instance independently, we suggest an\neffective end-to-end framework for multi-face 3D reconstruction, which is able\nto predict the model parameters of multiple instances simultaneously using\nsingle network inference. Our proposed approach not only greatly reduces the\ncomputational redundancy in feature extraction but also makes the deployment\nprocedure much easier using the single network model. More importantly, we\nemploy the same global camera model for the reconstructed faces in each image,\nwhich makes it possible to recover the relative head positions and orientations\nin the 3D scene. We have conducted extensive experiments to evaluate our\nproposed approach on the sparse and dense face alignment tasks. The\nexperimental results indicate that our proposed approach is very promising on\nface alignment tasks without fully-supervision and pre-processing like\ndetection and crop. Our implementation is publicly available at\n\\url{https://github.com/kalyo-zjl/WM3DR}.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 13:15:21 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Zhang", "Jialiang", ""], ["Lin", "Lixiang", ""], ["Zhu", "Jianke", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2101.02047", "submitter": "Mohammad Mahmudul Alam", "authors": "Mohammad Mahmudul Alam, Mohammad Tariqul Islam, S. M. Mahbubur Rahman", "title": "Unified Learning Approach for Egocentric Hand Gesture Recognition and\n  Fingertip Detection", "comments": "30 pages, 8 figures, Manuscript Accepted to the Pattern Recognition,\n  Elsevier Science Publishers, 2021", "journal-ref": null, "doi": "10.1016/j.patcog.2021.108200", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Head-mounted device-based human-computer interaction often requires\negocentric recognition of hand gestures and fingertips detection. In this\npaper, a unified approach of egocentric hand gesture recognition and fingertip\ndetection is introduced. The proposed algorithm uses a single convolutional\nneural network to predict the probabilities of finger class and positions of\nfingertips in one forward propagation. Instead of directly regressing the\npositions of fingertips from the fully connected layer, the ensemble of the\nposition of fingertips is regressed from the fully convolutional network.\nSubsequently, the ensemble average is taken to regress the final position of\nfingertips. Since the whole pipeline uses a single network, it is significantly\nfast in computation. Experimental results show that the proposed method\noutperforms the existing fingertip detection approaches including the Direct\nRegression and the Heatmap-based framework. The effectiveness of the proposed\nmethod is also shown in-the-wild scenario as well as in a use-case of virtual\nreality.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 14:05:13 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 23:53:55 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 16:27:55 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Alam", "Mohammad Mahmudul", ""], ["Islam", "Mohammad Tariqul", ""], ["Rahman", "S. M. Mahbubur", ""]]}, {"id": "2101.02069", "submitter": "Hailong Hu", "authors": "Hailong Hu, Jun Pang", "title": "Model Extraction and Defenses on Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model extraction attacks aim to duplicate a machine learning model through\nquery access to a target model. Early studies mainly focus on discriminative\nmodels. Despite the success, model extraction attacks against generative models\nare less well explored. In this paper, we systematically study the feasibility\nof model extraction attacks against generative adversarial networks (GANs).\nSpecifically, we first define accuracy and fidelity on model extraction attacks\nagainst GANs. Then we study model extraction attacks against GANs from the\nperspective of accuracy extraction and fidelity extraction, according to the\nadversary's goals and background knowledge. We further conduct a case study\nwhere an adversary can transfer knowledge of the extracted model which steals a\nstate-of-the-art GAN trained with more than 3 million images to new domains to\nbroaden the scope of applications of model extraction attacks. Finally, we\npropose effective defense techniques to safeguard GANs, considering a trade-off\nbetween the utility and security of GAN models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 14:36:21 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Hu", "Hailong", ""], ["Pang", "Jun", ""]]}, {"id": "2101.02073", "submitter": "Kartik Mittal", "authors": "Ankita Naik (1), Apurva Swarnakar (1), Kartik Mittal (1) ((1)\n  University of Massachusetts Amherst)", "title": "Shallow-UWnet : Compressed Model for Underwater Image Enhancement", "comments": "All the authors contributed equally and are listed by alphabetical\n  order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few decades, underwater image enhancement has attracted\nincreasing amount of research effort due to its significance in underwater\nrobotics and ocean engineering. Research has evolved from implementing\nphysics-based solutions to using very deep CNNs and GANs. However, these\nstate-of-art algorithms are computationally expensive and memory intensive.\nThis hinders their deployment on portable devices for underwater exploration\ntasks. These models are trained on either synthetic or limited real world\ndatasets making them less practical in real-world scenarios. In this paper we\npropose a shallow neural network architecture, \\textbf{Shallow-UWnet} which\nmaintains performance and has fewer parameters than the state-of-art models. We\nalso demonstrated the generalization of our model by benchmarking its\nperformance on combination of synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 14:49:29 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Naik", "Ankita", ""], ["Swarnakar", "Apurva", ""], ["Mittal", "Kartik", ""]]}, {"id": "2101.02099", "submitter": "Lucas Brynte", "authors": "Lucas Brynte, Viktor Larsson, Jos\\'e Pedro Iglesias, Carl Olsson,\n  Fredrik Kahl", "title": "On the Tightness of Semidefinite Relaxations for Rotation Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why is it that semidefinite relaxations have been so successful in numerous\napplications in computer vision and robotics for solving non-convex\noptimization problems involving rotations? In studying the empirical\nperformance, we note that there are hardly any failure cases reported in the\nliterature, motivating us to approach these problems from a theoretical\nperspective.\n  A general framework based on tools from algebraic geometry is introduced for\nanalyzing the power of semidefinite relaxations of problems with quadratic\nobjective functions and rotational constraints. Applications include\nregistration, hand-eye calibration, camera resectioning and rotation averaging.\nWe characterize the extreme points, and show that there are plenty of failure\ncases for which the relaxation is not tight, even in the case of a single\nrotation. We also show that for some problem classes, an appropriate rotation\nparametrization guarantees tight relaxations. Our theoretical findings are\naccompanied with numerical simulations, providing further evidence and\nunderstanding of the results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:42:02 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Brynte", "Lucas", ""], ["Larsson", "Viktor", ""], ["Iglesias", "Jos\u00e9 Pedro", ""], ["Olsson", "Carl", ""], ["Kahl", "Fredrik", ""]]}, {"id": "2101.02115", "submitter": "Ruben Ohana", "authors": "Alessandro Cappelli, Ruben Ohana, Julien Launay, Laurent Meunier,\n  Iacopo Poli, Florent Krzakala", "title": "Adversarial Robustness by Design through Analog Computing and Synthetic\n  Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new defense mechanism against adversarial attacks inspired by an\noptical co-processor, providing robustness without compromising natural\naccuracy in both white-box and black-box settings. This hardware co-processor\nperforms a nonlinear fixed random transformation, where the parameters are\nunknown and impossible to retrieve with sufficient precision for large enough\ndimensions. In the white-box setting, our defense works by obfuscating the\nparameters of the random projection. Unlike other defenses relying on\nobfuscated gradients, we find we are unable to build a reliable backward\ndifferentiable approximation for obfuscated parameters. Moreover, while our\nmodel reaches a good natural accuracy with a hybrid backpropagation - synthetic\ngradient method, the same approach is suboptimal if employed to generate\nadversarial examples. We find the combination of a random projection and\nbinarization in the optical system also improves robustness against various\ntypes of black-box attacks. Finally, our hybrid training method builds robust\nfeatures against transfer attacks. We demonstrate our approach on a VGG-like\narchitecture, placing the defense on top of the convolutional features, on\nCIFAR-10 and CIFAR-100. Code is available at\nhttps://github.com/lightonai/adversarial-robustness-by-design.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:15:29 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Cappelli", "Alessandro", ""], ["Ohana", "Ruben", ""], ["Launay", "Julien", ""], ["Meunier", "Laurent", ""], ["Poli", "Iacopo", ""], ["Krzakala", "Florent", ""]]}, {"id": "2101.02127", "submitter": "Shohrukh Bekmirzaev", "authors": "Shohrukh Bekmirzaev, Seoyoung Oh, Sangwook Yoo", "title": "RethNet: Object-by-Object Learning for Detecting Facial Skin Problems", "comments": "ICCV workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation is a hot topic in computer vision where the most\nchallenging tasks of object detection and recognition have been handling by the\nsuccess of semantic segmentation approaches. We propose a concept of\nobject-by-object learning technique to detect 11 types of facial skin lesions\nusing semantic segmentation methods. Detecting individual skin lesion in a\ndense group is a challenging task, because of ambiguities in the appearance of\nthe visual data. We observe that there exist co-occurrent visual relations\nbetween object classes (e.g., wrinkle and age spot, or papule and whitehead,\netc.). In fact, rich contextual information significantly helps to handle the\nissue. Therefore, we propose REthinker blocks that are composed of the locally\nconstructed convLSTM/Conv3D layers and SE module as a one-shot attention\nmechanism whose responsibility is to increase network's sensitivity in the\nlocal and global contextual representation that supports to capture ambiguously\nappeared objects and co-occurrence interactions between object classes.\nExperiments show that our proposed model reached MIoU of 79.46% on the test of\na prepared dataset, representing a 15.34% improvement over Deeplab v3+ (MIoU of\n64.12%).\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:41:03 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 19:08:17 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Bekmirzaev", "Shohrukh", ""], ["Oh", "Seoyoung", ""], ["Yoo", "Sangwook", ""]]}, {"id": "2101.02136", "submitter": "Vicky Kalogeiton", "authors": "Manuel J. Marin-Jimenez, Vicky Kalogeiton, Pablo Medina-Suarez, and\n  Andrew Zisserman", "title": "LAEO-Net++: revisiting people Looking At Each Other in videos", "comments": "16 pages, 16 Figures. arXiv admin note: substantial text overlap with\n  arXiv:1906.05261", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI), 2020", "doi": "10.1109/TPAMI.2020.3048482", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the 'mutual gaze' of people is essential for understanding and\ninterpreting the social interactions between them. To this end, this paper\naddresses the problem of detecting people Looking At Each Other (LAEO) in video\nsequences. For this purpose, we propose LAEO-Net++, a new deep CNN for\ndetermining LAEO in videos. In contrast to previous works, LAEO-Net++ takes\nspatio-temporal tracks as input and reasons about the whole track. It consists\nof three branches, one for each character's tracked head and one for their\nrelative position. Moreover, we introduce two new LAEO datasets: UCO-LAEO and\nAVA-LAEO. A thorough experimental evaluation demonstrates the ability of\nLAEO-Net++ to successfully determine if two people are LAEO and the temporal\nwindow where it happens. Our model achieves state-of-the-art results on the\nexisting TVHID-LAEO video dataset, significantly outperforming previous\napproaches. Finally, we apply LAEO-Net++ to a social network, where we\nautomatically infer the social relationship between pairs of people based on\nthe frequency and duration that they LAEO, and show that LAEO can be a useful\ntool for guided search of human interactions in videos. The code is available\nat https://github.com/AVAuco/laeonetplus.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:06:23 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Marin-Jimenez", "Manuel J.", ""], ["Kalogeiton", "Vicky", ""], ["Medina-Suarez", "Pablo", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2101.02141", "submitter": "Tasfia Shermin", "authors": "Tasfia Shermin, Shyh Wei Teng, Ferdous Sohel, Manzur Murshed, Guojun\n  Lu", "title": "An Integrated Attribute Guided Dense Attention Model for Fine-Grained\n  Generalized Zero-Shot Learning", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Embedding learning (EL) and feature synthesizing (FS) are two of the popular\ncategories of fine-grained GZSL methods. The global feature exploring EL or FS\nmethods do not explore fine distinction as they ignore local details. And, the\nlocal detail exploring EL or FS methods either neglect direct attribute\nguidance or global information. Consequently, neither method performs well. In\nthis paper, we propose to explore global and direct attribute-supervised local\nvisual features for both EL and FS categories in an integrated manner for\nfine-grained GZSL. The proposed integrated network has an EL sub-network and a\nFS sub-network. Consequently, the proposed integrated network can be tested in\ntwo ways. We propose a novel two-step dense attention mechanism to discover\nattribute-guided local visual features. We introduce new mutual learning\nbetween the sub-networks to exploit mutually beneficial information for\noptimization. Moreover, to reduce bias towards the source domain during\ntesting, we propose to compute source-target class similarity based on mutual\ninformation and transfer-learn the target classes. We demonstrate that our\nproposed method outperforms contemporary methods on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:38:46 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 03:24:02 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Shermin", "Tasfia", ""], ["Teng", "Shyh Wei", ""], ["Sohel", "Ferdous", ""], ["Murshed", "Manzur", ""], ["Lu", "Guojun", ""]]}, {"id": "2101.02143", "submitter": "Pichao Wang", "authors": "Xiangyu Li and Yonghong Hou and Pichao Wang and Zhimin Gao and\n  Mingliang Xu and Wanqing Li", "title": "Transformer Guided Geometry Model for Flow-Based Unsupervised Visual\n  Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing unsupervised visual odometry (VO) methods either match pairwise\nimages or integrate the temporal information using recurrent neural networks\nover a long sequence of images. They are either not accurate, time-consuming in\ntraining or error accumulative. In this paper, we propose a method consisting\nof two camera pose estimators that deal with the information from pairwise\nimages and a short sequence of images respectively. For image sequences, a\nTransformer-like structure is adopted to build a geometry model over a local\ntemporal window, referred to as Transformer-based Auxiliary Pose Estimator\n(TAPE). Meanwhile, a Flow-to-Flow Pose Estimator (F2FPE) is proposed to exploit\nthe relationship between pairwise images. The two estimators are constrained\nthrough a simple yet effective consistency loss in training. Empirical\nevaluation has shown that the proposed method outperforms the state-of-the-art\nunsupervised learning-based methods by a large margin and performs comparably\nto supervised and traditional ones on the KITTI and Malaga dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 19:39:26 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Li", "Xiangyu", ""], ["Hou", "Yonghong", ""], ["Wang", "Pichao", ""], ["Gao", "Zhimin", ""], ["Xu", "Mingliang", ""], ["Li", "Wanqing", ""]]}, {"id": "2101.02144", "submitter": "Joseph Chazalon", "authors": "Yizi Chen (1,2), Edwin Carlinet (1), Joseph Chazalon (1), Cl\\'ement\n  Mallet (2), Bertrand Dum\\'enieu (3), Julien Perret (2,3) ((1) EPITA Research\n  and Development Lab. (LRDE), EPITA, France, (2) Univ. Gustave Eiffel,\n  IGN-ENSG, LaSTIG, (3) LaD\\'eHiS, CRH, EHESS)", "title": "Combining Deep Learning and Mathematical Morphology for Historical Map\n  Segmentation", "comments": "Supplementary material (code, extra figures) available at\n  https://github.com/soduco/paper-dgmm2021/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The digitization of historical maps enables the study of ancient, fragile,\nunique, and hardly accessible information sources. Main map features can be\nretrieved and tracked through the time for subsequent thematic analysis. The\ngoal of this work is the vectorization step, i.e., the extraction of vector\nshapes of the objects of interest from raster images of maps. We are\nparticularly interested in closed shape detection such as buildings, building\nblocks, gardens, rivers, etc. in order to monitor their temporal evolution.\nHistorical map images present significant pattern recognition challenges. The\nextraction of closed shapes by using traditional Mathematical Morphology (MM)\nis highly challenging due to the overlapping of multiple map features and\ntexts. Moreover, state-of-the-art Convolutional Neural Networks (CNN) are\nperfectly designed for content image filtering but provide no guarantee about\nclosed shape detection. Also, the lack of textural and color information of\nhistorical maps makes it hard for CNN to detect shapes that are represented by\nonly their boundaries. Our contribution is a pipeline that combines the\nstrengths of CNN (efficient edge detection and filtering) and MM (guaranteed\nextraction of closed shapes) in order to achieve such a task. The evaluation of\nour approach on a public dataset shows its effectiveness for extracting the\nclosed boundaries of objects in historical maps.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:24:57 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Chen", "Yizi", ""], ["Carlinet", "Edwin", ""], ["Chazalon", "Joseph", ""], ["Mallet", "Cl\u00e9ment", ""], ["Dum\u00e9nieu", "Bertrand", ""], ["Perret", "Julien", ""]]}, {"id": "2101.02149", "submitter": "Linh Tran", "authors": "Linh Tran, Maja Pantic, Marc Peter Deisenroth", "title": "Cauchy-Schwarz Regularized Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent work in unsupervised learning has focused on efficient inference and\nlearning in latent variables models. Training these models by maximizing the\nevidence (marginal likelihood) is typically intractable. Thus, a common\napproximation is to maximize the Evidence Lower BOund (ELBO) instead.\nVariational autoencoders (VAE) are a powerful and widely-used class of\ngenerative models that optimize the ELBO efficiently for large datasets.\nHowever, the VAE's default Gaussian choice for the prior imposes a strong\nconstraint on its ability to represent the true posterior, thereby degrading\noverall performance. A Gaussian mixture model (GMM) would be a richer prior,\nbut cannot be handled efficiently within the VAE framework because of the\nintractability of the Kullback-Leibler divergence for GMMs. We deviate from the\ncommon VAE framework in favor of one with an analytical solution for Gaussian\nmixture prior. To perform efficient inference for GMM priors, we introduce a\nnew constrained objective based on the Cauchy-Schwarz divergence, which can be\ncomputed analytically for GMMs. This new objective allows us to incorporate\nricher, multi-modal priors into the autoencoding framework. We provide\nempirical studies on a range of datasets and show that our objective improves\nupon variational auto-encoding models in density estimation, unsupervised\nclustering, semi-supervised learning, and face analysis.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:36:26 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 18:47:39 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Tran", "Linh", ""], ["Pantic", "Maja", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "2101.02190", "submitter": "Abhisesh Silwal", "authors": "Abhisesh Silwal, Tanvir Parhar, Francisco Yandun and George Kantor", "title": "A Robust Illumination-Invariant Camera System for Agricultural\n  Applications", "comments": "8 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object detection and semantic segmentation are two of the most widely adopted\ndeep learning algorithms in agricultural applications. One of the major sources\nof variability in image quality acquired in the outdoors for such tasks is\nchanging lighting condition that can alter the appearance of the objects or the\ncontents of the entire image. While transfer learning and data augmentation to\nsome extent reduce the need for large amount of data to train deep neural\nnetworks, the large variety of cultivars and the lack of shared datasets in\nagriculture makes wide-scale field deployments difficult. In this paper, we\npresent a high throughput robust active lighting-based camera system that\ngenerates consistent images in all lighting conditions. We detail experiments\nthat show the consistency in images quality leading to relatively fewer images\nto train deep neural networks for the task of object detection. We further\npresent results from field experiment under extreme lighting conditions where\nimages without active lighting significantly lack to provide consistent\nresults. The experimental results show that on average, deep nets for object\ndetection trained on consistent data required nearly four times less data to\nachieve similar level of accuracy. This proposed work could potentially provide\npragmatic solutions to computer vision needs in agriculture.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:50:53 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Silwal", "Abhisesh", ""], ["Parhar", "Tanvir", ""], ["Yandun", "Francisco", ""], ["Kantor", "George", ""]]}, {"id": "2101.02194", "submitter": "Alan Wang", "authors": "Alan Q. Wang, Adrian V. Dalca, Mert R. Sabuncu", "title": "Regularization-Agnostic Compressed Sensing MRI Reconstruction with\n  Hypernetworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing under-sampled k-space measurements in Compressed Sensing MRI\n(CS-MRI) is classically solved with regularized least-squares. Recently, deep\nlearning has been used to amortize this optimization by training reconstruction\nnetworks on a dataset of under-sampled measurements. Here, a crucial design\nchoice is the regularization function(s) and corresponding weight(s). In this\npaper, we explore a novel strategy of using a hypernetwork to generate the\nparameters of a separate reconstruction network as a function of the\nregularization weight(s), resulting in a regularization-agnostic reconstruction\nmodel. At test time, for a given under-sampled image, our model can rapidly\ncompute reconstructions with different amounts of regularization. We analyze\nthe variability of these reconstructions, especially in situations when the\noverall quality is similar. Finally, we propose and empirically demonstrate an\nefficient and data-driven way of maximizing reconstruction performance given\nlimited hypernetwork capacity. Our code is publicly available at\nhttps://github.com/alanqrwang/RegAgnosticCSMRI.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:55:37 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wang", "Alan Q.", ""], ["Dalca", "Adrian V.", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2101.02196", "submitter": "Bin Zhao", "authors": "Bin Zhao, Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte", "title": "Generating Masks from Boxes by Mining Spatio-Temporal Consistencies in\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting objects in videos is a fundamental computer vision task. The\ncurrent deep learning based paradigm offers a powerful, but data-hungry\nsolution. However, current datasets are limited by the cost and human effort of\nannotating object masks in videos. This effectively limits the performance and\ngeneralization capabilities of existing video segmentation methods. To address\nthis issue, we explore weaker form of bounding box annotations.\n  We introduce a method for generating segmentation masks from per-frame\nbounding box annotations in videos. To this end, we propose a spatio-temporal\naggregation module that effectively mines consistencies in the object and\nbackground appearance across multiple frames. We use our resulting accurate\nmasks for weakly supervised training of video object segmentation (VOS)\nnetworks. We generate segmentation masks for large scale tracking datasets,\nusing only their bounding box annotations. The additional data provides\nsubstantially better generalization performance leading to state-of-the-art\nresults in both the VOS and more challenging tracking domain.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:56:24 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Zhao", "Bin", ""], ["Bhat", "Goutam", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2101.02232", "submitter": "Prateek Agrawal", "authors": "Prateek Agrawal and Pratik Prabhanjan Brahma", "title": "Single Shot Multitask Pedestrian Detection and Behavior Prediction", "comments": "6 pages, 3 figures, Neurips 2020 ML4AD workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detecting and predicting the behavior of pedestrians is extremely crucial for\nself-driving vehicles to plan and interact with them safely. Although there\nhave been several research works in this area, it is important to have fast and\nmemory efficient models such that it can operate in embedded hardware in these\nautonomous machines. In this work, we propose a novel architecture using\nspatial-temporal multi-tasking to do camera based pedestrian detection and\nintention prediction. Our approach significantly reduces the latency by being\nable to detect and predict all pedestrians' intention in a single shot manner\nwhile also being able to attain better accuracy by sharing features with\nrelevant object level information and interactions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 19:10:23 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Agrawal", "Prateek", ""], ["Brahma", "Pratik Prabhanjan", ""]]}, {"id": "2101.02268", "submitter": "Debesh Jha", "authors": "Debesh Jha, Anis Yazidi, Michael A. Riegler, Dag Johansen, H{\\aa}vard\n  D. Johansen, and P{\\aa}l Halvorsen", "title": "LightLayers: Parameter Efficient Dense and Convolutional Layers for\n  Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) have become the de-facto standard in computer\nvision, as well as in many other pattern recognition tasks. A key drawback of\nDNNs is that the training phase can be very computationally expensive.\nOrganizations or individuals that cannot afford purchasing state-of-the-art\nhardware or tapping into cloud-hosted infrastructures may face a long waiting\ntime before the training completes or might not be able to train a model at\nall. Investigating novel ways to reduce the training time could be a potential\nsolution to alleviate this drawback, and thus enabling more rapid development\nof new algorithms and models. In this paper, we propose LightLayers, a method\nfor reducing the number of trainable parameters in deep neural networks (DNN).\nThe proposed LightLayers consists of LightDense andLightConv2D layer that are\nas efficient as regular Conv2D and Dense layers, but uses less parameters. We\nresort to Matrix Factorization to reduce the complexity of the DNN models\nresulting into lightweight DNNmodels that require less computational power,\nwithout much loss in the accuracy. We have tested LightLayers on MNIST, Fashion\nMNIST, CI-FAR 10, and CIFAR 100 datasets. Promising results are obtained for\nMNIST, Fashion MNIST, CIFAR-10 datasets whereas CIFAR 100 shows acceptable\nperformance by using fewer parameters.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 21:18:36 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Jha", "Debesh", ""], ["Yazidi", "Anis", ""], ["Riegler", "Michael A.", ""], ["Johansen", "Dag", ""], ["Johansen", "H\u00e5vard D.", ""], ["Halvorsen", "P\u00e5l", ""]]}, {"id": "2101.02275", "submitter": "Sandipan Choudhuri", "authors": "Sandipan Choudhuri, Riti Paul, Arunabha Sen, Baoxin Li, Hemanth\n  Venkateswara", "title": "Partial Domain Adaptation Using Selective Representation Learning For\n  Class-Weight Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The generalization power of deep-learning models is dependent on\nrich-labelled data. This supervision using large-scaled annotated information\nis restrictive in most real-world scenarios where data collection and their\nannotation involve huge cost. Various domain adaptation techniques exist in\nliterature that bridge this distribution discrepancy. However, a majority of\nthese models require the label sets of both the domains to be identical. To\ntackle a more practical and challenging scenario, we formulate the problem\nstatement from a partial domain adaptation perspective, where the source label\nset is a super set of the target label set. Driven by the motivation that image\nstyles are private to each domain, in this work, we develop a method that\nidentifies outlier classes exclusively from image content information and train\na label classifier exclusively on class-content from source images.\nAdditionally, elimination of negative transfer of samples from classes private\nto the source domain is achieved by transforming the soft class-level weights\ninto two clusters, 0 (outlier source classes) and 1 (shared classes) by\nmaximizing the between-cluster variance between them.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 21:37:56 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Choudhuri", "Sandipan", ""], ["Paul", "Riti", ""], ["Sen", "Arunabha", ""], ["Li", "Baoxin", ""], ["Venkateswara", "Hemanth", ""]]}, {"id": "2101.02285", "submitter": "Kathleen M Lewis", "authors": "Kathleen M Lewis, Srivatsan Varadharajan, Ira Kemelmacher-Shlizerman", "title": "TryOnGAN: Body-Aware Try-On via Layered Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pair of images-target person and garment on another person-we\nautomatically generate the target person in the given garment. Previous methods\nmostly focused on texture transfer via paired data training, while overlooking\nbody shape deformations, skin color, and seamless blending of garment with the\nperson. This work focuses on those three components, while also not requiring\npaired data training. We designed a pose conditioned StyleGAN2 architecture\nwith a clothing segmentation branch that is trained on images of people wearing\ngarments. Once trained, we propose a new layered latent space interpolation\nmethod that allows us to preserve and synthesize skin color and target body\nshape while transferring the garment from a different person. We demonstrate\nresults on high resolution 512x512 images, and extensively compare to state of\nthe art in try-on on both latent space generated and real images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 22:01:46 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 19:38:57 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Lewis", "Kathleen M", ""], ["Varadharajan", "Srivatsan", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "2101.02323", "submitter": "Vishwesh Nath", "authors": "Vishwesh Nath, Dong Yang, Bennett A. Landman, Daguang Xu, Holger R.\n  Roth", "title": "Diminishing Uncertainty within the Training Pool: Active Learning for\n  Medical Image Segmentation", "comments": "19 pages, 13 figures, Transactions of Medical Imaging", "journal-ref": "IEEE Transactions on Medical Imaging, 2020", "doi": "10.1109/TMI.2020.3048055", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is a unique abstraction of machine learning techniques where\nthe model/algorithm could guide users for annotation of a set of data points\nthat would be beneficial to the model, unlike passive machine learning. The\nprimary advantage being that active learning frameworks select data points that\ncan accelerate the learning process of a model and can reduce the amount of\ndata needed to achieve full accuracy as compared to a model trained on a\nrandomly acquired data set. Multiple frameworks for active learning combined\nwith deep learning have been proposed, and the majority of them are dedicated\nto classification tasks. Herein, we explore active learning for the task of\nsegmentation of medical imaging data sets. We investigate our proposed\nframework using two datasets: 1.) MRI scans of the hippocampus, 2.) CT scans of\npancreas and tumors. This work presents a query-by-committee approach for\nactive learning where a joint optimizer is used for the committee. At the same\ntime, we propose three new strategies for active learning: 1.) increasing\nfrequency of uncertain data to bias the training data set; 2.) Using mutual\ninformation among the input images as a regularizer for acquisition to ensure\ndiversity in the training dataset; 3.) adaptation of Dice log-likelihood for\nStein variational gradient descent (SVGD). The results indicate an improvement\nin terms of data reduction by achieving full accuracy while only using 22.69 %\nand 48.85 % of the available data for each dataset, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 01:55:48 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Nath", "Vishwesh", ""], ["Yang", "Dong", ""], ["Landman", "Bennett A.", ""], ["Xu", "Daguang", ""], ["Roth", "Holger R.", ""]]}, {"id": "2101.02337", "submitter": "Dave Epstein", "authors": "Dave Epstein, Jiajun Wu, Cordelia Schmid, Chen Sun", "title": "Learning Temporal Dynamics from Cycles in Narrated Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to model how the world changes as time elapses has proven a\nchallenging problem for the computer vision community. We propose a\nself-supervised solution to this problem using temporal cycle consistency\njointly in vision and language, training on narrated video. Our model learns\nmodality-agnostic functions to predict forward and backward in time, which must\nundo each other when composed. This constraint leads to the discovery of\nhigh-level transitions between moments in time, since such transitions are\neasily inverted and shared across modalities. We justify the design of our\nmodel with an ablation study on different configurations of the cycle\nconsistency problem. We then show qualitatively and quantitatively that our\napproach yields a meaningful, high-level model of the future and past. We apply\nthe learned dynamics model without further training to various tasks, such as\npredicting future action and temporally ordering sets of images.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:41:32 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Epstein", "Dave", ""], ["Wu", "Jiajun", ""], ["Schmid", "Cordelia", ""], ["Sun", "Chen", ""]]}, {"id": "2101.02353", "submitter": "Shuwei Shen", "authors": "Shuwei Shen, Mengjuan Xu, Fan Zhang, Pengfei Shao, Honghong Liu, Liang\n  Xu, Chi Zhang, Peng Liu, Zhihong Zhang, Peng Yao, Ronald X. Xu", "title": "Low-cost and high-performance data augmentation for deep-learning-based\n  skin lesion classification", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolutional neural networks (DCNNs) have achieved significant\naccuracy in skin lesion classification comparable or even superior to those of\ndermatologists, practical implementation of these models for skin cancer\nscreening in low resource settings is hindered by their limitations in\ncomputational cost and training dataset. To overcome these limitations, we\npropose a low-cost and high-performance data augmentation strategy that\nincludes two consecutive stages of augmentation search and network search. At\nthe augmentation search stage, the augmentation strategy is optimized in the\nsearch space of Low-Cost-Augment (LCA) under the criteria of balanced accuracy\n(BACC) with 5-fold cross validation. At the network search stage, the DCNNs are\nfine-tuned with the full training set in order to select the model with the\nhighest BACC. The efficiency of the proposed data augmentation strategy is\nverified on the HAM10000 dataset using EfficientNets as a baseline. With the\nproposed strategy, we are able to reduce the search space to 60 and achieve a\nhigh BACC of 0.853 by using a single DCNN model without external database,\nsuitable to be implemented in mobile devices for DCNN-based skin lesion\ndetection in low resource settings.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:43:15 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Shen", "Shuwei", ""], ["Xu", "Mengjuan", ""], ["Zhang", "Fan", ""], ["Shao", "Pengfei", ""], ["Liu", "Honghong", ""], ["Xu", "Liang", ""], ["Zhang", "Chi", ""], ["Liu", "Peng", ""], ["Zhang", "Zhihong", ""], ["Yao", "Peng", ""], ["Xu", "Ronald X.", ""]]}, {"id": "2101.02358", "submitter": "SungKwon An", "authors": "Sungkwon An, Jeonghoon Kim, Myungjoo Kang, Shahbaz Razaei and Xin Liu", "title": "OAAE: Adversarial Autoencoders for Novelty Detection in Multi-modal\n  Normality Case via Orthogonalized Latent Space", "comments": "Accepted to AAAI 2021 Workshop: Towards Robust, Secure and Efficient\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection using deep generative models such as autoencoder,\ngenerative adversarial networks mostly takes image reconstruction error as\nnovelty score function. However, image data, high dimensional as it is,\ncontains a lot of different features other than class information which makes\nmodels hard to detect novelty data. The problem gets harder in multi-modal\nnormality case. To address this challenge, we propose a new way of measuring\nnovelty score in multi-modal normality cases using orthogonalized latent space.\nSpecifically, we employ orthogonal low-rank embedding in the latent space to\ndisentangle the features in the latent space using mutual class information.\nWith the orthogonalized latent space, novelty score is defined by the change of\neach latent vector. Proposed algorithm was compared to state-of-the-art novelty\ndetection algorithms using GAN such as RaPP and OCGAN, and experimental results\nshow that ours outperforms those algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:59:47 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["An", "Sungkwon", ""], ["Kim", "Jeonghoon", ""], ["Kang", "Myungjoo", ""], ["Razaei", "Shahbaz", ""], ["Liu", "Xin", ""]]}, {"id": "2101.02374", "submitter": "Le Hui", "authors": "Le Hui, Mingmei Cheng, Jin Xie, Jian Yang", "title": "Efficient 3D Point Cloud Feature Learning for Large-Scale Place\n  Recognition", "comments": "Project page: https://github.com/fpthink/EPC-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud based retrieval for place recognition is still a challenging\nproblem due to drastic appearance and illumination changes of scenes in\nchanging environments. Existing deep learning based global descriptors for the\nretrieval task usually consume a large amount of computation resources (e.g.,\nmemory), which may not be suitable for the cases of limited hardware resources.\nIn this paper, we develop an efficient point cloud learning network (EPC-Net)\nto form a global descriptor for visual place recognition, which can obtain good\nperformance and reduce computation memory and inference time. First, we propose\na lightweight but effective neural network module, called ProxyConv, to\naggregate the local geometric features of point clouds. We leverage the spatial\nadjacent matrix and proxy points to simplify the original edge convolution for\nlower memory consumption. Then, we design a lightweight grouped VLAD network\n(G-VLAD) to form global descriptors for retrieval. Compared with the original\nVLAD network, we propose a grouped fully connected (GFC) layer to decompose the\nhigh-dimensional vectors into a group of low-dimensional vectors, which can\nreduce the number of parameters of the network and maintain the discrimination\nof the feature vector. Finally, to further reduce the inference time, we\ndevelop a simple version of EPC-Net, called EPC-Net-L, which consists of two\nProxyConv modules and one max pooling layer to aggregate global descriptors. By\ndistilling the knowledge from EPC-Net, EPC-Net-L can obtain discriminative\nglobal descriptors for retrieval. Extensive experiments on the Oxford dataset\nand three in-house datasets demonstrate that our proposed method can achieve\nstate-of-the-art performance with lower parameters, FLOPs, and runtime per\nframe.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 05:15:31 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Hui", "Le", ""], ["Cheng", "Mingmei", ""], ["Xie", "Jin", ""], ["Yang", "Jian", ""]]}, {"id": "2101.02375", "submitter": "Kang Li", "authors": "Kang Li, Shujun Wang, Lequan Yu, Pheng-Ann Heng", "title": "Dual-Teacher++: Exploiting Intra-domain and Inter-domain Knowledge with\n  Reliable Transfer for Cardiac Segmentation", "comments": "Accepted by TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Annotation scarcity is a long-standing problem in medical image analysis\narea. To efficiently leverage limited annotations, abundant unlabeled data are\nadditionally exploited in semi-supervised learning, while well-established\ncross-modality data are investigated in domain adaptation. In this paper, we\naim to explore the feasibility of concurrently leveraging both unlabeled data\nand cross-modality data for annotation-efficient cardiac segmentation. To this\nend, we propose a cutting-edge semi-supervised domain adaptation framework,\nnamely Dual-Teacher++. Besides directly learning from limited labeled target\ndomain data (e.g., CT) via a student model adopted by previous literature, we\ndesign novel dual teacher models, including an inter-domain teacher model to\nexplore cross-modality priors from source domain (e.g., MR) and an intra-domain\nteacher model to investigate the knowledge beneath unlabeled target domain. In\nthis way, the dual teacher models would transfer acquired inter- and\nintra-domain knowledge to the student model for further integration and\nexploitation. Moreover, to encourage reliable dual-domain knowledge transfer,\nwe enhance the inter-domain knowledge transfer on the samples with higher\nsimilarity to target domain after appearance alignment, and also strengthen\nintra-domain knowledge transfer of unlabeled target data with higher prediction\nconfidence. In this way, the student model can obtain reliable dual-domain\nknowledge and yield improved performance on target domain data. We extensively\nevaluated the feasibility of our method on the MM-WHS 2017 challenge dataset.\nThe experiments have demonstrated the superiority of our framework over other\nsemi-supervised learning and domain adaptation methods. Moreover, our\nperformance gains could be yielded in bidirections,i.e., adapting from MR to\nCT, and from CT to MR.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 05:17:38 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Li", "Kang", ""], ["Wang", "Shujun", ""], ["Yu", "Lequan", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2101.02380", "submitter": "Jason Stock", "authors": "Jason Stock, Tom Cavey", "title": "Who's a Good Boy? Reinforcing Canine Behavior in Real-Time using Machine\n  Learning", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we outline the development methodology for an automatic dog\ntreat dispenser which combines machine learning and embedded hardware to\nidentify and reward dog behaviors in real-time. Using machine learning\ntechniques for training an image classification model we identify three\nbehaviors of our canine companions: \"sit\", \"stand\", and \"lie down\" with up to\n92% test accuracy and 39 frames per second. We evaluate a variety of neural\nnetwork architectures, interpretability methods, model quantization and\noptimization techniques to develop a model specifically for an NVIDIA Jetson\nNano. We detect the aforementioned behaviors in real-time and reinforce\npositive actions by making inference on the Jetson Nano and transmitting a\nsignal to a servo motor to release rewards from a treat delivery apparatus.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 05:38:09 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 17:22:19 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Stock", "Jason", ""], ["Cavey", "Tom", ""]]}, {"id": "2101.02381", "submitter": "Yuan Xie", "authors": "Jingyu Gong, Jiachen Xu, Xin Tan, Jie Zhou, Yanyun Qu, Yuan Xie,\n  Lizhuang Ma", "title": "Boundary-Aware Geometric Encoding for Semantic Segmentation of Point\n  Clouds", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundary information plays a significant role in 2D image segmentation, while\nusually being ignored in 3D point cloud segmentation where ambiguous features\nmight be generated in feature extraction, leading to misclassification in the\ntransition area between two objects. In this paper, firstly, we propose a\nBoundary Prediction Module (BPM) to predict boundary points. Based on the\npredicted boundary, a boundary-aware Geometric Encoding Module (GEM) is\ndesigned to encode geometric information and aggregate features with\ndiscrimination in a neighborhood, so that the local features belonging to\ndifferent categories will not be polluted by each other. To provide extra\ngeometric information for boundary-aware GEM, we also propose a light-weight\nGeometric Convolution Operation (GCO), making the extracted features more\ndistinguishing. Built upon the boundary-aware GEM, we build our network and\ntest it on benchmarks like ScanNet v2, S3DIS. Results show our methods can\nsignificantly improve the baseline and achieve state-of-the-art performance.\nCode is available at https://github.com/JchenXu/BoundaryAwareGEM.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 05:38:19 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Gong", "Jingyu", ""], ["Xu", "Jiachen", ""], ["Tan", "Xin", ""], ["Zhou", "Jie", ""], ["Qu", "Yanyun", ""], ["Xie", "Yuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2101.02384", "submitter": "Hongming Luo", "authors": "Hongming Luo, Guangsen Liao, Xianxu Hou, Bozhi Liu, Fei Zhou and\n  Guoping Qiu", "title": "VHS to HDTV Video Translation using Multi-task Adversarial Learning", "comments": "MMM2020 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are large amount of valuable video archives in Video Home System (VHS)\nformat. However, due to the analog nature, their quality is often poor.\nCompared to High-definition television (HDTV), VHS video not only has a dull\ncolor appearance but also has a lower resolution and often appears blurry. In\nthis paper, we focus on the problem of translating VHS video to HDTV video and\nhave developed a solution based on a novel unsupervised multi-task adversarial\nlearning model. Inspired by the success of generative adversarial network (GAN)\nand CycleGAN, we employ cycle consistency loss, adversarial loss and perceptual\nloss together to learn a translation model. An important innovation of our work\nis the incorporation of super-resolution model and color transfer model that\ncan solve unsupervised multi-task problem. To our knowledge, this is the first\nwork that dedicated to the study of the relation between VHS and HDTV and the\nfirst computational solution to translate VHS to HDTV. We present experimental\nresults to demonstrate the effectiveness of our solution qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:02:58 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Luo", "Hongming", ""], ["Liao", "Guangsen", ""], ["Hou", "Xianxu", ""], ["Liu", "Bozhi", ""], ["Zhou", "Fei", ""], ["Qiu", "Guoping", ""]]}, {"id": "2101.02385", "submitter": "Sergio Casas", "authors": "Katie Luo, Sergio Casas, Renjie Liao, Xinchen Yan, Yuwen Xiong,\n  Wenyuan Zeng, Raquel Urtasun", "title": "Safety-Oriented Pedestrian Motion and Scene Occupancy Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the important problem in self-driving of\nforecasting multi-pedestrian motion and their shared scene occupancy map,\ncritical for safe navigation. Our contributions are two-fold. First, we\nadvocate for predicting both the individual motions as well as the scene\noccupancy map in order to effectively deal with missing detections caused by\npostprocessing, e.g., confidence thresholding and non-maximum suppression.\nSecond, we propose a Scene-Actor Graph Neural Network (SA-GNN) which preserves\nthe relative spatial information of pedestrians via 2D convolution, and\ncaptures the interactions among pedestrians within the same scene, including\nthose that have not been detected, via message passing. On two large-scale\nreal-world datasets, nuScenes and ATG4D, we showcase that our scene-occupancy\npredictions are more accurate and better calibrated than those from\nstate-of-the-art motion forecasting methods, while also matching their\nperformance in pedestrian motion forecasting metrics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:08:21 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Luo", "Katie", ""], ["Casas", "Sergio", ""], ["Liao", "Renjie", ""], ["Yan", "Xinchen", ""], ["Xiong", "Yuwen", ""], ["Zeng", "Wenyuan", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.02391", "submitter": "Yuhao Liu", "authors": "Yu Qiao, Yuhao Liu, Qiang Zhu, Xin Yang, Yuxin Wang, Qiang Zhang, and\n  Xiaopeng Wei", "title": "Multi-scale Information Assembly for Image Matting", "comments": "Pacific Graphics 2020", "journal-ref": null, "doi": "10.1111/cgf.14168", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matting is a long-standing problem in computer graphics and vision,\nmostly identified as the accurate estimation of the foreground in input images.\nWe argue that the foreground objects can be represented by different-level\ninformation, including the central bodies, large-grained boundaries, refined\ndetails, etc. Based on this observation, in this paper, we propose a\nmulti-scale information assembly framework (MSIA-matte) to pull out\nhigh-quality alpha mattes from single RGB images. Technically speaking, given\nan input image, we extract advanced semantics as our subject content and retain\ninitial CNN features to encode different-level foreground expression, then\ncombine them by our well-designed information assembly strategy. Extensive\nexperiments can prove the effectiveness of the proposed MSIA-matte, and we can\nachieve state-of-the-art performance compared to most existing matting\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:15:48 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 11:06:54 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Qiao", "Yu", ""], ["Liu", "Yuhao", ""], ["Zhu", "Qiang", ""], ["Yang", "Xin", ""], ["Wang", "Yuxin", ""], ["Zhang", "Qiang", ""], ["Wei", "Xiaopeng", ""]]}, {"id": "2101.02412", "submitter": "Sheng Yang", "authors": "Sheng Yang, Weisi Lin, Guosheng Lin, Qiuping Jiang, Zichuan Liu", "title": "Progressive Self-Guided Loss for Salient Object Detection", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple yet effective progressive self-guided loss function to\nfacilitate deep learning-based salient object detection (SOD) in images. The\nsaliency maps produced by the most relevant works still suffer from incomplete\npredictions due to the internal complexity of salient objects. Our proposed\nprogressive self-guided loss simulates a morphological closing operation on the\nmodel predictions for epoch-wisely creating progressive and auxiliary training\nsupervisions to step-wisely guide the training process. We demonstrate that\nthis new loss function can guide the SOD model to highlight more complete\nsalient objects step-by-step and meanwhile help to uncover the spatial\ndependencies of the salient object pixels in a region growing manner. Moreover,\na new feature aggregation module is proposed to capture multi-scale features\nand aggregate them adaptively by a branch-wise attention mechanism. Benefiting\nfrom this module, our SOD framework takes advantage of adaptively aggregated\nmulti-scale features to locate and detect salient objects effectively.\nExperimental results on several benchmark datasets show that our loss function\nnot only advances the performance of existing SOD models without architecture\nmodification but also helps our proposed framework to achieve state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:33:38 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Yang", "Sheng", ""], ["Lin", "Weisi", ""], ["Lin", "Guosheng", ""], ["Jiang", "Qiuping", ""], ["Liu", "Zichuan", ""]]}, {"id": "2101.02447", "submitter": "Engkarat Techapanurak", "authors": "Engkarat Techapanurak, Takayuki Okatani", "title": "Practical Evaluation of Out-of-Distribution Detection Methods for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We reconsider the evaluation of OOD detection methods for image recognition.\nAlthough many studies have been conducted so far to build better OOD detection\nmethods, most of them follow Hendrycks and Gimpel's work for the method of\nexperimental evaluation. While the unified evaluation method is necessary for a\nfair comparison, there is a question of if its choice of tasks and datasets\nreflect real-world applications and if the evaluation results can generalize to\nother OOD detection application scenarios. In this paper, we experimentally\nevaluate the performance of representative OOD detection methods for three\nscenarios, i.e., irrelevant input detection, novel class detection, and domain\nshift detection, on various datasets and classification tasks. The results show\nthat differences in scenarios and datasets alter the relative performance among\nthe methods. Our results can also be used as a guide for practitioners for the\nselection of OOD detection methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 09:28:45 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Techapanurak", "Engkarat", ""], ["Okatani", "Takayuki", ""]]}, {"id": "2101.02458", "submitter": "Aite Zhao", "authors": "Aite Zhao, Junyu Dong, Jianbo Li, Lin Qi, Huiyu Zhou", "title": "Associated Spatio-Temporal Capsule Network for Gait Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a challenging task to identify a person based on her/his gait patterns.\nState-of-the-art approaches rely on the analysis of temporal or spatial\ncharacteristics of gait, and gait recognition is usually performed on single\nmodality data (such as images, skeleton joint coordinates, or force signals).\nEvidence has shown that using multi-modality data is more conducive to gait\nresearch. Therefore, we here establish an automated learning system, with an\nassociated spatio-temporal capsule network (ASTCapsNet) trained on multi-sensor\ndatasets, to analyze multimodal information for gait recognition. Specifically,\nwe first design a low-level feature extractor and a high-level feature\nextractor for spatio-temporal feature extraction of gait with a novel recurrent\nmemory unit and a relationship layer. Subsequently, a Bayesian model is\nemployed for the decision-making of class labels. Extensive experiments on\nseveral public datasets (normal and abnormal gait) validate the effectiveness\nof the proposed ASTCapsNet, compared against several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 09:55:17 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhao", "Aite", ""], ["Dong", "Junyu", ""], ["Li", "Jianbo", ""], ["Qi", "Lin", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2101.02469", "submitter": "Aite Zhao", "authors": "Aite Zhao, Jianbo Li, Junyu Dong, Lin Qi, Qianni Zhang, Ning Li, Xin\n  Wang, Huiyu Zhou", "title": "Multimodal Gait Recognition for Neurodegenerative Diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, single modality based gait recognition has been extensively\nexplored in the analysis of medical images or other sensory data, and it is\nrecognised that each of the established approaches has different strengths and\nweaknesses. As an important motor symptom, gait disturbance is usually used for\ndiagnosis and evaluation of diseases; moreover, the use of multi-modality\nanalysis of the patient's walking pattern compensates for the one-sidedness of\nsingle modality gait recognition methods that only learn gait changes in a\nsingle measurement dimension. The fusion of multiple measurement resources has\ndemonstrated promising performance in the identification of gait patterns\nassociated with individual diseases. In this paper, as a useful tool, we\npropose a novel hybrid model to learn the gait differences between three\nneurodegenerative diseases, between patients with different severity levels of\nParkinson's disease and between healthy individuals and patients, by fusing and\naggregating data from multiple sensors. A spatial feature extractor (SFE) is\napplied to generating representative features of images or signals. In order to\ncapture temporal information from the two modality data, a new correlative\nmemory neural network (CorrMNN) architecture is designed for extracting\ntemporal features. Afterwards, we embed a multi-switch discriminator to\nassociate the observations with individual state estimations. Compared with\nseveral state-of-the-art techniques, our proposed framework shows more accurate\nclassification results.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:17:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhao", "Aite", ""], ["Li", "Jianbo", ""], ["Dong", "Junyu", ""], ["Qi", "Lin", ""], ["Zhang", "Qianni", ""], ["Li", "Ning", ""], ["Wang", "Xin", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2101.02471", "submitter": "Abdallah Benzine", "authors": "Abdallah Benzine, Florian Chabot, Bertrand Luvison, Quoc Cong Pham,\n  Cahterine Achrd", "title": "PandaNet : Anchor-Based Single-Shot Multi-Person 3D Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several deep learning models have been proposed for 3D human pose\nestimation. Nevertheless, most of these approaches only focus on the\nsingle-person case or estimate 3D pose of a few people at high resolution.\nFurthermore, many applications such as autonomous driving or crowd analysis\nrequire pose estimation of a large number of people possibly at low-resolution.\nIn this work, we present PandaNet (Pose estimAtioN and Dectection Anchor-based\nNetwork), a new single-shot, anchor-based and multi-person 3D pose estimation\napproach. The proposed model performs bounding box detection and, for each\ndetected person, 2D and 3D pose regression into a single forward pass. It does\nnot need any post-processing to regroup joints since the network predicts a\nfull 3D pose for each bounding box and allows the pose estimation of a possibly\nlarge number of people at low resolution. To manage people overlapping, we\nintroduce a Pose-Aware Anchor Selection strategy. Moreover, as imbalance exists\nbetween different people sizes in the image, and joints coordinates have\ndifferent uncertainties depending on these sizes, we propose a method to\nautomatically optimize weights associated to different people scales and joints\nfor efficient training. PandaNet surpasses previous single-shot methods on\nseveral challenging datasets: a multi-person urban virtual but very realistic\ndataset (JTA Dataset), and two real world 3D multi-person datasets (CMU\nPanoptic and MuPoTS-3D).\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:32:17 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Benzine", "Abdallah", ""], ["Chabot", "Florian", ""], ["Luvison", "Bertrand", ""], ["Pham", "Quoc Cong", ""], ["Achrd", "Cahterine", ""]]}, {"id": "2101.02477", "submitter": "Alon Shoshan", "authors": "Alon Shoshan, Nadav Bhonker, Igor Kviatkovsky, Gerard Medioni", "title": "GAN-Control: Explicitly Controllable GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a framework for training GANs with explicit control over generated\nimages. We are able to control the generated image by settings exact attributes\nsuch as age, pose, expression, etc. Most approaches for editing GAN-generated\nimages achieve partial control by leveraging the latent space disentanglement\nproperties, obtained implicitly after standard GAN training. Such methods are\nable to change the relative intensity of certain attributes, but not explicitly\nset their values. Recently proposed methods, designed for explicit control over\nhuman faces, harness morphable 3D face models to allow fine-grained control\ncapabilities in GANs. Unlike these methods, our control is not constrained to\nmorphable 3D face model parameters and is extendable beyond the domain of human\nfaces. Using contrastive learning, we obtain GANs with an explicitly\ndisentangled latent space. This disentanglement is utilized to train\ncontrol-encoders mapping human-interpretable inputs to suitable latent vectors,\nthus allowing explicit control. In the domain of human faces we demonstrate\ncontrol over identity, age, pose, expression, hair color and illumination. We\nalso demonstrate control capabilities of our framework in the domains of\npainted portraits and dog image generation. We demonstrate that our approach\nachieves state-of-the-art performance both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:54:17 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Shoshan", "Alon", ""], ["Bhonker", "Nadav", ""], ["Kviatkovsky", "Igor", ""], ["Medioni", "Gerard", ""]]}, {"id": "2101.02480", "submitter": "Tugdual Ceillier", "authors": "Alex Goupilleau, Tugdual Ceillier, Marie-Caroline Corbineau", "title": "Active learning for object detection in high-resolution satellite images", "comments": null, "journal-ref": "Conference on Artificial Intelligence for Defense, Dec 2020,\n  Rennes, France", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, the term active learning regroups techniques that aim at\nselecting the most useful data to label from a large pool of unlabelled\nexamples. While supervised deep learning techniques have shown to be\nincreasingly efficient on many applications, they require a huge number of\nlabelled examples to reach operational performances. Therefore, the labelling\neffort linked to the creation of the datasets required is also increasing. When\nworking on defense-related remote sensing applications, labelling can be\nchallenging due to the large areas covered and often requires military experts\nwho are rare and whose time is primarily dedicated to operational needs.\nLimiting the labelling effort is thus of utmost importance. This study aims at\nreviewing the most relevant active learning techniques to be used for object\ndetection on very high resolution imagery and shows an example of the value of\nsuch techniques on a relevant operational use case: aircraft detection.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:57:38 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Goupilleau", "Alex", ""], ["Ceillier", "Tugdual", ""], ["Corbineau", "Marie-Caroline", ""]]}, {"id": "2101.02483", "submitter": "Rulin Shao", "authors": "Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh", "title": "Robust Text CAPTCHAs Using Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CAPTCHA (Completely Automated Public Truing test to tell Computers and Humans\nApart) is a widely used technology to distinguish real users and automated\nusers such as bots. However, the advance of AI technologies weakens many\nCAPTCHA tests and can induce security concerns. In this paper, we propose a\nuser-friendly text-based CAPTCHA generation method named Robust Text CAPTCHA\n(RTC). At the first stage, the foregrounds and backgrounds are constructed with\nrandomly sampled font and background images, which are then synthesized into\nidentifiable pseudo adversarial CAPTCHAs. At the second stage, we design and\napply a highly transferable adversarial attack for text CAPTCHAs to better\nobstruct CAPTCHA solvers. Our experiments cover comprehensive models including\nshallow models such as KNN, SVM and random forest, various deep neural networks\nand OCR models. Experiments show that our CAPTCHAs have a failure rate lower\nthan one millionth in general and high usability. They are also robust against\nvarious defensive techniques that attackers may employ, including adversarial\ntraining, data pre-processing and manual tagging.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:03:07 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Shao", "Rulin", ""], ["Shi", "Zhouxing", ""], ["Yi", "Jinfeng", ""], ["Chen", "Pin-Yu", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2101.02486", "submitter": "Leonardo Maria Millefiori", "authors": "Samuele Capobianco, Leonardo M. Millefiori, Nicola Forti, Paolo Braca,\n  and Peter Willett", "title": "Deep Learning Methods for Vessel Trajectory Prediction based on\n  Recurrent Neural Networks", "comments": "Accepted for publications in IEEE Transactions on Aerospace and\n  Electronic Systems, 17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven methods open up unprecedented possibilities for maritime\nsurveillance using Automatic Identification System (AIS) data. In this work, we\nexplore deep learning strategies using historical AIS observations to address\nthe problem of predicting future vessel trajectories with a prediction horizon\nof several hours. We propose novel sequence-to-sequence vessel trajectory\nprediction models based on encoder-decoder recurrent neural networks (RNNs)\nthat are trained on historical trajectory data to predict future trajectory\nsamples given previous observations. The proposed architecture combines Long\nShort-Term Memory (LSTM) RNNs for sequence modeling to encode the observed data\nand generate future predictions with different intermediate aggregation layers\nto capture space-time dependencies in sequential data. Experimental results on\nvessel trajectories from an AIS dataset made freely available by the Danish\nMaritime Authority show the effectiveness of deep-learning methods for\ntrajectory prediction based on sequence-to-sequence neural networks, which\nachieve better performance than baseline approaches based on linear regression\nor on the Multi-Layer Perceptron (MLP) architecture. The comparative evaluation\nof results shows: i) the superiority of attention pooling over static pooling\nfor the specific application, and ii) the remarkable performance improvement\nthat can be obtained with labeled trajectories, i.e., when predictions are\nconditioned on a low-level context representation encoded from the sequence of\npast observations, as well as on additional inputs (e.g., port of departure or\narrival) about the vessel's high-level intention, which may be available from\nAIS.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:05:47 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 11:49:02 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Capobianco", "Samuele", ""], ["Millefiori", "Leonardo M.", ""], ["Forti", "Nicola", ""], ["Braca", "Paolo", ""], ["Willett", "Peter", ""]]}, {"id": "2101.02496", "submitter": "Manuel Lagunas", "authors": "Manuel Lagunas, Ana Serrano, Diego Gutierrez, Belen Masia", "title": "The joint role of geometry and illumination on material recognition", "comments": "15 pages, 16 figures, Accepted to the Journal of Vision, 2021", "journal-ref": "Journal of Vision February 2021, Vol.21, 2", "doi": "10.1167/jov.21.2.2", "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Observing and recognizing materials is a fundamental part of our daily life.\nUnder typical viewing conditions, we are capable of effortlessly identifying\nthe objects that surround us and recognizing the materials they are made of.\nNevertheless, understanding the underlying perceptual processes that take place\nto accurately discern the visual properties of an object is a long-standing\nproblem. In this work, we perform a comprehensive and systematic analysis of\nhow the interplay of geometry, illumination, and their spatial frequencies\naffects human performance on material recognition tasks. We carry out\nlarge-scale behavioral experiments where participants are asked to recognize\ndifferent reference materials among a pool of candidate samples. In the\ndifferent experiments, we carefully sample the information in the frequency\ndomain of the stimuli. From our analysis, we find significant first-order\ninteractions between the geometry and the illumination, of both the reference\nand the candidates. In addition, we observe that simple image statistics and\nhigher-order image histograms do not correlate with human performance.\nTherefore, we perform a high-level comparison of highly non-linear statistics\nby training a deep neural network on material recognition tasks. Our results\nshow that such models can accurately classify materials, which suggests that\nthey are capable of defining a meaningful representation of material appearance\nfrom labeled proximal image data. Last, we find preliminary evidence that these\nhighly non-linear models and humans may use similar high-level factors for\nmaterial recognition tasks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:29:52 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 12:35:25 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Lagunas", "Manuel", ""], ["Serrano", "Ana", ""], ["Gutierrez", "Diego", ""], ["Masia", "Belen", ""]]}, {"id": "2101.02500", "submitter": "Engkarat Techapanurak", "authors": "Engkarat Techapanurak, Anh-Chuong Dang, Takayuki Okatani", "title": "Bridging In- and Out-of-distribution Samples for Their Better\n  Discriminability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a method for OOD detection. Questioning the premise of\nprevious studies that ID and OOD samples are separated distinctly, we consider\nsamples lying in the intermediate of the two and use them for training a\nnetwork. We generate such samples using multiple image transformations that\ncorrupt inputs in various ways and with different severity levels. We estimate\nwhere the generated samples by a single image transformation lie between ID and\nOOD using a network trained on clean ID samples. To be specific, we make the\nnetwork classify the generated samples and calculate their mean classification\naccuracy, using which we create a soft target label for them. We train the same\nnetwork from scratch using the original ID samples and the generated samples\nwith the soft labels created for them. We detect OOD samples by thresholding\nthe entropy of the predicted softmax probability. The experimental results show\nthat our method outperforms the previous state-of-the-art in the standard\nbenchmark tests. We also analyze the effect of the number and particular\ncombinations of image corrupting transformations on the performance.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:34:18 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Techapanurak", "Engkarat", ""], ["Dang", "Anh-Chuong", ""], ["Okatani", "Takayuki", ""]]}, {"id": "2101.02509", "submitter": "Joosoon Lee", "authors": "Joosoon Lee, Seongju Lee, Seunghyeok Back, Sungho Shin, Kyoobin Lee", "title": "Object Detection for Understanding Assembly Instruction Using\n  Context-aware Data Augmentation and Cascade Mask R-CNN", "comments": "5 pages, 7 figures, Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding assembly instruction has the potential to enhance the robot s\ntask planning ability and enables advanced robotic applications. To recognize\nthe key components from the 2D assembly instruction image, We mainly focus on\nsegmenting the speech bubble area, which contains lots of information about\ninstructions. For this, We applied Cascade Mask R-CNN and developed a\ncontext-aware data augmentation scheme for speech bubble segmentation, which\nrandomly combines images cuts by considering the context of assembly\ninstructions. We showed that the proposed augmentation scheme achieves a better\nsegmentation performance compared to the existing augmentation algorithm by\nincreasing the diversity of trainable data while considering the distribution\nof components locations. Also, we showed that deep learning can be useful to\nunderstand assembly instruction by detecting the essential objects in the\nassembly instruction, such as tools and parts.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 12:10:27 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 02:38:51 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Lee", "Joosoon", ""], ["Lee", "Seongju", ""], ["Back", "Seunghyeok", ""], ["Shin", "Sungho", ""], ["Lee", "Kyoobin", ""]]}, {"id": "2101.02515", "submitter": "Song Yan", "authors": "Song Yan and Joni-Kristian K\\\"am\\\"ar\\\"ainen", "title": "Learning Anthropometry from Rendered Humans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate estimation of anthropometric body measurements from RGB images has\nmany potential applications in industrial design, online clothing, medical\ndiagnosis and ergonomics. Research on this topic is limited by the fact that\nthere exist only generated datasets which are based on fitting a 3D body mesh\nto 3D body scans in the commercial CAESAR dataset. For 2D only silhouettes are\ngenerated. To circumvent the data bottleneck, we introduce a new 3D scan\ndataset of 2,675 female and 1,474 male scans. We also introduce a small dataset\nof 200 RGB images and tape measured ground truth. With the help of the two new\ndatasets we propose a part-based shape model and a deep neural network for\nestimating anthropometric measurements from 2D images. All data will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 12:26:39 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Yan", "Song", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""]]}, {"id": "2101.02523", "submitter": "Mateusz Ochal", "authors": "Mateusz Ochal, Massimiliano Patacchiola, Amos Storkey, Jose Vazquez,\n  Sen Wang", "title": "Few-Shot Learning with Class Imbalance", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-Shot Learning (FSL) algorithms are commonly trained through Meta-Learning\n(ML), which exposes models to batches of tasks sampled from a meta-dataset to\nmimic tasks seen during evaluation. However, the standard training procedures\noverlook the real-world dynamics where classes commonly occur at different\nfrequencies. While it is generally understood that class imbalance harms the\nperformance of supervised methods, limited research examines the impact of\nimbalance on the FSL evaluation task. Our analysis compares 10 state-of-the-art\nmeta-learning and FSL methods on different imbalance distributions and\nrebalancing techniques. Our results reveal that 1) some FSL methods display a\nnatural disposition against imbalance while most other approaches produce a\nperformance drop by up to 17\\% compared to the balanced task without the\nappropriate mitigation; 2) contrary to popular belief, many meta-learning\nalgorithms will not automatically learn to balance from exposure to imbalanced\ntraining tasks; 3) classical rebalancing strategies, such as random\noversampling, can still be very effective, leading to state-of-the-art\nperformances and should not be overlooked; 4) FSL methods are more robust\nagainst meta-dataset imbalance than imbalance at the task-level with a similar\nimbalance ratio ($\\rho<20$), with the effect holding even in long-tail datasets\nunder a larger imbalance ($\\rho=65$).\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 12:54:32 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 12:57:38 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ochal", "Mateusz", ""], ["Patacchiola", "Massimiliano", ""], ["Storkey", "Amos", ""], ["Vazquez", "Jose", ""], ["Wang", "Sen", ""]]}, {"id": "2101.02530", "submitter": "Alexander Neergaard Olesen", "authors": "Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot and Helge B.\n  D. Sorensen", "title": "MSED: a multi-modal sleep event detection model for clinical sleep\n  analysis", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study objective: Clinical sleep analysis require manual analysis of sleep\npatterns for correct diagnosis of sleep disorders. Several studies show\nsignificant variability in scoring discrete sleep events. We wished to\ninvestigate, whether an automatic method could be used for detection of\narousals (Ar), leg movements (LM) and sleep disordered breathing (SDB) events,\nand if the joint detection of these events performed better than having three\nseparate models.\n  Methods: We designed a single deep neural network architecture to jointly\ndetect sleep events in a polysomnogram. We trained the model on 1653 recordings\nof individuals, and tested the optimized model on 1000 separate recordings. The\nperformance of the model was quantified by F1, precision, and recall scores,\nand by correlating index values to clinical values using Pearson's correlation\ncoefficient.\n  Results: F1 scores for the optimized model was 0.70, 0.63, and 0.62 for Ar,\nLM, and SDB, respectively. The performance was higher, when detecting events\njointly compared to corresponding single-event models. Index values computed\nfrom detected events correlated well with manual annotations ($r^2$ = 0.73,\n$r^2$ = 0.77, $r^2$ = 0.78, respectively).\n  Conclusion: Detecting arousals, leg movements and sleep disordered breathing\nevents jointly is possible, and the computed index values correlates well with\nhuman annotations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 13:08:44 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Olesen", "Alexander Neergaard", ""], ["Jennum", "Poul", ""], ["Mignot", "Emmanuel", ""], ["Sorensen", "Helge B. D.", ""]]}, {"id": "2101.02568", "submitter": "Jiawei Ren", "authors": "Jiawei Ren, Xiao Ma, Chen Xu, Haiyu Zhao, Shuai Yi", "title": "HAVANA: Hierarchical and Variation-Normalized Autoencoder for Person\n  Re-identification", "comments": "Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (Re-ID) is of great importance to the many video\nsurveillance systems. Learning discriminative features for Re-ID remains a\nchallenge due to the large variations in the image space, e.g., continuously\nchanging human poses, illuminations and point of views. In this paper, we\npropose HAVANA, a novel extensible, light-weight HierArchical and\nVAriation-Normalized Autoencoder that learns features robust to intra-class\nvariations. In contrast to existing generative approaches that prune the\nvariations with heavy extra supervised signals, HAVANA suppresses the\nintra-class variations with a Variation-Normalized Autoencoder trained with no\nadditional supervision. We also introduce a novel Jensen-Shannon triplet loss\nfor contrastive distribution learning in Re-ID. In addition, we present\nHierarchical Variation Distiller, a hierarchical VAE to factorize the latent\nrepresentation and explicitly model the variations. To the best of our\nknowledge, HAVANA is the first VAE-based framework for person ReID.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 12:03:19 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 06:05:03 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ren", "Jiawei", ""], ["Ma", "Xiao", ""], ["Xu", "Chen", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""]]}, {"id": "2101.02637", "submitter": "Domenick Poster", "authors": "Domenick Poster, Matthew Thielke, Robert Nguyen, Srinivasan Rajaraman,\n  Xing Di, Cedric Nimpa Fondje, Vishal M. Patel, Nathaniel J. Short, Benjamin\n  S. Riggan, Nasser M. Nasrabadi, Shuowen Hu", "title": "A Large-Scale, Time-Synchronized Visible and Thermal Face Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal face imagery, which captures the naturally emitted heat from the\nface, is limited in availability compared to face imagery in the visible\nspectrum. To help address this scarcity of thermal face imagery for research\nand algorithm development, we present the DEVCOM Army Research Laboratory\nVisible-Thermal Face Dataset (ARL-VTF). With over 500,000 images from 395\nsubjects, the ARL-VTF dataset represents, to the best of our knowledge, the\nlargest collection of paired visible and thermal face images to date. The data\nwas captured using a modern long wave infrared (LWIR) camera mounted alongside\na stereo setup of three visible spectrum cameras. Variability in expressions,\npose, and eyewear has been systematically recorded. The dataset has been\ncurated with extensive annotations, metadata, and standardized protocols for\nevaluation. Furthermore, this paper presents extensive benchmark results and\nanalysis on thermal face landmark detection and thermal-to-visible face\nverification by evaluating state-of-the-art models on the ARL-VTF dataset.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:17:12 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Poster", "Domenick", ""], ["Thielke", "Matthew", ""], ["Nguyen", "Robert", ""], ["Rajaraman", "Srinivasan", ""], ["Di", "Xing", ""], ["Fondje", "Cedric Nimpa", ""], ["Patel", "Vishal M.", ""], ["Short", "Nathaniel J.", ""], ["Riggan", "Benjamin S.", ""], ["Nasrabadi", "Nasser M.", ""], ["Hu", "Shuowen", ""]]}, {"id": "2101.02639", "submitter": "Jian Dai", "authors": "Jian Dai, Shuge Lei, Licong Dong, Xiaona Lin, Huabin Zhang, Desheng\n  Sun, Kehong Yuan", "title": "More Reliable AI Solution: Breast Ultrasound Diagnosis Using Multi-AI\n  Combination", "comments": "12 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Breast cancer screening is of great significance in contemporary\nwomen's health prevention. The existing machines embedded in the AI system do\nnot reach the accuracy that clinicians hope. How to make intelligent systems\nmore reliable is a common problem. Methods: 1) Ultrasound image\nsuper-resolution: the SRGAN super-resolution network reduces the unclearness of\nultrasound images caused by the device itself and improves the accuracy and\ngeneralization of the detection model. 2) In response to the needs of medical\nimages, we have improved the YOLOv4 and the CenterNet models. 3) Multi-AI\nmodel: based on the respective advantages of different AI models, we employ two\nAI models to determine clinical resuls cross validation. And we accept the same\nresults and refuses others. Results: 1) With the help of the super-resolution\nmodel, the YOLOv4 model and the CenterNet model both increased the mAP score by\n9.6% and 13.8%. 2) Two methods for transforming the target model into a\nclassification model are proposed. And the unified output is in a specified\nformat to facilitate the call of the molti-AI model. 3) In the classification\nevaluation experiment, concatenated by the YOLOv4 model (sensitivity 57.73%,\nspecificity 90.08%) and the CenterNet model (sensitivity 62.64%, specificity\n92.54%), the multi-AI model will refuse to make judgments on 23.55% of the\ninput data. Correspondingly, the performance has been greatly improved to\n95.91% for the sensitivity and 96.02% for the specificity. Conclusion: Our work\nmakes the AI model more reliable in medical image diagnosis. Significance: 1)\nThe proposed method makes the target detection model more suitable for\ndiagnosing breast ultrasound images. 2) It provides a new idea for artificial\nintelligence in medical diagnosis, which can more conveniently introduce target\ndetection models from other fields to serve medical lesion screening.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:19:00 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 22:11:13 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Dai", "Jian", ""], ["Lei", "Shuge", ""], ["Dong", "Licong", ""], ["Lin", "Xiaona", ""], ["Zhang", "Huabin", ""], ["Sun", "Desheng", ""], ["Yuan", "Kehong", ""]]}, {"id": "2101.02647", "submitter": "Juana Valeria Hurtado", "authors": "Juana Valeria Hurtado, Laura Londo\\~no, and Abhinav Valada", "title": "From Learning to Relearning: A Framework for Diminishing Bias in Social\n  Robot Navigation", "comments": null, "journal-ref": "Frontiers in Robotics and AI, 2021", "doi": "10.3389/frobt.2021.650325", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponentially increasing advances in robotics and machine learning are\nfacilitating the transition of robots from being confined to controlled\nindustrial spaces to performing novel everyday tasks in domestic and urban\nenvironments. In order to make the presence of robots safe as well as\ncomfortable for humans, and to facilitate their acceptance in public\nenvironments, they are often equipped with social abilities for navigation and\ninteraction. Socially compliant robot navigation is increasingly being learned\nfrom human observations or demonstrations. We argue that these techniques that\ntypically aim to mimic human behavior do not guarantee fair behavior. As a\nconsequence, social navigation models can replicate, promote, and amplify\nsocietal unfairness such as discrimination and segregation. In this work, we\ninvestigate a framework for diminishing bias in social robot navigation models\nso that robots are equipped with the capability to plan as well as adapt their\npaths based on both physical and social demands. Our proposed framework\nconsists of two components: \\textit{learning} which incorporates social context\ninto the learning process to account for safety and comfort, and\n\\textit{relearning} to detect and correct potentially harmful outcomes before\nthe onset. We provide both technological and societal analysis using three\ndiverse case studies in different social scenarios of interaction. Moreover, we\npresent ethical implications of deploying robots in social environments and\npropose potential solutions. Through this study, we highlight the importance\nand advocate for fairness in human-robot interactions in order to promote more\nequitable social relationships, roles, and dynamics and consequently positively\ninfluence our society.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:42:35 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 18:42:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hurtado", "Juana Valeria", ""], ["Londo\u00f1o", "Laura", ""], ["Valada", "Abhinav", ""]]}, {"id": "2101.02663", "submitter": "Manoj Vemparala", "authors": "Manoj-Rohit Vemparala, Nael Fasfous, Alexander Frickenstein, Mhd Ali\n  Moraly, Aquib Jamal, Lukas Frickenstein, Christian Unger, Naveen-Shankar\n  Nagaraja, Walter Stechele", "title": "L2PF -- Learning to Prune Faster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various applications in the field of autonomous driving are based on\nconvolutional neural networks (CNNs), especially for processing camera data.\nThe optimization of such CNNs is a major challenge in continuous development.\nNewly learned features must be brought into vehicles as quickly as possible,\nand as such, it is not feasible to spend redundant GPU hours during\ncompression. In this context, we present Learning to Prune Faster which details\na multi-task, try-and-learn method, discretely learning redundant filters of\nthe CNN and a continuous action of how long the layers have to be fine-tuned.\nThis allows us to significantly speed up the convergence process of learning\nhow to find an embedded-friendly filter-wise pruned CNN. For ResNet20, we have\nachieved a compression ratio of 3.84 x with minimal accuracy degradation.\nCompared to the state-of-the-art pruning method, we reduced the GPU hours by\n1.71 x.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:13:37 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Vemparala", "Manoj-Rohit", ""], ["Fasfous", "Nael", ""], ["Frickenstein", "Alexander", ""], ["Moraly", "Mhd Ali", ""], ["Jamal", "Aquib", ""], ["Frickenstein", "Lukas", ""], ["Unger", "Christian", ""], ["Nagaraja", "Naveen-Shankar", ""], ["Stechele", "Walter", ""]]}, {"id": "2101.02672", "submitter": "Prarthana Bhattacharyya", "authors": "Prarthana Bhattacharyya, Chengjie Huang and Krzysztof Czarnecki", "title": "SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing point-cloud based 3D object detectors use convolution-like operators\nto process information in a local neighbourhood with fixed-weight kernels and\naggregate global context hierarchically. However, non-local neural networks and\nself-attention for 2D vision have shown that explicitly modeling long-range\ninteractions can lead to more robust and competitive models. In this paper, we\npropose two variants of self-attention for contextual modeling in 3D object\ndetection by augmenting convolutional features with self-attention features. We\nfirst incorporate the pairwise self-attention mechanism into the current\nstate-of-the-art BEV, voxel and point-based detectors and show consistent\nimprovement over strong baseline models of up to 1.5 3D AP while simultaneously\nreducing their parameter footprint and computational cost by 15-80% and 30-50%,\nrespectively, on the KITTI validation set. We next propose a self-attention\nvariant that samples a subset of the most representative features by learning\ndeformations over randomly sampled locations. This not only allows us to scale\nexplicit global contextual modeling to larger point-clouds, but also leads to\nmore discriminative and informative feature descriptors. Our method can be\nflexibly applied to most state-of-the-art detectors with increased accuracy and\nparameter and compute efficiency. We show our proposed method improves 3D\nobject detection performance on KITTI, nuScenes and Waymo Open datasets. Code\nis available at https://github.com/AutoVision-cloud/SA-Det3D.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:30:32 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 16:53:42 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 17:35:41 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Bhattacharyya", "Prarthana", ""], ["Huang", "Chengjie", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "2101.02691", "submitter": "Zaiwei Zhang", "authors": "Zaiwei Zhang, Rohit Girdhar, Armand Joulin, Ishan Misra", "title": "Self-Supervised Pretraining of 3D Features on any Point-Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining on large labeled datasets is a prerequisite to achieve good\nperformance in many computer vision tasks like 2D object recognition, video\nclassification etc. However, pretraining is not widely used for 3D recognition\ntasks where state-of-the-art methods train models from scratch. A primary\nreason is the lack of large annotated datasets because 3D data is both\ndifficult to acquire and time consuming to label. We present a simple\nself-supervised pertaining method that can work with any 3D data - single or\nmultiview, indoor or outdoor, acquired by varied sensors, without 3D\nregistration. We pretrain standard point cloud and voxel based model\narchitectures, and show that joint pretraining further improves performance. We\nevaluate our models on 9 benchmarks for object detection, semantic\nsegmentation, and object classification, where they achieve state-of-the-art\nresults and can outperform supervised pretraining. We set a new\nstate-of-the-art for object detection on ScanNet (69.0% mAP) and SUNRGBD (63.5%\nmAP). Our pretrained models are label efficient and improve performance for\nclasses with few examples.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:55:21 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhang", "Zaiwei", ""], ["Girdhar", "Rohit", ""], ["Joulin", "Armand", ""], ["Misra", "Ishan", ""]]}, {"id": "2101.02692", "submitter": "Kaichun Mo", "authors": "Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav Gupta, Shubham\n  Tulsiani", "title": "Where2Act: From Pixels to Actions for Articulated 3D Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental goals of visual perception is to allow agents to\nmeaningfully interact with their environment. In this paper, we take a step\ntowards that long-term goal -- we extract highly localized actionable\ninformation related to elementary actions such as pushing or pulling for\narticulated objects with movable parts. For example, given a drawer, our\nnetwork predicts that applying a pulling force on the handle opens the drawer.\nWe propose, discuss, and evaluate novel network architectures that given image\nand depth data, predict the set of actions possible at each pixel, and the\nregions over articulated parts that are likely to move under the force. We\npropose a learning-from-interaction framework with an online data sampling\nstrategy that allows us to train the network in simulation (SAPIEN) and\ngeneralizes across categories. But more importantly, our learned models even\ntransfer to real-world data. Check the project website for the code and data\nrelease.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:56:38 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Mo", "Kaichun", ""], ["Guibas", "Leonidas", ""], ["Mukadam", "Mustafa", ""], ["Gupta", "Abhinav", ""], ["Tulsiani", "Shubham", ""]]}, {"id": "2101.02697", "submitter": "Amit Raj", "authors": "Amit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke\n  Saito, James Hays and Stephen Lombardi", "title": "PVA: Pixel-aligned Volumetric Avatars", "comments": "Project page located at https://volumetric-avatars.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquisition and rendering of photo-realistic human heads is a highly\nchallenging research problem of particular importance for virtual telepresence.\nCurrently, the highest quality is achieved by volumetric approaches trained in\na person specific manner on multi-view data. These models better represent fine\nstructure, such as hair, compared to simpler mesh-based models. Volumetric\nmodels typically employ a global code to represent facial expressions, such\nthat they can be driven by a small set of animation parameters. While such\narchitectures achieve impressive rendering quality, they can not easily be\nextended to the multi-identity setting. In this paper, we devise a novel\napproach for predicting volumetric avatars of the human head given just a small\nnumber of inputs. We enable generalization across identities by a novel\nparameterization that combines neural radiance fields with local, pixel-aligned\nfeatures extracted directly from the inputs, thus sidestepping the need for\nvery deep or complex networks. Our approach is trained in an end-to-end manner\nsolely based on a photometric re-rendering loss without requiring explicit 3D\nsupervision.We demonstrate that our approach outperforms the existing state of\nthe art in terms of quality and is able to generate faithful facial expressions\nin a multi-identity setting.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:58:46 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Raj", "Amit", ""], ["Zollhoefer", "Michael", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["Saito", "Shunsuke", ""], ["Hays", "James", ""], ["Lombardi", "Stephen", ""]]}, {"id": "2101.02702", "submitter": "Tim Meinhardt", "authors": "Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, Christoph\n  Feichtenhofer", "title": "TrackFormer: Multi-Object Tracking with Transformers", "comments": "Tech. report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The challenging task of multi-object tracking (MOT) requires simultaneous\nreasoning about track initialization, identity, and spatiotemporal\ntrajectories. We formulate this task as a frame-to-frame set prediction problem\nand introduce TrackFormer, an end-to-end MOT approach based on an\nencoder-decoder Transformer architecture. Our model achieves data association\nbetween frames via attention by evolving a set of track predictions through a\nvideo sequence. The Transformer decoder initializes new tracks from static\nobject queries and autoregressively follows existing tracks in space and time\nwith the new concept of identity preserving track queries. Both decoder query\ntypes benefit from self- and encoder-decoder attention on global frame-level\nfeatures, thereby omitting any additional graph optimization and matching or\nmodeling of motion and appearance. TrackFormer represents a new\ntracking-by-attention paradigm and yields state-of-the-art performance on the\ntask of multi-object tracking (MOT17) and segmentation (MOTS20). The code is\navailable at https://github.com/timmeinhardt/trackformer .\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:59:29 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 18:22:25 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Meinhardt", "Tim", ""], ["Kirillov", "Alexander", ""], ["Leal-Taixe", "Laura", ""], ["Feichtenhofer", "Christoph", ""]]}, {"id": "2101.02703", "submitter": "Anastasios Angelopoulos", "authors": "Stephen Bates and Anastasios Angelopoulos and Lihua Lei and Jitendra\n  Malik and Michael I. Jordan", "title": "Distribution-Free, Risk-Controlling Prediction Sets", "comments": "Project website available at\n  http://www.angelopoulos.ai/blog/posts/rcps/ and codebase available at\n  https://github.com/aangelopoulos/rcps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While improving prediction accuracy has been the focus of machine learning in\nrecent years, this alone does not suffice for reliable decision-making.\nDeploying learning systems in consequential settings also requires calibrating\nand communicating the uncertainty of predictions. To convey instance-wise\nuncertainty for prediction tasks, we show how to generate set-valued\npredictions from a black-box predictor that control the expected loss on future\ntest points at a user-specified level. Our approach provides explicit\nfinite-sample guarantees for any dataset by using a holdout set to calibrate\nthe size of the prediction sets. This framework enables simple,\ndistribution-free, rigorous error control for many tasks, and we demonstrate it\nin five large-scale machine learning problems: (1) classification problems\nwhere some mistakes are more costly than others; (2) multi-label\nclassification, where each observation has multiple associated labels; (3)\nclassification problems where the labels have a hierarchical structure; (4)\nimage segmentation, where we wish to predict a set of pixels containing an\nobject of interest; and (5) protein structure prediction. Lastly, we discuss\nextensions to uncertainty quantification for ranking, metric learning and\ndistributionally robust learning.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:59:33 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 03:48:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bates", "Stephen", ""], ["Angelopoulos", "Anastasios", ""], ["Lei", "Lihua", ""], ["Malik", "Jitendra", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2101.02722", "submitter": "Rico Jonschkowski", "authors": "Austin Stone, Oscar Ramirez, Kurt Konolige, Rico Jonschkowski", "title": "The Distracting Control Suite -- A Challenging Benchmark for\n  Reinforcement Learning from Pixels", "comments": "Code available at\n  https://github.com/google-research/google-research/tree/master/distracting_control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots have to face challenging perceptual settings, including changes in\nviewpoint, lighting, and background. Current simulated reinforcement learning\n(RL) benchmarks such as DM Control provide visual input without such\ncomplexity, which limits the transfer of well-performing methods to the real\nworld. In this paper, we extend DM Control with three kinds of visual\ndistractions (variations in background, color, and camera pose) to produce a\nnew challenging benchmark for vision-based control, and we analyze state of the\nart RL algorithms in these settings. Our experiments show that current RL\nmethods for vision-based control perform poorly under distractions, and that\ntheir performance decreases with increasing distraction complexity, showing\nthat new methods are needed to cope with the visual complexities of the real\nworld. We also find that combinations of multiple distraction types are more\ndifficult than a mere combination of their individual effects.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 19:03:34 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Stone", "Austin", ""], ["Ramirez", "Oscar", ""], ["Konolige", "Kurt", ""], ["Jonschkowski", "Rico", ""]]}, {"id": "2101.02737", "submitter": "Antonia Stern", "authors": "Antonia Stern, Lalith Sharan, Gabriele Romano, Sven Koehler, Matthias\n  Karck, Raffaele De Simone, Ivo Wolf, Sandy Engelhardt", "title": "Heatmap-based 2D Landmark Detection with a Varying Number of Landmarks", "comments": "accepted for BVM 2021, 6 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitral valve repair is a surgery to restore the function of the mitral valve.\nTo achieve this, a prosthetic ring is sewed onto the mitral annulus. Analyzing\nthe sutures, which are punctured through the annulus for ring implantation, can\nbe useful in surgical skill assessment, for quantitative surgery and for\npositioning a virtual prosthetic ring model in the scene via augmented reality.\nThis work presents a neural network approach which detects the sutures in\nendoscopic images of mitral valve repair and therefore solves a landmark\ndetection problem with varying amount of landmarks, as opposed to most other\nexisting deep learning-based landmark detection approaches. The neural network\nis trained separately on two data collections from different domains with the\nsame architecture and hyperparameter settings. The datasets consist of more\nthan 1,300 stereo frame pairs each, with a total over 60,000 annotated\nlandmarks. The proposed heatmap-based neural network achieves a mean positive\npredictive value (PPV) of 66.68$\\pm$4.67% and a mean true positive rate (TPR)\nof 24.45$\\pm$5.06% on the intraoperative test dataset and a mean PPV of\n81.50\\pm5.77\\% and a mean TPR of 61.60$\\pm$6.11% on a dataset recorded during\nsurgical simulation. The best detection results are achieved when the camera is\npositioned above the mitral valve with good illumination. A detection from a\nsideward view is also possible if the mitral valve is well perceptible.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 19:42:44 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Stern", "Antonia", ""], ["Sharan", "Lalith", ""], ["Romano", "Gabriele", ""], ["Koehler", "Sven", ""], ["Karck", "Matthias", ""], ["De Simone", "Raffaele", ""], ["Wolf", "Ivo", ""], ["Engelhardt", "Sandy", ""]]}, {"id": "2101.02746", "submitter": "Lu Mi", "authors": "Lu Mi, Hao Wang, Yaron Meirovitch, Richard Schalek, Srinivas C.\n  Turaga, Jeff W. Lichtman, Aravinthan D.T. Samuel, Nir Shavit", "title": "Learning Guided Electron Microscopy with Active Acquisition", "comments": "MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59722-1_8", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-beam scanning electron microscopes (SEM) are widely used to acquire\nmassive data sets for biomedical study, material analysis, and fabrication\ninspection. Datasets are typically acquired with uniform acquisition: applying\nthe electron beam with the same power and duration to all image pixels, even if\nthere is great variety in the pixels' importance for eventual use. Many SEMs\nare now able to move the beam to any pixel in the field of view without delay,\nenabling them, in principle, to invest their time budget more effectively with\nnon-uniform imaging.\n  In this paper, we show how to use deep learning to accelerate and optimize\nsingle-beam SEM acquisition of images. Our algorithm rapidly collects an\ninformation-lossy image (e.g. low resolution) and then applies a novel learning\nmethod to identify a small subset of pixels to be collected at higher\nresolution based on a trade-off between the saliency and spatial diversity. We\ndemonstrate the efficacy of this novel technique for active acquisition by\nspeeding up the task of collecting connectomic datasets for neurobiology by up\nto an order of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 20:03:16 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Mi", "Lu", ""], ["Wang", "Hao", ""], ["Meirovitch", "Yaron", ""], ["Schalek", "Richard", ""], ["Turaga", "Srinivas C.", ""], ["Lichtman", "Jeff W.", ""], ["Samuel", "Aravinthan D. T.", ""], ["Shavit", "Nir", ""]]}, {"id": "2101.02767", "submitter": "Joris Gu\\'erin", "authors": "Joris Guerin, Stephane Thiery, Eric Nyiri, Olivier Gibaru, Byron Boots", "title": "Combining pretrained CNN feature extractors to enhance clustering of\n  complex natural images", "comments": "21 pages, 16 figures, 10 tables, preprint of our paper published in\n  Neurocomputing", "journal-ref": "Guerin, J., Thiery, S., Nyiri, E., Gibaru, O., & Boots, B. (2021).\n  Combining pretrained CNN feature extractors to enhance clustering of complex\n  natural images. Neurocomputing, 423, 551-571", "doi": "10.1016/j.neucom.2020.10.068", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a common starting point for solving complex unsupervised image\nclassification tasks is to use generic features, extracted with deep\nConvolutional Neural Networks (CNN) pretrained on a large and versatile dataset\n(ImageNet). However, in most research, the CNN architecture for feature\nextraction is chosen arbitrarily, without justification. This paper aims at\nproviding insight on the use of pretrained CNN features for image clustering\n(IC). First, extensive experiments are conducted and show that, for a given\ndataset, the choice of the CNN architecture for feature extraction has a huge\nimpact on the final clustering. These experiments also demonstrate that proper\nextractor selection for a given IC task is difficult. To solve this issue, we\npropose to rephrase the IC problem as a multi-view clustering (MVC) problem\nthat considers features extracted from different architectures as different\n\"views\" of the same data. This approach is based on the assumption that\ninformation contained in the different CNN may be complementary, even when\npretrained on the same data. We then propose a multi-input neural network\narchitecture that is trained end-to-end to solve the MVC problem effectively.\nThis approach is tested on nine natural image datasets, and produces\nstate-of-the-art results for IC.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 21:23:04 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Guerin", "Joris", ""], ["Thiery", "Stephane", ""], ["Nyiri", "Eric", ""], ["Gibaru", "Olivier", ""], ["Boots", "Byron", ""]]}, {"id": "2101.02774", "submitter": "Becky Mashaido", "authors": "Becky Mashaido", "title": "Learning Grammar of Complex Activities via Deep Neural Networks", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the growing amount of publicly available video data on online\nstreaming services and an increased interest in applications that analyze\ncontinuous video streams such as autonomous driving, this technical report\nprovides a theoretical insight into deep neural networks for video learning,\nunder label constraints. I build upon previous work in video learning for\ncomputer vision, make observations on model performance and propose further\nmechanisms to help improve our observations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 21:48:58 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Mashaido", "Becky", ""]]}, {"id": "2101.02797", "submitter": "Nisreen Ali", "authors": "Nisreen AbdAllah and Serestina Viriri", "title": "Off-Line Arabic Handwritten Words Segmentation using Morphological\n  Operators", "comments": "16 pages,27 figures", "journal-ref": "Signal & Image Processing: An International Journal (SIPIJ)\n  Vol.11, No.6, December 2020", "doi": "10.5121/sipij.2020.11602", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main aim of this study is the assessment and discussion of a model for\nhand-written Arabic through segmentation. The framework is proposed based on\nthree steps: pre-processing, segmentation, and evaluation. In the\npre-processing step, morphological operators are applied for Connecting Gaps\n(CGs) in written words. Gaps happen when pen lifting-off during writing,\nscanning documents, or while converting images to binary type. In the\nsegmentation step, first removed the small diacritics then bounded a connected\ncomponent to segment offline words. Huge data was utilized in the proposed\nmodel for applying a variety of handwriting styles so that to be more\ncompatible with real-life applications. Consequently, on the automatic\nevaluation stage, selected randomly 1,131 images from the IESK-ArDB database,\nand then segmented into sub-words. After small gaps been connected, the model\nperformance evaluation had been reached 88% against the standard ground truth\nof the database. The proposed model achieved the highest accuracy when compared\nwith the related works.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 23:38:53 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["AbdAllah", "Nisreen", ""], ["Viriri", "Serestina", ""]]}, {"id": "2101.02802", "submitter": "Baran Ataman", "authors": "Baran Ataman, Mert Seker and David Mckee", "title": "Single Image Super-Resolution", "comments": "5 pages, 7 figures, 1 table. A research paper prepared for Media\n  Analysis course at Tampere University", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study presents a chronological overview of the single image\nsuper-resolution problem. We first define the problem thoroughly and mention\nsome of the serious challenges. Then the problem formulation and the\nperformance metrics are defined. We give an overview of the previous methods\nrelying on reconstruction based solutions and then continue with the deep\nlearning approaches. We pick 3 landmark architectures and present their results\nquantitatively. We see that the latest proposed network gives favorable output\ncompared to the previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 00:10:03 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ataman", "Baran", ""], ["Seker", "Mert", ""], ["Mckee", "David", ""]]}, {"id": "2101.02824", "submitter": "Songjiang Li", "authors": "Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu and Jianzhuang Liu", "title": "Neighbor2Neighbor: Self-Supervised Denoising from Single Noisy Images", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, image denoising has benefited a lot from the fast\ndevelopment of neural networks. However, the requirement of large amounts of\nnoisy-clean image pairs for supervision limits the wide use of these models.\nAlthough there have been a few attempts in training an image denoising model\nwith only single noisy images, existing self-supervised denoising approaches\nsuffer from inefficient network training, loss of useful information, or\ndependence on noise modeling. In this paper, we present a very simple yet\neffective method named Neighbor2Neighbor to train an effective image denoising\nmodel with only noisy images. Firstly, a random neighbor sub-sampler is\nproposed for the generation of training image pairs. In detail, input and\ntarget used to train a network are images sub-sampled from the same noisy\nimage, satisfying the requirement that paired pixels of paired images are\nneighbors and have very similar appearance with each other. Secondly, a\ndenoising network is trained on sub-sampled training pairs generated in the\nfirst stage, with a proposed regularizer as additional loss for better\nperformance. The proposed Neighbor2Neighbor framework is able to enjoy the\nprogress of state-of-the-art supervised denoising networks in network\narchitecture design. Moreover, it avoids heavy dependence on the assumption of\nthe noise distribution. We explain our approach from a theoretical perspective\nand further validate it through extensive experiments, including synthetic\nexperiments with different noise distributions in sRGB space and real-world\nexperiments on a denoising benchmark dataset in raw-RGB space.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 02:03:25 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 02:43:29 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 03:12:02 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Huang", "Tao", ""], ["Li", "Songjiang", ""], ["Jia", "Xu", ""], ["Lu", "Huchuan", ""], ["Liu", "Jianzhuang", ""]]}, {"id": "2101.02839", "submitter": "Haojian Zhang", "authors": "Haojian Zhang, Yabin Zhang, Kui Jia, Lei Zhang", "title": "Unsupervised Domain Adaptation of Black-Box Source Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to learn models for a target domain\nof unlabeled data by transferring knowledge from a labeled source domain. In\nthe traditional UDA setting, labeled source data are assumed to be available\nfor adaptation. Due to increasing concerns for data privacy, source-free UDA is\nhighly appreciated as a new UDA setting, where only a trained source model is\nassumed to be available, while labeled source data remain private. However,\ntrained source models may also be unavailable in practice since source models\nmay have commercial values and exposing source models brings risks to the\nsource domain, e.g., problems of model misuse and white-box attacks. In this\nwork, we study a subtly different setting, named Black-Box Unsupervised Domain\nAdaptation (B$^2$UDA), where only the application programming interface of\nsource model is accessible to the target domain; in other words, the source\nmodel itself is kept as a black-box one. To tackle B$^2$UDA, we propose a\nsimple yet effective method, termed Iterative Learning with Noisy Labels\n(IterLNL). With black-box models as tools of noisy labeling, IterLNL conducts\nnoisy labeling and learning with noisy labels (LNL), iteratively. To facilitate\nthe implementation of LNL in B$^2$UDA, we estimate the noise rate from model\npredictions of unlabeled target data and propose category-wise sampling to\ntackle the unbalanced label noise among categories. Experiments on benchmark\ndatasets show the efficacy of IterLNL. Given neither source data nor source\nmodels, IterLNL performs comparably with traditional UDA methods that make full\nuse of labeled source data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 04:00:49 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 02:13:16 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Haojian", ""], ["Zhang", "Yabin", ""], ["Jia", "Kui", ""], ["Zhang", "Lei", ""]]}, {"id": "2101.02843", "submitter": "Dan Xu", "authors": "Dan Xu, Xavier Alameda-Pineda, Wanli Ouyang, Elisa Ricci, Xiaogang\n  Wang, Nicu Sebe", "title": "Probabilistic Graph Attention Network with Conditional Kernels for\n  Pixel-Wise Prediction", "comments": "Regular paper accepted at TPAMI 2020. arXiv admin note: text overlap\n  with arXiv:1801.00524", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-scale representations deeply learned via convolutional neural networks\nhave shown tremendous importance for various pixel-level prediction problems.\nIn this paper we present a novel approach that advances the state of the art on\npixel-level prediction in a fundamental aspect, i.e. structured multi-scale\nfeatures learning and fusion. In contrast to previous works directly\nconsidering multi-scale feature maps obtained from the inner layers of a\nprimary CNN architecture, and simply fusing the features with weighted\naveraging or concatenation, we propose a probabilistic graph attention network\nstructure based on a novel Attention-Gated Conditional Random Fields (AG-CRFs)\nmodel for learning and fusing multi-scale representations in a principled\nmanner. In order to further improve the learning capacity of the network\nstructure, we propose to exploit feature dependant conditional kernels within\nthe deep probabilistic framework. Extensive experiments are conducted on four\npublicly available datasets (i.e. BSDS500, NYUD-V2, KITTI, and Pascal-Context)\nand on three challenging pixel-wise prediction problems involving both discrete\nand continuous labels (i.e. monocular depth estimation, object contour\nprediction, and semantic segmentation). Quantitative and qualitative results\ndemonstrate the effectiveness of the proposed latent AG-CRF model and the\noverall probabilistic graph attention network with feature conditional kernels\nfor structured feature learning and pixel-wise prediction.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 04:14:29 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Xu", "Dan", ""], ["Alameda-Pineda", "Xavier", ""], ["Ouyang", "Wanli", ""], ["Ricci", "Elisa", ""], ["Wang", "Xiaogang", ""], ["Sebe", "Nicu", ""]]}, {"id": "2101.02870", "submitter": "Vishnu Sampathkumar", "authors": "Vishnu Ram Sampathkumar", "title": "ADiag: Graph Neural Network Based Diagnosis of Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alzheimer's Disease (AD) is the most widespread neurodegenerative disease,\naffecting over 50 million people across the world. While its progression cannot\nbe stopped, early and accurate diagnostic testing can drastically improve\nquality of life in patients. Currently, only qualitative means of testing are\nemployed in the form of scoring performance on a battery of cognitive tests.\nThe inherent disadvantage of this method is that the burden of an accurate\ndiagnosis falls on the clinician's competence. Quantitative methods like MRI\nscan assessment are inaccurate at best,due to the elusive nature of visually\nobservable changes in the brain. In lieu of these disadvantages to extant\nmethods of AD diagnosis, we have developed ADiag, a novel quantitative method\nto diagnose AD through GraphSAGE Network and Dense Differentiable Pooling (DDP)\nanalysis of large graphs based on thickness difference between different\nstructural regions of the cortex. Preliminary tests of ADiag have revealed a\nrobust accuracy of 83%, vastly outperforming other qualitative and quantitative\ndiagnostic techniques.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 06:23:30 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Sampathkumar", "Vishnu Ram", ""]]}, {"id": "2101.02876", "submitter": "Syed Anwar", "authors": "Ali Nawaz, Syed Muhammad Anwar, Rehan Liaqat, Javid Iqbal, Ulas Bagci,\n  Muhammad Majid", "title": "Deep Convolutional Neural Network based Classification of Alzheimer's\n  Disease using MRI data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alzheimer's disease (AD) is a progressive and incurable neurodegenerative\ndisease which destroys brain cells and causes loss to patient's memory. An\nearly detection can prevent the patient from further damage of the brain cells\nand hence avoid permanent memory loss. In past few years, various automatic\ntools and techniques have been proposed for diagnosis of AD. Several methods\nfocus on fast, accurate and early detection of the disease to minimize the loss\nto patients mental health. Although machine learning and deep learning\ntechniques have significantly improved medical imaging systems for AD by\nproviding diagnostic performance close to human level. But the main problem\nfaced during multi-class classification is the presence of highly correlated\nfeatures in the brain structure. In this paper, we have proposed a smart and\naccurate way of diagnosing AD based on a two-dimensional deep convolutional\nneural network (2D-DCNN) using imbalanced three-dimensional MRI dataset.\nExperimental results on Alzheimer Disease Neuroimaging Initiative magnetic\nresonance imaging (MRI) dataset confirms that the proposed 2D-DCNN model is\nsuperior in terms of accuracy, efficiency, and robustness. The model classifies\nMRI into three categories: AD, mild cognitive impairment, and normal control:\nand has achieved 99.89% classification accuracy with imbalanced classes. The\nproposed model exhibits noticeable improvement in accuracy as compared to the\nstate-fo-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 06:51:08 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Nawaz", "Ali", ""], ["Anwar", "Syed Muhammad", ""], ["Liaqat", "Rehan", ""], ["Iqbal", "Javid", ""], ["Bagci", "Ulas", ""], ["Majid", "Muhammad", ""]]}, {"id": "2101.02877", "submitter": "Jialin Peng", "authors": "Zhimin Yuan, Xiaofen Ma, Jiajin Yi, Zhengrong Luo, Jialin Peng", "title": "HIVE-Net: Centerline-Aware HIerarchical View-Ensemble Convolutional\n  Network for Mitochondria Segmentation in EM Images", "comments": "Accepted by CPMB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation of electron microscopy (EM) is an essential step to\nefficiently obtain reliable morphological statistics. Despite the great success\nachieved using deep convolutional neural networks (CNNs), they still produce\ncoarse segmentations with lots of discontinuities and false positives for\nmitochondria segmentation. In this study, we introduce a centerline-aware\nmultitask network by utilizing centerline as an intrinsic shape cue of\nmitochondria to regularize the segmentation. Since the application of 3D CNNs\non large medical volumes is usually hindered by their substantial computational\ncost and storage overhead, we introduce a novel hierarchical view-ensemble\nconvolution (HVEC), a simple alternative of 3D convolution to learn 3D spatial\ncontexts using more efficient 2D convolutions. The HVEC enables both\ndecomposing and sharing multi-view information, leading to increased learning\ncapacity. Extensive validation results on two challenging benchmarks show that,\nthe proposed method performs favorably against the state-of-the-art methods in\naccuracy and visual quality but with a greatly reduced model size. Moreover,\nthe proposed model also shows significantly improved generalization ability,\nespecially when training with quite limited amount of training data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 06:56:40 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Yuan", "Zhimin", ""], ["Ma", "Xiaofen", ""], ["Yi", "Jiajin", ""], ["Luo", "Zhengrong", ""], ["Peng", "Jialin", ""]]}, {"id": "2101.02882", "submitter": "Tatsuhito Hasegawa Dr.", "authors": "Tatsuhito Hasegawa", "title": "Octave Mix: Data augmentation using frequency decomposition for activity\n  recognition", "comments": "12 pages, 4 figures, this paper is a pre-print to submit to IEEE\n  Access journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the research field of activity recognition, although it is difficult to\ncollect a large amount of measured sensor data, there has not been much\ndiscussion about data augmentation (DA). In this study, I propose Octave Mix as\na new synthetic-style DA method for sensor-based activity recognition. Octave\nMix is a simple DA method that combines two types of waveforms by intersecting\nlow and high frequency waveforms using frequency decomposition. In addition, I\npropose a DA ensemble model and its training algorithm to acquire robustness to\nthe original sensor data while remaining a wide variety of feature\nrepresentation. I conducted experiments to evaluate the effectiveness of my\nproposed method using four different benchmark datasets of sensing-based\nactivity recognition. As a result, my proposed method achieved the best\nestimation accuracy. Furthermore, I found that ensembling two DA strategies:\nOctave Mix with rotation and mixup with rotation, make it possible to achieve\nhigher accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 07:09:08 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Hasegawa", "Tatsuhito", ""]]}, {"id": "2101.02888", "submitter": "Junaid Rahim", "authors": "Priyansi, Biswaroop Bhattacharjee, Junaid Rahim", "title": "Predicting Semen Motility using three-dimensional Convolutional Neural\n  Networks", "comments": "Corrected typos. Made slight changes as per the comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manual and computer aided methods to perform semen analysis are\ntime-consuming, requires extensive training and prone to human error. The use\nof classical machine learning and deep learning based methods using videos to\nperform semen analysis have yielded good results. The state-of-the-art method\nuses regular convolutional neural networks to perform quality assessments on a\nvideo of the provided sample. In this paper we propose an improved deep\nlearning based approach using three-dimensional convolutional neural networks\nto predict sperm motility from microscopic videos of the semen sample. We make\nuse of the VISEM dataset that consists of video and tabular data of semen\nsamples collected from 85 participants. We were able to achieve good results\nfrom significantly less data points. Our models indicate that deep learning\nbased automatic semen analysis may become a valuable and effective tool in\nfertility and IVF labs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 07:38:52 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 05:35:09 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Priyansi", "", ""], ["Bhattacharjee", "Biswaroop", ""], ["Rahim", "Junaid", ""]]}, {"id": "2101.02899", "submitter": "Josh Harguess", "authors": "Marissa Dotter, Sherry Xie, Keith Manville, Josh Harguess, Colin\n  Busho, Mikel Rodriguez", "title": "Adversarial Attack Attribution: Discovering Attributable Signals in\n  Adversarial ML Attacks", "comments": "Accepted to RSEML Workshop at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) models are known to be vulnerable to adversarial inputs\nand researchers have demonstrated that even production systems, such as\nself-driving cars and ML-as-a-service offerings, are susceptible. These systems\nrepresent a target for bad actors. Their disruption can cause real physical and\neconomic harm. When attacks on production ML systems occur, the ability to\nattribute the attack to the responsible threat group is a critical step in\nformulating a response and holding the attackers accountable. We pose the\nfollowing question: can adversarially perturbed inputs be attributed to the\nparticular methods used to generate the attack? In other words, is there a way\nto find a signal in these attacks that exposes the attack algorithm, model\narchitecture, or hyperparameters used in the attack? We introduce the concept\nof adversarial attack attribution and create a simple supervised learning\nexperimental framework to examine the feasibility of discovering attributable\nsignals in adversarial attacks. We find that it is possible to differentiate\nattacks generated with different attack algorithms, models, and hyperparameters\non both the CIFAR-10 and MNIST datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:16:41 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Dotter", "Marissa", ""], ["Xie", "Sherry", ""], ["Manville", "Keith", ""], ["Harguess", "Josh", ""], ["Busho", "Colin", ""], ["Rodriguez", "Mikel", ""]]}, {"id": "2101.02971", "submitter": "Fabian K\\\"uppers", "authors": "Franziska Schwaiger, Maximilian Henne, Fabian K\\\"uppers, Felippe\n  Schmoeller Roza, Karsten Roscher, Anselm Haselhoff", "title": "From Black-box to White-box: Examining Confidence Calibration under\n  different Conditions", "comments": "Accepted on SafeAI 2021 workshop on AAAI 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Confidence calibration is a major concern when applying artificial neural\nnetworks in safety-critical applications. Since most research in this area has\nfocused on classification in the past, confidence calibration in the scope of\nobject detection has gained more attention only recently. Based on previous\nwork, we study the miscalibration of object detection models with respect to\nimage location and box scale. Our main contribution is to additionally consider\nthe impact of box selection methods like non-maximum suppression to\ncalibration. We investigate the default intrinsic calibration of object\ndetection models and how it is affected by these post-processing techniques.\nFor this purpose, we distinguish between black-box calibration with non-maximum\nsuppression and white-box calibration with raw network outputs. Our experiments\nreveal that post-processing highly affects confidence calibration. We show that\nnon-maximum suppression has the potential to degrade initially well-calibrated\npredictions, leading to overconfident and thus miscalibrated models.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:45:30 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Schwaiger", "Franziska", ""], ["Henne", "Maximilian", ""], ["K\u00fcppers", "Fabian", ""], ["Roza", "Felippe Schmoeller", ""], ["Roscher", "Karsten", ""], ["Haselhoff", "Anselm", ""]]}, {"id": "2101.03003", "submitter": "Ziyi Liu", "authors": "Ziyi Liu", "title": "A review for Tone-mapping Operators on Wide Dynamic Range Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The dynamic range of our normal life can exceeds 120 dB, however, the\nsmart-phone cameras and the conventional digital cameras can only capture a\ndynamic range of 90 dB, which sometimes leads to loss of details for the\nrecorded image. Now, some professional hardware applications and image fusion\nalgorithms have been devised to take wide dynamic range (WDR), but\nunfortunately existing devices cannot display WDR image. Tone mapping (TM) thus\nbecomes an essential step for exhibiting WDR image on our ordinary screens,\nwhich convert the WDR image into low dynamic range (LDR) image. More and more\nresearchers are focusing on this topic, and give their efforts to design an\nexcellent tone mapping operator (TMO), showing detailed images as the same as\nthe perception that human eyes could receive. Therefore, it is important for us\nto know the history, development, and trend of TM before proposing a\npracticable TMO. In this paper, we present a comprehensive study of the most\nwell-known TMOs, which divides TMOs into traditional and machine learning-based\ncategory.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 13:32:26 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Liu", "Ziyi", ""]]}, {"id": "2101.03009", "submitter": "Fereshteh Lagzi", "authors": "Fereshteh Lagzi", "title": "Residual networks classify inputs based on their neural transient\n  dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the input-output behavior of residual networks from a dynamical\nsystem point of view by disentangling the residual dynamics from the output\nactivities before the classification stage. For a network with simple skip\nconnections between every successive layer, and for logistic activation\nfunction, and shared weights between layers, we show analytically that there is\na cooperation and competition dynamics between residuals corresponding to each\ninput dimension. Interpreting these kind of networks as nonlinear filters, the\nsteady state value of the residuals in the case of attractor networks are\nindicative of the common features between different input dimensions that the\nnetwork has observed during training, and has encoded in those components. In\ncases where residuals do not converge to an attractor state, their internal\ndynamics are separable for each input class, and the network can reliably\napproximate the output. We bring analytical and empirical evidence that\nresidual networks classify inputs based on the integration of the transient\ndynamics of the residuals, and will show how the network responds to input\nperturbations. We compare the network dynamics for a ResNet and a Multi-Layer\nPerceptron and show that the internal dynamics, and the noise evolution are\nfundamentally different in these networks, and ResNets are more robust to noisy\ninputs. Based on these findings, we also develop a new method to adjust the\ndepth for residual networks during training. As it turns out, after pruning the\ndepth of a ResNet using this algorithm, the network is still capable of\nclassifying inputs with a high accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 13:54:37 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 20:57:11 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Lagzi", "Fereshteh", ""]]}, {"id": "2101.03036", "submitter": "Chenyang Gao", "authors": "Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng, Jun Zhang, Yifei\n  Gong, Pai Peng, Xiaowei Guo, Xing Sun", "title": "Contextual Non-Local Alignment over Full-Scale Representation for\n  Text-Based Person Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based person search aims at retrieving target person in an image gallery\nusing a descriptive sentence of that person. It is very challenging since modal\ngap makes effectively extracting discriminative features more difficult.\nMoreover, the inter-class variance of both pedestrian images and descriptions\nis small. So comprehensive information is needed to align visual and textual\nclues across all scales. Most existing methods merely consider the local\nalignment between images and texts within a single scale (e.g. only global\nscale or only partial scale) then simply construct alignment at each scale\nseparately. To address this problem, we propose a method that is able to\nadaptively align image and textual features across all scales, called NAFS\n(i.e.Non-local Alignment over Full-Scale representations). Firstly, a novel\nstaircase network structure is proposed to extract full-scale image features\nwith better locality. Secondly, a BERT with locality-constrained attention is\nproposed to obtain representations of descriptions at different scales. Then,\ninstead of separately aligning features at each scale, a novel contextual\nnon-local attention mechanism is applied to simultaneously discover latent\nalignments across all scales. The experimental results show that our method\noutperforms the state-of-the-art methods by 5.53% in terms of top-1 and 5.35%\nin terms of top-5 on text-based person search dataset. The code is available at\nhttps://github.com/TencentYoutuResearch/PersonReID-NAFS\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 14:30:07 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Gao", "Chenyang", ""], ["Cai", "Guanyu", ""], ["Jiang", "Xinyang", ""], ["Zheng", "Feng", ""], ["Zhang", "Jun", ""], ["Gong", "Yifei", ""], ["Peng", "Pai", ""], ["Guo", "Xiaowei", ""], ["Sun", "Xing", ""]]}, {"id": "2101.03049", "submitter": "Yaohui Wang", "authors": "Yaohui Wang, Francois Bremond, Antitza Dantcheva", "title": "InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial\n  Network for Video Generation", "comments": "Please visit https://wyhsirius.github.io/InMoDeGAN/ for introductions\n  and more", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce an unconditional video generative model,\nInMoDeGAN, targeted to (a) generate high quality videos, as well as to (b)\nallow for interpretation of the latent space. For the latter, we place emphasis\non interpreting and manipulating motion. Towards this, we decompose motion into\nsemantic sub-spaces, which allow for control of generated samples. We design\nthe architecture of InMoDeGAN-generator in accordance to proposed Linear Motion\nDecomposition, which carries the assumption that motion can be represented by a\ndictionary, with related vectors forming an orthogonal basis in the latent\nspace. Each vector in the basis represents a semantic sub-space. In addition, a\nTemporal Pyramid Discriminator analyzes videos at different temporal\nresolutions. Extensive quantitative and qualitative analysis shows that our\nmodel systematically and significantly outperforms state-of-the-art methods on\nthe VoxCeleb2-mini and BAIR-robot datasets w.r.t. video quality related to (a).\nTowards (b) we present experimental results, confirming that decomposed\nsub-spaces are interpretable and moreover, generated motion is controllable.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 15:02:28 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wang", "Yaohui", ""], ["Bremond", "Francois", ""], ["Dantcheva", "Antitza", ""]]}, {"id": "2101.03057", "submitter": "Sebastian Palacio", "authors": "Sebastian Palacio, Philipp Engler, J\\\"orn Hees, Andreas Dengel", "title": "Contextual Classification Using Self-Supervised Auxiliary Models for\n  Deep Neural Networks", "comments": "Accepted for publication at the International Conference of Pattern\n  Recognition (ICPR) 2020", "journal-ref": null, "doi": "10.1109/ICPR48806.2021.9412175", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification problems solved with deep neural networks (DNNs) typically\nrely on a closed world paradigm, and optimize over a single objective (e.g.,\nminimization of the cross-entropy loss). This setup dismisses all kinds of\nsupporting signals that can be used to reinforce the existence or absence of a\nparticular pattern. The increasing need for models that are interpretable by\ndesign makes the inclusion of said contextual signals a crucial necessity. To\nthis end, we introduce the notion of Self-Supervised Autogenous Learning (SSAL)\nmodels. A SSAL objective is realized through one or more additional targets\nthat are derived from the original supervised classification task, following\narchitectural principles found in multi-task learning. SSAL branches impose\nlow-level priors into the optimization process (e.g., grouping). The ability of\nusing SSAL branches during inference, allow models to converge faster, focusing\non a richer set of class-relevant features. We show that SSAL models\nconsistently outperform the state-of-the-art while also providing structured\npredictions that are more interpretable.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:41:16 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Palacio", "Sebastian", ""], ["Engler", "Philipp", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "2101.03063", "submitter": "Yingni Wang", "authors": "Yingni Wang, Shuge Lei, Jian Dai and Kehong Yuan", "title": "Knowledge AI: New Medical AI Solution for Medical image Diagnosis", "comments": "9 pages,8 figures. arXiv admin note: text overlap with\n  arXiv:2101.02639", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of medical AI has always been a problem. The effect of\ntraditional perceptual AI algorithm in medical image processing needs to be\nimproved. Here we propose a method of knowledge AI, which is a combination of\nperceptual AI and clinical knowledge and experience. Based on this method, the\ngeometric information mining of medical images can represent the experience and\ninformation and evaluate the quality of medical images.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 15:30:09 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wang", "Yingni", ""], ["Lei", "Shuge", ""], ["Dai", "Jian", ""], ["Yuan", "Kehong", ""]]}, {"id": "2101.03064", "submitter": "Poojan Oza", "authors": "Pramuditha Perera, Poojan Oza, Vishal M. Patel", "title": "One-Class Classification: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-Class Classification (OCC) is a special case of multi-class\nclassification, where data observed during training is from a single positive\nclass. The goal of OCC is to learn a representation and/or a classifier that\nenables recognition of positively labeled queries during inference. This topic\nhas received considerable amount of interest in the computer vision, machine\nlearning and biometrics communities in recent years. In this article, we\nprovide a survey of classical statistical and recent deep learning-based OCC\nmethods for visual recognition. We discuss the merits and drawbacks of existing\nOCC approaches and identify promising avenues for research in this field. In\naddition, we present a discussion of commonly used datasets and evaluation\nmetrics for OCC.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 15:30:29 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Perera", "Pramuditha", ""], ["Oza", "Poojan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2101.03135", "submitter": "Nader Tavaf", "authors": "Nader Tavaf, Amirsina Torfi, Kamil Ugurbil, Pierre-Francois Van de\n  Moortele", "title": "GRAPPA-GANs for Parallel MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-space undersampling is a standard technique to accelerate MR image\nacquisitions. Reconstruction techniques including GeneRalized Autocalibrating\nPartial Parallel Acquisition(GRAPPA) and its variants are utilized extensively\nin clinical and research settings. A reconstruction model combining GRAPPA with\na conditional generative adversarial network (GAN) was developed and tested on\nmulti-coil human brain images from the fastMRI dataset. For various\nacceleration rates, GAN and GRAPPA reconstructions were compared in terms of\npeak signal-to-noise ratio (PSNR) and structural similarity (SSIM). For an\nacceleration rate of R=4, PSNR improved from 33.88 using regularized GRAPPA to\n37.65 using GAN. GAN consistently outperformed GRAPPA for various acceleration\nrates.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:47:20 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 19:48:12 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Tavaf", "Nader", ""], ["Torfi", "Amirsina", ""], ["Ugurbil", "Kamil", ""], ["Van de Moortele", "Pierre-Francois", ""]]}, {"id": "2101.03149", "submitter": "Ruohan Gao", "authors": "Ruohan Gao and Kristen Grauman", "title": "VisualVoice: Audio-Visual Speech Separation with Cross-Modal Consistency", "comments": "In CVPR 2021. Project page:\n  http://vision.cs.utexas.edu/projects/VisualVoice/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach for audio-visual speech separation. Given a\nvideo, the goal is to extract the speech associated with a face in spite of\nsimultaneous background sounds and/or other human speakers. Whereas existing\nmethods focus on learning the alignment between the speaker's lip movements and\nthe sounds they generate, we propose to leverage the speaker's face appearance\nas an additional prior to isolate the corresponding vocal qualities they are\nlikely to produce. Our approach jointly learns audio-visual speech separation\nand cross-modal speaker embeddings from unlabeled video. It yields\nstate-of-the-art results on five benchmark datasets for audio-visual speech\nseparation and enhancement, and generalizes well to challenging real-world\nvideos of diverse scenarios. Our video results and code:\nhttp://vision.cs.utexas.edu/projects/VisualVoice/.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 18:25:24 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 04:17:44 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Gao", "Ruohan", ""], ["Grauman", "Kristen", ""]]}, {"id": "2101.03154", "submitter": "Fanjie Kong", "authors": "Fanjie Kong, Xiao-yang Liu, Ricardo Henao", "title": "Quantum Tensor Network in Machine Learning: An Application to Tiny\n  Object Classification", "comments": "8 pages, 7 figures", "journal-ref": "https://tensorworkshop.github.io/NeurIPS2020/CFP.html", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tiny object classification problem exists in many machine learning\napplications like medical imaging or remote sensing, where the object of\ninterest usually occupies a small region of the whole image. It is challenging\nto design an efficient machine learning model with respect to tiny object of\ninterest. Current neural network structures are unable to deal with tiny object\nefficiently because they are mainly developed for images featured by large\nscale objects. However, in quantum physics, there is a great theoretical\nfoundation guiding us to analyze the target function for image classification\nregarding to specific objects size ratio. In our work, we apply Tensor Networks\nto solve this arising tough machine learning problem. First, we summarize the\nprevious work that connects quantum spin model to image classification and\nbring the theory into the scenario of tiny object classification. Second, we\npropose using 2D multi-scale entanglement renormalization ansatz (MERA) to\nclassify tiny objects in image. In the end, our experimental results indicate\nthat tensor network models are effective for tiny object classification problem\nand potentially will beat state-of-the-art. Our codes will be available online\nhttps://github.com/timqqt/MERA_Image_Classification.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 18:33:52 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Kong", "Fanjie", ""], ["Liu", "Xiao-yang", ""], ["Henao", "Ricardo", ""]]}, {"id": "2101.03169", "submitter": "Wen Liu", "authors": "Maohan Liang, Ryan Wen Liu, Shichen Li, Zhe Xiao, Xin Liu, Feng Lu", "title": "An Unsupervised Learning Method with Convolutional Auto-Encoder for\n  Vessel Trajectory Similarity Computation", "comments": "22 pages, 16 figures", "journal-ref": "Ocean Engineering, 2021", "doi": "10.1016/j.oceaneng.2021.108803", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To achieve reliable mining results for massive vessel trajectories, one of\nthe most important challenges is how to efficiently compute the similarities\nbetween different vessel trajectories. The computation of vessel trajectory\nsimilarity has recently attracted increasing attention in the maritime data\nmining research community. However, traditional shape- and warping-based\nmethods often suffer from several drawbacks such as high computational cost and\nsensitivity to unwanted artifacts and non-uniform sampling rates, etc. To\neliminate these drawbacks, we propose an unsupervised learning method which\nautomatically extracts low-dimensional features through a convolutional\nauto-encoder (CAE). In particular, we first generate the informative trajectory\nimages by remapping the raw vessel trajectories into two-dimensional matrices\nwhile maintaining the spatio-temporal properties. Based on the massive vessel\ntrajectories collected, the CAE can learn the low-dimensional representations\nof informative trajectory images in an unsupervised manner. The trajectory\nsimilarity is finally equivalent to efficiently computing the similarities\nbetween the learned low-dimensional features, which strongly correlate with the\nraw vessel trajectories. Comprehensive experiments on realistic data sets have\ndemonstrated that the proposed method largely outperforms traditional\ntrajectory similarity computation methods in terms of efficiency and\neffectiveness. The high-quality trajectory clustering performance could also be\nguaranteed according to the CAE-based trajectory similarity computation\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 04:42:11 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liang", "Maohan", ""], ["Liu", "Ryan Wen", ""], ["Li", "Shichen", ""], ["Xiao", "Zhe", ""], ["Liu", "Xin", ""], ["Lu", "Feng", ""]]}, {"id": "2101.03197", "submitter": "Abiy Tasissa", "authors": "Abiy Tasissa, Duc Nguyen, James Murphy", "title": "Deep Diffusion Processes for Active Learning of Hyperspectral Images", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A method for active learning of hyperspectral images (HSI) is proposed, which\ncombines deep learning with diffusion processes on graphs. A deep variational\nautoencoder extracts smoothed, denoised features from a high-dimensional HSI,\nwhich are then used to make labeling queries based on graph diffusion\nprocesses. The proposed method combines the robust representations of deep\nlearning with the mathematical tractability of diffusion geometry, and leads to\nstrong performance on real HSI.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 19:38:54 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Tasissa", "Abiy", ""], ["Nguyen", "Duc", ""], ["Murphy", "James", ""]]}, {"id": "2101.03198", "submitter": "Badri Narayanan", "authors": "Badri Narayanan, Mohamed Saadeldin, Paul Albert, Kevin McGuinness, and\n  Brian Mac Namee", "title": "Extracting Pasture Phenotype and Biomass Percentages using Weakly\n  Supervised Multi-target Deep Learning on a Small Dataset", "comments": null, "journal-ref": "Irish Machine Vision and Image Processing Conference (2020) 21-28", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The dairy industry uses clover and grass as fodder for cows. Accurate\nestimation of grass and clover biomass yield enables smart decisions in\noptimizing fertilization and seeding density, resulting in increased\nproductivity and positive environmental impact. Grass and clover are usually\nplanted together, since clover is a nitrogen-fixing plant that brings nutrients\nto the soil. Adjusting the right percentages of clover and grass in a field\nreduces the need for external fertilization. Existing approaches for estimating\nthe grass-clover composition of a field are expensive and time consuming -\nrandom samples of the pasture are clipped and then the components are\nphysically separated to weigh and calculate percentages of dry grass, clover\nand weeds in each sample. There is growing interest in developing novel deep\nlearning based approaches to non-destructively extract pasture phenotype\nindicators and biomass yield predictions of different plant species from\nagricultural imagery collected from the field. Providing these indicators and\npredictions from images alone remains a significant challenge. Heavy occlusions\nin the dense mixture of grass, clover and weeds make it difficult to estimate\neach component accurately. Moreover, although supervised deep learning models\nperform well with large datasets, it is tedious to acquire large and diverse\ncollections of field images with precise ground truth for different biomass\nyields. In this paper, we demonstrate that applying data augmentation and\ntransfer learning is effective in predicting multi-target biomass percentages\nof different plant species, even with a small training dataset. The scheme\nproposed in this paper used a training set of only 261 images and provided\npredictions of biomass percentages of grass, clover, white clover, red clover,\nand weeds with mean absolute error of 6.77%, 6.92%, 6.21%, 6.89%, and 4.80%\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 19:41:46 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Narayanan", "Badri", ""], ["Saadeldin", "Mohamed", ""], ["Albert", "Paul", ""], ["McGuinness", "Kevin", ""], ["Mac Namee", "Brian", ""]]}, {"id": "2101.03203", "submitter": "Kashif Ahmad", "authors": "Muhammad Usman, Kashif Ahmad, Amir Sohail, Marwa Qaraqe", "title": "The Diabetic Buddy: A Diet Regulator andTracking System for Diabetics", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prevalence of Diabetes mellitus (DM) in the Middle East is exceptionally\nhigh as compared to the rest of the world. In fact, the prevalence of diabetes\nin the Middle East is 17-20%, which is well above the global average of 8-9%.\nResearch has shown that food intake has strong connections with the blood\nglucose levels of a patient. In this regard, there is a need to build automatic\ntools to monitor the blood glucose levels of diabetics and their daily food\nintake. This paper presents an automatic way of tracking continuous glucose and\nfood intake of diabetics using off-the-shelf sensors and machine learning,\nrespectively. Our system not only helps diabetics to track their daily food\nintake but also assists doctors to analyze the impact of the food in-take on\nblood glucose in real-time. For food recognition, we collected a large-scale\nMiddle-Eastern food dataset and proposed a fusion-based framework incorporating\nseveral existing pre-trained deep models for Middle-Eastern food recognition.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:03:58 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Usman", "Muhammad", ""], ["Ahmad", "Kashif", ""], ["Sohail", "Amir", ""], ["Qaraqe", "Marwa", ""]]}, {"id": "2101.03244", "submitter": "Anindo Saha", "authors": "Anindo Saha, Matin Hosseinzadeh, Henkjan Huisman", "title": "End-to-end Prostate Cancer Detection in bpMRI via 3D CNNs: Effects of\n  Attention Mechanisms, Clinical Priori and Decoupled False Positive Reduction", "comments": "Accepted to MedIA: Medical Image Analysis. This manuscript\n  incorporates and expands upon our 2020 Medical Imaging Meets NeurIPS Workshop\n  paper (arXiv:2011.00263)", "journal-ref": null, "doi": "10.1016/j.media.2021.102155", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a multi-stage 3D computer-aided detection and diagnosis (CAD)\nmodel for automated localization of clinically significant prostate cancer\n(csPCa) in bi-parametric MR imaging (bpMRI). Deep attention mechanisms drive\nits detection network, targeting salient structures and highly discriminative\nfeature dimensions across multiple resolutions. Its goal is to accurately\nidentify csPCa lesions from indolent cancer and the wide range of benign\npathology that can afflict the prostate gland. Simultaneously, a decoupled\nresidual classifier is used to achieve consistent false positive reduction,\nwithout sacrificing high sensitivity or computational efficiency. In order to\nguide model generalization with domain-specific clinical knowledge, a\nprobabilistic anatomical prior is used to encode the spatial prevalence and\nzonal distinction of csPCa. Using a large dataset of 1950 prostate bpMRI paired\nwith radiologically-estimated annotations, we hypothesize that such CNN-based\nmodels can be trained to detect biopsy-confirmed malignancies in an independent\ncohort.\n  For 486 institutional testing scans, the 3D CAD system achieves\n83.69$\\pm$5.22% and 93.19$\\pm$2.96% detection sensitivity at 0.50 and 1.46\nfalse positive(s) per patient, respectively, with 0.882$\\pm$0.030 AUROC in\npatient-based diagnosis $-$significantly outperforming four state-of-the-art\nbaseline architectures (U-SEResNet, UNet++, nnU-Net, Attention U-Net) from\nrecent literature. For 296 external biopsy-confirmed testing scans, the\nensembled CAD system shares moderate agreement with a consensus of expert\nradiologists (76.69%; $kappa$ $=$ 0.51$\\pm$0.04) and independent pathologists\n(81.08%; $kappa$ $=$ 0.56$\\pm$0.06); demonstrating strong generalization to\nhistologically-confirmed csPCa diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 22:59:30 GMT"}, {"version": "v10", "created": "Wed, 30 Jun 2021 20:16:35 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 01:41:29 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 16:21:16 GMT"}, {"version": "v4", "created": "Fri, 22 Jan 2021 18:05:25 GMT"}, {"version": "v5", "created": "Thu, 28 Jan 2021 22:39:13 GMT"}, {"version": "v6", "created": "Sun, 4 Apr 2021 17:12:33 GMT"}, {"version": "v7", "created": "Tue, 6 Apr 2021 22:30:52 GMT"}, {"version": "v8", "created": "Sat, 5 Jun 2021 14:59:16 GMT"}, {"version": "v9", "created": "Tue, 8 Jun 2021 15:06:29 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Saha", "Anindo", ""], ["Hosseinzadeh", "Matin", ""], ["Huisman", "Henkjan", ""]]}, {"id": "2101.03247", "submitter": "AmirAbbas Davari", "authors": "Michael Holzmann, Amirabbas Davari, Thorsten Seehaus, Matthias Braun,\n  Andreas Maier, Vincent Christlein", "title": "Glacier Calving Front Segmentation Using Attention U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An essential climate variable to determine the tidewater glacier status is\nthe location of the calving front position and the separation of seasonal\nvariability from long-term trends. Previous studies have proposed deep\nlearning-based methods to semi-automatically delineate the calving fronts of\ntidewater glaciers. They used U-Net to segment the ice and non-ice regions and\nextracted the calving fronts in a post-processing step. In this work, we show a\nmethod to segment the glacier calving fronts from SAR images in an end-to-end\nfashion using Attention U-Net. The main objective is to investigate the\nattention mechanism in this application. Adding attention modules to the\nstate-of-the-art U-Net network lets us analyze the learning process by\nextracting its attention maps. We use these maps as a tool to search for proper\nhyperparameters and loss functions in order to generate higher qualitative\nresults. Our proposed attention U-Net performs comparably to the standard U-Net\nwhile providing additional insight into those regions on which the network\nlearned to focus more. In the best case, the attention U-Net achieves a 1.5%\nbetter Dice score compared to the canonical U-Net with a glacier front line\nprediction certainty of up to 237.12 meters.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:06:21 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Holzmann", "Michael", ""], ["Davari", "Amirabbas", ""], ["Seehaus", "Thorsten", ""], ["Braun", "Matthias", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2101.03249", "submitter": "AmirAbbas Davari", "authors": "Andreas Hartmann, Amirabbas Davari, Thorsten Seehaus, Matthias Braun,\n  Andreas Maier, Vincent Christlein", "title": "Bayesian U-Net for Segmenting Glaciers in SAR Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluctuations of the glacier calving front have an important influence over\nthe ice flow of whole glacier systems. It is therefore important to precisely\nmonitor the position of the calving front. However, the manual delineation of\nSAR images is a difficult, laborious and subjective task. Convolutional neural\nnetworks have previously shown promising results in automating the glacier\nsegmentation in SAR images, making them desirable for further exploration of\ntheir possibilities. In this work, we propose to compute uncertainty and use it\nin an Uncertainty Optimization regime as a novel two-stage process. By using\ndropout as a random sampling layer in a U-Net architecture, we create a\nprobabilistic Bayesian Neural Network. With several forward passes, we create a\nsampling distribution, which can estimate the model uncertainty for each pixel\nin the segmentation mask. The additional uncertainty map information can serve\nas a guideline for the experts in the manual annotation of the data.\nFurthermore, feeding the uncertainty map to the network leads to 95.24% Dice\nsimilarity, which is an overall improvement in the segmentation performance\ncompared to the state-of-the-art deterministic U-Net-based glacier segmentation\npipelines.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:17:49 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 08:39:17 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Hartmann", "Andreas", ""], ["Davari", "Amirabbas", ""], ["Seehaus", "Thorsten", ""], ["Braun", "Matthias", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2101.03251", "submitter": "Babak Taati", "authors": "Siavash Rezaei, Abhishek Moturu, Shun Zhao, Kenneth M. Prkachin,\n  Thomas Hadjistavropoulos, and Babak Taati", "title": "Unobtrusive Pain Monitoring in Older Adults with Dementia using Pairwise\n  and Contrastive Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although pain is frequent in old age, older adults are often undertreated for\npain. This is especially the case for long-term care residents with moderate to\nsevere dementia who cannot report their pain because of cognitive impairments\nthat accompany dementia. Nursing staff acknowledge the challenges of\neffectively recognizing and managing pain in long-term care facilities due to\nlack of human resources and, sometimes, expertise to use validated pain\nassessment approaches on a regular basis. Vision-based ambient monitoring will\nallow for frequent automated assessments so care staff could be automatically\nnotified when signs of pain are displayed. However, existing computer vision\ntechniques for pain detection are not validated on faces of older adults or\npeople with dementia, and this population is not represented in existing facial\nexpression datasets of pain. We present the first fully automated vision-based\ntechnique validated on a dementia cohort. Our contributions are threefold.\nFirst, we develop a deep learning-based computer vision system for detecting\npainful facial expressions on a video dataset that is collected unobtrusively\nfrom older adult participants with and without dementia. Second, we introduce a\npairwise comparative inference method that calibrates to each person and is\nsensitive to changes in facial expression while using training data more\nefficiently than sequence models. Third, we introduce a fast contrastive\ntraining method that improves cross-dataset performance. Our pain estimation\nmodel outperforms baselines by a wide margin, especially when evaluated on\nfaces of people with dementia. Pre-trained model and demo code available at\nhttps://github.com/TaatiTeam/pain_detection_demo\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:28:30 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Rezaei", "Siavash", ""], ["Moturu", "Abhishek", ""], ["Zhao", "Shun", ""], ["Prkachin", "Kenneth M.", ""], ["Hadjistavropoulos", "Thomas", ""], ["Taati", "Babak", ""]]}, {"id": "2101.03252", "submitter": "AmirAbbas Davari", "authors": "Rosanna Dietrich-Sussner, Amirabbas Davari, Thorsten Seehaus, Matthias\n  Braun, Vincent Christlein, Andreas Maier, Christian Riess", "title": "Synthetic Glacier SAR Image Generation from Arbitrary Masks Using\n  Pix2Pix Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised machine learning requires a large amount of labeled data to\nachieve proper test results. However, generating accurately labeled\nsegmentation maps on remote sensing imagery, including images from synthetic\naperture radar (SAR), is tedious and highly subjective. In this work, we\npropose to alleviate the issue of limited training data by generating synthetic\nSAR images with the pix2pix algorithm. This algorithm uses conditional\nGenerative Adversarial Networks (cGANs) to generate an artificial image while\npreserving the structure of the input. In our case, the input is a segmentation\nmask, from which a corresponding synthetic SAR image is generated. We present\ndifferent models, perform a comparative study and demonstrate that this\napproach synthesizes convincing glaciers in SAR images with promising\nqualitative and quantitative results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:30:00 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 22:07:48 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Dietrich-Sussner", "Rosanna", ""], ["Davari", "Amirabbas", ""], ["Seehaus", "Thorsten", ""], ["Braun", "Matthias", ""], ["Christlein", "Vincent", ""], ["Maier", "Andreas", ""], ["Riess", "Christian", ""]]}, {"id": "2101.03255", "submitter": "Haoyu Ma", "authors": "Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie,\n  Zhangyang Wang", "title": "Good Students Play Big Lottery Better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lottery ticket hypothesis suggests that a dense neural network contains a\nsparse sub-network that can match the test accuracy of the original dense net\nwhen trained in isolation from (the same) random initialization. However, the\nhypothesis failed to generalize to larger dense networks such as ResNet-50. As\na remedy, recent studies demonstrate that a sparse sub-network can still be\nobtained by using a rewinding technique, which is to re-train it from\nearly-phase training weights or learning rates of the dense model, rather than\nfrom random initialization.\n  Is rewinding the only or the best way to scale up lottery tickets? This paper\nproposes a new, simpler and yet powerful technique for re-training the\nsub-network, called \"Knowledge Distillation ticket\" (KD ticket). Rewinding\nexploits the value of inheriting knowledge from the early training phase to\nimprove lottery tickets in large networks. In comparison, KD ticket addresses a\ncomplementary possibility - inheriting useful knowledge from the late training\nphase of the dense model. It is achieved by leveraging the soft labels\ngenerated by the trained dense model to re-train the sub-network, instead of\nthe hard labels. Extensive experiments are conducted using several large deep\nnetworks (e.g ResNet-50 and ResNet-110) on CIFAR-10 and ImageNet datasets.\nWithout bells and whistles, when applied by itself, KD ticket performs on par\nor better than rewinding, while being nearly free of hyperparameters or ad-hoc\nselection. KD ticket can be further applied together with rewinding, yielding\nstate-of-the-art results for large-scale lottery tickets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:33:53 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 07:25:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ma", "Haoyu", ""], ["Chen", "Tianlong", ""], ["Hu", "Ting-Kuei", ""], ["You", "Chenyu", ""], ["Xie", "Xiaohui", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2101.03272", "submitter": "Dongze Li", "authors": "Dongze Li, Wei Wang, Hongxing Fan, Jing Dong", "title": "Exploring Adversarial Fake Images on Face Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Images synthesized by powerful generative adversarial network (GAN) based\nmethods have drawn moral and privacy concerns. Although image forensic models\nhave reached great performance in detecting fake images from real ones, these\nmodels can be easily fooled with a simple adversarial attack. But, the noise\nadding adversarial samples are also arousing suspicion. In this paper, instead\nof adding adversarial noise, we optimally search adversarial points on face\nmanifold to generate anti-forensic fake face images. We iteratively do a\ngradient-descent with each small step in the latent space of a generative\nmodel, e.g. Style-GAN, to find an adversarial latent vector, which is similar\nto norm-based adversarial attack but in latent space. Then, the generated fake\nimages driven by the adversarial latent vectors with the help of GANs can\ndefeat main-stream forensic models. For examples, they make the accuracy of\ndeepfake detection models based on Xception or EfficientNet drop from over 90%\nto nearly 0%, meanwhile maintaining high visual quality. In addition, we find\nmanipulating style vector $z$ or noise vectors $n$ at different levels have\nimpacts on attack success rate. The generated adversarial images mainly have\nfacial texture or face attributes changing.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 02:08:59 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 07:14:32 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Li", "Dongze", ""], ["Wang", "Wei", ""], ["Fan", "Hongxing", ""], ["Dong", "Jing", ""]]}, {"id": "2101.03275", "submitter": "Anirudh Shah", "authors": "Jordan Lee, Willy Lin, Konstantinos Ntalis, Anirudh Shah, William\n  Tung, Maxwell Wulff", "title": "Identifying Human Edited Images using a CNN", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most non-professional photo manipulations are not made using propriety\nsoftware like Adobe Photoshop, which is expensive and complicated to use for\nthe average consumer selfie-taker or meme-maker. Instead, these individuals opt\nfor user friendly mobile applications like FaceTune and Pixlr to make human\nface edits and alterations. Unfortunately, there is no existing dataset to\ntrain a model to classify these type of manipulations. In this paper, we\npresent a generative model that approximates the distribution of human face\nedits and a method for detecting Facetune and Pixlr manipulations to human\nfaces.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 02:43:35 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Lee", "Jordan", ""], ["Lin", "Willy", ""], ["Ntalis", "Konstantinos", ""], ["Shah", "Anirudh", ""], ["Tung", "William", ""], ["Wulff", "Maxwell", ""]]}, {"id": "2101.03279", "submitter": "Fang-Chieh Chou", "authors": "Abhishek Mohta, Fang-Chieh Chou, Brian C. Becker, Carlos\n  Vallespi-Gonzalez, Nemanja Djuric", "title": "Investigating the Effect of Sensor Modalities in Multi-Sensor\n  Detection-Prediction Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of surrounding objects and their motion prediction are critical\ncomponents of a self-driving system. Recently proposed models that jointly\naddress these tasks rely on a number of sensors to achieve state-of-the-art\nperformance. However, this increases system complexity and may result in a\nbrittle model that overfits to any single sensor modality while ignoring\nothers, leading to reduced generalization. We focus on this important problem\nand analyze the contribution of sensor modalities towards the model\nperformance. In addition, we investigate the use of sensor dropout to mitigate\nthe above-mentioned issues, leading to a more robust, better-performing model\non real-world driving data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 03:21:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mohta", "Abhishek", ""], ["Chou", "Fang-Chieh", ""], ["Becker", "Brian C.", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Djuric", "Nemanja", ""]]}, {"id": "2101.03285", "submitter": "Yu Tian", "authors": "Yu Tian, Leonardo Zorron Cheng Tao Pu, Yuyuan Liu, Gabriel Maicas,\n  Johan W. Verjans, Alastair D. Burt, Seon Ho Shin, Rajvinder Singh, Gustavo\n  Carneiro", "title": "Detecting, Localising and Classifying Polyps from Colonoscopy Videos\n  using Deep Learning", "comments": "Preprint to submit to IEEE journals", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and analyse a system that can automatically detect,\nlocalise and classify polyps from colonoscopy videos. The detection of frames\nwith polyps is formulated as a few-shot anomaly classification problem, where\nthe training set is highly imbalanced with the large majority of frames\nconsisting of normal images and a small minority comprising frames with polyps.\nColonoscopy videos may contain blurry images and frames displaying feces and\nwater jet sprays to clean the colon -- such frames can mistakenly be detected\nas anomalies, so we have implemented a classifier to reject these two types of\nframes before polyp detection takes place. Next, given a frame containing a\npolyp, our method localises (with a bounding box around the polyp) and\nclassifies it into five different classes. Furthermore, we study a method to\nimprove the reliability and interpretability of the classification result using\nuncertainty estimation and classification calibration. Classification\nuncertainty and calibration not only help improve classification accuracy by\nrejecting low-confidence and high-uncertain results, but can be used by doctors\nto decide how to decide on the classification of a polyp. All the proposed\ndetection, localisation and classification methods are tested using large data\nsets and compared with relevant baseline approaches.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 04:25:34 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Tian", "Yu", ""], ["Pu", "Leonardo Zorron Cheng Tao", ""], ["Liu", "Yuyuan", ""], ["Maicas", "Gabriel", ""], ["Verjans", "Johan W.", ""], ["Burt", "Alastair D.", ""], ["Shin", "Seon Ho", ""], ["Singh", "Rajvinder", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2101.03287", "submitter": "Liang Lin", "authors": "Fuyu Wang and Xiaodan Liang and Lin Xu and Liang Lin", "title": "Unifying Relational Sentence Generation and Retrieval for Medical Image\n  Report Composition", "comments": "To appear in IEEE Transactions on Cybernetics 2021. We attempt to\n  resolve the challenging medical report composition task by i) enforcing the\n  semantic consistency of medical terms to be incorporated into the final\n  reports; and ii) encouraging the sentence generation for rare abnormal\n  descriptions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond generating long and topic-coherent paragraphs in traditional\ncaptioning tasks, the medical image report composition task poses more\ntask-oriented challenges by requiring both the highly-accurate medical term\ndiagnosis and multiple heterogeneous forms of information including impression\nand findings. Current methods often generate the most common sentences due to\ndataset bias for individual case, regardless of whether the sentences properly\ncapture key entities and relationships. Such limitations severely hinder their\napplicability and generalization capability in medical report composition where\nthe most critical sentences lie in the descriptions of abnormal diseases that\nare relatively rare. Moreover, some medical terms appearing in one report are\noften entangled with each other and co-occurred, e.g. symptoms associated with\na specific disease. To enforce the semantic consistency of medical terms to be\nincorporated into the final reports and encourage the sentence generation for\nrare abnormal descriptions, we propose a novel framework that unifies template\nretrieval and sentence generation to handle both common and rare abnormality\nwhile ensuring the semantic-coherency among the detected medical terms.\nSpecifically, our approach exploits hybrid-knowledge co-reasoning: i) explicit\nrelationships among all abnormal medical terms to induce the visual attention\nlearning and topic representation encoding for better topic-oriented symptoms\ndescriptions; ii) adaptive generation mode that changes between the template\nretrieval and sentence generation according to a contextual topic encoder.\nExperimental results on two medical report benchmarks demonstrate the\nsuperiority of the proposed framework in terms of both human and metrics\nevaluation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 04:33:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wang", "Fuyu", ""], ["Liang", "Xiaodan", ""], ["Xu", "Lin", ""], ["Lin", "Liang", ""]]}, {"id": "2101.03292", "submitter": "Zhi Chen", "authors": "Zhi Chen, Zi Huang, Jingjing Li, Zheng Zhang", "title": "Entropy-Based Uncertainty Calibration for Generalized Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to conventional zero-shot learning (ZSL) where recognising unseen\nclasses is the primary or only aim, the goal of generalized zero-shot learning\n(GZSL) is to recognise both seen and unseen classes. Most GZSL methods\ntypically learn to synthesise visual representations from semantic information\non the unseen classes. However, these types of models are prone to overfitting\nthe seen classes, resulting in distribution overlap between the generated\nfeatures of the seen and unseen classes. The overlapping region is filled with\nuncertainty as the model struggles to determine whether a test case from within\nthe overlap is seen or unseen. Further, these generative methods suffer in\nscenarios with sparse training samples. The models struggle to learn the\ndistribution of high dimensional visual features and, therefore, fail to\ncapture the most discriminative inter-class features. To address these issues,\nin this paper, we propose a novel framework that leverages dual variational\nautoencoders with a triplet loss to learn discriminative latent features and\napplies the entropy-based calibration to minimize the uncertainty in the\noverlapped area between the seen and unseen classes. Specifically, the dual\ngenerative model with the triplet loss synthesises inter-class discriminative\nlatent features that can be mapped from either visual or semantic space. To\ncalibrate the uncertainty for seen classes, we calculate the entropy over the\nsoftmax probability distribution from a general classifier. With this approach,\nrecognising the seen samples within the seen classes is relatively\nstraightforward, and there is less risk that a seen sample will be\nmisclassified into an unseen class in the overlapped region. Extensive\nexperiments on six benchmark datasets demonstrate that the proposed method\noutperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 05:21:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chen", "Zhi", ""], ["Huang", "Zi", ""], ["Li", "Jingjing", ""], ["Zhang", "Zheng", ""]]}, {"id": "2101.03321", "submitter": "Vineet Mehta", "authors": "Vineet Mehta, Parul Gupta, Ramanathan Subramanian, and Abhinav Dhall", "title": "FakeBuster: A DeepFakes Detection Tool for Video Conferencing Scenarios", "comments": "5 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new DeepFake detector FakeBuster for detecting\nimpostors during video conferencing and manipulated faces on social media.\nFakeBuster is a standalone deep learning based solution, which enables a user\nto detect if another person's video is manipulated or spoofed during a video\nconferencing based meeting. This tool is independent of video conferencing\nsolutions and has been tested with Zoom and Skype applications. It uses a 3D\nconvolutional neural network for predicting video segment-wise fakeness scores.\nThe network is trained on a combination of datasets such as Deeperforensics,\nDFDC, VoxCeleb, and deepfake videos created using locally captured (for video\nconferencing scenarios) images. This leads to different environments and\nperturbations in the dataset, which improves the generalization of the deepfake\nnetwork.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 09:06:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mehta", "Vineet", ""], ["Gupta", "Parul", ""], ["Subramanian", "Ramanathan", ""], ["Dhall", "Abhinav", ""]]}, {"id": "2101.03326", "submitter": "Liang Xu", "authors": "Liang Xu, Taro Hatsutani, Xing Liu, Engkarat Techapanurak, Han Zou and\n  Takayuki Okatani", "title": "Pushing the Envelope of Thin Crack Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we consider the problem of detecting cracks from the image of\na concrete surface for automated inspection of infrastructure, such as bridges.\nIts overall accuracy is determined by how accurately thin cracks with sub-pixel\nwidths can be detected. Our interest is in making it possible to detect cracks\nclose to the limit of thinness if it can be defined. Toward this end, we first\npropose a method for training a CNN to make it detect cracks more accurately\nthan humans while training them on human-annotated labels. To achieve this\nseemingly impossible goal, we intentionally lower the spatial resolution of\ninput images while maintaining that of their labels when training a CNN. This\nmakes it possible to annotate cracks that are too thin for humans to detect,\nwhich we call super-human labels. We experimentally show that this makes it\npossible to detect cracks from an image of one-third the resolution of images\nused for annotation with about the same accuracy. We additionally propose three\nmethods for further improving the detection accuracy of thin cracks: i)\nP-pooling to maintain small image structures during downsampling operations;\nii) Removal of short-segment cracks in a post-processing step utilizing a prior\nof crack shapes learned using the VAE-GAN framework; iii) Modeling uncertainty\nof the prediction to better handle hard labels beyond the limit of CNNs'\ndetection ability, which technically work as noisy labels. We experimentally\nexamine the effectiveness of these methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 09:42:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Xu", "Liang", ""], ["Hatsutani", "Taro", ""], ["Liu", "Xing", ""], ["Techapanurak", "Engkarat", ""], ["Zou", "Han", ""], ["Okatani", "Takayuki", ""]]}, {"id": "2101.03409", "submitter": "Andr\\'e Fusioka", "authors": "Gabriel Henrique de Almeida Pereira and Andr\\'e Minoro Fusioka and\n  Bogdan Tomoyuki Nassu and Rodrigo Minetto", "title": "Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a\n  Deep-Learning Study", "comments": "23 pages, 17 figures", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2021.06.002", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Active fire detection in satellite imagery is of critical importance to the\nmanagement of environmental conservation policies, supporting decision-making\nand law enforcement. This is a well established field, with many techniques\nbeing proposed over the years, usually based on pixel or region-level\ncomparisons involving sensor-specific thresholds and neighborhood statistics.\nIn this paper, we address the problem of active fire detection using deep\nlearning techniques. In recent years, deep learning techniques have been\nenjoying an enormous success in many fields, but their use for active fire\ndetection is relatively new, with open questions and demand for datasets and\narchitectures for evaluation. This paper addresses these issues by introducing\na new large-scale dataset for active fire detection, with over 150,000 image\npatches (more than 200 GB of data) extracted from Landsat-8 images captured\naround the world in August and September 2020, containing wildfires in several\nlocations. The dataset was split in two parts, and contains 10-band spectral\nimages with associated outputs, produced by three well known handcrafted\nalgorithms for active fire detection in the first part, and manually annotated\nmasks in the second part. We also present a study on how different\nconvolutional neural network architectures can be used to approximate these\nhandcrafted algorithms, and how models trained on automatically segmented\npatches can be combined to achieve better performance than the original\nalgorithms - with the best combination having 87.2% precision and 92.4% recall\non our manually annotated dataset. The proposed dataset, source codes and\ntrained models are available on Github\n(https://github.com/pereira-gha/activefire), creating opportunities for further\nadvances in the field\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 19:05:03 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 16:50:53 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pereira", "Gabriel Henrique de Almeida", ""], ["Fusioka", "Andr\u00e9 Minoro", ""], ["Nassu", "Bogdan Tomoyuki", ""], ["Minetto", "Rodrigo", ""]]}, {"id": "2101.03431", "submitter": "Shane Storks", "authors": "Shane Storks, Qiaozi Gao, Govind Thattai, Gokhan Tur", "title": "Are We There Yet? Learning to Localize in Embodied Instruction Following", "comments": "Accepted to HAI @ AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied instruction following is a challenging problem requiring an agent to\ninfer a sequence of primitive actions to achieve a goal environment state from\ncomplex language and visual inputs. Action Learning From Realistic Environments\nand Directives (ALFRED) is a recently proposed benchmark for this problem\nconsisting of step-by-step natural language instructions to achieve subgoals\nwhich compose to an ultimate high-level goal. Key challenges for this task\ninclude localizing target locations and navigating to them through visual\ninputs, and grounding language instructions to visual appearance of objects. To\naddress these challenges, in this study, we augment the agent's field of view\nduring navigation subgoals with multiple viewing angles, and train the agent to\npredict its relative spatial relation to the target location at each timestep.\nWe also improve language grounding by introducing a pre-trained object\ndetection module to the model pipeline. Empirical studies show that our\napproach exceeds the baseline model performance.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 21:49:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Storks", "Shane", ""], ["Gao", "Qiaozi", ""], ["Thattai", "Govind", ""], ["Tur", "Gokhan", ""]]}, {"id": "2101.03477", "submitter": "Peter Washington", "authors": "Peter Washington, Onur Cezmi Mutlu, Emilie Leblanc, Aaron Kline, Cathy\n  Hou, Brianna Chrisman, Nate Stockham, Kelley Paskov, Catalin Voss, Nick\n  Haber, Dennis Wall", "title": "Using Crowdsourcing to Train Facial Emotion Machine Learning Models with\n  Ambiguous Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Current emotion detection classifiers predict discrete emotions. However,\nliterature in psychology has documented that compound and ambiguous facial\nexpressions are often evoked by humans. As a stride towards development of\nmachine learning models that more accurately reflect compound and ambiguous\nemotions, we replace traditional one-hot encoded label representations with a\ncrowd's distribution of labels. We center our study on the Child Affective\nFacial Expression (CAFE) dataset, a gold standard dataset of pediatric facial\nexpressions which includes 100 human labels per image. We first acquire\ncrowdsourced labels for 207 emotions from CAFE and demonstrate that the\nconsensus labels from the crowd tend to match the consensus from the original\nCAFE raters, validating the utility of crowdsourcing. We then train two\nversions of a ResNet-152 classifier on CAFE images with two types of labels (1)\ntraditional one-hot encoding and (2) vector labels representing the crowd\ndistribution of responses. We compare the resulting output distributions of the\ntwo classifiers. While the traditional F1-score for the one-hot encoding\nclassifier is much higher (94.33% vs. 78.68%), the output probability vector of\nthe crowd-trained classifier much more closely resembles the distribution of\nhuman labels (t=3.2827, p=0.0014). For many applications of affective\ncomputing, reporting an emotion probability distribution that more closely\nresembles human interpretation can be more important than traditional machine\nlearning metrics. This work is a first step for engineers of interactive\nsystems to account for machine learning cases with ambiguous classes and we\nhope it will generate a discussion about machine learning with ambiguous labels\nand leveraging crowdsourcing as a potential solution.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 05:26:55 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Washington", "Peter", ""], ["Mutlu", "Onur Cezmi", ""], ["Leblanc", "Emilie", ""], ["Kline", "Aaron", ""], ["Hou", "Cathy", ""], ["Chrisman", "Brianna", ""], ["Stockham", "Nate", ""], ["Paskov", "Kelley", ""], ["Voss", "Catalin", ""], ["Haber", "Nick", ""], ["Wall", "Dennis", ""]]}, {"id": "2101.03478", "submitter": "Peter Washington", "authors": "Peter Washington, Aaron Kline, Onur Cezmi Mutlu, Emilie Leblanc, Cathy\n  Hou, Nate Stockham, Kelley Paskov, Brianna Chrisman, Dennis P. Wall", "title": "Activity Recognition with Moving Cameras and Few Training Examples:\n  Applications for Detection of Autism-Related Headbanging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Activity recognition computer vision algorithms can be used to detect the\npresence of autism-related behaviors, including what are termed \"restricted and\nrepetitive behaviors\", or stimming, by diagnostic instruments. The limited data\nthat exist in this domain are usually recorded with a handheld camera which can\nbe shaky or even moving, posing a challenge for traditional feature\nrepresentation approaches for activity detection which mistakenly capture the\ncamera's motion as a feature. To address these issues, we first document the\nadvantages and limitations of current feature representation techniques for\nactivity recognition when applied to head banging detection. We then propose a\nfeature representation consisting exclusively of head pose keypoints. We create\na computer vision classifier for detecting head banging in home videos using a\ntime-distributed convolutional neural network (CNN) in which a single CNN\nextracts features from each frame in the input sequence, and these extracted\nfeatures are fed as input to a long short-term memory (LSTM) network. On the\nbinary task of predicting head banging and no head banging within videos from\nthe Self Stimulatory Behaviour Dataset (SSBD), we reach a mean F1-score of\n90.77% using 3-fold cross validation (with individual fold F1-scores of 83.3%,\n89.0%, and 100.0%) when ensuring that no child who appeared in the train set\nwas in the test set for all folds. This work documents a successful technique\nfor training a computer vision classifier which can detect human motion with\nfew training examples and even when the camera recording the source clips is\nunstable. The general methods described here can be applied by designers and\ndevelopers of interactive systems towards other human motion and pose\nclassification problems used in mobile and ubiquitous interactive systems.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 05:37:05 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Washington", "Peter", ""], ["Kline", "Aaron", ""], ["Mutlu", "Onur Cezmi", ""], ["Leblanc", "Emilie", ""], ["Hou", "Cathy", ""], ["Stockham", "Nate", ""], ["Paskov", "Kelley", ""], ["Chrisman", "Brianna", ""], ["Wall", "Dennis P.", ""]]}, {"id": "2101.03492", "submitter": "Yuansheng Hua", "authors": "Yuansheng Hua, Diego Marcos, Lichao Mou, Xiao Xiang Zhu, Devis Tuia", "title": "Semantic Segmentation of Remote Sensing Images with Sparse Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Convolutional Neural Networks (CNNs) for very high resolution images\nrequires a large quantity of high-quality pixel-level annotations, which is\nextremely labor- and time-consuming to produce. Moreover, professional photo\ninterpreters might have to be involved for guaranteeing the correctness of\nannotations. To alleviate such a burden, we propose a framework for semantic\nsegmentation of aerial images based on incomplete annotations, where annotators\nare asked to label a few pixels with easy-to-draw scribbles. To exploit these\nsparse scribbled annotations, we propose the FEature and Spatial relaTional\nregulArization (FESTA) method to complement the supervised task with an\nunsupervised learning signal that accounts for neighbourhood structures both in\nspatial and feature terms.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 07:21:21 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Hua", "Yuansheng", ""], ["Marcos", "Diego", ""], ["Mou", "Lichao", ""], ["Zhu", "Xiao Xiang", ""], ["Tuia", "Devis", ""]]}, {"id": "2101.03503", "submitter": "Alireza Sepas-Moghaddam", "authors": "Alireza Sepas-Moghaddam, Ali Etemad, Fernando Pereira, Paulo Lobato\n  Correia", "title": "CapsField: Light Field-based Face and Expression Recognition in the Wild\n  using Capsule Routing", "comments": "Accepted in IEEE Transactions on Image Processing (IEEE T-IP)", "journal-ref": null, "doi": "10.1109/TIP.2021.3054476", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Light field (LF) cameras provide rich spatio-angular visual representations\nby sensing the visual scene from multiple perspectives and have recently\nemerged as a promising technology to boost the performance of human-machine\nsystems such as biometrics and affective computing. Despite the significant\nsuccess of LF representation for constrained facial image analysis, this\ntechnology has never been used for face and expression recognition in the wild.\nIn this context, this paper proposes a new deep face and expression recognition\nsolution, called CapsField, based on a convolutional neural network and an\nadditional capsule network that utilizes dynamic routing to learn hierarchical\nrelations between capsules. CapsField extracts the spatial features from facial\nimages and learns the angular part-whole relations for a selected set of 2D\nsub-aperture images rendered from each LF image. To analyze the performance of\nthe proposed solution in the wild, the first in the wild LF face dataset, along\nwith a new complementary constrained face dataset captured from the same\nsubjects recorded earlier have been captured and are made available. A subset\nof the in the wild dataset contains facial images with different expressions,\nannotated for usage in the context of face expression recognition tests. An\nextensive performance assessment study using the new datasets has been\nconducted for the proposed and relevant prior solutions, showing that the\nCapsField proposed solution achieves superior performance for both face and\nexpression recognition tasks when compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 09:06:02 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Sepas-Moghaddam", "Alireza", ""], ["Etemad", "Ali", ""], ["Pereira", "Fernando", ""], ["Correia", "Paulo Lobato", ""]]}, {"id": "2101.03531", "submitter": "Shoaib Azam", "authors": "Shoaib Azam, Farzeen Munir and Moongu Jeon", "title": "Channel Boosting Feature Ensemble for Radar-based Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous vehicles are conceived to provide safe and secure services by\nvalidating the safety standards as indicated by SOTIF-ISO/PAS-21448 (Safety of\nthe intended functionality). Keeping in this context, the perception of the\nenvironment plays an instrumental role in conjunction with localization,\nplanning and control modules. As a pivotal algorithm in the perception stack,\nobject detection provides extensive insights into the autonomous vehicle's\nsurroundings. Camera and Lidar are extensively utilized for object detection\namong different sensor modalities, but these exteroceptive sensors have\nlimitations in resolution and adverse weather conditions. In this work,\nradar-based object detection is explored provides a counterpart sensor modality\nto be deployed and used in adverse weather conditions. The radar gives complex\ndata; for this purpose, a channel boosting feature ensemble method with\ntransformer encoder-decoder network is proposed. The object detection task\nusing radar is formulated as a set prediction problem and evaluated on the\npublicly available dataset in both good and good-bad weather conditions. The\nproposed method's efficacy is extensively evaluated using the COCO evaluation\nmetric, and the best-proposed model surpasses its state-of-the-art counterpart\nmethod by $12.55\\%$ and $12.48\\%$ in both good and good-bad weather conditions.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 12:20:58 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Azam", "Shoaib", ""], ["Munir", "Farzeen", ""], ["Jeon", "Moongu", ""]]}, {"id": "2101.03541", "submitter": "Fabian Amherd", "authors": "Fabian Amherd, Elias Rodriguez", "title": "Heatmap-based Object Detection and Tracking with a Fully Convolutional\n  Neural Network", "comments": "30 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main topic of this paper is a brief overview of the field of Artificial\nIntelligence. The core of this paper is a practical implementation of an\nalgorithm for object detection and tracking. The ability to detect and track\nfast-moving objects is crucial for various applications of Artificial\nIntelligence like autonomous driving, ball tracking in sports, robotics or\nobject counting. As part of this paper the Fully Convolutional Neural Network\n\"CueNet\" was developed. It detects and tracks the cueball on a labyrinth game\nrobustly and reliably. While CueNet V1 has a single input image, the approach\nwith CueNet V2 was to take three consecutive 240 x 180-pixel images as an input\nand transform them into a probability heatmap for the cueball's location. The\nnetwork was tested with a separate video that contained all sorts of\ndistractions to test its robustness. When confronted with our testing data,\nCueNet V1 predicted the correct cueball location in 99.6% of all frames, while\nCueNet V2 had 99.8% accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:13:38 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Amherd", "Fabian", ""], ["Rodriguez", "Elias", ""]]}, {"id": "2101.03549", "submitter": "Koby Bibas", "authors": "Koby Bibas, Gili Weiss-Dicker, Dana Cohen, Noa Cahan, Hayit Greenspan", "title": "Learning Rotation Invariant Features for Cryogenic Electron Microscopy\n  Image Reconstruction", "comments": "Accepted IEEE-ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cryo-Electron Microscopy (Cryo-EM) is a Nobel prize-winning technology for\ndetermining the 3D structure of particles at near-atomic resolution. A\nfundamental step in the recovering of the 3D single-particle structure is to\nalign its 2D projections; thus, the construction of a canonical representation\nwith a fixed rotation angle is required. Most approaches use discrete\nclustering which fails to capture the continuous nature of image rotation,\nothers suffer from low-quality image reconstruction. We propose a novel method\nthat leverages the recent development in the generative adversarial networks.\nWe introduce an encoder-decoder with a rotation angle classifier. In addition,\nwe utilize a discriminator on the decoder output to minimize the reconstruction\nerror. We demonstrate our approach with the Cryo-EM 5HDB and the rotated MNIST\ndatasets showing substantial improvement over recent methods.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:38:16 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bibas", "Koby", ""], ["Weiss-Dicker", "Gili", ""], ["Cohen", "Dana", ""], ["Cahan", "Noa", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2101.03588", "submitter": "Ibrahim Jubran", "authors": "Ibrahim Jubran, Alaa Maalouf, Ron Kimmel, Dan Feldman", "title": "Provably Approximated ICP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the \\emph{alignment problem} is to align a (given) point cloud $P\n= \\{p_1,\\cdots,p_n\\}$ to another (observed) point cloud $Q =\n\\{q_1,\\cdots,q_n\\}$. That is, to compute a rotation matrix $R \\in \\mathbb{R}^{3\n\\times 3}$ and a translation vector $t \\in \\mathbb{R}^{3}$ that minimize the\nsum of paired distances $\\sum_{i=1}^n D(Rp_i-t,q_i)$ for some distance function\n$D$. A harder version is the \\emph{registration problem}, where the\ncorrespondence is unknown, and the minimum is also over all possible\ncorrespondence functions from $P$ to $Q$. Heuristics such as the Iterative\nClosest Point (ICP) algorithm and its variants were suggested for these\nproblems, but none yield a provable non-trivial approximation for the global\noptimum.\n  We prove that there \\emph{always} exists a \"witness\" set of $3$ pairs in $P\n\\times Q$ that, via novel alignment algorithm, defines a constant factor\napproximation (in the worst case) to this global optimum. We then provide\nalgorithms that recover this witness set and yield the first provable constant\nfactor approximation for the: (i) alignment problem in $O(n)$ expected time,\nand (ii) registration problem in polynomial time. Such small witness sets exist\nfor many variants including points in $d$-dimensional space, outlier-resistant\ncost functions, and different correspondence types.\n  Extensive experimental results on real and synthetic datasets show that our\napproximation constants are, in practice, close to $1$, and up to x$10$ times\nsmaller than state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 18:09:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Jubran", "Ibrahim", ""], ["Maalouf", "Alaa", ""], ["Kimmel", "Ron", ""], ["Feldman", "Dan", ""]]}, {"id": "2101.03603", "submitter": "Isaac Sledge", "authors": "Isaac J. Sledge, Matthew S. Emigh, Jonathan L. King, Denton L. Woods,\n  J. Tory Cobb, Jose C. Principe", "title": "Target Detection and Segmentation in Circular-Scan\n  Synthetic-Aperture-Sonar Images using Semi-Supervised Convolutional\n  Encoder-Decoders", "comments": "Submitted to IEEE JOE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a saliency-based, multi-target detection and segmentation\nframework for multi-aspect, semi-coherent imagery formed from circular-scan,\nsynthetic-aperture sonar (CSAS). Our framework relies on a multi-branch,\nconvolutional encoder-decoder network (MB-CEDN). The encoder portion extracts\nfeatures from one or more CSAS images of the targets. These features are then\nsplit off and fed into multiple decoders that perform pixel-level\nclassification on the extracted features to roughly mask the target in an\nunsupervised-trained manner and detect foreground and background pixels in a\nsupervised-trained manner. Each of these target-detection estimates provide\ndifferent perspectives as to what constitute a target. These opinions are\ncascaded into a deep-parsing network to model contextual and spatial\nconstraints that help isolate targets better than either solution estimate\nalone.\n  We evaluate our framework using real-world CSAS data with five broad target\nclasses. Since we are the first to consider both CSAS target detection and\nsegmentation, we adapt existing image and video-processing network topologies\nfrom the literature for comparative purposes. We show that our framework\noutperforms supervised deep networks. It greatly outperforms state-of-the-art\nunsupervised approaches for diverse target and seafloor types.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 18:58:45 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 16:36:41 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Sledge", "Isaac J.", ""], ["Emigh", "Matthew S.", ""], ["King", "Jonathan L.", ""], ["Woods", "Denton L.", ""], ["Cobb", "J. Tory", ""], ["Principe", "Jose C.", ""]]}, {"id": "2101.03604", "submitter": "Indraneel Ghosh", "authors": "Indraneel Ghosh, Siddhant Kundu", "title": "Combining Neural Network Models for Blood Cell Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of the study is to evaluate the efficiency of a multi layer\nneural network models built by combining Recurrent Neural Network(RNN) and\nConvolutional Neural Network(CNN) for solving the problem of classifying\ndifferent types of White Blood Cells. This can have applications in the\npharmaceutical and healthcare industry for automating the analysis of blood\ntests and other processes requiring identifying the nature of blood cells in a\ngiven image sample. It can also be used in the diagnosis of various\nblood-related diseases in patients.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 18:58:46 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ghosh", "Indraneel", ""], ["Kundu", "Siddhant", ""]]}, {"id": "2101.03682", "submitter": "Juan Leon Alcazar", "authors": "Juan Le\\'on-Alc\\'azar, Fabian Caba Heilbron, Ali Thabet, and Bernard\n  Ghanem", "title": "MAAS: Multi-modal Assignation for Active Speaker Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active speaker detection requires a solid integration of multi-modal cues.\nWhile individual modalities can approximate a solution, accurate predictions\ncan only be achieved by explicitly fusing the audio and visual features and\nmodeling their temporal progression. Despite its inherent muti-modal nature,\ncurrent methods still focus on modeling and fusing short-term audiovisual\nfeatures for individual speakers, often at frame level. In this paper we\npresent a novel approach to active speaker detection that directly addresses\nthe multi-modal nature of the problem, and provides a straightforward strategy\nwhere independent visual features from potential speakers in the scene are\nassigned to a previously detected speech event. Our experiments show that, an\nsmall graph data structure built from a single frame, allows to approximate an\ninstantaneous audio-visual assignment problem. Moreover, the temporal extension\nof this initial graph achieves a new state-of-the-art on the AVA-ActiveSpeaker\ndataset with a mAP of 88.8\\%.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 02:57:25 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Le\u00f3n-Alc\u00e1zar", "Juan", ""], ["Heilbron", "Fabian Caba", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2101.03694", "submitter": "Gengshan Yang", "authors": "Gengshan Yang and Deva Ramanan", "title": "Learning to Segment Rigid Motions from Two Frames", "comments": "code will be released at https://github.com/gengshan-y/rigidmask", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Appearance-based detectors achieve remarkable performance on common scenes,\nbut tend to fail for scenarios lack of training data. Geometric motion\nsegmentation algorithms, however, generalize to novel scenes, but have yet to\nachieve comparable performance to appearance-based ones, due to noisy motion\nestimations and degenerate motion configurations. To combine the best of both\nworlds, we propose a modular network, whose architecture is motivated by a\ngeometric analysis of what independent object motions can be recovered from an\negomotion field. It takes two consecutive frames as input and predicts\nsegmentation masks for the background and multiple rigidly moving objects,\nwhich are then parameterized by 3D rigid transformations. Our method achieves\nstate-of-the-art performance for rigid motion segmentation on KITTI and Sintel.\nThe inferred rigid motions lead to a significant improvement for depth and\nscene flow estimation. At the time of submission, our method ranked 1st on\nKITTI scene flow leaderboard, out-performing the best published method (scene\nflow error: 4.89% vs 6.31%).\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 04:20:30 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yang", "Gengshan", ""], ["Ramanan", "Deva", ""]]}, {"id": "2101.03697", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding,\n  Jian Sun", "title": "RepVGG: Making VGG-style ConvNets Great Again", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple but powerful architecture of convolutional neural\nnetwork, which has a VGG-like inference-time body composed of nothing but a\nstack of 3x3 convolution and ReLU, while the training-time model has a\nmulti-branch topology. Such decoupling of the training-time and inference-time\narchitecture is realized by a structural re-parameterization technique so that\nthe model is named RepVGG. On ImageNet, RepVGG reaches over 80% top-1 accuracy,\nwhich is the first time for a plain model, to the best of our knowledge. On\nNVIDIA 1080Ti GPU, RepVGG models run 83% faster than ResNet-50 or 101% faster\nthan ResNet-101 with higher accuracy and show favorable accuracy-speed\ntrade-off compared to the state-of-the-art models like EfficientNet and RegNet.\nThe code and trained models are available at\nhttps://github.com/megvii-model/RepVGG.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 04:46:11 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 15:02:08 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 13:02:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ding", "Xiaohan", ""], ["Zhang", "Xiangyu", ""], ["Ma", "Ningning", ""], ["Han", "Jungong", ""], ["Ding", "Guiguang", ""], ["Sun", "Jian", ""]]}, {"id": "2101.03710", "submitter": "Ki Beom Hong", "authors": "Kibeom Hong, Youngjung Uh, Hyeran Byun", "title": "ArrowGAN : Learning to Generate Videos by Learning Arrow of Time", "comments": "Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Training GANs on videos is even more sophisticated than on images because\nvideos have a distinguished dimension: time. While recent methods designed a\ndedicated architecture considering time, generated videos are still far from\nindistinguishable from real videos. In this paper, we introduce ArrowGAN\nframework, where the discriminators learns to classify arrow of time as an\nauxiliary task and the generators tries to synthesize forward-running videos.\nWe argue that the auxiliary task should be carefully chosen regarding the\ntarget domain. In addition, we explore categorical ArrowGAN with recent\ntechniques in conditional image generation upon ArrowGAN framework, achieving\nthe state-of-the-art performance on categorical video generation. Our extensive\nexperiments validate the effectiveness of arrow of time as a self-supervisory\ntask, and demonstrate that all our components of categorical ArrowGAN lead to\nthe improvement regarding video inception score and Frechet video distance on\nthree datasets: Weizmann, UCFsports, and UCF-101.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:38:19 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Hong", "Kibeom", ""], ["Uh", "Youngjung", ""], ["Byun", "Hyeran", ""]]}, {"id": "2101.03711", "submitter": "Zhendong Liu", "authors": "Zhendong Liu, Xiaoqiong Huang, Xin Yang, Rui Gao, Rui Li, Yuanji\n  Zhang, Yankai Huang, Guangquan Zhou, Yi Xiong, Alejandro F Frangi, Dong Ni", "title": "Generalize Ultrasound Image Segmentation via Instant and Plug & Play\n  Style Transfer", "comments": "Accepted by IEEE ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep segmentation models that generalize to images with unknown appearance\nare important for real-world medical image analysis. Retraining models leads to\nhigh latency and complex pipelines, which are impractical in clinical settings.\nThe situation becomes more severe for ultrasound image analysis because of\ntheir large appearance shifts. In this paper, we propose a novel method for\nrobust segmentation under unknown appearance shifts. Our contribution is\nthree-fold. First, we advance a one-stage plug-and-play solution by embedding\nhierarchical style transfer units into a segmentation architecture. Our\nsolution can remove appearance shifts and perform segmentation simultaneously.\nSecond, we adopt Dynamic Instance Normalization to conduct precise and dynamic\nstyle transfer in a learnable manner, rather than previously fixed style\nnormalization. Third, our solution is fast and lightweight for routine clinical\nadoption. Given 400*400 image input, our solution only needs an additional\n0.2ms and 1.92M FLOPs to handle appearance shifts compared to the baseline\npipeline. Extensive experiments are conducted on a large dataset from three\nvendors demonstrate our proposed method enhances the robustness of deep\nsegmentation models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:45:30 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Liu", "Zhendong", ""], ["Huang", "Xiaoqiong", ""], ["Yang", "Xin", ""], ["Gao", "Rui", ""], ["Li", "Rui", ""], ["Zhang", "Yuanji", ""], ["Huang", "Yankai", ""], ["Zhou", "Guangquan", ""], ["Xiong", "Yi", ""], ["Frangi", "Alejandro F", ""], ["Ni", "Dong", ""]]}, {"id": "2101.03713", "submitter": "Kunpeng Li", "authors": "Kunpeng Li, Zizhao Zhang, Guanhang Wu, Xuehan Xiong, Chen-Yu Lee,\n  Zhichao Lu, Yun Fu, Tomas Pfister", "title": "Learning from Weakly-labeled Web Videos via Exploring Sub-Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning visual knowledge from massive weakly-labeled web videos has\nattracted growing research interests thanks to the large corpus of easily\naccessible video data on the Internet. However, for video action recognition,\nthe action of interest might only exist in arbitrary clips of untrimmed web\nvideos, resulting in high label noises in the temporal space. To address this\nissue, we introduce a new method for pre-training video action recognition\nmodels using queried web videos. Instead of trying to filter out, we propose to\nconvert the potential noises in these queried videos to useful supervision\nsignals by defining the concept of Sub-Pseudo Label (SPL). Specifically, SPL\nspans out a new set of meaningful \"middle ground\" label space constructed by\nextrapolating the original weak labels during video querying and the prior\nknowledge distilled from a teacher model. Consequently, SPL provides enriched\nsupervision for video models to learn better representations. SPL is fairly\nsimple and orthogonal to popular teacher-student self-training frameworks\nwithout extra training cost. We validate the effectiveness of our method on\nfour video action recognition datasets and a weakly-labeled image dataset to\nstudy the generalization ability. Experiments show that SPL outperforms several\nexisting pre-training strategies using pseudo-labels and the learned\nrepresentations lead to competitive results when fine-tuning on HMDB-51 and\nUCF-101 compared with recent pre-training methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:50:16 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Li", "Kunpeng", ""], ["Zhang", "Zizhao", ""], ["Wu", "Guanhang", ""], ["Xiong", "Xuehan", ""], ["Lee", "Chen-Yu", ""], ["Lu", "Zhichao", ""], ["Fu", "Yun", ""], ["Pfister", "Tomas", ""]]}, {"id": "2101.03747", "submitter": "Yuan Yuan Ding", "authors": "Yuanyuan Ding and Junchi Yan and Guoqiang Hu and Jun Zhu", "title": "Cognitive Visual Inspection Service for LCD Manufacturing Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of display devices, quality inspection via machine\nvision technology has become increasingly important for flat-panel displays\n(FPD) industry. This paper discloses a novel visual inspection system for\nliquid crystal display (LCD), which is currently a dominant type in the FPD\nindustry. The system is based on two cornerstones: robust/high-performance\ndefect recognition model and cognitive visual inspection service architecture.\nA hybrid application of conventional computer vision technique and the latest\ndeep convolutional neural network (DCNN) leads to an integrated defect\ndetection, classfication and impact evaluation model that can be economically\ntrained with only image-level class annotations to achieve a high inspection\naccuracy. In addition, the properly trained model is robust to the variation of\nthe image qulity, significantly alleviating the dependency between the model\nprediction performance and the image aquisition environment. This in turn\njustifies the decoupling of the defect recognition functions from the front-end\ndevice to the back-end serivce, motivating the design and realization of the\ncognitive visual inspection service architecture. Empirical case study is\nperformed on a large-scale real-world LCD dataset from a manufacturing line\nwith different layers and products, which shows the promising utility of our\nsystem, which has been deployed in a real-world LCD manufacturing line from a\nmajor player in the world.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 08:14:35 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ding", "Yuanyuan", ""], ["Yan", "Junchi", ""], ["Hu", "Guoqiang", ""], ["Zhu", "Jun", ""]]}, {"id": "2101.03749", "submitter": "Junjun Jiang", "authors": "Junjun Jiang, Chenyang Wang, Xianming Liu, and Jiayi Ma", "title": "Deep Learning-based Face Super-resolution: A Survey", "comments": "40 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face super-resolution, also known as face hallucination, which is aimed at\nenhancing the resolution of low-resolution (LR) one or a sequence of face\nimages to generate the corresponding high-resolution (HR) face images, is a\ndomain-specific image super-resolution problem. Recently, face super-resolution\nhas received considerable attention, and witnessed dazzling advances with deep\nlearning techniques. To date, few summaries of the studies on the deep\nlearning-based face super-resolution are available. In this survey, we present\na comprehensive review of deep learning techniques in face super-resolution in\na systematic manner. First, we summarize the problem formulation of face\nsuper-resolution. Second, we compare the differences between generic image\nsuper-resolution and face super-resolution. Third, datasets and performance\nmetrics commonly used in facial hallucination are presented. Fourth, we roughly\ncategorize existing methods according to the utilization of face-specific\ninformation. In each category, we start with a general description of design\nprinciples, present an overview of representative approaches, and compare the\nsimilarities and differences among various methods. Finally, we envision\nprospects for further technical advancement in this field.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 08:17:11 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Jiang", "Junjun", ""], ["Wang", "Chenyang", ""], ["Liu", "Xianming", ""], ["Ma", "Jiayi", ""]]}, {"id": "2101.03771", "submitter": "Savvas A Chatzichristofis", "authors": "Socratis Gkelios, Yiannis Boutalis, Savvas A. Chatzichristofis", "title": "Investigating the Vision Transformer Model for Image Retrieval Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a plug-and-play descriptor that can be effectively\nadopted for image retrieval tasks without prior initialization or preparation.\nThe description method utilizes the recently proposed Vision Transformer\nnetwork while it does not require any training data to adjust parameters. In\nimage retrieval tasks, the use of Handcrafted global and local descriptors has\nbeen very successfully replaced, over the last years, by the Convolutional\nNeural Networks (CNN)-based methods. However, the experimental evaluation\nconducted in this paper on several benchmarking datasets against 36\nstate-of-the-art descriptors from the literature demonstrates that a neural\nnetwork that contains no convolutional layer, such as Vision Transformer, can\nshape a global descriptor and achieve competitive results. As fine-tuning is\nnot required, the presented methodology's low complexity encourages adoption of\nthe architecture as an image retrieval baseline model, replacing the\ntraditional and well adopted CNN-based approaches and inaugurating a new era in\nimage retrieval approaches.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 08:59:54 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Gkelios", "Socratis", ""], ["Boutalis", "Yiannis", ""], ["Chatzichristofis", "Savvas A.", ""]]}, {"id": "2101.03783", "submitter": "Renhao Sun", "authors": "Renhao Sun, Yang Wang, Zhao Zhang, Richang Hong, and Meng Wang", "title": "Deep Adversarial Inconsistent Cognitive Sampling for Multi-view\n  Progressive Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep multi-view clustering methods have achieved remarkable performance.\nHowever, all of them failed to consider the difficulty labels (uncertainty of\nground-truth for training samples) over multi-view samples, which may result\ninto a nonideal clustering network for getting stuck into poor local optima\nduring training process; worse still, the difficulty labels from multi-view\nsamples are always inconsistent, such fact makes it even more challenging to\nhandle. In this paper, we propose a novel Deep Adversarial Inconsistent\nCognitive Sampling (DAICS) method for multi-view progressive subspace\nclustering. A multiview binary classification (easy or difficult) loss and a\nfeature similarity loss are proposed to jointly learn a binary classifier and a\ndeep consistent feature embedding network, throughout an adversarial minimax\ngame over difficulty labels of multiview consistent samples. We develop a\nmulti-view cognitive sampling strategy to select the input samples from easy to\ndifficult for multi-view clustering network training. However, the\ndistributions of easy and difficult samples are mixed together, hence not\ntrivial to achieve the goal. To resolve it, we define a sampling probability\nwith theoretical guarantee. Based on that, a golden section mechanism is\nfurther designed to generate a sample set boundary to progressively select the\nsamples with varied difficulty labels via a gate unit, which is utilized to\njointly learn a multi-view common progressive subspace and clustering network\nfor more efficient clustering. Experimental results on four real-world datasets\ndemonstrate the superiority of DAICS over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 09:32:34 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 04:55:26 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Sun", "Renhao", ""], ["Wang", "Yang", ""], ["Zhang", "Zhao", ""], ["Hong", "Richang", ""], ["Wang", "Meng", ""]]}, {"id": "2101.03787", "submitter": "Nour Karessli", "authors": "Hazel Doughty, Nour Karessli, Kathryn Leonard, Boyi Li, Carianne\n  Martinez, Azadeh Mobasher, Arsha Nagrani, Srishti Yadav", "title": "WiCV 2020: The Seventh Women In Computer Vision Workshop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the details of Women in Computer Vision Workshop -\nWiCV 2020, organized in alongside virtual CVPR 2020. This event aims at\nencouraging the women researchers in the field of computer vision. It provides\na voice to a minority (female) group in computer vision community and focuses\non increasingly the visibility of these researchers, both in academia and\nindustry. WiCV believes that such an event can play an important role in\nlowering the gender imbalance in the field of computer vision. WiCV is\norganized each year where it provides a.) opportunity for collaboration with\nbetween researchers b.) mentorship to female junior researchers c.) financial\nsupport to presenters to overcome monetary burden and d.) large and diverse\nchoice of role models, who can serve as examples to younger researchers at the\nbeginning of their careers. In this paper, we present a report on the workshop\nprogram, trends over the past years, a summary of statistics regarding\npresenters, attendees, and sponsorship for the current workshop.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 09:48:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Doughty", "Hazel", ""], ["Karessli", "Nour", ""], ["Leonard", "Kathryn", ""], ["Li", "Boyi", ""], ["Martinez", "Carianne", ""], ["Mobasher", "Azadeh", ""], ["Nagrani", "Arsha", ""], ["Yadav", "Srishti", ""]]}, {"id": "2101.03793", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl and Nikolai Sanamrad and Enkelejda Kasneci", "title": "The Gaze and Mouse Signal as additional Source for User Fingerprints in\n  Browser Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we inspect different data sources for browser fingerprints. We\nshow which disadvantages and limitations browser statistics have and how this\ncan be avoided with other data sources. Since human visual behavior is a rich\nsource of information and also contains person specific information, it is a\nvaluable source for browser fingerprints. However, human gaze acquisition in\nthe browser also has disadvantages, such as inaccuracies via webcam and the\nrestriction that the user must first allow access to the camera. However, it is\nalso known that the mouse movements and the human gaze correlate and therefore,\nthe mouse movements can be used instead of the gaze signal. In our evaluation\nwe show the influence of all possible combinations of the three information\nsources for user recognition and describe our simple approach in detail. The\ndata and the Matlab code can be downloaded here\nhttps://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FThe%20Gaze%20and%20Mouse%20Signal%20as%20additional%20Source%20...&mode=list\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 10:04:52 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 12:10:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Sanamrad", "Nikolai", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2101.03799", "submitter": "Felix Denzinger", "authors": "Felix Denzinger, Michael Wels, Christian Hopfgartner, Jing Lu, Max\n  Sch\\\"obinger, Andreas Maier, Michael S\\\"uhling", "title": "Coronary Plaque Analysis for CT Angiography Clinical Research", "comments": "Accepted to BVM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of plaque deposits in the coronary vasculature is an important\ntopic in current clinical research. From a technical side mostly new algorithms\nfor different sub tasks - e.g. centerline extraction or vessel/plaque\nsegmentation - are proposed. However, to enable clinical research with the help\nof these algorithms, a software solution, which enables manual correction,\ncomprehensive visual feedback and tissue analysis capabilities, is needed.\nTherefore, we want to present such an integrated software solution. It is able\nto perform robust automatic centerline extraction and inner and outer vessel\nwall segmentation, while providing easy to use manual correction tools. Also,\nit allows for annotation of lesions along the centerlines, which can be further\nanalyzed regarding their tissue composition. Furthermore, it enables research\nin upcoming technologies and research directions: it does support dual energy\nCT scans with dedicated plaque analysis and the quantification of the fatty\ntissue surrounding the vasculature, also in automated set-ups.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 10:22:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Denzinger", "Felix", ""], ["Wels", "Michael", ""], ["Hopfgartner", "Christian", ""], ["Lu", "Jing", ""], ["Sch\u00f6binger", "Max", ""], ["Maier", "Andreas", ""], ["S\u00fchling", "Michael", ""]]}, {"id": "2101.03806", "submitter": "St\\'ephane Lathuili\\`ere", "authors": "The-Phuc Nguyen, St\\'ephane Lathuili\\`ere, Elisa Ricci", "title": "Multi-Domain Image-to-Image Translation with Adaptive Inference Graph", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we address the problem of multi-domain image-to-image\ntranslation with particular attention paid to computational cost. In\nparticular, current state of the art models require a large and deep model in\norder to handle the visual diversity of multiple domains. In a context of\nlimited computational resources, increasing the network size may not be\npossible. Therefore, we propose to increase the network capacity by using an\nadaptive graph structure. At inference time, the network estimates its own\ngraph by selecting specific sub-networks. Sub-network selection is implemented\nusing Gumbel-Softmax in order to allow end-to-end training. This approach leads\nto an adjustable increase in number of parameters while preserving an almost\nconstant computational cost. Our evaluation on two publicly available datasets\nof facial and painting images shows that our adaptive strategy generates better\nimages with fewer artifacts than literature methods\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 10:47:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Nguyen", "The-Phuc", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Ricci", "Elisa", ""]]}, {"id": "2101.03814", "submitter": "Sten Hanke", "authors": "Josef Steppan and Sten Hanke", "title": "Analysis of skin lesion images with deep learning", "comments": "for source code see: http://github.com/j05t/lesion-analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Skin cancer is the most common cancer worldwide, with melanoma being the\ndeadliest form. Dermoscopy is a skin imaging modality that has shown an\nimprovement in the diagnosis of skin cancer compared to visual examination\nwithout support. We evaluate the current state of the art in the classification\nof dermoscopic images based on the ISIC-2019 Challenge for the classification\nof skin lesions and current literature. Various deep neural network\narchitectures pre-trained on the ImageNet data set are adapted to a combined\ntraining data set comprised of publicly available dermoscopic and clinical\nimages of skin lesions using transfer learning and model fine-tuning. The\nperformance and applicability of these models for the detection of eight\nclasses of skin lesions are examined. Real-time data augmentation, which uses\nrandom rotation, translation, shear, and zoom within specified bounds is used\nto increase the number of available training samples. Model predictions are\nmultiplied by inverse class frequencies and normalized to better approximate\nactual probability distributions. Overall prediction accuracy is further\nincreased by using the arithmetic mean of the predictions of several\nindependently trained models. The best single model has been published as a web\nservice.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 10:58:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Steppan", "Josef", ""], ["Hanke", "Sten", ""]]}, {"id": "2101.03826", "submitter": "Ziyi Liu", "authors": "Ziyi Liu, Jie Yang, Mengchen Lin, Kenneth Kam Fai Lai, Svetlana\n  Yanushkevich, Orly Yadid-Pecht", "title": "WDR FACE: The First Database for Studying Face Detection in Wide Dynamic\n  Range", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Currently, face detection approaches focus on facial information by varying\nspecific parameters including pose, occlusion, lighting, background, race, and\ngender. These studies only utilized the information obtained from low dynamic\nrange images, however, face detection in wide dynamic range (WDR) scenes has\nreceived little attention. To our knowledge, there is no publicly available WDR\ndatabase for face detection research. To facilitate and support future face\ndetection research in the WDR field, we propose the first WDR database for face\ndetection, called WDR FACE, which contains a total of 398 16-bit megapixel\ngrayscale wide dynamic range images collected from 29 subjects. These WDR\nimages (WDRIs) were taken in eight specific WDR scenes. The dynamic range of\n90% images surpasses 60,000:1, and that of 70% images exceeds 65,000:1.\nFurthermore, we show the effect of different face detection procedures on the\nWDRIs in our database. This is done with 25 different tone mapping operators\nand five different face detectors. We provide preliminary experimental results\nof face detection on this unique WDR database.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 11:45:05 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Liu", "Ziyi", ""], ["Yang", "Jie", ""], ["Lin", "Mengchen", ""], ["Lai", "Kenneth Kam Fai", ""], ["Yanushkevich", "Svetlana", ""], ["Yadid-Pecht", "Orly", ""]]}, {"id": "2101.03848", "submitter": "Shen Cai", "authors": "Haikuan Du and Hui Cao and Shen Cai and Junchi Yan and Siyu Zhang", "title": "Spherical Transformer: Adapting Spherical Signal to CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been widely used in various vision\ntasks, e.g. image classification, semantic segmentation, etc. Unfortunately,\nstandard 2D CNNs are not well suited for spherical signals such as panorama\nimages or spherical projections, as the sphere is an unstructured grid. In this\npaper, we present Spherical Transformer which can transform spherical signals\ninto vectors that can be directly processed by standard CNNs such that many\nwell-designed CNNs architectures can be reused across tasks and datasets by\npretraining. To this end, the proposed method first uses locally structured\nsampling methods such as HEALPix to construct a transformer grid by using the\ninformation of spherical points and its adjacent points, and then transforms\nthe spherical signals to the vectors through the grid. By building the\nSpherical Transformer module, we can use multiple CNN architectures directly.\nWe evaluate our approach on the tasks of spherical MNIST recognition, 3D object\nclassification and omnidirectional image semantic segmentation. For 3D object\nclassification, we further propose a rendering-based projection method to\nimprove the performance and a rotational-equivariant model to improve the\nanti-rotation ability. Experimental results on three tasks show that our\napproach achieves superior performance over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 12:33:16 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 10:57:25 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Du", "Haikuan", ""], ["Cao", "Hui", ""], ["Cai", "Shen", ""], ["Yan", "Junchi", ""], ["Zhang", "Siyu", ""]]}, {"id": "2101.03904", "submitter": "Pichao Wang", "authors": "Xiangyu Li and Yonghong Hou and Pichao Wang and Zhimin Gao and\n  Mingliang Xu and Wanqing Li", "title": "Trear: Transformer-based RGB-D Egocentric Action Recognition", "comments": "Accepted by IEEE Transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a \\textbf{Tr}ansformer-based RGB-D\n\\textbf{e}gocentric \\textbf{a}ction \\textbf{r}ecognition framework, called\nTrear. It consists of two modules, inter-frame attention encoder and\nmutual-attentional fusion block. Instead of using optical flow or recurrent\nunits, we adopt self-attention mechanism to model the temporal structure of the\ndata from different modalities. Input frames are cropped randomly to mitigate\nthe effect of the data redundancy. Features from each modality are interacted\nthrough the proposed fusion block and combined through a simple yet effective\nfusion operation to produce a joint RGB-D representation. Empirical experiments\non two large egocentric RGB-D datasets, THU-READ and FPHA, and one small\ndataset, WCVS, have shown that the proposed method outperforms the\nstate-of-the-art results by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 19:59:30 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Li", "Xiangyu", ""], ["Hou", "Yonghong", ""], ["Wang", "Pichao", ""], ["Gao", "Zhimin", ""], ["Xu", "Mingliang", ""], ["Li", "Wanqing", ""]]}, {"id": "2101.03919", "submitter": "Sandareka Wickramanayake", "authors": "Sandareka Wickramanayake, Wynne Hsu, Mong Li Lee", "title": "Comprehensible Convolutional Neural Networks via Guided Concept Learning", "comments": "Accepted to IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning concepts that are consistent with human perception is important for\nDeep Neural Networks to win end-user trust. Post-hoc interpretation methods\nlack transparency in the feature representations learned by the models. This\nwork proposes a guided learning approach with an additional concept layer in a\nCNN- based architecture to learn the associations between visual features and\nword phrases. We design an objective function that optimizes both prediction\naccuracy and semantics of the learned feature representations. Experiment\nresults demonstrate that the proposed model can learn concepts that are\nconsistent with human perception and their corresponding contributions to the\nmodel decision without compromising accuracy. Further, these learned concepts\nare transferable to new classes of objects that have similar concepts.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 14:35:16 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 16:09:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wickramanayake", "Sandareka", ""], ["Hsu", "Wynne", ""], ["Lee", "Mong Li", ""]]}, {"id": "2101.03921", "submitter": "Anugrah Akbar Praramadhan", "authors": "Anugrah Akbar Praramadhan and Guntur Eka Saputra", "title": "Cycle Generative Adversarial Networks Algorithm With Style Transfer For\n  Image Generation", "comments": "in Indonesian language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The biggest challenge faced by a Machine Learning Engineer is the lack of\ndata they have, especially for 2-dimensional images. The image is processed to\nbe trained into a Machine Learning model so that it can recognize patterns in\nthe data and provide predictions. This research is intended to create a\nsolution using the Cycle Generative Adversarial Networks (GANs) algorithm in\novercoming the problem of lack of data. Then use Style Transfer to be able to\ngenerate a new image based on the given style. Based on the results of testing\nthe resulting model has been carried out several improvements, previously the\nloss value of the photo generator: 3.1267, monet style generator: 3.2026, photo\ndiscriminator: 0.6325, and monet style discriminator: 0.6931 to photo\ngenerator: 2.3792, monet style generator: 2.7291, photo discriminator: 0.5956,\nand monet style discriminator: 0.4940. It is hoped that the research will make\nthe application of this solution useful in the fields of Education, Arts,\nInformation Technology, Medicine, Astronomy, Automotive and other important\nfields.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 14:37:25 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Praramadhan", "Anugrah Akbar", ""], ["Saputra", "Guntur Eka", ""]]}, {"id": "2101.03923", "submitter": "Michalis Lazarou Mr", "authors": "Michalis Lazarou, Bo Li, Tania Stathaki", "title": "A novel shape matching descriptor for real-time hand gesture recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art hand gesture recognition methodologies heavily\nrely in the use of machine learning. However there are scenarios that machine\nlearning cannot be applied successfully, for example in situations where data\nis scarce. This is the case when one-to-one matching is required between a\nquery and a dataset of hand gestures where each gesture represents a unique\nclass. In situations where learning algorithms cannot be trained, classic\ncomputer vision techniques such as feature extraction can be used to identify\nsimilarities between objects. Shape is one of the most important features that\ncan be extracted from images, however the most accurate shape matching\nalgorithms tend to be computationally inefficient for real-time applications.\nIn this work we present a novel shape matching methodology for real-time hand\ngesture recognition. Extensive experiments were carried out comparing our\nmethod with other shape matching methods with respect to accuracy and\ncomputational complexity using our own collected hand gesture dataset and a\nmodified version of the MPEG-7 dataset.%that is widely used for comparing 2D\nshape matching algorithms. Our method outperforms the other methods and\nprovides a good combination of accuracy and computational efficiency for\nreal-time applications.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 14:41:57 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 20:37:38 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lazarou", "Michalis", ""], ["Li", "Bo", ""], ["Stathaki", "Tania", ""]]}, {"id": "2101.03924", "submitter": "Andreas B\\\"ar", "authors": "Andreas B\\\"ar, Jonas L\\\"ohdefink, Nikhil Kapoor, Serin J. Varghese,\n  Fabian H\\\"uger, Peter Schlicht, Tim Fingscheidt", "title": "The Vulnerability of Semantic Segmentation Networks to Adversarial\n  Attacks in Autonomous Driving: Enhancing Extensive Environment Sensing", "comments": "IEEE Signal Processing Magazine (Volume: 38, Issue: 1, Jan. 2021),\n  pp. 42 - 52", "journal-ref": null, "doi": "10.1109/MSP.2020.2983666", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling autonomous driving (AD) can be considered one of the biggest\nchallenges in today's technology. AD is a complex task accomplished by several\nfunctionalities, with environment perception being one of its core functions.\nEnvironment perception is usually performed by combining the semantic\ninformation captured by several sensors, i.e., lidar or camera. The semantic\ninformation from the respective sensor can be extracted by using convolutional\nneural networks (CNNs) for dense prediction. In the past, CNNs constantly\nshowed state-of-the-art performance on several vision-related tasks, such as\nsemantic segmentation of traffic scenes using nothing but the red-green-blue\n(RGB) images provided by a camera. Although CNNs obtain state-of-the-art\nperformance on clean images, almost imperceptible changes to the input,\nreferred to as adversarial perturbations, may lead to fatal deception. The goal\nof this article is to illuminate the vulnerability aspects of CNNs used for\nsemantic segmentation with respect to adversarial attacks, and share insights\ninto some of the existing known adversarial defense strategies. We aim to\nclarify the advantages and disadvantages associated with applying CNNs for\nenvironment perception in AD to serve as a motivation for future research in\nthis field.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 14:43:11 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 08:42:00 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["B\u00e4r", "Andreas", ""], ["L\u00f6hdefink", "Jonas", ""], ["Kapoor", "Nikhil", ""], ["Varghese", "Serin J.", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2101.03929", "submitter": "Shaofei Huang", "authors": "Shaofei Huang, Si Liu, Tianrui Hui, Jizhong Han, Bo Li, Jiashi Feng\n  and Shuicheng Yan", "title": "ORDNet: Capturing Omni-Range Dependencies for Scene Parsing", "comments": "Published at TIP", "journal-ref": "IEEE Transactions on Image Processing, 2020, 29: 8251-8263", "doi": "10.1109/TIP.2020.3013142", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to capture dependencies between spatial positions is essential to\nmany visual tasks, especially the dense labeling problems like scene parsing.\nExisting methods can effectively capture long-range dependencies with\nself-attention mechanism while short ones by local convolution. However, there\nis still much gap between long-range and short-range dependencies, which\nlargely reduces the models' flexibility in application to diverse spatial\nscales and relationships in complicated natural scene images. To fill such a\ngap, we develop a Middle-Range (MR) branch to capture middle-range dependencies\nby restricting self-attention into local patches. Also, we observe that the\nspatial regions which have large correlations with others can be emphasized to\nexploit long-range dependencies more accurately, and thus propose a Reweighed\nLong-Range (RLR) branch. Based on the proposed MR and RLR branches, we build an\nOmni-Range Dependencies Network (ORDNet) which can effectively capture short-,\nmiddle- and long-range dependencies. Our ORDNet is able to extract more\ncomprehensive context information and well adapt to complex spatial variance in\nscene images. Extensive experiments show that our proposed ORDNet outperforms\nprevious state-of-the-art methods on three scene parsing benchmarks including\nPASCAL Context, COCO Stuff and ADE20K, demonstrating the superiority of\ncapturing omni-range dependencies in deep models for scene parsing task.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 14:51:11 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Huang", "Shaofei", ""], ["Liu", "Si", ""], ["Hui", "Tianrui", ""], ["Han", "Jizhong", ""], ["Li", "Bo", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2101.03966", "submitter": "Anis Rahman", "authors": "Maryam Qamar Butt and Anis Ur Rahman", "title": "Audiovisual Saliency Prediction in Uncategorized Video Sequences based\n  on Audio-Video Correlation", "comments": "9 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Substantial research has been done in saliency modeling to develop\nintelligent machines that can perceive and interpret their surroundings. But\nexisting models treat videos as merely image sequences excluding any audio\ninformation, unable to cope with inherently varying content. Based on the\nhypothesis that an audiovisual saliency model will be an improvement over\ntraditional saliency models for natural uncategorized videos, this work aims to\nprovide a generic audio/video saliency model augmenting a visual saliency map\nwith an audio saliency map computed by synchronizing low-level audio and visual\nfeatures. The proposed model was evaluated using different criteria against eye\nfixations data for a publicly available DIEM video dataset. The results show\nthat the model outperformed two state-of-the-art visual saliency models.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 14:22:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Butt", "Maryam Qamar", ""], ["Rahman", "Anis Ur", ""]]}, {"id": "2101.04001", "submitter": "Nikhil Kumar Tomar", "authors": "Nikhil Kumar Tomar", "title": "Automatic Polyp Segmentation using Fully Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Colorectal cancer is one of fatal cancer worldwide. Colonoscopy is the\nstandard treatment for examination, localization, and removal of colorectal\npolyps. However, it has been shown that the miss-rate of colorectal polyps\nduring colonoscopy is between 6 to 27%. The use of an automated, accurate, and\nreal-time polyp segmentation during colonoscopy examinations can help the\nclinicians to eliminate missing lesions and prevent further progression of\ncolorectal cancer. The ``Medico automatic polyp segmentation challenge''\nprovides an opportunity to study polyp segmentation and build a fast\nsegmentation model. The challenge organizers provide a Kvasir-SEG dataset to\ntrain the model. Then it is tested on a separate unseen dataset to validate the\nefficiency and speed of the segmentation model. The experiments demonstrate\nthat the model trained on the Kvasir-SEG dataset and tested on an unseen\ndataset achieves a dice coefficient of 0.7801, mIoU of 0.6847, recall of\n0.8077, and precision of 0.8126, demonstrating the generalization ability of\nour model. The model has achieved 80.60 FPS on the unseen dataset with an image\nresolution of $512 \\times 512$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:20:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Tomar", "Nikhil Kumar", ""]]}, {"id": "2101.04028", "submitter": "Guanting Liu", "authors": "Guanting Liu, Yujie Zhong, Sheng Guo, Matthew R. Scott, Weilin Huang", "title": "Unchain the Search Space with Hierarchical Differentiable Architecture\n  Search", "comments": "To appear in AAAI2021. Code is available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable architecture search (DAS) has made great progress in searching\nfor high-performance architectures with reduced computational cost. However,\nDAS-based methods mainly focus on searching for a repeatable cell structure,\nwhich is then stacked sequentially in multiple stages to form the networks.\nThis configuration significantly reduces the search space, and ignores the\nimportance of connections between the cells. To overcome this limitation, in\nthis paper, we propose a Hierarchical Differentiable Architecture Search\n(H-DAS) that performs architecture search both at the cell level and at the\nstage level. Specifically, the cell-level search space is relaxed so that the\nnetworks can learn stage-specific cell structures. For the stage-level search,\nwe systematically study the architectures of stages, including the number of\ncells in each stage and the connections between the cells. Based on insightful\nobservations, we design several search rules and losses, and mange to search\nfor better stage-level architectures. Such hierarchical search space greatly\nimproves the performance of the networks without introducing expensive search\ncost. Extensive experiments on CIFAR10 and ImageNet demonstrate the\neffectiveness of the proposed H-DAS. Moreover, the searched stage-level\narchitectures can be combined with the cell structures searched by existing DAS\nmethods to further boost the performance. Code is available at:\nhttps://github.com/MalongTech/research-HDAS\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:01:43 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 04:00:56 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Liu", "Guanting", ""], ["Zhong", "Yujie", ""], ["Guo", "Sheng", ""], ["Scott", "Matthew R.", ""], ["Huang", "Weilin", ""]]}, {"id": "2101.04034", "submitter": "Xinzi Sun", "authors": "Xinzi Sun, Dechun Wang, Chenxi Zhang, Pengfei Zhang, Zinan Xiong, Yu\n  Cao, Benyuan Liu, Xiaowei Liu, Shuijiao Chen", "title": "Colorectal Polyp Detection in Real-world Scenario: Design and Experiment\n  Study", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal polyps are abnormal tissues growing on the intima of the colon or\nrectum with a high risk of developing into colorectal cancer, the third leading\ncause of cancer death worldwide. Early detection and removal of colon polyps\nvia colonoscopy have proved to be an effective approach to prevent colorectal\ncancer. Recently, various CNN-based computer-aided systems have been developed\nto help physicians detect polyps. However, these systems do not perform well in\nreal-world colonoscopy operations due to the significant difference between\nimages in a real colonoscopy and those in the public datasets. Unlike the\nwell-chosen clear images with obvious polyps in the public datasets, images\nfrom a colonoscopy are often blurry and contain various artifacts such as\nfluid, debris, bubbles, reflection, specularity, contrast, saturation, and\nmedical instruments, with a wide variety of polyps of different sizes, shapes,\nand textures. All these factors pose a significant challenge to effective polyp\ndetection in a colonoscopy. To this end, we collect a private dataset that\ncontains 7,313 images from 224 complete colonoscopy procedures. This dataset\nrepresents realistic operation scenarios and thus can be used to better train\nthe models and evaluate a system's performance in practice. We propose an\nintegrated system architecture to address the unique challenges for polyp\ndetection. Extensive experiments results show that our system can effectively\ndetect polyps in a colonoscopy with excellent performance in real time.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:10:47 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sun", "Xinzi", ""], ["Wang", "Dechun", ""], ["Zhang", "Chenxi", ""], ["Zhang", "Pengfei", ""], ["Xiong", "Zinan", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""], ["Liu", "Xiaowei", ""], ["Chen", "Shuijiao", ""]]}, {"id": "2101.04041", "submitter": "Rapha\\\"el Dang-Nhu", "authors": "Rapha\\\"el Dang-Nhu and Angelika Steger", "title": "Evaluating Disentanglement of Structured Latent Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We design the first multi-layer disentanglement metric operating at all\nhierarchy levels of a structured latent representation, and derive its\ntheoretical properties. Applied to object-centric representations, our metric\nunifies the evaluation of both object separation between latent slots and\ninternal slot disentanglement into a common mathematical framework. It also\naddresses the problematic dependence on segmentation mask sharpness of previous\npixel-level segmentation metrics such as ARI. Perhaps surprisingly, our\nexperimental results show that good ARI values do not guarantee a disentangled\nrepresentation, and that the exclusive focus on this metric has led to\ncounterproductive choices in some previous evaluations. As an additional\ntechnical contribution, we present a new algorithm for obtaining feature\nimportances that handles slot permutation invariance in the representation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:24:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Dang-Nhu", "Rapha\u00ebl", ""], ["Steger", "Angelika", ""]]}, {"id": "2101.04051", "submitter": "Tun Zhu", "authors": "Tun Zhu, Daoxin Zhang, Yao Hu, Tianran Wang, Xiaolong Jiang, Jianke\n  Zhu, Jiawei Li", "title": "Horizontal-to-Vertical Video Conversion", "comments": "Accept by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alongside the prevalence of mobile videos, the general public leans towards\nconsuming vertical videos on hand-held devices. To revitalize the exposure of\nhorizontal contents, we hereby set forth the exploration of automated\nhorizontal-to-vertical (abbreviated as H2V) video conversion with our proposed\nH2V framework, accompanied by an accurately annotated H2V-142K dataset.\nConcretely, H2V framework integrates video shot boundary detection, subject\nselection and multi-object tracking to facilitate the subject-preserving\nconversion, wherein the key is subject selection. To achieve so, we propose a\nRank-SS module that detects human objects, then selects the subject-to-preserve\nvia exploiting location, appearance, and salient cues. Afterward, the framework\nautomatically crops the video around the subject to produce vertical contents\nfrom horizontal sources. To build and evaluate our H2V framework, H2V-142K\ndataset is densely annotated with subject bounding boxes for 125 videos with\n132K frames and 9,500 video covers, upon which we demonstrate superior subject\nselection performance comparing to traditional salient approaches, and exhibit\npromising horizontal-to-vertical conversion performance overall. By publicizing\nthis dataset as well as our approach, we wish to pave the way for more valuable\nendeavors on the horizontal-to-vertical video conversion task.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:37:31 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 15:37:45 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Zhu", "Tun", ""], ["Zhang", "Daoxin", ""], ["Hu", "Yao", ""], ["Wang", "Tianran", ""], ["Jiang", "Xiaolong", ""], ["Zhu", "Jianke", ""], ["Li", "Jiawei", ""]]}, {"id": "2101.04061", "submitter": "Xintao Wang", "authors": "Xintao Wang, Yu Li, Honglun Zhang, Ying Shan", "title": "Towards Real-World Blind Face Restoration with Generative Facial Prior", "comments": "CVPR 2021. Codes: https://github.com/TencentARC/GFPGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind face restoration usually relies on facial priors, such as facial\ngeometry prior or reference prior, to restore realistic and faithful details.\nHowever, very low-quality inputs cannot offer accurate geometric prior while\nhigh-quality references are inaccessible, limiting the applicability in\nreal-world scenarios. In this work, we propose GFP-GAN that leverages rich and\ndiverse priors encapsulated in a pretrained face GAN for blind face\nrestoration. This Generative Facial Prior (GFP) is incorporated into the face\nrestoration process via novel channel-split spatial feature transform layers,\nwhich allow our method to achieve a good balance of realness and fidelity.\nThanks to the powerful generative facial prior and delicate designs, our\nGFP-GAN could jointly restore facial details and enhance colors with just a\nsingle forward pass, while GAN inversion methods require expensive\nimage-specific optimization at inference. Extensive experiments show that our\nmethod achieves superior performance to prior art on both synthetic and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:54:38 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 02:29:27 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wang", "Xintao", ""], ["Li", "Yu", ""], ["Zhang", "Honglun", ""], ["Shan", "Ying", ""]]}, {"id": "2101.04073", "submitter": "Anush Sankaran", "authors": "Anush Sankaran, Olivier Mastropietro, Ehsan Saboori, Yasser Idris,\n  Davis Sawyer, MohammadHossein AskariHemmat, Ghouthi Boukli Hacene", "title": "Deeplite Neutrino: An End-to-End Framework for Constrained Deep Learning\n  Model Optimization", "comments": "\"IAAI Deployed Application Award\", IAAI 2021 @ AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing deep learning-based solutions is becoming a race for training\ndeeper models with a greater number of layers. While a large-size deeper model\ncould provide competitive accuracy, it creates a lot of logistical challenges\nand unreasonable resource requirements during development and deployment. This\nhas been one of the key reasons for deep learning models not being excessively\nused in various production environments, especially in edge devices. There is\nan immediate requirement for optimizing and compressing these deep learning\nmodels, to enable on-device intelligence. In this research, we introduce a\nblack-box framework, Deeplite Neutrino for production-ready optimization of\ndeep learning models. The framework provides an easy mechanism for the\nend-users to provide constraints such as a tolerable drop in accuracy or target\nsize of the optimized models, to guide the whole optimization process. The\nframework is easy to include in an existing production pipeline and is\navailable as a Python Package, supporting PyTorch and Tensorflow libraries. The\noptimization performance of the framework is shown across multiple benchmark\ndatasets and popular deep learning models. Further, the framework is currently\nused in production and the results and testimonials from several clients are\nsummarized.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:07:45 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 14:57:12 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Sankaran", "Anush", ""], ["Mastropietro", "Olivier", ""], ["Saboori", "Ehsan", ""], ["Idris", "Yasser", ""], ["Sawyer", "Davis", ""], ["AskariHemmat", "MohammadHossein", ""], ["Hacene", "Ghouthi Boukli", ""]]}, {"id": "2101.04096", "submitter": "Jeremy Speth", "authors": "Jeremy Speth, Nathan Vance, Patrick Flynn, Kevin Bowyer, Adam Czajka", "title": "Remote Pulse Estimation in the Presence of Face Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote photoplethysmography (rPPG), a family of techniques for monitoring\nblood volume changes, may be especially useful for widespread contactless\nhealth monitoring using face video from consumer-grade visible-light cameras.\nThe COVID-19 pandemic has caused the widespread use of protective face masks.\nWe found that occlusions from cloth face masks increased the mean absolute\nerror of heart rate estimation by more than 80\\% when deploying methods\ndesigned on unmasked faces. We show that augmenting unmasked face videos by\nadding patterned synthetic face masks forces the model to attend to the\nperiocular and forehead regions, improving performance and closing the gap\nbetween masked and unmasked pulse estimation. To our knowledge, this paper is\nthe first to analyse the impact of face masks on the accuracy of pulse\nestimation and offers several novel contributions: (a) 3D CNN-based method\ndesigned for remote photoplethysmography in a presence of face masks, (b) two\npublicly available pulse estimation datasets acquired from 86 unmasked and 61\nmasked subjects, (c) evaluations of handcrafted algorithms and a 3D CNN trained\non videos of unmasked faces and with masks synthetically added, and (d) data\naugmentation method to add a synthetic mask to a face video.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:43:14 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 16:03:21 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Speth", "Jeremy", ""], ["Vance", "Nathan", ""], ["Flynn", "Patrick", ""], ["Bowyer", "Kevin", ""], ["Czajka", "Adam", ""]]}, {"id": "2101.04104", "submitter": "Kripasindhu Sarkar", "authors": "Kripasindhu Sarkar, Dushyant Mehta, Weipeng Xu, Vladislav Golyanik,\n  Christian Theobalt", "title": "Neural Re-Rendering of Humans from a Single Image", "comments": "Published in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human re-rendering from a single image is a starkly under-constrained\nproblem, and state-of-the-art algorithms often exhibit undesired artefacts,\nsuch as over-smoothing, unrealistic distortions of the body parts and garments,\nor implausible changes of the texture. To address these challenges, we propose\na new method for neural re-rendering of a human under a novel user-defined pose\nand viewpoint, given one input image. Our algorithm represents body pose and\nshape as a parametric mesh which can be reconstructed from a single image and\neasily reposed. Instead of a colour-based UV texture map, our approach further\nemploys a learned high-dimensional UV feature map to encode appearance. This\nrich implicit representation captures detailed appearance variation across\nposes, viewpoints, person identities and clothing styles better than learned\ncolour texture maps. The body model with the rendered feature maps is fed\nthrough a neural image-translation network that creates the final rendered\ncolour image. The above components are combined in an end-to-end-trained neural\nnetwork architecture that takes as input a source person image, and images of\nthe parametric body model in the source pose and desired target pose.\nExperimental evaluation demonstrates that our approach produces higher quality\nsingle image re-rendering results than existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:53:47 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sarkar", "Kripasindhu", ""], ["Mehta", "Dushyant", ""], ["Xu", "Weipeng", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""]]}, {"id": "2101.04170", "submitter": "Joseph DiPalma", "authors": "Joseph DiPalma, Arief A. Suriawinata, Laura J. Tafe, Lorenzo\n  Torresani, Saeed Hassanpour", "title": "Resolution-Based Distillation for Efficient Histology Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing deep learning models to analyze histology images has been\ncomputationally challenging, as the massive size of the images causes excessive\nstrain on all parts of the computing pipeline. This paper proposes a novel deep\nlearning-based methodology for improving the computational efficiency of\nhistology image classification. The proposed approach is robust when used with\nimages that have reduced input resolution and can be trained effectively with\nlimited labeled data. Pre-trained on the original high-resolution (HR) images,\nour method uses knowledge distillation (KD) to transfer learned knowledge from\na teacher model to a student model trained on the same images at a much lower\nresolution. To address the lack of large-scale labeled histology image\ndatasets, we perform KD in a self-supervised manner. We evaluate our approach\non two histology image datasets associated with celiac disease (CD) and lung\nadenocarcinoma (LUAD). Our results show that a combination of KD and\nself-supervision allows the student model to approach, and in some cases,\nsurpass the classification accuracy of the teacher, while being much more\nefficient. Additionally, we observe an increase in student classification\nperformance as the size of the unlabeled dataset increases, indicating that\nthere is potential to scale further. For the CD data, our model outperforms the\nHR teacher model, while needing 4 times fewer computations. For the LUAD data,\nour student model results at 1.25x magnification are within 3% of the teacher\nmodel at 10x magnification, with a 64 times computational cost reduction.\nMoreover, our CD outcomes benefit from performance scaling with the use of more\nunlabeled data. For 0.625x magnification, using unlabeled data improves\naccuracy by 4% over the baseline. Thus, our method can improve the feasibility\nof deep learning solutions for digital pathology with standard computational\nhardware.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 20:00:35 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["DiPalma", "Joseph", ""], ["Suriawinata", "Arief A.", ""], ["Tafe", "Laura J.", ""], ["Torresani", "Lorenzo", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "2101.04194", "submitter": "Jenn-Bing Ong", "authors": "Jenn-Bing Ong, Wee-Keong Ng, Ivan Tjuawinata, Chao Li, Jielin Yang,\n  Sai None Myne, Huaxiong Wang, Kwok-Yan Lam, C.-C. Jay Kuo", "title": "Protecting Big Data Privacy Using Randomized Tensor Network\n  Decomposition and Dispersed Tensor Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data privacy is an important issue for organizations and enterprises to\nsecurely outsource data storage, sharing, and computation on clouds / fogs.\nHowever, data encryption is complicated in terms of the key management and\ndistribution; existing secure computation techniques are expensive in terms of\ncomputational / communication cost and therefore do not scale to big data\ncomputation. Tensor network decomposition and distributed tensor computation\nhave been widely used in signal processing and machine learning for\ndimensionality reduction and large-scale optimization. However, the potential\nof distributed tensor networks for big data privacy preservation have not been\nconsidered before, this motivates the current study. Our primary intuition is\nthat tensor network representations are mathematically non-unique, unlinkable,\nand uninterpretable; tensor network representations naturally support a range\nof multilinear operations for compressed and distributed / dispersed\ncomputation. Therefore, we propose randomized algorithms to decompose big data\ninto randomized tensor network representations and analyze the privacy leakage\nfor 1D to 3D data tensors. The randomness mainly comes from the complex\nstructural information commonly found in big data; randomization is based on\ncontrolled perturbation applied to the tensor blocks prior to decomposition.\nThe distributed tensor representations are dispersed on multiple clouds / fogs\nor servers / devices with metadata privacy, this provides both distributed\ntrust and management to seamlessly secure big data storage, communication,\nsharing, and computation. Experiments show that the proposed randomization\ntechniques are helpful for big data anonymization and efficient for big data\nstorage and computation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 22:14:52 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Ong", "Jenn-Bing", ""], ["Ng", "Wee-Keong", ""], ["Tjuawinata", "Ivan", ""], ["Li", "Chao", ""], ["Yang", "Jielin", ""], ["Myne", "Sai None", ""], ["Wang", "Huaxiong", ""], ["Lam", "Kwok-Yan", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2101.04199", "submitter": "Behzad Javaheri", "authors": "Behzad Javaheri", "title": "Where you live matters: a spatial analysis of COVID-19 mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has caused ~ 2 million fatalities. Significant progress\nhas been made in advancing our understanding of the disease process, one of the\nunanswered questions, however, is the anomaly in the case/mortality ratio with\nMexico as a clear example. Herein, this anomaly is explored by spatial analysis\nand whether mortality varies locally according to local factors. To address\nthis, hexagonal cartogram maps (hexbin) used to spatially map COVID-19\nmortality and visualise association with patient-level data on demographics and\npre-existing health conditions. This was further interrogated at local Mexico\nCity level by choropleth mapping. Our data show that the use of hexagonal\ncartograms is a better approach for spatial mapping of COVID-19 data in Mexico\nas it addresses bias in area size and population. We report sex/age-related\nspatial relationship with mortality amongst the Mexican states and a trend\nbetween health conditions and mortality at the state level. Within Mexico City,\nthere is a clear south, north divide with higher mortality in the northern\nmunicipalities. Deceased patients in these northern municipalities have the\nhighest pre-existing health conditions. Taken together, this study provides an\nimproved presentation of COVID-19 mapping in Mexico and demonstrates spatial\ndivergence of the mortality in Mexico.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 21:25:13 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Javaheri", "Behzad", ""]]}, {"id": "2101.04206", "submitter": "Akshay Rangesh", "authors": "Akshay Rangesh, Pranav Maheshwari, Mez Gebre, Siddhesh Mhatre, Vahid\n  Ramezani, Mohan M. Trivedi", "title": "TrackMPNN: A Message Passing Graph Neural Architecture for Multi-Object\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study follows many classical approaches to multi-object tracking (MOT)\nthat model the problem using dynamic graphical data structures, and adapts this\nformulation to make it amenable to modern neural networks. Our main\ncontributions in this work are the creation of a framework based on dynamic\nundirected graphs that represent the data association problem over multiple\ntimesteps, and a message passing graph neural network (MPNN) that operates on\nthese graphs to produce the desired likelihood for every association therein.\nWe also provide solutions and propositions for the computational problems that\nneed to be addressed to create a memory-efficient, real-time, online algorithm\nthat can reason over multiple timesteps, correct previous mistakes, update\nbeliefs, and handle missed/false detections. To demonstrate the efficacy of our\napproach, we only use the 2D box location and object category ID to construct\nthe descriptor for each object instance. Despite this, our model performs on\npar with state-of-the-art approaches that make use of additional sensors, as\nwell as multiple hand-crafted and/or learned features. This illustrates that\ngiven the right problem formulation and model design, raw bounding boxes (and\ntheir kinematics) from any off-the-shelf detector are sufficient to achieve\ncompetitive tracking results on challenging MOT benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 21:52:25 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 00:31:35 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 19:59:10 GMT"}, {"version": "v4", "created": "Fri, 7 May 2021 01:55:19 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Rangesh", "Akshay", ""], ["Maheshwari", "Pranav", ""], ["Gebre", "Mez", ""], ["Mhatre", "Siddhesh", ""], ["Ramezani", "Vahid", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "2101.04215", "submitter": "\\\"Omer S\\\"umer", "authors": "\\\"Omer S\\\"umer, Patricia Goldberg, Sidney D'Mello, Peter Gerjets,\n  Ulrich Trautwein, Enkelejda Kasneci", "title": "Multimodal Engagement Analysis from Facial Videos in the Classroom", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student engagement is a key construct for learning and teaching. While most\nof the literature explored the student engagement analysis on computer-based\nsettings, this paper extends that focus to classroom instruction. To best\nexamine student visual engagement in the classroom, we conducted a study\nutilizing the audiovisual recordings of classes at a secondary school over one\nand a half month's time, acquired continuous engagement labeling per student\n(N=15) in repeated sessions, and explored computer vision methods to classify\nengagement levels from faces in the classroom. We trained deep embeddings for\nattentional and emotional features, training Attention-Net for head pose\nestimation and Affect-Net for facial expression recognition. We additionally\ntrained different engagement classifiers, consisting of Support Vector\nMachines, Random Forest, Multilayer Perceptron, and Long Short-Term Memory, for\nboth features. The best performing engagement classifiers achieved AUCs of .620\nand .720 in Grades 8 and 12, respectively. We further investigated fusion\nstrategies and found score-level fusion either improves the engagement\nclassifiers or is on par with the best performing modality. We also\ninvestigated the effect of personalization and found that using only 60-seconds\nof person-specific data selected by margin uncertainty of the base classifier\nyielded an average AUC improvement of .084. 4.Our main aim with this work is to\nprovide the technical means to facilitate the manual data analysis of classroom\nvideos in research on teaching quality and in the context of teacher training.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:15:04 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 18:53:32 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["S\u00fcmer", "\u00d6mer", ""], ["Goldberg", "Patricia", ""], ["D'Mello", "Sidney", ""], ["Gerjets", "Peter", ""], ["Trautwein", "Ulrich", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2101.04230", "submitter": "Sumedha Singla", "authors": "Sumedha Singla, Brian Pollack, Stephen Wallace and Kayhan\n  Batmanghelich", "title": "Explaining the Black-box Smoothly- A Counterfactual Approach", "comments": "Under review for IEEE-TMI journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a BlackBox \\emph{Counterfactual Explainer} that is explicitly\ndeveloped for medical imaging applications. Classical approaches (e.g. saliency\nmaps) assessing feature importance do not explain \\emph{how} and \\emph{why}\nvariations in a particular anatomical region is relevant to the outcome, which\nis crucial for transparent decision making in healthcare application. Our\nframework explains the outcome by gradually \\emph{exaggerating} the semantic\neffect of the given outcome label. Given a query input to a classifier,\nGenerative Adversarial Networks produce a progressive set of perturbations to\nthe query image that gradually changes the posterior probability from its\noriginal class to its negation. We design the loss function to ensure that\nessential and potentially relevant details, such as support devices, are\npreserved in the counterfactually generated images. We provide an extensive\nevaluation of different classification tasks on the chest X-Ray images. Our\nexperiments show that a counterfactually generated visual explanation is\nconsistent with the disease's clinical relevant measurements, both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 23:12:11 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 05:36:02 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Singla", "Sumedha", ""], ["Pollack", "Brian", ""], ["Wallace", "Stephen", ""], ["Batmanghelich", "Kayhan", ""]]}, {"id": "2101.04240", "submitter": "Sodiq Adewole", "authors": "Sodiq Adewole, Philip Fernandez, Michelle Yeghyayan, James Jablonski,\n  Andrew Copland, Michael Porter, Sana Syed, Donald Brown", "title": "Lesion2Vec: Deep Metric Learning for Few-Shot Multiple Lesions\n  Recognition in Wireless Capsule Endoscopy Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective and rapid detection of lesions in the Gastrointestinal tract is\ncritical to gastroenterologist's response to some life-threatening diseases.\nWireless Capsule Endoscopy (WCE) has revolutionized traditional endoscopy\nprocedure by allowing gastroenterologists visualize the entire GI tract\nnon-invasively. Once the tiny capsule is swallowed, it sequentially capture\nimages of the GI tract at about 2 to 6 frames per second (fps). A single video\ncan last up to 8 hours producing between 30,000 to 100,000 images. Automating\nthe detection of frames containing specific lesion in WCE video would relieve\ngastroenterologists the arduous task of reviewing the entire video before\nmaking diagnosis. While the WCE produces large volume of images, only about 5\\%\nof the frames contain lesions that aid the diagnosis process. Convolutional\nNeural Network (CNN) based models have been very successful in various image\nclassification tasks. However, they suffer excessive parameters, are sample\ninefficient and rely on very large amount of training data. Deploying a CNN\nclassifier for lesion detection task will require time-to-time fine-tuning to\ngeneralize to any unforeseen category. In this paper, we propose a metric-based\nlearning framework followed by a few-shot lesion recognition in WCE data.\nMetric-based learning is a meta-learning framework designed to establish\nsimilarity or dissimilarity between concepts while few-shot learning (FSL) aims\nto identify new concepts from only a small number of examples. We train a\nfeature extractor to learn a representation for different small bowel lesions\nusing metric-based learning. At the testing stage, the category of an unseen\nsample is predicted from only a few support examples, thereby allowing the\nmodel to generalize to a new category that has never been seen before. We\ndemonstrated the efficacy of this method on real patient capsule endoscopy\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 23:58:56 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 22:46:36 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Adewole", "Sodiq", ""], ["Fernandez", "Philip", ""], ["Yeghyayan", "Michelle", ""], ["Jablonski", "James", ""], ["Copland", "Andrew", ""], ["Porter", "Michael", ""], ["Syed", "Sana", ""], ["Brown", "Donald", ""]]}, {"id": "2101.04262", "submitter": "Praveen Abbaraju", "authors": "Upinder Kaur, Praveen Abbaraju, Harrison McCarty, and Richard M.\n  Voyles", "title": "Clutter Slices Approach for Identification-on-the-fly of Indoor Spaces", "comments": "First two authors share equal contribution. Presented at ICPR2020 The\n  25th International Conference on Pattern Recognition, PRAConBE Workshop", "journal-ref": "2020 Springer Lecture Notes in Computer Science", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Construction spaces are constantly evolving, dynamic environments in need of\ncontinuous surveying, inspection, and assessment. Traditional manual inspection\nof such spaces proves to be an arduous and time-consuming activity. Automation\nusing robotic agents can be an effective solution. Robots, with perception\ncapabilities can autonomously classify and survey indoor construction spaces.\nIn this paper, we present a novel identification-on-the-fly approach for coarse\nclassification of indoor spaces using the unique signature of clutter. Using\nthe context granted by clutter, we recognize common indoor spaces such as\ncorridors, staircases, shared spaces, and restrooms. The proposed clutter\nslices pipeline achieves a maximum accuracy of 93.6% on the presented clutter\nslices dataset. This sensor independent approach can be generalized to various\ndomains to equip intelligent autonomous agents in better perceiving their\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 02:05:33 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Kaur", "Upinder", ""], ["Abbaraju", "Praveen", ""], ["McCarty", "Harrison", ""], ["Voyles", "Richard M.", ""]]}, {"id": "2101.04266", "submitter": "Yi Liu", "authors": "Yi Liu, Shuiwang Ji", "title": "CleftNet: Augmented Deep Learning for Synaptic Cleft Detection from\n  Brain Electron Microscopy", "comments": "10 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting synaptic clefts is a crucial step to investigate the biological\nfunction of synapses. The volume electron microscopy (EM) allows the\nidentification of synaptic clefts by photoing EM images with high resolution\nand fine details. Machine learning approaches have been employed to\nautomatically predict synaptic clefts from EM images. In this work, we propose\na novel and augmented deep learning model, known as CleftNet, for improving\nsynaptic cleft detection from brain EM images. We first propose two novel\nnetwork components, known as the feature augmentor and the label augmentor, for\naugmenting features and labels to improve cleft representations. The feature\naugmentor can fuse global information from inputs and learn common\nmorphological patterns in clefts, leading to augmented cleft features. In\naddition, it can generate outputs with varying dimensions, making it flexible\nto be integrated in any deep network. The proposed label augmentor augments the\nlabel of each voxel from a value to a vector, which contains both the\nsegmentation label and boundary label. This allows the network to learn\nimportant shape information and to produce more informative cleft\nrepresentations. Based on the proposed feature augmentor and label augmentor,\nWe build the CleftNet as a U-Net like network. The effectiveness of our methods\nis evaluated on both online and offline tasks. Our CleftNet currently ranks \\#1\non the online task of the CREMI open challenge. In addition, both quantitative\nand qualitative results in the offline tasks show that our method outperforms\nthe baseline approaches significantly.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 02:45:53 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Liu", "Yi", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2101.04269", "submitter": "Yan Han", "authors": "Yan Han, Chongyan Chen, Ahmed H Tewfik, Ying Ding, Yifan Peng", "title": "Pneumonia Detection on Chest X-ray using Radiomic Features and\n  Contrastive Learning", "comments": "Accepted for ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray becomes one of the most common medical diagnoses due to its\nnoninvasiveness. The number of chest X-ray images has skyrocketed, but reading\nchest X-rays still have been manually performed by radiologists, which creates\nhuge burnouts and delays. Traditionally, radiomics, as a subfield of radiology\nthat can extract a large number of quantitative features from medical images,\ndemonstrates its potential to facilitate medical imaging diagnosis before the\ndeep learning era. With the rise of deep learning, the explainability of deep\nneural networks on chest X-ray diagnosis remains opaque. In this study, we\nproposed a novel framework that leverages radiomics features and contrastive\nlearning to detect pneumonia in chest X-ray. Experiments on the RSNA Pneumonia\nDetection Challenge dataset show that our model achieves superior results to\nseveral state-of-the-art models (> 10% in F1-score) and increases the model's\ninterpretability.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 02:52:24 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Han", "Yan", ""], ["Chen", "Chongyan", ""], ["Tewfik", "Ahmed H", ""], ["Ding", "Ying", ""], ["Peng", "Yifan", ""]]}, {"id": "2101.04279", "submitter": "Geng Chen", "authors": "Geng Chen and Peirong Guo", "title": "Enhanced Information Fusion Network for Crowd Counting", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, crowd counting, a technique for predicting the number of\npeople in an image, becomes a challenging task in computer vision. In this\npaper, we propose a cross-column feature fusion network to solve the problem of\ninformation redundancy in columns. We introduce the Information Fusion Module\n(IFM) which provides a channel for information flow to help different columns\nto obtain significant information from another column. Through this channel,\ndifferent columns exchange information with each other and extract useful\nfeatures from the other column to enhance key information. Hence, there is no\nneed for columns to pay attention to all areas in the image. Each column can be\nresponsible for different regions, thereby reducing the burden of each column.\nIn experiments, the generalizability of our model is more robust and the\nresults of transferring between different datasets acheive the comparable\nresults with the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 03:35:22 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Chen", "Geng", ""], ["Guo", "Peirong", ""]]}, {"id": "2101.04281", "submitter": "Nathan Louis", "authors": "Nathan Louis, Luowei Zhou, Steven J. Yule, Roger D. Dias, Milisa\n  Manojlovich, Francis D. Pagani, Donald S. Likosky, Jason J. Corso", "title": "Temporally Guided Articulated Hand Pose Tracking in Surgical Videos", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Articulated hand pose tracking is an underexplored problem that carries the\npotential for use in an extensive number of applications, especially in the\nmedical domain. With a robust and accurate tracking system on in-vivo surgical\nvideos, the motion dynamics and movement patterns of the hands can be captured\nand analyzed for rich tasks including skills assessment, training surgical\nresidents, and temporal action recognition. In this work, we propose a novel\nhand pose estimation model, Res152- CondPose, which improves tracking accuracy\nby incorporating a hand pose prior into its pose prediction. We show\nimprovements over state-of-the-art methods which provide frame-wise independent\npredictions, by following a temporally guided approach that effectively\nleverages past predictions. Additionally, we collect the first dataset,\nSurgical Hands, that provides multi-instance articulated hand pose annotations\nfor in-vivo videos. Our dataset contains 76 video clips from 28 publicly\navailable surgical videos and over 8.1k annotated hand pose instances. We\nprovide bounding boxes, articulated hand pose annotations, and tracking IDs to\nenable multi-instance area-based and articulated tracking. When evaluated on\nSurgical Hands, we show our method outperforms the state-of-the-art method\nusing mean Average Precision (mAP), to measure pose estimation accuracy, and\nMultiple Object Tracking Accuracy (MOTA), to assess pose tracking performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 03:44:04 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Louis", "Nathan", ""], ["Zhou", "Luowei", ""], ["Yule", "Steven J.", ""], ["Dias", "Roger D.", ""], ["Manojlovich", "Milisa", ""], ["Pagani", "Francis D.", ""], ["Likosky", "Donald S.", ""], ["Corso", "Jason J.", ""]]}, {"id": "2101.04287", "submitter": "Haokui Zhang", "authors": "Haokui Zhang, Chengrong Gong, Yunpeng Bai, Zongwen Bai and Ying Li", "title": "3D-ANAS: 3D Asymmetric Neural Architecture Search for Fast Hyperspectral\n  Image Classification", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral images involve abundant spectral and spatial information,\nplaying an irreplaceable role in land-cover classification. Recently, based on\ndeep learning technologies, an increasing number of HSI classification\napproaches have been proposed, which demonstrate promising performance.\nHowever, previous studies suffer from two major drawbacks: 1) the architecture\nof most deep learning models is manually designed, relies on specialized\nknowledge, and is relatively tedious. Moreover, in HSI classifications,\ndatasets captured by different sensors have different physical properties.\nCorrespondingly, different models need to be designed for different datasets,\nwhich further increases the workload of designing architectures; 2) the\nmainstream framework is a patch-to-pixel framework. The overlap regions of\npatches of adjacent pixels are calculated repeatedly, which increases\ncomputational cost and time cost. Besides, the classification accuracy is\nsensitive to the patch size, which is artificially set based on extensive\ninvestigation experiments. To overcome the issues mentioned above, we firstly\npropose a 3D asymmetric neural network search algorithm and leverage it to\nautomatically search for efficient architectures for HSI classifications. By\nanalysing the characteristics of HSIs, we specifically build a 3D asymmetric\ndecomposition search space, where spectral and spatial information are\nprocessed with different decomposition convolutions. Furthermore, we propose a\nnew fast classification framework, i,e., pixel-to-pixel classification\nframework, which has no repetitive operations and reduces the overall cost.\nExperiments on three public HSI datasets captured by different sensors\ndemonstrate the networks designed by our 3D-ANAS achieve competitive\nperformance compared to several state-of-the-art methods, while having a much\nfaster inference speed.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 04:15:40 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Zhang", "Haokui", ""], ["Gong", "Chengrong", ""], ["Bai", "Yunpeng", ""], ["Bai", "Zongwen", ""], ["Li", "Ying", ""]]}, {"id": "2101.04307", "submitter": "Zheng Ge", "authors": "Zheng Ge, Jianfeng Wang, Xin Huang, Songtao Liu, Osamu Yoshie", "title": "LLA: Loss-aware Label Assignment for Dense Pedestrian Detection", "comments": "In the reviewing process of Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label assignment has been widely studied in general object detection because\nof its great impact on detectors' performance. However, none of these works\nfocus on label assignment in dense pedestrian detection. In this paper, we\npropose a simple yet effective assigning strategy called Loss-aware Label\nAssignment (LLA) to boost the performance of pedestrian detectors in crowd\nscenarios. LLA first calculates classification (cls) and regression (reg)\nlosses between each anchor and ground-truth (GT) pair. A joint loss is then\ndefined as the weighted summation of cls and reg losses as the assigning\nindicator. Finally, anchors with top K minimum joint losses for a certain GT\nbox are assigned as its positive anchors. Anchors that are not assigned to any\nGT box are considered negative. Loss-aware label assignment is based on an\nobservation that anchors with lower joint loss usually contain richer semantic\ninformation and thus can better represent their corresponding GT boxes.\nExperiments on CrowdHuman and CityPersons show that such a simple label\nassigning strategy can boost MR by 9.53% and 5.47% on two famous one-stage\ndetectors - RetinaNet and FCOS, respectively, demonstrating the effectiveness\nof LLA.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 05:51:32 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 05:59:52 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 02:49:48 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ge", "Zheng", ""], ["Wang", "Jianfeng", ""], ["Huang", "Xin", ""], ["Liu", "Songtao", ""], ["Yoshie", "Osamu", ""]]}, {"id": "2101.04318", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl and Enkelejda Kasneci", "title": "A Multimodal Eye Movement Dataset and a Multimodal Eye Movement\n  Segmentation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dataset with annotated eye movements. The dataset consists\nof over 800,000 gaze points recorded during a car ride in the real world and in\nthe simulator. In total, the eye movements of 19 subjects were annotated. In\nthis dataset there are several data sources such as the eyelid closure, the\npupil center, the optical vector, and a vector into the pupil center starting\nfrom the center of the eye corners. These different data sources are analyzed\nand evaluated individually as well as in combination with respect to their\ngoodness of fit for eye movement classification. These results will help\ndevelopers of real-time systems and algorithms to find the best data sources\nfor their application. Also, new algorithms can be trained and evaluated on\nthis data set. The data and the Matlab code can be downloaded here\nhttps://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FA%20Multimodal%20Eye%20Movement%20Dataset%20and%20...&mode=list\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 06:34:56 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2101.04321", "submitter": "Bo Yang", "authors": "Bo Yang, Kaiyong Xu, Hengjun Wang, Hengwei Zhang", "title": "Random Transformation of Image Brightness for Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples, which are\ncrafted by adding small, human-imperceptible perturbations to the original\nimages, but make the model output inaccurate predictions. Before deep neural\nnetworks are deployed, adversarial attacks can thus be an important method to\nevaluate and select robust models in safety-critical applications. However,\nunder the challenging black-box setting, the attack success rate, i.e., the\ntransferability of adversarial examples, still needs to be improved. Based on\nimage augmentation methods, we found that random transformation of image\nbrightness can eliminate overfitting in the generation of adversarial examples\nand improve their transferability. To this end, we propose an adversarial\nexample generation method based on this phenomenon, which can be integrated\nwith Fast Gradient Sign Method (FGSM)-related methods to build a more robust\ngradient-based attack and generate adversarial examples with better\ntransferability. Extensive experiments on the ImageNet dataset demonstrate the\nmethod's effectiveness. Whether on normally or adversarially trained networks,\nour method has a higher success rate for black-box attacks than other attack\nmethods based on data augmentation. We hope that this method can help to\nevaluate and improve the robustness of models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 07:00:04 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Yang", "Bo", ""], ["Xu", "Kaiyong", ""], ["Wang", "Hengjun", ""], ["Zhang", "Hengwei", ""]]}, {"id": "2101.04340", "submitter": "Wei Zhang", "authors": "Xuanyu He, Wei Zhang, Ran Song, Qian Zhang, Xiangyuan Lan, Lin Ma", "title": "Take More Positives: An Empirical Study of Contrastive Learing in\n  Unsupervised Person Re-Identification", "comments": "Technical report, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised person re-identification (re-ID) aims at closing the performance\ngap to supervised methods. These methods build reliable relationship between\ndata points while learning representations. However, we empirically show that\nthe reason why they are successful is not only their label generation\nmechanisms, but also their unexplored designs. By studying two unsupervised\nperson re-ID methods in a cross-method way, we point out a hard negative\nproblem is handled implicitly by their designs of data augmentations and PK\nsampler respectively. In this paper, we find another simple solution for the\nproblem, i.e., taking more positives during training, by which we generate\npseudo-labels and update models in an iterative manner. Based on our findings,\nwe propose a contrastive learning method without a memory back for unsupervised\nperson re-ID. Our method works well on benchmark datasets and outperforms the\nstate-of-the-art methods. Code will be made available.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 08:06:11 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 06:23:34 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["He", "Xuanyu", ""], ["Zhang", "Wei", ""], ["Song", "Ran", ""], ["Zhang", "Qian", ""], ["Lan", "Xiangyuan", ""], ["Ma", "Lin", ""]]}, {"id": "2101.04342", "submitter": "Hao Yu", "authors": "Hao Yu, Huanyu Wang, Jianxin Wu", "title": "Mixup Without Hesitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixup linearly interpolates pairs of examples to form new samples, which is\neasy to implement and has been shown to be effective in image classification\ntasks. However, there are two drawbacks in mixup: one is that more training\nepochs are needed to obtain a well-trained model; the other is that mixup\nrequires tuning a hyper-parameter to gain appropriate capacity but that is a\ndifficult task. In this paper, we find that mixup constantly explores the\nrepresentation space, and inspired by the exploration-exploitation dilemma in\nreinforcement learning, we propose mixup Without hesitation (mWh), a concise,\neffective, and easy-to-use training algorithm. We show that mWh strikes a good\nbalance between exploration and exploitation by gradually replacing mixup with\nbasic data augmentation. It can achieve a strong baseline with less training\ntime than original mixup and without searching for optimal hyper-parameter,\ni.e., mWh acts as mixup without hesitation. mWh can also transfer to CutMix,\nand gain consistent improvement on other machine learning and computer vision\ntasks such as object detection. Our code is open-source and available at\nhttps://github.com/yuhao318/mwh\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 08:11:08 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Yu", "Hao", ""], ["Wang", "Huanyu", ""], ["Wu", "Jianxin", ""]]}, {"id": "2101.04350", "submitter": "Neslihan Bayramoglu", "authors": "Neslihan Bayramoglu, Miika T. Nieminen, Simo Saarakkala", "title": "Automated Detection of Patellofemoral Osteoarthritis from Knee Lateral\n  View Radiographs Using Deep Learning: Data from the Multicenter\n  Osteoarthritis Study (MOST)", "comments": "24 pages, preprint, Submitted to Osteoarthritis and Cartilage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To assess the ability of imaging-based deep learning to predict\nradiographic patellofemoral osteoarthritis (PFOA) from knee lateral view\nradiographs.\n  Design: Knee lateral view radiographs were extracted from The Multicenter\nOsteoarthritis Study (MOST) (n = 18,436 knees). Patellar region-of-interest\n(ROI) was first automatically detected, and subsequently, end-to-end deep\nconvolutional neural networks (CNNs) were trained and validated to detect the\nstatus of patellofemoral OA. Patellar ROI was detected using\ndeep-learning-based object detection method. Manual PFOA status assessment\nprovided in the MOST dataset was used as a classification outcome for the CNNs.\nPerformance of prediction models was assessed using the area under the receiver\noperating characteristic curve (ROC AUC) and the average precision (AP)\nobtained from the precision-recall (PR) curve in the stratified 5-fold cross\nvalidation setting.\n  Results: Of the 18,436 knees, 3,425 (19%) had PFOA. AUC and AP for the\nreference model including age, sex, body mass index (BMI), the total Western\nOntario and McMaster Universities Arthritis Index (WOMAC) score, and\ntibiofemoral Kellgren-Lawrence (KL) grade to predict PFOA were 0.806 and 0.478,\nrespectively. The CNN model that used only image data significantly improved\nthe prediction of PFOA status (ROC AUC= 0.958, AP= 0.862).\n  Conclusion: We present the first machine learning based automatic PFOA\ndetection method. Furthermore, our deep learning based model trained on patella\nregion from knee lateral view radiographs performs better at predicting PFOA\nthan models based on patient characteristics and clinical assessments.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 08:37:55 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Bayramoglu", "Neslihan", ""], ["Nieminen", "Miika T.", ""], ["Saarakkala", "Simo", ""]]}, {"id": "2101.04378", "submitter": "Jordao Bragantini", "authors": "Jord\\~ao Bragantini (UNICAMP), Alexandre Falc\\~ao (UNICAMP), Laurent\n  Najman (ligm)", "title": "Rethinking Interactive Image Segmentation: Feature Space Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the progress of interactive image segmentation methods, high-quality\npixel-level annotation is still time-consuming and laborious -- a bottleneck\nfor several deep learning applications. We take a step back to propose\ninteractive and simultaneous segment annotation from multiple images guided by\nfeature space projection and optimized by metric learning as the labeling\nprogresses. This strategy is in stark contrast to existing interactive\nsegmentation methodologies, which perform annotation in the image domain. We\nshow that our approach can surpass the accuracy of state-of-the-art methods in\nforeground segmentation datasets: iCoSeg, DAVIS, and Rooftop. Moreover, it\nachieves 91.5\\% accuracy in a known semantic segmentation dataset, Cityscapes,\nbeing 74.75 times faster than the original annotation procedure. The appendix\npresents additional qualitative results. Code and video demonstration will be\nreleased upon publication.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:13:35 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Bragantini", "Jord\u00e3o", "", "UNICAMP"], ["Falc\u00e3o", "Alexandre", "", "UNICAMP"], ["Najman", "Laurent", "", "ligm"]]}, {"id": "2101.04386", "submitter": "Ishaan Bhat", "authors": "Ishaan Bhat, Hugo J. Kuijf, Veronika Cheplygina and Josien P.W. Pluim", "title": "Using uncertainty estimation to reduce false positives in liver lesion\n  detection", "comments": "Accepted at IEEE ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the successes of deep learning techniques at detecting objects in\nmedical images, false positive detections occur which may hinder an accurate\ndiagnosis. We propose a technique to reduce false positive detections made by a\nneural network using an SVM classifier trained with features derived from the\nuncertainty map of the neural network prediction. We demonstrate the\neffectiveness of this method for the detection of liver lesions on a dataset of\nabdominal MR images. We find that the use of a dropout rate of 0.5 produces the\nleast number of false positives in the neural network predictions and the\ntrained classifier filters out approximately 90% of these false positives\ndetections in the test-set.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:26:41 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 12:30:54 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 11:02:34 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Bhat", "Ishaan", ""], ["Kuijf", "Hugo J.", ""], ["Cheplygina", "Veronika", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "2101.04407", "submitter": "Jun Wang", "authors": "Jun Wang, Yinglu Liu, Yibo Hu, Hailin Shi and Tao Mei", "title": "FaceX-Zoo: A PyTorch Toolbox for Face Recognition", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based face recognition has achieved significant progress in\nrecent years. Yet, the practical model production and further research of deep\nface recognition are in great need of corresponding public support. For\nexample, the production of face representation network desires a modular\ntraining scheme to consider the proper choice from various candidates of\nstate-of-the-art backbone and training supervision subject to the real-world\nface recognition demand; for performance analysis and comparison, the standard\nand automatic evaluation with a bunch of models on multiple benchmarks will be\na desired tool as well; besides, a public groundwork is welcomed for deploying\nthe face recognition in the shape of holistic pipeline. Furthermore, there are\nsome newly-emerged challenges, such as the masked face recognition caused by\nthe recent world-wide COVID-19 pandemic, which draws increasing attention in\npractical applications. A feasible and elegant solution is to build an\neasy-to-use unified framework to meet the above demands. To this end, we\nintroduce a novel open-source framework, named FaceX-Zoo, which is oriented to\nthe research-development community of face recognition. Resorting to the highly\nmodular and scalable design, FaceX-Zoo provides a training module with various\nsupervisory heads and backbones towards state-of-the-art face recognition, as\nwell as a standardized evaluation module which enables to evaluate the models\nin most of the popular benchmarks just by editing a simple configuration. Also,\na simple yet fully functional face SDK is provided for the validation and\nprimary application of the trained models. Rather than including as many as\npossible of the prior techniques, we enable FaceX-Zoo to easily upgrade and\nextend along with the development of face related domains. The source code and\nmodels are available at https://github.com/JDAI-CV/FaceX-Zoo.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:06:50 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 06:14:09 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Wang", "Jun", ""], ["Liu", "Yinglu", ""], ["Hu", "Yibo", ""], ["Shi", "Hailin", ""], ["Mei", "Tao", ""]]}, {"id": "2101.04431", "submitter": "Jorge Beltr\\'an", "authors": "Jorge Beltr\\'an, Carlos Guindel, Fernando Garc\\'ia", "title": "Automatic Extrinsic Calibration Method for LiDAR and Camera Sensor\n  Setups", "comments": "Submitted to IEEE Transactions on Intelligent Transportation Systems.\n  This work has been submitted to the IEEE for possible publication. Copyright\n  may be transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most sensor setups for onboard autonomous perception are composed of LiDARs\nand vision systems, as they provide complementary information that improves the\nreliability of the different algorithms necessary to obtain a robust scene\nunderstanding. However, the effective use of information from different sources\nrequires an accurate calibration between the sensors involved, which usually\nimplies a tedious and burdensome process. We present a method to calibrate the\nextrinsic parameters of any pair of sensors involving LiDARs, monocular or\nstereo cameras, of the same or different modalities. The procedure is composed\nof two stages: first, reference points belonging to a custom calibration target\nare extracted from the data provided by the sensors to be calibrated, and\nsecond, the optimal rigid transformation is found through the registration of\nboth point sets. The proposed approach can handle devices with very different\nresolutions and poses, as usually found in vehicle setups. In order to assess\nthe performance of the proposed method, a novel evaluation suite built on top\nof a popular simulation framework is introduced. Experiments on the synthetic\nenvironment show that our calibration algorithm significantly outperforms\nexisting methods, whereas real data tests corroborate the results obtained in\nthe evaluation suite. Open-source code is available at\nhttps://github.com/beltransen/velo2cam_calibration\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:02:26 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Beltr\u00e1n", "Jorge", ""], ["Guindel", "Carlos", ""], ["Garc\u00eda", "Fernando", ""]]}, {"id": "2101.04442", "submitter": "Jierun Chen", "authors": "Jierun Chen, Song Wen, S.-H. Gary Chan", "title": "Joint Demosaicking and Denoising in the Wild: The Case of Training Under\n  Ground Truth Uncertainty", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image demosaicking and denoising are the two key fundamental steps in digital\ncamera pipelines, aiming to reconstruct clean color images from noisy luminance\nreadings. In this paper, we propose and study Wild-JDD, a novel learning\nframework for joint demosaicking and denoising in the wild. In contrast to\nprevious works which generally assume the ground truth of training data is a\nperfect reflection of the reality, we consider here the more common imperfect\ncase of ground truth uncertainty in the wild. We first illustrate its\nmanifestation as various kinds of artifacts including zipper effect, color\nmoire and residual noise. Then we formulate a two-stage data degradation\nprocess to capture such ground truth uncertainty, where a conjugate prior\ndistribution is imposed upon a base distribution. After that, we derive an\nevidence lower bound (ELBO) loss to train a neural network that approximates\nthe parameters of the conjugate prior distribution conditioned on the degraded\ninput. Finally, to further enhance the performance for out-of-distribution\ninput, we design a simple but effective fine-tuning strategy by taking the\ninput as a weakly informative prior. Taking into account ground truth\nuncertainty, Wild-JDD enjoys good interpretability during optimization.\nExtensive experiments validate that it outperforms state-of-the-art schemes on\njoint demosaicking and denoising tasks on both synthetic and realistic raw\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:33:41 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Chen", "Jierun", ""], ["Wen", "Song", ""], ["Chan", "S. -H. Gary", ""]]}, {"id": "2101.04450", "submitter": "Georg Wimmer", "authors": "Georg Wimmer and Rudolf Schraml and Heinz Hofbauer and Alexander\n  Petutschnigg and Andreas Uhl", "title": "Two-stage CNN-based wood log recognition", "comments": "submitted to ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proof of origin of logs is becoming increasingly important. In the\ncontext of Industry 4.0 and to combat illegal logging there is an increasing\nmotivation to track each individual log. Our previous works in this field\nfocused on log tracking using digital log end images based on methods inspired\nby fingerprint and iris-recognition. This work presents a convolutional neural\nnetwork (CNN) based approach which comprises a CNN-based segmentation of the\nlog end combined with a final CNN-based recognition of the segmented log end\nusing the triplet loss function for CNN training. Results show that the\nproposed two-stage CNN-based approach outperforms traditional approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:51:32 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wimmer", "Georg", ""], ["Schraml", "Rudolf", ""], ["Hofbauer", "Heinz", ""], ["Petutschnigg", "Alexander", ""], ["Uhl", "Andreas", ""]]}, {"id": "2101.04493", "submitter": "Kseniya Cherenkova", "authors": "Kseniya Cherenkova, Djamila Aouada, Gleb Gusev", "title": "PvDeConv: Point-Voxel Deconvolution for Autoencoding CAD Construction in\n  3D", "comments": "2020 IEEE International Conference on Image Processing (ICIP)", "journal-ref": "2020 IEEE International Conference on Image Processing (ICIP),\n  2020, pp. 2741-2745", "doi": "10.1109/ICIP40778.2020.9191095", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a Point-Voxel DeConvolution (PVDeConv) module for 3D data\nautoencoder. To demonstrate its efficiency we learn to synthesize\nhigh-resolution point clouds of 10k points that densely describe the underlying\ngeometry of Computer Aided Design (CAD) models. Scanning artifacts, such as\nprotrusions, missing parts, smoothed edges and holes, inevitably appear in real\n3D scans of fabricated CAD objects. Learning the original CAD model\nconstruction from a 3D scan requires a ground truth to be available together\nwith the corresponding 3D scan of an object. To solve the gap, we introduce a\nnew dedicated dataset, the CC3D, containing 50k+ pairs of CAD models and their\ncorresponding 3D meshes. This dataset is used to learn a convolutional\nautoencoder for point clouds sampled from the pairs of 3D scans - CAD models.\nThe challenges of this new dataset are demonstrated in comparison with other\ngenerative point cloud sampling models trained on ShapeNet. The CC3D\nautoencoder is efficient with respect to memory consumption and training time\nas compared to stateof-the-art models for 3D data generation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 14:14:13 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Cherenkova", "Kseniya", ""], ["Aouada", "Djamila", ""], ["Gusev", "Gleb", ""]]}, {"id": "2101.04506", "submitter": "Changcheng Wang", "authors": "Yongsheng Zang, Dongming Zhou, Changcheng Wang, Rencan Nie, and Yanbu\n  Guo", "title": "UFA-FUSE: A novel deep supervised and hybrid model for multi-focus image\n  fusion", "comments": "17pages,11 figures, 7 tables. IEEE Transactions on Instrumentation\n  and Measurement", "journal-ref": null, "doi": "10.1109/TIM.2021.3072124", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional and deep learning-based fusion methods generated the intermediate\ndecision map to obtain the fusion image through a series of post-processing\nprocedures. However, the fusion results generated by these methods are easy to\nlose some source image details or results in artifacts. Inspired by the image\nreconstruction techniques based on deep learning, we propose a multi-focus\nimage fusion network framework without any post-processing to solve these\nproblems in the end-to-end and supervised learning way. To sufficiently train\nthe fusion model, we have generated a large-scale multi-focus image dataset\nwith ground-truth fusion images. What's more, to obtain a more informative\nfusion image, we further designed a novel fusion strategy based on unity fusion\nattention, which is composed of a channel attention module and a spatial\nattention module. Specifically, the proposed fusion approach mainly comprises\nthree key components: feature extraction, feature fusion and image\nreconstruction. We firstly utilize seven convolutional blocks to extract the\nimage features from source images. Then, the extracted convolutional features\nare fused by the proposed fusion strategy in the feature fusion layer. Finally,\nthe fused image features are reconstructed by four convolutional blocks.\nExperimental results demonstrate that the proposed approach for multi-focus\nimage fusion achieves remarkable fusion performance compared to 19\nstate-of-the-art fusion methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 14:33:13 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 11:01:45 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 02:55:00 GMT"}, {"version": "v4", "created": "Tue, 20 Apr 2021 08:41:48 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zang", "Yongsheng", ""], ["Zhou", "Dongming", ""], ["Wang", "Changcheng", ""], ["Nie", "Rencan", ""], ["Guo", "Yanbu", ""]]}, {"id": "2101.04544", "submitter": "Ziyue Zhang", "authors": "Ziyue Zhang, Shuai Jiang, Congzhentao Huang, Richard Yi Da Xu", "title": "Resolution-invariant Person ReID Based on Feature Transformation and\n  Self-weighted Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-identification (ReID) is a critical computer vision task which aims\nto match the same person in images or video sequences. Most current works focus\non settings where the resolution of images is kept the same. However, the\nresolution is a crucial factor in person ReID, especially when the cameras are\nat different distances from the person or the camera's models are different\nfrom each other. In this paper, we propose a novel two-stream network with a\nlightweight resolution association ReID feature transformation (RAFT) module\nand a self-weighted attention (SWA) ReID module to evaluate features under\ndifferent resolutions. RAFT transforms the low resolution features to\ncorresponding high resolution features. SWA evaluates both features to get\nweight factors for the person ReID. Both modules are jointly trained to get a\nresolution-invariant representation. Extensive experiments on five benchmark\ndatasets show the effectiveness of our method. For instance, we achieve Rank-1\naccuracy of 43.3% and 83.2% on CAVIAR and MLR-CUHK03, outperforming the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 15:22:41 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 02:50:42 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zhang", "Ziyue", ""], ["Jiang", "Shuai", ""], ["Huang", "Congzhentao", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "2101.04558", "submitter": "Pengyang Li", "authors": "Pengyang Li and Donghui Wang", "title": "Fine-grained Semantic Constraint in Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-stage and high-resolution model for image\nsynthesis that uses fine-grained attributes and masks as input. With a\nfine-grained attribute, the proposed model can detailedly constrain the\nfeatures of the generated image through rich and fine-grained semantic\ninformation in the attribute. With mask as prior, the model in this paper is\nconstrained so that the generated images conform to visual senses, which will\nreduce the unexpected diversity of samples generated from the generative\nadversarial network. This paper also proposes a scheme to improve the\ndiscriminator of the generative adversarial network by simultaneously\ndiscriminating the total image and sub-regions of the image. In addition, we\npropose a method for optimizing the labeled attribute in datasets, which\nreduces the manual labeling noise. Extensive quantitative results show that our\nimage synthesis model generates more realistic images.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 15:51:49 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Li", "Pengyang", ""], ["Wang", "Donghui", ""]]}, {"id": "2101.04562", "submitter": "Wei Peng", "authors": "Wei Peng, Tuomas Varanka, Abdelrahman Mostafa, Henglin Shi, Guoying\n  Zhao", "title": "Hyperbolic Deep Neural Networks: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, there has been a rising surge of momentum for deep representation\nlearning in hyperbolic spaces due to theirhigh capacity of modeling data like\nknowledge graphs or synonym hierarchies, possessing hierarchical structure. We\nrefer to the model as hyperbolic deep neural network in this paper. Such a\nhyperbolic neural architecture potentially leads to drastically compact model\nwithmuch more physical interpretability than its counterpart in Euclidean\nspace. To stimulate future research, this paper presents acoherent and\ncomprehensive review of the literature around the neural components in the\nconstruction of hyperbolic deep neuralnetworks, as well as the generalization\nof the leading deep approaches to the Hyperbolic space. It also presents\ncurrent applicationsaround various machine learning tasks on several publicly\navailable datasets, together with insightful observations and identifying\nopenquestions and promising future directions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 15:55:16 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 10:03:17 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 14:59:23 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Peng", "Wei", ""], ["Varanka", "Tuomas", ""], ["Mostafa", "Abdelrahman", ""], ["Shi", "Henglin", ""], ["Zhao", "Guoying", ""]]}, {"id": "2101.04563", "submitter": "Lingkun Luo Dr.", "authors": "Lingkun Luo, Liming Chen, Shiqiang Hu", "title": "Discriminative Noise Robust Sparse Orthogonal Label Regression-based\n  Domain Adaptation", "comments": "17figures,17pages. arXiv admin note: text overlap with\n  arXiv:1712.10042", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Domain adaptation (DA) aims to enable a learning model trained from a source\ndomain to generalize well on a target domain, despite the mismatch of data\ndistributions between the two domains. State-of-the-art DA methods have so far\nfocused on the search of a latent shared feature space where source and target\ndomain data can be aligned either statistically and/or geometrically. In this\npaper, we propose a novel unsupervised DA method, namely Discriminative Noise\nRobust Sparse Orthogonal Label Regression-based Domain Adaptation (DOLL-DA).\nThe proposed DOLL-DA derives from a novel integrated model which searches a\nshared feature subspace where source and target domain data are, through\noptimization of some repulse force terms, discriminatively aligned\nstatistically, while at same time regresses orthogonally data labels thereof\nusing a label embedding trick. Furthermore, in minimizing a novel Noise Robust\nSparse Orthogonal Label Regression(NRS_OLR) term, the proposed model explicitly\naccounts for data outliers to avoid negative transfer and introduces the\nproperty of sparsity when regressing data labels.\n  Due to the character restriction. Please read our detailed abstract in our\npaper.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 07:10:13 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Luo", "Lingkun", ""], ["Chen", "Liming", ""], ["Hu", "Shiqiang", ""]]}, {"id": "2101.04574", "submitter": "Christian Wilms", "authors": "Christian Wilms and Simone Frintrop", "title": "Superpixel-based Refinement for Object Proposal Generation", "comments": "Accepted at ICPR 2020. Code is available at\n  https://github.com/chwilms/superpixelRefinement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Precise segmentation of objects is an important problem in tasks like\nclass-agnostic object proposal generation or instance segmentation. Deep\nlearning-based systems usually generate segmentations of objects based on\ncoarse feature maps, due to the inherent downsampling in CNNs. This leads to\nsegmentation boundaries not adhering well to the object boundaries in the\nimage. To tackle this problem, we introduce a new superpixel-based refinement\napproach on top of the state-of-the-art object proposal system AttentionMask.\nThe refinement utilizes superpixel pooling for feature extraction and a novel\nsuperpixel classifier to determine if a high precision superpixel belongs to an\nobject or not. Our experiments show an improvement of up to 26.0% in terms of\naverage recall compared to original AttentionMask. Furthermore, qualitative and\nquantitative analyses of the segmentations reveal significant improvements in\nterms of boundary adherence for the proposed refinement compared to various\ndeep learning-based state-of-the-art object proposal generation systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 16:06:48 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wilms", "Christian", ""], ["Frintrop", "Simone", ""]]}, {"id": "2101.04626", "submitter": "Stefan Cassar", "authors": "Stefan Cassar, Adrian Muscat, Dylan Seychell", "title": "Predicting Relative Depth between Objects from Semantic Features", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vision and language tasks such as Visual Relation Detection and Visual\nQuestion Answering benefit from semantic features that afford proper grounding\nof language. The 3D depth of objects depicted in 2D images is one such feature.\nHowever it is very difficult to obtain accurate depth information without\nlearning the appropriate features, which are scene dependent. The state of the\nart in this area are complex Neural Network models trained on stereo image data\nto predict depth per pixel. Fortunately, in some tasks, its only the relative\ndepth between objects that is required. In this paper the extent to which\nsemantic features can predict course relative depth is investigated. The\nproblem is casted as a classification one and geometrical features based on\nobject bounding boxes, object labels and scene attributes are computed and used\nas inputs to pattern recognition models to predict relative depth. i.e behind,\nin-front and neutral. The results are compared to those obtained from averaging\nthe output of the monodepth neural network model, which represents the\nstate-of-the art. An overall increase of 14% in relative depth accuracy over\nrelative depth computed from the monodepth model derived results is achieved.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:28:23 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Cassar", "Stefan", ""], ["Muscat", "Adrian", ""], ["Seychell", "Dylan", ""]]}, {"id": "2101.04631", "submitter": "Majed El Helou", "authors": "Xiaoqi Ma, Xiaoyu Lin, Majed El Helou, Sabine S\\\"usstrunk", "title": "Deep Gaussian Denoiser Epistemic Uncertainty and Decoupled\n  Dual-Attention Fusion", "comments": "Code and models are publicly available on https://github.com/IVRL/DEU", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the performance breakthrough of denoising networks, improvements\nhave come chiefly through novel architecture designs and increased depth. While\nnovel denoising networks were designed for real images coming from different\ndistributions, or for specific applications, comparatively small improvement\nwas achieved on Gaussian denoising. The denoising solutions suffer from\nepistemic uncertainty that can limit further advancements. This uncertainty is\ntraditionally mitigated through different ensemble approaches. However, such\nensembles are prohibitively costly with deep networks, which are already large\nin size.\n  Our work focuses on pushing the performance limits of state-of-the-art\nmethods on Gaussian denoising. We propose a model-agnostic approach for\nreducing epistemic uncertainty while using only a single pretrained network. We\nachieve this by tapping into the epistemic uncertainty through augmented and\nfrequency-manipulated images to obtain denoised images with varying error. We\npropose an ensemble method with two decoupled attention paths, over the pixel\ndomain and over that of our different manipulations, to learn the final fusion.\nOur results significantly improve over the state-of-the-art baselines and\nacross varying noise levels.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:38:32 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 11:05:15 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 17:25:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ma", "Xiaoqi", ""], ["Lin", "Xiaoyu", ""], ["Helou", "Majed El", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2101.04632", "submitter": "Fares Ben Slimane", "authors": "Fares Ben Slimane and Mohamed Bouguessa", "title": "Context Matters: Self-Attention for Sign Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an attentional network for the task of Continuous Sign\nLanguage Recognition. The proposed approach exploits co-independent streams of\ndata to model the sign language modalities. These different channels of\ninformation can share a complex temporal structure between each other. For that\nreason, we apply attention to synchronize and help capture entangled\ndependencies between the different sign language components. Even though Sign\nLanguage is multi-channel, handshapes represent the central entities in sign\ninterpretation. Seeing handshapes in their correct context defines the meaning\nof a sign. Taking that into account, we utilize the attention mechanism to\nefficiently aggregate the hand features with their appropriate spatio-temporal\ncontext for better sign recognition. We found that by doing so the model is\nable to identify the essential Sign Language components that revolve around the\ndominant hand and the face areas. We test our model on the benchmark dataset\nRWTH-PHOENIX-Weather 2014, yielding competitive results.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:40:19 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Slimane", "Fares Ben", ""], ["Bouguessa", "Mohamed", ""]]}, {"id": "2101.04639", "submitter": "Carlos Busso", "authors": "Sumit Jha, Mohamed F. Marzban, Tiancheng Hu, Mohamed H. Mahmoud,\n  Naofal Al-Dhahir, Carlos Busso", "title": "The Multimodal Driver Monitoring Database: A Naturalistic Corpus to\n  Study Driver Attention", "comments": "14 pages, 12 Figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A smart vehicle should be able to monitor the actions and behaviors of the\nhuman driver to provide critical warnings or intervene when necessary. Recent\nadvancements in deep learning and computer vision have shown great promise in\nmonitoring human behaviors and activities. While these algorithms work well in\na controlled environment, naturalistic driving conditions add new challenges\nsuch as illumination variations, occlusions and extreme head poses. A vast\namount of in-domain data is required to train models that provide high\nperformance in predicting driving related tasks to effectively monitor driver\nactions and behaviors. Toward building the required infrastructure, this paper\npresents the multimodal driver monitoring (MDM) dataset, which was collected\nwith 59 subjects that were recorded performing various tasks. We use the Fi-\nCap device that continuously tracks the head movement of the driver using\nfiducial markers, providing frame-based annotations to train head pose\nalgorithms in naturalistic driving conditions. We ask the driver to look at\npredetermined gaze locations to obtain accurate correlation between the\ndriver's facial image and visual attention. We also collect data when the\ndriver performs common secondary activities such as navigation using a smart\nphone and operating the in-car infotainment system. All of the driver's\nactivities are recorded with high definition RGB cameras and time-of-flight\ndepth camera. We also record the controller area network-bus (CAN-Bus),\nextracting important information. These high quality recordings serve as the\nideal resource to train various efficient algorithms for monitoring the driver,\nproviding further advancements in the field of in-vehicle safety systems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 16:37:17 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Jha", "Sumit", ""], ["Marzban", "Mohamed F.", ""], ["Hu", "Tiancheng", ""], ["Mahmoud", "Mohamed H.", ""], ["Al-Dhahir", "Naofal", ""], ["Busso", "Carlos", ""]]}, {"id": "2101.04699", "submitter": "Daniel Osaku", "authors": "D. Osaku, J.F. Gomes, A.X. Falc\\~ao", "title": "Convolutional Neural Network Simplification with Progressive Retraining", "comments": "7 pages, 4 figures. This paper was submitted to Pattern Recognition\n  Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Kernel pruning methods have been proposed to speed up, simplify, and improve\nexplanation of convolutional neural network (CNN) models. However, the\neffectiveness of a simplified model is often below the original one. In this\nletter, we present new methods based on objective and subjective relevance\ncriteria for kernel elimination in a layer-by-layer fashion. During the\nprocess, a CNN model is retrained only when the current layer is entirely\nsimplified, by adjusting the weights from the next layer to the first one and\npreserving weights of subsequent layers not involved in the process. We call\nthis strategy \\emph{progressive retraining}, differently from kernel pruning\nmethods that usually retrain the entire model after each simplification action\n-- e.g., the elimination of one or a few kernels. Our subjective relevance\ncriterion exploits the ability of humans in recognizing visual patterns and\nimproves the designer's understanding of the simplification process. The\ncombination of suitable relevance criteria and progressive retraining shows\nthat our methods can increase effectiveness with considerable model\nsimplification. We also demonstrate that our methods can provide better results\nthan two popular ones and another one from the state-of-the-art using four\nchallenging image datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:05:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Osaku", "D.", ""], ["Gomes", "J. F.", ""], ["Falc\u00e3o", "A. X.", ""]]}, {"id": "2101.04702", "submitter": "Jing Yu Koh", "authors": "Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, Yinfei Yang", "title": "Cross-Modal Contrastive Learning for Text-to-Image Generation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The output of text-to-image synthesis systems should be coherent, clear,\nphoto-realistic scenes with high semantic fidelity to their conditioned text\ndescriptions. Our Cross-Modal Contrastive Generative Adversarial Network\n(XMC-GAN) addresses this challenge by maximizing the mutual information between\nimage and text. It does this via multiple contrastive losses which capture\ninter-modality and intra-modality correspondences. XMC-GAN uses an attentional\nself-modulation generator, which enforces strong text-image correspondence, and\na contrastive discriminator, which acts as a critic as well as a feature\nencoder for contrastive learning. The quality of XMC-GAN's output is a major\nstep up from previous models, as we show on three challenging datasets. On\nMS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33,\nbut--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1\nfor image-text alignment, compared to three other recent models. XMC-GAN also\ngeneralizes to the challenging Localized Narratives dataset (which has longer,\nmore detailed descriptions), improving state-of-the-art FID from 48.70 to\n14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images\ndata, establishing a strong benchmark FID score of 26.91.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:16:51 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 06:00:20 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 19:47:23 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 06:55:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhang", "Han", ""], ["Koh", "Jing Yu", ""], ["Baldridge", "Jason", ""], ["Lee", "Honglak", ""], ["Yang", "Yinfei", ""]]}, {"id": "2101.04704", "submitter": "Xuebin Qin", "authors": "Xuebin Qin and Deng-Ping Fan and Chenyang Huang and Cyril Diagne and\n  Zichen Zhang and Adri\\`a Cabeza Sant'Anna and Albert Su\\`arez and Martin\n  Jagersand and Ling Shao", "title": "Boundary-Aware Segmentation Network for Mobile and Web Applications", "comments": "18 pages, 16 figures, submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although deep models have greatly improved the accuracy and robustness of\nimage segmentation, obtaining segmentation results with highly accurate\nboundaries and fine structures is still a challenging problem. In this paper,\nwe propose a simple yet powerful Boundary-Aware Segmentation Network (BASNet),\nwhich comprises a predict-refine architecture and a hybrid loss, for highly\naccurate image segmentation. The predict-refine architecture consists of a\ndensely supervised encoder-decoder network and a residual refinement module,\nwhich are respectively used to predict and refine a segmentation probability\nmap. The hybrid loss is a combination of the binary cross entropy, structural\nsimilarity and intersection-over-union losses, which guide the network to learn\nthree-level (ie, pixel-, patch- and map- level) hierarchy representations. We\nevaluate our BASNet on two reverse tasks including salient object segmentation,\ncamouflaged object segmentation, showing that it achieves very competitive\nperformance with sharp segmentation boundaries. Importantly, BASNet runs at\nover 70 fps on a single GPU which benefits many potential real applications.\nBased on BASNet, we further developed two (close to) commercial applications:\nAR COPY & PASTE, in which BASNet is integrated with augmented reality for\n\"COPYING\" and \"PASTING\" real-world objects, and OBJECT CUT, which is a\nweb-based tool for automatic object background removal. Both applications have\nalready drawn huge amount of attention and have important real-world impacts.\nThe code and two applications will be publicly available at:\nhttps://github.com/NathanUA/BASNet.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:20:26 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 09:05:13 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Qin", "Xuebin", ""], ["Fan", "Deng-Ping", ""], ["Huang", "Chenyang", ""], ["Diagne", "Cyril", ""], ["Zhang", "Zichen", ""], ["Sant'Anna", "Adri\u00e0 Cabeza", ""], ["Su\u00e0rez", "Albert", ""], ["Jagersand", "Martin", ""], ["Shao", "Ling", ""]]}, {"id": "2101.04713", "submitter": "David Torpey", "authors": "David Torpey and Richard Klein", "title": "Explicit homography estimation improves contrastive self-supervised\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The typical contrastive self-supervised algorithm uses a similarity measure\nin latent space as the supervision signal by contrasting positive and negative\nimages directly or indirectly. Although the utility of self-supervised\nalgorithms has improved recently, there are still bottlenecks hindering their\nwidespread use, such as the compute needed. In this paper, we propose a module\nthat serves as an additional objective in the self-supervised contrastive\nlearning paradigm. We show how the inclusion of this module to regress the\nparameters of an affine transformation or homography, in addition to the\noriginal contrastive objective, improves both performance and learning speed.\nImportantly, we ensure that this module does not enforce invariance to the\nvarious components of the affine transform, as this is not always ideal. We\ndemonstrate the effectiveness of the additional objective on two recent,\npopular self-supervised algorithms. We perform an extensive experimental\nanalysis of the proposed method and show an improvement in performance for all\nconsidered datasets. Further, we find that although both the general homography\nand affine transformation are sufficient to improve performance and\nconvergence, the affine transformation performs better in all cases.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:33:37 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Torpey", "David", ""], ["Klein", "Richard", ""]]}, {"id": "2101.04731", "submitter": "Zhiyuan Fang", "authors": "Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang,\n  Zicheng Liu", "title": "SEED: Self-supervised Distillation For Visual Representation", "comments": "Accepted as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with self-supervised learning for small models. The\nproblem is motivated by our empirical studies that while the widely used\ncontrastive self-supervised learning method has shown great progress on large\nmodel training, it does not work well for small models. To address this\nproblem, we propose a new learning paradigm, named SElf-SupErvised Distillation\n(SEED), where we leverage a larger network (as Teacher) to transfer its\nrepresentational knowledge into a smaller architecture (as Student) in a\nself-supervised fashion. Instead of directly learning from unlabeled data, we\ntrain a student encoder to mimic the similarity score distribution inferred by\na teacher over a set of instances. We show that SEED dramatically boosts the\nperformance of small networks on downstream tasks. Compared with\nself-supervised baselines, SEED improves the top-1 accuracy from 42.2% to 67.6%\non EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the\nImageNet-1k dataset.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:04:50 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 22:16:01 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Fang", "Zhiyuan", ""], ["Wang", "Jianfeng", ""], ["Wang", "Lijuan", ""], ["Zhang", "Lei", ""], ["Yang", "Yezhou", ""], ["Liu", "Zicheng", ""]]}, {"id": "2101.04741", "submitter": "Qi Feng", "authors": "Qi Feng, Vitaly Ablavsky, Stan Sclaroff", "title": "CityFlow-NL: Tracking and Retrieval of Vehicles at City Scale by Natural\n  Language Descriptions", "comments": "The code and data we use in this paper are available at:\n  https://github.com/fredfung007/cityflow-nl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language (NL) descriptions can be one of the most convenient or the\nonly way to interact with systems built to understand and detect city scale\ntraffic patterns and vehicle-related events. In this paper, we extend the\nwidely adopted CityFlow Benchmark with NL descriptions for vehicle targets and\nintroduce the CityFlow-NL Benchmark. The CityFlow-NL contains more than 5,000\nunique and precise NL descriptions of vehicle targets, making it the first\nmulti-target multi-camera tracking with NL descriptions dataset to our\nknowledge. Moreover, the dataset facilitates research at the intersection of\nmulti-object tracking, retrieval by NL descriptions, and temporal localization\nof events. In this paper, we focus on two foundational tasks: the Vehicle\nRetrieval by NL task and the Vehicle Tracking by NL task, which take advantage\nof the proposed CityFlow-NL benchmark and provide a strong basis for future\nresearch on the multi-target multi-camera tracking by NL description task.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:26:17 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 03:51:21 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 18:38:10 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Feng", "Qi", ""], ["Ablavsky", "Vitaly", ""], ["Sclaroff", "Stan", ""]]}, {"id": "2101.04756", "submitter": "Mohammad Akbari", "authors": "Seyedkooshan Hashemifard and Mohammad Akbari", "title": "A Compact Deep Learning Model for Face Spoofing Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, face biometric security systems are rapidly increasing,\ntherefore, the presentation attack detection (PAD) has received significant\nattention from research communities and has become a major field of research.\nResearchers have tackled the problem with various methods, from exploiting\nconventional texture feature extraction such as LBP, BSIF, and LPQ to using\ndeep neural networks with different architectures. Despite the results each of\nthese techniques has achieved for a certain attack scenario or dataset, most of\nthem still failed to generalized the problem for unseen conditions, as the\nefficiency of each is limited to certain type of presentation attacks and\ninstruments (PAI). In this paper, instead of completely extracting hand-crafted\ntexture features or relying only on deep neural networks, we address the\nproblem via fusing both wide and deep features in a unified neural\narchitecture. The main idea is to take advantage of the strength of both\nmethods to derive well-generalized solution for the problem. We also evaluated\nthe effectiveness of our method by comparing the results with each of the\nmentioned techniques separately. The procedure is done on different spoofing\ndatasets such as ROSE-Youtu, SiW and NUAA Imposter datasets.\n  In particular, we simultanously learn a low dimensional latent space\nempowered with data-driven features learnt via Convolutional Neural Network\ndesignes for spoofing detection task (i.e., deep channel) as well as leverages\nspoofing detection feature already popular for spoofing in frequency and\ntemporal dimensions ( i.e., via wide channel).\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 21:20:09 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hashemifard", "Seyedkooshan", ""], ["Akbari", "Mohammad", ""]]}, {"id": "2101.04772", "submitter": "Oliver Wang", "authors": "Jan Rueegg, Oliver Wang, Aljoscha Smolic, Markus Gross", "title": "DuctTake: Spatiotemporal Video Compositing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  DuctTake is a system designed to enable practical compositing of multiple\ntakes of a scene into a single video. Current industry solutions are based\naround object segmentation, a hard problem that requires extensive manual input\nand cleanup, making compositing an expensive part of the film-making process.\nOur method instead composites shots together by finding optimal spatiotemporal\nseams using motion-compensated 3D graph cuts through the video volume. We\ndescribe in detail the required components, decisions, and new techniques that\ntogether make a usable, interactive tool for compositing HD video, paying\nspecial attention to running time and performance of each section. We validate\nour approach by presenting a wide variety of examples and by comparing result\nquality and creation time to composites made by professional artists using\ncurrent state-of-the-art tools.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 21:58:47 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Rueegg", "Jan", ""], ["Wang", "Oliver", ""], ["Smolic", "Aljoscha", ""], ["Gross", "Markus", ""]]}, {"id": "2101.04775", "submitter": "Bingchen Liu", "authors": "Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal", "title": "Towards Faster and Stabilized GAN Training for High-fidelity Few-shot\n  Image Synthesis", "comments": "ICLR-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Generative Adversarial Networks (GAN) on high-fidelity images\nusually requires large-scale GPU-clusters and a vast number of training images.\nIn this paper, we study the few-shot image synthesis task for GAN with minimum\ncomputing cost. We propose a light-weight GAN structure that gains superior\nquality on 1024*1024 resolution. Notably, the model converges from scratch with\njust a few hours of training on a single RTX-2080 GPU, and has a consistent\nperformance, even with less than 100 training samples. Two technique designs\nconstitute our work, a skip-layer channel-wise excitation module and a\nself-supervised discriminator trained as a feature-encoder. With thirteen\ndatasets covering a wide variety of image domains (The datasets and code are\navailable at: https://github.com/odegeasslbc/FastGAN-pytorch), we show our\nmodel's superior performance compared to the state-of-the-art StyleGAN2, when\ndata and computing budget are limited.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:02:54 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Liu", "Bingchen", ""], ["Zhu", "Yizhe", ""], ["Song", "Kunpeng", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "2101.04777", "submitter": "Abhishek Badki", "authors": "Abhishek Badki, Orazio Gallo, Jan Kautz, Pradeep Sen", "title": "Binary TTC: A Temporal Geofence for Autonomous Navigation", "comments": "To be presented at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-to-contact (TTC), the time for an object to collide with the observer's\nplane, is a powerful tool for path planning: it is potentially more informative\nthan the depth, velocity, and acceleration of objects in the scene -- even for\nhumans. TTC presents several advantages, including requiring only a monocular,\nuncalibrated camera. However, regressing TTC for each pixel is not\nstraightforward, and most existing methods make over-simplifying assumptions\nabout the scene. We address this challenge by estimating TTC via a series of\nsimpler, binary classifications. We predict with low latency whether the\nobserver will collide with an obstacle within a certain time, which is often\nmore critical than knowing exact, per-pixel TTC. For such scenarios, our method\noffers a temporal geofence in 6.4 ms -- over 25x faster than existing methods.\nOur approach can also estimate per-pixel TTC with arbitrarily fine quantization\n(including continuous values), when the computational budget allows for it. To\nthe best of our knowledge, our method is the first to offer TTC information\n(binary or coarsely quantized) at sufficiently high frame-rates for practical\nuse.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:05:21 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 16:50:11 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Badki", "Abhishek", ""], ["Gallo", "Orazio", ""], ["Kautz", "Jan", ""], ["Sen", "Pradeep", ""]]}, {"id": "2101.04781", "submitter": "Kilian Kleeberger", "authors": "Kilian Kleeberger and Markus V\\\"olk and Marius Moosmann and Erik\n  Thiessenhusen and Florian Roth and Richard Bormann and Marco F. Huber", "title": "Transferring Experience from Simulation to the Real World for Precise\n  Pick-And-Place Tasks in Highly Cluttered Scenes", "comments": "Accepted at 2020 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel learning-based approach for grasping\nknown rigid objects in highly cluttered scenes and precisely placing them based\non depth images. Our Placement Quality Network (PQ-Net) estimates the object\npose and the quality for each automatically generated grasp pose for multiple\nobjects simultaneously at 92 fps in a single forward pass of a neural network.\nAll grasping and placement trials are executed in a physics simulation and the\ngained experience is transferred to the real world using domain randomization.\nWe demonstrate that our policy successfully transfers to the real world. PQ-Net\noutperforms other model-free approaches in terms of grasping success rate and\nautomatically scales to new objects of arbitrary symmetry without any human\nintervention.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:16:47 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Kleeberger", "Kilian", ""], ["V\u00f6lk", "Markus", ""], ["Moosmann", "Marius", ""], ["Thiessenhusen", "Erik", ""], ["Roth", "Florian", ""], ["Bormann", "Richard", ""], ["Huber", "Marco F.", ""]]}, {"id": "2101.04793", "submitter": "Xiaocong Chen", "authors": "Xiaocong Chen and Yun Li and Lina Yao and Ehsan Adeli and Yu Zhang", "title": "Generative Adversarial U-Net for Domain-free Medical Image Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shortage of annotated medical images is one of the biggest challenges in\nthe field of medical image computing. Without a sufficient number of training\nsamples, deep learning based models are very likely to suffer from over-fitting\nproblem. The common solution is image manipulation such as image rotation,\ncropping, or resizing. Those methods can help relieve the over-fitting problem\nas more training samples are introduced. However, they do not really introduce\nnew images with additional information and may lead to data leakage as the test\nset may contain similar samples which appear in the training set. To address\nthis challenge, we propose to generate diverse images with generative\nadversarial network. In this paper, we develop a novel generative method named\ngenerative adversarial U-Net , which utilizes both generative adversarial\nnetwork and U-Net. Different from existing approaches, our newly designed model\nis domain-free and generalizable to various medical images. Extensive\nexperiments are conducted over eight diverse datasets including computed\ntomography (CT) scan, pathology, X-ray, etc. The visualization and quantitative\nresults demonstrate the efficacy and good generalization of the proposed method\non generating a wide array of high-quality medical images.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:02:26 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Chen", "Xiaocong", ""], ["Li", "Yun", ""], ["Yao", "Lina", ""], ["Adeli", "Ehsan", ""], ["Zhang", "Yu", ""]]}, {"id": "2101.04800", "submitter": "Nicolas Tobis", "authors": "Ognjen Rudovic, Nicolas Tobis, Sebastian Kaltwang, Bj\\\"orn Schuller,\n  Daniel Rueckert, Jeffrey F. Cohn and Rosalind W. Picard", "title": "Personalized Federated Deep Learning for Pain Estimation From Face\n  Images", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard machine learning approaches require centralizing the users' data in\none computer or a shared database, which raises data privacy and\nconfidentiality concerns. Therefore, limiting central access is important,\nespecially in healthcare settings, where data regulations are strict. A\npotential approach to tackling this is Federated Learning (FL), which enables\nmultiple parties to collaboratively learn a shared prediction model by using\nparameters of locally trained models while keeping raw training data locally.\nIn the context of AI-assisted pain-monitoring, we wish to enable\nconfidentiality-preserving and unobtrusive pain estimation for long-term\npain-monitoring and reduce the burden on the nursing staff who perform frequent\nroutine check-ups. To this end, we propose a novel Personalized Federated Deep\nLearning (PFDL) approach for pain estimation from face images. PFDL performs\ncollaborative training of a deep model, implemented using a lightweight CNN\narchitecture, across different clients (i.e., subjects) without sharing their\nface images. Instead of sharing all parameters of the model, as in standard FL,\nPFDL retains the last layer locally (used to personalize the pain estimates).\nThis (i) adds another layer of data confidentiality, making it difficult for an\nadversary to infer pain levels of the target subject, while (ii) personalizing\nthe pain estimation to each subject through local parameter tuning. We show\nusing a publicly available dataset of face videos of pain (UNBC-McMaster\nShoulder Pain Database), that PFDL performs comparably or better than the\nstandard centralized and FL algorithms, while further enhancing data privacy.\nThis, has the potential to improve traditional pain monitoring by making it\nmore secure, computationally efficient, and scalable to a large number of\nindividuals (e.g., for in-home pain monitoring), providing timely and\nunobtrusive pain measurement.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:21:25 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Rudovic", "Ognjen", ""], ["Tobis", "Nicolas", ""], ["Kaltwang", "Sebastian", ""], ["Schuller", "Bj\u00f6rn", ""], ["Rueckert", "Daniel", ""], ["Cohn", "Jeffrey F.", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "2101.04804", "submitter": "Beatriz Asfora", "authors": "Beatriz Arruda Asfora", "title": "Embedded Computer Vision System Applied to a Four-Legged Line Follower\n  Robot", "comments": null, "journal-ref": "23rd ABCM International Congress of Mechanical\n  Engineering,December 6-11, 2015, Rio de Janeiro, RJ, Brazil", "doi": "10.20906/CPS/COB-2015-1649", "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.IV eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Robotics can be defined as the connection of perception to action. Taking\nthis further, this project aims to drive a robot using an automated computer\nvision embedded system, connecting the robot's vision to its behavior. In order\nto implement a color recognition system on the robot, open source tools are\nchosen, such as Processing language, Android system, Arduino platform and Pixy\ncamera. The constraints are clear: simplicity, replicability and financial\nviability. In order to integrate Robotics, Computer Vision and Image\nProcessing, the robot is applied on a typical mobile robot's issue: line\nfollowing. The problem of distinguishing the path from the background is\nanalyzed through different approaches: the popular Otsu's Method, thresholding\nbased on color combinations through experimentation and color tracking via hue\nand saturation. Decision making of where to move next is based on the line\ncenter of the path and is fully automated. Using a four-legged robot as\nplatform and a camera as its only sensor, the robot is capable of successfully\nfollow a line. From capturing the image to moving the robot, it's evident how\nintegrative Robotics can be. The issue of this paper alone involves knowledge\nof Mechanical Engineering, Electronics, Control Systems and Programming.\nEverything related to this work was documented and made available on an open\nsource online page, so it can be useful in learning and experimenting with\nrobotics.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:52:53 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Asfora", "Beatriz Arruda", ""]]}, {"id": "2101.04812", "submitter": "Casey Handmer", "authors": "Casey Handmer", "title": "Digital Elevation Model enhancement using Deep Learning", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.EP cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate high fidelity enhancement of planetary digital elevation\nmodels (DEMs) using optical images and deep learning with convolutional neural\nnetworks. Enhancement can be applied recursively to the limit of available\noptical data, representing a 90x resolution improvement in global Mars DEMs.\nDeep learning-based photoclinometry robustly recovers features obscured by\nnon-ideal lighting conditions. Method can be automated at global scale.\nAnalysis shows enhanced DEM slope errors are comparable with high resolution\nmaps using conventional, labor intensive methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:07:57 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Handmer", "Casey", ""]]}, {"id": "2101.04822", "submitter": "Yang Liu", "authors": "Xin Yuan, Yang Liu, Jinli Suo, Fr\\'edo Durand, Qionghai Dai", "title": "Plug-and-Play Algorithms for Video Snapshot Compressive Imaging", "comments": "18 pages, 12 figures and 4 tables. Journal extension of\n  arXiv:2003.13654. Code available at\n  https://github.com/liuyang12/PnP-SCI_python", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the reconstruction problem of video snapshot compressive imaging\n(SCI), which captures high-speed videos using a low-speed 2D sensor (detector).\nThe underlying principle of SCI is to modulate sequential high-speed frames\nwith different masks and then these encoded frames are integrated into a\nsnapshot on the sensor and thus the sensor can be of low-speed. On one hand,\nvideo SCI enjoys the advantages of low-bandwidth, low-power and low-cost. On\nthe other hand, applying SCI to large-scale problems (HD or UHD videos) in our\ndaily life is still challenging and one of the bottlenecks lies in the\nreconstruction algorithm. Exiting algorithms are either too slow (iterative\noptimization algorithms) or not flexible to the encoding process (deep learning\nbased end-to-end networks). In this paper, we develop fast and flexible\nalgorithms for SCI based on the plug-and-play (PnP) framework. In addition to\nthe PnP-ADMM method, we further propose the PnP-GAP (generalized alternating\nprojection) algorithm with a lower computational workload. We first employ the\nimage deep denoising priors to show that PnP can recover a UHD color video with\n30 frames from a snapshot measurement. Since videos have strong temporal\ncorrelation, by employing the video deep denoising priors, we achieve a\nsignificant improvement in the results. Furthermore, we extend the proposed PnP\nalgorithms to the color SCI system using mosaic sensors, where each pixel only\ncaptures the red, green or blue channels. A joint reconstruction and\ndemosaicing paradigm is developed for flexible and high quality reconstruction\nof color video SCI systems. Extensive results on both simulation and real\ndatasets verify the superiority of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:51:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Yuan", "Xin", ""], ["Liu", "Yang", ""], ["Suo", "Jinli", ""], ["Durand", "Fr\u00e9do", ""], ["Dai", "Qionghai", ""]]}, {"id": "2101.04823", "submitter": "Alexandre de Siqueira", "authors": "Alexandre Fioravante de Siqueira and Daniela Mayumi Ushizima and\n  St\\'efan van der Walt", "title": "A reusable pipeline for large-scale fiber segmentation on unidirectional\n  fiber beds using fully convolutional neural networks", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fiber-reinforced ceramic-matrix composites are advanced materials resistant\nto high temperatures, with application to aerospace engineering. Their analysis\ndepends on the detection of embedded fibers, with semi-supervised techniques\nusually employed to separate fibers within the fiber beds. Here we present an\nopen computational pipeline to detect fibers in ex-situ X-ray computed\ntomography fiber beds. To separate the fibers in these samples, we tested four\ndifferent architectures of fully convolutional neural networks. When comparing\nour neural network approach to a semi-supervised one, we obtained Dice and\nMatthews coefficients greater than $92.28 \\pm 9.65\\%$, reaching up to $98.42\n\\pm 0.03 \\%$, showing that the network results are close to the\nhuman-supervised ones in these fiber beds, in some cases separating fibers that\nhuman-curated algorithms could not find. The software we generated in this\nproject is open source, released under a permissive license, and can be freely\nadapted and re-used in other domains. All data and instructions on how to\ndownload and use it are also available.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:58:29 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 00:33:14 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["de Siqueira", "Alexandre Fioravante", ""], ["Ushizima", "Daniela Mayumi", ""], ["van der Walt", "St\u00e9fan", ""]]}, {"id": "2101.04827", "submitter": "Zhinan Qiao", "authors": "Zhinan Qiao, Xiaohui Yuan", "title": "Urban land-use analysis using proximate sensing imagery: a survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban regions are complicated functional systems that are closely associated\nwith and reshaped by human activities. The propagation of online geographic\ninformation-sharing platforms and mobile devices equipped with Global\nPositioning System (GPS) greatly proliferates proximate sensing images taken\nnear or on the ground at a close distance to urban targets. Studies leveraging\nproximate sensing imagery have demonstrated great potential to address the need\nfor local data in urban land-use analysis. This paper reviews and summarizes\nthe state-of-the-art methods and publicly available datasets from proximate\nsensing to support land-use analysis. We identify several research problems in\nthe perspective of examples to support training of models and means of\nintegrating diverse data sets. Our discussions highlight the challenges,\nstrategies, and opportunities faced by the existing methods using proximate\nsensing imagery in urban land-use studies.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 01:30:21 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 04:24:43 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Qiao", "Zhinan", ""], ["Yuan", "Xiaohui", ""]]}, {"id": "2101.04829", "submitter": "Junyoung Byun", "authors": "Junyoung Byun, Hyojun Go, Changick Kim", "title": "Small Input Noise is Enough to Defend Against Query-based Black-box\n  Attacks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks show unprecedented performance in various tasks,\nthe vulnerability to adversarial examples hinders their deployment in\nsafety-critical systems. Many studies have shown that attacks are also possible\neven in a black-box setting where an adversary cannot access the target model's\ninternal information. Most black-box attacks are based on queries, each of\nwhich obtains the target model's output for an input, and many recent studies\nfocus on reducing the number of required queries. In this paper, we pay\nattention to an implicit assumption of these attacks that the target model's\noutput exactly corresponds to the query input. If some randomness is introduced\ninto the model to break this assumption, query-based attacks may have\ntremendous difficulty in both gradient estimation and local search, which are\nthe core of their attack process. From this motivation, we observe even a small\nadditive input noise can neutralize most query-based attacks and name this\nsimple yet effective approach Small Noise Defense (SND). We analyze how SND can\ndefend against query-based black-box attacks and demonstrate its effectiveness\nagainst eight different state-of-the-art attacks with CIFAR-10 and ImageNet\ndatasets. Even with strong defense ability, SND almost maintains the original\nclean accuracy and computational speed. SND is readily applicable to\npre-trained models by adding only one line of code at the inference stage, so\nwe hope that it will be used as a baseline of defense against query-based\nblack-box attacks in the future.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 01:45:59 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Byun", "Junyoung", ""], ["Go", "Hyojun", ""], ["Kim", "Changick", ""]]}, {"id": "2101.04836", "submitter": "Sriramya Bhamidipati", "authors": "Sriramya Bhamidipati and Grace Xingxin Gao", "title": "Robust GPS-Vision Localization via Integrity-Driven Landmark Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For robust GPS-vision navigation in urban areas, we propose an\nIntegrity-driven Landmark Attention (ILA) technique via stochastic\nreachability. Inspired by cognitive attention in humans, we perform convex\noptimization to select a subset of landmarks from GPS and vision measurements\nthat maximizes integrity-driven performance. Given known measurement error\nbounds in non-faulty conditions, our ILA follows a unified approach to address\nboth GPS and vision faults and is compatible with any off-the-shelf estimator.\nWe analyze measurement deviation to estimate the stochastic reachable set of\nexpected position for each landmark, which is parameterized via probabilistic\nzonotope (p-Zonotope). We apply set union to formulate a p-Zonotopic cost that\nrepresents the size of position bounds based on landmark inclusion/exclusion.\nWe jointly minimize the p-Zonotopic cost and maximize the number of landmarks\nvia convex relaxation. For an urban dataset, we demonstrate improved\nlocalization accuracy and robust predicted availability for a pre-defined alert\nlimit.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 02:16:13 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Bhamidipati", "Sriramya", ""], ["Gao", "Grace Xingxin", ""]]}, {"id": "2101.04838", "submitter": "Ling Zhou", "authors": "Ling Zhou, Qirong Mao, Xiaohua Huang, Feifei Zhang, Zhihong Zhang", "title": "Feature refinement: An expression-specific feature learning and fusion\n  method for micro-expression recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-Expression Recognition has become challenging, as it is extremely\ndifficult to extract the subtle facial changes of micro-expressions. Recently,\nseveral approaches proposed several expression-shared features algorithms for\nmicro-expression recognition. However, they do not reveal the specific\ndiscriminative characteristics, which lead to sub-optimal performance. This\npaper proposes a novel Feature Refinement ({FR}) with expression-specific\nfeature learning and fusion for micro-expression recognition. It aims to obtain\nsalient and discriminative features for specific expressions and also predict\nexpression by fusing the expression-specific features. FR consists of an\nexpression proposal module with attention mechanism and a classification\nbranch. First, an inception module is designed based on optical flow to obtain\nexpression-shared features. Second, in order to extract salient and\ndiscriminative features for specific expression, expression-shared features are\nfed into an expression proposal module with attention factors and proposal\nloss. Last, in the classification branch, labels of categories are predicted by\na fusion of the expression-specific features. Experiments on three publicly\navailable databases validate the effectiveness of FR under different protocol.\nResults on public benchmarks demonstrate that our FR provides salient and\ndiscriminative information for micro-expression recognition. The results also\nshow our FR achieves better or competitive performance with the existing\nstate-of-the-art methods on micro-expression recognition.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 02:26:07 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zhou", "Ling", ""], ["Mao", "Qirong", ""], ["Huang", "Xiaohua", ""], ["Zhang", "Feifei", ""], ["Zhang", "Zhihong", ""]]}, {"id": "2101.04844", "submitter": "Senwei Liang", "authors": "Senwei Liang and Liyao Lyu and Chunmei Wang and Haizhao Yang", "title": "Reproducing Activation Function for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose reproducing activation functions (RAFs) to improve deep learning\naccuracy for various applications ranging from computer vision to scientific\ncomputing. The idea is to employ several basic functions and their learnable\nlinear combination to construct neuron-wise data-driven activation functions\nfor each neuron. Armed with RAFs, neural networks (NNs) can reproduce\ntraditional approximation tools and, therefore, approximate target functions\nwith a smaller number of parameters than traditional NNs. In NN training, RAFs\ncan generate neural tangent kernels (NTKs) with a better condition number than\ntraditional activation functions lessening the spectral bias of deep learning.\nAs demonstrated by extensive numerical tests, the proposed RAFs can facilitate\nthe convergence of deep learning optimization for a solution with higher\naccuracy than existing deep learning solvers for audio/image/video\nreconstruction, PDEs, and eigenvalue problems. With RAFs, the errors of\naudio/video reconstruction, PDEs, and eigenvalue problems are decreased by over\n14%, 73%, 99%, respectively, compared with baseline, while the performance of\nimage reconstruction increases by 58%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 02:53:22 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 04:29:58 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Liang", "Senwei", ""], ["Lyu", "Liyao", ""], ["Wang", "Chunmei", ""], ["Yang", "Haizhao", ""]]}, {"id": "2101.04882", "submitter": "Lilian Weng", "authors": "OpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya,\n  Vineet Kosaraju, Peter Welinder, Ruben D'Sa, Arthur Petron, Henrique P. d.O.\n  Pinto, Alex Paino, Hyeonwoo Noh, Lilian Weng, Qiming Yuan, Casey Chu,\n  Wojciech Zaremba", "title": "Asymmetric self-play for automatic goal discovery in robotic\n  manipulation", "comments": "Videos are shown at https://robotics-self-play.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train a single, goal-conditioned policy that can solve many robotic\nmanipulation tasks, including tasks with previously unseen goals and objects.\nWe rely on asymmetric self-play for goal discovery, where two agents, Alice and\nBob, play a game. Alice is asked to propose challenging goals and Bob aims to\nsolve them. We show that this method can discover highly diverse and complex\ngoals without any human priors. Bob can be trained with only sparse rewards,\nbecause the interaction between Alice and Bob results in a natural curriculum\nand Bob can learn from Alice's trajectory when relabeled as a goal-conditioned\ndemonstration. Finally, our method scales, resulting in a single policy that\ncan generalize to many unseen tasks such as setting a table, stacking blocks,\nand solving simple puzzles. Videos of a learned policy is available at\nhttps://robotics-self-play.github.io.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 05:20:20 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["OpenAI", "OpenAI", ""], ["Plappert", "Matthias", ""], ["Sampedro", "Raul", ""], ["Xu", "Tao", ""], ["Akkaya", "Ilge", ""], ["Kosaraju", "Vineet", ""], ["Welinder", "Peter", ""], ["D'Sa", "Ruben", ""], ["Petron", "Arthur", ""], ["Pinto", "Henrique P. d. O.", ""], ["Paino", "Alex", ""], ["Noh", "Hyeonwoo", ""], ["Weng", "Lilian", ""], ["Yuan", "Qiming", ""], ["Chu", "Casey", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "2101.04884", "submitter": "Paritosh Parmar", "authors": "Paritosh Parmar, Jaiden Reddy, Brendan Morris", "title": "Piano Skills Assessment", "comments": "Dataset is available from:\n  https://github.com/ParitoshParmar/Piano-Skills-Assessment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can a computer determine a piano player's skill level? Is it preferable to\nbase this assessment on visual analysis of the player's performance or should\nwe trust our ears over our eyes? Since current CNNs have difficulty processing\nlong video videos, how can shorter clips be sampled to best reflect the players\nskill level? In this work, we collect and release a first-of-its-kind dataset\nfor multimodal skill assessment focusing on assessing piano player's skill\nlevel, answer the asked questions, initiate work in automated evaluation of\npiano playing skills and provide baselines for future work. Dataset is\navailable from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 05:26:29 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 01:57:59 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Parmar", "Paritosh", ""], ["Reddy", "Jaiden", ""], ["Morris", "Brendan", ""]]}, {"id": "2101.04889", "submitter": "Yuzhou Lin", "authors": "Yuzhou Lin, Xiaolin Chang", "title": "Towards Interpretable Ensemble Learning for Image-based Malware\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) models for image-based malware detection have exhibited\ntheir capability in producing high prediction accuracy. But model\ninterpretability is posing challenges to their widespread application in\nsecurity and safety-critical application domains. This paper aims for designing\nan Interpretable Ensemble learning approach for image-based Malware Detection\n(IEMD). We first propose a Selective Deep Ensemble Learning-based (SDEL)\ndetector and then design an Ensemble Deep Taylor Decomposition (EDTD) approach,\nwhich can give the pixel-level explanation to SDEL detector outputs.\nFurthermore, we develop formulas for calculating fidelity, robustness and\nexpressiveness on pixel-level heatmaps in order to assess the quality of EDTD\nexplanation. With EDTD explanation, we develop a novel Interpretable Dropout\napproach (IDrop), which establishes IEMD by training SDEL detector. Experiment\nresults exhibit the better explanation of our EDTD than the previous\nexplanation methods for image-based malware detection. Besides, experiment\nresults indicate that IEMD achieves a higher detection accuracy up to 99.87%\nwhile exhibiting interpretability with high quality of prediction results.\nMoreover, experiment results indicate that IEMD interpretability increases with\nthe increasing detection accuracy during the construction of IEMD. This\nconsistency suggests that IDrop can mitigate the tradeoff between model\ninterpretability and detection accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 05:46:44 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Lin", "Yuzhou", ""], ["Chang", "Xiaolin", ""]]}, {"id": "2101.04898", "submitter": "Hanxun Huang", "authors": "Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, Yisen\n  Wang", "title": "Unlearnable Examples: Making Personal Data Unexploitable", "comments": "ICLR2021, In International Conference on Learning Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The volume of \"free\" data on the internet has been key to the current success\nof deep learning. However, it also raises privacy concerns about the\nunauthorized exploitation of personal data for training commercial models. It\nis thus crucial to develop methods to prevent unauthorized data exploitation.\nThis paper raises the question: \\emph{can data be made unlearnable for deep\nlearning models?} We present a type of \\emph{error-minimizing} noise that can\nindeed make training examples unlearnable. Error-minimizing noise is\nintentionally generated to reduce the error of one or more of the training\nexample(s) close to zero, which can trick the model into believing there is\n\"nothing\" to learn from these example(s). The noise is restricted to be\nimperceptible to human eyes, and thus does not affect normal data utility. We\nempirically verify the effectiveness of error-minimizing noise in both\nsample-wise and class-wise forms. We also demonstrate its flexibility under\nextensive experimental settings and practicability in a case study of face\nrecognition. Our work establishes an important first step towards making\npersonal data unexploitable to deep learning models.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 06:15:56 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 22:53:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Huang", "Hanxun", ""], ["Ma", "Xingjun", ""], ["Erfani", "Sarah Monazam", ""], ["Bailey", "James", ""], ["Wang", "Yisen", ""]]}, {"id": "2101.04904", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "comments": "Added link to the code in the paper. A preliminary version of this\n  work was presented at ICML 2020 Workshop on Lifelong Machine Learning:\n  arXiv:2007.06637", "journal-ref": "International Conference on Learning Representations (ICLR) 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The two main impediments to continual learning are catastrophic forgetting\nand memory limitations on the storage of data. To cope with these challenges,\nwe propose a novel, cognitively-inspired approach which trains autoencoders\nwith Neural Style Transfer to encode and store images. During training on a new\ntask, reconstructed images from encoded episodes are replayed in order to avoid\ncatastrophic forgetting. The loss function for the reconstructed images is\nweighted to reduce its effect during classifier training to cope with image\ndegradation. When the system runs out of memory the encoded episodes are\nconverted into centroids and covariance matrices, which are used to generate\npseudo-images during classifier training, keeping classifier performance stable\nwhile using less memory. Our approach increases classification accuracy by\n13-17% over state-of-the-art methods on benchmark datasets, while requiring 78%\nless storage space.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 06:43:10 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 09:16:24 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 05:05:05 GMT"}, {"version": "v4", "created": "Sun, 2 May 2021 05:45:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2101.04909", "submitter": "Anuroop Sriram", "authors": "Anuroop Sriram, Matthew Muckley, Koustuv Sinha, Farah Shamout, Joelle\n  Pineau, Krzysztof J. Geras, Lea Azour, Yindalon Aphinyanaphongs, Nafissa\n  Yakubova, William Moore", "title": "COVID-19 Prognosis via Self-Supervised Representation Learning and\n  Multi-Image Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid spread of COVID-19 cases in recent months has strained hospital\nresources, making rapid and accurate triage of patients presenting to emergency\ndepartments a necessity. Machine learning techniques using clinical data such\nas chest X-rays have been used to predict which patients are most at risk of\ndeterioration. We consider the task of predicting two types of patient\ndeterioration based on chest X-rays: adverse event deterioration (i.e.,\ntransfer to the intensive care unit, intubation, or mortality) and increased\noxygen requirements beyond 6 L per day. Due to the relative scarcity of\nCOVID-19 patient data, existing solutions leverage supervised pretraining on\nrelated non-COVID images, but this is limited by the differences between the\npretraining data and the target COVID-19 patient data. In this paper, we use\nself-supervised learning based on the momentum contrast (MoCo) method in the\npretraining phase to learn more general image representations to use for\ndownstream tasks. We present three results. The first is deterioration\nprediction from a single image, where our model achieves an area under receiver\noperating characteristic curve (AUC) of 0.742 for predicting an adverse event\nwithin 96 hours (compared to 0.703 with supervised pretraining) and an AUC of\n0.765 for predicting oxygen requirements greater than 6 L a day at 24 hours\n(compared to 0.749 with supervised pretraining). We then propose a new\ntransformer-based architecture that can process sequences of multiple images\nfor prediction and show that this model can achieve an improved AUC of 0.786\nfor predicting an adverse event at 96 hours and an AUC of 0.848 for predicting\nmortalities at 96 hours. A small pilot clinical study suggested that the\nprediction accuracy of our model is comparable to that of experienced\nradiologists analyzing the same information.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 07:03:17 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 04:52:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sriram", "Anuroop", ""], ["Muckley", "Matthew", ""], ["Sinha", "Koustuv", ""], ["Shamout", "Farah", ""], ["Pineau", "Joelle", ""], ["Geras", "Krzysztof J.", ""], ["Azour", "Lea", ""], ["Aphinyanaphongs", "Yindalon", ""], ["Yakubova", "Nafissa", ""], ["Moore", "William", ""]]}, {"id": "2101.04924", "submitter": "Yu Wu", "authors": "Yu Wu, Linchao Zhu, Xiaohan Wang, Yi Yang, Fei Wu", "title": "Learning to Anticipate Egocentric Actions by Imagination", "comments": "Accepted to IEEE Transactions on Image Processing (TIP)", "journal-ref": "IEEE Transactions on Image Processing, vol. 30, pp. 1143-1152,\n  2021", "doi": "10.1109/TIP.2020.3040521", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating actions before they are executed is crucial for a wide range of\npractical applications, including autonomous driving and robotics. In this\npaper, we study the egocentric action anticipation task, which predicts future\naction seconds before it is performed for egocentric videos. Previous\napproaches focus on summarizing the observed content and directly predicting\nfuture action based on past observations. We believe it would benefit the\naction anticipation if we could mine some cues to compensate for the missing\ninformation of the unobserved frames. We then propose to decompose the action\nanticipation into a series of future feature predictions. We imagine how the\nvisual feature changes in the near future and then predicts future action\nlabels based on these imagined representations. Differently, our ImagineRNN is\noptimized in a contrastive learning way instead of feature regression. We\nutilize a proxy task to train the ImagineRNN, i.e., selecting the correct\nfuture states from distractors. We further improve ImagineRNN by residual\nanticipation, i.e., changing its target to predicting the feature difference of\nadjacent frames instead of the frame content. This promotes the network to\nfocus on our target, i.e., the future action, as the difference between\nadjacent frame features is more important for forecasting the future. Extensive\nexperiments on two large-scale egocentric action datasets validate the\neffectiveness of our method. Our method significantly outperforms previous\nmethods on both the seen test set and the unseen test set of the EPIC Kitchens\nAction Anticipation Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 08:04:10 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 11:02:10 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Wu", "Yu", ""], ["Zhu", "Linchao", ""], ["Wang", "Xiaohan", ""], ["Yang", "Yi", ""], ["Wu", "Fei", ""]]}, {"id": "2101.04927", "submitter": "Boris Faizov", "authors": "Anton Konushin, Boris Faizov, Vlad Shakhuro", "title": "Road images augmentation with synthetic traffic signs using neural\n  networks", "comments": "The paper was submitted to the journal \"Computer Optics\" and is\n  currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic sign recognition is a well-researched problem in computer vision.\nHowever, the state of the art methods works only for frequent sign classes,\nwhich are well represented in training datasets. We consider the task of rare\ntraffic sign detection and classification. We aim to solve that problem by\nusing synthetic training data. Such training data is obtained by embedding\nsynthetic images of signs in the real photos. We propose three methods for\nmaking synthetic signs consistent with a scene in appearance. These methods are\nbased on modern generative adversarial network (GAN) architectures. Our\nproposed methods allow realistic embedding of rare traffic sign classes that\nare absent in the training set. We adapt a variational autoencoder for sampling\nplausible locations of new traffic signs in images. We demonstrate that using a\nmixture of our synthetic data with real data improves the accuracy of both\nclassifier and detector.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 08:10:33 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Konushin", "Anton", ""], ["Faizov", "Boris", ""], ["Shakhuro", "Vlad", ""]]}, {"id": "2101.04929", "submitter": "Emmanuel Hartman", "authors": "Emmanuel Hartman, Yashil Sukurdeep, Nicolas Charon, Eric Klassen,\n  Martin Bauer", "title": "Supervised deep learning of elastic SRV distances on the shape space of\n  curves", "comments": "8 pages, 7 figures, 3 tables. Accepted to DiffCVML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by applications from computer vision to bioinformatics, the field\nof shape analysis deals with problems where one wants to analyze geometric\nobjects, such as curves, while ignoring actions that preserve their shape, such\nas translations, rotations, or reparametrizations. Mathematical tools have been\ndeveloped to define notions of distances, averages, and optimal deformations\nfor geometric objects. One such framework, which has proven to be successful in\nmany applications, is based on the square root velocity (SRV) transform, which\nallows one to define a computable distance between spatial curves regardless of\nhow they are parametrized. This paper introduces a supervised deep learning\nframework for the direct computation of SRV distances between curves, which\nusually requires an optimization over the group of reparametrizations that act\non the curves. The benefits of our approach in terms of computational speed and\naccuracy are illustrated via several numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 08:15:23 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 08:01:35 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hartman", "Emmanuel", ""], ["Sukurdeep", "Yashil", ""], ["Charon", "Nicolas", ""], ["Klassen", "Eric", ""], ["Bauer", "Martin", ""]]}, {"id": "2101.04935", "submitter": "Bohan Zhuang", "authors": "Jing Liu, Bohan Zhuang, Peng Chen, Yong Guo, Chunhua Shen, Jianfei\n  Cai, Mingkui Tan", "title": "LBS: Loss-aware Bit Sharing for Automatic Model Compression", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-bitwidth model compression is an effective method to reduce the model\nsize and computational overhead. Existing compression methods rely on some\ncompression configurations (such as pruning rates, and/or bitwidths), which are\noften determined manually and not optimal. Some attempts have been made to\nsearch them automatically, but the optimization process is often very\nexpensive. To alleviate this, we devise a simple yet effective method named\nLoss-aware Bit Sharing (LBS) to automatically search for optimal model\ncompression configurations. To this end, we propose a novel single-path model\nto encode all candidate compression configurations, where a high bitwidth\nquantized value can be decomposed into the sum of the lowest bitwidth quantized\nvalue and a series of re-assignment offsets. We then introduce learnable binary\ngates to encode the choice of bitwidth, including filter-wise 0-bit for filter\npruning. By jointly training the binary gates in conjunction with network\nparameters, the compression configurations of each layer can be automatically\ndetermined. Extensive experiments on both CIFAR-100 and ImageNet show that LBS\nis able to significantly reduce computational cost while preserving promising\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 08:28:21 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 12:07:31 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Liu", "Jing", ""], ["Zhuang", "Bohan", ""], ["Chen", "Peng", ""], ["Guo", "Yong", ""], ["Shen", "Chunhua", ""], ["Cai", "Jianfei", ""], ["Tan", "Mingkui", ""]]}, {"id": "2101.04943", "submitter": "Christian Marzahl", "authors": "Christian Marzahl, Christof A. Bertram, Frauke Wilm, J\\\"orn Voigt, Ann\n  K. Barton, Robert Klopfleisch, Katharina Breininger, Andreas Maier, Marc\n  Aubreville", "title": "Learning to be EXACT, Cell Detection for Asthma on Partially Annotated\n  Whole Slide Images", "comments": "Submitted to BVM", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Asthma is a chronic inflammatory disorder of the lower respiratory tract and\nnaturally occurs in humans and animals including horses. The annotation of an\nasthma microscopy whole slide image (WSI) is an extremely labour-intensive task\ndue to the hundreds of thousands of cells per WSI. To overcome the limitation\nof annotating WSI incompletely, we developed a training pipeline which can\ntrain a deep learning-based object detection model with partially annotated\nWSIs and compensate class imbalances on the fly. With this approach we can\nfreely sample from annotated WSIs areas and are not restricted to fully\nannotated extracted sub-images of the WSI as with classical approaches. We\nevaluated our pipeline in a cross-validation setup with a fixed training set\nusing a dataset of six equine WSIs of which four are partially annotated and\nused for training, and two fully annotated WSI are used for validation and\ntesting. Our WSI-based training approach outperformed classical sub-image-based\ntraining methods by up to 15\\% $mAP$ and yielded human-like performance when\ncompared to the annotations of ten trained pathologists.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:11:38 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Marzahl", "Christian", ""], ["Bertram", "Christof A.", ""], ["Wilm", "Frauke", ""], ["Voigt", "J\u00f6rn", ""], ["Barton", "Ann K.", ""], ["Klopfleisch", "Robert", ""], ["Breininger", "Katharina", ""], ["Maier", "Andreas", ""], ["Aubreville", "Marc", ""]]}, {"id": "2101.04954", "submitter": "Dazhen Deng", "authors": "Dazhen Deng, Jiang Wu, Jiachen Wang, Yihong Wu, Xiao Xie, Zheng Zhou,\n  Hui Zhang, Xiaolong Zhang, Yingcai Wu", "title": "EventAnchor: Reducing Human Interactions in Event Annotation of Racket\n  Sports Videos", "comments": null, "journal-ref": "Proceedings of the 2021 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3411764.3445431", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The popularity of racket sports (e.g., tennis and table tennis) leads to high\ndemands for data analysis, such as notational analysis, on player performance.\nWhile sports videos offer many benefits for such analysis, retrieving accurate\ninformation from sports videos could be challenging. In this paper, we propose\nEventAnchor, a data analysis framework to facilitate interactive annotation of\nracket sports video with the support of computer vision algorithms. Our\napproach uses machine learning models in computer vision to help users acquire\nessential events from videos (e.g., serve, the ball bouncing on the court) and\noffers users a set of interactive tools for data annotation. An evaluation\nstudy on a table tennis annotation system built on this framework shows\nsignificant improvement of user performances in simple annotation tasks on\nobjects of interest and complex annotation tasks requiring domain knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:32:05 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 03:10:54 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Deng", "Dazhen", ""], ["Wu", "Jiang", ""], ["Wang", "Jiachen", ""], ["Wu", "Yihong", ""], ["Xie", "Xiao", ""], ["Zhou", "Zheng", ""], ["Zhang", "Hui", ""], ["Zhang", "Xiaolong", ""], ["Wu", "Yingcai", ""]]}, {"id": "2101.04976", "submitter": "Jean Aymar Biyiha Nlend", "authors": "Jean Aymar Biyiha Nlend, Ibrahim Moukouop Nguena and Thomas Bouetou\n  Bouetou", "title": "Large scale deduplication based on fingerprints", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fingerprint-based systems, the size of databases increases considerably\nwith population growth. In developing countries, because of the difficulty in\nusing a central system when enlisting voters, it often happens that several\nregional voter databases are created and then merged to form a central\ndatabase. A process is used to remove duplicates and ensure uniqueness by\nvoter. Until now, companies specializing in biometrics use several costly\ncomputing servers with algorithms to perform large-scale deduplication based on\nfingerprints. These algorithms take a considerable time because of their\ncomplexity in O (n2), where n is the size of the database. This article\npresents an algorithm that can perform this operation in O (2n), with just a\ncomputer. It is based on the development of an index obtained using a 5 * 5\nmatrix performed on each fingerprint. This index makes it possible to build\nclusters of O (1) in size in order to compare fingerprints. This approach has\nbeen evaluated using close to 11 4000 fingerprints, and the results obtained\nshow that this approach allows a penetration rate of less than 1%, an almost O\n(1) identification, and an O (n) deduplication. A base of 10 000 000\nfingerprints can be deduplicated with a just computer in less than two hours,\ncontrary to several days and servers for the usual tools.\n  Keywords: fingerprint, cluster, index, deduplication.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 10:10:58 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Nlend", "Jean Aymar Biyiha", ""], ["Nguena", "Ibrahim Moukouop", ""], ["Bouetou", "Thomas Bouetou", ""]]}, {"id": "2101.04989", "submitter": "Nati Daniel", "authors": "Tomer Czyzewski, Nati Daniel, Mark Rochman, Julie M. Caldwell, Garrett\n  A. Osswald, Margaret H. Collins, Marc E. Rothenberg, and Yonatan Savir", "title": "Machine learning approach for biopsy-based identification of\n  eosinophilic esophagitis reveals importance of global features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal: Eosinophilic esophagitis (EoE) is an allergic inflammatory condition\ncharacterized by eosinophil accumulation in the esophageal mucosa. EoE\ndiagnosis includes a manual assessment of eosinophil levels in mucosal biopsies\n- a time-consuming, laborious task that is difficult to standardize. One of the\nmain challenges in automating this process, like many other biopsy-based\ndiagnostics, is detecting features that are small relative to the size of the\nbiopsy. Results: In this work, we utilized hematoxylin- and eosin-stained\nslides from esophageal biopsies from patients with active EoE and control\nsubjects to develop a platform based on a deep convolutional neural network\n(DCNN) that can classify esophageal biopsies with an accuracy of 85%,\nsensitivity of 82.5%, and specificity of 87%. Moreover, by combining several\ndownscaling and cropping strategies, we show that some of the features\ncontributing to the correct classification are global rather than specific,\nlocal features. Conclusions: We report the ability of artificial intelligence\nto identify EoE using computer vision analysis of esophageal biopsy slides.\nFurther, the DCNN features associated with EoE are based on not only local\neosinophils but also global histologic changes. Our approach can be used for\nother conditions that rely on biopsy-based histologic diagnostics.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 10:38:46 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Czyzewski", "Tomer", ""], ["Daniel", "Nati", ""], ["Rochman", "Mark", ""], ["Caldwell", "Julie M.", ""], ["Osswald", "Garrett A.", ""], ["Collins", "Margaret H.", ""], ["Rothenberg", "Marc E.", ""], ["Savir", "Yonatan", ""]]}, {"id": "2101.05018", "submitter": "Xinggang Wang", "authors": "Mengting Chen and Xinggang Wang and Heng Luo and Yifeng Geng and Wenyu\n  Liu", "title": "Learning to Focus: Cascaded Feature Matching Network for Few-shot Image\n  Recognition", "comments": "14 pages", "journal-ref": "SCIENCE CHINA Information Sciences, 2021", "doi": "10.1007/s11432-020-2973-7", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep networks can learn to accurately recognize objects of a category by\ntraining on a large number of annotated images. However, a meta-learning\nchallenge known as a low-shot image recognition task comes when only a few\nimages with annotations are available for learning a recognition model for one\ncategory. The objects in testing/query and training/support images are likely\nto be different in size, location, style, and so on. Our method, called\nCascaded Feature Matching Network (CFMN), is proposed to solve this problem. We\ntrain the meta-learner to learn a more fine-grained and adaptive deep distance\nmetric by focusing more on the features that have high correlations between\ncompared images by the feature matching block which can align associated\nfeatures together and naturally ignore those non-discriminative features. By\napplying the proposed feature matching block in different layers of the\nfew-shot recognition network, multi-scale information among the compared images\ncan be incorporated into the final cascaded matching feature, which boosts the\nrecognition performance further and generalizes better by learning on\nrelationships. The experiments for few-shot learning on two standard datasets,\n\\emph{mini}ImageNet and Omniglot, have confirmed the effectiveness of our\nmethod. Besides, the multi-label few-shot task is first studied on a new data\nsplit of COCO which further shows the superiority of the proposed feature\nmatching network when performing few-shot learning in complex images. The code\nwill be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 11:37:28 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Chen", "Mengting", ""], ["Wang", "Xinggang", ""], ["Luo", "Heng", ""], ["Geng", "Yifeng", ""], ["Liu", "Wenyu", ""]]}, {"id": "2101.05021", "submitter": "Jorge F. Lazo", "authors": "Jorge F. Lazo, Aldo Marzullo, Sara Moccia, Michele Catellani, Benoit\n  Rosa, Michel de Mathelin, Elena De Momi", "title": "A Lumen Segmentation Method in Ureteroscopy Images based on a Deep\n  Residual U-Net architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ureteroscopy is becoming the first surgical treatment option for the majority\nof urinary affections. This procedure is performed using an endoscope which\nprovides the surgeon with the visual information necessary to navigate inside\nthe urinary tract. Having in mind the development of surgical assistance\nsystems, that could enhance the performance of surgeon, the task of lumen\nsegmentation is a fundamental part since this is the visual reference which\nmarks the path that the endoscope should follow. This is something that has not\nbeen analyzed in ureteroscopy data before. However, this task presents several\nchallenges given the image quality and the conditions itself of ureteroscopy\nprocedures. In this paper, we study the implementation of a Deep Neural Network\nwhich exploits the advantage of residual units in an architecture based on\nU-Net. For the training of these networks, we analyze the use of two different\ncolor spaces: gray-scale and RGB data images. We found that training on\ngray-scale images gives the best results obtaining mean values of Dice Score,\nPrecision, and Recall of 0.73, 0.58, and 0.92 respectively. The results\nobtained shows that the use of residual U-Net could be a suitable model for\nfurther development for a computer-aided system for navigation and guidance\nthrough the urinary system.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 11:41:39 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Lazo", "Jorge F.", ""], ["Marzullo", "Aldo", ""], ["Moccia", "Sara", ""], ["Catellani", "Michele", ""], ["Rosa", "Benoit", ""], ["de Mathelin", "Michel", ""], ["De Momi", "Elena", ""]]}, {"id": "2101.05022", "submitter": "Sangdoo Yun", "authors": "Sangdoo Yun, Seong Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe,\n  Sanghyuk Chun", "title": "Re-labeling ImageNet: from Single to Multi-Labels, from Global to\n  Localized Labels", "comments": "CVPR 2021 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  ImageNet has been arguably the most popular image classification benchmark,\nbut it is also the one with a significant level of label noise. Recent studies\nhave shown that many samples contain multiple classes, despite being assumed to\nbe a single-label benchmark. They have thus proposed to turn ImageNet\nevaluation into a multi-label task, with exhaustive multi-label annotations per\nimage. However, they have not fixed the training set, presumably because of a\nformidable annotation cost. We argue that the mismatch between single-label\nannotations and effectively multi-label images is equally, if not more,\nproblematic in the training setup, where random crops are applied. With the\nsingle-label annotations, a random crop of an image may contain an entirely\ndifferent object from the ground truth, introducing noisy or even incorrect\nsupervision during training. We thus re-label the ImageNet training set with\nmulti-labels. We address the annotation cost barrier by letting a strong image\nclassifier, trained on an extra source of data, generate the multi-labels. We\nutilize the pixel-wise multi-label predictions before the final pooling layer,\nin order to exploit the additional location-specific supervision signals.\nTraining on the re-labeled samples results in improved model performances\nacross the board. ResNet-50 attains the top-1 classification accuracy of 78.9%\non ImageNet with our localized multi-labels, which can be further boosted to\n80.2% with the CutMix regularization. We show that the models trained with\nlocalized multi-labels also outperforms the baselines on transfer learning to\nobject detection and instance segmentation tasks, and various robustness\nbenchmarks. The re-labeled ImageNet training set, pre-trained weights, and the\nsource code are available at {https://github.com/naver-ai/relabel_imagenet}.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 11:55:58 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 07:04:55 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Yun", "Sangdoo", ""], ["Oh", "Seong Joon", ""], ["Heo", "Byeongho", ""], ["Han", "Dongyoon", ""], ["Choe", "Junsuk", ""], ["Chun", "Sanghyuk", ""]]}, {"id": "2101.05036", "submitter": "Ali Harakeh", "authors": "Ali Harakeh and Steven L. Waslander", "title": "Estimating and Evaluating Regression Predictive Uncertainty in Deep\n  Object Detectors", "comments": "Published as a conference paper at ICLR 2021. Link:\n  https://openreview.net/forum?id=YLewtnvKgR7. This is the final camera-ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predictive uncertainty estimation is an essential next step for the reliable\ndeployment of deep object detectors in safety-critical tasks. In this work, we\nfocus on estimating predictive distributions for bounding box regression output\nwith variance networks. We show that in the context of object detection,\ntraining variance networks with negative log likelihood (NLL) can lead to high\nentropy predictive distributions regardless of the correctness of the output\nmean. We propose to use the energy score as a non-local proper scoring rule and\nfind that when used for training, the energy score leads to better calibrated\nand lower entropy predictive distributions than NLL. We also address the\nwidespread use of non-proper scoring metrics for evaluating predictive\ndistributions from deep object detectors by proposing an alternate evaluation\napproach founded on proper scoring rules. Using the proposed evaluation tools,\nwe show that although variance networks can be used to produce high quality\npredictive distributions, ad-hoc approaches used by seminal object detectors\nfor choosing regression targets during training do not provide wide enough data\nsupport for reliable variance learning. We hope that our work helps shift\nevaluation in probabilistic object detection to better align with predictive\nuncertainty evaluation in other machine learning domains. Code for all models,\nevaluation, and datasets is available at:\nhttps://github.com/asharakeh/probdet.git.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 12:53:54 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 13:33:53 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 18:16:36 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Harakeh", "Ali", ""], ["Waslander", "Steven L.", ""]]}, {"id": "2101.05043", "submitter": "David Fernandez-Llorca", "authors": "Mahdi Biparva, David Fern\\'andez-Llorca, Rub\\'en Izquierdo-Gonzalo,\n  John K. Tsotsos", "title": "Video action recognition for lane-change classification and prediction\n  of surrounding vehicles", "comments": "This work has been submitted to the IEEE Transactions on Intelligent\n  Transportation Systems. arXiv admin note: substantial text overlap with\n  arXiv:2008.10869", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In highway scenarios, an alert human driver will typically anticipate early\ncut-in/cut-out maneuvers of surrounding vehicles using visual cues mainly.\nAutonomous vehicles must anticipate these situations at an early stage too, to\nincrease their safety and efficiency. In this work, lane-change recognition and\nprediction tasks are posed as video action recognition problems. Up to four\ndifferent two-stream-based approaches, that have been successfully applied to\naddress human action recognition, are adapted here by stacking visual cues from\nforward-looking video cameras to recognize and anticipate lane-changes of\ntarget vehicles. We study the influence of context and observation horizons on\nperformance, and different prediction horizons are analyzed. The different\nmodels are trained and evaluated using the PREVENTION dataset. The obtained\nresults clearly demonstrate the potential of these methodologies to serve as\nrobust predictors of future lane-changes of surrounding vehicles proving an\naccuracy higher than 90% in time horizons of between 1-2 seconds.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 13:25:00 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Biparva", "Mahdi", ""], ["Fern\u00e1ndez-Llorca", "David", ""], ["Izquierdo-Gonzalo", "Rub\u00e9n", ""], ["Tsotsos", "John K.", ""]]}, {"id": "2101.05061", "submitter": "Iori Yanokura", "authors": "Iori Yanokura, Naoki Wake, Kazuhiro Sasabuchi, Katsushi Ikeuchi,\n  Masayuki Inaba", "title": "Understanding Action Sequences based on Video Captioning for\n  Learning-from-Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning actions from human demonstration video is promising for intelligent\nrobotic systems. Extracting the exact section and re-observing the extracted\nvideo section in detail is important for imitating complex skills because human\nmotions give valuable hints for robots. However, the general video\nunderstanding methods focus more on the understanding of the full frame,lacking\nconsideration on extracting accurate sections and aligning them with the\nhuman's intent. We propose a Learning-from-Observation framework that splits\nand understands a video of a human demonstration with verbal instructions to\nextract accurate action sequences. The splitting is done based on local minimum\npoints of the hand velocity, which align human daily-life actions with\nobject-centered face contact transitions required for generating robot motion.\nThen, we extract a motion description on the split videos using video\ncaptioning techniques that are trained from our new daily-life action video\ndataset. Finally, we match the motion descriptions with the verbal instructions\nto understand the correct human intent and ignore the unintended actions inside\nthe video. We evaluate the validity of hand velocity-based video splitting and\ndemonstrate that it is effective. The experimental results on our new video\ncaptioning dataset focusing on daily-life human actions demonstrate the\neffectiveness of the proposed method. The source code, trained models, and the\ndataset will be made available.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 05:22:01 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Yanokura", "Iori", ""], ["Wake", "Naoki", ""], ["Sasabuchi", "Kazuhiro", ""], ["Ikeuchi", "Katsushi", ""], ["Inaba", "Masayuki", ""]]}, {"id": "2101.05062", "submitter": "Mariam El Oussini", "authors": "Mariam El Oussini", "title": "Detection and extraction of biological particles in a three-dimensional\n  imaging of biological structures by TEM (Transmission Electron Microscopy)", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cells segmentation shows rapid growth in biology. Indeed, using the classical\nsegmentation methods only is not enough to segment this type of images. In this\nmanuscript, we will present a new method of ribosomes segmentation. A\npre-treatment phase will precedes the segmentation process and after that a\npost-processing will proceed.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 14:01:14 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Oussini", "Mariam El", ""]]}, {"id": "2101.05066", "submitter": "Zehai Yu", "authors": "Zehai Yu, Hui Zhu, Linglong Lin, Huawei Liang, Biao Yu, Weixin Huang", "title": "Laser Data Based Automatic Generation of Lane-Level Road Map for\n  Intelligent Vehicles", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of intelligent vehicle systems, a high-precision road\nmap is increasingly needed in many aspects. The automatic lane lines extraction\nand modeling are the most essential steps for the generation of a precise\nlane-level road map. In this paper, an automatic lane-level road map generation\nsystem is proposed. To extract the road markings on the ground, the\nmulti-region Otsu thresholding method is applied, which calculates the\nintensity value of laser data that maximizes the variance between background\nand road markings. The extracted road marking points are then projected to the\nraster image and clustered using a two-stage clustering algorithm. Lane lines\nare subsequently recognized from these clusters by the shape features of their\nminimum bounding rectangle. To ensure the storage efficiency of the map, the\nlane lines are approximated to cubic polynomial curves using a Bayesian\nestimation approach. The proposed lane-level road map generation system has\nbeen tested on urban and expressway conditions in Hefei, China. The\nexperimental results on the datasets show that our method can achieve excellent\nextraction and clustering effect, and the fitted lines can reach a high\nposition accuracy with an error of less than 10 cm\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 01:09:04 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Yu", "Zehai", ""], ["Zhu", "Hui", ""], ["Lin", "Linglong", ""], ["Liang", "Huawei", ""], ["Yu", "Biao", ""], ["Huang", "Weixin", ""]]}, {"id": "2101.05068", "submitter": "Sanghyuk Chun", "authors": "Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio de Rezende, Yannis\n  Kalantidis, Diane Larlus", "title": "Probabilistic Embeddings for Cross-Modal Retrieval", "comments": "Accepted to CVPR 2021; Code is available at\n  https://github.com/naver-ai/pcme", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval methods build a common representation space for samples\nfrom multiple modalities, typically from the vision and the language domains.\nFor images and their captions, the multiplicity of the correspondences makes\nthe task particularly challenging. Given an image (respectively a caption),\nthere are multiple captions (respectively images) that equally make sense. In\nthis paper, we argue that deterministic functions are not sufficiently powerful\nto capture such one-to-many correspondences. Instead, we propose to use\nProbabilistic Cross-Modal Embedding (PCME), where samples from the different\nmodalities are represented as probabilistic distributions in the common\nembedding space. Since common benchmarks such as COCO suffer from\nnon-exhaustive annotations for cross-modal matches, we propose to additionally\nevaluate retrieval on the CUB dataset, a smaller yet clean database where all\npossible image-caption pairs are annotated. We extensively ablate PCME and\ndemonstrate that it not only improves the retrieval performance over its\ndeterministic counterpart but also provides uncertainty estimates that render\nthe embeddings more interpretable. Code is available at\nhttps://github.com/naver-ai/pcme\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 13:58:00 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:39:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chun", "Sanghyuk", ""], ["Oh", "Seong Joon", ""], ["de Rezende", "Rafael Sampaio", ""], ["Kalantidis", "Yannis", ""], ["Larlus", "Diane", ""]]}, {"id": "2101.05069", "submitter": "Tomas Langer", "authors": "Tomas Langer, Natalia Fedorova, Ron Hagensieker", "title": "Formatting the Landscape: Spatial conditional GAN for varying population\n  in satellite imagery", "comments": "Presented as a poster at Tackling Climate Change with Machine\n  Learning workshop at NeurIPS 2020. Code:\n  https://github.com/LendelTheGreat/SCALAE/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Climate change is expected to reshuffle the settlement landscape: forcing\npeople in affected areas to migrate, to change their lifeways, and continuing\nto affect demographic change throughout the world. Changes to the geographic\ndistribution of population will have dramatic impacts on land use and land\ncover and thus constitute one of the major challenges of planning for climate\nchange scenarios. In this paper, we explore a generative model framework for\ngenerating satellite imagery conditional on gridded population distributions.\nWe make additions to the existing ALAE architecture, creating a spatially\nconditional version: SCALAE. This method allows us to explicitly disentangle\npopulation from the model's latent space and thus input custom population\nforecasts into the generated imagery. We postulate that such imagery could then\nbe directly used for land cover and land use change estimation using existing\nframeworks, as well as for realistic visualisation of expected local change. We\nevaluate the model by comparing pixel and semantic reconstructions, as well as\ncalculate the standard FID metric. The results suggest the model captures\npopulation distributions accurately and delivers a controllable method to\ngenerate realistic satellite imagery.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 13:31:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Langer", "Tomas", ""], ["Fedorova", "Natalia", ""], ["Hagensieker", "Ron", ""]]}, {"id": "2101.05077", "submitter": "Md Saiful Islam", "authors": "Manal, Ali Hasan Md. Linkon, Md. Mahir Labib, Marium-E-Jannat and Md\n  Saiful Islam", "title": "Restyling Images with the Bangladeshi Paintings Using Neural Style\n  Transfer: A Comprehensive Experiment, Evaluation, and Human Perspective", "comments": "6 pages", "journal-ref": "International Conference on Computer and Information Technology\n  (ICCIT), 19-21 December, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's world, Neural Style Transfer (NST) has become a trendsetting term.\nNST combines two pictures, a content picture and a reference image in style\n(such as the work of a renowned painter) in a way that makes the output image\nlook like an image of the material, but rendered with the form of a reference\npicture. However, there is no study using the artwork or painting of\nBangladeshi painters. Bangladeshi painting has a long history of more than two\nthousand years and is still being practiced by Bangladeshi painters. This study\ngenerates NST stylized image on Bangladeshi paintings and analyzes the human\npoint of view regarding the aesthetic preference of NST on Bangladeshi\npaintings. To assure our study's acceptance, we performed qualitative human\nevaluations on generated stylized images by 60 individual humans of different\nage and gender groups. We have explained how NST works for Bangladeshi\npaintings and assess NST algorithms, both qualitatively \\& quantitatively. Our\nstudy acts as a pre-requisite for the impact of NST stylized image using\nBangladeshi paintings on mobile UI/GUI and material translation from the human\nperspective. We hope that this study will encourage new collaborations to\ncreate more NST related studies and expand the use of Bangladeshi artworks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:22:51 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Manal", "", ""], ["Linkon", "Ali Hasan Md.", ""], ["Labib", "Md. Mahir", ""], ["Marium-E-Jannat", "", ""], ["Islam", "Md Saiful", ""]]}, {"id": "2101.05081", "submitter": "Md Saiful Islam", "authors": "Ali Hasan Md. Linkon, Md. Mahir Labib, Faisal Haque Bappy, Soumik\n  Sarker, Marium-E-Jannat and Md Saiful Islam", "title": "Deep Learning Approach Combining Lightweight CNN Architecture with\n  Transfer Learning: An Automatic Approach for the Detection and Recognition of\n  Bangladeshi Banknotes", "comments": "4 pages", "journal-ref": "2020 11th International Conference on Electrical and Computer\n  Engineering (ICECE)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic detection and recognition of banknotes can be a very useful\ntechnology for people with visual difficulties and also for the banks itself by\nproviding efficient management for handling different paper currencies.\nLightweight models can easily be integrated into any handy IoT based\ngadgets/devices. This article presents our experiments on several\nstate-of-the-art deep learning methods based on Lightweight Convolutional\nNeural Network architectures combining with transfer learning. ResNet152v2,\nMobileNet, and NASNetMobile were used as the base models with two different\ndatasets containing Bangladeshi banknote images. The Bangla Currency dataset\nhas 8000 Bangladeshi banknote images where the Bangla Money dataset consists of\n1970 images. The performances of the models were measured using both the\ndatasets and the combination of the two datasets. In order to achieve maximum\nefficiency, we used various augmentations, hyperparameter tuning, and\noptimizations techniques. We have achieved maximum test accuracy of 98.88\\% on\n8000 images dataset using MobileNet, 100\\% on the 1970 images dataset using\nNASNetMobile, and 97.77\\% on the combined dataset (9970 images) using\nMobileNet.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:36:41 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Linkon", "Ali Hasan Md.", ""], ["Labib", "Md. Mahir", ""], ["Bappy", "Faisal Haque", ""], ["Sarker", "Soumik", ""], ["Marium-E-Jannat", "", ""], ["Islam", "Md Saiful", ""]]}, {"id": "2101.05084", "submitter": "Adam Czajka", "authors": "Patrick Tinsley, Adam Czajka, Patrick Flynn", "title": "This Face Does Not Exist ... But It Might Be Yours! Identity Leakage in\n  Generative Models", "comments": "To appear at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative adversarial networks (GANs) are able to generate high resolution\nphoto-realistic images of objects that \"do not exist.\" These synthetic images\nare rather difficult to detect as fake. However, the manner in which these\ngenerative models are trained hints at a potential for information leakage from\nthe supplied training data, especially in the context of synthetic faces. This\npaper presents experiments suggesting that identity information in face images\ncan flow from the training corpus into synthetic samples without any\nadversarial actions when building or using the existing model. This raises\nprivacy-related questions, but also stimulates discussions of (a) the face\nmanifold's characteristics in the feature space and (b) how to create\ngenerative models that do not inadvertently reveal identity information of real\nsubjects whose images were used for training. We used five different face\nmatchers (face_recognition, FaceNet, ArcFace, SphereFace and Neurotechnology\nMegaMatcher) and the StyleGAN2 synthesis model, and show that this identity\nleakage does exist for some, but not all methods. So, can we say that these\nsynthetically generated faces truly do not exist? Databases of real and\nsynthetically generated faces are made available with this paper to allow full\nreplicability of the results discussed in this work.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:32:17 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Tinsley", "Patrick", ""], ["Czajka", "Adam", ""], ["Flynn", "Patrick", ""]]}, {"id": "2101.05091", "submitter": "Vasudevan Lakshminarayanan", "authors": "Darwin Castillo, Vasudevan Lakshminarayanan, Maria J.\n  Rodriguez-Alvarez", "title": "MRI Images, Brain Lesions and Deep Learning", "comments": "Submitted to: Computer Programs and Methods in Biomedicine update\n  (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical brain image analysis is a necessary step in Computer Assisted /Aided\nDiagnosis (CAD) systems. Advancements in both hardware and software in the past\nfew years have led to improved segmentation and classification of various\ndiseases. In the present work, we review the published literature on systems\nand algorithms that allow for classification, identification, and detection of\nWhite Matter Hyperintensities (WMHs) of brain MRI images specifically in cases\nof ischemic stroke and demyelinating diseases. For the selection criteria, we\nused the bibliometric networks. Out of a total of 140 documents we selected 38\narticles that deal with the main objectives of this study. Based on the\nanalysis and discussion of the revised documents, there is constant growth in\nthe research and proposal of new models of deep learning to achieve the highest\naccuracy and reliability of the segmentation of ischemic and demyelinating\nlesions. Models with indicators (Dice Score, DSC: 0.99) were found, however\nwith little practical application due to the uses of small datasets and lack of\nreproducibility. Therefore, the main conclusion is to establish\nmultidisciplinary research groups to overcome the gap between CAD developments\nand their complete utilization in the clinical environment.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 14:18:48 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 15:30:59 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Castillo", "Darwin", ""], ["Lakshminarayanan", "Vasudevan", ""], ["Rodriguez-Alvarez", "Maria J.", ""]]}, {"id": "2101.05107", "submitter": "Benjamin Congram", "authors": "Benjamin Congram and Timothy D. Barfoot", "title": "Relatively Lazy: Indoor-Outdoor Navigation Using Vision and GNSS", "comments": "Presented at CRV2021", "journal-ref": "In Proceedings of the 18th Conference on Robots and Vision (CRV),\n  pages 25-32. Burnaby, British Columbia, 26-28 May 2021", "doi": "10.1109/CRV52889.2021.00015", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Teach and Repeat has shown relative navigation is a robust and\nefficient solution for autonomous vision-based path following in difficult\nenvironments. Adding additional absolute sensors such as Global Navigation\nSatellite Systems (GNSS) has the potential to expand the domain of Visual Teach\nand Repeat to environments where the ability to visually localize is not\nguaranteed. Our method of lazy mapping and delaying estimation until a\npath-tracking error is needed avoids the need to estimate absolute states. As a\nresult, map optimization is not required and paths can be driven immediately\nafter being taught. We validate our approach on a real robot through an\nexperiment in a joint indoor-outdoor environment comprising 3.5km of autonomous\nroute repeating across a variety of lighting conditions. We achieve smooth\nerror signals throughout the runs despite large sections of dropout for each\nsensor.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 14:43:45 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 19:47:18 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Congram", "Benjamin", ""], ["Barfoot", "Timothy D.", ""]]}, {"id": "2101.05108", "submitter": "Thea Aarrestad", "authors": "Thea Aarrestad, Vladimir Loncar, Nicol\\`o Ghielmetti, Maurizio\n  Pierini, Sioni Summers, Jennifer Ngadiuba, Christoffer Petersson, Hampus\n  Linander, Yutaro Iiyama, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris,\n  Dylan Rankin, Sergo Jindariani, Kevin Pedro, Nhan Tran, Mia Liu, Edward\n  Kreinar, Zhenbin Wu, and Duc Hoang", "title": "Fast convolutional neural networks on FPGAs with hls4ml", "comments": "18 pages, 18 figures, 4 tables", "journal-ref": "Mach. Learn.: Sci. Technol. 2 045015 (2021)", "doi": "10.1088/2632-2153/ac0ea1", "report-no": null, "categories": "cs.LG cs.CV hep-ex physics.ins-det stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an automated tool for deploying ultra low-latency, low-power\ndeep neural networks with convolutional layers on FPGAs. By extending the\nhls4ml library, we demonstrate an inference latency of $5\\,\\mu$s using\nconvolutional architectures, targeting microsecond latency applications like\nthose at the CERN Large Hadron Collider. Considering benchmark models trained\non the Street View House Numbers Dataset, we demonstrate various methods for\nmodel compression in order to fit the computational constraints of a typical\nFPGA device used in trigger and data acquisition systems of particle detectors.\nIn particular, we discuss pruning and quantization-aware training, and\ndemonstrate how resource utilization can be significantly reduced with little\nto no loss in model accuracy. We show that the FPGA critical resource\nconsumption can be reduced by 97% with zero loss in model accuracy, and by 99%\nwhen tolerating a 6% accuracy degradation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 14:47:11 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 11:30:02 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Aarrestad", "Thea", ""], ["Loncar", "Vladimir", ""], ["Ghielmetti", "Nicol\u00f2", ""], ["Pierini", "Maurizio", ""], ["Summers", "Sioni", ""], ["Ngadiuba", "Jennifer", ""], ["Petersson", "Christoffer", ""], ["Linander", "Hampus", ""], ["Iiyama", "Yutaro", ""], ["Di Guglielmo", "Giuseppe", ""], ["Duarte", "Javier", ""], ["Harris", "Philip", ""], ["Rankin", "Dylan", ""], ["Jindariani", "Sergo", ""], ["Pedro", "Kevin", ""], ["Tran", "Nhan", ""], ["Liu", "Mia", ""], ["Kreinar", "Edward", ""], ["Wu", "Zhenbin", ""], ["Hoang", "Duc", ""]]}, {"id": "2101.05130", "submitter": "Jasjeet Dhaliwal", "authors": "Jasjeet Dhaliwal, Kyle Hambrook", "title": "DAEs for Linear Inverse Problems: Improved Recovery with Provable\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative priors have been shown to provide improved results over sparsity\npriors in linear inverse problems. However, current state of the art methods\nsuffer from one or more of the following drawbacks: (a) speed of recovery is\nslow; (b) reconstruction quality is deficient; (c) reconstruction quality is\ncontingent on a computationally expensive process of tuning hyperparameters. In\nthis work, we address these issues by utilizing Denoising Auto Encoders (DAEs)\nas priors and a projected gradient descent algorithm for recovering the\noriginal signal. We provide rigorous theoretical guarantees for our method and\nexperimentally demonstrate its superiority over existing state of the art\nmethods in compressive sensing, inpainting, and super-resolution. We find that\nour algorithm speeds up recovery by two orders of magnitude (over 100x),\nimproves quality of reconstruction by an order of magnitude (over 10x), and\ndoes not require tuning hyperparameters.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:24:37 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Dhaliwal", "Jasjeet", ""], ["Hambrook", "Kyle", ""]]}, {"id": "2101.05131", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Fangxu Xing, Chao Yang, C.-C. Jay Kuo, Suma Babu,\n  Georges El Fakhri, Thomas Jenkins, Jonghye Woo", "title": "VoxelHop: Successive Subspace Learning for ALS Disease Classification\n  Using Structural MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has great potential for accurate detection and classification\nof diseases with medical imaging data, but the performance is often limited by\nthe number of training datasets and memory requirements. In addition, many deep\nlearning models are considered a \"black-box,\" thereby often limiting their\nadoption in clinical applications. To address this, we present a successive\nsubspace learning model, termed VoxelHop, for accurate classification of\nAmyotrophic Lateral Sclerosis (ALS) using T2-weighted structural MRI data.\nCompared with popular convolutional neural network (CNN) architectures,\nVoxelHop has modular and transparent structures with fewer parameters without\nany backpropagation, so it is well-suited to small dataset size and 3D imaging\ndata. Our VoxelHop has four key components, including (1) sequential expansion\nof near-to-far neighborhood for multi-channel 3D data; (2) subspace\napproximation for unsupervised dimension reduction; (3) label-assisted\nregression for supervised dimension reduction; and (4) concatenation of\nfeatures and classification between controls and patients. Our experimental\nresults demonstrate that our framework using a total of 20 controls and 26\npatients achieves an accuracy of 93.48$\\%$ and an AUC score of 0.9394 in\ndifferentiating patients from controls, even with a relatively small number of\ndatasets, showing its robustness and effectiveness. Our thorough evaluations\nalso show its validity and superiority to the state-of-the-art 3D CNN\nclassification methods. Our framework can easily be generalized to other\nclassification tasks using different imaging modalities.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:25:57 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Xing", "Fangxu", ""], ["Yang", "Chao", ""], ["Kuo", "C. -C. Jay", ""], ["Babu", "Suma", ""], ["Fakhri", "Georges El", ""], ["Jenkins", "Thomas", ""], ["Woo", "Jonghye", ""]]}, {"id": "2101.05145", "submitter": "Rohit Jena", "authors": "Rohit Jena, Sumedha Singla, Kayhan Batmanghelich", "title": "Self-Supervised Vessel Enhancement Using Flow-Based Consistencies", "comments": "Early accept at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vessel segmentation is an essential task in many clinical applications.\nAlthough supervised methods have achieved state-of-art performance, acquiring\nexpert annotation is laborious and mostly limited for two-dimensional datasets\nwith a small sample size. On the contrary, unsupervised methods rely on\nhandcrafted features to detect tube-like structures such as vessels. However,\nthose methods require complex pipelines involving several hyper-parameters and\ndesign choices rendering the procedure sensitive, dataset-specific, and not\ngeneralizable. We propose a self-supervised method with a limited number of\nhyper-parameters that is generalizable across modalities. Our method uses\ntube-like structure properties, such as connectivity, profile consistency, and\nbifurcation, to introduce inductive bias into a learning algorithm. To model\nthose properties, we generate a vector field that we refer to as a flow. Our\nexperiments on various public datasets in 2D and 3D show that our method\nperforms better than unsupervised methods while learning useful transferable\nfeatures from unlabeled data. Unlike generic self-supervised methods, the\nlearned features learn vessel-relevant features that are transferable for\nsupervised approaches, which is essential when the number of annotated data is\nlimited.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:38:23 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 16:24:25 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 14:52:23 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jena", "Rohit", ""], ["Singla", "Sumedha", ""], ["Batmanghelich", "Kayhan", ""]]}, {"id": "2101.05181", "submitter": "Lina Mezghani", "authors": "Lina Mezghani, Sainbayar Sukhbaatar, Thibaut Lavril, Oleksandr\n  Maksymets, Dhruv Batra, Piotr Bojanowski, Karteek Alahari", "title": "Memory-Augmented Reinforcement Learning for Image-Goal Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a memory-augmented approach for image-goal\nnavigation. Our key hypothesis is that, while episodic reinforcement learning\nmay be a convenient framework for tackling this task, embodied agents, once\ndeployed, do not simply cease to exist after an episode has ended. They persist\nand so should their memories. Our approach leverages a cross-episode memory to\nlearn to navigate. First, we train a state-embedding network in a\nself-supervised fashion, and then use it to embed previously-visited states\ninto the agent's memory. Our navigation policy takes advantage of the\ninformation stored in the memory via an attention mechanism. We validate our\napproach through extensive evaluations, and show that our model establishes a\nnew state of the art on the challenging Gibson dataset. We obtain this\ncompetitive performance from RGB input alone, without access to additional\ninformation such as position or depth.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 16:30:20 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 13:02:39 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Mezghani", "Lina", ""], ["Sukhbaatar", "Sainbayar", ""], ["Lavril", "Thibaut", ""], ["Maksymets", "Oleksandr", ""], ["Batra", "Dhruv", ""], ["Bojanowski", "Piotr", ""], ["Alahari", "Karteek", ""]]}, {"id": "2101.05198", "submitter": "Beat Signer", "authors": "Maxim Van de Wynckel and Beat Signer", "title": "OpenHPS: An Open Source Hybrid Positioning System", "comments": null, "journal-ref": null, "doi": null, "report-no": "WISE-2021-01", "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positioning systems and frameworks use various techniques to determine the\nposition of an object. Some of the existing solutions combine different sensory\ndata at the time of positioning in order to compute more accurate positions by\nreducing the error introduced by the used individual positioning techniques. We\npresent OpenHPS, a generic hybrid positioning system implemented in TypeScript,\nthat can not only reduce the error during tracking by fusing different sensory\ndata based on different algorithms, but also also make use of combined tracking\ntechniques when calibrating or training the system. In addition to a detailed\ndiscussion of the architecture, features and implementation of the extensible\nopen source OpenHPS framework, we illustrate the use of our solution in a\ndemonstrator application fusing different positioning techniques. While OpenHPS\noffers a number of positioning techniques, future extensions might integrate\nnew positioning methods or algorithms and support additional levels of\nabstraction including symbolic locations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 22:03:54 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Van de Wynckel", "Maxim", ""], ["Signer", "Beat", ""]]}, {"id": "2101.05202", "submitter": "Mihails Birjukovs", "authors": "Peteris Zvejnieks, Mihails Birjukovs, Martins Klevs, Megumi Akashi,\n  Sven Eckert, Andris Jakovics", "title": "MHT-X: Offline Multiple Hypothesis Tracking with Algorithm X", "comments": "18 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient and versatile implementation of offline multiple hypothesis\ntracking with Algorithm X for optimal association search was developed using\nPython. The code is intended for scientific applications that do not require\nonline processing. Directed graph framework is used and multiple scans with\nprogressively increasing time window width are used for edge construction for\nmaximum likelihood trajectories. The current version of the code was developed\nfor applications in multiphase hydrodynamics, e.g. bubble and particle\ntracking, and is capable of resolving object motion, merges and splits.\nFeasible object associations and trajectory graph edge likelihoods are\ndetermined using weak mass and momentum conservation laws translated to\nstatistical functions for object properties. The code is compatible with\nn-dimensional motion with arbitrarily many tracked object properties. This\nframework is easily extendable beyond the present application by replacing the\ncurrently used heuristics with ones more appropriate for the problem at hand.\nThe code is open-source and will be continuously developed further.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 02:04:46 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 14:09:27 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zvejnieks", "Peteris", ""], ["Birjukovs", "Mihails", ""], ["Klevs", "Martins", ""], ["Akashi", "Megumi", ""], ["Eckert", "Sven", ""], ["Jakovics", "Andris", ""]]}, {"id": "2101.05204", "submitter": "Yen-Chen Lin", "authors": "Frank Dellaert, Lin Yen-Chen", "title": "Neural Volume Rendering: NeRF And Beyond", "comments": "Blog: https://dellaert.github.io/NeRF/ Bibtex:\n  https://github.com/yenchenlin/awesome-NeRF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides the COVID-19 pandemic and political upheaval in the US, 2020 was also\nthe year in which neural volume rendering exploded onto the scene, triggered by\nthe impressive NeRF paper by Mildenhall et al. (2020). Both of us have tried to\ncapture this excitement, Frank on a blog post (Dellaert, 2020) and Yen-Chen in\na Github collection (Yen-Chen, 2020). This note is an annotated bibliography of\nthe relevant papers, and we posted the associated bibtex file on the\nrepository.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:37:25 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 21:29:40 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Dellaert", "Frank", ""], ["Yen-Chen", "Lin", ""]]}, {"id": "2101.05205", "submitter": "Chang Min Hyun", "authors": "Hye Sun Yun, Chang Min Hyun, Seong Hyeon Baek, Sang-Hwy Lee, Jin Keun\n  Seo", "title": "Automated 3D cephalometric landmark identification using computerized\n  tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identification of 3D cephalometric landmarks that serve as proxy to the shape\nof human skull is the fundamental step in cephalometric analysis. Since manual\nlandmarking from 3D computed tomography (CT) images is a cumbersome task even\nfor the trained experts, automatic 3D landmark detection system is in a great\nneed. Recently, automatic landmarking of 2D cephalograms using deep learning\n(DL) has achieved great success, but 3D landmarking for more than 80 landmarks\nhas not yet reached a satisfactory level, because of the factors hindering\nmachine learning such as the high dimensionality of the input data and limited\namount of training data due to ethical restrictions on the use of medical data.\nThis paper presents a semi-supervised DL method for 3D landmarking that takes\nadvantage of anonymized landmark dataset with paired CT data being removed. The\nproposed method first detects a small number of easy-to-find reference\nlandmarks, then uses them to provide a rough estimation of the entire landmarks\nby utilizing the low dimensional representation learned by variational\nautoencoder (VAE). Anonymized landmark dataset is used for training the VAE.\nFinally, coarse-to-fine detection is applied to the small bounding box provided\nby rough estimation, using separate strategies suitable for mandible and\ncranium. For mandibular landmarks, patch-based 3D CNN is applied to the\nsegmented image of the mandible (separated from the maxilla), in order to\ncapture 3D morphological features of mandible associated with the landmarks. We\ndetect 6 landmarks around the condyle all at once, instead of one by one,\nbecause they are closely related to each other. For cranial landmarks, we again\nuse VAE-based latent representation for more accurate annotation. In our\nexperiment, the proposed method achieved an averaged 3D point-to-point error of\n2.91 mm for 90 landmarks only with 15 paired training data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 07:29:32 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Yun", "Hye Sun", ""], ["Hyun", "Chang Min", ""], ["Baek", "Seong Hyeon", ""], ["Lee", "Sang-Hwy", ""], ["Seo", "Jin Keun", ""]]}, {"id": "2101.05208", "submitter": "Dexin Wang", "authors": "Dexin Wang and Deyi Xiong", "title": "Efficient Object-Level Visual Context Modeling for Multimodal Machine\n  Translation: Masking Irrelevant Objects Helps Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual context provides grounding information for multimodal machine\ntranslation (MMT). However, previous MMT models and probing studies on visual\nfeatures suggest that visual information is less explored in MMT as it is often\nredundant to textual information. In this paper, we propose an object-level\nvisual context modeling framework (OVC) to efficiently capture and explore\nvisual information for multimodal machine translation. With detected objects,\nthe proposed OVC encourages MMT to ground translation on desirable visual\nobjects by masking irrelevant objects in the visual modality. We equip the\nproposed with an additional object-masking loss to achieve this goal. The\nobject-masking loss is estimated according to the similarity between masked\nobjects and the source texts so as to encourage masking source-irrelevant\nobjects. Additionally, in order to generate vision-consistent target words, we\nfurther propose a vision-weighted translation loss for OVC. Experiments on MMT\ndatasets demonstrate that the proposed OVC model outperforms state-of-the-art\nMMT models and analyses show that masking irrelevant objects helps grounding in\nMMT.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 11:10:00 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Wang", "Dexin", ""], ["Xiong", "Deyi", ""]]}, {"id": "2101.05209", "submitter": "Xinghong Qin", "authors": "Xinghong Qin, Shunquan Tan, Bin Li, Weixuan Tang and Jiwu Huang", "title": "Image Steganography based on Iteratively Adversarial Samples of A\n  Synchronized-directions Sub-image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays a steganography has to face challenges of both feature based\nstaganalysis and convolutional neural network (CNN) based steganalysis. In this\npaper, we present a novel steganography scheme denoted as ITE-SYN (based on\nITEratively adversarial perturbations onto a SYNchronized-directions\nsub-image), by which security data is embedded with synchronizing modification\ndirections to enhance security and then iteratively increased perturbations are\nadded onto a sub-image to reduce loss with cover class label of the target CNN\nclassifier. Firstly an exist steganographic function is employed to compute\ninitial costs. Then the cover image is decomposed into some non-overlapped\nsub-images. After each sub-image is embedded, costs will be adjusted following\nclustering modification directions profile. And then the next sub-image will be\nembedded with adjusted costs until all secret data has been embedded. If the\ntarget CNN classifier does not discriminate the stego image as a cover image,\nbased on adjusted costs, we change costs with adversarial manners according to\nsigns of gradients back-propagated from the CNN classifier. And then a\nsub-image is chosen to be re-embedded with changed costs. Adversarial intensity\nwill be iteratively increased until the adversarial stego image can fool the\ntarget CNN classifier. Experiments demonstrate that the proposed method\neffectively enhances security to counter both conventional feature-based\nclassifiers and CNN classifiers, even other non-target CNN classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 17:22:27 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Qin", "Xinghong", ""], ["Tan", "Shunquan", ""], ["Li", "Bin", ""], ["Tang", "Weixuan", ""], ["Huang", "Jiwu", ""]]}, {"id": "2101.05212", "submitter": "Wenbo Dong", "authors": "Wenbo Dong, Volkan Isler", "title": "Ellipse Regression with Predicted Uncertainties for Accurate Multi-View\n  3D Object Estimation", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) based architectures, such as Mask R-CNN,\nconstitute the state of the art in object detection and segmentation. Recently,\nthese methods have been extended for model-based segmentation where the network\noutputs the parameters of a geometric model (e.g. an ellipse) directly. This\nwork considers objects whose three-dimensional models can be represented as\nellipsoids. We present a variant of Mask R-CNN for estimating the parameters of\nellipsoidal objects by segmenting each object and accurately regressing the\nparameters of projection ellipses. We show that model regression is sensitive\nto the underlying occlusion scenario and that prediction quality for each\nobject needs to be characterized individually for accurate 3D object\nestimation. We present a novel ellipse regression loss which can learn the\noffset parameters with their uncertainties and quantify the overall geometric\nquality of detection for each ellipse. These values, in turn, allow us to fuse\nmulti-view detections to obtain 3D ellipsoid parameters in a principled\nfashion. The experiments on both synthetic and real datasets quantitatively\ndemonstrate the high accuracy of our proposed method in estimating 3D objects\nunder heavy occlusions compared to previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 19:52:58 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Dong", "Wenbo", ""], ["Isler", "Volkan", ""]]}, {"id": "2101.05214", "submitter": "FIrhan Maulana Rusli", "authors": "Firhan Maulana Rusli, Kevin Akbar Adhiguna, Hendy Irawan", "title": "Indonesian ID Card Extractor Using Optical Character Recognition and\n  Natural Language Post-Processing", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of Information Technology has been increasingly changing the\nmeans of information exchange leading to the need of digitizing print\ndocuments. In the present era, there is a lot of fraud that often occur. To\navoid account fraud there was verification using ID card extraction using OCR\nand NLP. Optical Character Recognition (OCR) is technology that used to\ngenerate text from image. With OCR we can extract Indonesian ID card or kartu\ntanda penduduk (KTP) into text too. This is using to make easier service\noperator to do data entry. To improve the accuracy we made text correction\nusing Natural language Processing (NLP) method to fixing the text. With 50\nIndonesian ID card image we got 0.78 F-score, and we need 4510 milliseconds to\nextract per ID card.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 11:28:48 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Rusli", "Firhan Maulana", ""], ["Adhiguna", "Kevin Akbar", ""], ["Irawan", "Hendy", ""]]}, {"id": "2101.05216", "submitter": "Souvik Kundu", "authors": "Souvik Kundu, Sairam Sundaresan", "title": "AttentionLite: Towards Efficient Self-Attention Models for Vision", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for producing a class of parameter and compute\nefficient models called AttentionLitesuitable for resource-constrained\napplications. Prior work has primarily focused on optimizing models either via\nknowledge distillation or pruning. In addition to fusing these two mechanisms,\nour joint optimization framework also leverages recent advances in\nself-attention as a substitute for convolutions. We can simultaneously distill\nknowledge from a compute-heavy teacher while also pruning the student model in\na single pass of training thereby reducing training and fine-tuning times\nconsiderably. We evaluate the merits of our proposed approach on the CIFAR-10,\nCIFAR-100, and Tiny-ImageNet datasets. Not only do our AttentionLite models\nsignificantly outperform their unoptimized counterparts in accuracy, we find\nthat in some cases, that they perform almost as well as their compute-heavy\nteachers while consuming only a fraction of the parameters and FLOPs.\nConcretely, AttentionLite models can achieve upto30x parameter efficiency and\n2x computation efficiency with no significant accuracy drop compared to their\nteacher.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 17:54:09 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Kundu", "Souvik", ""], ["Sundaresan", "Sairam", ""]]}, {"id": "2101.05218", "submitter": "Mahmut Yurt", "authors": "Muzaffer \\\"Ozbey, Mahmut Yurt, Salman Ul Hassan Dar, Tolga \\c{C}ukur", "title": "Three Dimensional MR Image Synthesis with Progressive Generative\n  Adversarial Networks", "comments": "Presented on April 4, 2020 in the IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mainstream deep models for three-dimensional MRI synthesis are either\ncross-sectional or volumetric depending on the input. Cross-sectional models\ncan decrease the model complexity, but they may lead to discontinuity\nartifacts. On the other hand, volumetric models can alleviate the discontinuity\nartifacts, but they might suffer from loss of spatial resolution due to\nincreased model complexity coupled with scarce training data. To mitigate the\nlimitations of both approaches, we propose a novel model that progressively\nrecovers the target volume via simpler synthesis tasks across individual\norientations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 16:38:40 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["\u00d6zbey", "Muzaffer", ""], ["Yurt", "Mahmut", ""], ["Dar", "Salman Ul Hassan", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "2101.05219", "submitter": "Jonathan Helland", "authors": "Jonathan Helland, Nathan VanHoudnos", "title": "On the human-recognizability phenomenon of adversarially trained deep\n  image classifiers", "comments": null, "journal-ref": "In JSM Proceedings, Statistical Computing Section. Alexandria, VA:\n  American Statistical Association. 1121-1131 (2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the phenomenon that robust image classifiers\nhave human-recognizable features -- often referred to as interpretability -- as\nrevealed through the input gradients of their score functions and their\nsubsequent adversarial perturbations. In particular, we demonstrate that\nstate-of-the-art methods for adversarial training incorporate two terms -- one\nthat orients the decision boundary via minimizing the expected loss, and\nanother that induces smoothness of the classifier's decision surface by\npenalizing the local Lipschitz constant. Through this demonstration, we provide\na unified discussion of gradient and Jacobian-based regularizers that have been\nused to encourage adversarial robustness in prior works. Following this\ndiscussion, we give qualitative evidence that the coupling of smoothness and\norientation of the decision boundary is sufficient to induce the aforementioned\nhuman-recognizability phenomenon.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 17:33:52 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Helland", "Jonathan", ""], ["VanHoudnos", "Nathan", ""]]}, {"id": "2101.05224", "submitter": "Shekoofeh Azizi", "authors": "Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan\n  Freyberg, Jonathan Deaton, Aaron Loh, Alan Karthikesalingam, Simon Kornblith,\n  Ting Chen, Vivek Natarajan, Mohammad Norouzi", "title": "Big Self-Supervised Models Advance Medical Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised pretraining followed by supervised fine-tuning has seen\nsuccess in image recognition, especially when labeled examples are scarce, but\nhas received limited attention in medical image analysis. This paper studies\nthe effectiveness of self-supervised learning as a pretraining strategy for\nmedical image classification. We conduct experiments on two distinct tasks:\ndermatology skin condition classification from digital camera images and\nmulti-label chest X-ray classification, and demonstrate that self-supervised\nlearning on ImageNet, followed by additional self-supervised learning on\nunlabeled domain-specific medical images significantly improves the accuracy of\nmedical image classifiers. We introduce a novel Multi-Instance Contrastive\nLearning (MICLe) method that uses multiple images of the underlying pathology\nper patient case, when available, to construct more informative positive pairs\nfor self-supervised learning. Combining our contributions, we achieve an\nimprovement of 6.7% in top-1 accuracy and an improvement of 1.1% in mean AUC on\ndermatology and chest X-ray classification respectively, outperforming strong\nsupervised baselines pretrained on ImageNet. In addition, we show that big\nself-supervised models are robust to distribution shift and can learn\nefficiently with a small number of labeled medical images.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 17:36:31 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:43:59 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Azizi", "Shekoofeh", ""], ["Mustafa", "Basil", ""], ["Ryan", "Fiona", ""], ["Beaver", "Zachary", ""], ["Freyberg", "Jan", ""], ["Deaton", "Jonathan", ""], ["Loh", "Aaron", ""], ["Karthikesalingam", "Alan", ""], ["Kornblith", "Simon", ""], ["Chen", "Ting", ""], ["Natarajan", "Vivek", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "2101.05231", "submitter": "Keaton Hamm", "authors": "HanQin Cai, Keaton Hamm, Longxiu Huang, Deanna Needell", "title": "Robust CUR Decomposition: Theory and Imaging Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the use of Robust PCA in a CUR decomposition framework\nand applications thereof. Our main algorithms produce a robust version of\ncolumn-row factorizations of matrices $\\mathbf{D}=\\mathbf{L}+\\mathbf{S}$ where\n$\\mathbf{L}$ is low-rank and $\\mathbf{S}$ contains sparse outliers. These\nmethods yield interpretable factorizations at low computational cost, and\nprovide new CUR decompositions that are robust to sparse outliers, in contrast\nto previous methods. We consider two key imaging applications of Robust PCA:\nvideo foreground-background separation and face modeling. This paper examines\nthe qualitative behavior of our Robust CUR decompositions on the benchmark\nvideos and face datasets, and find that our method works as well as standard\nRobust PCA while being significantly faster. Additionally, we consider hybrid\nrandomized and deterministic sampling methods which produce a compact CUR\ndecomposition of a given matrix, and apply this to video sequences to produce\ncanonical frames thereof.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 17:58:15 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Cai", "HanQin", ""], ["Hamm", "Keaton", ""], ["Huang", "Longxiu", ""], ["Needell", "Deanna", ""]]}, {"id": "2101.05260", "submitter": "Nathanael Lemessa Baisa", "authors": "Nathanael L. Baisa, Zheheng Jiang, Ritesh Vyas, Bryan Williams,\n  Hossein Rahmani, Plamen Angelov, Sue Black", "title": "Hand-Based Person Identification using Global and Part-Aware Deep\n  Feature Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cases of serious crime, including sexual abuse, often the only available\ninformation with demonstrated potential for identification is images of the\nhands. Since this evidence is captured in uncontrolled situations, it is\ndifficult to analyse. As global approaches to feature comparison are limited in\nthis case, it is important to extend to consider local information. In this\nwork, we propose hand-based person identification by learning both global and\nlocal deep feature representation. Our proposed method, Global and Part-Aware\nNetwork (GPA-Net), creates global and local branches on the conv-layer for\nlearning robust discriminative global and part-level features. For learning the\nlocal (part-level) features, we perform uniform partitioning on the conv-layer\nin both horizontal and vertical directions. We retrieve the parts by conducting\na soft partition without explicitly partitioning the images or requiring\nexternal cues such as pose estimation. We make extensive evaluations on two\nlarge multi-ethnic and publicly available hand datasets, demonstrating that our\nproposed method significantly outperforms competing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:47:47 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 22:12:18 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 14:18:19 GMT"}, {"version": "v4", "created": "Mon, 31 May 2021 12:34:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Baisa", "Nathanael L.", ""], ["Jiang", "Zheheng", ""], ["Vyas", "Ritesh", ""], ["Williams", "Bryan", ""], ["Rahmani", "Hossein", ""], ["Angelov", "Plamen", ""], ["Black", "Sue", ""]]}, {"id": "2101.05278", "submitter": "Weihao Xia", "authors": "Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou,\n  Ming-Hsuan Yang", "title": "GAN Inversion: A Survey", "comments": "papers on generative modeling:\n  https://github.com/zhoubolei/awesome-generative-modeling awesome\n  gan-inversion papers: https://github.com/weihaox/awesome-gan-inversion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GAN inversion aims to invert a given image back into the latent space of a\npretrained GAN model, for the image to be faithfully reconstructed from the\ninverted code by the generator. As an emerging technique to bridge the real and\nfake image domains, GAN inversion plays an essential role in enabling the\npretrained GAN models such as StyleGAN and BigGAN to be used for real image\nediting applications. Meanwhile, GAN inversion also provides insights on the\ninterpretation of GAN's latent space and how the realistic images can be\ngenerated. In this paper, we provide an overview of GAN inversion with a focus\non its recent algorithms and applications. We cover important techniques of GAN\ninversion and their applications to image restoration and image manipulation.\nWe further elaborate on some trends and challenges for future directions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 14:11:00 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 07:05:20 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 07:59:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xia", "Weihao", ""], ["Zhang", "Yulun", ""], ["Yang", "Yujiu", ""], ["Xue", "Jing-Hao", ""], ["Zhou", "Bolei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2101.05307", "submitter": "Eloi Zablocki", "authors": "\\'Eloi Zablocki, H\\'edi Ben-Younes, Patrick P\\'erez, Matthieu Cord", "title": "Explainability of vision-based autonomous driving systems: Review and\n  challenges", "comments": "submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This survey reviews explainability methods for vision-based self-driving\nsystems. The concept of explainability has several facets and the need for\nexplainability is strong in driving, a safety-critical application. Gathering\ncontributions from several research fields, namely computer vision, deep\nlearning, autonomous driving, explainable AI (X-AI), this survey tackles\nseveral points. First, it discusses definitions, context, and motivation for\ngaining more interpretability and explainability from self-driving systems.\nSecond, major recent state-of-the-art approaches to develop self-driving\nsystems are quickly presented. Third, methods providing explanations to a\nblack-box self-driving system in a post-hoc fashion are comprehensively\norganized and detailed. Fourth, approaches from the literature that aim at\nbuilding more interpretable self-driving systems by design are presented and\ndiscussed in detail. Finally, remaining open-challenges and potential future\nresearch directions are identified and examined.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:09:38 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zablocki", "\u00c9loi", ""], ["Ben-Younes", "H\u00e9di", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "2101.05326", "submitter": "William Adorno", "authors": "William Adorno III, Alexis Catalano, Lubaina Ehsan, Hans Vitzhum von\n  Eckstaedt, Barrett Barnes, Emily McGowan, Sana Syed, Donald E. Brown", "title": "Advancing Eosinophilic Esophagitis Diagnosis and Phenotype Assessment\n  with Deep Learning Computer Vision", "comments": "This paper contains 12 pages, 9 figures, and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Eosinophilic Esophagitis (EoE) is an inflammatory esophageal disease which is\nincreasing in prevalence. The diagnostic gold-standard involves manual review\nof a patient's biopsy tissue sample by a clinical pathologist for the presence\nof 15 or greater eosinophils within a single high-power field (400x\nmagnification). Diagnosing EoE can be a cumbersome process with added\ndifficulty for assessing the severity and progression of disease. We propose an\nautomated approach for quantifying eosinophils using deep image segmentation. A\nU-Net model and post-processing system are applied to generate eosinophil-based\nstatistics that can diagnose EoE as well as describe disease severity and\nprogression. These statistics are captured in biopsies at the initial EoE\ndiagnosis and are then compared with patient metadata: clinical and treatment\nphenotypes. The goal is to find linkages that could potentially guide treatment\nplans for new patients at their initial disease diagnosis. A deep image\nclassification model is further applied to discover features other than\neosinophils that can be used to diagnose EoE. This is the first study to\nutilize a deep learning computer vision approach for EoE diagnosis and to\nprovide an automated process for tracking disease severity and progression.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 20:01:48 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Adorno", "William", "III"], ["Catalano", "Alexis", ""], ["Ehsan", "Lubaina", ""], ["von Eckstaedt", "Hans Vitzhum", ""], ["Barnes", "Barrett", ""], ["McGowan", "Emily", ""], ["Syed", "Sana", ""], ["Brown", "Donald E.", ""]]}, {"id": "2101.05356", "submitter": "Gaurav Bharaj", "authors": "Abdallah Dib, Gaurav Bharaj, Junghyun Ahn, C\\'edric Th\\'ebault,\n  Philippe-Henri Gosselin, Marco Romeo, Louis Chevallier", "title": "Practical Face Reconstruction via Differentiable Ray Tracing", "comments": "16 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a differentiable ray-tracing based novel face reconstruction\napproach where scene attributes - 3D geometry, reflectance (diffuse, specular\nand roughness), pose, camera parameters, and scene illumination - are estimated\nfrom unconstrained monocular images. The proposed method models scene\nillumination via a novel, parameterized virtual light stage, which\nin-conjunction with differentiable ray-tracing, introduces a coarse-to-fine\noptimization formulation for face reconstruction. Our method can not only\nhandle unconstrained illumination and self-shadows conditions, but also\nestimates diffuse and specular albedos. To estimate the face attributes\nconsistently and with practical semantics, a two-stage optimization strategy\nsystematically uses a subset of parametric attributes, where subsequent\nattribute estimations factor those previously estimated. For example,\nself-shadows estimated during the first stage, later prevent its baking into\nthe personalized diffuse and specular albedos in the second stage. We show the\nefficacy of our approach in several real-world scenarios, where face attributes\ncan be estimated even under extreme illumination conditions. Ablation studies,\nanalyses and comparisons against several recent state-of-the-art methods show\nimproved accuracy and versatility of our approach. With consistent face\nattributes reconstruction, our method leads to several style -- illumination,\nalbedo, self-shadow -- edit and transfer applications, as discussed in the\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:36:11 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Dib", "Abdallah", ""], ["Bharaj", "Gaurav", ""], ["Ahn", "Junghyun", ""], ["Th\u00e9bault", "C\u00e9dric", ""], ["Gosselin", "Philippe-Henri", ""], ["Romeo", "Marco", ""], ["Chevallier", "Louis", ""]]}, {"id": "2101.05361", "submitter": "Osama Mazhar", "authors": "Osama Mazhar and Jens Kober", "title": "Random Shadows and Highlights: A new data augmentation method for\n  extreme lighting conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new data augmentation method, Random Shadows and\nHighlights (RSH) to acquire robustness against lighting perturbations. Our\nmethod creates random shadows and highlights on images, thus challenging the\nneural network during the learning process such that it acquires immunity\nagainst such input corruptions in real world applications. It is a\nparameter-learning free method which can be integrated into most vision related\nlearning applications effortlessly. With extensive experimentation, we\ndemonstrate that RSH not only increases the robustness of the models against\nlighting perturbations, but also reduces over-fitting significantly. Thus RSH\nshould be considered essential for all vision related learning systems. Code is\navailable at: https://github.com/OsamaMazhar/Random-Shadows-Highlights.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:58:43 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 10:38:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mazhar", "Osama", ""], ["Kober", "Jens", ""]]}, {"id": "2101.05403", "submitter": "Jianzhong Wang", "authors": "Yanni Zhang, Yiming Liu, Qiang Li, Miao Qi, Dahong Xu, Jun Kong,\n  Jianzhong Wang", "title": "Image deblurring based on lightweight multi-information fusion network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, deep learning based image deblurring has been well developed.\nHowever, exploiting the detailed image features in a deep learning framework\nalways requires a mass of parameters, which inevitably makes the network suffer\nfrom high computational burden. To solve this problem, we propose a lightweight\nmultiinformation fusion network (LMFN) for image deblurring. The proposed LMFN\nis designed as an encoder-decoder architecture. In the encoding stage, the\nimage feature is reduced to various smallscale spaces for multi-scale\ninformation extraction and fusion without a large amount of information loss.\nThen, a distillation network is used in the decoding stage, which allows the\nnetwork benefit the most from residual learning while remaining sufficiently\nlightweight. Meanwhile, an information fusion strategy between distillation\nmodules and feature channels is also carried out by attention mechanism.\nThrough fusing different information in the proposed approach, our network can\nachieve state-of-the-art image deblurring result with smaller number of\nparameters and outperforms existing methods in model complexity.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:37:37 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zhang", "Yanni", ""], ["Liu", "Yiming", ""], ["Li", "Qiang", ""], ["Qi", "Miao", ""], ["Xu", "Dahong", ""], ["Kong", "Jun", ""], ["Wang", "Jianzhong", ""]]}, {"id": "2101.05404", "submitter": "Justyna P. Zwolak", "authors": "Shangjie Guo, Amilson R. Fritsch, Craig Greenberg, I. B. Spielman,\n  Justyna P. Zwolak", "title": "Machine-learning enhanced dark soliton detection in Bose-Einstein\n  condensates", "comments": "17 pages, 5 figures", "journal-ref": "Mach. Learn.: Sci. Technol. 2: 035020 (2021)", "doi": "10.1088/2632-2153/abed1e", "report-no": null, "categories": "cond-mat.quant-gas cs.CV cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most data in cold-atom experiments comes from images, the analysis of which\nis limited by our preconceptions of the patterns that could be present in the\ndata. We focus on the well-defined case of detecting dark solitons -- appearing\nas local density depletions in a Bose-Einstein condensate (BEC) -- using a\nmethodology that is extensible to the general task of pattern recognition in\nimages of cold atoms. Studying soliton dynamics over a wide range of parameters\nrequires the analysis of large datasets, making the existing\nhuman-inspection-based methodology a significant bottleneck. Here we describe\nan automated classification and positioning system for identifying localized\nexcitations in atomic BECs utilizing deep convolutional neural networks to\neliminate the need for human image examination. Furthermore, we openly publish\nour labeled dataset of dark solitons, the first of its kind, for further\nmachine learning research.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:44:56 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 17:41:14 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Guo", "Shangjie", ""], ["Fritsch", "Amilson R.", ""], ["Greenberg", "Craig", ""], ["Spielman", "I. B.", ""], ["Zwolak", "Justyna P.", ""]]}, {"id": "2101.05410", "submitter": "Yi Liu", "authors": "Yi Liu, Shuiwang Ji", "title": "A Multi-Stage Attentive Transfer Learning Framework for Improving\n  COVID-19 Diagnosis", "comments": "12 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) imaging is a promising approach to diagnosing the\nCOVID-19. Machine learning methods can be employed to train models from labeled\nCT images and predict whether a case is positive or negative. However, there\nexists no publicly-available and large-scale CT data to train accurate models.\nIn this work, we propose a multi-stage attentive transfer learning framework\nfor improving COVID-19 diagnosis. Our proposed framework consists of three\nstages to train accurate diagnosis models through learning knowledge from\nmultiple source tasks and data of different domains. Importantly, we propose a\nnovel self-supervised learning method to learn multi-scale representations for\nlung CT images. Our method captures semantic information from the whole lung\nand highlights the functionality of each lung region for better representation\nlearning. The method is then integrated to the last stage of the proposed\ntransfer learning framework to reuse the complex patterns learned from the same\nCT images. We use a base model integrating self-attention (ATTNs) and\nconvolutional operations. Experimental results show that networks with ATTNs\ninduce greater performance improvement through transfer learning than networks\nwithout ATTNs. This indicates attention exhibits higher transferability than\nconvolution. Our results also show that the proposed self-supervised learning\nmethod outperforms several baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 01:39:19 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Liu", "Yi", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2101.05419", "submitter": "Gaoang Wang", "authors": "Gaoang Wang, Lin Chen, Tianqiang Liu, Mingwei He, and Jiebo Luo", "title": "DAIL: Dataset-Aware and Invariant Learning for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To achieve good performance in face recognition, a large scale training\ndataset is usually required. A simple yet effective way to improve recognition\nperformance is to use a dataset as large as possible by combining multiple\ndatasets in the training. However, it is problematic and troublesome to naively\ncombine different datasets due to two major issues. First, the same person can\npossibly appear in different datasets, leading to an identity overlapping issue\nbetween different datasets. Naively treating the same person as different\nclasses in different datasets during training will affect back-propagation and\ngenerate non-representative embeddings. On the other hand, manually cleaning\nlabels may take formidable human efforts, especially when there are millions of\nimages and thousands of identities. Second, different datasets are collected in\ndifferent situations and thus will lead to different domain distributions.\nNaively combining datasets will make it difficult to learn domain invariant\nembeddings across different datasets. In this paper, we propose DAIL:\nDataset-Aware and Invariant Learning to resolve the above-mentioned issues. To\nsolve the first issue of identity overlapping, we propose a dataset-aware loss\nfor multi-dataset training by reducing the penalty when the same person appears\nin multiple datasets. This can be readily achieved with a modified softmax loss\nwith a dataset-aware term. To solve the second issue, domain adaptation with\ngradient reversal layers is employed for dataset invariant learning. The\nproposed approach not only achieves state-of-the-art results on several\ncommonly used face recognition validation sets, including LFW, CFP-FP, and\nAgeDB-30, but also shows great benefit for practical use.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 01:59:52 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wang", "Gaoang", ""], ["Chen", "Lin", ""], ["Liu", "Tianqiang", ""], ["He", "Mingwei", ""], ["Luo", "Jiebo", ""]]}, {"id": "2101.05434", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Fangxu Xing, Georges El Fakhri, Jonghye Woo", "title": "A Unified Conditional Disentanglement Framework for Multimodal Brain MR\n  Image Translation", "comments": "Published in IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2021 for Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal MRI provides complementary and clinically relevant information to\nprobe tissue condition and to characterize various diseases. However, it is\noften difficult to acquire sufficiently many modalities from the same subject\ndue to limitations in study plans, while quantitative analysis is still\ndemanded. In this work, we propose a unified conditional disentanglement\nframework to synthesize any arbitrary modality from an input modality. Our\nframework hinges on a cycle-constrained conditional adversarial training\napproach, where it can extract a modality-invariant anatomical feature with a\nmodality-agnostic encoder and generate a target modality with a conditioned\ndecoder. We validate our framework on four MRI modalities, including\nT1-weighted, T1 contrast enhanced, T2-weighted, and FLAIR MRI, from the\nBraTS'18 database, showing superior performance on synthesis quality over the\ncomparison methods. In addition, we report results from experiments on a tumor\nsegmentation task carried out with synthesized data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 03:14:24 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 15:15:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Xing", "Fangxu", ""], ["Fakhri", "Georges El", ""], ["Woo", "Jonghye", ""]]}, {"id": "2101.05439", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Fangxu Xing, Jerry L. Prince, Aaron Carass, Maureen\n  Stone, Georges El Fakhri, Jonghye Woo", "title": "Dual-cycle Constrained Bijective VAE-GAN For Tagged-to-Cine Magnetic\n  Resonance Image Synthesis", "comments": "Accepted to IEEE International Symposium on Biomedical Imaging (ISBI)\n  2021", "journal-ref": "2021 IEEE 18th International Symposium on Biomedical Imaging\n  (ISBI)", "doi": "10.1109/ISBI48211.2021.9433852", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tagged magnetic resonance imaging (MRI) is a widely used imaging technique\nfor measuring tissue deformation in moving organs. Due to tagged MRI's\nintrinsic low anatomical resolution, another matching set of cine MRI with\nhigher resolution is sometimes acquired in the same scanning session to\nfacilitate tissue segmentation, thus adding extra time and cost. To mitigate\nthis, in this work, we propose a novel dual-cycle constrained bijective VAE-GAN\napproach to carry out tagged-to-cine MR image synthesis. Our method is based on\na variational autoencoder backbone with cycle reconstruction constrained\nadversarial training to yield accurate and realistic cine MR images given\ntagged MR images. Our framework has been trained, validated, and tested using\n1,768, 416, and 1,560 subject-independent paired slices of tagged and cine MRI\nfrom twenty healthy subjects, respectively, demonstrating superior performance\nover the comparison methods. Our method can potentially be used to reduce the\nextra acquisition time and cost, while maintaining the same workflow for\nfurther motion analyses.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 03:27:16 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Xing", "Fangxu", ""], ["Prince", "Jerry L.", ""], ["Carass", "Aaron", ""], ["Stone", "Maureen", ""], ["Fakhri", "Georges El", ""], ["Woo", "Jonghye", ""]]}, {"id": "2101.05442", "submitter": "Xin He", "authors": "Xin He, Shihao Wang, Xiaowen Chu, Shaohuai Shi, Jiangping Tang, Xin\n  Liu, Chenggang Yan, Jiyong Zhang, Guiguang Ding", "title": "Automated Model Design and Benchmarking of 3D Deep Learning Models for\n  COVID-19 Detection with Chest CT Scans", "comments": "Accepted by AAAI 2021, COVID-19, Neural Architecture Search, AutoML", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has spread globally for several months. Because its\ntransmissibility and high pathogenicity seriously threaten people's lives, it\nis crucial to accurately and quickly detect COVID-19 infection. Many recent\nstudies have shown that deep learning (DL) based solutions can help detect\nCOVID-19 based on chest CT scans. However, most existing work focuses on 2D\ndatasets, which may result in low quality models as the real CT scans are 3D\nimages. Besides, the reported results span a broad spectrum on different\ndatasets with a relatively unfair comparison. In this paper, we first use three\nstate-of-the-art 3D models (ResNet3D101, DenseNet3D121, and MC3\\_18) to\nestablish the baseline performance on the three publicly available chest CT\nscan datasets. Then we propose a differentiable neural architecture search\n(DNAS) framework to automatically search for the 3D DL models for 3D chest CT\nscans classification with the Gumbel Softmax technique to improve the searching\nefficiency. We further exploit the Class Activation Mapping (CAM) technique on\nour models to provide the interpretability of the results. The experimental\nresults show that our automatically searched models (CovidNet3D) outperform the\nbaseline human-designed models on the three datasets with tens of times smaller\nmodel size and higher accuracy. Furthermore, the results also verify that CAM\ncan be well applied in CovidNet3D for COVID-19 datasets to provide\ninterpretability for medical diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 03:45:01 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 05:02:43 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["He", "Xin", ""], ["Wang", "Shihao", ""], ["Chu", "Xiaowen", ""], ["Shi", "Shaohuai", ""], ["Tang", "Jiangping", ""], ["Liu", "Xin", ""], ["Yan", "Chenggang", ""], ["Zhang", "Jiyong", ""], ["Ding", "Guiguang", ""]]}, {"id": "2101.05456", "submitter": "Abhinav Dhere", "authors": "Abhinav Dhere, Jayanthi Sivaswamy", "title": "Self-Supervised Learning for Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Self-supervised learning is emerging as an effective substitute for transfer\nlearning from large datasets. In this work, we use kidney segmentation to\nexplore this idea. The anatomical asymmetry of kidneys is leveraged to define\nan effective proxy task for kidney segmentation via self-supervised learning. A\nsiamese convolutional neural network (CNN) is used to classify a given pair of\nkidney sections from CT volumes as being kidneys of the same or different\nsides. This knowledge is then transferred for the segmentation of kidneys using\nanother deep CNN using one branch of the siamese CNN as the encoder for the\nsegmentation network. Evaluation results on a publicly available dataset\ncontaining computed tomography (CT) scans of the abdominal region shows that a\nboost in performance and fast convergence can be had relative to a network\ntrained conventionally from scratch. This is notable given that no additional\ndata/expensive annotations or augmentation were used in training.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 04:28:47 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Dhere", "Abhinav", ""], ["Sivaswamy", "Jayanthi", ""]]}, {"id": "2101.05470", "submitter": "Mario Michael Krell", "authors": "Daniel Ma, Gerald Friedland, Mario Michael Krell", "title": "OrigamiSet1.0: Two New Datasets for Origami Classification and\n  Difficulty Estimation", "comments": "In Proceedings of Origami Science Maths Education, 7OSME, Oxford UK\n  (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Origami is becoming more and more relevant to research. However, there is no\npublic dataset yet available and there hasn't been any research on this topic\nin machine learning. We constructed an origami dataset using images from the\nmultimedia commons and other databases. It consists of two subsets: one for\nclassification of origami images and the other for difficulty estimation. We\nobtained 16000 images for classification (half origami, half other objects) and\n1509 for difficulty estimation with $3$ different categories (easy: 764,\nintermediate: 427, complex: 318). The data can be downloaded at:\nhttps://github.com/multimedia-berkeley/OriSet. Finally, we provide machine\nlearning baselines.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 06:32:46 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Ma", "Daniel", ""], ["Friedland", "Gerald", ""], ["Krell", "Mario Michael", ""]]}, {"id": "2101.05479", "submitter": "Sharanya Chakravarthy", "authors": "Vinay Damodaran, Sharanya Chakravarthy, Akshay Kumar, Anjana Umapathy,\n  Teruko Mitamura, Yuta Nakashima, Noa Garcia, Chenhui Chu", "title": "Understanding the Role of Scene Graphs in Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Visual Question Answering (VQA) is of tremendous interest to the research\ncommunity with important applications such as aiding visually impaired users\nand image-based search. In this work, we explore the use of scene graphs for\nsolving the VQA task. We conduct experiments on the GQA dataset which presents\na challenging set of questions requiring counting, compositionality and\nadvanced reasoning capability, and provides scene graphs for a large number of\nimages. We adopt image + question architectures for use with scene graphs,\nevaluate various scene graph generation techniques for unseen images, propose a\ntraining curriculum to leverage human-annotated and auto-generated scene\ngraphs, and build late fusion architectures to learn from multiple image\nrepresentations. We present a multi-faceted study into the use of scene graphs\nfor VQA, making this work the first of its kind.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:27:37 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 04:17:07 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Damodaran", "Vinay", ""], ["Chakravarthy", "Sharanya", ""], ["Kumar", "Akshay", ""], ["Umapathy", "Anjana", ""], ["Mitamura", "Teruko", ""], ["Nakashima", "Yuta", ""], ["Garcia", "Noa", ""], ["Chu", "Chenhui", ""]]}, {"id": "2101.05544", "submitter": "Alexandre Rame", "authors": "Alexandre Rame and Matthieu Cord", "title": "DICE: Diversity in Deep Ensembles via Conditional Redundancy Adversarial\n  Estimation", "comments": "Published as a conference paper at ICLR 2021. 9 main pages, 13\n  figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep ensembles perform better than a single network thanks to the diversity\namong their members. Recent approaches regularize predictions to increase\ndiversity; however, they also drastically decrease individual members'\nperformances. In this paper, we argue that learning strategies for deep\nensembles need to tackle the trade-off between ensemble diversity and\nindividual accuracies. Motivated by arguments from information theory and\nleveraging recent advances in neural estimation of conditional mutual\ninformation, we introduce a novel training criterion called DICE: it increases\ndiversity by reducing spurious correlations among features. The main idea is\nthat features extracted from pairs of members should only share information\nuseful for target class prediction without being conditionally redundant.\nTherefore, besides the classification loss with information bottleneck, we\nadversarially prevent features from being conditionally predictable from each\nother. We manage to reduce simultaneous errors while protecting class\ninformation. We obtain state-of-the-art accuracy results on CIFAR-10/100: for\nexample, an ensemble of 5 networks trained with DICE matches an ensemble of 7\nnetworks trained independently. We further analyze the consequences on\ncalibration, uncertainty estimation, out-of-distribution detection and online\nco-distillation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 10:53:26 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Rame", "Alexandre", ""], ["Cord", "Matthieu", ""]]}, {"id": "2101.05564", "submitter": "M. F. Mridha", "authors": "Abu Quwsar Ohi, M. F. Mridha, Md. Abdul Hamid, Muhammad Mostafa\n  Monowar, Faris A Kateb", "title": "FabricNet: A Fiber Recognition Architecture Using Ensemble ConvNets", "comments": "Accepted in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3051980", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fabric is a planar material composed of textile fibers. Textile fibers are\ngenerated from many natural sources; including plants, animals, minerals, and\neven, it can be synthetic. A particular fabric may contain different types of\nfibers that pass through a complex production process. Fiber identification is\nusually carried out through chemical tests and microscopic tests. However,\nthese testing processes are complicated as well as time-consuming. We propose\nFabricNet, a pioneering approach for the image-based textile fiber recognition\nsystem, which may have a revolutionary impact from individual to the industrial\nfiber recognition process. The FabricNet can recognize a large scale of fibers\nby only utilizing a surface image of fabric. The recognition system is\nconstructed using a distinct category of class-based ensemble convolutional\nneural network (CNN) architecture. The experiment is conducted on recognizing\n50 different types of textile fibers. This experiment includes a significantly\nlarge number of unique textile fibers than previous research endeavors to the\nbest of our knowledge. We experiment with popular CNN architectures that\ninclude Inception, ResNet, VGG, MobileNet, DenseNet, and Xception. Finally, the\nexperimental results demonstrate that FabricNet outperforms the\nstate-of-the-art popular CNN architectures by reaching an accuracy of 84% and\nF1-score of 90%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 12:11:23 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Ohi", "Abu Quwsar", ""], ["Mridha", "M. F.", ""], ["Hamid", "Md. Abdul", ""], ["Monowar", "Muhammad Mostafa", ""], ["Kateb", "Faris A", ""]]}, {"id": "2101.05570", "submitter": "Aythami Morales", "authors": "Alejandro Acien and Aythami Morales and John V. Monaco and Ruben\n  Vera-Rodriguez and Julian Fierrez", "title": "TypeNet: Deep Learning Keystroke Biometrics", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.03627", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study the performance of Long Short-Term Memory networks for keystroke\nbiometric authentication at large scale in free-text scenarios. For this we\nintroduce TypeNet, a Recurrent Neural Network (RNN) trained with a moderate\nnumber of keystrokes per identity. We evaluate different learning approaches\ndepending on the loss function (softmax, contrastive, and triplet loss), number\nof gallery samples, length of the keystroke sequences, and device type\n(physical vs touchscreen keyboard). With 5 gallery sequences and test sequences\nof length 50, TypeNet achieves state-of-the-art keystroke biometric\nauthentication performance with an Equal Error Rate of 2.2% and 9.2% for\nphysical and touchscreen keyboards, respectively, significantly outperforming\nprevious approaches. Our experiments demonstrate a moderate increase in error\nwith up to 100,000 subjects, demonstrating the potential of TypeNet to operate\nat an Internet scale. We utilize two Aalto University keystroke databases, one\ncaptured on physical keyboards and the second on mobile devices (touchscreen\nkeyboards). To the best of our knowledge, both databases are the largest\nexisting free-text keystroke databases available for research with more than\n136 million keystrokes from 168,000 subjects in physical keyboards, and 60,000\nsubjects with more than 63 million keystrokes acquired on mobile touchscreens.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 12:49:09 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 17:40:57 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Acien", "Alejandro", ""], ["Morales", "Aythami", ""], ["Monaco", "John V.", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""]]}, {"id": "2101.05608", "submitter": "Lasitha Vidyaratne", "authors": "Lasitha Vidyaratne, Mahbubul Alam, Alexander Glandon, Anna Shabalina,\n  Christopher Tennant, and Khan Iftekharuddin", "title": "Deep Cellular Recurrent Network for Efficient Analysis of Time-Series\n  Data with Spatial Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Efficient processing of large-scale time series data is an intricate problem\nin machine learning. Conventional sensor signal processing pipelines with hand\nengineered feature extraction often involve huge computational cost with high\ndimensional data. Deep recurrent neural networks have shown promise in\nautomated feature learning for improved time-series processing. However,\ngeneric deep recurrent models grow in scale and depth with increased complexity\nof the data. This is particularly challenging in presence of high dimensional\ndata with temporal and spatial characteristics. Consequently, this work\nproposes a novel deep cellular recurrent neural network (DCRNN) architecture to\nefficiently process complex multi-dimensional time series data with spatial\ninformation. The cellular recurrent architecture in the proposed model allows\nfor location-aware synchronous processing of time series data from spatially\ndistributed sensor signal sources. Extensive trainable parameter sharing due to\ncellularity in the proposed architecture ensures efficiency in the use of\nrecurrent processing units with high-dimensional inputs. This study also\ninvestigates the versatility of the proposed DCRNN model for classification of\nmulti-class time series data from different application domains. Consequently,\nthe proposed DCRNN architecture is evaluated using two time-series datasets: a\nmultichannel scalp EEG dataset for seizure detection, and a machine fault\ndetection dataset obtained in-house. The results suggest that the proposed\narchitecture achieves state-of-the-art performance while utilizing\nsubstantially less trainable parameters when compared to comparable methods in\nthe literature.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:08:18 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Vidyaratne", "Lasitha", ""], ["Alam", "Mahbubul", ""], ["Glandon", "Alexander", ""], ["Shabalina", "Anna", ""], ["Tennant", "Christopher", ""], ["Iftekharuddin", "Khan", ""]]}, {"id": "2101.05616", "submitter": "Takato Yasuno", "authors": "Takato Yasuno, Junichiro Fujii, Hiroaki Sugawara, Masazumi Amakata", "title": "Road Surface Translation Under Snow-covered and Semantic Segmentation\n  for Snow Hazard Index", "comments": "9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2020, there was a record heavy snowfall owing to climate change. In\nreality, 2,000 vehicles were stuck on the highway for three days. Because of\nthe freezing of the road surface, 10 vehicles had a billiard accident. Road\nmanagers are required to provide indicators to alert drivers regarding snow\ncover at hazardous locations. This study proposes a deep learning application\nwith live image post-processing to automatically calculate a snow hazard ratio\nindicator. First, the road surface hidden under snow is translated using a\ngenerative adversarial network, pix2pix. Second, snow-covered and road surface\nclasses are detected by semantic segmentation using DeepLabv3+ with MobileNet\nas a backbone. Based on these trained networks, we automatically compute the\nroad to snow rate hazard index, indicating the amount of snow covered on the\nroad surface. We demonstrate the applied results to 1,155 live snow images of\nthe cold region in Japan. We mention the usefulness and the practical\nrobustness of our study.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 14:25:12 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 14:51:10 GMT"}, {"version": "v3", "created": "Sat, 23 Jan 2021 10:12:59 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 11:52:19 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yasuno", "Takato", ""], ["Fujii", "Junichiro", ""], ["Sugawara", "Hiroaki", ""], ["Amakata", "Masazumi", ""]]}, {"id": "2101.05650", "submitter": "Arnav Chavan", "authors": "Arnav Chavan, Udbhav Bamba, Rishabh Tiwari, Deepak Gupta", "title": "Rescaling CNN through Learnable Repetition of Network Parameters", "comments": "Under Review at ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deeper and wider CNNs are known to provide improved performance for deep\nlearning tasks. However, most such networks have poor performance gain per\nparameter increase. In this paper, we investigate whether the gain observed in\ndeeper models is purely due to the addition of more optimization parameters or\nwhether the physical size of the network as well plays a role. Further, we\npresent a novel rescaling strategy for CNNs based on learnable repetition of\nits parameters. Based on this strategy, we rescale CNNs without changing their\nparameter count, and show that learnable sharing of weights itself can provide\nsignificant boost in the performance of any given model without changing its\nparameter count. We show that small base networks when rescaled, can provide\nperformance comparable to deeper networks with as low as 6% of optimization\nparameters of the deeper one.\n  The relevance of weight sharing is further highlighted through the example of\ngroup-equivariant CNNs. We show that the significant improvements obtained with\ngroup-equivariant CNNs over the regular CNNs on classification problems are\nonly partly due to the added equivariance property, and part of it comes from\nthe learnable repetition of network weights. For rot-MNIST dataset, we show\nthat up to 40% of the relative gain reported by state-of-the-art methods for\nrotation equivariance could actually be due to just the learnt repetition of\nweights.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:03:25 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Chavan", "Arnav", ""], ["Bamba", "Udbhav", ""], ["Tiwari", "Rishabh", ""], ["Gupta", "Deepak", ""]]}, {"id": "2101.05661", "submitter": "Carson Schubert", "authors": "Carson Schubert, Kevin Black, Daniel Fonseka, Abhimanyu Dhir, Jacob\n  Deutsch, Nihal Dhamani, Gavin Martin, Maruthi Akella", "title": "A Pipeline for Vision-Based On-Orbit Proximity Operations Using Deep\n  Learning and Synthetic Imagery", "comments": "Accepted to IEEE Aerospace Conference 2021. 14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become the gold standard for image processing over the past\ndecade. Simultaneously, we have seen growing interest in orbital activities\nsuch as satellite servicing and debris removal that depend on proximity\noperations between spacecraft. However, two key challenges currently pose a\nmajor barrier to the use of deep learning for vision-based on-orbit proximity\noperations. Firstly, efficient implementation of these techniques relies on an\neffective system for model development that streamlines data curation,\ntraining, and evaluation. Secondly, a scarcity of labeled training data (images\nof a target spacecraft) hinders creation of robust deep learning models. This\npaper presents an open-source deep learning pipeline, developed specifically\nfor on-orbit visual navigation applications, that addresses these challenges.\nThe core of our work consists of two custom software tools built on top of a\ncloud architecture that interconnects all stages of the model development\nprocess. The first tool leverages Blender, an open-source 3D graphics toolset,\nto generate labeled synthetic training data with configurable model poses\n(positions and orientations), lighting conditions, backgrounds, and commonly\nobserved in-space image aberrations. The second tool is a plugin-based\nframework for effective dataset curation and model training; it provides common\nfunctionality like metadata generation and remote storage access to all\nprojects while giving complete independence to project-specific code.\nTime-consuming, graphics-intensive processes such as synthetic image generation\nand model training run on cloud-based computational resources which scale to\nany scope and budget and allow development of even the largest datasets and\nmodels from any machine. The presented system has been used in the Texas\nSpacecraft Laboratory with marked benefits in development speed and quality.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:17:54 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Schubert", "Carson", ""], ["Black", "Kevin", ""], ["Fonseka", "Daniel", ""], ["Dhir", "Abhimanyu", ""], ["Deutsch", "Jacob", ""], ["Dhamani", "Nihal", ""], ["Martin", "Gavin", ""], ["Akella", "Maruthi", ""]]}, {"id": "2101.05682", "submitter": "Yuying Chen", "authors": "Congcong Liu, Yuying Chen, Ming Liu, Bertram E. Shi", "title": "AVGCN: Trajectory Prediction using Graph Convolutional Networks Guided\n  by Human Attention", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian trajectory prediction is a critical yet challenging task,\nespecially for crowded scenes. We suggest that introducing an attention\nmechanism to infer the importance of different neighbors is critical for\naccurate trajectory prediction in scenes with varying crowd size. In this work,\nwe propose a novel method, AVGCN, for trajectory prediction utilizing graph\nconvolutional networks (GCN) based on human attention (A denotes attention, V\ndenotes visual field constraints). First, we train an attention network that\nestimates the importance of neighboring pedestrians, using gaze data collected\nas subjects perform a bird's eye view crowd navigation task. Then, we\nincorporate the learned attention weights modulated by constraints on the\npedestrian's visual field into a trajectory prediction network that uses a GCN\nto aggregate information from neighbors efficiently. AVGCN also considers the\nstochastic nature of pedestrian trajectories by taking advantage of variational\ntrajectory prediction. Our approach achieves state-of-the-art performance on\nseveral trajectory prediction benchmarks, and the lowest average prediction\nerror over all considered benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:00:31 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Liu", "Congcong", ""], ["Chen", "Yuying", ""], ["Liu", "Ming", ""], ["Shi", "Bertram E.", ""]]}, {"id": "2101.05687", "submitter": "Geng Chen", "authors": "Bo Dong, Mingchen Zhuge, Yongxiong Wang, Hongbo Bi, Geng Chen", "title": "Accurate Camouflaged Object Detection via Mixture Convolution and\n  Interactive Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camouflaged object detection (COD), which aims to identify the objects that\nconceal themselves into the surroundings, has recently drawn increasing\nresearch efforts in the field of computer vision. In practice, the success of\ndeep learning based COD is mainly determined by two key factors, including (i)\nA significantly large receptive field, which provides rich context information,\nand (ii) An effective fusion strategy, which aggregates the rich multi-level\nfeatures for accurate COD. Motivated by these observations, in this paper, we\npropose a novel deep learning based COD approach, which integrates the large\nreceptive field and effective feature fusion into a unified framework.\nSpecifically, we first extract multi-level features from a backbone network.\nThe resulting features are then fed to the proposed dual-branch mixture\nconvolution modules, each of which utilizes multiple asymmetric convolutional\nlayers and two dilated convolutional layers to extract rich context features\nfrom a large receptive field. Finally, we fuse the features using\nspecially-designed multi-level interactive fusion modules, each of which\nemploys an attention mechanism along with feature interaction for effective\nfeature fusion. Our method detects camouflaged objects with an effective fusion\nstrategy, which aggregates the rich context information from a large receptive\nfield. All of these designs meet the requirements of COD well, allowing the\naccurate detection of camouflaged objects. Extensive experiments on widely-used\nbenchmark datasets demonstrate that our method is capable of accurately\ndetecting camouflaged objects and outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:06:08 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 05:13:55 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Dong", "Bo", ""], ["Zhuge", "Mingchen", ""], ["Wang", "Yongxiong", ""], ["Bi", "Hongbo", ""], ["Chen", "Geng", ""]]}, {"id": "2101.05725", "submitter": "Leonardo Parisi", "authors": "Riccardo Beschi, Xiao Feng, Stefania Melillo, Leonardo Parisi, Lorena\n  Postiglione", "title": "Stereo camera system calibration: the need of two sets of parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of a scene via a stereo-camera system is a two-steps\nprocess, where at first images from different cameras are matched to identify\nthe set of point-to-point correspondences that then will actually be\nreconstructed in the three dimensional real world. The performance of the\nsystem strongly relies of the calibration procedure, which has to be carefully\ndesigned to guarantee optimal results. We implemented three different\ncalibration methods and we compared their performance over 19 datasets. We\npresent the experimental evidence that, due to the image noise, a single set of\nparameters is not sufficient to achieve high accuracy in the identification of\nthe correspondences and in the 3D reconstruction at the same time. We propose\nto calibrate the system twice to estimate two different sets of parameters: the\none obtained by minimizing the reprojection error that will be used when\ndealing with quantities defined in the 2D space of the cameras, and the one\nobtained by minimizing the reconstruction error that will be used when dealing\nwith quantities defined in the real 3D world.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 17:03:17 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Beschi", "Riccardo", ""], ["Feng", "Xiao", ""], ["Melillo", "Stefania", ""], ["Parisi", "Leonardo", ""], ["Postiglione", "Lorena", ""]]}, {"id": "2101.05778", "submitter": "Benjamin Filippenko", "authors": "Ephy R. Love, Benjamin Filippenko, Vasileios Maroulas, Gunnar Carlsson", "title": "Topological Deep Learning", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces the Topological CNN (TCNN), which encompasses several\ntopologically defined convolutional methods. Manifolds with important\nrelationships to the natural image space are used to parameterize image filters\nwhich are used as convolutional weights in a TCNN. These manifolds also\nparameterize slices in layers of a TCNN across which the weights are localized.\nWe show evidence that TCNNs learn faster, on less data, with fewer learned\nparameters, and with greater generalizability and interpretability than\nconventional CNNs. We introduce and explore TCNN layers for both image and\nvideo data. We propose extensions to 3D images and 3D video.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:32:11 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 11:30:12 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Love", "Ephy R.", ""], ["Filippenko", "Benjamin", ""], ["Maroulas", "Vasileios", ""], ["Carlsson", "Gunnar", ""]]}, {"id": "2101.05791", "submitter": "Thomas (Teddy) Koker", "authors": "Teddy Koker, Fatemehsadat Mireshghallah, Tom Titcombe, Georgios\n  Kaissis", "title": "U-Noise: Learnable Noise Masks for Interpretable Image Segmentation", "comments": "Submitted to ICIP. Revision: corrected affiliation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) are widely used for decision making in a myriad\nof critical applications, ranging from medical to societal and even judicial.\nGiven the importance of these decisions, it is crucial for us to be able to\ninterpret these models. We introduce a new method for interpreting image\nsegmentation models by learning regions of images in which noise can be applied\nwithout hindering downstream model performance. We apply this method to\nsegmentation of the pancreas in CT scans, and qualitatively compare the quality\nof the method to existing explainability techniques, such as Grad-CAM and\nocclusion sensitivity. Additionally we show that, unlike other methods, our\ninterpretability model can be quantitatively evaluated based on the downstream\nperformance over obscured images.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:50:06 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 17:04:28 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Koker", "Teddy", ""], ["Mireshghallah", "Fatemehsadat", ""], ["Titcombe", "Tom", ""], ["Kaissis", "Georgios", ""]]}, {"id": "2101.05796", "submitter": "Valentin Wolf", "authors": "Valentin Wolf, Andreas Lugmayr, Martin Danelljan, Luc Van Gool, Radu\n  Timofte", "title": "DeFlow: Learning Complex Image Degradations from Unpaired Data with\n  Conditional Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of obtaining paired data remains a major bottleneck for\nlearning image restoration and enhancement models for real-world applications.\nCurrent strategies aim to synthesize realistic training data by modeling noise\nand degradations that appear in real-world settings. We propose DeFlow, a\nmethod for learning stochastic image degradations from unpaired data. Our\napproach is based on a novel unpaired learning formulation for conditional\nnormalizing flows. We model the degradation process in the latent space of a\nshared flow encoder-decoder network. This allows us to learn the conditional\ndistribution of a noisy image given the clean input by solely minimizing the\nnegative log-likelihood of the marginal distributions. We validate our DeFlow\nformulation on the task of joint image restoration and super-resolution. The\nmodels trained with the synthetic data generated by DeFlow outperform previous\nlearnable approaches on all three datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:58:01 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wolf", "Valentin", ""], ["Lugmayr", "Andreas", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2101.05806", "submitter": "Ganesh Samarth Chamarahalli Arunkumar", "authors": "Praveen S V, Akhilesh Bharadwaj, Harsh Raj, Janhavi Dadhania, Ganesh\n  Samarth C.A, Nikhil Pareek, S R M Prasanna", "title": "Exploration of Visual Features and their weighted-additive fusion for\n  Video Captioning", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video captioning is a popular task that challenges models to describe events\nin videos using natural language. In this work, we investigate the ability of\nvarious visual feature representations derived from state-of-the-art\nconvolutional neural networks to capture high-level semantic context. We\nintroduce the Weighted Additive Fusion Transformer with Memory Augmented\nEncoders (WAFTM), a captioning model that incorporates memory in a transformer\nencoder and uses a novel method, to fuse features, that ensures due importance\nis given to more significant representations. We illustrate a gain in\nperformance realized by applying Word-Piece Tokenization and a popular\nREINFORCE algorithm. Finally, we benchmark our model on two datasets and obtain\na CIDEr of 92.4 on MSVD and a METEOR of 0.091 on the ActivityNet Captions\nDataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:21:13 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["S", "Praveen", "V"], ["Bharadwaj", "Akhilesh", ""], ["Raj", "Harsh", ""], ["Dadhania", "Janhavi", ""], ["A", "Ganesh Samarth C.", ""], ["Pareek", "Nikhil", ""], ["Prasanna", "S R M", ""]]}, {"id": "2101.05833", "submitter": "Li-Yun Wang", "authors": "Li-Yun Wang, Yeganeh Jalalpour, Wu-chi Feng", "title": "Context-Aware Image Denoising with Auto-Threshold Canny Edge Detection\n  to Suppress Adversarial Perturbation", "comments": "5 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel context-aware image denoising algorithm that\ncombines an adaptive image smoothing technique and color reduction techniques\nto remove perturbation from adversarial images. Adaptive image smoothing is\nachieved using auto-threshold canny edge detection to produce an accurate edge\nmap used to produce a blurred image that preserves more edge features. The\nproposed algorithm then uses color reduction techniques to reconstruct the\nimage using only a few representative colors. Through this technique, the\nalgorithm can reduce the effects of adversarial perturbations on images. We\nalso discuss experimental data on classification accuracy. Our results showed\nthat the proposed approach reduces adversarial perturbation in adversarial\nattacks and increases the robustness of the deep convolutional neural network\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:15:28 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Wang", "Li-Yun", ""], ["Jalalpour", "Yeganeh", ""], ["Feng", "Wu-chi", ""]]}, {"id": "2101.05846", "submitter": "Josef Lorenz Rumberger", "authors": "Josef Lorenz Rumberger, Xiaoyan Yu, Peter Hirsch, Melanie Dohmen,\n  Vanessa Emanuela Guarino, Ashkan Mokarian, Lisa Mais, Jan Funke, Dagmar\n  Kainmueller", "title": "How Shift Equivariance Impacts Metric Learning for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning has received conflicting assessments concerning its\nsuitability for solving instance segmentation tasks. It has been dismissed as\ntheoretically flawed due to the shift equivariance of the employed CNNs and\ntheir respective inability to distinguish same-looking objects. Yet it has been\nshown to yield state of the art results for a variety of tasks, and practical\nissues have mainly been reported in the context of tile-and-stitch approaches,\nwhere discontinuities at tile boundaries have been observed. To date, neither\nof the reported issues have undergone thorough formal analysis. In our work, we\ncontribute a comprehensive formal analysis of the shift equivariance properties\nof encoder-decoder-style CNNs, which yields a clear picture of what can and\ncannot be achieved with metric learning in the face of same-looking objects. In\nparticular, we prove that a standard encoder-decoder network that takes\n$d$-dimensional images as input, with $l$ pooling layers and pooling factor\n$f$, has the capacity to distinguish at most $f^{dl}$ same-looking objects, and\nwe show that this upper limit can be reached. Furthermore, we show that to\navoid discontinuities in a tile-and-stitch approach, assuming standard batch\nsize 1, it is necessary to employ valid convolutions in combination with a\ntraining output window size strictly greater than $f^l$, while at test-time it\nis necessary to crop tiles to size $n\\cdot f^l$ before stitching, with $n\\geq\n1$. We complement these theoretical findings by discussing a number of\ninsightful special cases for which we show empirical results on synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:48:24 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Rumberger", "Josef Lorenz", ""], ["Yu", "Xiaoyan", ""], ["Hirsch", "Peter", ""], ["Dohmen", "Melanie", ""], ["Guarino", "Vanessa Emanuela", ""], ["Mokarian", "Ashkan", ""], ["Mais", "Lisa", ""], ["Funke", "Jan", ""], ["Kainmueller", "Dagmar", ""]]}, {"id": "2101.05893", "submitter": "Xin Wang", "authors": "Jinkun Cao, Xin Wang, Trevor Darrell, Fisher Yu", "title": "Instance-Aware Predictive Navigation in Multi-Agent Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to achieve efficient end-to-end learning of driving\npolicies in dynamic multi-agent environments. Predicting and anticipating\nfuture events at the object level are critical for making informed driving\ndecisions. We propose an Instance-Aware Predictive Control (IPC) approach,\nwhich forecasts interactions between agents as well as future scene structures.\nWe adopt a novel multi-instance event prediction module to estimate the\npossible interaction among agents in the ego-centric view, conditioned on the\nselected action sequence of the ego-vehicle. To decide the action at each step,\nwe seek the action sequence that can lead to safe future states based on the\nprediction module outputs by repeatedly sampling likely action sequences. We\ndesign a sequential action sampling strategy to better leverage predicted\nstates on both scene-level and instance-level. Our method establishes a new\nstate of the art in the challenging CARLA multi-agent driving simulation\nenvironments without expert demonstration, giving better explainability and\nsample efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 22:21:25 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Cao", "Jinkun", ""], ["Wang", "Xin", ""], ["Darrell", "Trevor", ""], ["Yu", "Fisher", ""]]}, {"id": "2101.05913", "submitter": "Basil Mustafa", "authors": "Basil Mustafa, Aaron Loh, Jan Freyberg, Patricia MacWilliams, Megan\n  Wilson, Scott Mayer McKinney, Marcin Sieniek, Jim Winkens, Yuan Liu, Peggy\n  Bui, Shruthi Prabhakara, Umesh Telang, Alan Karthikesalingam, Neil Houlsby\n  and Vivek Natarajan", "title": "Supervised Transfer Learning at Scale for Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is a standard technique to improve performance on tasks\nwith limited data. However, for medical imaging, the value of transfer learning\nis less clear. This is likely due to the large domain mismatch between the\nusual natural-image pre-training (e.g. ImageNet) and medical images. However,\nrecent advances in transfer learning have shown substantial improvements from\nscale. We investigate whether modern methods can change the fortune of transfer\nlearning for medical imaging. For this, we study the class of large-scale\npre-trained networks presented by Kolesnikov et al. on three diverse imaging\ntasks: chest radiography, mammography, and dermatology. We study both transfer\nperformance and critical properties for the deployment in the medical domain,\nincluding: out-of-distribution generalization, data-efficiency, sub-group\nfairness, and uncertainty estimation. Interestingly, we find that for some of\nthese properties transfer from natural to medical images is indeed extremely\neffective, but only when performed at sufficient scale.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 23:55:49 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 13:18:00 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 18:07:21 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Mustafa", "Basil", ""], ["Loh", "Aaron", ""], ["Freyberg", "Jan", ""], ["MacWilliams", "Patricia", ""], ["Wilson", "Megan", ""], ["McKinney", "Scott Mayer", ""], ["Sieniek", "Marcin", ""], ["Winkens", "Jim", ""], ["Liu", "Yuan", ""], ["Bui", "Peggy", ""], ["Prabhakara", "Shruthi", ""], ["Telang", "Umesh", ""], ["Karthikesalingam", "Alan", ""], ["Houlsby", "Neil", ""], ["Natarajan", "Vivek", ""]]}, {"id": "2101.05922", "submitter": "Xiangyuan Zhu", "authors": "Xiangyuan Zhu, Xiaoming Xiao, Tardi Tjahjadi, Zhihu Wu, Jin Tang", "title": "Image Enhancement using Fuzzy Intensity Measure and Adaptive Clipping\n  Histogram Equalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image enhancement aims at processing an input image so that the visual\ncontent of the output image is more pleasing or more useful for certain\napplications. Although histogram equalization is widely used in image\nenhancement due to its simplicity and effectiveness, it changes the mean\nbrightness of the enhanced image and introduces a high level of noise and\ndistortion. To address these problems, this paper proposes image enhancement\nusing fuzzy intensity measure and adaptive clipping histogram equalization\n(FIMHE). FIMHE uses fuzzy intensity measure to first segment the histogram of\nthe original image, and then clip the histogram adaptively in order to prevent\nexcessive image enhancement. Experiments on the Berkeley database and\nCVF-UGR-Image database show that FIMHE outperforms state-of-the-art histogram\nequalization based methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 00:59:55 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Zhu", "Xiangyuan", ""], ["Xiao", "Xiaoming", ""], ["Tjahjadi", "Tardi", ""], ["Wu", "Zhihu", ""], ["Tang", "Jin", ""]]}, {"id": "2101.05954", "submitter": "Devshree Patel", "authors": "Devshree Patel, Ratnam Parikh, and Yesha Shastri", "title": "Recent Advances in Video Question Answering: A Review of Datasets and\n  Methods", "comments": "18 pages, 5 tables, Video and Image Question Answering Workshop, 25th\n  International Conference on Pattern Recognition", "journal-ref": "Pattern Recognition. ICPR International Workshops and Challenges.\n  ICPR 2021. Lecture Notes in Computer Science, vol 12662. Springer", "doi": "10.1007/978-3-030-68790-8_27", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video Question Answering (VQA) is a recent emerging challenging task in the\nfield of Computer Vision. Several visual information retrieval techniques like\nVideo Captioning/Description and Video-guided Machine Translation have preceded\nthe task of VQA. VQA helps to retrieve temporal and spatial information from\nthe video scenes and interpret it. In this survey, we review a number of\nmethods and datasets for the task of VQA. To the best of our knowledge, no\nprevious survey has been conducted for the VQA task.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 03:26:24 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 14:30:16 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Patel", "Devshree", ""], ["Parikh", "Ratnam", ""], ["Shastri", "Yesha", ""]]}, {"id": "2101.05992", "submitter": "Enzo Tartaglione", "authors": "Umberto A. Gava, Federico D'Agata, Enzo Tartaglione, Marco Grangetto,\n  Francesca Bertolino, Ambra Santonocito, Edwin Bennink, Mauro Bergui", "title": "Neural Network-derived perfusion maps: a Model-free approach to computed\n  tomography perfusion in patients with acute ischemic stroke", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: In this study we investigate whether a Convolutional Neural Network\n(CNN) can generate clinically relevant parametric maps from CT perfusion data\nin a clinical setting of patients with acute ischemic stroke. Methods: Training\nof the CNN was done on a subset of 100 perfusion data, while 15 samples were\nused as validation. All the data used for the training/validation of the\nnetwork and to generate ground truth (GT) maps, using a state-of-the-art\ndeconvolution-algorithm, were previously pre-processed using a standard\npipeline. Validation was carried out through manual segmentation of infarct\ncore and penumbra on both CNN-derived maps and GT maps. Concordance among\nsegmented lesions was assessed using the Dice and the Pearson correlation\ncoefficients across lesion volumes. Results: Mean Dice scores from two\ndifferent raters and the GT maps were > 0.70 (good-matching). Inter-rater\nconcordance was also high and strong correlation was found between lesion\nvolumes of CNN maps and GT maps (0.99, 0.98). Conclusion: Our CNN-based\napproach generated clinically relevant perfusion maps that are comparable to\nstate-of-the-art perfusion analysis methods based on deconvolution of the data.\nMoreover, the proposed technique requires less information to estimate the\nischemic core and thus might allow the development of novel perfusion protocols\nwith lower radiation dose.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:11:02 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Gava", "Umberto A.", ""], ["D'Agata", "Federico", ""], ["Tartaglione", "Enzo", ""], ["Grangetto", "Marco", ""], ["Bertolino", "Francesca", ""], ["Santonocito", "Ambra", ""], ["Bennink", "Edwin", ""], ["Bergui", "Mauro", ""]]}, {"id": "2101.05995", "submitter": "Rui Tian", "authors": "Rui Tian, Yunzhou Zhang, Delong Zhu, Shiwen Liang, Sonya Coleman,\n  Dermot Kerr", "title": "Accurate and Robust Scale Recovery for Monocular Visual Odometry Based\n  on Plane Geometry", "comments": "Submitting to IEEE International Conference on Robotics and\n  Automation 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scale ambiguity is a fundamental problem in monocular visual odometry.\nTypical solutions include loop closure detection and environment information\nmining. For applications like self-driving cars, loop closure is not always\navailable, hence mining prior knowledge from the environment becomes a more\npromising approach. In this paper, with the assumption of a constant height of\nthe camera above the ground, we develop a light-weight scale recovery framework\nleveraging an accurate and robust estimation of the ground plane. The framework\nincludes a ground point extraction algorithm for selecting high-quality points\non the ground plane, and a ground point aggregation algorithm for joining the\nextracted ground points in a local sliding window. Based on the aggregated\ndata, the scale is finally recovered by solving a least-squares problem using a\nRANSAC-based optimizer. Sufficient data and robust optimizer enable a highly\naccurate scale recovery. Experiments on the KITTI dataset show that the\nproposed framework can achieve state-of-the-art accuracy in terms of\ntranslation errors, while maintaining competitive performance on the rotation\nerror. Due to the light-weight design, our framework also demonstrates a high\nfrequency of 20Hz on the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:21:24 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 01:21:20 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tian", "Rui", ""], ["Zhang", "Yunzhou", ""], ["Zhu", "Delong", ""], ["Liang", "Shiwen", ""], ["Coleman", "Sonya", ""], ["Kerr", "Dermot", ""]]}, {"id": "2101.05996", "submitter": "Mengyu Chen", "authors": "Mengyu Chen", "title": "Convolutional Neural Network with Pruning Method for Handwritten Digit\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNN model is a popular method for imagery analysis, so it could be utilized\nto recognize handwritten digits based on MNIST datasets. For higher recognition\naccuracy, various CNN models with different fully connected layer sizes are\nexploited to figure out the relationship between the CNN fully connected layer\nsize and the recognition accuracy. Inspired by previous pruning work, we\nperformed pruning methods of distinctiveness on CNN models and compared the\npruning performance with NN models. For better pruning performances on CNN, the\neffect of angle threshold on the pruning performance was explored. The\nevaluation results show that: for the fully connected layer size, there is a\nthreshold, so that when the layer size increases, the recognition accuracy\ngrows if the layer size smaller than the threshold, and falls if the layer size\nlarger than the threshold; the performance of pruning performed on CNN is worse\nthan on NN; as pruning angle threshold increases, the fully connected layer\nsize and the recognition accuracy decreases. This paper also shows that for CNN\nmodels trained by the MNIST dataset, they are capable of handwritten digit\nrecognition and achieve the highest recognition accuracy with fully connected\nlayer size 400. In addition, for same dataset MNIST, CNN models work better\nthan big, deep, simple NN models in a published paper.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:25:13 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Mengyu", ""]]}, {"id": "2101.06013", "submitter": "Violetta Shevchenko", "authors": "Violetta Shevchenko, Damien Teney, Anthony Dick, Anton van den Hengel", "title": "Reasoning over Vision and Language: Exploring the Benefits of\n  Supplemental Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limits of applicability of vision-and-language models are defined by the\ncoverage of their training data. Tasks like vision question answering (VQA)\noften require commonsense and factual information beyond what can be learned\nfrom task-specific datasets. This paper investigates the injection of knowledge\nfrom general-purpose knowledge bases (KBs) into vision-and-language\ntransformers. We use an auxiliary training objective that encourages the\nlearned representations to align with graph embeddings of matching entities in\na KB. We empirically study the relevance of various KBs to multiple tasks and\nbenchmarks. The technique brings clear benefits to knowledge-demanding question\nanswering tasks (OK-VQA, FVQA) by capturing semantic and relational knowledge\nabsent from existing models. More surprisingly, the technique also benefits\nvisual reasoning tasks (NLVR2, SNLI-VE). We perform probing experiments and\nshow that the injection of additional knowledge regularizes the space of\nembeddings, which improves the representation of lexical and semantic\nsimilarities. The technique is model-agnostic and can expand the applicability\nof any vision-and-language transformer with minimal computational overhead.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 08:37:55 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Shevchenko", "Violetta", ""], ["Teney", "Damien", ""], ["Dick", "Anthony", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2101.06021", "submitter": "Pei Wang", "authors": "Pei Wang, Wei Sun, Qingsen Yan, Axi Niu, Rui Li, Yu Zhu, Jinqiu Sun,\n  Yanning Zhang", "title": "Non-uniform Motion Deblurring with Blurry Component Divided Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring is a fundamental and challenging computer vision\nproblem, which aims to recover both the blur kernel and the latent sharp image\nfrom only a blurry observation. Despite the superiority of deep learning\nmethods in image deblurring have displayed, there still exists major challenge\nwith various non-uniform motion blur. Previous methods simply take all the\nimage features as the input to the decoder, which handles different degrees\n(e.g. large blur, small blur) simultaneously, leading to challenges for sharp\nimage generation. To tackle the above problems, we present a deep two-branch\nnetwork to deal with blurry images via a component divided module, which\ndivides an image into two components based on the representation of blurry\ndegree. Specifically, two component attentive blocks are employed to learn\nattention maps to exploit useful deblurring feature representations on both\nlarge and small blurry regions. Then, the blur-aware features are fed into\ntwo-branch reconstruction decoders respectively. In addition, a new feature\nfusion mechanism, orientation-based feature fusion, is proposed to merge sharp\nfeatures of the two branches. Both qualitative and quantitative experimental\nresults show that our method performs favorably against the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 09:10:35 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Wang", "Pei", ""], ["Sun", "Wei", ""], ["Yan", "Qingsen", ""], ["Niu", "Axi", ""], ["Li", "Rui", ""], ["Zhu", "Yu", ""], ["Sun", "Jinqiu", ""], ["Zhang", "Yanning", ""]]}, {"id": "2101.06022", "submitter": "Junshen Kevin Chen", "authors": "Junshen Kevin Chen, Wanze Xie, Yutong He", "title": "Motion-Based Handwriting Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We attempt to overcome the restriction of requiring a writing surface for\nhandwriting recognition. In this study, we design a prototype of a stylus\nequipped with motion sensor, and utilizes gyroscopic and acceleration sensor\nreading to perform written letter classification using various deep learning\ntechniques such as CNN and RNNs. We also explore various data augmentation\ntechniques and their effects, reaching up to 86% accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 09:14:10 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Junshen Kevin", ""], ["Xie", "Wanze", ""], ["He", "Yutong", ""]]}, {"id": "2101.06025", "submitter": "Junshen Kevin Chen", "authors": "Junshen Kevin Chen, Wanze Xie, Yutong He", "title": "Motion-Based Handwriting Recognition and Word Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this project, we leverage a trained single-letter classifier to predict\nthe written word from a continuously written word sequence, by designing a word\nreconstruction pipeline consisting of a dynamic-programming algorithm and an\nauto-correction model. We conduct experiments to optimize models in this\npipeline, then employ domain adaptation to explore using this pipeline on\nunseen data distributions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 09:24:04 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Junshen Kevin", ""], ["Xie", "Wanze", ""], ["He", "Yutong", ""]]}, {"id": "2101.06040", "submitter": "Evangelos Mazomenos", "authors": "Patrick Brandao, Odysseas Zisimopoulos, Evangelos Mazomenos, Gastone\n  Ciuti, Jorge Bernal, Marco Visentini-Scarzanella, Arianna Menciassi, Paolo\n  Dario, Anastasios Koulaouzidis, Alberto Arezzo, David J Hawkes, Danail\n  Stoyanov", "title": "Towards a Computed-Aided Diagnosis System in Colonoscopy: Automatic\n  Polyp Segmentation Using Convolution Neural Networks", "comments": "10 pages, 6 figures", "journal-ref": "Journal of Medical Robotics Research, Volume 03, No. 02, 1840002\n  (2018) G", "doi": "10.1142/S2424905X18400020", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Early diagnosis is essential for the successful treatment of bowel cancers\nincluding colorectal cancer (CRC) and capsule endoscopic imaging with robotic\nactuation can be a valuable diagnostic tool when combined with automated image\nanalysis. We present a deep learning rooted detection and segmentation\nframework for recognizing lesions in colonoscopy and capsule endoscopy images.\nWe restructure established convolution architectures, such as VGG and ResNets,\nby converting them into fully-connected convolution networks (FCNs), fine-tune\nthem and study their capabilities for polyp segmentation and detection. We\nadditionally use Shape from-Shading (SfS) to recover depth and provide a richer\nrepresentation of the tissue's structure in colonoscopy images. Depth is\nincorporated into our network models as an additional input channel to the RGB\ninformation and we demonstrate that the resulting network yields improved\nperformance. Our networks are tested on publicly available datasets and the\nmost accurate segmentation model achieved a mean segmentation IU of 47.78% and\n56.95% on the ETIS-Larib and CVC-Colon datasets, respectively. For polyp\ndetection, the top performing models we propose surpass the current state of\nthe art with detection recalls superior to 90% for all datasets tested. To our\nknowledge, we present the first work to use FCNs for polyp segmentation in\naddition to proposing a novel combination of SfS and RGB that boosts\nperformance\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:08:53 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Brandao", "Patrick", ""], ["Zisimopoulos", "Odysseas", ""], ["Mazomenos", "Evangelos", ""], ["Ciuti", "Gastone", ""], ["Bernal", "Jorge", ""], ["Visentini-Scarzanella", "Marco", ""], ["Menciassi", "Arianna", ""], ["Dario", "Paolo", ""], ["Koulaouzidis", "Anastasios", ""], ["Arezzo", "Alberto", ""], ["Hawkes", "David J", ""], ["Stoyanov", "Danail", ""]]}, {"id": "2101.06046", "submitter": "Axel Sauer", "authors": "Axel Sauer, Andreas Geiger", "title": "Counterfactual Generative Networks", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are prone to learning shortcuts -- they often model simple\ncorrelations, ignoring more complex ones that potentially generalize better.\nPrior works on image classification show that instead of learning a connection\nto object shape, deep classifiers tend to exploit spurious correlations with\nlow-level texture or the background for solving the classification task. In\nthis work, we take a step towards more robust and interpretable classifiers\nthat explicitly expose the task's causal structure. Building on current\nadvances in deep generative modeling, we propose to decompose the image\ngeneration process into independent causal mechanisms that we train without\ndirect supervision. By exploiting appropriate inductive biases, these\nmechanisms disentangle object shape, object texture, and background; hence,\nthey allow for generating counterfactual images. We demonstrate the ability of\nour model to generate such images on MNIST and ImageNet. Further, we show that\nthe counterfactual images can improve out-of-distribution robustness with a\nmarginal drop in performance on the original classification task, despite being\nsynthetic. Lastly, our generative model can be trained efficiently on a single\nGPU, exploiting common pre-trained models as inductive biases.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:23:12 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Sauer", "Axel", ""], ["Geiger", "Andreas", ""]]}, {"id": "2101.06069", "submitter": "Gaurav Kumar Nayak", "authors": "Gaurav Kumar Nayak, Konda Reddy Mopuri, Saksham Jain, Anirban\n  Chakraborty", "title": "Mining Data Impressions from Deep Models as Substitute for the\n  Unavailable Training Data", "comments": "PAMI Submission (Under Review). arXiv admin note: text overlap with\n  arXiv:1905.08114", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:37:29 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 13:41:36 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Nayak", "Gaurav Kumar", ""], ["Mopuri", "Konda Reddy", ""], ["Jain", "Saksham", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "2101.06072", "submitter": "Evlampios Apostolidis", "authors": "Evlampios Apostolidis, Eleni Adamantidou, Alexandros I. Metsai,\n  Vasileios Mezaris, Ioannis Patras", "title": "Video Summarization Using Deep Neural Networks: A Survey", "comments": "Journal paper; Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization technologies aim to create a concise and complete\nsynopsis by selecting the most informative parts of the video content. Several\napproaches have been developed over the last couple of decades and the current\nstate of the art is represented by methods that rely on modern deep neural\nnetwork architectures. This work focuses on the recent advances in the area and\nprovides a comprehensive survey of the existing deep-learning-based methods for\ngeneric video summarization. After presenting the motivation behind the\ndevelopment of technologies for video summarization, we formulate the video\nsummarization task and discuss the main characteristics of a typical\ndeep-learning-based analysis pipeline. Then, we suggest a taxonomy of the\nexisting algorithms and provide a systematic review of the relevant literature\nthat shows the evolution of the deep-learning-based video summarization\ntechnologies and leads to suggestions for future developments. We then report\non protocols for the objective evaluation of video summarization algorithms and\nwe compare the performance of several deep-learning-based approaches. Based on\nthe outcomes of these comparisons, as well as some documented considerations\nabout the suitability of evaluation protocols, we indicate potential future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:41:29 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Apostolidis", "Evlampios", ""], ["Adamantidou", "Eleni", ""], ["Metsai", "Alexandros I.", ""], ["Mezaris", "Vasileios", ""], ["Patras", "Ioannis", ""]]}, {"id": "2101.06073", "submitter": "Chuan Liu", "authors": "Chuan Liu, Yi Gao, Jiancheng Lv", "title": "Dynamic Normalization", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization has become one of the essential components in CNN. It\nallows the network to use a higher learning rate and speed up training. And the\nnetwork doesn't need to be initialized carefully. However, in our work, we find\nthat a simple extension of BN can increase the performance of the network.\nFirst, we extend BN to adaptively generate scale and shift parameters for each\nmini-batch data, called DN-C (Batch-shared and Channel-wise). We use the\nstatistical characteristics of mini-batch data ($E[X],\nStd[X]\\in\\mathbb{R}^{c}$) as the input of SC module. Then we extend BN to\nadaptively generate scale and shift parameters for each channel of each sample,\ncalled DN-B (Batch and Channel-wise). Our experiments show that DN-C model\ncan't train normally, but DN-B model has very good robustness. In\nclassification task, DN-B can improve the accuracy of the MobileNetV2 on\nImageNet-100 more than 2% with only 0.6% additional Mult-Adds. In detection\ntask, DN-B can improve the accuracy of the SSDLite on MS-COCO nearly 4% mAP\nwith the same settings. Compared with BN, DN-B has stable performance when\nusing higher learning rate or smaller batch size.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:41:41 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Liu", "Chuan", ""], ["Gao", "Yi", ""], ["Lv", "Jiancheng", ""]]}, {"id": "2101.06085", "submitter": "Yuanduo Hong", "authors": "Yuanduo Hong, Huihui Pan, Weichao Sun, Senior Member, IEEE, Yisong Jia", "title": "Deep Dual-resolution Networks for Real-time and Accurate Semantic\n  Segmentation of Road Scenes", "comments": "12 pages, 6 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a critical technology for autonomous vehicles to\nunderstand surrounding scenes. For practical autonomous vehicles, it is\nundesirable to spend a considerable amount of inference time to achieve\nhigh-accuracy segmentation results. Using light-weight architectures\n(encoder-decoder or two-pathway) or reasoning on low-resolution images, recent\nmethods realize very fast scene parsing which even run at more than 100 FPS on\nsingle 1080Ti GPU. However, there are still evident gaps in performance between\nthese real-time methods and models based on dilation backbones. To tackle this\nproblem, we propose novel deep dual-resolution networks (DDRNets) for real-time\nsemantic segmentation of road scenes. Besides, we design a new contextual\ninformation extractor named Deep Aggregation Pyramid Pooling Module (DAPPM) to\nenlarge effective receptive fields and fuse multi-scale context. Our method\nachieves new state-of-the-art trade-off between accuracy and speed on both\nCityscapes and CamVid dataset. Specially, on single 2080Ti GPU, DDRNet-23-slim\nyields 77.4% mIoU at 109 FPS on Cityscapes test set and 74.4% mIoU at 230 FPS\non CamVid test set. Without utilizing attention mechanism, pre-training on\nlarger semantic segmentation dataset or inference acceleration, DDRNet-39\nattains 80.4% test mIoU at 23 FPS on Cityscapes. With widely used test\naugmentation, our method is still superior to most state-of-the-art models,\nrequiring much less computation. Codes and trained models will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 12:56:18 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Hong", "Yuanduo", ""], ["Pan", "Huihui", ""], ["Sun", "Weichao", ""], ["Member", "Senior", ""], ["IEEE", "", ""], ["Jia", "Yisong", ""]]}, {"id": "2101.06092", "submitter": "K Naveen Kumar", "authors": "K Naveen Kumar, C Vishnu, Reshmi Mitra, C Krishna Mohan", "title": "Black-box Adversarial Attacks in Autonomous Vehicle Technology", "comments": "7 pages, 10 figures, published in 49th Annual IEEE AIPR 2020: Trusted\n  Computing, Privacy, and Securing Multimedia Washington, D.C. October 13-15,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the high quality performance of the deep neural network in real-world\napplications, they are susceptible to minor perturbations of adversarial\nattacks. This is mostly undetectable to human vision. The impact of such\nattacks has become extremely detrimental in autonomous vehicles with real-time\n\"safety\" concerns. The black-box adversarial attacks cause drastic\nmisclassification in critical scene elements such as road signs and traffic\nlights leading the autonomous vehicle to crash into other vehicles or\npedestrians. In this paper, we propose a novel query-based attack method called\nModified Simple black-box attack (M-SimBA) to overcome the use of a white-box\nsource in transfer based attack method. Also, the issue of late convergence in\na Simple black-box attack (SimBA) is addressed by minimizing the loss of the\nmost confused class which is the incorrect class predicted by the model with\nthe highest probability, instead of trying to maximize the loss of the correct\nclass. We evaluate the performance of the proposed approach to the German\nTraffic Sign Recognition Benchmark (GTSRB) dataset. We show that the proposed\nmodel outperforms the existing models like Transfer-based projected gradient\ndescent (T-PGD), SimBA in terms of convergence time, flattening the\ndistribution of confused class probability, and producing adversarial samples\nwith least confidence on the true class.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 13:18:18 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Kumar", "K Naveen", ""], ["Vishnu", "C", ""], ["Mitra", "Reshmi", ""], ["Mohan", "C Krishna", ""]]}, {"id": "2101.06116", "submitter": "Muhammad Ahmad", "authors": "Sidrah Shabbir and Muhammad Ahmad", "title": "Hyperspectral Image Classification -- Traditional to Deep Models: A\n  Survey for Future Prospects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral Imaging (HSI) has been extensively utilized in many real-life\napplications because it benefits from the detailed spectral information\ncontained in each pixel. Notably, the complex characteristics i.e., the\nnonlinear relation among the captured spectral information and the\ncorresponding object of HSI data make accurate classification challenging for\ntraditional methods. In the last few years, deep learning (DL) has been\nsubstantiated as a powerful feature extractor that effectively addresses the\nnonlinear problems that appeared in a number of computer vision tasks. This\nprompts the deployment of DL for HSI classification (HSIC) which revealed good\nperformance. This survey enlists a systematic overview of DL for HSIC and\ncompared state-of-the-art strategies of the said topic. Primarily, we will\nencapsulate the main challenges of traditional machine learning for HSIC and\nthen we will acquaint the superiority of DL to address these problems. This\nsurvey breakdown the state-of-the-art DL frameworks into spectral-features,\nspatial-features, and together spatial-spectral features to systematically\nanalyze the achievements (future directions as well) of these frameworks for\nHSIC. Moreover, we will consider the fact that DL requires a large number of\nlabeled training examples whereas acquiring such a number for HSIC is\nchallenging in terms of time and cost. Therefore, this survey discusses some\nstrategies to improve the generalization performance of DL strategies which can\nprovide some future guidelines.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 13:59:22 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Shabbir", "Sidrah", ""], ["Ahmad", "Muhammad", ""]]}, {"id": "2101.06159", "submitter": "David Fernandez Llorca", "authors": "David Fern\\'andez Llorca, Antonio Hern\\'andez Mart\\'inez, Iv\\'an\n  Garc\\'ia Daza", "title": "Vision-based Vehicle Speed Estimation: A Survey", "comments": "Manuscript published in the IET Intelligent Transport Systems journal", "journal-ref": "IET Intelligent Transport Systems 2021", "doi": "10.1049/itr2.12079", "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to accurately estimate the speed of road vehicles is becoming\nincreasingly important for at least two main reasons. First, the number of\nspeed cameras installed worldwide has been growing in recent years, as the\nintroduction and enforcement of appropriate speed limits is considered one of\nthe most effective means to increase the road safety. Second, traffic\nmonitoring and forecasting in road networks plays a fundamental role to enhance\ntraffic, emissions and energy consumption in smart cities, being the speed of\nthe vehicles one of the most relevant parameters of the traffic state. Among\nthe technologies available for the accurate detection of vehicle speed, the use\nof vision-based systems brings great challenges to be solved, but also great\npotential advantages, such as the drastic reduction of costs due to the absence\nof expensive range sensors, and the possibility of identifying vehicles\naccurately. This paper provides a review of vision-based vehicle speed\nestimation. We describe the terminology, the application domains, and propose a\ncomplete taxonomy of a large selection of works that categorizes all stages\ninvolved. An overview of performance evaluation metrics and available datasets\nis provided. Finally, we discuss current limitations and future directions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:07:54 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 08:45:27 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Llorca", "David Fern\u00e1ndez", ""], ["Mart\u00ednez", "Antonio Hern\u00e1ndez", ""], ["Daza", "Iv\u00e1n Garc\u00eda", ""]]}, {"id": "2101.06162", "submitter": "Ghada Sokar", "authors": "Ghada Sokar, Decebal Constantin Mocanu, Mykola Pechenizkiy", "title": "Learning Invariant Representation for Continual Learning", "comments": "Accepted at the AAAI Meta-Learning for Computer Vision Workshop\n  (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to provide intelligent agents that are capable of\nlearning continually a sequence of tasks, building on previously learned\nknowledge. A key challenge in this learning paradigm is catastrophically\nforgetting previously learned tasks when the agent faces a new one. Current\nrehearsal-based methods show their success in mitigating the catastrophic\nforgetting problem by replaying samples from previous tasks during learning a\nnew one. However, these methods are infeasible when the data of previous tasks\nis not accessible. In this work, we propose a new pseudo-rehearsal-based\nmethod, named learning Invariant Representation for Continual Learning (IRCL),\nin which class-invariant representation is disentangled from a conditional\ngenerative model and jointly used with class-specific representation to learn\nthe sequential tasks. Disentangling the shared invariant representation helps\nto learn continually a sequence of tasks, while being more robust to forgetting\nand having better knowledge transfer. We focus on class incremental learning\nwhere there is no knowledge about task identity during inference. We\nempirically evaluate our proposed method on two well-known benchmarks for\ncontinual learning: split MNIST and split Fashion MNIST. The experimental\nresults show that our proposed method outperforms regularization-based methods\nby a big margin and is better than the state-of-the-art pseudo-rehearsal-based\nmethod. Finally, we analyze the role of the shared invariant representation in\nmitigating the forgetting problem especially when the number of replayed\nsamples for each previous task is small.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:12:51 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Sokar", "Ghada", ""], ["Mocanu", "Decebal Constantin", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2101.06175", "submitter": "Yi Liu", "authors": "Yi Liu, Lutao Chu, Guowei Chen, Zewu Wu, Zeyu Chen, Baohua Lai, Yuying\n  Hao", "title": "PaddleSeg: A High-Efficient Development Toolkit for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Segmentation plays an essential role in computer vision and image\nprocessing with various applications from medical diagnosis to autonomous car\ndriving. A lot of segmentation algorithms have been proposed for addressing\nspecific problems. In recent years, the success of deep learning techniques has\ntremendously influenced a wide range of computer vision areas, and the modern\napproaches of image segmentation based on deep learning are becoming prevalent.\nIn this article, we introduce a high-efficient development toolkit for image\nsegmentation, named PaddleSeg. The toolkit aims to help both developers and\nresearchers in the whole process of designing segmentation models, training\nmodels, optimizing performance and inference speed, and deploying models.\nCurrently, PaddleSeg supports around 20 popular segmentation models and more\nthan 50 pre-trained models from real-time and high-accuracy levels. With\nmodular components and backbone networks, users can easily build over one\nhundred models for different requirements. Furthermore, we provide\ncomprehensive benchmarks and evaluations to show that these segmentation\nalgorithms trained on our toolkit have more competitive accuracy. Also, we\nprovide various real industrial applications and practical cases based on\nPaddleSeg. All codes and examples of PaddleSeg are available at\nhttps://github.com/PaddlePaddle/PaddleSeg.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:36:22 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Liu", "Yi", ""], ["Chu", "Lutao", ""], ["Chen", "Guowei", ""], ["Wu", "Zewu", ""], ["Chen", "Zeyu", ""], ["Lai", "Baohua", ""], ["Hao", "Yuying", ""]]}, {"id": "2101.06184", "submitter": "Toby Perrett", "authors": "Toby Perrett and Alessandro Masullo and Tilo Burghardt and Majid\n  Mirmehdi and Dima Damen", "title": "Temporal-Relational CrossTransformers for Few-Shot Action Recognition", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach to few-shot action recognition, finding\ntemporally-corresponding frame tuples between the query and videos in the\nsupport set. Distinct from previous few-shot works, we construct class\nprototypes using the CrossTransformer attention mechanism to observe relevant\nsub-sequences of all support videos, rather than using class averages or single\nbest matches. Video representations are formed from ordered tuples of varying\nnumbers of frames, which allows sub-sequences of actions at different speeds\nand temporal offsets to be compared.\n  Our proposed Temporal-Relational CrossTransformers (TRX) achieve\nstate-of-the-art results on few-shot splits of Kinetics, Something-Something V2\n(SSv2), HMDB51 and UCF101. Importantly, our method outperforms prior work on\nSSv2 by a wide margin (12%) due to the its ability to model temporal relations.\nA detailed ablation showcases the importance of matching to multiple support\nset videos and learning higher-order relational CrossTransformers.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:47:35 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 15:02:00 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 14:19:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Perrett", "Toby", ""], ["Masullo", "Alessandro", ""], ["Burghardt", "Tilo", ""], ["Mirmehdi", "Majid", ""], ["Damen", "Dima", ""]]}, {"id": "2101.06189", "submitter": "Samuel Yen-Chi Chen", "authors": "Samuel Yen-Chi Chen, Tzu-Chieh Wei, Chao Zhang, Haiwang Yu, Shinjae\n  Yoo", "title": "Hybrid Quantum-Classical Graph Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV hep-ex physics.data-an quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high energy physics (HEP) community has a long history of dealing with\nlarge-scale datasets. To manage such voluminous data, classical machine\nlearning and deep learning techniques have been employed to accelerate physics\ndiscovery. Recent advances in quantum machine learning (QML) have indicated the\npotential of applying these techniques in HEP. However, there are only limited\nresults in QML applications currently available. In particular, the challenge\nof processing sparse data, common in HEP datasets, has not been extensively\nstudied in QML models. This research provides a hybrid quantum-classical graph\nconvolutional network (QGCNN) for learning HEP data. The proposed framework\ndemonstrates an advantage over classical multilayer perceptron and\nconvolutional neural networks in the aspect of number of parameters. Moreover,\nin terms of testing accuracy, the QGCNN shows comparable performance to a\nquantum convolutional neural network on the same HEP dataset while requiring\nless than $50\\%$ of the parameters. Based on numerical simulation results,\nstudying the application of graph convolutional operations and other QML models\nmay prove promising in advancing HEP research and other scientific fields.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:02:52 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Samuel Yen-Chi", ""], ["Wei", "Tzu-Chieh", ""], ["Zhang", "Chao", ""], ["Yu", "Haiwang", ""], ["Yoo", "Shinjae", ""]]}, {"id": "2101.06217", "submitter": "Prajwal Singh", "authors": "Aalok Gangopadhyay, Prajwal Singh, Shanmuganathan Raman", "title": "APEX-Net: Automatic Plot Extractor Network", "comments": "https://sites.google.com/view/apexnetpaper/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic extraction of raw data from 2D line plot images is a problem of\ngreat importance having many real-world applications. Several algorithms have\nbeen proposed for solving this problem. However, these algorithms involve a\nsignificant amount of human intervention. To minimize this intervention, we\npropose APEX-Net, a deep learning based framework with novel loss functions for\nsolving the plot extraction problem. We introduce APEX-1M, a new large scale\ndataset which contains both the plot images and the raw data. We demonstrate\nthe performance of APEX-Net on the APEX-1M test set and show that it obtains\nimpressive accuracy. We also show visual results of our network on unseen plot\nimages and demonstrate that it extracts the shape of the plots to a great\nextent. Finally, we develop a GUI based software for plot extraction that can\nbenefit the community at large. For dataset and more information visit\nhttps://sites.google.com/view/apexnetpaper/.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:02:36 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 16:17:26 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 05:03:01 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Gangopadhyay", "Aalok", ""], ["Singh", "Prajwal", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2101.06224", "submitter": "Farshad Barahimi", "authors": "Farshad Barahimi and Fernando Paulovich", "title": "Multi-point dimensionality reduction to improve projection layout\n  reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ordinary Dimensionality Reduction (DR), each data instance in an\nm-dimensional space (original space) is mapped to one point in a d-dimensional\nspace (visual space), preserving as much as possible distances and/or\nneighborhood relationships. Despite their popularity, even for simple datasets,\nthe existing DR techniques unavoidably may produce misleading visual\nrepresentations. The problem is not with the existing solutions but with\nproblem formulation. For two dimensional visual space, if data instances are\nnot co-planar or do not lie on a 2D manifold, there is no solution for the\nproblem, and the possible approximations usually result in layouts with\ninaccuracies in the distance preservation and overlapped neighborhoods. In this\npaper, we elaborate on the concept of Multi-point Dimensionality Reduction\nwhere each data instance can be mapped to possibly more than one point in the\nvisual space by providing the first general solution to it as a step toward\nmitigating this issue. By duplicating points, background information is added\nto the visual representation making local neighborhoods in the visual space\nmore faithful to the original space. Our solution, named Red Gray Plus, is\nbuilt upon and extends a combination of ordinary DR and graph drawing\ntechniques. We show that not only Multi-point Dimensionality Reduction can be\none of the potential directions to improve DR layouts' reliability but also\nthat our initial solution to the problem outperforms popular ordinary DR\nmethods quantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:17:02 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 20:30:37 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Barahimi", "Farshad", ""], ["Paulovich", "Fernando", ""]]}, {"id": "2101.06228", "submitter": "Ronglin Gong", "authors": "Ronglin Gong, Zhiyang Lu and Jun Shi", "title": "Task-driven Self-supervised Bi-channel Networks Learning for Diagnosis\n  of Breast Cancers with Mammography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning can promote the mammography-based computer-aided diagnosis\n(CAD) for breast cancers, but it generally suffers from the small size sample\nproblem. In this work, a task-driven self-supervised bi-channel networks\n(TSBNL) framework is proposed to improve the performance of classification\nnetwork with limited mammograms. In particular, a new gray-scale image mapping\n(GSIM) task for image restoration is designed as the pretext task to improve\ndiscriminate feature representation with label information of mammograms. The\nTSBNL then innovatively integrates this image restoration network and the\ndownstream classification network into a unified SSL framework, and transfers\nthe knowledge from the pretext network to the classification network with\nimproved diagnostic accuracy. The proposed algorithm is evaluated on a public\nINbreast mammogram dataset. The experimental results indicate that it\noutperforms the conventional SSL algorithms for diagnosis of breast cancers\nwith limited samples.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:28:52 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Gong", "Ronglin", ""], ["Lu", "Zhiyang", ""], ["Shi", "Jun", ""]]}, {"id": "2101.06241", "submitter": "Sajjad Amrollahi Biyouki", "authors": "Sajjad Amrollahi Biyouki, Hoon Hwangbo", "title": "Blind Image Deblurring based on Kernel Mixture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blind Image deblurring tries to estimate blurriness and a latent image out of\na blurred image. This estimation, as being an ill-posed problem, requires\nimposing restrictions on the latent image or a blur kernel that represents\nblurriness. Different from recent studies that impose some priors on the latent\nimage, this paper regulates the structure of the blur kernel. We propose a\nkernel mixture structure while using the Gaussian kernel as a base kernel. By\ncombining multiple Gaussian kernels structurally enhanced in terms of scales\nand centers, the kernel mixture becomes capable of modeling nearly\nnon-parametric shape of blurriness. A data-driven decision for the number of\nbase kernels to combine makes the structure even more flexible. We apply this\napproach to a remote sensing problem to recover images from blurry images of\nsatellite. This case study shows the superiority of the proposed method\nregulating the blur kernel in comparison with state-of-the-art methods that\nregulates the latent image.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:56:37 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Biyouki", "Sajjad Amrollahi", ""], ["Hwangbo", "Hoon", ""]]}, {"id": "2101.06255", "submitter": "Daniel Moyer", "authors": "Daniel Moyer and Polina Golland", "title": "Harmonization and the Worst Scanner Syndrome", "comments": "Med-NeurIPS 2020 Workshop Paper, updated 4/2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for a wide class of harmonization/domain-invariance schemes\nseveral undesirable properties are unavoidable. If a predictive machine is made\ninvariant to a set of domains, the accuracy of the output predictions (as\nmeasured by mutual information) is limited by the domain with the least amount\nof information to begin with. If a real label value is highly informative about\nthe source domain, it cannot be accurately predicted by an invariant predictor.\nThese results are simple and intuitive, but we believe that it is beneficial to\nstate them for medical imaging harmonization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 18:41:41 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 17:16:42 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Moyer", "Daniel", ""], ["Golland", "Polina", ""]]}, {"id": "2101.06278", "submitter": "Shivangi Aneja Ms", "authors": "Shivangi Aneja, Chris Bregler and Matthias Nie{\\ss}ner", "title": "COSMOS: Catching Out-of-Context Misinformation with Self-Supervised\n  Learning", "comments": "Video : https://youtu.be/riI3Cl2xy10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent attention to DeepFakes, one of the most prevalent ways to\nmislead audiences on social media is the use of unaltered images in a new but\nfalse context. To address these challenges and support fact-checkers, we\npropose a new method that automatically detects out-of-context image and text\npairs. Our key insight is to leverage the grounding of image with text to\ndistinguish out-of-context scenarios that cannot be disambiguated with language\nalone. We propose a self-supervised training strategy where we only need a set\nof captioned images. At train time, our method learns to selectively align\nindividual objects in an image with textual claims, without explicit\nsupervision. At test time, we check if both captions correspond to the same\nobject(s) in the image but are semantically different, which allows us to make\nfairly accurate out-of-context predictions. Our method achieves 85%\nout-of-context detection accuracy. To facilitate benchmarking of this task, we\ncreate a large-scale dataset of 200K images with 450K textual captions from a\nvariety of news websites, blogs, and social media posts. The dataset and source\ncode is publicly available at\nhttps://shivangi-aneja.github.io/projects/cosmos/.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 19:00:42 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 15:52:58 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 18:00:07 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Aneja", "Shivangi", ""], ["Bregler", "Chris", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2101.06310", "submitter": "Daniel Osaku", "authors": "D. Osaku, C. F. Cuba, Celso T.N. Suzuki, J.F. Gomes, A.X. Falc\\~ao", "title": "Automated Diagnosis of Intestinal Parasites: A new hybrid approach and\n  its benefits", "comments": "18 pages, 11 figures", "journal-ref": "Computers in Biology and Medicine, Volume 123, August 2020, 103917", "doi": "10.1016/j.compbiomed.2020.103917", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intestinal parasites are responsible for several diseases in human beings. In\norder to eliminate the error-prone visual analysis of optical microscopy\nslides, we have investigated automated, fast, and low-cost systems for the\ndiagnosis of human intestinal parasites. In this work, we present a hybrid\napproach that combines the opinion of two decision-making systems with\ncomplementary properties: ($DS_1$) a simpler system based on very fast\nhandcrafted image feature extraction and support vector machine classification\nand ($DS_2$) a more complex system based on a deep neural network, Vgg-16, for\nimage feature extraction and classification. $DS_1$ is much faster than $DS_2$,\nbut it is less accurate than $DS_2$. Fortunately, the errors of $DS_1$ are not\nthe same of $DS_2$. During training, we use a validation set to learn the\nprobabilities of misclassification by $DS_1$ on each class based on its\nconfidence values. When $DS_1$ quickly classifies all images from a microscopy\nslide, the method selects a number of images with higher chances of\nmisclassification for characterization and reclassification by $DS_2$. Our\nhybrid system can improve the overall effectiveness without compromising\nefficiency, being suitable for the clinical routine -- a strategy that might be\nsuitable for other real applications. As demonstrated on large datasets, the\nproposed system can achieve, on average, 94.9%, 87.8%, and 92.5% of Cohen's\nKappa on helminth eggs, helminth larvae, and protozoa cysts, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 05:11:01 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Osaku", "D.", ""], ["Cuba", "C. F.", ""], ["Suzuki", "Celso T. N.", ""], ["Gomes", "J. F.", ""], ["Falc\u00e3o", "A. X.", ""]]}, {"id": "2101.06329", "submitter": "Mamshad Nayeem Rizve", "authors": "Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, Mubarak Shah", "title": "In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label\n  Selection Framework for Semi-Supervised Learning", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent research in semi-supervised learning (SSL) is mostly dominated by\nconsistency regularization based methods which achieve strong performance.\nHowever, they heavily rely on domain-specific data augmentations, which are not\neasy to generate for all data modalities. Pseudo-labeling (PL) is a general SSL\napproach that does not have this constraint but performs relatively poorly in\nits original formulation. We argue that PL underperforms due to the erroneous\nhigh confidence predictions from poorly calibrated models; these predictions\ngenerate many incorrect pseudo-labels, leading to noisy training. We propose an\nuncertainty-aware pseudo-label selection (UPS) framework which improves pseudo\nlabeling accuracy by drastically reducing the amount of noise encountered in\nthe training process. Furthermore, UPS generalizes the pseudo-labeling process,\nallowing for the creation of negative pseudo-labels; these negative\npseudo-labels can be used for multi-label classification as well as negative\nlearning to improve the single-label classification. We achieve strong\nperformance when compared to recent SSL methods on the CIFAR-10 and CIFAR-100\ndatasets. Also, we demonstrate the versatility of our method on the video\ndataset UCF-101 and the multi-label dataset Pascal VOC.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:29:57 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 04:42:48 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 17:59:03 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rizve", "Mamshad Nayeem", ""], ["Duarte", "Kevin", ""], ["Rawat", "Yogesh S", ""], ["Shah", "Mubarak", ""]]}, {"id": "2101.06333", "submitter": "Yang Jiao", "authors": "Yang Jiao, Guangming Shi and Trac D. Tran", "title": "Optical Flow Estimation via Motion Feature Recovery", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation with occlusion or large displacement is a problematic\nchallenge due to the lost of corresponding pixels between consecutive frames.\nIn this paper, we discover that the lost information is related to a large\nquantity of motion features (more than 40%) computed from the popular\ndiscriminative cost-volume feature would completely vanish due to invalid\nsampling, leading to the low efficiency of optical flow learning. We call this\nphenomenon the Vanishing Cost Volume Problem. Inspired by the fact that local\nmotion tends to be highly consistent within a short temporal window, we propose\na novel iterative Motion Feature Recovery (MFR) method to address the vanishing\ncost volume via modeling motion consistency across multiple frames. In each MFR\niteration, invalid entries from original motion features are first determined\nbased on the current flow. Then, an efficient network is designed to adaptively\nlearn the motion correlation to recover invalid features for lost-information\nrestoration. The final optical flow is then decoded from the recovered motion\nfeatures. Experimental results on Sintel and KITTI show that our method\nachieves state-of-the-art performances. In fact, MFR currently ranks second on\nSintel public website.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 00:07:29 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Jiao", "Yang", ""], ["Shi", "Guangming", ""], ["Tran", "Trac D.", ""]]}, {"id": "2101.06354", "submitter": "Abhinau Venkataramanan", "authors": "Abhinau K. Venkataramanan and Chengyang Wu and Alan C. Bovik and\n  Ioannis Katsavounidis and Zafar Shahid", "title": "A Hitchhiker's Guide to Structural Similarity", "comments": "Submitted final version to IEEE Access on January 30, 2021", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3056504", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Structural Similarity (SSIM) Index is a very widely used image/video\nquality model that continues to play an important role in the perceptual\nevaluation of compression algorithms, encoding recipes and numerous other\nimage/video processing algorithms. Several public implementations of the SSIM\nand Multiscale-SSIM (MS-SSIM) algorithms have been developed, which differ in\nefficiency and performance. This \"bendable ruler\" makes the process of quality\nassessment of encoding algorithms unreliable. To address this situation, we\nstudied and compared the functions and performances of popular and widely used\nimplementations of SSIM, and we also considered a variety of design choices.\nBased on our studies and experiments, we have arrived at a collection of\nrecommendations on how to use SSIM most effectively, including ways to reduce\nits computational burden.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 02:51:06 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 23:40:28 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Venkataramanan", "Abhinau K.", ""], ["Wu", "Chengyang", ""], ["Bovik", "Alan C.", ""], ["Katsavounidis", "Ioannis", ""], ["Shahid", "Zafar", ""]]}, {"id": "2101.06381", "submitter": "Zhizhong Wang", "authors": "Zhizhong Wang, Lei Zhao, Haibo Chen, Zhiwen Zuo, Ailin Li, Wei Xing,\n  Dongming Lu", "title": "Diversified Patch-based Style Transfer with Shifted Style Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gram-based and patch-based approaches are two important research lines of\nimage style transfer. Recent diversified Gram-based methods have been able to\nproduce multiple and diverse reasonable solutions for the same content and\nstyle inputs. However, as another popular research interest, the diversity of\npatch-based methods remains challenging due to the stereotyped style swapping\nprocess based on nearest patch matching. To resolve this dilemma, in this\npaper, we dive into the core style swapping process of patch-based style\ntransfer and explore possible ways to diversify it. What stands out is an\noperation called shifted style normalization (SSN), the most effective and\nefficient way to empower existing patch-based methods to generate diverse\nresults for arbitrary styles. The key insight is to use an important intuition\nthat neural patches with higher activation values could contribute more to\ndiversity. Theoretical analyses and extensive experiments are conducted to\ndemonstrate the effectiveness of our method, and compared with other possible\noptions and state-of-the-art algorithms, it shows remarkable superiority in\nboth diversity and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 06:34:15 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Zhizhong", ""], ["Zhao", "Lei", ""], ["Chen", "Haibo", ""], ["Zuo", "Zhiwen", ""], ["Li", "Ailin", ""], ["Xing", "Wei", ""], ["Lu", "Dongming", ""]]}, {"id": "2101.06383", "submitter": "Soumendu Chakraborty", "authors": "Soumendu Chakraborty, and Anand Singh Jalal", "title": "A Novel Local Binary Pattern Based Blind Feature Image Steganography", "comments": null, "journal-ref": "Multimedia Tools and Applications, vol-79, no-27-28, pp.\n  19561-19574, 2020", "doi": "10.1007/s11042-020-08828-3", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Steganography methods in general terms tend to embed more and more secret\nbits in the cover images. Most of these methods are designed to embed secret\ninformation in such a way that the change in the visual quality of the\nresulting stego image is not detectable. There exists some methods which\npreserve the global structure of the cover after embedding. However, the\nembedding capacity of these methods is very less. In this paper a novel feature\nbased blind image steganography technique is proposed, which preserves the LBP\n(Local binary pattern) feature of the cover with comparable embedding rates.\nLocal binary pattern is a well known image descriptor used for image\nrepresentation. The proposed scheme computes the local binary pattern to hide\nthe bits of the secret image in such a way that the local relationship that\nexists in the cover are preserved in the resulting stego image. The performance\nof the proposed steganography method has been tested on several images of\ndifferent types to show the robustness. State of the art LSB based\nsteganography methods are compared with the proposed method to show the\neffectiveness of feature based image steganography\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 06:37:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chakraborty", "Soumendu", ""], ["Jalal", "Anand Singh", ""]]}, {"id": "2101.06390", "submitter": "Bohao Huang", "authors": "Bohao Huang, Jichen Yang, Artem Streltsov, Kyle Bradbury, Leslie M.\n  Collins, and Jordan Malof", "title": "GridTracer: Automatic Mapping of Power Grids using Deep Learning and\n  Overhead Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Energy system information valuable for electricity access planning such as\nthe locations and connectivity of electricity transmission and distribution\ntowers, termed the power grid, is often incomplete, outdated, or altogether\nunavailable. Furthermore, conventional means for collecting this information is\ncostly and limited. We propose to automatically map the grid in overhead\nremotely sensed imagery using deep learning. Towards this goal, we develop and\npublicly-release a large dataset ($263km^2$) of overhead imagery with ground\ntruth for the power grid, to our knowledge this is the first dataset of its\nkind in the public domain. Additionally, we propose scoring metrics and\nbaseline algorithms for two grid mapping tasks: (1) tower recognition and (2)\npower line interconnection (i.e., estimating a graph representation of the\ngrid). We hope the availability of the training data, scoring metrics, and\nbaselines will facilitate rapid progress on this important problem to help\ndecision-makers address the energy needs of societies around the world.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 07:23:42 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Huang", "Bohao", ""], ["Yang", "Jichen", ""], ["Streltsov", "Artem", ""], ["Bradbury", "Kyle", ""], ["Collins", "Leslie M.", ""], ["Malof", "Jordan", ""]]}, {"id": "2101.06391", "submitter": "Minxian Li", "authors": "Minxian Li, Xiatian Zhu, Shaogang Gong", "title": "Unsupervised Noisy Tracklet Person Re-identification", "comments": "was submitted to ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing person re-identification (re-id) methods mostly rely on supervised\nmodel learning from a large set of person identity labelled training data per\ndomain. This limits their scalability and usability in large scale deployments.\nIn this work, we present a novel selective tracklet learning (STL) approach\nthat can train discriminative person re-id models from unlabelled tracklet data\nin an unsupervised manner. This avoids the tedious and costly process of\nexhaustively labelling person image/tracklet true matching pairs across camera\nviews. Importantly, our method is particularly more robust against arbitrary\nnoisy data of raw tracklets therefore scalable to learning discriminative\nmodels from unconstrained tracking data. This differs from a handful of\nexisting alternative methods that often assume the existence of true matches\nand balanced tracklet samples per identity class. This is achieved by\nformulating a data adaptive image-to-tracklet selective matching loss function\nexplored in a multi-camera multi-task deep learning model structure. Extensive\ncomparative experiments demonstrate that the proposed STL model surpasses\nsignificantly the state-of-the-art unsupervised learning and one-shot learning\nre-id methods on three large tracklet person re-id benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 07:31:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Li", "Minxian", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "2101.06393", "submitter": "Ashish Kumar", "authors": "Ashish Kumar, James R. McBride, Gaurav Pandey", "title": "Real Time Incremental Foveal Texture Mapping for Autonomous Vehicles", "comments": "8 Pages, 10 Figures, 2 Tables. 2018 IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS). IEEE, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an end-to-end real time framework to generate high resolution\ngraphics grade textured 3D map of urban environment. The generated detailed map\nfinds its application in the precise localization and navigation of autonomous\nvehicles. It can also serve as a virtual test bed for various vision and\nplanning algorithms as well as a background map in the computer games. In this\npaper, we focus on two important issues: (i) incrementally generating a map\nwith coherent 3D surface, in real time and (ii) preserving the quality of color\ntexture. To handle the above issues, firstly, we perform a pose-refinement\nprocedure which leverages camera image information, Delaunay triangulation and\nexisting scan matching techniques to produce high resolution 3D map from the\nsparse input LIDAR scan. This 3D map is then texturized and accumulated by\nusing a novel technique of ray-filtering which handles occlusion and\ninconsistencies in pose-refinement. Further, inspired by human fovea, we\nintroduce foveal-processing which significantly reduces the computation time\nand also assists ray-filtering to maintain consistency in color texture and\ncoherency in 3D surface of the output map. Moreover, we also introduce texture\nerror (TE) and mean texture mapping error (MTME), which provides quantitative\nmeasure of texturing and overall quality of the textured maps.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 07:41:24 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kumar", "Ashish", ""], ["McBride", "James R.", ""], ["Pandey", "Gaurav", ""]]}, {"id": "2101.06395", "submitter": "Shuo Yang", "authors": "Shuo Yang, Lu Liu, Min Xu", "title": "Free Lunch for Few-shot Learning: Distribution Calibration", "comments": "ICLR 2021", "journal-ref": "The 9th International Conference on Learning Representations (ICLR\n  2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from a limited number of samples is challenging since the learned\nmodel can easily become overfitted based on the biased distribution formed by\nonly a few training examples. In this paper, we calibrate the distribution of\nthese few-sample classes by transferring statistics from the classes with\nsufficient examples, then an adequate number of examples can be sampled from\nthe calibrated distribution to expand the inputs to the classifier. We assume\nevery dimension in the feature representation follows a Gaussian distribution\nso that the mean and the variance of the distribution can borrow from that of\nsimilar classes whose statistics are better estimated with an adequate number\nof samples. Our method can be built on top of off-the-shelf pretrained feature\nextractors and classification models without extra parameters. We show that a\nsimple logistic regression classifier trained using the features sampled from\nour calibrated distribution can outperform the state-of-the-art accuracy on two\ndatasets (~5% improvement on miniImageNet compared to the next best). The\nvisualization of these generated features demonstrates that our calibrated\ndistribution is an accurate estimation.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 07:58:40 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 08:34:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Yang", "Shuo", ""], ["Liu", "Lu", ""], ["Xu", "Min", ""]]}, {"id": "2101.06399", "submitter": "Zixu Wang", "authors": "Zixu Wang, Yishu Miao, Lucia Specia", "title": "Latent Variable Models for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional models for Visual Question Answering (VQA) explore deterministic\napproaches with various types of image features, question features, and\nattention mechanisms. However, there exist other modalities that can be\nexplored in addition to image and question pairs to bring extra information to\nthe models. In this work, we propose latent variable models for VQA where extra\ninformation (e.g. captions and answer categories) are incorporated as latent\nvariables to improve inference, which in turn benefits question-answering\nperformance. Experiments on the VQA v2.0 benchmarking dataset demonstrate the\neffectiveness of our proposed models in that they improve over strong\nbaselines, especially those that do not rely on extensive language-vision\npre-training.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 08:21:43 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Zixu", ""], ["Miao", "Yishu", ""], ["Specia", "Lucia", ""]]}, {"id": "2101.06405", "submitter": "Ashish Kumar", "authors": "Ashish Kumar, L. Behera", "title": "Semi Supervised Deep Quick Instance Detection and Segmentation", "comments": "7 Pages, 7 Figures, 5 Tables. 2019 International Conference on\n  Robotics and Automation (ICRA). IEEE, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a semi supervised deep quick learning framework for\ninstance detection and pixel-wise semantic segmentation of images in a dense\nclutter of items. The framework can quickly and incrementally learn novel items\nin an online manner by real-time data acquisition and generating corresponding\nground truths on its own. To learn various combinations of items, it can\nsynthesize cluttered scenes, in real time. The overall approach is based on the\ntutor-child analogy in which a deep network (tutor) is pretrained for\nclass-agnostic object detection which generates labeled data for another deep\nnetwork (child). The child utilizes a customized convolutional neural network\nhead for the purpose of quick learning. There are broadly four key components\nof the proposed framework semi supervised labeling, occlusion aware clutter\nsynthesis, a customized convolutional neural network head, and instance\ndetection. The initial version of this framework was implemented during our\nparticipation in Amazon Robotics Challenge (ARC), 2017. Our system was ranked\n3rd, 4th and 5th worldwide in pick, stow-pick and stow task respectively. The\nproposed framework is an improved version over ARC17 where novel features such\nas instance detection and online learning has been added.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 08:50:36 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kumar", "Ashish", ""], ["Behera", "L.", ""]]}, {"id": "2101.06407", "submitter": "Jingfei Chang", "authors": "Jingfei Chang, Yang Lu, Ping Xue, Yiqun Xu, and Zhen Wei", "title": "ACP: Automatic Channel Pruning via Clustering and Swarm Intelligence\n  Optimization for CNN", "comments": "13 pages, 9 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As the convolutional neural network (CNN) gets deeper and wider in recent\nyears, the requirements for the amount of data and hardware resources have\ngradually increased. Meanwhile, CNN also reveals salient redundancy in several\ntasks. The existing magnitude-based pruning methods are efficient, but the\nperformance of the compressed network is unpredictable. While the accuracy loss\nafter pruning based on the structure sensitivity is relatively slight, the\nprocess is time-consuming and the algorithm complexity is notable. In this\narticle, we propose a novel automatic channel pruning method (ACP).\nSpecifically, we firstly perform layer-wise channel clustering via the\nsimilarity of the feature maps to perform preliminary pruning on the network.\nThen a population initialization method is introduced to transform the pruned\nstructure into a candidate population. Finally, we conduct searching and\noptimizing iteratively based on the particle swarm optimization (PSO) to find\nthe optimal compressed structure. The compact network is then retrained to\nmitigate the accuracy loss from pruning. Our method is evaluated against\nseveral state-of-the-art CNNs on three different classification datasets\nCIFAR-10/100 and ILSVRC-2012. On the ILSVRC-2012, when removing 64.36%\nparameters and 63.34% floating-point operations (FLOPs) of ResNet-50, the Top-1\nand Top-5 accuracy drop are less than 0.9%. Moreover, we demonstrate that\nwithout harming overall performance it is possible to compress SSD by more than\n50% on the target detection dataset PASCAL VOC. It further verifies that the\nproposed method can also be applied to other CNNs and application scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 08:56:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chang", "Jingfei", ""], ["Lu", "Yang", ""], ["Xue", "Ping", ""], ["Xu", "Yiqun", ""], ["Wei", "Zhen", ""]]}, {"id": "2101.06409", "submitter": "Ashish Kumar", "authors": "Ashish Kumar, L. Behera", "title": "Shape Back-Projection In 3D Scenes", "comments": "7 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we propose a novel framework shape back-projection for\ncomputationally efficient point cloud processing in a probabilistic manner. The\nprimary component of the technique is shape histogram and a back-projection\nprocedure. The technique measures similarity between 3D surfaces, by analyzing\ntheir geometrical properties. It is analogous to color back-projection which\nmeasures similarity between images, simply by looking at their color\ndistributions. In the overall process, first, shape histogram of a sample\nsurface (e.g. planar) is computed, which captures the profile of surface\nnormals around a point in form of a probability distribution. Later, the\nhistogram is back-projected onto a test surface and a likelihood score is\nobtained. The score depicts that how likely a point in the test surface behaves\nsimilar to the sample surface, geometrically. Shape back-projection finds its\napplication in binary surface classification, high curvature edge detection in\nunorganized point cloud, automated point cloud labeling for 3D-CNNs\n(convolutional neural network) etc. The algorithm can also be used for\nreal-time robotic operations such as autonomous object picking in warehouse\nautomation, ground plane extraction for autonomous vehicles and can be deployed\neasily on computationally limited platforms (UAVs).\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 09:00:34 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kumar", "Ashish", ""], ["Behera", "L.", ""]]}, {"id": "2101.06411", "submitter": "Ashish Kumar", "authors": "Ashish Kumar, Laxmidhar Behera", "title": "DeepMI: A Mutual Information Based Framework For Unsupervised Deep\n  Learning of Tasks", "comments": "10 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we propose an information theory based framework DeepMI to\ntrain deep neural networks (DNN) using Mutual Information (MI). The DeepMI\nframework is especially targeted but not limited to the learning of real world\ntasks in an unsupervised manner. The primary motivation behind this work is the\ninsufficiency of traditional loss functions for unsupervised task learning.\nMoreover, directly using MI for the training purpose is quite challenging to\ndeal because of its unbounded above nature. Hence, we develop an alternative\nlinearized representation of MI as a part of the framework. Contributions of\nthis paper are three fold: i) investigation of MI to train deep neural\nnetworks, ii) novel loss function LLMI, and iii) a fuzzy logic based end-to-end\ndifferentiable pipeline to integrate DeepMI into deep learning framework. We\nchoose a few unsupervised learning tasks for our experimental study. We\ndemonstrate that L LM I alone provides better gradients to achieve a neural\nnetwork better performance over the cases when multiple loss functions are used\nfor a given task.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 09:09:58 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kumar", "Ashish", ""], ["Behera", "Laxmidhar", ""]]}, {"id": "2101.06414", "submitter": "Ashish Kumar", "authors": "Ashish Kumar, Mohit Vohra, Ravi Prakash, L. Behera", "title": "Towards Deep Learning Assisted Autonomous UAVs for Manipulation Tasks in\n  GPS-Denied Environments", "comments": "8 pages, 5 figures, 5 tables, 2020 IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS). IEEE, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we present a pragmatic approach to enable unmanned aerial\nvehicle (UAVs) to autonomously perform highly complicated tasks of object pick\nand place. This paper is largely inspired by challenge-2 of MBZIRC 2020 and is\nprimarily focused on the task of assembling large 3D structures in outdoors and\nGPS-denied environments. Primary contributions of this system are: (i) a novel\ncomputationally efficient deep learning based unified multi-task visual\nperception system for target localization, part segmentation, and tracking,\n(ii) a novel deep learning based grasp state estimation, (iii) a retracting\nelectromagnetic gripper design, (iv) a remote computing approach which exploits\nstate-of-the-art MIMO based high speed (5000Mb/s) wireless links to allow the\nUAVs to execute compute intensive tasks on remote high end compute servers, and\n(v) system integration in which several system components are weaved together\nin order to develop an optimized software stack. We use DJI Matrice-600 Pro, a\nhex-rotor UAV and interface it with the custom designed gripper. Our framework\nis deployed on the specified UAV in order to report the performance analysis of\nthe individual modules. Apart from the manipulation system, we also highlight\nseveral hidden challenges associated with the UAVs in this context.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 09:20:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kumar", "Ashish", ""], ["Vohra", "Mohit", ""], ["Prakash", "Ravi", ""], ["Behera", "L.", ""]]}, {"id": "2101.06425", "submitter": "Qianye Yang", "authors": "Qianye Yang, Tom Vercauteren, Yunguan Fu, Francesco Giganti, Nooshin\n  Ghavami, Vasilis Stavrinides, Caroline Moore, Matt Clarkson, Dean Barratt,\n  Yipeng Hu", "title": "Morphological Change Forecasting for Prostate Glands using Feature-based\n  Registration and Kernel Density Extrapolation", "comments": "Accepted by ISBI 2021", "journal-ref": null, "doi": "10.1109/ISBI48211.2021.9433798", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organ morphology is a key indicator for prostate disease diagnosis and\nprognosis. For instance, In longitudinal study of prostate cancer patients\nunder active surveillance, the volume, boundary smoothness and their changes\nare closely monitored on time-series MR image data. In this paper, we describe\na new framework for forecasting prostate morphological changes, as the ability\nto detect such changes earlier than what is currently possible may enable\ntimely treatment or avoiding unnecessary confirmatory biopsies. In this work,\nan efficient feature-based MR image registration is first developed to align\ndelineated prostate gland capsules to quantify the morphological changes using\nthe inferred dense displacement fields (DDFs). We then propose to use kernel\ndensity estimation (KDE) of the probability density of the DDF-represented\n\\textit{future morphology changes}, between current and future time points,\nbefore the future data become available. The KDE utilises a novel distance\nfunction that takes into account morphology, stage-of-progression and\nduration-of-change, which are considered factors in such subject-specific\nforecasting. We validate the proposed approach on image masks unseen to\nregistration network training, without using any data acquired at the future\ntarget time points. The experiment results are presented on a longitudinal data\nset with 331 images from 73 patients, yielding an average Dice score of 0.865\non a holdout set, between the ground-truth and the image masks warped by the\nKDE-predicted-DDFs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 10:45:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yang", "Qianye", ""], ["Vercauteren", "Tom", ""], ["Fu", "Yunguan", ""], ["Giganti", "Francesco", ""], ["Ghavami", "Nooshin", ""], ["Stavrinides", "Vasilis", ""], ["Moore", "Caroline", ""], ["Clarkson", "Matt", ""], ["Barratt", "Dean", ""], ["Hu", "Yipeng", ""]]}, {"id": "2101.06438", "submitter": "Nuo Xu", "authors": "Nuo Xu, Chunlei Huo, Jiacheng Guo, Yiwei Liu, Jian Wang and Chunhong\n  Pan", "title": "Adaptive Remote Sensing Image Attribute Learning for Active Object\n  Detection", "comments": "Accepted in 25th International Conference on Pattern Recognition\n  (ICPR), (Milan, Italy), January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, deep learning methods bring incredible progress to the field\nof object detection. However, in the field of remote sensing image processing,\nexisting methods neglect the relationship between imaging configuration and\ndetection performance, and do not take into account the importance of detection\nperformance feedback for improving image quality. Therefore, detection\nperformance is limited by the passive nature of the conventional object\ndetection framework. In order to solve the above limitations, this paper takes\nadaptive brightness adjustment and scale adjustment as examples, and proposes\nan active object detection method based on deep reinforcement learning. The\ngoal of adaptive image attribute learning is to maximize the detection\nperformance. With the help of active object detection and image attribute\nadjustment strategies, low-quality images can be converted into high-quality\nimages, and the overall performance is improved without retraining the\ndetector.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 11:37:50 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xu", "Nuo", ""], ["Huo", "Chunlei", ""], ["Guo", "Jiacheng", ""], ["Liu", "Yiwei", ""], ["Wang", "Jian", ""], ["Pan", "Chunhong", ""]]}, {"id": "2101.06440", "submitter": "Tom Vercauteren", "authors": "M. Jorge Cardoso, Marc Modat, Tom Vercauteren, Sebastien Ourselin", "title": "Scale factor point spread function matching: Beyond aliasing in image\n  resampling", "comments": "Published in MICCAI 2015", "journal-ref": null, "doi": "10.1007/978-3-319-24571-3_81", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imaging devices exploit the Nyquist-Shannon sampling theorem to avoid both\naliasing and redundant oversampling by design. Conversely, in medical image\nresampling, images are considered as continuous functions, are warped by a\nspatial transformation, and are then sampled on a regular grid. In most cases,\nthe spatial warping changes the frequency characteristics of the continuous\nfunction and no special care is taken to ensure that the resampling grid\nrespects the conditions of the sampling theorem. This paper shows that this\noversight introduces artefacts, including aliasing, that can lead to important\nbias in clinical applications. One notable exception to this common practice is\nwhen multi-resolution pyramids are constructed, with low-pass \"anti-aliasing\"\nfilters being applied prior to downsampling. In this work, we illustrate why\nsimilar caution is needed when resampling images under general spatial\ntransformations and propose a novel method that is more respectful of the\nsampling theorem, minimising aliasing and loss of information. We introduce the\nnotion of scale factor point spread function (sfPSF) and employ Gaussian\nkernels to achieve a computationally tractable resampling scheme that can cope\nwith arbitrary non-linear spatial transformations and grid sizes. Experiments\ndemonstrate significant (p<1e-4) technical and clinical implications of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 11:40:58 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Cardoso", "M. Jorge", ""], ["Modat", "Marc", ""], ["Vercauteren", "Tom", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "2101.06459", "submitter": "Subramanyam Natarajan", "authors": "Sumukh Aithal K, Dhruva Kashyap, Natarajan Subramanyam", "title": "Robustness to Augmentations as a Generalization metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization is the ability of a model to predict on unseen domains and is\na fundamental task in machine learning. Several generalization bounds, both\ntheoretical and empirical have been proposed but they do not provide tight\nbounds .In this work, we propose a simple yet effective method to predict the\ngeneralization performance of a model by using the concept that models that are\nrobust to augmentations are more generalizable than those which are not. We\nexperiment with several augmentations and composition of augmentations to check\nthe generalization capacity of a model. We also provide a detailed motivation\nbehind the proposed method. The proposed generalization metric is calculated\nbased on the change in the output of the model after augmenting the input. The\nproposed method was the first runner up solution for the NeurIPS competition on\nPredicting Generalization in Deep Learning.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:36:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["K", "Sumukh Aithal", ""], ["Kashyap", "Dhruva", ""], ["Subramanyam", "Natarajan", ""]]}, {"id": "2101.06462", "submitter": "Yunpeng Luo", "authors": "Yunpeng Luo, Jiayi Ji, Xiaoshuai Sun, Liujuan Cao, Yongjian Wu, Feiyue\n  Huang, Chia-Wen Lin, Rongrong Ji", "title": "Dual-Level Collaborative Transformer for Image Captioning", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Descriptive region features extracted by object detection networks have\nplayed an important role in the recent advancements of image captioning.\nHowever, they are still criticized for the lack of contextual information and\nfine-grained details, which in contrast are the merits of traditional grid\nfeatures. In this paper, we introduce a novel Dual-Level Collaborative\nTransformer (DLCT) network to realize the complementary advantages of the two\nfeatures. Concretely, in DLCT, these two features are first processed by a\nnovelDual-way Self Attenion (DWSA) to mine their intrinsic properties, where a\nComprehensive Relation Attention component is also introduced to embed the\ngeometric information. In addition, we propose a Locality-Constrained Cross\nAttention module to address the semantic noises caused by the direct fusion of\nthese two features, where a geometric alignment graph is constructed to\naccurately align and reinforce region and grid features. To validate our model,\nwe conduct extensive experiments on the highly competitive MS-COCO dataset, and\nachieve new state-of-the-art performance on both local and online test sets,\ni.e., 133.8% CIDEr-D on Karpathy split and 135.4% CIDEr on the official split.\nCode is available at https://github.com/luo3300612/image-captioning-DLCT.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:43:17 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Luo", "Yunpeng", ""], ["Ji", "Jiayi", ""], ["Sun", "Xiaoshuai", ""], ["Cao", "Liujuan", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""], ["Lin", "Chia-Wen", ""], ["Ji", "Rongrong", ""]]}, {"id": "2101.06468", "submitter": "Khrystyna Faryna", "authors": "Khrystyna Faryna, Kevin Koschmieder, Marcella M. Paul, Thomas van den\n  Heuvel, Anke van der Eerden, Rashindra Manniesing, Bram van Ginneken", "title": "Adversarial cycle-consistent synthesis of cerebral microbleeds for data\n  augmentation", "comments": "Accepted in Medical Imaging meets NIPS Workshop, NIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for controllable pathological image synthesis\nfor data augmentation. Inspired by CycleGAN, we perform cycle-consistent\nimage-to-image translation between two domains: healthy and pathological.\nGuided by a semantic mask, an adversarially trained generator synthesizes\npathology on a healthy image in the specified location. We demonstrate our\napproach on an institutional dataset of cerebral microbleeds in traumatic brain\ninjury patients. We utilize synthetic images generated with our method for data\naugmentation in cerebral microbleeds detection. Enriching the training dataset\nwith synthetic images exhibits the potential to increase detection performance\nfor cerebral microbleeds in traumatic brain injury patients.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:58:17 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Faryna", "Khrystyna", ""], ["Koschmieder", "Kevin", ""], ["Paul", "Marcella M.", ""], ["Heuvel", "Thomas van den", ""], ["van der Eerden", "Anke", ""], ["Manniesing", "Rashindra", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2101.06474", "submitter": "Vinamra Agrawal", "authors": "Roberto Perera, Davide Guzzetti, Vinamra Agrawal", "title": "Optimized and autonomous machine learning framework for characterizing\n  pores, particles, grains and grain boundaries in microstructural images", "comments": null, "journal-ref": null, "doi": "10.1016/j.commatsci.2021.110524", "report-no": null, "categories": "eess.IV cond-mat.mtrl-sci cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additively manufactured metals exhibit heterogeneous microstructure which\ndictates their material and failure properties. Experimental microstructural\ncharacterization techniques generate a large amount of data that requires\nexpensive computationally resources. In this work, an optimized machine\nlearning (ML) framework is proposed to autonomously and efficiently\ncharacterize pores, particles, grains and grain boundaries (GBs) from a given\nmicrostructure image. First, using a classifier Convolutional Neural Network\n(CNN), defects such as pores, powder particles, or GBs were recognized from a\ngiven microstructure. Depending on the type of defect, two different processes\nwere used. For powder particles or pores, binary segmentations were generated\nusing an optimized Convolutional Encoder-Decoder Network (CEDN). The binary\nsegmentations were used to used obtain particle and pore size and bounding\nboxes using an object detection ML network (YOLOv5). For GBs, another optimized\nCEDN was developed to generate RGB segmentation images, which were used to\nobtain grain size distribution using two regression CNNS. To optimize the RGB\nCEDN, the Deep Emulator Network SEarch (DENSE) method which employs the\nCovariance Matrix Adaptation - Evolution Strategy (CMA-ES) was implemented. The\noptimized RGB segmentation network showed a substantial reduction in training\ntime and GPU usage compared to the unoptimized network, while maintaining high\naccuracy. Lastly, the proposed framework showed a significant improvement in\nanalysis time when compared to conventional methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 16:42:19 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 20:40:18 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Perera", "Roberto", ""], ["Guzzetti", "Davide", ""], ["Agrawal", "Vinamra", ""]]}, {"id": "2101.06480", "submitter": "Byoungjip Kim", "authors": "Byoungjip Kim, Jinho Choo, Yeong-Dae Kwon, Seongho Joe, Seungjai Min,\n  Youngjune Gwon", "title": "SelfMatch: Combining Contrastive Self-Supervision and Consistency for\n  Semi-Supervised Learning", "comments": "4 pages, NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and\n  Practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SelfMatch, a semi-supervised learning method that\ncombines the power of contrastive self-supervised learning and consistency\nregularization. SelfMatch consists of two stages: (1) self-supervised\npre-training based on contrastive learning and (2) semi-supervised fine-tuning\nbased on augmentation consistency regularization. We empirically demonstrate\nthat SelfMatch achieves the state-of-the-art results on standard benchmark\ndatasets such as CIFAR-10 and SVHN. For example, for CIFAR-10 with 40 labeled\nexamples, SelfMatch achieves 93.19% accuracy that outperforms the strong\nprevious methods such as MixMatch (52.46%), UDA (70.95%), ReMixMatch (80.9%),\nand FixMatch (86.19%). We note that SelfMatch can close the gap between\nsupervised learning (95.87%) and semi-supervised learning (93.19%) by using\nonly a few labels for each class.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 17:03:53 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kim", "Byoungjip", ""], ["Choo", "Jinho", ""], ["Kwon", "Yeong-Dae", ""], ["Joe", "Seongho", ""], ["Min", "Seungjai", ""], ["Gwon", "Youngjune", ""]]}, {"id": "2101.06498", "submitter": "Jose Dolz", "authors": "Mark G. Bandyk, Dheeraj R Gopireddy, Chandana Lall, K.C. Balaji, Jose\n  Dolz", "title": "Bladder segmentation based on deep learning approaches: current\n  limitations and lessons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise determination and assessment of bladder cancer (BC) extent of muscle\ninvasion involvement guides proper risk stratification and personalized therapy\nselection. In this context, segmentation of both bladder walls and cancer are\nof pivotal importance, as it provides invaluable information to stage the\nprimary tumour. Hence, multi region segmentation on patients presenting with\nsymptoms of bladder tumours using deep learning heralds a new level of staging\naccuracy and prediction of the biologic behaviour of the tumour. Nevertheless,\ndespite the success of these models in other medical problems, progress in\nmulti region bladder segmentation is still at a nascent stage, with just a\nhandful of works tackling a multi region scenario. Furthermore, most existing\napproaches systematically follow prior literature in other clinical problems,\nwithout casting a doubt on the validity of these methods on bladder\nsegmentation, which may present different challenges. Inspired by this, we\nprovide an in-depth look at bladder cancer segmentation using deep learning\nmodels. The critical determinants for accurate differentiation of muscle\ninvasive disease, current status of deep learning based bladder segmentation,\nlessons and limitations of prior work are highlighted.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 18:20:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Bandyk", "Mark G.", ""], ["Gopireddy", "Dheeraj R", ""], ["Lall", "Chandana", ""], ["Balaji", "K. C.", ""], ["Dolz", "Jose", ""]]}, {"id": "2101.06507", "submitter": "Jia Liu", "authors": "Jia Liu and Yaochu Jin", "title": "Multi-objective Search of Robust Neural Architectures against Multiple\n  Types of Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing deep learning models are vulnerable to adversarial examples\nthat are imperceptible to humans. To address this issue, various methods have\nbeen proposed to design network architectures that are robust to one particular\ntype of adversarial attacks. It is practically impossible, however, to predict\nbeforehand which type of attacks a machine learn model may suffer from. To\naddress this challenge, we propose to search for deep neural architectures that\nare robust to five types of well-known adversarial attacks using a\nmulti-objective evolutionary algorithm. To reduce the computational cost, a\nnormalized error rate of a randomly chosen attack is calculated as the\nrobustness for each newly generated neural architecture at each generation. All\nnon-dominated network architectures obtained by the proposed method are then\nfully trained against randomly chosen adversarial attacks and tested on two\nwidely used datasets. Our experimental results demonstrate the superiority of\noptimized neural architectures found by the proposed approach over\nstate-of-the-art networks that are widely used in the literature in terms of\nthe classification accuracy under different adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 19:38:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Jia", ""], ["Jin", "Yaochu", ""]]}, {"id": "2101.06541", "submitter": "Kelvin Wong", "authors": "Shuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Manivasagam, Mengye\n  Ren, Raquel Urtasun", "title": "SceneGen: Learning to Generate Realistic Traffic Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of generating realistic traffic scenes automatically.\nExisting methods typically insert actors into the scene according to a set of\nhand-crafted heuristics and are limited in their ability to model the true\ncomplexity and diversity of real traffic scenes, thus inducing a content gap\nbetween synthesized traffic scenes versus real ones. As a result, existing\nsimulators lack the fidelity necessary to train and test self-driving vehicles.\nTo address this limitation, we present SceneGen, a neural autoregressive model\nof traffic scenes that eschews the need for rules and heuristics. In\nparticular, given the ego-vehicle state and a high definition map of\nsurrounding area, SceneGen inserts actors of various classes into the scene and\nsynthesizes their sizes, orientations, and velocities. We demonstrate on two\nlarge-scale datasets SceneGen's ability to faithfully model distributions of\nreal traffic scenes. Moreover, we show that SceneGen coupled with sensor\nsimulation can be used to train perception models that generalize to the real\nworld.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:51:43 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Tan", "Shuhan", ""], ["Wong", "Kelvin", ""], ["Wang", "Shenlong", ""], ["Manivasagam", "Sivabalan", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06543", "submitter": "Yun Chen", "authors": "Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan,\n  Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, Raquel Urtasun", "title": "GeoSim: Realistic Video Simulation via Geometry-Aware Composition for\n  Self-Driving", "comments": "Accepted by CVPR 2021 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable sensor simulation is an important yet challenging open problem for\nsafety-critical domains such as self-driving. Current works in image simulation\neither fail to be photorealistic or do not model the 3D environment and the\ndynamic objects within, losing high-level control and physical realism. In this\npaper, we present GeoSim, a geometry-aware image composition process which\nsynthesizes novel urban driving scenarios by augmenting existing images with\ndynamic objects extracted from other scenes and rendered at novel poses.\nTowards this goal, we first build a diverse bank of 3D objects with both\nrealistic geometry and appearance from sensor data. During simulation, we\nperform a novel geometry-aware simulation-by-composition procedure which 1)\nproposes plausible and realistic object placements into a given scene, 2)\nrender novel views of dynamic objects from the asset bank, and 3) composes and\nblends the rendered image segments. The resulting synthetic images are\nrealistic, traffic-aware, and geometrically consistent, allowing our approach\nto scale to complex use cases. We demonstrate two such important applications:\nlong-range realistic video simulation across multiple camera sensors, and\nsynthetic data generation for data augmentation on downstream segmentation\ntasks. Please check https://tmux.top/publication/geosim/ for high-resolution\nvideo results.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:00:33 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 16:06:21 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 05:19:19 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Yun", ""], ["Rong", "Frieda", ""], ["Duggal", "Shivam", ""], ["Wang", "Shenlong", ""], ["Yan", "Xinchen", ""], ["Manivasagam", "Sivabalan", ""], ["Xue", "Shangjie", ""], ["Yumer", "Ersin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06545", "submitter": "Namdar Homayounfar", "authors": "Namdar Homayounfar, Justin Liang, Wei-Chiu Ma, Raquel Urtasun", "title": "VideoClick: Video Object Segmentation with a Single Click", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating videos with object segmentation masks typically involves a two\nstage procedure of drawing polygons per object instance for all the frames and\nthen linking them through time. While simple, this is a very tedious, time\nconsuming and expensive process, making the creation of accurate annotations at\nscale only possible for well-funded labs. What if we were able to segment an\nobject in the full video with only a single click? This will enable video\nsegmentation at scale with a very low budget opening the door to many\napplications. Towards this goal, in this paper we propose a bottom up approach\nwhere given a single click for each object in a video, we obtain the\nsegmentation masks of these objects in the full video. In particular, we\nconstruct a correlation volume that assigns each pixel in a target frame to\neither one of the objects in the reference frame or the background. We then\nrefine this correlation volume via a recurrent attention module and decode the\nfinal segmentation. To evaluate the performance, we label the popular and\nchallenging Cityscapes dataset with video object segmentations. Results on this\nnew CityscapesVideo dataset show that our approach outperforms all the\nbaselines in this challenging setting.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:07:48 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Homayounfar", "Namdar", ""], ["Liang", "Justin", ""], ["Ma", "Wei-Chiu", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06547", "submitter": "Sergio Casas", "authors": "Alexander Cui, Sergio Casas, Abbas Sadat, Renjie Liao, Raquel Urtasun", "title": "LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present LookOut, a novel autonomy system that perceives the\nenvironment, predicts a diverse set of futures of how the scene might unroll\nand estimates the trajectory of the SDV by optimizing a set of contingency\nplans over these future realizations. In particular, we learn a diverse joint\ndistribution over multi-agent future trajectories in a traffic scene that\ncovers a wide range of future modes with high sample efficiency while\nleveraging the expressive power of generative models. Unlike previous work in\ndiverse motion forecasting, our diversity objective explicitly rewards sampling\nfuture scenarios that require distinct reactions from the self-driving vehicle\nfor improved safety. Our contingency planner then finds comfortable and\nnon-conservative trajectories that ensure safe reactions to a wide range of\nfuture scenarios. Through extensive evaluations, we show that our model\ndemonstrates significantly more diverse and sample-efficient motion forecasting\nin a large-scale self-driving dataset as well as safer and less-conservative\nmotion plans in long-term closed-loop simulations when compared to current\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:19:22 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 16:30:19 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 18:47:54 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cui", "Alexander", ""], ["Casas", "Sergio", ""], ["Sadat", "Abbas", ""], ["Liao", "Renjie", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06549", "submitter": "Jingkang Wang", "authors": "Jingkang Wang, Ava Pun, James Tu, Sivabalan Manivasagam, Abbas Sadat,\n  Sergio Casas, Mengye Ren, Raquel Urtasun", "title": "AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As self-driving systems become better, simulating scenarios where the\nautonomy stack may fail becomes more important. Traditionally, those scenarios\nare generated for a few scenes with respect to the planning module that takes\nground-truth actor states as input. This does not scale and cannot identify all\npossible autonomy failures, such as perception failures due to occlusion. In\nthis paper, we propose AdvSim, an adversarial framework to generate\nsafety-critical scenarios for any LiDAR-based autonomy system. Given an initial\ntraffic scenario, AdvSim modifies the actors' trajectories in a physically\nplausible manner and updates the LiDAR sensor data to match the perturbed\nworld. Importantly, by simulating directly from sensor data, we obtain\nadversarial scenarios that are safety-critical for the full autonomy stack. Our\nexperiments show that our approach is general and can identify thousands of\nsemantically meaningful safety-critical scenarios for a wide range of modern\nself-driving systems. Furthermore, we show that the robustness and safety of\nthese systems can be further improved by training them with scenarios generated\nby AdvSim.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:23:12 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 03:42:18 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Jingkang", ""], ["Pun", "Ava", ""], ["Tu", "James", ""], ["Manivasagam", "Sivabalan", ""], ["Sadat", "Abbas", ""], ["Casas", "Sergio", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06553", "submitter": "Yuwen Xiong", "authors": "Yuwen Xiong, Mengye Ren, Wenyuan Zeng, Raquel Urtasun", "title": "Self-Supervised Representation Learning from Flow Equivariance", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised representation learning is able to learn semantically\nmeaningful features; however, much of its recent success relies on multiple\ncrops of an image with very few objects. Instead of learning view-invariant\nrepresentation from simple images, humans learn representations in a complex\nworld with changing scenes by observing object movement, deformation, pose\nvariation, and ego motion. Motivated by this ability, we present a new\nself-supervised learning representation framework that can be directly deployed\non a video stream of complex scenes with many moving objects. Our framework\nfeatures a simple flow equivariance objective that encourages the network to\npredict the features of another frame by applying a flow transformation to the\nfeatures of the current frame. Our representations, learned from\nhigh-resolution raw video, can be readily used for downstream tasks on static\nimages. Readout experiments on challenging semantic segmentation, instance\nsegmentation, and object detection benchmarks show that we are able to\noutperform representations obtained from previous state-of-the-art methods\nincluding SimCLR and BYOL.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:44:09 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xiong", "Yuwen", ""], ["Ren", "Mengye", ""], ["Zeng", "Wenyuan", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06557", "submitter": "Simon Suo", "authors": "Simon Suo, Sebastian Regalado, Sergio Casas, Raquel Urtasun", "title": "TrafficSim: Learning to Simulate Realistic Multi-Agent Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulation has the potential to massively scale evaluation of self-driving\nsystems enabling rapid development as well as safe deployment. To close the gap\nbetween simulation and the real world, we need to simulate realistic\nmulti-agent behaviors. Existing simulation environments rely on heuristic-based\nmodels that directly encode traffic rules, which cannot capture irregular\nmaneuvers (e.g., nudging, U-turns) and complex interactions (e.g., yielding,\nmerging). In contrast, we leverage real-world data to learn directly from human\ndemonstration and thus capture a more diverse set of actor behaviors. To this\nend, we propose TrafficSim, a multi-agent behavior model for realistic traffic\nsimulation. In particular, we leverage an implicit latent variable model to\nparameterize a joint actor policy that generates socially-consistent plans for\nall actors in the scene jointly. To learn a robust policy amenable for long\nhorizon simulation, we unroll the policy in training and optimize through the\nfully differentiable simulation across time. Our learning objective\nincorporates both human demonstrations as well as common sense. We show\nTrafficSim generates significantly more realistic and diverse traffic scenarios\nas compared to a diverse set of baselines. Notably, we can exploit trajectories\ngenerated by TrafficSim as effective data augmentation for training better\nmotion planner.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 00:29:30 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Suo", "Simon", ""], ["Regalado", "Sebastian", ""], ["Casas", "Sergio", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06560", "submitter": "James Tu", "authors": "James Tu, Tsunhsuan Wang, Jingkang Wang, Sivabalan Manivasagam, Mengye\n  Ren, Raquel Urtasun", "title": "Adversarial Attacks On Multi-Agent Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Growing at a very fast pace, modern autonomous systems will soon be deployed\nat scale, opening up the possibility for cooperative multi-agent systems. By\nsharing information and distributing workloads, autonomous agents can better\nperform their tasks and enjoy improved computation efficiency. However, such\nadvantages rely heavily on communication channels which have been shown to be\nvulnerable to security breaches. Thus, communication can be compromised to\nexecute adversarial attacks on deep learning models which are widely employed\nin modern systems. In this paper, we explore such adversarial attacks in a\nnovel multi-agent setting where agents communicate by sharing learned\nintermediate representations. We observe that an indistinguishable adversarial\nmessage can severely degrade performance, but becomes weaker as the number of\nbenign agents increase. Furthermore, we show that transfer attacks are more\ndifficult in this setting when compared to directly perturbing the inputs, as\nit is necessary to align the distribution of communication messages with domain\nadaptation. Finally, we show that low-budget online attacks can be achieved by\nexploiting the temporal consistency of streaming sensory inputs.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 00:35:26 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Tu", "James", ""], ["Wang", "Tsunhsuan", ""], ["Wang", "Jingkang", ""], ["Manivasagam", "Sivabalan", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06562", "submitter": "Ioan Andrei B\\^arsan", "authors": "Anqi Joyce Yang, Can Cui, Ioan Andrei B\\^arsan, Raquel Urtasun,\n  Shenlong Wang", "title": "Asynchronous Multi-View SLAM", "comments": "25 pages, 23 figures, 13 tables", "journal-ref": "Published at ICRA 2021", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing multi-camera SLAM systems assume synchronized shutters for all\ncameras, which is often not the case in practice. In this work, we propose a\ngeneralized multi-camera SLAM formulation which accounts for asynchronous\nsensor observations. Our framework integrates a continuous-time motion model to\nrelate information across asynchronous multi-frames during tracking, local\nmapping, and loop closing. For evaluation, we collected AMV-Bench, a\nchallenging new SLAM dataset covering 482 km of driving recorded using our\nasynchronous multi-camera robotic platform. AMV-Bench is over an order of\nmagnitude larger than previous multi-view HD outdoor SLAM datasets, and covers\ndiverse and challenging motions and environments. Our experiments emphasize the\nnecessity of asynchronous sensor modeling, and show that the use of multiple\ncameras is critical towards robust and accurate SLAM in challenging outdoor\nscenes. For additional information, please see the project website at:\nhttps://www.cs.toronto.edu/~ajyang/amv-slam\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 00:50:01 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 01:42:54 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 00:48:52 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Yang", "Anqi Joyce", ""], ["Cui", "Can", ""], ["B\u00e2rsan", "Ioan Andrei", ""], ["Urtasun", "Raquel", ""], ["Wang", "Shenlong", ""]]}, {"id": "2101.06571", "submitter": "Ze Yang", "authors": "Ze Yang, Shenlong Wang, Sivabalan Manivasagam, Zeng Huang, Wei-Chiu\n  Ma, Xinchen Yan, Ersin Yumer, Raquel Urtasun", "title": "S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Constructing and animating humans is an important component for building\nvirtual worlds in a wide variety of applications such as virtual reality or\nrobotics testing in simulation. As there are exponentially many variations of\nhumans with different shape, pose and clothing, it is critical to develop\nmethods that can automatically reconstruct and animate humans at scale from\nreal world data. Towards this goal, we represent the pedestrian's shape, pose\nand skinning weights as neural implicit functions that are directly learned\nfrom data. This representation enables us to handle a wide variety of different\npedestrian shapes and poses without explicitly fitting a human parametric body\nmodel, allowing us to handle a wider range of human geometries and topologies.\nWe demonstrate the effectiveness of our approach on various datasets and show\nthat our reconstructions outperform existing state-of-the-art methods.\nFurthermore, our re-animation experiments show that we can generate 3D human\nanimations at scale from a single RGB image (and/or an optional LiDAR sweep) as\ninput.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 02:16:56 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yang", "Ze", ""], ["Wang", "Shenlong", ""], ["Manivasagam", "Sivabalan", ""], ["Huang", "Zeng", ""], ["Ma", "Wei-Chiu", ""], ["Yan", "Xinchen", ""], ["Yumer", "Ersin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06586", "submitter": "Bin Yang", "authors": "Bin Yang, Min Bai, Ming Liang, Wenyuan Zeng, Raquel Urtasun", "title": "Auto4D: Learning to Label 4D Objects from Sequential Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the past few years we have seen great advances in object perception\n(particularly in 4D space-time dimensions) thanks to deep learning methods.\nHowever, they typically rely on large amounts of high-quality labels to achieve\ngood performance, which often require time-consuming and expensive work by\nhuman annotators. To address this we propose an automatic annotation pipeline\nthat generates accurate object trajectories in 3D space (i.e., 4D labels) from\nLiDAR point clouds. The key idea is to decompose the 4D object label into two\nparts: the object size in 3D that's fixed through time for rigid objects, and\nthe motion path describing the evolution of the object's pose through time.\nInstead of generating a series of labels in one shot, we adopt an iterative\nrefinement process where online generated object detections are tracked through\ntime as the initialization. Given the cheap but noisy input, our model produces\nhigher quality 4D labels by re-estimating the object size and smoothing the\nmotion path, where the improvement is achieved by exploiting aggregated\nobservations and motion cues over the entire trajectory. We validate the\nproposed method on a large-scale driving dataset and show a 25% reduction of\nhuman annotation efforts. We also showcase the benefits of our approach in the\nannotator-in-the-loop setting.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 04:23:05 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 19:27:19 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Yang", "Bin", ""], ["Bai", "Min", ""], ["Liang", "Ming", ""], ["Zeng", "Wenyuan", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06590", "submitter": "Jingkang Wang", "authors": "Jingkang Wang, Mengye Ren, Ilija Bogunovic, Yuwen Xiong, Raquel\n  Urtasun", "title": "Cost-Efficient Online Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on hyperparameters optimization (HPO) has shown the possibility\nof training certain hyperparameters together with regular parameters. However,\nthese online HPO algorithms still require running evaluation on a set of\nvalidation examples at each training step, steeply increasing the training\ncost. To decide when to query the validation loss, we model online HPO as a\ntime-varying Bayesian optimization problem, on top of which we propose a novel\n\\textit{costly feedback} setting to capture the concept of the query cost.\nUnder this setting, standard algorithms are cost-inefficient as they evaluate\non the validation set at every round. In contrast, the cost-efficient GP-UCB\nalgorithm proposed in this paper queries the unknown function only when the\nmodel is less confident about current decisions. We evaluate our proposed\nalgorithm by tuning hyperparameters online for VGG and ResNet on CIFAR-10 and\nImageNet100. Our proposed online HPO algorithm reaches human expert-level\nperformance within a single run of the experiment, while incurring only modest\ncomputational overhead compared to regular training.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 04:55:30 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Jingkang", ""], ["Ren", "Mengye", ""], ["Bogunovic", "Ilija", ""], ["Xiong", "Yuwen", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06594", "submitter": "Bin Yang", "authors": "Yan Wang, Bin Yang, Rui Hu, Ming Liang, Raquel Urtasun", "title": "PLUME: Efficient 3D Object Detection from Stereo Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D object detection plays a significant role in various robotic applications\nincluding self-driving. While many approaches rely on expensive 3D sensors like\nLiDAR to produce accurate 3D estimates, stereo-based methods have recently\nshown promising results at a lower cost. Existing methods tackle the problem in\ntwo steps: first depth estimation is performed, a pseudo LiDAR point cloud\nrepresentation is computed from the depth estimates, and then object detection\nis performed in 3D space. However, because the two separate tasks are optimized\nin different metric spaces, the depth estimation is biased towards nearby\nobjects and may cause sub-optimal performance of 3D detection. In this paper we\npropose a model that unifies these two tasks in the same metric space.\nSpecifically, our model directly constructs a pseudo LiDAR feature volume\n(PLUME) in 3D space, which is used to solve both occupancy estimation and\nobject detection tasks. Our approach achieves state-of-the-art performance on\nthe challenging KITTI benchmark, with significantly reduced inference time\ncompared with existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 05:11:38 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 19:32:16 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wang", "Yan", ""], ["Yang", "Bin", ""], ["Hu", "Rui", ""], ["Liang", "Ming", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06605", "submitter": "Jiahui Huang", "authors": "Jiahui Huang, He Wang, Tolga Birdal, Minhyuk Sung, Federica Arrigoni,\n  Shi-Min Hu, Leonidas Guibas", "title": "MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan\n  Synchronization", "comments": "Contact: huang-jh18<at>mails<dot>tsinghua<dot>edu<dot>cn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present MultiBodySync, a novel, end-to-end trainable multi-body motion\nsegmentation and rigid registration framework for multiple input 3D point\nclouds. The two non-trivial challenges posed by this multi-scan multibody\nsetting that we investigate are: (i) guaranteeing correspondence and\nsegmentation consistency across multiple input point clouds capturing different\nspatial arrangements of bodies or body parts; and (ii) obtaining robust\nmotion-based rigid body segmentation applicable to novel object categories. We\npropose an approach to address these issues that incorporates spectral\nsynchronization into an iterative deep declarative network, so as to\nsimultaneously recover consistent correspondences as well as motion\nsegmentation. At the same time, by explicitly disentangling the correspondence\nand motion segmentation estimation modules, we achieve strong generalizability\nacross different object categories. Our extensive evaluations demonstrate that\nour method is effective on various datasets ranging from rigid parts in\narticulated objects to individually moving objects in a 3D scene, be it\nsingle-view or full point clouds.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 06:36:28 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 15:12:13 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 01:41:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Huang", "Jiahui", ""], ["Wang", "He", ""], ["Birdal", "Tolga", ""], ["Sung", "Minhyuk", ""], ["Arrigoni", "Federica", ""], ["Hu", "Shi-Min", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2101.06608", "submitter": "Wenyuan Zeng", "authors": "Wenyuan Zeng, Yuwen Xiong, Raquel Urtasun", "title": "Network Automatic Pruning: Start NAP and Take a Nap", "comments": "An updated version of 'MLPrune: Multi-Layer Pruning for Automated\n  Neural Network Compression'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network pruning can significantly reduce the computation and memory footprint\nof large neural networks. To achieve a good trade-off between model size and\nperformance, popular pruning techniques usually rely on hand-crafted heuristics\nand require manually setting the compression ratio for each layer. This process\nis typically time-consuming and requires expert knowledge to achieve good\nresults. In this paper, we propose NAP, a unified and automatic pruning\nframework for both fine-grained and structured pruning. It can find out\nunimportant components of a network and automatically decide appropriate\ncompression ratios for different layers, based on a theoretically sound\ncriterion. Towards this goal, NAP uses an efficient approximation of the\nHessian for evaluating the importances of components, based on a\nKronecker-factored Approximate Curvature method. Despite its simpleness to use,\nNAP outperforms previous pruning methods by large margins. For fine-grained\npruning, NAP can compress AlexNet and VGG16 by 25x, and ResNet-50 by 6.7x\nwithout loss in accuracy on ImageNet. For structured pruning (e.g. channel\npruning), it can reduce flops of VGG16 by 5.4x and ResNet-50 by 2.3x with only\n1% accuracy drop. More importantly, this method is almost free from\nhyper-parameter tuning and requires no expert knowledge. You can start NAP and\nthen take a nap!\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 07:09:19 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zeng", "Wenyuan", ""], ["Xiong", "Yuwen", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06616", "submitter": "Erlei Zhang", "authors": "Jinye Peng, Jiaxin Wang, Jun Wang, Erlei Zhang, Qunxi Zhang, Yongqin\n  Zhang, Xianlin Peng, Kai Yu", "title": "A relic sketch extraction framework based on detail-aware hierarchical\n  deep network", "comments": null, "journal-ref": null, "doi": "10.1016/j.sigpro.2021.108008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the first step of the restoration process of painted relics, sketch\nextraction plays an important role in cultural research. However, sketch\nextraction suffers from serious disease corrosion, which results in broken\nlines and noise. To overcome these problems, we propose a deep learning-based\nhierarchical sketch extraction framework for painted cultural relics. We design\nthe sketch extraction process into two stages: coarse extraction and fine\nextraction. In the coarse extraction stage, we develop a novel detail-aware\nbi-directional cascade network that integrates flow-based\ndifference-of-Gaussians (FDoG) edge detection and a bi-directional cascade\nnetwork (BDCN) under a transfer learning framework. It not only uses the\npre-trained strategy to extenuate the requirements of large datasets for deep\nnetwork training but also guides the network to learn the detail\ncharacteristics by the prior knowledge from FDoG. For the fine extraction\nstage, we design a new multiscale U-Net (MSU-Net) to effectively remove disease\nnoise and refine the sketch. Specifically, all the features extracted from\nmultiple intermediate layers in the decoder of MSU-Net are fused for sketch\npredication. Experimental results showed that the proposed method outperforms\nthe other seven state-of-the-art methods in terms of visual and quantitative\nmetrics and can also deal with complex backgrounds.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 08:09:28 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Peng", "Jinye", ""], ["Wang", "Jiaxin", ""], ["Wang", "Jun", ""], ["Zhang", "Erlei", ""], ["Zhang", "Qunxi", ""], ["Zhang", "Yongqin", ""], ["Peng", "Xianlin", ""], ["Yu", "Kai", ""]]}, {"id": "2101.06634", "submitter": "Ardhendu Behera", "authors": "Ardhendu Behera, Zachary Wharton, Morteza Ghahremani, Swagat Kumar,\n  Nik Bessis", "title": "Regional Attention Network (RAN) for Head Pose and Fine-grained Gesture\n  Recognition", "comments": "This manuscript is the accepted version of the published paper in\n  IEEE Transaction on Affective Computing", "journal-ref": "IEEE Transaction on Affective Computing 2020", "doi": "10.1109/TAFFC.2020.3031841", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Affect is often expressed via non-verbal body language such as\nactions/gestures, which are vital indicators for human behaviors. Recent\nstudies on recognition of fine-grained actions/gestures in monocular images\nhave mainly focused on modeling spatial configuration of body parts\nrepresenting body pose, human-objects interactions and variations in local\nappearance. The results show that this is a brittle approach since it relies on\naccurate body parts/objects detection. In this work, we argue that there exist\nlocal discriminative semantic regions, whose \"informativeness\" can be evaluated\nby the attention mechanism for inferring fine-grained gestures/actions. To this\nend, we propose a novel end-to-end \\textbf{Regional Attention Network (RAN)},\nwhich is a fully Convolutional Neural Network (CNN) to combine multiple\ncontextual regions through attention mechanism, focusing on parts of the images\nthat are most relevant to a given task. Our regions consist of one or more\nconsecutive cells and are adapted from the strategies used in computing HOG\n(Histogram of Oriented Gradient) descriptor. The model is extensively evaluated\non ten datasets belonging to 3 different scenarios: 1) head pose recognition,\n2) drivers state recognition, and 3) human action and facial expression\nrecognition. The proposed approach outperforms the state-of-the-art by a\nconsiderable margin in different metrics.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:14:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Behera", "Ardhendu", ""], ["Wharton", "Zachary", ""], ["Ghahremani", "Morteza", ""], ["Kumar", "Swagat", ""], ["Bessis", "Nik", ""]]}, {"id": "2101.06635", "submitter": "Ardhendu Behera", "authors": "Ardhendu Behera, Zachary Wharton, Pradeep Hewage, Asish Bera", "title": "Context-aware Attentional Pooling (CAP) for Fine-grained Visual\n  Classification", "comments": "Extended version of the accepted paper in 35th AAAI Conference on\n  Artificial Intelligence 2021", "journal-ref": "35th AAAI Conference on Artificial Intelligence 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks (CNNs) have shown a strong ability in\nmining discriminative object pose and parts information for image recognition.\nFor fine-grained recognition, context-aware rich feature representation of\nobject/scene plays a key role since it exhibits a significant variance in the\nsame subcategory and subtle variance among different subcategories. Finding the\nsubtle variance that fully characterizes the object/scene is not\nstraightforward. To address this, we propose a novel context-aware attentional\npooling (CAP) that effectively captures subtle changes via sub-pixel gradients,\nand learns to attend informative integral regions and their importance in\ndiscriminating different subcategories without requiring the bounding-box\nand/or distinguishable part annotations. We also introduce a novel feature\nencoding by considering the intrinsic consistency between the informativeness\nof the integral regions and their spatial structures to capture the semantic\ncorrelation among them. Our approach is simple yet extremely effective and can\nbe easily applied on top of a standard classification backbone network. We\nevaluate our approach using six state-of-the-art (SotA) backbone networks and\neight benchmark datasets. Our method significantly outperforms the SotA\napproaches on six datasets and is very competitive with the remaining two.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:15:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Behera", "Ardhendu", ""], ["Wharton", "Zachary", ""], ["Hewage", "Pradeep", ""], ["Bera", "Asish", ""]]}, {"id": "2101.06636", "submitter": "Ardhendu Behera", "authors": "Zachary Wharton, Ardhendu Behera, Yonghuai Liu, Nik Bessis", "title": "Coarse Temporal Attention Network (CTA-Net) for Driver's Activity\n  Recognition", "comments": "Extended version of the accepted WACV 2021", "journal-ref": "Winter Conference on Applications of Computer Vision (WACV 2021)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is significant progress in recognizing traditional human activities\nfrom videos focusing on highly distinctive actions involving discriminative\nbody movements, body-object and/or human-human interactions. Driver's\nactivities are different since they are executed by the same subject with\nsimilar body parts movements, resulting in subtle changes. To address this, we\npropose a novel framework by exploiting the spatiotemporal attention to model\nthe subtle changes. Our model is named Coarse Temporal Attention Network\n(CTA-Net), in which coarse temporal branches are introduced in a trainable\nglimpse network. The goal is to allow the glimpse to capture high-level\ntemporal relationships, such as 'during', 'before' and 'after' by focusing on a\nspecific part of a video. These branches also respect the topology of the\ntemporal dynamics in the video, ensuring that different branches learn\nmeaningful spatial and temporal changes. The model then uses an innovative\nattention mechanism to generate high-level action specific contextual\ninformation for activity recognition by exploring the hidden states of an LSTM.\nThe attention mechanism helps in learning to decide the importance of each\nhidden state for the recognition task by weighing them when constructing the\nrepresentation of the video. Our approach is evaluated on four publicly\naccessible datasets and significantly outperforms the state-of-the-art by a\nconsiderable margin with only RGB video as input.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:15:37 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wharton", "Zachary", ""], ["Behera", "Ardhendu", ""], ["Liu", "Yonghuai", ""], ["Bessis", "Nik", ""]]}, {"id": "2101.06639", "submitter": "Saehyung Lee", "authors": "Saehyung Lee, Changhwa Park, Hyungyu Lee, Jihun Yi, Jonghyun Lee,\n  Sungroh Yoon", "title": "Removing Undesirable Feature Contributions Using Out-of-Distribution\n  Data", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several data augmentation methods deploy unlabeled-in-distribution (UID) data\nto bridge the gap between the training and inference of neural networks.\nHowever, these methods have clear limitations in terms of availability of UID\ndata and dependence of algorithms on pseudo-labels. Herein, we propose a data\naugmentation method to improve generalization in both adversarial and standard\nlearning by using out-of-distribution (OOD) data that are devoid of the\nabovementioned issues. We show how to improve generalization theoretically\nusing OOD data in each learning scenario and complement our theoretical\nanalysis with experiments on CIFAR-10, CIFAR-100, and a subset of ImageNet. The\nresults indicate that undesirable features are shared even among image data\nthat seem to have little correlation from a human point of view. We also\npresent the advantages of the proposed method through comparison with other\ndata augmentation methods, which can be used in the absence of UID data.\nFurthermore, we demonstrate that the proposed method can further improve the\nexisting state-of-the-art adversarial training.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:26:34 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 05:40:51 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Lee", "Saehyung", ""], ["Park", "Changhwa", ""], ["Lee", "Hyungyu", ""], ["Yi", "Jihun", ""], ["Lee", "Jonghyun", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2101.06644", "submitter": "Theophile Sautory", "authors": "Theophile Sautory, Nuri Cingillioglu, Alessandra Russo", "title": "HySTER: A Hybrid Spatio-Temporal Event Reasoner", "comments": "Preprint accepted by the 35th AAAI Conference on Artificial\n  Intelligence (AAAI-21) Workshop on Hybrid Artificial Intelligence (HAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of Video Question Answering (VideoQA) consists in answering natural\nlanguage questions about a video and serves as a proxy to evaluate the\nperformance of a model in scene sequence understanding. Most methods designed\nfor VideoQA up-to-date are end-to-end deep learning architectures which\nstruggle at complex temporal and causal reasoning and provide limited\ntransparency in reasoning steps. We present the HySTER: a Hybrid\nSpatio-Temporal Event Reasoner to reason over physical events in videos. Our\nmodel leverages the strength of deep learning methods to extract information\nfrom video frames with the reasoning capabilities and explainability of\nsymbolic artificial intelligence in an answer set programming framework. We\ndefine a method based on general temporal, causal and physics rules which can\nbe transferred across tasks. We apply our model to the CLEVRER dataset and\ndemonstrate state-of-the-art results in question answering accuracy. This work\nsets the foundations for the incorporation of inductive logic programming in\nthe field of VideoQA.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 11:07:17 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Sautory", "Theophile", ""], ["Cingillioglu", "Nuri", ""], ["Russo", "Alessandra", ""]]}, {"id": "2101.06650", "submitter": "Liang Liao", "authors": "Liang Liao, Xuechun Zhang, Xinqiang Wang, Sen Lin, Xin Liu", "title": "Generalized Image Reconstruction over T-Algebra", "comments": "6 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is well known for its capability of\ndimension reduction and data compression. However, when using PCA for\ncompressing/reconstructing images, images need to be recast to vectors. The\nvectorization of images makes some correlation constraints of neighboring\npixels and spatial information lost. To deal with the drawbacks of the\nvectorizations adopted by PCA, we used small neighborhoods of each pixel to\nform compounded pixels and use a tensorial version of PCA, called TPCA\n(Tensorial Principal Component Analysis), to compress and reconstruct a\ncompounded image of compounded pixels. Our experiments on public data show that\nTPCA compares favorably with PCA in compressing and reconstructing images. We\nalso show in our experiments that the performance of TPCA increases when the\norder of compounded pixels increases.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 11:44:50 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 01:42:02 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 14:51:12 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liao", "Liang", ""], ["Zhang", "Xuechun", ""], ["Wang", "Xinqiang", ""], ["Lin", "Sen", ""], ["Liu", "Xin", ""]]}, {"id": "2101.06653", "submitter": "Wenyuan Zeng", "authors": "Wenyuan Zeng, Ming Liang, Renjie Liao, Raquel Urtasun", "title": "LaneRCNN: Distributed Representations for Graph-Centric Motion\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting the future behaviors of dynamic actors is an important task in\nmany robotics applications such as self-driving. It is extremely challenging as\nactors have latent intentions and their trajectories are governed by complex\ninteractions between the other actors, themselves, and the maps. In this paper,\nwe propose LaneRCNN, a graph-centric motion forecasting model. Importantly,\nrelying on a specially designed graph encoder, we learn a local lane graph\nrepresentation per actor (LaneRoI) to encode its past motions and the local map\ntopology. We further develop an interaction module which permits efficient\nmessage passing among local graph representations within a shared global lane\ngraph. Moreover, we parameterize the output trajectories based on lane graphs,\na more amenable prediction parameterization. Our LaneRCNN captures the\nactor-to-actor and the actor-to-map relations in a distributed and map-aware\nmanner. We demonstrate the effectiveness of our approach on the large-scale\nArgoverse Motion Forecasting Benchmark. We achieve the 1st place on the\nleaderboard and significantly outperform previous best results.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 11:54:49 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zeng", "Wenyuan", ""], ["Liang", "Ming", ""], ["Liao", "Renjie", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06658", "submitter": "Zhiwu Huang", "authors": "Yan Wu, Zhiwu Huang, Suryansh Kumar, Rhea Sanjay Sukthanker, Radu\n  Timofte, Luc Van Gool", "title": "Trilevel Neural Architecture Search for Efficient Single Image\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern solutions to the single image super-resolution (SISR) problem using\ndeep neural networks aim not only at better performance accuracy but also at a\nlighter and computationally efficient model. To that end, recently, neural\narchitecture search (NAS) approaches have shown some tremendous potential.\nFollowing the same underlying, in this paper, we suggest a novel trilevel NAS\nmethod that provides a better balance between different efficiency metrics and\nperformance to solve SISR. Unlike available NAS, our search is more complete,\nand therefore it leads to an efficient, optimized, and compressed architecture.\nWe innovatively introduce a trilevel search space modeling, i.e., hierarchical\nmodeling on network-, cell-, and kernel-level structures. To make the search on\ntrilevel spaces differentiable and efficient, we exploit a new sparsestmax\ntechnique that is excellent at generating sparse distributions of individual\nneural architecture candidates so that they can be better disentangled for the\nfinal selection from the enlarged search space. We further introduce the\nsorting technique to the sparsestmax relaxation for better network-level\ncompression. The proposed NAS optimization additionally facilitates\nsimultaneous search and training in a single phase, reducing search time and\ntrain time. Comprehensive evaluations on the benchmark datasets show our\nmethod's clear superiority over the state-of-the-art NAS in terms of a good\ntrade-off between model size, performance, and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 12:19:49 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 15:50:09 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wu", "Yan", ""], ["Huang", "Zhiwu", ""], ["Kumar", "Suryansh", ""], ["Sukthanker", "Rhea Sanjay", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "2101.06663", "submitter": "Champagne Jin", "authors": "Shuangping Jin, Zhenhua Feng, Wankou Yang, Josef Kittler", "title": "Separable Batch Normalization for Robust Facial Landmark Localization\n  with Cross-protocol Network Training", "comments": "10 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A big, diverse and balanced training data is the key to the success of deep\nneural network training. However, existing publicly available datasets used in\nfacial landmark localization are usually much smaller than those for other\ncomputer vision tasks. A small dataset without diverse and balanced training\nsamples cannot support the training of a deep network effectively. To address\nthe above issues, this paper presents a novel Separable Batch Normalization\n(SepBN) module with a Cross-protocol Network Training (CNT) strategy for robust\nfacial landmark localization. Different from the standard BN layer that uses\nall the training data to calculate a single set of parameters, SepBN considers\nthat the samples of a training dataset may belong to different sub-domains.\nAccordingly, the proposed SepBN module uses multiple sets of parameters, each\ncorresponding to a specific sub-domain. However, the selection of an\nappropriate branch in the inference stage remains a challenging task because\nthe sub-domain of a test sample is unknown. To mitigate this difficulty, we\npropose a novel attention mechanism that assigns different weights to each\nbranch for automatic selection in an effective style. As a further innovation,\nthe proposed CNT strategy trains a network using multiple datasets having\ndifferent facial landmark annotation systems, boosting the performance and\nenhancing the generalization capacity of the trained network. The experimental\nresults obtained on several well-known datasets demonstrate the effectiveness\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 13:04:06 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Jin", "Shuangping", ""], ["Feng", "Zhenhua", ""], ["Yang", "Wankou", ""], ["Kittler", "Josef", ""]]}, {"id": "2101.06679", "submitter": "Wenyuan Zeng", "authors": "Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio\n  Casas, Raquel Urtasun", "title": "End-to-end Interpretable Neural Motion Planner", "comments": "CVPR 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a neural motion planner (NMP) for learning to drive\nautonomously in complex urban scenarios that include traffic-light handling,\nyielding, and interactions with multiple road-users. Towards this goal, we\ndesign a holistic model that takes as input raw LIDAR data and a HD map and\nproduces interpretable intermediate representations in the form of 3D\ndetections and their future trajectories, as well as a cost volume defining the\ngoodness of each position that the self-driving car can take within the\nplanning horizon. We then sample a set of diverse physically possible\ntrajectories and choose the one with the minimum learned cost. Importantly, our\ncost volume is able to naturally capture multi-modality. We demonstrate the\neffectiveness of our approach in real-world driving data captured in several\ncities in North America. Our experiments show that the learned cost volume can\ngenerate safer planning than all the baselines.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 14:16:12 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zeng", "Wenyuan", ""], ["Luo", "Wenjie", ""], ["Suo", "Simon", ""], ["Sadat", "Abbas", ""], ["Yang", "Bin", ""], ["Casas", "Sergio", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06686", "submitter": "Po Hsiang Yu", "authors": "Po-Hsiang Yu, Sih-Sian Wu and Liang-Gee Chen", "title": "KCP: Kernel Cluster Pruning for Dense Labeling Neural Networks", "comments": "17 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning has become a promising technique used to compress and accelerate\nneural networks. Existing methods are mainly evaluated on spare labeling\napplications. However, dense labeling applications are those closer to real\nworld problems that require real-time processing on resource-constrained mobile\ndevices. Pruning for dense labeling applications is still a largely unexplored\nfield. The prevailing filter channel pruning method removes the entire filter\nchannel. Accordingly, the interaction between each kernel in one filter channel\nis ignored.\n  In this study, we proposed kernel cluster pruning (KCP) to prune dense\nlabeling networks. We developed a clustering technique to identify the least\nrepresentational kernels in each layer. By iteratively removing those kernels,\nthe parameter that can better represent the entire network is preserved; thus,\nwe achieve better accuracy with a decent model size and computation reduction.\nWhen evaluated on stereo matching and semantic segmentation neural networks,\nour method can reduce more than 70% of FLOPs with less than 1% of accuracy\ndrop. Moreover, for ResNet-50 on ILSVRC-2012, our KCP can reduce more than 50%\nof FLOPs reduction with 0.13% Top-1 accuracy gain. Therefore, KCP achieves\nstate-of-the-art pruning results.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 14:59:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yu", "Po-Hsiang", ""], ["Wu", "Sih-Sian", ""], ["Chen", "Liang-Gee", ""]]}, {"id": "2101.06702", "submitter": "Dachuan Shi", "authors": "Dachuan Shi, Eldar Sabanovic, Luca Rizzetto, Viktor Skrickij, Roberto\n  Oliverio, Nadia Kaviani, Yunguang Ye, Gintautas Bureika, Stefano Ricci,\n  Markus Hecht", "title": "Deep Learning based Virtual Point Tracking for Real-Time Target-less\n  Dynamic Displacement Measurement in Railway Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the application of computer-vision based displacement measurement, an\noptical target is usually required to prove the reference. In the case that the\noptical target cannot be attached to the measuring objective, edge detection,\nfeature matching and template matching are the most common approaches in\ntarget-less photogrammetry. However, their performance significantly relies on\nparameter settings. This becomes problematic in dynamic scenes where\ncomplicated background texture exists and varies over time. To tackle this\nissue, we propose virtual point tracking for real-time target-less dynamic\ndisplacement measurement, incorporating deep learning techniques and domain\nknowledge. Our approach consists of three steps: 1) automatic calibration for\ndetection of region of interest; 2) virtual point detection for each video\nframe using deep convolutional neural network; 3) domain-knowledge based rule\nengine for point tracking in adjacent frames. The proposed approach can be\nexecuted on an edge computer in a real-time manner (i.e. over 30 frames per\nsecond). We demonstrate our approach for a railway application, where the\nlateral displacement of the wheel on the rail is measured during operation. We\nalso implement an algorithm using template matching and line detection as the\nbaseline for comparison. The numerical experiments have been performed to\nevaluate the performance and the latency of our approach in the harsh railway\nenvironment with noisy and varying backgrounds.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 16:19:47 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 23:02:37 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Shi", "Dachuan", ""], ["Sabanovic", "Eldar", ""], ["Rizzetto", "Luca", ""], ["Skrickij", "Viktor", ""], ["Oliverio", "Roberto", ""], ["Kaviani", "Nadia", ""], ["Ye", "Yunguang", ""], ["Bureika", "Gintautas", ""], ["Ricci", "Stefano", ""], ["Hecht", "Markus", ""]]}, {"id": "2101.06704", "submitter": "Xingjun Ma", "authors": "Nodens Koren, Qiuhong Ke, Yisen Wang, James Bailey, Xingjun Ma", "title": "Adversarial Interaction Attack: Fooling AI to Misinterpret Human\n  Intentions", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the actions of both humans and artificial intelligence (AI)\nagents is important before modern AI systems can be fully integrated into our\ndaily life. In this paper, we show that, despite their current huge success,\ndeep learning based AI systems can be easily fooled by subtle adversarial noise\nto misinterpret the intention of an action in interaction scenarios. Based on a\ncase study of skeleton-based human interactions, we propose a novel adversarial\nattack on interactions, and demonstrate how DNN-based interaction models can be\ntricked to predict the participants' reactions in unexpected ways. From a\nbroader perspective, the scope of our proposed attack method is not confined to\nproblems related to skeleton data but can also be extended to any type of\nproblems involving sequential regressions. Our study highlights potential risks\nin the interaction loop with AI and humans, which need to be carefully\naddressed when deploying AI systems in safety-critical applications.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 16:23:20 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Koren", "Nodens", ""], ["Ke", "Qiuhong", ""], ["Wang", "Yisen", ""], ["Bailey", "James", ""], ["Ma", "Xingjun", ""]]}, {"id": "2101.06709", "submitter": "Niloy Sikder", "authors": "Niloy Sikder, Md. Sanaullah Chowdhury, Abu Shamim Mohammad Arif,\n  Abdullah-Al Nahid", "title": "Human Activity Recognition Using Multichannel Convolutional Neural\n  Network", "comments": "10 pages, Proceedings of the 2019 5th International Conference on\n  Advances in Electrical Engineering (ICAEE), 26-28 September, Dhaka,\n  Bangladesh", "journal-ref": null, "doi": "10.1109/ICAEE48663.2019.8975649", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human Activity Recognition (HAR) simply refers to the capacity of a machine\nto perceive human actions. HAR is a prominent application of advanced Machine\nLearning and Artificial Intelligence techniques that utilize computer vision to\nunderstand the semantic meanings of heterogeneous human actions. This paper\ndescribes a supervised learning method that can distinguish human actions based\non data collected from practical human movements. The primary challenge while\nworking with HAR is to overcome the difficulties that come with the\ncyclostationary nature of the activity signals. This study proposes a HAR\nclassification model based on a two-channel Convolutional Neural Network (CNN)\nthat makes use of the frequency and power features of the collected human\naction signals. The model was tested on the UCI HAR dataset, which resulted in\na 95.25% classification accuracy. This approach will help to conduct further\nresearches on the recognition of human activities based on their biomedical\nsignals.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 16:48:17 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Sikder", "Niloy", ""], ["Chowdhury", "Md. Sanaullah", ""], ["Arif", "Abu Shamim Mohammad", ""], ["Nahid", "Abdullah-Al", ""]]}, {"id": "2101.06715", "submitter": "Niloy Sikder", "authors": "Niloy Sikder, Abu Shamim Mohammad Arif, Abdullah-Al Nahid", "title": "Heterogeneous Hand Guise Classification Based on Surface\n  Electromyographic Signals Using Multichannel Convolutional Neural Network", "comments": "10 pages, 2019 22nd International Conference of Computer and\n  Information Technology (ICCIT), 18-20 December, 2019", "journal-ref": null, "doi": "10.1109/ICCIT48885.2019.9038173", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electromyography (EMG) is a way of measuring the bioelectric activities that\ntake place inside the muscles. EMG is usually performed to detect abnormalities\nwithin the nerves or muscles of a target area. The recent developments in the\nfield of Machine Learning allow us to use EMG signals to teach machines the\ncomplex properties of human movements. Modern machines are capable of detecting\nnumerous human activities and distinguishing among them solely based on the EMG\nsignals produced by those activities. However, success in accomplishing this\ntask mostly depends on the learning technique used by the machine to analyze\nEMG signals; and even the latest algorithms do not result in flawless\nclassification. In this study, a novel classification method has been described\nemploying a multichannel Convolutional Neural Network (CNN) that interprets\nsurface EMG signals by the properties they exhibit in the power domain. The\nproposed method was tested on a well-established EMG dataset, and the result\nyields very high classification accuracy. This learning model will help\nresearchers to develop prosthetic arms capable of detecting various hand\ngestures to mimic them afterwards.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 17:02:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Sikder", "Niloy", ""], ["Arif", "Abu Shamim Mohammad", ""], ["Nahid", "Abdullah-Al", ""]]}, {"id": "2101.06720", "submitter": "Julieta Martinez", "authors": "John Phillips, Julieta Martinez, Ioan Andrei B\\^arsan, Sergio Casas,\n  Abbas Sadat, Raquel Urtasun", "title": "Deep Multi-Task Learning for Joint Localization, Perception, and\n  Prediction", "comments": "CVPR 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, we have witnessed tremendous progress on many\nsubtasks of autonomous driving, including perception, motion forecasting, and\nmotion planning. However, these systems often assume that the car is accurately\nlocalized against a high-definition map. In this paper we question this\nassumption, and investigate the issues that arise in state-of-the-art autonomy\nstacks under localization error. Based on our observations, we design a system\nthat jointly performs perception, prediction, and localization. Our\narchitecture is able to reuse computation between both tasks, and is thus able\nto correct localization errors efficiently. We show experiments on a\nlarge-scale autonomy dataset, demonstrating the efficiency and accuracy of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 17:20:31 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 03:17:34 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 23:31:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Phillips", "John", ""], ["Martinez", "Julieta", ""], ["B\u00e2rsan", "Ioan Andrei", ""], ["Casas", "Sergio", ""], ["Sadat", "Abbas", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06742", "submitter": "Simon Suo", "authors": "Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, Raquel\n  Urtasun", "title": "Deep Parametric Continuous Convolutional Neural Networks", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": "10.1109/CVPR.2018.00274", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard convolutional neural networks assume a grid structured input is\navailable and exploit discrete convolutions as their fundamental building\nblocks. This limits their applicability to many real-world applications. In\nthis paper we propose Parametric Continuous Convolution, a new learnable\noperator that operates over non-grid structured data. The key idea is to\nexploit parameterized kernel functions that span the full continuous vector\nspace. This generalization allows us to learn over arbitrary data structures as\nlong as their support relationship is computable. Our experiments show\nsignificant improvement over the state-of-the-art in point cloud segmentation\nof indoor and outdoor scenes, and lidar motion estimation of driving scenes.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 18:28:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Shenlong", ""], ["Suo", "Simon", ""], ["Ma", "Wei-Chiu", ""], ["Pokrovsky", "Andrei", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06747", "submitter": "Mateus Roder", "authors": "Mateus Roder, Leandro A. Passos, Luiz Carlos Felix Ribeiro, Barbara\n  Caroline Benato, Alexandre Xavier Falc\\~ao, Jo\\~ao Paulo Papa", "title": "Intestinal Parasites Classification Using Deep Belief Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-61401-0_23", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, approximately $4$ billion people are infected by intestinal\nparasites worldwide. Diseases caused by such infections constitute a public\nhealth problem in most tropical countries, leading to physical and mental\ndisorders, and even death to children and immunodeficient individuals. Although\nsubjected to high error rates, human visual inspection is still in charge of\nthe vast majority of clinical diagnoses. In the past years, some works\naddressed intelligent computer-aided intestinal parasites classification, but\nthey usually suffer from misclassification due to similarities between\nparasites and fecal impurities. In this paper, we introduce Deep Belief\nNetworks to the context of automatic intestinal parasites classification.\nExperiments conducted over three datasets composed of eggs, larvae, and\nprotozoa provided promising results, even considering unbalanced classes and\nalso fecal impurities.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 18:47:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Roder", "Mateus", ""], ["Passos", "Leandro A.", ""], ["Ribeiro", "Luiz Carlos Felix", ""], ["Benato", "Barbara Caroline", ""], ["Falc\u00e3o", "Alexandre Xavier", ""], ["Papa", "Jo\u00e3o Paulo", ""]]}, {"id": "2101.06770", "submitter": "Qing Tian", "authors": "Qing Tian, Sampath Chanda, K C Amit Kumar, Douglas Gray", "title": "Improving Apparel Detection with Category Grouping and Multi-grained\n  Branches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training an accurate object detector is expensive and time-consuming. One\nmain reason lies in the laborious labeling process, i.e., annotating category\nand bounding box information for all instances in every image. In this paper,\nwe examine ways to improve performance of deep object detectors without extra\nlabeling. We first explore to group existing categories of high visual and\nsemantic similarities together as one super category (or, a superclass). Then,\nwe study how this knowledge of hierarchical categories can be exploited to\nbetter detect object using multi-grained RCNN top branches. Experimental\nresults on DeepFashion2 and OpenImagesV4-Clothing reveal that the proposed\ndetection heads with multi-grained branches can boost the overall performance\nby 2.3 mAP for DeepFashion2 and 2.5 mAP for OpenImagesV4-Clothing with no\nadditional time-consuming annotations. More importantly, classes that have\nfewer training samples tend to benefit more from the proposed multi-grained\nheads with superclass grouping. In particular, we improve the mAP for last 30%\ncategories (in terms of training sample number) by 2.6 and 4.6 for DeepFashion2\nand OpenImagesV4-Clothing, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 20:14:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Tian", "Qing", ""], ["Chanda", "Sampath", ""], ["Kumar", "K C Amit", ""], ["Gray", "Douglas", ""]]}, {"id": "2101.06771", "submitter": "Wang Zejin", "authors": "Zejin Wang, Guodong Sun, Lina Zhang, Guoqing Li, Hua Han", "title": "Temporal Spatial-Adaptive Interpolation with Deformable Refinement for\n  Electron Microscopic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, flow-based methods have achieved promising success in video frame\ninterpolation. However, electron microscopic (EM) images suffer from unstable\nimage quality, low PSNR, and disorderly deformation. Existing flow-based\ninterpolation methods cannot precisely compute optical flow for EM images since\nonly predicting each position's unique offset. To overcome these problems, we\npropose a novel interpolation framework for EM images that progressively\nsynthesizes interpolated features in a coarse-to-fine manner. First, we extract\nmissing intermediate features by the proposed temporal spatial-adaptive (TSA)\ninterpolation module. The TSA interpolation module aggregates temporal contexts\nand then adaptively samples the spatial-related features with the proposed\nresidual spatial adaptive block. Second, we introduce a stacked deformable\nrefinement block (SDRB) further enhance the reconstruction quality, which is\naware of the matching positions and relevant features from input frames with\nthe feedback mechanism. Experimental results demonstrate the superior\nperformance of our approach compared to previous works, both quantitatively and\nqualitatively.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 20:22:52 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Zejin", ""], ["Sun", "Guodong", ""], ["Zhang", "Lina", ""], ["Li", "Guoqing", ""], ["Han", "Hua", ""]]}, {"id": "2101.06772", "submitter": "Christopher Vogelsanger", "authors": "Christopher Vogelsanger and Christian Federau", "title": "Latent Space Analysis of VAE and Intro-VAE applied to 3-dimensional MR\n  Brain Volumes of Multiple Sclerosis, Leukoencephalopathy, and Healthy\n  Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple Sclerosis (MS) and microvascular leukoencephalopathy are two\ndistinct neurological conditions, the first caused by focal autoimmune\ninflammation in the central nervous system, the second caused by chronic white\nmatter damage from atherosclerotic microvascular disease. Both conditions lead\nto signal anomalies on Fluid Attenuated Inversion Recovery (FLAIR) magnetic\nresonance (MR) images, which can be distinguished by an expert\nneuroradiologist, but which can look very similar to the untrained eye as well\nas in the early stage of both diseases. In this paper, we attempt to train a\n3-dimensional deep neural network to learn the specific features of both\ndiseases in an unsupervised manner. For this manner, in a first step we train a\ngenerative neural network to create artificial MR images of both conditions\nwith approximate explicit density, using a mixed dataset of multiple sclerosis,\nleukoencephalopathy and healthy patients containing in total 5404 volumes of\n3096 patients. In a second step, we distinguish features between the different\ndiseases in the latent space of this network, and use them to classify new\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 20:31:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Vogelsanger", "Christopher", ""], ["Federau", "Christian", ""]]}, {"id": "2101.06773", "submitter": "Adria Ruiz", "authors": "Adria Ruiz, Antonio Agudo and Francesc Moreno", "title": "Generating Attribution Maps with Disentangled Masked Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribution map visualization has arisen as one of the most effective\ntechniques to understand the underlying inference process of Convolutional\nNeural Networks. In this task, the goal is to compute an score for each image\npixel related with its contribution to the final network output. In this paper,\nwe introduce Disentangled Masked Backpropagation (DMBP), a novel gradient-based\nmethod that leverages on the piecewise linear nature of ReLU networks to\ndecompose the model function into different linear mappings. This decomposition\naims to disentangle the positive, negative and nuisance factors from the\nattribution maps by learning a set of variables masking the contribution of\neach filter during back-propagation. A thorough evaluation over standard\narchitectures (ResNet50 and VGG16) and benchmark datasets (PASCAL VOC and\nImageNet) demonstrates that DMBP generates more visually interpretable\nattribution maps than previous approaches. Additionally, we quantitatively show\nthat the maps produced by our method are more consistent with the true\ncontribution of each pixel to the final network output.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 20:32:14 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ruiz", "Adria", ""], ["Agudo", "Antonio", ""], ["Moreno", "Francesc", ""]]}, {"id": "2101.06775", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Fangxu Xing, Chao Yang, C.-C. Jay Kuo, Georges ElFakhri,\n  Jonghye Woo", "title": "Symmetric-Constrained Irregular Structure Inpainting for Brain MRI\n  Registration with Tumor Pathology", "comments": "Published at MICCAI Brainles 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deformable registration of magnetic resonance images between patients with\nbrain tumors and healthy subjects has been an important tool to specify tumor\ngeometry through location alignment and facilitate pathological analysis. Since\ntumor region does not match with any ordinary brain tissue, it has been\ndifficult to deformably register a patients brain to a normal one. Many patient\nimages are associated with irregularly distributed lesions, resulting in\nfurther distortion of normal tissue structures and complicating registration's\nsimilarity measure. In this work, we follow a multi-step context-aware image\ninpainting framework to generate synthetic tissue intensities in the tumor\nregion. The coarse image-to-image translation is applied to make a rough\ninference of the missing parts. Then, a feature-level patch-match refinement\nmodule is applied to refine the details by modeling the semantic relevance\nbetween patch-wise features. A symmetry constraint reflecting a large degree of\nanatomical symmetry in the brain is further proposed to achieve better\nstructure understanding. Deformable registration is applied between inpainted\npatient images and normal brains, and the resulting deformation field is\neventually used to deform original patient data for the final alignment. The\nmethod was applied to the Multimodal Brain Tumor Segmentation (BraTS) 2018\nchallenge database and compared against three existing inpainting methods. The\nproposed method yielded results with increased peak signal-to-noise ratio,\nstructural similarity index, inception score, and reduced L1 error, leading to\nsuccessful patient-to-normal brain image registration.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 20:38:50 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Xing", "Fangxu", ""], ["Yang", "Chao", ""], ["Kuo", "C. -C. Jay", ""], ["ElFakhri", "Georges", ""], ["Woo", "Jonghye", ""]]}, {"id": "2101.06784", "submitter": "James Tu", "authors": "James Tu, Huichen Li, Xinchen Yan, Mengye Ren, Yun Chen, Ming Liang,\n  Eilyan Bitar, Ersin Yumer, Raquel Urtasun", "title": "Exploring Adversarial Robustness of Multi-Sensor Perception Systems in\n  Self Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern self-driving perception systems have been shown to improve upon\nprocessing complementary inputs such as LiDAR with images. In isolation, 2D\nimages have been found to be extremely vulnerable to adversarial attacks. Yet,\nthere have been limited studies on the adversarial robustness of multi-modal\nmodels that fuse LiDAR features with image features. Furthermore, existing\nworks do not consider physically realizable perturbations that are consistent\nacross the input modalities. In this paper, we showcase practical\nsusceptibilities of multi-sensor detection by placing an adversarial object on\ntop of a host vehicle. We focus on physically realizable and input-agnostic\nattacks as they are feasible to execute in practice, and show that a single\nuniversal adversary can hide different host vehicles from state-of-the-art\nmulti-modal detectors. Our experiments demonstrate that successful attacks are\nprimarily caused by easily corrupted image features. Furthermore, we find that\nin modern sensor fusion methods which project image features into 3D,\nadversarial attacks can exploit the projection process to generate false\npositives across distant regions in 3D. Towards more robust multi-modal\nperception systems, we show that adversarial training with feature denoising\ncan boost robustness to such attacks significantly. However, we find that\nstandard adversarial defenses still struggle to prevent false positives which\nare also caused by inaccurate associations between 3D LiDAR points and 2D\npixels.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 21:15:34 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 00:40:48 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Tu", "James", ""], ["Li", "Huichen", ""], ["Yan", "Xinchen", ""], ["Ren", "Mengye", ""], ["Chen", "Yun", ""], ["Liang", "Ming", ""], ["Bitar", "Eilyan", ""], ["Yumer", "Ersin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06806", "submitter": "Sergio Casas", "authors": "Sergio Casas, Abbas Sadat, Raquel Urtasun", "title": "MP3: A Unified Model to Map, Perceive, Predict and Plan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-definition maps (HD maps) are a key component of most modern\nself-driving systems due to their valuable semantic and geometric information.\nUnfortunately, building HD maps has proven hard to scale due to their cost as\nwell as the requirements they impose in the localization system that has to\nwork everywhere with centimeter-level accuracy. Being able to drive without an\nHD map would be very beneficial to scale self-driving solutions as well as to\nincrease the failure tolerance of existing ones (e.g., if localization fails or\nthe map is not up-to-date). Towards this goal, we propose MP3, an end-to-end\napproach to mapless driving where the input is raw sensor data and a high-level\ncommand (e.g., turn left at the intersection). MP3 predicts intermediate\nrepresentations in the form of an online map and the current and future state\nof dynamic agents, and exploits them in a novel neural motion planner to make\ninterpretable decisions taking into account uncertainty. We show that our\napproach is significantly safer, more comfortable, and can follow commands\nbetter than the baselines in challenging long-term closed-loop simulations, as\nwell as when compared to an expert driver in a large-scale real-world dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:09:30 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Casas", "Sergio", ""], ["Sadat", "Abbas", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06820", "submitter": "Uno Fang", "authors": "Uno Fang, Jianxin Li, Xuequan Lu, Mumtaz Ali, Longxiang Gao and Yong\n  Xiang", "title": "Chaotic-to-Fine Clustering for Unlabeled Plant Disease Images", "comments": "This paper has been submitted to Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current annotation for plant disease images depends on manual sorting and\nhandcrafted features by agricultural experts, which is time-consuming and\nlabour-intensive. In this paper, we propose a self-supervised clustering\nframework for grouping plant disease images based on the vulnerability of\nKernel K-means. The main idea is to establish a cross iterative\nunder-clustering algorithm based on Kernel K-means to produce the\npseudo-labeled training set and a chaotic cluster to be further classified by a\ndeep learning module. In order to verify the effectiveness of our proposed\nframework, we conduct extensive experiments on three different plant disease\ndatatsets with five plants and 17 plant diseases. The experimental results show\nthe high superiority of our method to do image-based plant disease\nclassification over balanced and unbalanced datasets by comparing with five\nstate-of-the-art existing works in terms of different metrics.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:44:12 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fang", "Uno", ""], ["Li", "Jianxin", ""], ["Lu", "Xuequan", ""], ["Ali", "Mumtaz", ""], ["Gao", "Longxiang", ""], ["Xiang", "Yong", ""]]}, {"id": "2101.06832", "submitter": "Jerry Liu", "authors": "Jerry Liu, Wenyuan Zeng, Raquel Urtasun, Ersin Yumer", "title": "Deep Structured Reactive Planning", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intelligent agent operating in the real-world must balance achieving its\ngoal with maintaining the safety and comfort of not only itself, but also other\nparticipants within the surrounding scene. This requires jointly reasoning\nabout the behavior of other actors while deciding its own actions as these two\nprocesses are inherently intertwined - a vehicle will yield to us if we decide\nto proceed first at the intersection but will proceed first if we decide to\nyield. However, this is not captured in most self-driving pipelines, where\nplanning follows prediction. In this paper we propose a novel data-driven,\nreactive planning objective which allows a self-driving vehicle to jointly\nreason about its own plans as well as how other actors will react to them. We\nformulate the problem as an energy-based deep structured model that is learned\nfrom observational data and encodes both the planning and prediction problems.\nThrough simulations based on both real-world driving and synthetically\ngenerated dense traffic, we demonstrate that our reactive model outperforms a\nnon-reactive variant in successfully completing highly complex maneuvers (lane\nmerges/turns in traffic) faster, without trading off collision rate.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 01:43:36 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 06:26:56 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Liu", "Jerry", ""], ["Zeng", "Wenyuan", ""], ["Urtasun", "Raquel", ""], ["Yumer", "Ersin", ""]]}, {"id": "2101.06848", "submitter": "Isaac Sledge", "authors": "Isaac J. Sledge and Jose C. Principe", "title": "Faster Convergence in Deep-Predictive-Coding Networks to Learn Deeper\n  Representations", "comments": "Submitted to IEEE TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep-predictive-coding networks (DPCNs) are hierarchical, generative models.\nThey rely on feed-forward and feed-back connections to modulate latent feature\nrepresentations of stimuli in a dynamic and context-sensitive manner. A crucial\nelement of DPCNs is a forward-backward inference procedure to uncover sparse,\ninvariant features. However, this inference is a major computational\nbottleneck. It severely limits the network depth due to learning stagnation.\nHere, we prove why this bottleneck occurs. We then propose a new\nforward-inference strategy based on accelerated proximal gradients. This\nstrategy has faster theoretical convergence guarantees than the one used for\nDPCNs. It overcomes learning stagnation. We also demonstrate that it permits\nconstructing deep and wide predictive-coding networks. Such convolutional\nnetworks implement receptive fields that capture well the entire classes of\nobjects on which the networks are trained. This improves the feature\nrepresentations compared with our lab's previous non-convolutional and\nconvolutional DPCNs. It yields unsupervised object recognition that surpass\nconvolutional autoencoders and are on par with convolutional networks trained\nin a supervised manner.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:30:13 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 07:03:20 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 21:52:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sledge", "Isaac J.", ""], ["Principe", "Jose C.", ""]]}, {"id": "2101.06849", "submitter": "Qi Ming", "authors": "Qi Ming, Lingjuan Miao, Zhiqiang Zhou, Yunpeng Dong", "title": "CFC-Net: A Critical Feature Capturing Network for Arbitrary-Oriented\n  Object Detection in Remote Sensing Images", "comments": "The code and models are available at\n  https://github.com/ming71/CFC-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in optical remote sensing images is an important and\nchallenging task. In recent years, the methods based on convolutional neural\nnetworks have made good progress. However, due to the large variation in object\nscale, aspect ratio, and arbitrary orientation, the detection performance is\ndifficult to be further improved. In this paper, we discuss the role of\ndiscriminative features in object detection, and then propose a Critical\nFeature Capturing Network (CFC-Net) to improve detection accuracy from three\naspects: building powerful feature representation, refining preset anchors, and\noptimizing label assignment. Specifically, we first decouple the classification\nand regression features, and then construct robust critical features adapted to\nthe respective tasks through the Polarization Attention Module (PAM). With the\nextracted discriminative regression features, the Rotation Anchor Refinement\nModule (R-ARM) performs localization refinement on preset horizontal anchors to\nobtain superior rotation anchors. Next, the Dynamic Anchor Learning (DAL)\nstrategy is given to adaptively select high-quality anchors based on their\nability to capture critical features. The proposed framework creates more\npowerful semantic representations for objects in remote sensing images and\nachieves high-performance real-time object detection. Experimental results on\nthree remote sensing datasets including HRSC2016, DOTA, and UCAS-AOD show that\nour method achieves superior detection performance compared with many\nstate-of-the-art approaches. Code and models are available at\nhttps://github.com/ming71/CFC-Net.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:31:09 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ming", "Qi", ""], ["Miao", "Lingjuan", ""], ["Zhou", "Zhiqiang", ""], ["Dong", "Yunpeng", ""]]}, {"id": "2101.06853", "submitter": "Lei Qi", "authors": "Xiaoting Han, Lei Qi, Qian Yu, Ziqi Zhou, Yefeng Zheng, Yinghuan Shi,\n  Yang Gao", "title": "Deep Symmetric Adaptation Network for Cross-modality Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) methods have shown their promising\nperformance in the cross-modality medical image segmentation tasks. These\ntypical methods usually utilize a translation network to transform images from\nthe source domain to target domain or train the pixel-level classifier merely\nusing translated source images and original target images. However, when there\nexists a large domain shift between source and target domains, we argue that\nthis asymmetric structure could not fully eliminate the domain gap. In this\npaper, we present a novel deep symmetric architecture of UDA for medical image\nsegmentation, which consists of a segmentation sub-network, and two symmetric\nsource and target domain translation sub-networks. To be specific, based on two\ntranslation sub-networks, we introduce a bidirectional alignment scheme via a\nshared encoder and private decoders to simultaneously align features 1) from\nsource to target domain and 2) from target to source domain, which helps\neffectively mitigate the discrepancy between domains. Furthermore, for the\nsegmentation sub-network, we train a pixel-level classifier using not only\noriginal target images and translated source images, but also original source\nimages and translated target images, which helps sufficiently leverage the\nsemantic information from the images with different styles. Extensive\nexperiments demonstrate that our method has remarkable advantages compared to\nthe state-of-the-art methods in both cross-modality Cardiac and BraTS\nsegmentation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:54:30 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Han", "Xiaoting", ""], ["Qi", "Lei", ""], ["Yu", "Qian", ""], ["Zhou", "Ziqi", ""], ["Zheng", "Yefeng", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "2101.06860", "submitter": "Shivam Duggal", "authors": "Shivam Duggal, Zihao Wang, Wei-Chiu Ma, Sivabalan Manivasagam, Justin\n  Liang, Shenlong Wang and Raquel Urtasun", "title": "Secrets of 3D Implicit Object Shape Reconstruction in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing high-fidelity 3D objects from sparse, partial observation is\nof crucial importance for various applications in computer vision, robotics,\nand graphics. While recent neural implicit modeling methods show promising\nresults on synthetic or dense datasets, they perform poorly on real-world data\nthat is sparse and noisy. This paper analyzes the root cause of such deficient\nperformance of a popular neural implicit model. We discover that the\nlimitations are due to highly complicated objectives, lack of regularization,\nand poor initialization. To overcome these issues, we introduce two simple yet\neffective modifications: (i) a deep encoder that provides a better and more\nstable initialization for latent code optimization; and (ii) a deep\ndiscriminator that serves as a prior model to boost the fidelity of the shape.\nWe evaluate our approach on two real-wold self-driving datasets and show\nsuperior performance over state-of-the-art 3D object reconstruction methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 03:24:48 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Duggal", "Shivam", ""], ["Wang", "Zihao", ""], ["Ma", "Wei-Chiu", ""], ["Manivasagam", "Sivabalan", ""], ["Liang", "Justin", ""], ["Wang", "Shenlong", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06865", "submitter": "Min Bai", "authors": "Min Bai, Shenlong Wang, Kelvin Wong, Ersin Yumer, Raquel Urtasun", "title": "Non-parametric Memory for Spatio-Temporal Segmentation of Construction\n  Zones for Self-Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a non-parametric memory representation for\nspatio-temporal segmentation that captures the local space and time around an\nautonomous vehicle (AV). Our representation has three important properties: (i)\nit remembers what it has seen in the past, (ii) it reinforces and (iii) forgets\nits past beliefs based on new evidence. Reinforcing is important as the first\ntime we see an element we might be uncertain, e.g, if the element is heavily\noccluded or at range. Forgetting is desirable, as otherwise false positives\nwill make the self driving vehicle behave erratically. Our process is informed\nby 3D reasoning, as occlusion is key to distinguishing between the desire to\nforget and to remember. We show how our method can be used as an online\ncomponent to complement static world representations such as HD maps by\ndetecting and remembering changes that should be superimposed on top of this\nstatic view due to such events.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 04:02:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Bai", "Min", ""], ["Wang", "Shenlong", ""], ["Wong", "Kelvin", ""], ["Yumer", "Ersin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06871", "submitter": "Pranav Rajpurkar", "authors": "Alexander Ke, William Ellsworth, Oishi Banerjee, Andrew Y. Ng, Pranav\n  Rajpurkar", "title": "CheXtransfer: Performance and Parameter Efficiency of ImageNet Models\n  for Chest X-Ray Interpretation", "comments": null, "journal-ref": null, "doi": "10.1145/3450439.3451867", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning methods for chest X-ray interpretation typically rely on\npretrained models developed for ImageNet. This paradigm assumes that better\nImageNet architectures perform better on chest X-ray tasks and that\nImageNet-pretrained weights provide a performance boost over random\ninitialization. In this work, we compare the transfer performance and parameter\nefficiency of 16 popular convolutional architectures on a large chest X-ray\ndataset (CheXpert) to investigate these assumptions. First, we find no\nrelationship between ImageNet performance and CheXpert performance for both\nmodels without pretraining and models with pretraining. Second, we find that,\nfor models without pretraining, the choice of model family influences\nperformance more than size within a family for medical imaging tasks. Third, we\nobserve that ImageNet pretraining yields a statistically significant boost in\nperformance across architectures, with a higher boost for smaller\narchitectures. Fourth, we examine whether ImageNet architectures are\nunnecessarily large for CheXpert by truncating final blocks from pretrained\nmodels, and find that we can make models 3.25x more parameter-efficient on\naverage without a statistically significant drop in performance. Our work\ncontributes new experimental evidence about the relation of ImageNet to chest\nx-ray interpretation performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 04:48:24 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 02:06:43 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ke", "Alexander", ""], ["Ellsworth", "William", ""], ["Banerjee", "Oishi", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2101.06894", "submitter": "Antoni Rosinol", "authors": "Antoni Rosinol, Andrew Violette, Marcus Abate, Nathan Hughes, Yun\n  Chang, Jingnan Shi, Arjun Gupta, Luca Carlone", "title": "Kimera: from SLAM to Spatial Perception with 3D Dynamic Scene Graphs", "comments": "34 pages, 25 figures, 9 tables. arXiv admin note: text overlap with\n  arXiv:2002.06289", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans are able to form a complex mental model of the environment they move\nin. This mental model captures geometric and semantic aspects of the scene,\ndescribes the environment at multiple levels of abstractions (e.g., objects,\nrooms, buildings), includes static and dynamic entities and their relations\n(e.g., a person is in a room at a given time). In contrast, current robots'\ninternal representations still provide a partial and fragmented understanding\nof the environment, either in the form of a sparse or dense set of geometric\nprimitives (e.g., points, lines, planes, voxels) or as a collection of objects.\nThis paper attempts to reduce the gap between robot and human perception by\nintroducing a novel representation, a 3D Dynamic Scene Graph(DSG), that\nseamlessly captures metric and semantic aspects of a dynamic environment. A DSG\nis a layered graph where nodes represent spatial concepts at different levels\nof abstraction, and edges represent spatio-temporal relations among nodes. Our\nsecond contribution is Kimera, the first fully automatic method to build a DSG\nfrom visual-inertial data. Kimera includes state-of-the-art techniques for\nvisual-inertial SLAM, metric-semantic 3D reconstruction, object localization,\nhuman pose and shape estimation, and scene parsing. Our third contribution is a\ncomprehensive evaluation of Kimera in real-life datasets and photo-realistic\nsimulations, including a newly released dataset, uHumans2, which simulates a\ncollection of crowded indoor and outdoor scenes. Our evaluation shows that\nKimera achieves state-of-the-art performance in visual-inertial SLAM, estimates\nan accurate 3D metric-semantic mesh model in real-time, and builds a DSG of a\ncomplex indoor environment with tens of objects and humans in minutes. Our\nfinal contribution shows how to use a DSG for real-time hierarchical semantic\npath-planning. The core modules in Kimera are open-source.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 06:17:52 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 18:00:50 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rosinol", "Antoni", ""], ["Violette", "Andrew", ""], ["Abate", "Marcus", ""], ["Hughes", "Nathan", ""], ["Chang", "Yun", ""], ["Shi", "Jingnan", ""], ["Gupta", "Arjun", ""], ["Carlone", "Luca", ""]]}, {"id": "2101.06898", "submitter": "Shihao Zhao", "authors": "Shihao Zhao, Xingjun Ma, Yisen Wang, James Bailey, Bo Li, Yu-Gang\n  Jiang", "title": "What Do Deep Nets Learn? Class-wise Patterns Revealed in the Input Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are increasingly deployed in different\napplications to achieve state-of-the-art performance. However, they are often\napplied as a black box with limited understanding of what knowledge the model\nhas learned from the data. In this paper, we focus on image classification and\npropose a method to visualize and understand the class-wise knowledge\n(patterns) learned by DNNs under three different settings including natural,\nbackdoor and adversarial. Different to existing visualization methods, our\nmethod searches for a single predictive pattern in the pixel space to represent\nthe knowledge learned by the model for each class. Based on the proposed\nmethod, we show that DNNs trained on natural (clean) data learn abstract shapes\nalong with some texture, and backdoored models learn a suspicious pattern for\nthe backdoored class. Interestingly, the phenomenon that DNNs can learn a\nsingle predictive pattern for each class indicates that DNNs can learn a\nbackdoor even from clean data, and the pattern itself is a backdoor trigger. In\nthe adversarial setting, we show that adversarially trained models tend to\nlearn more simplified shape patterns. Our method can serve as a useful tool to\nbetter understand the knowledge learned by DNNs on different datasets under\ndifferent settings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 06:38:41 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 05:09:40 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Zhao", "Shihao", ""], ["Ma", "Xingjun", ""], ["Wang", "Yisen", ""], ["Bailey", "James", ""], ["Li", "Bo", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2101.06910", "submitter": "Suranjan Goswami", "authors": "Suranjan Goswami, Satish Kumar Singh", "title": "A Novel Registration & Colorization Technique for Thermal to Cross\n  Domain Colorized Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thermal images can be obtained as either grayscale images or pseudo colored\nimages based on the thermal profile of the object being captured. We present a\nnovel registration method that works on images captured via multiple thermal\nimagers irrespective of make and internal resolution as well as a colorization\nscheme that can be used to obtain a colorized thermal image which is similar to\nan optical image, while retaining the information of the thermal profile as a\npart of the output, thus providing information of both domains jointly. We call\nthis a cross domain colorized image. We also outline a new public\nthermal-optical paired database that we are presenting as a part of this paper,\ncontaining unique data points obtained via multiple thermal imagers. Finally,\nwe compare the results with prior literature, show how our results are\ndifferent and discuss on some future work that can be explored further in this\ndomain as well.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 07:30:51 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Goswami", "Suranjan", ""], ["Singh", "Satish Kumar", ""]]}, {"id": "2101.06915", "submitter": "Praveen Damacharla", "authors": "Praveen Damacharla, Achuth Rao M. V., Jordan Ringenberg, and Ahmad Y\n  Javaid", "title": "TLU-Net: A Deep Learning Approach for Automatic Steel Surface Defect\n  Detection", "comments": null, "journal-ref": "International Conference on Applied Artificial Intelligence\n  (ICAPAI 2021), Halden, Norway, May 19-21, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual steel surface defect detection is an essential step in steel sheet\nmanufacturing. Several machine learning-based automated visual inspection (AVI)\nmethods have been studied in recent years. However, most steel manufacturing\nindustries still use manual visual inspection due to training time and\ninaccuracies involved with AVI methods. Automatic steel defect detection\nmethods could be useful in less expensive and faster quality control and\nfeedback. But preparing the annotated training data for segmentation and\nclassification could be a costly process. In this work, we propose to use the\nTransfer Learning-based U-Net (TLU-Net) framework for steel surface defect\ndetection. We use a U-Net architecture as the base and explore two kinds of\nencoders: ResNet and DenseNet. We compare these nets' performance using random\ninitialization and the pre-trained networks trained using the ImageNet data\nset. The experiments are performed using Severstal data. The results\ndemonstrate that the transfer learning performs 5% (absolute) better than that\nof the random initialization in defect classification. We found that the\ntransfer learning performs 26% (relative) better than that of the random\ninitialization in defect segmentation. We also found the gain of transfer\nlearning increases as the training data decreases, and the convergence rate\nwith transfer learning is better than that of the random initialization.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 07:53:20 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Damacharla", "Praveen", ""], ["V.", "Achuth Rao M.", ""], ["Ringenberg", "Jordan", ""], ["Javaid", "Ahmad Y", ""]]}, {"id": "2101.06931", "submitter": "Shi Xian", "authors": "Xian Shi, Xun Xu, Ke Chen, Lile Cai, Chuan Sheng Foo, Kui Jia", "title": "Label-Efficient Point Cloud Semantic Segmentation: An Active Learning\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are the state-of-the-art methods for semantic point\ncloud segmentation, the success of which relies on the availability of\nlarge-scale annotated datasets. However, it can be extremely time-consuming and\nprohibitively expensive to compile such datasets. In this work, we propose an\nactive learning approach to maximize model performance given limited annotation\nbudgets. We investigate the appropriate sample granularity for active selection\nunder realistic annotation cost measurement (clicks), and demonstrate that\nsuper-point based selection allows for more efficient usage of the limited\nbudget compared to point-level and instance-level selection. We further exploit\nlocal consistency constraints to boost the performance of the super-point based\napproach. We evaluate our methods on two benchmarking datasets (ShapeNet and\nS3DIS) and the results demonstrate that active learning is an effective\nstrategy to address the high annotation costs in semantic point cloud\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 08:37:21 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 15:13:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Shi", "Xian", ""], ["Xu", "Xun", ""], ["Chen", "Ke", ""], ["Cai", "Lile", ""], ["Foo", "Chuan Sheng", ""], ["Jia", "Kui", ""]]}, {"id": "2101.06958", "submitter": "Huang Ling", "authors": "Ling Huang, Su Ruan, Thierry Denoeux", "title": "Covid-19 classification with deep neural network and belief functions", "comments": "medical image, Covid-19, belief function, BIHI conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) image provides useful information for radiologists\nto diagnose Covid-19. However, visual analysis of CT scans is time-consuming.\nThus, it is necessary to develop algorithms for automatic Covid-19 detection\nfrom CT images. In this paper, we propose a belief function-based convolutional\nneural network with semi-supervised training to detect Covid-19 cases. Our\nmethod first extracts deep features, maps them into belief degree maps and\nmakes the final classification decision. Our results are more reliable and\nexplainable than those of traditional deep learning-based classification\nmodels. Experimental results show that our approach is able to achieve a good\nperformance with an accuracy of 0.81, an F1 of 0.812 and an AUC of 0.875.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 09:43:11 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Huang", "Ling", ""], ["Ruan", "Su", ""], ["Denoeux", "Thierry", ""]]}, {"id": "2101.06963", "submitter": "Taro Langner", "authors": "Taro Langner, Fredrik K. Gustafsson, Benny Avelin, Robin Strand,\n  H\\r{a}kan Ahlstr\\\"om, and Joel Kullberg", "title": "Uncertainty-Aware Body Composition Analysis with Deep Regression\n  Ensembles on UK Biobank MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Along with rich health-related metadata, an ongoing imaging study has\nacquired MRI of over 40,000 male and female UK Biobank participants aged 44-82\nsince 2014. Phenotypes derived from these images, such as measurements of body\ncomposition, can reveal new links between genetics, cardiovascular disease, and\nmetabolic conditions. In this retrospective study, six measurements of body\ncomposition were automatically estimated by ResNet50 neural networks for\nimage-based regression from neck-to-knee body MRI. Despite the potential for\nhigh speed and accuracy, these networks produce no output segmentations that\ncould indicate the reliability of individual measurements. The presented\nexperiments therefore examine mean-variance regression and ensembling for\npredictive uncertainty estimation, which can quantify individual measurement\nerrors and thereby help to identify potential outliers, anomalies, and other\nfailure cases automatically. In 10-fold cross-validation on data of about 8,500\nsubjects, mean-variance regression and ensembling showed complementary\nbenefits, reducing the mean absolute error across all predictions by 12%. Both\nimproved the calibration of uncertainties and their ability to identify high\nprediction errors. With intra-class correlation coefficients (ICC) above 0.97,\nall targets except the liver fat content yielded relative measurement errors\nbelow 5%. Testing on another 1,000 subjects showed consistent performance, and\nthe method was finally deployed for inference to 30,000 subjects with missing\nreference values. The results indicate that deep regression ensembles could\nultimately provide automated, uncertainty-aware measurements of body\ncomposition for more than 120,000 UK Biobank neck-to-knee body MRI that are to\nbe acquired within the coming years.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 09:59:20 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 14:25:20 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Langner", "Taro", ""], ["Gustafsson", "Fredrik K.", ""], ["Avelin", "Benny", ""], ["Strand", "Robin", ""], ["Ahlstr\u00f6m", "H\u00e5kan", ""], ["Kullberg", "Joel", ""]]}, {"id": "2101.06969", "submitter": "Zhengyan Zhang", "authors": "Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi,\n  Zhiyuan Liu, Yasheng Wang, Xin Jiang, Maosong Sun", "title": "Red Alarm for Pre-trained Models: Universal Vulnerability to\n  Neuron-Level Backdoor Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained models (PTMs) have been widely used in various downstream tasks.\nThe parameters of PTMs are distributed on the Internet and may suffer backdoor\nattacks. In this work, we demonstrate the universal vulnerability of PTMs,\nwhere fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary\ndownstream tasks. Specifically, attackers can add a simple pre-training task,\nwhich restricts the output representations of trigger instances to pre-defined\nvectors, namely neuron-level backdoor attack (NeuBA). If the backdoor\nfunctionality is not eliminated during fine-tuning, the triggers can make the\nfine-tuned model predict fixed labels by pre-defined vectors. In the\nexperiments of both natural language processing (NLP) and computer vision (CV),\nwe show that NeuBA absolutely controls the predictions for trigger instances\nwithout any knowledge of downstream tasks. Finally, we apply several defense\nmethods to NeuBA and find that model pruning is a promising direction to resist\nNeuBA by excluding backdoored neurons. Our findings sound a red alarm for the\nwide use of PTMs. Our source code and models are available at\n\\url{https://github.com/thunlp/NeuBA}.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:18:42 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 05:23:52 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 08:30:39 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhang", "Zhengyan", ""], ["Xiao", "Guangxuan", ""], ["Li", "Yongwei", ""], ["Lv", "Tian", ""], ["Qi", "Fanchao", ""], ["Liu", "Zhiyuan", ""], ["Wang", "Yasheng", ""], ["Jiang", "Xin", ""], ["Sun", "Maosong", ""]]}, {"id": "2101.06977", "submitter": "Aybora Koksal", "authors": "Kutalmis Gokalp Ince, Aybora Koksal, Arda Fazla, A. Aydin Alatan", "title": "Semi-Automatic Video Annotation For Object Detection", "comments": "Submitted to ICIP 2021. Resulting uav_detection_2 annotations and our\n  codes are publicly available at\n  https://github.com/aybora/Semi-Automatic-Video-Annotation-OGAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a semi-automatic video annotation method is proposed which\nutilizes temporal information to eliminate false-positives with a\ntracking-by-detection approach by employing multiple hypothesis tracking (MHT).\nMHT method automatically forms tracklets which are confirmed by human operators\nto enlarge the training set. A novel incremental learning approach helps to\nannotate videos in an iterative way. The experiments performed on AUTH\nMultidrone Dataset reveals that the annotation workload can be reduced up to\n96% by the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:35:32 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 13:20:59 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 09:49:20 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ince", "Kutalmis Gokalp", ""], ["Koksal", "Aybora", ""], ["Fazla", "Arda", ""], ["Alatan", "A. Aydin", ""]]}, {"id": "2101.06979", "submitter": "Vincent Couteaux", "authors": "Vincent Couteaux, Mathilde Trintignac, Olivier Nempont, Guillaume\n  Pizaine, Anna Sesilia Vlachomitrou, Pierre-Jean Valette, Laurent Milot,\n  Isabelle Bloch", "title": "Comparing Deep Learning strategies for paired but unregistered\n  multimodal segmentation of the liver in T1 and T2-weighted MRI", "comments": "4 pages, 3 figures and 3 tables. Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of multimodal liver segmentation in paired but\nunregistered T1 and T2-weighted MR images. We compare several strategies\ndescribed in the literature, with or without multi-task training, with or\nwithout pre-registration. We also compare different loss functions\n(cross-entropy, Dice loss, and three adversarial losses). All methods achieved\ncomparable performances with the exception of a multi-task setting that\nperforms both segmentations at once, which performed poorly.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:38:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Couteaux", "Vincent", ""], ["Trintignac", "Mathilde", ""], ["Nempont", "Olivier", ""], ["Pizaine", "Guillaume", ""], ["Vlachomitrou", "Anna Sesilia", ""], ["Valette", "Pierre-Jean", ""], ["Milot", "Laurent", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2101.07005", "submitter": "Marta Boche\\'nska", "authors": "Piotr E. Srokosz, Marcin Bujko, Marta Boche\\'nska and Rafa{\\l}\n  Ossowski", "title": "Optical Flow Method for Measuring Deformation of Soil Specimen Subjected\n  to Torsional Shearing", "comments": "To appear in Measurement", "journal-ref": "Measurement, Vol. 174 (2021)", "doi": "10.1016/j.measurement.2021.109064", "report-no": null, "categories": "cs.CE cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this study optical flow method was used for soil small deformation\nmeasurement in laboratory tests. The main objective was to observe how the\ndeformation distributes along the whole height of cylindrical soil specimen\nsubjected to torsional shearing (TS test). The experiments were conducted on\ndry non-cohesive soil specimens under two values of isotropic pressure.\nSpecimens were loaded with low-amplitude cyclic torque to analyze the\ndeformation within the small strain range (0.001-0.01%). Optical flow method\nvariant by Ce Liu (2009) was used for motion estimation from series of images.\nThis algorithm uses scale-invariant feature transform (SIFT) for image feature\nextraction and coarse-to-fine matching scheme for faster calculations. The\nresults were validated with the Particle Image Velocimetry (PIV). The results\nshow that the displacement distribution deviates from commonly assumed\nlinearity. Moreover, the observed deformation mechanisms analysis suggest that\nthe shear modulus $G$ commonly determined through TS tests can be considerably\noverestimated.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 11:12:46 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 08:46:18 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Srokosz", "Piotr E.", ""], ["Bujko", "Marcin", ""], ["Boche\u0144ska", "Marta", ""], ["Ossowski", "Rafa\u0142", ""]]}, {"id": "2101.07017", "submitter": "Jae Woong Soh", "authors": "Jae Woong Soh, Nam Ik Cho", "title": "Deep Universal Blind Image Denoising", "comments": "Presented in ICPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image denoising is an essential part of many image processing and computer\nvision tasks due to inevitable noise corruption during image acquisition.\nTraditionally, many researchers have investigated image priors for the\ndenoising, within the Bayesian perspective based on image properties and\nstatistics. Recently, deep convolutional neural networks (CNNs) have shown\ngreat success in image denoising by incorporating large-scale synthetic\ndatasets. However, they both have pros and cons. While the deep CNNs are\npowerful for removing the noise with known statistics, they tend to lack\nflexibility and practicality for the blind and real-world noise. Moreover, they\ncannot easily employ explicit priors. On the other hand, traditional\nnon-learning methods can involve explicit image priors, but they require\nconsiderable computation time and cannot exploit large-scale external datasets.\nIn this paper, we present a CNN-based method that leverages the advantages of\nboth methods based on the Bayesian perspective. Concretely, we divide the blind\nimage denoising problem into sub-problems and conquer each inference problem\nseparately. As the CNN is a powerful tool for inference, our method is rooted\nin CNNs and propose a novel design of network for efficient inference. With our\nproposed method, we can successfully remove blind and real-world noise, with a\nmoderate number of parameters of universal CNN.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 11:49:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Soh", "Jae Woong", ""], ["Cho", "Nam Ik", ""]]}, {"id": "2101.07034", "submitter": "Gusi Te", "authors": "Gusi Te, Wei Hu, Yinglu Liu, Hailin Shi, Tao Mei", "title": "Adaptive Graph Representation Learning and Reasoning for Face Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face parsing infers a pixel-wise label to each facial component, which has\ndrawn much attention recently. Previous methods have shown their success in\nface parsing, which however overlook the correlation among facial components.\nAs a matter of fact, the component-wise relationship is a critical clue in\ndiscriminating ambiguous pixels in facial area. To address this issue, we\npropose adaptive graph representation learning and reasoning over facial\ncomponents, aiming to learn representative vertices that describe each\ncomponent, exploit the component-wise relationship and thereby produce accurate\nparsing results against ambiguity. In particular, we devise an adaptive and\ndifferentiable graph abstraction method to represent the components on a graph\nvia pixel-to-vertex projection under the initial condition of a predicted\nparsing map, where pixel features within a certain facial region are aggregated\nonto a vertex. Further, we explicitly incorporate the image edge as a prior in\nthe model, which helps to discriminate edge and non-edge pixels during the\nprojection, thus leading to refined parsing results along the edges. Then, our\nmodel learns and reasons over the relations among components by propagating\ninformation across vertices on the graph. Finally, the refined vertex features\nare projected back to pixel grids for the prediction of the final parsing map.\nTo train our model, we propose a discriminative loss to penalize small\ndistances between vertices in the feature space, which leads to distinct\nvertices with strong semantics. Experimental results show the superior\nperformance of the proposed model on multiple face parsing datasets, along with\nthe validation on the human parsing task to demonstrate the generalizability of\nour model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:17:40 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Te", "Gusi", ""], ["Hu", "Wei", ""], ["Liu", "Yinglu", ""], ["Shi", "Hailin", ""], ["Mei", "Tao", ""]]}, {"id": "2101.07036", "submitter": "Hacer Yalim Keles", "authors": "Yahya Dogan and Hacer Yalim Keles", "title": "Iterative Facial Image Inpainting using Cyclic Reverse Generator", "comments": "This paper is under consideration at Neural Computing and\n  Applications Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Facial image inpainting is a challenging problem as it requires generating\nnew pixels that include semantic information for masked key components in a\nface, e.g., eyes and nose. Recently, remarkable methods have been proposed in\nthis field. Most of these approaches use encoder-decoder architectures and have\ndifferent limitations such as allowing unique results for a given image and a\nparticular mask. Alternatively, some approaches generate promising results\nusing different masks with generator networks. However, these approaches are\noptimization-based and usually require quite a number of iterations. In this\npaper, we propose an efficient solution to the facial image painting problem\nusing the Cyclic Reverse Generator (CRG) architecture, which provides an\nencoder-generator model. We use the encoder to embed a given image to the\ngenerator space and incrementally inpaint the masked regions until a plausible\nimage is generated; a discriminator network is utilized to assess the generated\nimages during the iterations. We empirically observed that only a few\niterations are sufficient to generate realistic images with the proposed model.\nAfter the generation process, for the post processing, we utilize a Unet model\nthat we trained specifically for this task to remedy the artifacts close to the\nmask boundaries. Our method allows applying sketch-based inpaintings, using\nvariety of mask types, and producing multiple and diverse results. We\nqualitatively compared our method with the state-of-the-art models and observed\nthat our method can compete with the other models in all mask types; it is\nparticularly better in images where larger masks are utilized.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:19:58 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dogan", "Yahya", ""], ["Keles", "Hacer Yalim", ""]]}, {"id": "2101.07042", "submitter": "Shreyank N Gowda", "authors": "Shreyank N Gowda, Laura Sevilla-Lara, Frank Keller, Marcus Rohrbach", "title": "CLASTER: Clustering with Reinforcement Learning for Zero-Shot Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zero-shot action recognition is the task of recognizingaction classes without\nvisual examples, only with a seman-tic embedding which relates unseen to seen\nclasses. Theproblem can be seen as learning a function which general-izes well\nto instances of unseen classes without losing dis-crimination between classes.\nNeural networks can modelthe complex boundaries between visual classes, which\nex-plains their success as supervised models. However, inzero-shot learning,\nthese highly specialized class bound-aries may not transfer well from seen to\nunseen classes.In this paper we propose a centroid-based representation,which\nclusters visual and semantic representation, consid-ers all training samples at\nonce, and in this way generaliz-ing well to instances from unseen classes. We\noptimize theclustering using Reinforcement Learning which we show iscritical\nfor our approach to work. We call the proposedmethod CLASTER and observe that\nit consistently outper-forms the state-of-the-art in all standard datasets,\ninclud-ing UCF101, HMDB51 and Olympic Sports; both in thestandard zero-shot\nevaluation and the generalized zero-shotlearning. Further, we show that our\nmodel performs com-petitively in the image domain as well, outperforming\nthestate-of-the-art in many settings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:46:24 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 09:34:02 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Gowda", "Shreyank N", ""], ["Sevilla-Lara", "Laura", ""], ["Keller", "Frank", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "2101.07116", "submitter": "Yong Huang", "authors": "Yong Huang, Ben Chen, Daiming Qu", "title": "LNSMM: Eye Gaze Estimation With Local Network Share Multiview Multitask", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye gaze estimation has become increasingly significant in computer vision.In\nthis paper,we systematically study the mainstream of eye gaze estimation\nmethods,propose a novel methodology to estimate eye gaze points and eye gaze\ndirections simultaneously.First,we construct a local sharing network for\nfeature extraction of gaze points and gaze directions estimation,which can\nreduce network computational parameters and converge quickly;Second,we propose\na Multiview Multitask Learning (MTL) framework,for gaze directions,a coplanar\nconstraint is proposed for the left and right eyes,for gaze points,three views\ndata input indirectly introduces eye position information,a cross-view pooling\nmodule is designed, propose joint loss which handle both gaze points and gaze\ndirections estimation.Eventually,we collect a dataset to use of gaze\npoints,which have three views to exist public dataset.The experiment show our\nmethod is state-of-the-art the current mainstream methods on two indicators of\ngaze points and gaze directions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:14:24 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Huang", "Yong", ""], ["Chen", "Ben", ""], ["Qu", "Daiming", ""]]}, {"id": "2101.07172", "submitter": "Chien-Hsiang Huang", "authors": "Chien-Hsiang Huang, Hung-Yu Wu, and Youn-Long Lin", "title": "HarDNet-MSEG: A Simple Encoder-Decoder Polyp Segmentation Neural Network\n  that Achieves over 0.9 Mean Dice and 86 FPS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a new convolution neural network called HarDNet-MSEG for polyp\nsegmentation. It achieves SOTA in both accuracy and inference speed on five\npopular datasets. For Kvasir-SEG, HarDNet-MSEG delivers 0.904 mean Dice running\nat 86.7 FPS on a GeForce RTX 2080 Ti GPU. It consists of a backbone and a\ndecoder. The backbone is a low memory traffic CNN called HarDNet68, which has\nbeen successfully applied to various CV tasks including image classification,\nobject detection, multi-object tracking and semantic segmentation, etc. The\ndecoder part is inspired by the Cascaded Partial Decoder, known for fast and\naccurate salient object detection. We have evaluated HarDNet-MSEG using those\nfive popular datasets. The code and all experiment details are available at\nGithub. https://github.com/james128333/HarDNet-MSEG\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 17:20:11 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 15:58:47 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Huang", "Chien-Hsiang", ""], ["Wu", "Hung-Yu", ""], ["Lin", "Youn-Long", ""]]}, {"id": "2101.07195", "submitter": "Sara Mardanisamani", "authors": "Sara Mardanisamani, Zahra Karimi, Akram Jamshidzadeh, Mehran Yazdi,\n  Melika Farshad, Amirmehdi Farshad", "title": "A New Approach for Automatic Segmentation and Evaluation of Pigmentation\n  Lesion by using Active Contour Model and Speeded Up Robust Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Digital image processing techniques have wide applications in different\nscientific fields including the medicine. By use of image processing\nalgorithms, physicians have been more successful in diagnosis of different\ndiseases and have achieved much better treatment results. In this paper, we\npropose an automatic method for segmenting the skin lesions and extracting\nfeatures that are associated to them. At this aim, a combination of Speeded-Up\nRobust Features (SURF) and Active Contour Model (ACM), is used. In the\nsuggested method, at first region of skin lesion is segmented from the whole\nskin image, and then some features like the mean, variance, RGB and HSV\nparameters are extracted from the segmented region. Comparing the segmentation\nresults, by use of Otsu thresholding, our proposed method, shows the\nsuperiority of our procedure over the Otsu theresholding method. Segmentation\nof the skin lesion by the proposed method and Otsu thresholding compared the\nresults with physician's manual method. The proposed method for skin lesion\nsegmentation, which is a combination of SURF and ACM, gives the best result.\nFor empirical evaluation of our method, we have applied it on twenty different\nskin lesion images. Obtained results confirm the high performance, speed and\naccuracy of our method.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 17:57:42 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mardanisamani", "Sara", ""], ["Karimi", "Zahra", ""], ["Jamshidzadeh", "Akram", ""], ["Yazdi", "Mehran", ""], ["Farshad", "Melika", ""], ["Farshad", "Amirmehdi", ""]]}, {"id": "2101.07209", "submitter": "Luis Souza Jr.", "authors": "Luis A. de Souza Jr., Luis C. S. Afonso, Alanna Ebigbo, Andreas\n  Probst, Helmut Messmann, Robert Mendel, Christoph Palm and Jo\\~ao P. Papa", "title": "Learning Visual Representations with Optimum-Path Forest and its\n  Applications to Barrett's Esophagus and Adenocarcinoma Diagnosis", "comments": null, "journal-ref": null, "doi": "10.1007/s00521-018-03982-0", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce the unsupervised Optimum-Path Forest (OPF)\nclassifier for learning visual dictionaries in the context of Barrett's\nesophagus (BE) and automatic adenocarcinoma diagnosis. The proposed approach\nwas validated in two datasets (MICCAI 2015 and Augsburg) using three different\nfeature extractors (SIFT, SURF, and the not yet applied to the BE context\nA-KAZE), as well as five supervised classifiers, including two variants of the\nOPF, Support Vector Machines with Radial Basis Function and Linear kernels, and\na Bayesian classifier. Concerning MICCAI 2015 dataset, the best results were\nobtained using unsupervised OPF for dictionary generation using supervised OPF\nfor classification purposes and using SURF feature extractor with accuracy\nnearly to 78% for distinguishing BE patients from adenocarcinoma ones.\nRegarding the Augsburg dataset, the most accurate results were also obtained\nusing both OPF classifiers but with A-KAZE as the feature extractor with\naccuracy close to 73%. The combination of feature extraction and\nbag-of-visual-words techniques showed results that outperformed others obtained\nrecently in the literature, as well as we highlight new advances in the related\nresearch area. Reinforcing the significance of this work, to the best of our\nknowledge, this is the first one that aimed at addressing computer-aided BE\nidentification using bag-of-visual-words and OPF classifiers, being this\napplication of unsupervised technique in the BE feature calculation the major\ncontribution of this work. It is also proposed a new BE and adenocarcinoma\ndescription using the A-KAZE features, not yet applied in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:15:26 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 20:41:20 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Souza", "Luis A. de", "Jr."], ["Afonso", "Luis C. S.", ""], ["Ebigbo", "Alanna", ""], ["Probst", "Andreas", ""], ["Messmann", "Helmut", ""], ["Mendel", "Robert", ""], ["Palm", "Christoph", ""], ["Papa", "Jo\u00e3o P.", ""]]}, {"id": "2101.07235", "submitter": "Jean-Francois Rajotte", "authors": "Jean-Francois Rajotte, Sumit Mukherjee, Caleb Robinson, Anthony Ortiz,\n  Christopher West, Juan Lavista Ferres, Raymond T Ng", "title": "Reducing bias and increasing utility by federated generative modeling of\n  medical images using a centralized adversary", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce FELICIA (FEderated LearnIng with a CentralIzed Adversary) a\ngenerative mechanism enabling collaborative learning. In particular, we show\nhow a data owner with limited and biased data could benefit from other data\nowners while keeping data from all the sources private. This is a common\nscenario in medical image analysis where privacy legislation prevents data from\nbeing shared outside local premises. FELICIA works for a large family of\nGenerative Adversarial Networks (GAN) architectures including vanilla and\nconditional GANs as demonstrated in this work. We show that by using the\nFELICIA mechanism, a data owner with limited image samples can generate\nhigh-quality synthetic images with high utility while neither data owners has\nto provide access to its data. The sharing happens solely through a central\ndiscriminator that has access limited to synthetic data. Here, utility is\ndefined as classification performance on a real test set. We demonstrate these\nbenefits on several realistic healthcare scenarions using benchmark image\ndatasets (MNIST, CIFAR-10) as well as on medical images for the task of skin\nlesion classification. With multiple experiments, we show that even in the\nworst cases, combining FELICIA with real data gracefully achieves performance\non par with real data while most results significantly improves the utility.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:40:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Rajotte", "Jean-Francois", ""], ["Mukherjee", "Sumit", ""], ["Robinson", "Caleb", ""], ["Ortiz", "Anthony", ""], ["West", "Christopher", ""], ["Ferres", "Juan Lavista", ""], ["Ng", "Raymond T", ""]]}, {"id": "2101.07241", "submitter": "Haoyu Xiong", "authors": "Haoyu Xiong, Quanzhou Li, Yun-Chun Chen, Homanga Bharadhwaj, Samarth\n  Sinha, Animesh Garg", "title": "Learning by Watching: Physical Imitation of Manipulation Skills from\n  Human Videos", "comments": "Project Website: https://www.pair.toronto.edu/lbw-kp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present an approach for physical imitation from human videos for robot\nmanipulation tasks. The key idea of our method lies in explicitly exploiting\nthe kinematics and motion information embedded in the video to learn structured\nrepresentations that endow the robot with the ability to imagine how to perform\nmanipulation tasks in its own context. To achieve this, we design a perception\nmodule that learns to translate human videos to the robot domain followed by\nunsupervised keypoint detection. The resulting keypoint-based representations\nprovide semantically meaningful information that can be directly used for\nreward computing and policy learning. We evaluate the effectiveness of our\napproach on five robot manipulation tasks, including reaching, pushing,\nsliding, coffee making, and drawer closing. Detailed experimental evaluations\ndemonstrate that our method performs favorably against previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:50:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xiong", "Haoyu", ""], ["Li", "Quanzhou", ""], ["Chen", "Yun-Chun", ""], ["Bharadhwaj", "Homanga", ""], ["Sinha", "Samarth", ""], ["Garg", "Animesh", ""]]}, {"id": "2101.07253", "submitter": "Maximilian Jaritz", "authors": "Maximilian Jaritz, Tuan-Hung Vu, Raoul de Charette, \\'Emilie Wirbel,\n  and Patrick P\\'erez", "title": "Cross-modal Learning for Domain Adaptation in 3D Semantic Segmentation", "comments": "arXiv admin note: text overlap with arXiv:1911.12676", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is an important task to enable learning when labels are\nscarce. While most works focus only on the image modality, there are many\nimportant multi-modal datasets. In order to leverage multi-modality for domain\nadaptation, we propose cross-modal learning, where we enforce consistency\nbetween the predictions of two modalities via mutual mimicking. We constrain\nour network to make correct predictions on labeled data and consistent\npredictions across modalities on unlabeled target-domain data. Experiments in\nunsupervised and semi-supervised domain adaptation settings prove the\neffectiveness of this novel domain adaptation strategy. Specifically, we\nevaluate on the task of 3D semantic segmentation using the image and point\ncloud modality. We leverage recent autonomous driving datasets to produce a\nwide variety of domain adaptation scenarios including changes in scene layout,\nlighting, sensor setup and weather, as well as the synthetic-to-real setup. Our\nmethod significantly improves over previous uni-modal adaptation baselines on\nall adaption scenarios. Code will be made available.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:59:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Jaritz", "Maximilian", ""], ["Vu", "Tuan-Hung", ""], ["de Charette", "Raoul", ""], ["Wirbel", "\u00c9milie", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "2101.07280", "submitter": "Saad Nadeem", "authors": "Shawn Mathew, Saad Nadeem and Arie Kaufman", "title": "Visualizing Missing Surfaces In Colonoscopy Videos using Shared Latent\n  Space Representations", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI) 2021,\n  **Shawn Mathew and Saad Nadeem contributed equally", "journal-ref": null, "doi": "10.1109/ISBI48211.2021.9433982", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical colonoscopy (OC), the most prevalent colon cancer screening tool, has\na high miss rate due to a number of factors, including the geometry of the\ncolon (haustral fold and sharp bends occlusions), endoscopist inexperience or\nfatigue, endoscope field of view, etc. We present a framework to visualize the\nmissed regions per-frame during the colonoscopy, and provides a workable\nclinical solution. Specifically, we make use of 3D reconstructed virtual\ncolonoscopy (VC) data and the insight that VC and OC share the same underlying\ngeometry but differ in color, texture and specular reflections, embedded in the\nOC domain. A lossy unpaired image-to-image translation model is introduced with\nenforced shared latent space for OC and VC. This shared latent space captures\nthe geometric information while deferring the color, texture, and specular\ninformation creation to additional Gaussian noise input. This additional noise\ninput can be utilized to generate one-to-many mappings from VC to OC and OC to\nOC. The code, data and trained models will be released via our Computational\nEndoscopy Platform at https://github.com/nadeemlab/CEP.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:00:51 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 14:38:09 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Mathew", "Shawn", ""], ["Nadeem", "Saad", ""], ["Kaufman", "Arie", ""]]}, {"id": "2101.07295", "submitter": "Anh Thai", "authors": "Anh Thai, Stefan Stojanov, Isaac Rehg, James M. Rehg", "title": "Does Continual Learning = Catastrophic Forgetting?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning is known for suffering from catastrophic forgetting, a\nphenomenon where earlier learned concepts are forgotten at the expense of more\nrecent samples. In this work, we challenge the assumption that continual\nlearning is inevitably associated with catastrophic forgetting by presenting a\nset of tasks that surprisingly do not suffer from catastrophic forgetting when\nlearned continually. The robustness of these tasks leads to the potential of\nhaving a proxy representation learning task for continual classification. We\nfurther introduce a novel yet simple algorithm, YASS that achieves\nstate-of-the-art performance in the class-incremental categorization learning\ntask and provide an insight into the benefit of learning the representation\ncontinuously. Finally, we present converging evidence on the forgetting\ndynamics of representation learning in continual models. The codebase, dataset,\nand pre-trained models released with this article can be found at\nhttps://github.com/rehg-lab/CLRec.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:29:12 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 03:42:31 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Thai", "Anh", ""], ["Stojanov", "Stefan", ""], ["Rehg", "Isaac", ""], ["Rehg", "James M.", ""]]}, {"id": "2101.07296", "submitter": "Stefan Stojanov", "authors": "Stefan Stojanov, Anh Thai, James M. Rehg", "title": "Using Shape to Categorize: Low-Shot Learning with an Explicit Shape Bias", "comments": "Accepted at CVPR2021. Project page, code and data available at\n  https://rehg-lab.github.io/publication-pages/lowshot-shapebias/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely accepted that reasoning about object shape is important for\nobject recognition. However, the most powerful object recognition methods today\ndo not explicitly make use of object shape during learning. In this work,\nmotivated by recent developments in low-shot learning, findings in\ndevelopmental psychology, and the increased use of synthetic data in computer\nvision research, we investigate how reasoning about 3D shape can be used to\nimprove low-shot learning methods' generalization performance. We propose a new\nway to improve existing low-shot learning approaches by learning a\ndiscriminative embedding space using 3D object shape, and using this embedding\nby learning how to map images into it. Our new approach improves the\nperformance of image-only low-shot learning approaches on multiple datasets. We\nalso introduce Toys4K, a 3D object dataset with the largest number of object\ncategories currently available, which supports low-shot learning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:29:41 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 23:13:19 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Stojanov", "Stefan", ""], ["Thai", "Anh", ""], ["Rehg", "James M.", ""]]}, {"id": "2101.07299", "submitter": "Vasiliki Kougia Ms", "authors": "John Pavlopoulos, Vasiliki Kougia, Ion Androutsopoulos, Dimitris\n  Papamichail", "title": "Diagnostic Captioning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic Captioning (DC) concerns the automatic generation of a diagnostic\ntext from a set of medical images of a patient collected during an examination.\nDC can assist inexperienced physicians, reducing clinical errors. It can also\nhelp experienced physicians produce diagnostic reports faster. Following the\nadvances of deep learning, especially in generic image captioning, DC has\nrecently attracted more attention, leading to several systems and datasets.\nThis article is an extensive overview of DC. It presents relevant datasets,\nevaluation measures, and up to date systems. It also highlights shortcomings\nthat hinder DC's progress and proposes future directions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:40:32 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Pavlopoulos", "John", ""], ["Kougia", "Vasiliki", ""], ["Androutsopoulos", "Ion", ""], ["Papamichail", "Dimitris", ""]]}, {"id": "2101.07308", "submitter": "Le Thanh Nguyen-Meidine", "authors": "Le Thanh Nguyen-Meidine, Atif Belal, Madhu Kiran, Jose Dolz,\n  Louis-Antoine Blais-Morin, Eric Granger", "title": "Knowledge Distillation Methods for Efficient Unsupervised Adaptation\n  Across Multiple Domains", "comments": "This is the extended journal version of arXiv:2005.07839", "journal-ref": null, "doi": "10.1016/j.imavis.2021.104096", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond the complexity of CNNs that require training on large annotated\ndatasets, the domain shift between design and operational data has limited the\nadoption of CNNs in many real-world applications. For instance, in person\nre-identification, videos are captured over a distributed set of cameras with\nnon-overlapping viewpoints. The shift between the source (e.g. lab setting) and\ntarget (e.g. cameras) domains may lead to a significant decline in recognition\naccuracy. Additionally, state-of-the-art CNNs may not be suitable for such\nreal-time applications given their computational requirements. Although several\ntechniques have recently been proposed to address domain shift problems through\nunsupervised domain adaptation (UDA), or to accelerate/compress CNNs through\nknowledge distillation (KD), we seek to simultaneously adapt and compress CNNs\nto generalize well across multiple target domains. In this paper, we propose a\nprogressive KD approach for unsupervised single-target DA (STDA) and\nmulti-target DA (MTDA) of CNNs. Our method for KD-STDA adapts a CNN to a single\ntarget domain by distilling from a larger teacher CNN, trained on both target\nand source domain data in order to maintain its consistency with a common\nrepresentation. Our proposed approach is compared against state-of-the-art\nmethods for compression and STDA of CNNs on the Office31 and ImageClef-DA image\nclassification datasets. It is also compared against state-of-the-art methods\nfor MTDA on Digits, Office31, and OfficeHome. In both settings -- KD-STDA and\nKD-MTDA -- results indicate that our approach can achieve the highest level of\naccuracy across target domains, while requiring a comparable or lower CNN\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:53:16 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Nguyen-Meidine", "Le Thanh", ""], ["Belal", "Atif", ""], ["Kiran", "Madhu", ""], ["Dolz", "Jose", ""], ["Blais-Morin", "Louis-Antoine", ""], ["Granger", "Eric", ""]]}, {"id": "2101.07314", "submitter": "Takuma Yagi", "authors": "Takuma Yagi, Takumi Nishiyasu, Kunimasa Kawasaki, Moe Matsuki, Yoichi\n  Sato", "title": "GO-Finder: A Registration-Free Wearable System for Assisting Users in\n  Finding Lost Objects via Hand-Held Object Discovery", "comments": "13 pages, 13 figures, ACM IUI 2021", "journal-ref": null, "doi": "10.1145/3397481.3450664", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People spend an enormous amount of time and effort looking for lost objects.\nTo help remind people of the location of lost objects, various computational\nsystems that provide information on their locations have been developed.\nHowever, prior systems for assisting people in finding objects require users to\nregister the target objects in advance. This requirement imposes a cumbersome\nburden on the users, and the system cannot help remind them of unexpectedly\nlost objects. We propose GO-Finder (\"Generic Object Finder\"), a\nregistration-free wearable camera based system for assisting people in finding\nan arbitrary number of objects based on two key features: automatic discovery\nof hand-held objects and image-based candidate selection. Given a video taken\nfrom a wearable camera, Go-Finder automatically detects and groups hand-held\nobjects to form a visual timeline of the objects. Users can retrieve the last\nappearance of the object by browsing the timeline through a smartphone app. We\nconducted a user study to investigate how users benefit from using GO-Finder\nand confirmed improved accuracy and reduced mental load regarding the object\nsearch task by providing clear visual cues on object locations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 20:04:56 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 11:16:44 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Yagi", "Takuma", ""], ["Nishiyasu", "Takumi", ""], ["Kawasaki", "Kunimasa", ""], ["Matsuki", "Moe", ""], ["Sato", "Yoichi", ""]]}, {"id": "2101.07338", "submitter": "Marcus Angeloni", "authors": "Marcus de Assis Angeloni and Helio Pedrini", "title": "Improving Makeup Face Verification by Exploring Part-Based\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we have seen an increase in the global facial recognition market\nsize. Despite significant advances in face recognition technology with the\nadoption of convolutional neural networks, there are still open challenges, as\nwhen there is makeup in the face. To address this challenge, we propose and\nevaluate the adoption of facial parts to fuse with current holistic\nrepresentations. We propose two strategies of facial parts: one with four\nregions (left periocular, right periocular, nose and mouth) and another with\nthree facial thirds (upper, middle and lower). Experimental results obtained in\nfour public makeup face datasets and in a challenging cross-dataset protocol\nshow that the fusion of deep features extracted of facial parts with holistic\nrepresentation increases the accuracy of face verification systems and\ndecreases the error rates, even without any retraining of the CNN models. Our\nproposed pipeline achieved state-of-the-art performance for the YMU dataset and\ncompetitive results for other three datasets (EMFD, FAM and M501).\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 21:51:38 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Angeloni", "Marcus de Assis", ""], ["Pedrini", "Helio", ""]]}, {"id": "2101.07347", "submitter": "Shuvo Kumar Paul", "authors": "S. K. Paul, M. T. Chowdhury, M. Nicolescu, M. Nicolescu", "title": "Object Detection and Pose Estimation from RGB and Depth Data for\n  Real-time, Adaptive Robotic Grasping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In recent times, object detection and pose estimation have gained significant\nattention in the context of robotic vision applications. Both the\nidentification of objects of interest as well as the estimation of their pose\nremain important capabilities in order for robots to provide effective\nassistance for numerous robotic applications ranging from household tasks to\nindustrial manipulation. This problem is particularly challenging because of\nthe heterogeneity of objects having different and potentially complex shapes,\nand the difficulties arising due to background clutter and partial occlusions\nbetween objects. As the main contribution of this work, we propose a system\nthat performs real-time object detection and pose estimation, for the purpose\nof dynamic robot grasping. The robot has been pre-trained to perform a small\nset of canonical grasps from a few fixed poses for each object. When presented\nwith an unknown object in an arbitrary pose, the proposed approach allows the\nrobot to detect the object identity and its actual pose, and then adapt a\ncanonical grasp in order to be used with the new pose. For training, the system\ndefines a canonical grasp by capturing the relative pose of an object with\nrespect to the gripper attached to the robot's wrist. During testing, once a\nnew pose is detected, a canonical grasp for the object is identified and then\ndynamically adapted by adjusting the robot arm's joint angles, so that the\ngripper can grasp the object in its new pose. We conducted experiments using a\nhumanoid PR2 robot and showed that the proposed framework can detect\nwell-textured objects, and provide accurate pose estimation in the presence of\ntolerable amounts of out-of-plane rotation. The performance is also illustrated\nby the robot successfully grasping objects from a wide range of arbitrary\nposes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:22:47 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Paul", "S. K.", ""], ["Chowdhury", "M. T.", ""], ["Nicolescu", "M.", ""], ["Nicolescu", "M.", ""]]}, {"id": "2101.07370", "submitter": "Berat Kurar Barakat", "authors": "Berat Kurar Barakat, Ahmad Droby, Reem Alaasam, Boraq Madi, Irina\n  Rabaev, Jihad El-Sana", "title": "Text line extraction using fully convolutional network and energy\n  minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text lines are important parts of handwritten document images and easier to\nanalyze by further applications. Despite recent progress in text line\ndetection, text line extraction from a handwritten document remains an unsolved\ntask. This paper proposes to use a fully convolutional network for text line\ndetection and energy minimization for text line extraction. Detected text lines\nare represented by blob lines that strike through the text lines. These blob\nlines assist an energy function for text line extraction. The detection stage\ncan locate arbitrarily oriented text lines. Furthermore, the extraction stage\nis capable of finding out the pixels of text lines with various heights and\ninterline proximity independent of their orientations. Besides, it can finely\nsplit the touching and overlapping text lines without an orientation\nassumption. We evaluate the proposed method on VML-AHTE, VML-MOC, and\nDiva-HisDB datasets. The VML-AHTE dataset contains overlapping, touching and\nclose text lines with rich diacritics. The VML-MOC dataset is very challenging\nby its multiply oriented and skewed text lines. The Diva-HisDB dataset exhibits\ndistinct text line heights and touching text lines. The results demonstrate the\neffectiveness of the method despite various types of challenges, yet using the\nsame parameters in all the experiments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 23:23:03 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Barakat", "Berat Kurar", ""], ["Droby", "Ahmad", ""], ["Alaasam", "Reem", ""], ["Madi", "Boraq", ""], ["Rabaev", "Irina", ""], ["El-Sana", "Jihad", ""]]}, {"id": "2101.07376", "submitter": "Khalid Alsamadony", "authors": "Khalid L. Alsamadony, Ertugrul U. Yildirim, Guenther Glatz, Umair bin\n  Waheed, Sherif M. Hanafy", "title": "Deep-Learning Driven Noise Reduction for Reduced Flux Computed\n  Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have received considerable attention in clinical\nimaging, particularly with respect to the reduction of radiation risk. Lowering\nthe radiation dose by reducing the photon flux inevitably results in the\ndegradation of the scanned image quality. Thus, researchers have sought to\nexploit deep convolutional neural networks (DCNNs) to map low-quality, low-dose\nimages to higher-dose, higher-quality images thereby minimizing the associated\nradiation hazard. Conversely, computed tomography (CT) measurements of\ngeomaterials are not limited by the radiation dose. In contrast to the human\nbody, however, geomaterials may be comprised of high-density constituents\ncausing increased attenuation of the X-Rays. Consequently, higher dosage images\nare required to obtain an acceptable scan quality. The problem of prolonged\nacquisition times is particularly severe for micro-CT based scanning\ntechnologies. Depending on the sample size and exposure time settings, a single\nscan may require several hours to complete. This is of particular concern if\nphenomena with an exponential temperature dependency are to be elucidated. A\nprocess may happen too fast to be adequately captured by CT scanning. To\naddress the aforementioned issues, we apply DCNNs to improve the quality of\nrock CT images and reduce exposure times by more than 60\\%, simultaneously. We\nhighlight current results based on micro-CT derived datasets and apply transfer\nlearning to improve DCNN results without increasing training time. The approach\nis applicable to any computed tomography technology. Furthermore, we contrast\nthe performance of the DCNN trained by minimizing different loss functions such\nas mean squared error and structural similarity index.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 23:31:37 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Alsamadony", "Khalid L.", ""], ["Yildirim", "Ertugrul U.", ""], ["Glatz", "Guenther", ""], ["Waheed", "Umair bin", ""], ["Hanafy", "Sherif M.", ""]]}, {"id": "2101.07383", "submitter": "Kai Yao", "authors": "Kai Yao, Alberto Ortiz, Francisco Bonnin-Pascual", "title": "A DCNN-based Arbitrarily-Oriented Object Detector for Quality Control\n  and Inspection Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the success of machine vision systems for on-line automated quality\ncontrol and inspection processes, an object recognition solution is presented\nin this work for two different specific applications, i.e., the detection of\nquality control items in surgery toolboxes prepared for sterilizing in a\nhospital, as well as the detection of defects in vessel hulls to prevent\npotential structural failures. The solution has two stages. First, a feature\npyramid architecture based on Single Shot MultiBox Detector (SSD) is used to\nimprove the detection performance, and a statistical analysis based on ground\ntruth is employed to select parameters of a range of default boxes. Second, a\nlightweight neural network is exploited to achieve oriented detection results\nusing a regression method. The first stage of the proposed method is capable of\ndetecting the small targets considered in the two scenarios. In the second\nstage, despite the simplicity, it is efficient to detect elongated targets\nwhile maintaining high running efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 00:23:27 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Yao", "Kai", ""], ["Ortiz", "Alberto", ""], ["Bonnin-Pascual", "Francisco", ""]]}, {"id": "2101.07389", "submitter": "Qiufan Lin", "authors": "Qiufan Lin and Dominique Fouchez and J\\'er\\^ome Pasquet", "title": "Galaxy Image Translation with Semi-supervised Noise-reconstructed\n  Generative Adversarial Networks", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-to-image translation with Deep Learning neural networks, particularly\nwith Generative Adversarial Networks (GANs), is one of the most powerful\nmethods for simulating astronomical images. However, current work is limited to\nutilizing paired images with supervised translation, and there has been rare\ndiscussion on reconstructing noise background that encodes instrumental and\nobservational effects. These limitations might be harmful for subsequent\nscientific applications in astrophysics. Therefore, we aim to develop methods\nfor using unpaired images and preserving noise characteristics in image\ntranslation. In this work, we propose a two-way image translation model using\nGANs that exploits both paired and unpaired images in a semi-supervised manner,\nand introduce a noise emulating module that is able to learn and reconstruct\nnoise characterized by high-frequency features. By experimenting on multi-band\ngalaxy images from the Sloan Digital Sky Survey (SDSS) and the Canada France\nHawaii Telescope Legacy Survey (CFHT), we show that our method recovers global\nand local properties effectively and outperforms benchmark image translation\nmodels. To our best knowledge, this work is the first attempt to apply\nsemi-supervised methods and noise reconstruction techniques in astrophysical\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 00:43:14 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Lin", "Qiufan", ""], ["Fouchez", "Dominique", ""], ["Pasquet", "J\u00e9r\u00f4me", ""]]}, {"id": "2101.07396", "submitter": "Panos Achlioptas", "authors": "Panos Achlioptas, Maks Ovsjanikov, Kilichbek Haydarov, Mohamed\n  Elhoseiny, Leonidas Guibas", "title": "ArtEmis: Affective Language for Visual Art", "comments": "https://artemisdataset.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel large-scale dataset and accompanying machine learning\nmodels aimed at providing a detailed understanding of the interplay between\nvisual content, its emotional effect, and explanations for the latter in\nlanguage. In contrast to most existing annotation datasets in computer vision,\nwe focus on the affective experience triggered by visual artworks and ask the\nannotators to indicate the dominant emotion they feel for a given image and,\ncrucially, to also provide a grounded verbal explanation for their emotion\nchoice. As we demonstrate below, this leads to a rich set of signals for both\nthe objective content and the affective impact of an image, creating\nassociations with abstract concepts (e.g., \"freedom\" or \"love\"), or references\nthat go beyond what is directly visible, including visual similes and\nmetaphors, or subjective references to personal experiences. We focus on visual\nart (e.g., paintings, artistic photographs) as it is a prime example of imagery\ncreated to elicit emotional responses from its viewers. Our dataset, termed\nArtEmis, contains 439K emotion attributions and explanations from humans, on\n81K artworks from WikiArt. Building on this data, we train and demonstrate a\nseries of captioning systems capable of expressing and explaining emotions from\nvisual stimuli. Remarkably, the captions produced by these systems often\nsucceed in reflecting the semantic and abstract content of the image, going\nwell beyond systems trained on existing datasets. The collected dataset and\ndeveloped methods are available at https://artemisdataset.org.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 01:03:40 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Achlioptas", "Panos", ""], ["Ovsjanikov", "Maks", ""], ["Haydarov", "Kilichbek", ""], ["Elhoseiny", "Mohamed", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2101.07406", "submitter": "Nakamasa Inoue", "authors": "Nakamasa Inoue, Eisuke Yamagata, Hirokatsu Kataoka", "title": "Initialization Using Perlin Noise for Training Networks with a Limited\n  Amount of Data", "comments": "Accepted to ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel network initialization method using Perlin noise for\ntraining image classification networks with a limited amount of data. Our main\nidea is to initialize the network parameters by solving an artificial noise\nclassification problem, where the aim is to classify Perlin noise samples into\ntheir noise categories. Specifically, the proposed method consists of two\nsteps. First, it generates Perlin noise samples with category labels defined\nbased on noise complexity. Second, it solves a classification problem, in which\nnetwork parameters are optimized to classify the generated noise samples. This\nmethod produces a reasonable set of initial weights (filters) for image\nclassification. To the best of our knowledge, this is the first work to\ninitialize networks by solving an artificial optimization problem without using\nany real-world images. Our experiments show that the proposed method\noutperforms conventional initialization methods on four image classification\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 01:43:10 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Inoue", "Nakamasa", ""], ["Yamagata", "Eisuke", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "2101.07419", "submitter": "Haiwei Wu", "authors": "Haiwei Wu and Jiantao Zhou", "title": "GIID-Net: Generalizable Image Inpainting Detection via Neural\n  Architecture Search and Attention", "comments": "Some errors are found in the Section V of Experimental Results, and\n  more experiments are needed to be added. Besides, there are some\n  modifications we want to present in the Section III of the Methods, e.g.,\n  updating the figures for better describe the proposed methods. Thanks!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has demonstrated its powerful capabilities in the field of\nimage inpainting, which could produce visually plausible results. Meanwhile,\nthe malicious use of advanced image inpainting tools (e.g. removing key objects\nto report fake news) has led to increasing threats to the reliability of image\ndata. To fight against the inpainting forgeries, in this work, we propose a\nnovel end-to-end Generalizable Image Inpainting Detection Network (GIID-Net),\nto detect the inpainted regions at pixel accuracy. The proposed GIID-Net\nconsists of three sub-blocks: the enhancement block, the extraction block and\nthe decision block. Specifically, the enhancement block aims to enhance the\ninpainting traces by using hierarchically combined special layers. The\nextraction block, automatically designed by Neural Architecture Search (NAS)\nalgorithm, is targeted to extract features for the actual inpainting detection\ntasks. In order to further optimize the extracted latent features, we integrate\nglobal and local attention modules in the decision block, where the global\nattention reduces the intra-class differences by measuring the similarity of\nglobal features, while the local attention strengthens the consistency of local\nfeatures. Furthermore, we thoroughly study the generalizability of our\nGIID-Net, and find that different training data could result in vastly\ndifferent generalization capability. Extensive experimental results are\npresented to validate the superiority of the proposed GIID-Net, compared with\nthe state-of-the-art competitors. Our results would suggest that common\nartifacts are shared across diverse image inpainting methods. Finally, we build\na public inpainting dataset of 10K image pairs for the future research in this\narea.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:29:40 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 05:44:31 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Wu", "Haiwei", ""], ["Zhou", "Jiantao", ""]]}, {"id": "2101.07422", "submitter": "Lei He", "authors": "Lei He, Jiwen Lu, Guanghui Wang, Shiyu Song, Jie Zhou", "title": "SOSD-Net: Joint Semantic Object Segmentation and Depth Estimation from\n  Monocular images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth estimation and semantic segmentation play essential roles in scene\nunderstanding. The state-of-the-art methods employ multi-task learning to\nsimultaneously learn models for these two tasks at the pixel-wise level. They\nusually focus on sharing the common features or stitching feature maps from the\ncorresponding branches. However, these methods lack in-depth consideration on\nthe correlation of the geometric cues and the scene parsing. In this paper, we\nfirst introduce the concept of semantic objectness to exploit the geometric\nrelationship of these two tasks through an analysis of the imaging process,\nthen propose a Semantic Object Segmentation and Depth Estimation Network\n(SOSD-Net) based on the objectness assumption. To the best of our knowledge,\nSOSD-Net is the first network that exploits the geometry constraint for\nsimultaneous monocular depth estimation and semantic segmentation. In addition,\nconsidering the mutual implicit relationship between these two tasks, we\nexploit the iterative idea from the expectation-maximization algorithm to train\nthe proposed network more effectively. Extensive experimental results on the\nCityscapes and NYU v2 dataset are presented to demonstrate the superior\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:41:03 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["He", "Lei", ""], ["Lu", "Jiwen", ""], ["Wang", "Guanghui", ""], ["Song", "Shiyu", ""], ["Zhou", "Jie", ""]]}, {"id": "2101.07429", "submitter": "Fei Gao", "authors": "Hanliang Jiang, Fuhao Shen, Fei Gao, Weidong Han", "title": "Learning Efficient, Explainable and Discriminative Representations for\n  Pulmonary Nodules Classification", "comments": null, "journal-ref": "Pattern Recognition, 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic pulmonary nodules classification is significant for early diagnosis\nof lung cancers. Recently, deep learning techniques have enabled remarkable\nprogress in this field. However, these deep models are typically of high\ncomputational complexity and work in a black-box manner. To combat these\nchallenges, in this work, we aim to build an efficient and (partially)\nexplainable classification model. Specially, we use \\emph{neural architecture\nsearch} (NAS) to automatically search 3D network architectures with excellent\naccuracy/speed trade-off. Besides, we use the convolutional block attention\nmodule (CBAM) in the networks, which helps us understand the reasoning process.\nDuring training, we use A-Softmax loss to learn angularly discriminative\nrepresentations. In the inference stage, we employ an ensemble of diverse\nneural networks to improve the prediction accuracy and robustness. We conduct\nextensive experiments on the LIDC-IDRI database. Compared with previous\nstate-of-the-art, our model shows highly comparable performance by using less\nthan 1/40 parameters. Besides, empirical study shows that the reasoning process\nof learned networks is in conformity with physicians' diagnosis. Related code\nand results have been released at: https://github.com/fei-hdu/NAS-Lung.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:53:44 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Jiang", "Hanliang", ""], ["Shen", "Fuhao", ""], ["Gao", "Fei", ""], ["Han", "Weidong", ""]]}, {"id": "2101.07433", "submitter": "Alexander Wong", "authors": "Hayden Gunraj, Ali Sabri, David Koff, and Alexander Wong", "title": "COVID-Net CT-2: Enhanced Deep Neural Networks for Detection of COVID-19\n  from Chest CT Images Through Bigger, More Diverse Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic continues to rage on, with multiple waves causing\nsubstantial harm to health and economies around the world. Motivated by the use\nof CT imaging at clinical institutes around the world as an effective\ncomplementary screening method to RT-PCR testing, we introduced COVID-Net CT, a\nneural network tailored for detection of COVID-19 cases from chest CT images as\npart of the open source COVID-Net initiative. However, one potential limiting\nfactor is restricted quantity and diversity given the single nation patient\ncohort used. In this study, we introduce COVID-Net CT-2, enhanced deep neural\nnetworks for COVID-19 detection from chest CT images trained on the largest\nquantity and diversity of multinational patient cases in research literature.\nWe introduce two new CT benchmark datasets, the largest comprising a\nmultinational cohort of 4,501 patients from at least 15 countries. We leverage\nexplainability to investigate the decision-making behaviour of COVID-Net CT-2,\nwith the results for select cases reviewed and reported on by two\nboard-certified radiologists with over 10 and 30 years of experience,\nrespectively. The COVID-Net CT-2 neural networks achieved accuracy, COVID-19\nsensitivity, PPV, specificity, and NPV of 98.1%/96.2%/96.7%/99%/98.8% and\n97.9%/95.7%/96.4%/98.9%/98.7%, respectively. Explainability-driven performance\nvalidation shows that COVID-Net CT-2's decision-making behaviour is consistent\nwith radiologist interpretation by leveraging correct, clinically relevant\ncritical factors. The results are promising and suggest the strong potential of\ndeep neural networks as an effective tool for computer-aided COVID-19\nassessment. While not a production-ready solution, we hope the open-source,\nopen-access release of COVID-Net CT-2 and benchmark datasets will continue to\nenable researchers, clinicians, and citizen data scientists alike to build upon\nthem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 03:04:09 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 13:51:26 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gunraj", "Hayden", ""], ["Sabri", "Ali", ""], ["Koff", "David", ""], ["Wong", "Alexander", ""]]}, {"id": "2101.07434", "submitter": "Ye Huang", "authors": "Ye Huang, Wenjing Jia, Xiangjian He, Liu Liu, Yuxin Li, Dacheng Tao", "title": "Channelized Axial Attention for Semantic Segmentation", "comments": "Update experimental results on EfficientNet-B7; Ref errors fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-attention and channel attention, modelling thesemantic interdependencies\nin spatial and channel dimensionsrespectively, have recently been widely used\nfor semantic seg-mentation. However, computing spatial-attention and\nchannelattention separately and then fusing them directly can causeconflicting\nfeature representations. In this paper, we proposethe Channelized Axial\nAttention (CAA) to seamlessly integratechannel attention and axial attention\nwith reduced computationalcomplexity. After computing axial attention maps, we\npropose tochannelize the intermediate results obtained from the\ntransposeddot-product so that the channel importance of each axial\nrepre-sentation is optimized across the whole receptive field. We\nfurtherdevelop grouped vectorization, which allows our model to be runwith very\nlittle memory consumption at a speed comparableto the full vectorization.\nComparative experiments conductedon multiple benchmark datasets, including\nCityscapes, PASCALContext and COCO-Stuff, demonstrate that our CAA not\nonlyrequires much less computation resources compared with otherdual attention\nmodels such as DANet, but also outperformsthe state-of-the-art ResNet-101-based\nsegmentation models on alltested datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 03:08:03 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:24:11 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 16:07:07 GMT"}, {"version": "v4", "created": "Tue, 20 Apr 2021 00:59:58 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Huang", "Ye", ""], ["Jia", "Wenjing", ""], ["He", "Xiangjian", ""], ["Liu", "Liu", ""], ["Li", "Yuxin", ""], ["Tao", "Dacheng", ""]]}, {"id": "2101.07448", "submitter": "Peng Gao", "authors": "Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai, Hongsheng Li", "title": "Fast Convergence of DETR with Spatially Modulated Co-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recently proposed Detection Transformer (DETR) model successfully applies\nTransformer to objects detection and achieves comparable performance with\ntwo-stage object detection frameworks, such as Faster-RCNN. However, DETR\nsuffers from its slow convergence. Training DETR \\cite{carion2020end} from\nscratch needs 500 epochs to achieve a high accuracy. To accelerate its\nconvergence, we propose a simple yet effective scheme for improving the DETR\nframework, namely Spatially Modulated Co-Attention (SMCA) mechanism. The core\nidea of SMCA is to conduct regression-aware co-attention in DETR by\nconstraining co-attention responses to be high near initially estimated\nbounding box locations. Our proposed SMCA increases DETR's convergence speed by\nreplacing the original co-attention mechanism in the decoder while keeping\nother operations in DETR unchanged. Furthermore, by integrating multi-head and\nscale-selection attention designs into SMCA, our fully-fledged SMCA can achieve\nbetter performance compared to DETR with a dilated convolution-based backbone\n(45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We perform extensive\nablation studies on COCO dataset to validate the effectiveness of the proposed\nSMCA.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 03:52:44 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Gao", "Peng", ""], ["Zheng", "Minghang", ""], ["Wang", "Xiaogang", ""], ["Dai", "Jifeng", ""], ["Li", "Hongsheng", ""]]}, {"id": "2101.07458", "submitter": "Wei Lian", "authors": "Wei Lian and Wangmeng Zuo and Lei Zhang", "title": "Hybrid Trilinear and Bilinear Programming for Aligning Partially\n  Overlapping Point Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Alignment methods which can handle partially overlapping point sets and are\ninvariant to the corresponding transformations are desirable in computer\nvision, with applications such as providing initial transformation\nconfiguration for local search based methods like ICP. To this end, we first\nshow that the objective of the robust point matching (RPM) algorithm is a cubic\npolynomial. We then utilize the convex envelopes of trilinear and bilinear\nmonomials to develop its lower bounding function. The resulting lower bounding\nproblem can be efficiently solved via linear assignment and low dimensional\nconvex quadratic programming. We next develop a branch-and-bound (BnB)\nalgorithm which only branches over the transformation parameters and converges\nquickly. Experimental results demonstrated favorable performance of the\nproposed method over the state-of-the-art methods in terms of robustness and\nspeed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 04:24:23 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 07:24:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lian", "Wei", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "2101.07462", "submitter": "Xinhan Di", "authors": "Xinhan Di, Pengqian Yu", "title": "Deep Reinforcement Learning for Producing Furniture Layout in Indoor\n  Scenes", "comments": "computer vision reinforcement learning. arXiv admin note: text\n  overlap with arXiv:2012.08514, arXiv:2012.08131", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the industrial interior design process, professional designers plan the\nsize and position of furniture in a room to achieve a satisfactory design for\nselling. In this paper, we explore the interior scene design task as a Markov\ndecision process (MDP), which is solved by deep reinforcement learning. The\ngoal is to produce an accurate position and size of the furniture\nsimultaneously for the indoor layout task. In particular, we first formulate\nthe furniture layout task as a MDP problem by defining the state, action, and\nreward function. We then design the simulated environment and train\nreinforcement learning agents to produce the optimal layout for the MDP\nformulation. We conduct our experiments on a large-scale real-world interior\nlayout dataset that contains industrial designs from professional designers.\nOur numerical results demonstrate that the proposed model yields higher-quality\nlayouts as compared with the state-of-art model. The developed simulator and\ncodes are available at \\url{https://github.com/CODE-SUBMIT/simulator1}.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 04:38:58 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Di", "Xinhan", ""], ["Yu", "Pengqian", ""]]}, {"id": "2101.07487", "submitter": "Berat Kurar Barakat", "authors": "Ahmad Droby, Berat Kurar Barakat, Borak Madi, Reem Alaasam and Jihad\n  El-Sana", "title": "Unsupervised Deep Learning for Handwritten Page Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmenting handwritten document images into regions with homogeneous patterns\nis an important pre-processing step for many document images analysis tasks.\nHand-labeling data to train a deep learning model for layout analysis requires\nsignificant human effort. In this paper, we present an unsupervised deep\nlearning method for page segmentation, which revokes the need for annotated\nimages. A siamese neural network is trained to differentiate between patches\nusing their measurable properties such as number of foreground pixels, and\naverage component height and width. The network is trained that spatially\nnearby patches are similar. The network's learned features are used for page\nsegmentation, where patches are classified as main and side text based on the\nextracted features. We tested the method on a dataset of handwritten document\nimages with quite complex layouts. Our experiments show that the proposed\nunsupervised method is as effective as typical supervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 07:13:38 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Droby", "Ahmad", ""], ["Barakat", "Berat Kurar", ""], ["Madi", "Borak", ""], ["Alaasam", "Reem", ""], ["El-Sana", "Jihad", ""]]}, {"id": "2101.07512", "submitter": "Zhaoxia Yin", "authors": "Jie Wang, Zhaoxia Yin, Jing Jiang, and Yang Du", "title": "Attention-Guided Black-box Adversarial Attacks with Large-Scale\n  Multiobjective Evolutionary Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fooling deep neural networks (DNNs) with the black-box optimization has\nbecome a popular adversarial attack fashion, as the structural prior knowledge\nof DNNs is always unknown. Nevertheless, recent black-box adversarial attacks\nmay struggle to balance their attack ability and visual quality of the\ngenerated adversarial examples (AEs) in tackling high-resolution images. In\nthis paper, we propose an attention-guided black-box adversarial attack based\non the large-scale multiobjective evolutionary optimization, termed as LMOA. By\nconsidering the spatial semantic information of images, we firstly take\nadvantage of the attention map to determine the perturbed pixels. Instead of\nattacking the entire image, reducing the perturbed pixels with the attention\nmechanism can help to avoid the notorious curse of dimensionality and thereby\nimproves the performance of attacking. Secondly, a large-scale multiobjective\nevolutionary algorithm is employed to traverse the reduced pixels in the\nsalient region. Benefiting from its characteristics, the generated AEs have the\npotential to fool target DNNs while being imperceptible by the human vision.\nExtensive experimental results have verified the effectiveness of the proposed\nLMOA on the ImageNet dataset. More importantly, it is more competitive to\ngenerate high-resolution AEs with better visual quality compared with the\nexisting black-box adversarial attacks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 08:48:44 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Wang", "Jie", ""], ["Yin", "Zhaoxia", ""], ["Jiang", "Jing", ""], ["Du", "Yang", ""]]}, {"id": "2101.07518", "submitter": "Fu-Jen Tsai", "authors": "Fu-Jen Tsai*, Yan-Tsung Peng*, Yen-Yu Lin, Chung-Chi Tsai, and\n  Chia-Wen Lin", "title": "BANet: Blur-aware Attention Networks for Dynamic Scene Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image motion blur usually results from moving objects or camera shakes. Such\nblur is generally directional and non-uniform. Previous research efforts\nattempt to solve non-uniform blur by using self-recurrent multi-scale or\nmulti-patch architectures accompanying with self-attention. However, using\nself-recurrent frameworks typically leads to a longer inference time, while\ninter-pixel or inter-channel self-attention may cause excessive memory usage.\nThis paper proposes blur-aware attention networks (BANet) that accomplish\naccurate and efficient deblurring via a single forward pass. Our BANet utilizes\nregion-based self-attention with multi-kernel strip pooling to disentangle blur\npatterns of different degrees and with cascaded parallel dilated convolution to\naggregate multi-scale content features. Extensive experimental results on the\nGoPro and HIDE benchmarks demonstrate that the proposed BANet performs\nfavorably against the state-of-the-art in blurred image restoration and can\nprovide deblurred results in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:03:40 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 03:40:12 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Tsai*", "Fu-Jen", ""], ["Peng*", "Yan-Tsung", ""], ["Lin", "Yen-Yu", ""], ["Tsai", "Chung-Chi", ""], ["Lin", "Chia-Wen", ""]]}, {"id": "2101.07525", "submitter": "Zeming Li", "authors": "Zeming Li, Songtao Liu, Jian Sun", "title": "Momentum^2 Teacher: Momentum Teacher with Momentum Statistics for\n  Self-Supervised Learning", "comments": "11 pages, Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel approach, Momentum$^2$ Teacher, for\nstudent-teacher based self-supervised learning. The approach performs momentum\nupdate on both network weights and batch normalization (BN) statistics. The\nteacher's weight is a momentum update of the student, and the teacher's BN\nstatistics is a momentum update of those in history. The Momentum$^2$ Teacher\nis simple and efficient. It can achieve the state of the art results (74.5\\%)\nunder ImageNet linear evaluation protocol using small-batch size(\\eg, 128),\nwithout requiring large-batch training on special hardware like TPU or\ninefficient across GPU operation (\\eg, shuffling BN, synced BN). Our\nimplementation and pre-trained models will be given on\nGitHub\\footnote{https://github.com/zengarden/momentum2-teacher}.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:27:03 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Li", "Zeming", ""], ["Liu", "Songtao", ""], ["Sun", "Jian", ""]]}, {"id": "2101.07528", "submitter": "Edouard Oyallon", "authors": "Louis Thiry (DI-ENS), Michael Arbel (UCL), Eugene Belilovsky (MILA),\n  Edouard Oyallon (MLIA)", "title": "The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels\n  Methods", "comments": null, "journal-ref": "International Conference on Learning Representation (ICLR 2021),\n  2021, Vienna (online), Austria", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work showed that various forms of convolutional kernel\nmethods can be competitive with standard supervised deep convolutional networks\non datasets like CIFAR-10, obtaining accuracies in the range of 87-90% while\nbeing more amenable to theoretical analysis. In this work, we highlight the\nimportance of a data-dependent feature extraction step that is key to the\nobtain good performance in convolutional kernel methods. This step typically\ncorresponds to a whitened dictionary of patches, and gives rise to a\ndata-driven convolutional kernel methods. We extensively study its effect,\ndemonstrating it is the key ingredient for high performance of these methods.\nSpecifically, we show that one of the simplest instances of such kernel\nmethods, based on a single layer of image patches followed by a linear\nclassifier is already obtaining classification accuracies on CIFAR-10 in the\nsame range as previous more sophisticated convolutional kernel methods. We\nscale this method to the challenging ImageNet dataset, showing such a simple\napproach can exceed all existing non-learned representation methods. This is a\nnew baseline for object recognition without representation learning methods,\nthat initiates the investigation of convolutional kernel models on ImageNet. We\nconduct experiments to analyze the dictionary that we used, our ablations\nshowing they exhibit low-dimensional properties.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:30:58 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Thiry", "Louis", "", "DI-ENS"], ["Arbel", "Michael", "", "UCL"], ["Belilovsky", "Eugene", "", "MILA"], ["Oyallon", "Edouard", "", "MLIA"]]}, {"id": "2101.07529", "submitter": "Mattijs Baert", "authors": "Mattijs Baert, Sam Leroux, Pieter Simoens", "title": "Intelligent Frame Selection as a Privacy-Friendlier Alternative to Face\n  Recognition", "comments": "accepted for AAAI 2021 Workshop on Privacy-Preserving Artificial\n  Intelligence (PPAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread deployment of surveillance cameras for facial recognition\ngives rise to many privacy concerns. This study proposes a privacy-friendly\nalternative to large scale facial recognition. While there are multiple\ntechniques to preserve privacy, our work is based on the minimization principle\nwhich implies minimizing the amount of collected personal data. Instead of\nrunning facial recognition software on all video data, we propose to\nautomatically extract a high quality snapshot of each detected person without\nrevealing his or her identity. This snapshot is then encrypted and access is\nonly granted after legal authorization. We introduce a novel unsupervised face\nimage quality assessment method which is used to select the high quality\nsnapshots. For this, we train a variational autoencoder on high quality face\nimages from a publicly available dataset and use the reconstruction probability\nas a metric to estimate the quality of each face crop. We experimentally\nconfirm that the reconstruction probability can be used as biometric quality\npredictor. Unlike most previous studies, we do not rely on a manually defined\nface quality metric as everything is learned from data. Our face quality\nassessment method outperforms supervised, unsupervised and general image\nquality assessment methods on the task of improving face verification\nperformance by rejecting low quality images. The effectiveness of the whole\nsystem is validated qualitatively on still images and videos.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:31:42 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 13:29:00 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Baert", "Mattijs", ""], ["Leroux", "Sam", ""], ["Simoens", "Pieter", ""]]}, {"id": "2101.07538", "submitter": "Zhaoxia Yin", "authors": "Jie Wang, Zhaoxia Yin, Jin Tang, Jing Jiang, and Bin Luo", "title": "PICA: A Pixel Correlation-based Attentional Black-box Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The studies on black-box adversarial attacks have become increasingly\nprevalent due to the intractable acquisition of the structural knowledge of\ndeep neural networks (DNNs). However, the performance of emerging attacks is\nnegatively impacted when fooling DNNs tailored for high-resolution images. One\nof the explanations is that these methods usually focus on attacking the entire\nimage, regardless of its spatial semantic information, and thereby encounter\nthe notorious curse of dimensionality. To this end, we propose a pixel\ncorrelation-based attentional black-box adversarial attack, termed as PICA.\nFirstly, we take only one of every two neighboring pixels in the salient region\nas the target by leveraging the attentional mechanism and pixel correlation of\nimages, such that the dimension of the black-box attack reduces. After that, a\ngeneral multiobjective evolutionary algorithm is employed to traverse the\nreduced pixels and generate perturbations that are imperceptible by the human\nvision. Extensive experimental results have verified the effectiveness of the\nproposed PICA on the ImageNet dataset. More importantly, PICA is\ncomputationally more efficient to generate high-resolution adversarial examples\ncompared with the existing black-box attacks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:53:52 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Wang", "Jie", ""], ["Yin", "Zhaoxia", ""], ["Tang", "Jin", ""], ["Jiang", "Jing", ""], ["Luo", "Bin", ""]]}, {"id": "2101.07542", "submitter": "Berat Kurar Barakat", "authors": "Berat Kurar Barakat, Rafi Cohen, Irina Rabaev, and Jihad El-Sana", "title": "VML-MOC: Segmenting a multiply oriented and curved handwritten text\n  lines dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper publishes a natural and very complicated dataset of handwritten\ndocuments with multiply oriented and curved text lines, namely VML-MOC dataset.\nThese text lines were written as remarks on the page margins by different\nwriters over the years. They appear at different locations within the\norientations that range between 0 and 180 or as curvilinear forms. We evaluate\na multi-oriented Gaussian based method to segment these handwritten text lines\nthat are skewed or curved in any orientation. It achieves a mean pixel\nIntersection over Union score of 80.96% on the test documents. The results are\ncompared with the results of a single-oriented Gaussian based text line\nsegmentation method.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 10:10:45 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Barakat", "Berat Kurar", ""], ["Cohen", "Rafi", ""], ["Rabaev", "Irina", ""], ["El-Sana", "Jihad", ""]]}, {"id": "2101.07549", "submitter": "Niels Ole Salscheider", "authors": "Niels Ole Salscheider", "title": "Object Tracking by Detection with Visual and Motion Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving cars and other autonomous vehicles need to detect and track\nobjects in camera images. We present a simple online tracking algorithm that is\nbased on a constant velocity motion model with a Kalman filter, and an\nassignment heuristic. The assignment heuristic relies on four metrics: An\nembedding vector that describes the appearance of objects and can be used to\nre-identify them, a displacement vector that describes the object movement\nbetween two consecutive video frames, the Mahalanobis distance between the\nKalman filter states and the new detections, and a class distance. These\nmetrics are combined with a linear SVM, and then the assignment problem is\nsolved by the Hungarian algorithm. We also propose an efficient CNN\narchitecture that estimates these metrics. Our multi-frame model accepts two\nconsecutive video frames which are processed individually in the backbone, and\nthen optical flow is estimated on the resulting feature maps. This allows the\nnetwork heads to estimate the displacement vectors. We evaluate our approach on\nthe challenging BDD100K tracking dataset. Our multi-frame model achieves a good\nMOTA value of 39.1% with low localization error of 0.206 in MOTP. Our fast\nsingle-frame model achieves an even lower localization error of 0.202 in MOTP,\nand a MOTA value of 36.8%.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 10:29:16 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Salscheider", "Niels Ole", ""]]}, {"id": "2101.07555", "submitter": "Ru Li", "authors": "Ru Li, Shuaicheng Liu, Guangfu Wang, Guanghui Liu and Bing Zeng", "title": "JigsawGAN: Self-supervised Learning for Solving Jigsaw Puzzles with\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a solution based on Generative Adversarial Network (GAN)\nfor solving jigsaw puzzles. The problem assumes that an image is cut into equal\nsquare pieces, and asks to recover the image according to pieces information.\nConventional jigsaw solvers often determine piece relationships based on the\npiece boundaries, which ignore the important semantic information. In this\npaper, we propose JigsawGAN, a GAN-based self-supervised method for solving\njigsaw puzzles with unpaired images (with no prior knowledge of the initial\nimages). We design a multi-task pipeline that includes, (1) a classification\nbranch to classify jigsaw permutations, and (2) a GAN branch to recover\nfeatures to images with correct orders. The classification branch is\nconstrained by the pseudo-labels generated according to the shuffled pieces.\nThe GAN branch concentrates on the image semantic information, among which the\ngenerator produces the natural images to fool the discriminator with\nreassembled pieces, while the discriminator distinguishes whether a given image\nbelongs to the synthesized or the real target manifold. These two branches are\nconnected by a flow-based warp that is applied to warp features to correct\norder according to the classification results. The proposed method can solve\njigsaw puzzles more efficiently by utilizing both semantic information and edge\ninformation simultaneously. Qualitative and quantitative comparisons against\nseveral leading prior methods demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 10:40:38 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Li", "Ru", ""], ["Liu", "Shuaicheng", ""], ["Wang", "Guangfu", ""], ["Liu", "Guanghui", ""], ["Zeng", "Bing", ""]]}, {"id": "2101.07563", "submitter": "Kathryn Schutte", "authors": "Kathryn Schutte, Olivier Moindrot, Paul H\\'erent, Jean-Baptiste\n  Schiratti, Simon J\\'egou", "title": "Using StyleGAN for Visual Interpretability of Deep Learning Models on\n  Medical Images", "comments": "Accepted for oral session of Medical Imaging meets NeurIPS 2020\n  workshop:\n  http://www.cse.cuhk.edu.hk/~qdou/public/medneurips2020/70_neurips2020_cameraready_opt.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As AI-based medical devices are becoming more common in imaging fields like\nradiology and histology, interpretability of the underlying predictive models\nis crucial to expand their use in clinical practice. Existing heatmap-based\ninterpretability methods such as GradCAM only highlight the location of\npredictive features but do not explain how they contribute to the prediction.\nIn this paper, we propose a new interpretability method that can be used to\nunderstand the predictions of any black-box model on images, by showing how the\ninput image would be modified in order to produce different predictions. A\nStyleGAN is trained on medical images to provide a mapping between latent\nvectors and images. Our method identifies the optimal direction in the latent\nspace to create a change in the model prediction. By shifting the latent\nrepresentation of an input image along this direction, we can produce a series\nof new synthetic images with changed predictions. We validate our approach on\nhistology and radiology images, and demonstrate its ability to provide\nmeaningful explanations that are more informative than GradCAM heatmaps. Our\nmethod reveals the patterns learned by the model, which allows clinicians to\nbuild trust in the model's predictions, discover new biomarkers and eventually\nreveal potential biases.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:13:20 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Schutte", "Kathryn", ""], ["Moindrot", "Olivier", ""], ["H\u00e9rent", "Paul", ""], ["Schiratti", "Jean-Baptiste", ""], ["J\u00e9gou", "Simon", ""]]}, {"id": "2101.07571", "submitter": "Tomoe Kishimoto", "authors": "Tomoe Kishimoto, Masahiko Saito, Junichi Tanaka, Yutaro Iiyama, Ryu\n  Sawada and Koji Terashi", "title": "An Improvement of Object Detection Performance using Multi-step Machine\n  Learnings", "comments": "Submitted to ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connecting multiple machine learning models into a pipeline is effective for\nhandling complex problems. By breaking down the problem into steps, each\ntackled by a specific component model of the pipeline, the overall solution can\nbe made accurate and explainable. This paper describes an enhancement of object\ndetection based on this multi-step concept, where a post-processing step called\nthe calibration model is introduced. The calibration model consists of a\nconvolutional neural network, and utilizes rich contextual information based on\nthe domain knowledge of the input. Improvements of object detection performance\nby 0.8-1.9 in average precision metric over existing object detectors have been\nobserved using the new model.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:32:27 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Kishimoto", "Tomoe", ""], ["Saito", "Masahiko", ""], ["Tanaka", "Junichi", ""], ["Iiyama", "Yutaro", ""], ["Sawada", "Ryu", ""], ["Terashi", "Koji", ""]]}, {"id": "2101.07576", "submitter": "Rita Pucci", "authors": "Rita Pucci, Christian Micheloni, Niki Martinel", "title": "Collaboration among Image and Object Level Features for Image\n  Colourisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image colourisation is an ill-posed problem, with multiple correct solutions\nwhich depend on the context and object instances present in the input datum.\nPrevious approaches attacked the problem either by requiring intense user\ninteractions or by exploiting the ability of convolutional neural networks\n(CNNs) in learning image level (context) features. However, obtaining human\nhints is not always feasible and CNNs alone are not able to learn object-level\nsemantics unless multiple models pretrained with supervision are considered. In\nthis work, we propose a single network, named UCapsNet, that separate\nimage-level features obtained through convolutions and object-level features\ncaptured by means of capsules. Then, by skip connections over different layers,\nwe enforce collaboration between such disentangling factors to produce high\nquality and plausible image colourisation. We pose the problem as a\nclassification task that can be addressed by a fully self-supervised approach,\nthus requires no human effort. Experimental results on three benchmark datasets\nshow that our approach outperforms existing methods on standard quality metrics\nand achieves a state of the art performances on image colourisation. A large\nscale user study shows that our method is preferred over existing solutions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:48:12 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Pucci", "Rita", ""], ["Micheloni", "Christian", ""], ["Martinel", "Niki", ""]]}, {"id": "2101.07589", "submitter": "Dengxin Dai", "authors": "Ke Li, Dengxin Dai, Ender Konukoglu, Luc Van Gool", "title": "Hyperspectral Image Super-Resolution with Spectral Mixup and\n  Heterogeneous Datasets", "comments": "16 pages, 14 tables, 5 figures; Code available at\n  https://github.com/kli8996/HSISR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies Hyperspectral image (HSI) super-resolution (SR). HSI SR is\ncharacterized by high-dimensional data and a limited amount of training\nexamples. This exacerbates the undesirable behaviors of neural networks such as\nmemorization and sensitivity to out-of-distribution samples. This work\naddresses these issues with three contributions. First, we observe that HSI SR\nand RGB image SR are correlated and develop a novel multi-tasking network to\ntrain them jointly so that the auxiliary task RGB image SR can provide\nadditional supervision. Second, we propose a simple, yet effective data\naugmentation routine, termed Spectral Mixup, to construct effective virtual\ntraining samples to enlarge the training set. Finally, we extend the network to\na semi-supervised setting so that it can learn from datasets containing only\nlow-resolution HSIs. With these contributions, our method is able to learn from\nheterogeneous datasets and lift the requirement for having a large amount of HD\nHSI training samples. Extensive experiments on four standard datasets show that\nour method outperforms existing methods significantly and underpin the\nrelevance of our contributions. Code has been made available at\nhttps://github.com/kli8996/HSISR.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:19:53 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 11:00:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Li", "Ke", ""], ["Dai", "Dengxin", ""], ["Konukoglu", "Ender", ""], ["Van Gool", "Luc", ""]]}, {"id": "2101.07594", "submitter": "Ken Deng", "authors": "Ken Deng, Chang Sun, Yitong Liu, Hongwen Yang", "title": "Real-Time Limited-View CT Inpainting and Reconstruction with Dual Domain\n  Based on Spatial Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-dose Computed Tomography is a common issue in reality. Current reduction,\nsparse sampling and limited-view scanning can all cause it. Between them,\nlimited-view CT is general in the industry due to inevitable mechanical and\nphysical limitation. However, limited-view CT can cause serious imaging problem\non account of its massive information loss. Thus, we should effectively utilize\nthe scant prior information to perform completion. It is an undeniable fact\nthat CT imaging slices are extremely dense, which leads to high continuity\nbetween successive images. We realized that fully exploit the spatial\ncorrelation between consecutive frames can significantly improve restoration\nresults in video inpainting. Inspired by this, we propose a deep learning-based\nthree-stage algorithm that hoist limited-view CT imaging quality based on\nspatial information. In stage one, to better utilize prior information in the\nRadon domain, we design an adversarial autoencoder to complement the Radon\ndata. In the second stage, a model is built to perform inpainting based on\nspatial continuity in the image domain. At this point, we have roughly restored\nthe imaging, while its texture still needs to be finely repaired. Hence, we\npropose a model to accurately restore the image in stage three, and finally\nachieve an ideal inpainting result. In addition, we adopt FBP instead of\nSART-TV to make our algorithm more suitable for real-time use. In the\nexperiment, we restore and reconstruct the Radon data that has been cut the\nrear one-third part, they achieve PSNR of 40.209, SSIM of 0.943, while\nprecisely present the texture.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:42:58 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Deng", "Ken", ""], ["Sun", "Chang", ""], ["Liu", "Yitong", ""], ["Yang", "Hongwen", ""]]}, {"id": "2101.07606", "submitter": "Viraj Kulkarni", "authors": "Tanveer Gupte, Mrunmai Niljikar, Manish Gawali, Viraj Kulkarni, Amit\n  Kharat, Aniruddha Pant", "title": "Deep Learning Models for Calculation of Cardiothoracic Ratio from Chest\n  Radiographs for Assisted Diagnosis of Cardiomegaly", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an automated method based on deep learning to compute the\ncardiothoracic ratio and detect the presence of cardiomegaly from chest\nradiographs. We develop two separate models to demarcate the heart and chest\nregions in an X-ray image using bounding boxes and use their outputs to\ncalculate the cardiothoracic ratio. We obtain a sensitivity of 0.96 at a\nspecificity of 0.81 with a mean absolute error of 0.0209 on a held-out test\ndataset and a sensitivity of 0.84 at a specificity of 0.97 with a mean absolute\nerror of 0.018 on an independent dataset from a different hospital. We also\ncompare three different segmentation model architectures for the proposed\nmethod and observe that Attention U-Net yields better results than SE-Resnext\nU-Net and EfficientNet U-Net. By providing a numeric measurement of the\ncardiothoracic ratio, we hope to mitigate human subjectivity arising out of\nvisual assessment in the detection of cardiomegaly.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:09:29 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Gupte", "Tanveer", ""], ["Niljikar", "Mrunmai", ""], ["Gawali", "Manish", ""], ["Kulkarni", "Viraj", ""], ["Kharat", "Amit", ""], ["Pant", "Aniruddha", ""]]}, {"id": "2101.07612", "submitter": "Viraj Kulkarni", "authors": "Abhishek Shivdeo, Rohit Lokwani, Viraj Kulkarni, Amit Kharat,\n  Aniruddha Pant", "title": "Comparative Evaluation of 3D and 2D Deep Learning Techniques for\n  Semantic Segmentation in CT Scans", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation plays a pivotal role in several medical-imaging\napplications by assisting the segmentation of the regions of interest. Deep\nlearning-based approaches have been widely adopted for semantic segmentation of\nmedical data. In recent years, in addition to 2D deep learning architectures,\n3D architectures have been employed as the predictive algorithms for 3D medical\nimage data. In this paper, we propose a 3D stack-based deep learning technique\nfor segmenting manifestations of consolidation and ground-glass opacities in 3D\nComputed Tomography (CT) scans. We also present a comparison based on the\nsegmentation results, the contextual information retained, and the inference\ntime between this 3D technique and a traditional 2D deep learning technique. We\nalso define the area-plot, which represents the peculiar pattern observed in\nthe slice-wise areas of the pathology regions predicted by these deep learning\nmodels. In our exhaustive evaluation, 3D technique performs better than the 2D\ntechnique for the segmentation of CT scans. We get dice scores of 79% and 73%\nfor the 3D and the 2D techniques respectively. The 3D technique results in a 5X\nreduction in the inference time compared to the 2D technique. Results also show\nthat the area-plots predicted by the 3D model are more similar to the ground\ntruth than those predicted by the 2D model. We also show how increasing the\namount of contextual information retained during the training can improve the\n3D model's performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:23:43 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Shivdeo", "Abhishek", ""], ["Lokwani", "Rohit", ""], ["Kulkarni", "Viraj", ""], ["Kharat", "Amit", ""], ["Pant", "Aniruddha", ""]]}, {"id": "2101.07613", "submitter": "Ken Deng", "authors": "Yitong Liu, Ken Deng, Chang Sun, Hongwen Yang", "title": "A Lightweight Structure Aimed to Utilize Spatial Correlation for\n  Sparse-View CT Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse-view computed tomography (CT) is known as a widely used approach to\nreduce radiation dose while accelerating imaging through lowered projection\nviews and correlated calculations. However, its severe imaging noise and\nstreaking artifacts turn out to be a major issue in the low dose protocol. In\nthis paper, we propose a dual-domain deep learning-based method that breaks\nthrough the limitations of currently prevailing algorithms that merely process\nsingle image slices. Since the scanned object usually contains a high degree of\nspatial continuity, the obtained consecutive imaging slices embody rich\ninformation that is largely unexplored. Therefore, we establish a cascade model\nnamed LS-AAE which aims to tackle the above problem. In addition, in order to\nadapt to the social trend of lightweight medical care, our model adopts the\ninverted residual with linear bottleneck in the module design to make it mobile\nand lightweight (reduce model parameters to one-eighth of its original) without\nsacrificing its performance. In our experiments, sparse sampling is conducted\nat intervals of 4{\\deg}, 8{\\deg} and 16{\\deg}, which appears to be a\nchallenging sparsity that few scholars have attempted before. Nevertheless, our\nmethod still exhibits its robustness and achieves the state-of-the-art\nperformance by reaching the PSNR of 40.305 and the SSIM of 0.948, while\nensuring high model mobility. Particularly, it still exceeds other current\nmethods when the sampling rate is one-fourth of them, thereby demonstrating its\nremarkable superiority.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:26:17 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Liu", "Yitong", ""], ["Deng", "Ken", ""], ["Sun", "Chang", ""], ["Yang", "Hongwen", ""]]}, {"id": "2101.07618", "submitter": "Qian Huang", "authors": "Chang Li and Qian Huang and Xing Li and Qianhan Wu", "title": "Human Action Recognition Based on Multi-scale Feature Maps from Depth\n  Video Sequences", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition is an active research area in computer vision.\nAlthough great process has been made, previous methods mostly recognize actions\nbased on depth data at only one scale, and thus they often neglect multi-scale\nfeatures that provide additional information action recognition in practical\napplication scenarios. In this paper, we present a novel framework focusing on\nmulti-scale motion information to recognize human actions from depth video\nsequences. We propose a multi-scale feature map called Laplacian pyramid depth\nmotion images(LP-DMI). We employ depth motion images (DMI) as the templates to\ngenerate the multi-scale static representation of actions. Then, we caculate\nLP-DMI to enhance multi-scale dynamic information of motions and reduces\nredundant static information in human bodies. We further extract the\nmulti-granularity descriptor called LP-DMI-HOG to provide more discriminative\nfeatures. Finally, we utilize extreme learning machine (ELM) for action\nclassification. The proposed method yeilds the recognition accuracy of 93.41%,\n85.12%, 91.94% on public MSRAction3D dataset, UTD-MHAD and DHA dataset. Through\nextensive experiments, we prove that our method outperforms state-of-the-art\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:46:42 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Li", "Chang", ""], ["Huang", "Qian", ""], ["Li", "Xing", ""], ["Wu", "Qianhan", ""]]}, {"id": "2101.07653", "submitter": "Sven Koehler", "authors": "Sven Koehler, Tarique Hussain, Zach Blair, Tyler Huffaker, Florian\n  Ritzmann, Animesh Tandon, Thomas Pickardt, Samir Sarikouch, Heiner Latus,\n  Gerald Greil, Ivo Wolf, Sandy Engelhardt", "title": "Unsupervised Domain Adaptation from Axial to Short-Axis Multi-Slice\n  Cardiac MR Images by Incorporating Pretrained Task Networks", "comments": "Accepted for IEEE Transaction on Medical Imaging (TMI) 2021 on\n  13.01.2021", "journal-ref": null, "doi": "10.1109/TMI.2021.3052972", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Anisotropic multi-slice Cardiac Magnetic Resonance (CMR) Images are\nconventionally acquired in patient-specific short-axis (SAX) orientation. In\nspecific cardiovascular diseases that affect right ventricular (RV) morphology,\nacquisitions in standard axial (AX) orientation are preferred by some\ninvestigators, due to potential superiority in RV volume measurement for\ntreatment planning. Unfortunately, due to the rare occurrence of these\ndiseases, data in this domain is scarce. Recent research in deep learning-based\nmethods mainly focused on SAX CMR images and they had proven to be very\nsuccessful. In this work, we show that there is a considerable domain shift\nbetween AX and SAX images, and therefore, direct application of existing models\nyield sub-optimal results on AX samples. We propose a novel unsupervised domain\nadaptation approach, which uses task-related probabilities in an attention\nmechanism. Beyond that, cycle consistency is imposed on the learned\npatient-individual 3D rigid transformation to improve stability when\nautomatically re-sampling the AX images to SAX orientations. The network was\ntrained on 122 registered 3D AX-SAX CMR volume pairs from a multi-centric\npatient cohort. A mean 3D Dice of $0.86\\pm{0.06}$ for the left ventricle,\n$0.65\\pm{0.08}$ for the myocardium, and $0.77\\pm{0.10}$ for the right ventricle\ncould be achieved. This is an improvement of $25\\%$ in Dice for RV in\ncomparison to direct application on axial slices. To conclude, our pre-trained\ntask module has neither seen CMR images nor labels from the target domain, but\nis able to segment them after the domain gap is reduced. Code:\nhttps://github.com/Cardio-AI/3d-mri-domain-adaptation\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:39:30 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 08:25:53 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Koehler", "Sven", ""], ["Hussain", "Tarique", ""], ["Blair", "Zach", ""], ["Huffaker", "Tyler", ""], ["Ritzmann", "Florian", ""], ["Tandon", "Animesh", ""], ["Pickardt", "Thomas", ""], ["Sarikouch", "Samir", ""], ["Latus", "Heiner", ""], ["Greil", "Gerald", ""], ["Wolf", "Ivo", ""], ["Engelhardt", "Sandy", ""]]}, {"id": "2101.07654", "submitter": "Yuzhe Lu", "authors": "Yuzhe Lu, Haichun Yang, Zheyu Zhu, Ruining Deng, Agnes B. Fogo, and\n  Yuankai Huo", "title": "Improve Global Glomerulosclerosis Classification with Imbalanced Data\n  using CircleMix Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of glomerular lesions is a routine and essential task in\nrenal pathology. Recently, machine learning approaches, especially deep\nlearning algorithms, have been used to perform computer-aided lesion\ncharacterization of glomeruli. However, one major challenge of developing such\nmethods is the naturally imbalanced distribution of different lesions. In this\npaper, we propose CircleMix, a novel data augmentation technique, to improve\nthe accuracy of classifying globally sclerotic glomeruli with a hierarchical\nlearning strategy. Different from the recently proposed CutMix method, the\nCircleMix augmentation is optimized for the ball-shaped biomedical objects,\nsuch as glomeruli. 6,861 glomeruli with five classes (normal, periglomerular\nfibrosis, obsolescent glomerulosclerosis, solidified glomerulosclerosis, and\ndisappearing glomerulosclerosis) were employed to develop and evaluate the\nproposed methods. From five-fold cross-validation, the proposed CircleMix\naugmentation achieved superior performance (Balanced Accuracy=73.0%) compared\nwith the EfficientNet-B0 baseline (Balanced Accuracy=69.4%)\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:35:38 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Lu", "Yuzhe", ""], ["Yang", "Haichun", ""], ["Zhu", "Zheyu", ""], ["Deng", "Ruining", ""], ["Fogo", "Agnes B.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2101.07663", "submitter": "Dingwen Zhang", "authors": "Mingchen Zhuge, Deng-Ping Fan, Nian Liu, Dingwen Zhang, Dong Xu, and\n  Ling Shao", "title": "Salient Object Detection via Integrity Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Albeit current salient object detection (SOD) works have achieved fantastic\nprogress, they are cast into the shade when it comes to the integrity of the\npredicted salient regions. We define the concept of integrity at both the micro\nand macro level. Specifically, at the micro level, the model should highlight\nall parts that belong to a certain salient object, while at the macro level,\nthe model needs to discover all salient objects from the given image scene. To\nfacilitate integrity learning for salient object detection, we design a novel\nIntegrity Cognition Network (ICON), which explores three important components\nto learn strong integrity features. 1) Unlike the existing models that focus\nmore on feature discriminability, we introduce a diverse feature aggregation\n(DFA) component to aggregate features with various receptive fields (i.e.,,\nkernel shape and context) and increase the feature diversity. Such diversity is\nthe foundation for mining the integral salient objects. 2) Based on the DFA\nfeatures, we introduce the integrity channel enhancement (ICE) component with\nthe goal of enhancing feature channels that highlight the integral salient\nobjects at the macro level, while suppressing the other distracting ones. 3)\nAfter extracting the enhanced features, the part-whole verification (PWV)\nmethod is employed to determine whether the part and whole object features have\nstrong agreement. Such part-whole agreements can further improve the\nmicro-level integrity for each salient object. To demonstrate the effectiveness\nof ICON, comprehensive experiments are conducted on seven challenging\nbenchmarks, where promising results are achieved.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:53:12 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 03:55:27 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 07:01:56 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhuge", "Mingchen", ""], ["Fan", "Deng-Ping", ""], ["Liu", "Nian", ""], ["Zhang", "Dingwen", ""], ["Xu", "Dong", ""], ["Shao", "Ling", ""]]}, {"id": "2101.07681", "submitter": "Sheng Liu", "authors": "Sheng Liu, Xiaozhen Xie and Wenfeng Kong", "title": "Hyperspectral Image Restoration via Multi-mode and Double-weighted\n  Tensor Nuclear Norm Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tensor nuclear norm (TNN) induced by tensor singular value decomposition\nplays an important role in hyperspectral image (HSI) restoration tasks. In this\nletter, we first consider three inconspicuous but crucial phenomenons in TNN.\nIn the Fourier transform domain of HSIs, different frequency components contain\ndifferent information; different singular values of each frequency component\nalso represent different information. The two physical phenomenons lie not only\nin the spectral dimension but also in the spatial dimensions. Then, to improve\nthe capability and flexibility of TNN for HSI restoration, we propose a\nmulti-mode and double-weighted TNN based on the above three crucial\nphenomenons. It can adaptively shrink the frequency components and singular\nvalues according to their physical meanings in all modes of HSIs. In the\nframework of the alternating direction method of multipliers, we design an\neffective alternating iterative strategy to optimize our proposed model.\nRestoration experiments on both synthetic and real HSI datasets demonstrate\ntheir superiority against related methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:20:38 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 04:31:17 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Liu", "Sheng", ""], ["Xie", "Xiaozhen", ""], ["Kong", "Wenfeng", ""]]}, {"id": "2101.07709", "submitter": "Nicholas Marshall", "authors": "Tamir Bendory, Ti-Yen Lan, Nicholas F. Marshall, Iris Rukshin, Amit\n  Singer", "title": "Multi-target detection with rotations", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi-target detection problem of estimating a\ntwo-dimensional target image from a large noisy measurement image that contains\nmany randomly rotated and translated copies of the target image. Motivated by\nsingle-particle cryo-electron microscopy, we focus on the low signal-to-noise\nregime, where it is difficult to estimate the locations and orientations of the\ntarget images in the measurement. Our approach uses autocorrelation analysis to\nestimate rotationally and translationally invariant features of the target\nimage. We demonstrate that, regardless of the level of noise, our technique can\nbe used to recover the target image when the measurement is sufficiently large.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:20:01 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bendory", "Tamir", ""], ["Lan", "Ti-Yen", ""], ["Marshall", "Nicholas F.", ""], ["Rukshin", "Iris", ""], ["Singer", "Amit", ""]]}, {"id": "2101.07715", "submitter": "David Bouget", "authors": "David Bouget, Andr\\'e Pedersen, Sayied Abdol Mohieb Hosainey, Ole\n  Solheim, Ingerid Reinertsen", "title": "Meningioma segmentation in T1-weighted MRI leveraging global context and\n  attention mechanisms", "comments": "16 pages, 5 figures, 3 tables. Submitted to Artificial Intelligence\n  in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meningiomas are the most common type of primary brain tumor, accounting for\napproximately 30% of all brain tumors. A substantial number of these tumors are\nnever surgically removed but rather monitored over time. Automatic and precise\nmeningioma segmentation is therefore beneficial to enable reliable growth\nestimation and patient-specific treatment planning. In this study, we propose\nthe inclusion of attention mechanisms over a U-Net architecture: (i)\nAttention-gated U-Net (AGUNet) and (ii) Dual Attention U-Net (DAUNet), using a\n3D MRI volume as input. Attention has the potential to leverage the global\ncontext and identify features' relationships across the entire volume. To limit\nspatial resolution degradation and loss of detail inherent to encoder-decoder\narchitectures, we studied the impact of multi-scale input and deep supervision\ncomponents. The proposed architectures are trainable end-to-end and each\nconcept can be seamlessly disabled for ablation studies. The validation studies\nwere performed using a 5-fold cross validation over 600 T1-weighted MRI volumes\nfrom St. Olavs University Hospital, Trondheim, Norway. For the best performing\narchitecture, an average Dice score of 81.6% was reached for an F1-score of\n95.6%. With an almost perfect precision of 98%, meningiomas smaller than 3ml\nwere occasionally missed hence reaching an overall recall of 93%. Leveraging\nglobal context from a 3D MRI volume provided the best performances, even if the\nnative volume resolution could not be processed directly. Overall, near-perfect\ndetection was achieved for meningiomas larger than 3ml which is relevant for\nclinical use. In the future, the use of multi-scale designs and refinement\nnetworks should be further investigated to improve the performance. A larger\nnumber of cases with meningiomas below 3ml might also be needed to improve the\nperformance for the smallest tumors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:40:53 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bouget", "David", ""], ["Pedersen", "Andr\u00e9", ""], ["Hosainey", "Sayied Abdol Mohieb", ""], ["Solheim", "Ole", ""], ["Reinertsen", "Ingerid", ""]]}, {"id": "2101.07717", "submitter": "S M Raju", "authors": "Sheikh Md Hanif Hossain, S M Raju and Amelia Ritahani Ismail", "title": "Predicting Pneumonia and Region Detection from X-Ray Images using Deep\n  Neural Network", "comments": "5 figures, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biomedical images are increasing drastically. Along the way, many machine\nlearning algorithms have been proposed to predict and identify various kinds of\ndiseases. One such disease is Pneumonia which is an infection caused by both\nbacteria and viruses through the inflammation of a person's lung air sacs. In\nthis paper, an algorithm was proposed that receives x-ray images as input and\nverifies whether this patient is infected by Pneumonia as well as specific\nregion of the lungs that the inflammation has occurred at. The algorithm is\nbased on the transfer learning mechanism where pre-trained ResNet-50\n(Convolutional Neural Network) was used followed by some custom layer for\nmaking the prediction. The model has achieved an accuracy of 90.6 percent which\nconfirms that the model is effective and can be implemented for the detection\nof Pneumonia in patients. Furthermore, a class activation map is used for the\ndetection of the infected region in the lungs. Also, PneuNet was developed so\nthat users can access more easily and use the services.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:43:05 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Hossain", "Sheikh Md Hanif", ""], ["Raju", "S M", ""], ["Ismail", "Amelia Ritahani", ""]]}, {"id": "2101.07719", "submitter": "Wei-Chiu Ma", "authors": "Wei-Chiu Ma, Shenlong Wang, Jiayuan Gu, Sivabalan Manivasagam, Antonio\n  Torralba, Raquel Urtasun", "title": "Deep Feedback Inverse Problem Solver", "comments": "ECCV 2020 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient, effective, and generic approach towards solving\ninverse problems. The key idea is to leverage the feedback signal provided by\nthe forward process and learn an iterative update model. Specifically, at each\niteration, the neural network takes the feedback as input and outputs an update\non the current estimation. Our approach does not have any restrictions on the\nforward process; it does not require any prior knowledge either. Through the\nfeedback information, our model not only can produce accurate estimations that\nare coherent to the input observation but also is capable of recovering from\nearly incorrect predictions. We verify the performance of our approach over a\nwide range of inverse problems, including 6-DOF pose estimation, illumination\nestimation, as well as inverse kinematics. Comparing to traditional\noptimization-based methods, we can achieve comparable or better performance\nwhile being two to three orders of magnitude faster. Compared to deep\nlearning-based approaches, our model consistently improves the performance on\nall metrics. Please refer to the project page for videos, animations,\nsupplementary materials, etc.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:49:06 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Ma", "Wei-Chiu", ""], ["Wang", "Shenlong", ""], ["Gu", "Jiayuan", ""], ["Manivasagam", "Sivabalan", ""], ["Torralba", "Antonio", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.07720", "submitter": "Peer Neubert", "authors": "Peer Neubert and Stefan Schubert", "title": "Hyperdimensional computing as a framework for systematic aggregation of\n  image descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and video descriptors are an omnipresent tool in computer vision and\nits application fields like mobile robotics. Many hand-crafted and in\nparticular learned image descriptors are numerical vectors with a potentially\n(very) large number of dimensions. Practical considerations like memory\nconsumption or time for comparisons call for the creation of compact\nrepresentations. In this paper, we use hyperdimensional computing (HDC) as an\napproach to systematically combine information from a set of vectors in a\nsingle vector of the same dimensionality. HDC is a known technique to perform\nsymbolic processing with distributed representation in numerical vectors with\nthousands of dimensions. We present a HDC implementation that is suitable for\nprocessing the output of existing and future (deep-learning based) image\ndescriptors. We discuss how this can be used as a framework to process\ndescriptors together with additional knowledge by simple and fast vector\noperations. A concrete outcome is a novel HDC-based approach to aggregate a set\nof local image descriptors together with their image positions in a single\nholistic descriptor. The comparison to available holistic descriptors and\naggregation methods on a series of standard mobile robotics place recognition\nexperiments shows a 20% improvement in average performance compared to\nrunner-up and 3.6x better worst-case performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:49:58 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Neubert", "Peer", ""], ["Schubert", "Stefan", ""]]}, {"id": "2101.07721", "submitter": "Simon Bohlender", "authors": "Simon Bohlender, Ilkay Oksuz, Anirban Mukhopadhyay", "title": "A survey on shape-constraint deep learning for medical image\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the advent of U-Net, fully convolutional deep neural networks and its\nmany variants have completely changed the modern landscape of deep learning\nbased medical image segmentation. However, the over dependence of these methods\non pixel level classification and regression has been identified early on as a\nproblem. Especially when trained on medical databases with sparse available\nannotation, these methods are prone to generate segmentation artifacts such as\nfragmented structures, topological inconsistencies and islands of pixel. These\nartefacts are especially problematic in medical imaging since segmentation is\nalmost always a pre-processing step for some downstream evaluation. The range\nof possible downstream evaluations is rather big, for example surgical\nplanning, visualization, shape analysis, prognosis, treatment planning etc.\nHowever, one common thread across all these downstream tasks is the demand of\nanatomical consistency. To ensure the segmentation result is anatomically\nconsistent, approaches based on Markov/ Conditional Random Fields, Statistical\nShape Models are becoming increasingly popular over the past 5 years. In this\nreview paper, a broad overview of recent literature on bringing anatomical\nconstraints for medical image segmentation is given, the shortcomings and\nopportunities of the proposed methods are thoroughly discussed and potential\nfuture work is elaborated. We review the most relevant papers published until\nthe submission date. For quick access, important details such as the underlying\nmethod, datasets and performance are tabulated.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:52:10 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bohlender", "Simon", ""], ["Oksuz", "Ilkay", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "2101.07755", "submitter": "Tolga Birdal", "authors": "Tolga Birdal, Vladislav Golyanik, Christian Theobalt, Leonidas Guibas", "title": "Quantum Permutation Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV cs.ET cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present QuantumSync, the first quantum algorithm for solving a\nsynchronization problem in the context of computer vision. In particular, we\nfocus on permutation synchronization which involves solving a non-convex\noptimization problem in discrete variables. We start by formulating\nsynchronization into a quadratic unconstrained binary optimization problem\n(QUBO). While such formulation respects the binary nature of the problem,\nensuring that the result is a set of permutations requires extra care. Hence,\nwe: (i) show how to insert permutation constraints into a QUBO problem and (ii)\nsolve the constrained QUBO problem on the current generation of the adiabatic\nquantum computers D-Wave. Thanks to the quantum annealing, we guarantee global\noptimality with high probability while sampling the energy landscape to yield\nconfidence estimates. Our proof-of-concepts realization on the adiabatic D-Wave\ncomputer demonstrates that quantum machines offer a promising way to solve the\nprevalent yet difficult synchronization problems.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 17:51:02 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Birdal", "Tolga", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2101.07757", "submitter": "Milad Sikaroudi", "authors": "Milad Sikaroudi, Benyamin Ghojogh, Fakhri Karray, Mark Crowley, H.R.\n  Tizhoosh", "title": "Magnification Generalization for Histopathology Image Embedding", "comments": "Accepted for presentation at International Symposium on Biomedical\n  Imaging (ISBI'2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Histopathology image embedding is an active research area in computer vision.\nMost of the embedding models exclusively concentrate on a specific\nmagnification level. However, a useful task in histopathology embedding is to\ntrain an embedding space regardless of the magnification level. Two main\napproaches for tackling this goal are domain adaptation and domain\ngeneralization, where the target magnification levels may or may not be\nintroduced to the model in training, respectively. Although magnification\nadaptation is a well-studied topic in the literature, this paper, to the best\nof our knowledge, is the first work on magnification generalization for\nhistopathology image embedding. We use an episodic trainable domain\ngeneralization technique for magnification generalization, namely Model\nAgnostic Learning of Semantic Features (MASF), which works based on the Model\nAgnostic Meta-Learning (MAML) concept. Our experimental results on a breast\ncancer histopathology dataset with four different magnification levels show the\nproposed method's effectiveness for magnification generalization.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:46:26 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Sikaroudi", "Milad", ""], ["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "2101.07831", "submitter": "Flora Dellinger", "authors": "Flora Dellinger, Thomas Boulay, Diego Mendoza Barrenechea, Said\n  El-Hachimi, Isabelle Leang, Fabian B\\\"urger", "title": "Multi-Task Network Pruning and Embedded Optimization for Real-time\n  Deployment in ADAS", "comments": "Accepted at workshop on Machine Learning for Autonomous Driving\n  (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera-based Deep Learning algorithms are increasingly needed for perception\nin Automated Driving systems. However, constraints from the automotive industry\nchallenge the deployment of CNNs by imposing embedded systems with limited\ncomputational resources. In this paper, we propose an approach to embed a\nmulti-task CNN network under such conditions on a commercial prototype\nplatform, i.e. a low power System on Chip (SoC) processing four surround-view\nfisheye cameras at 10 FPS.\n  The first focus is on designing an efficient and compact multi-task network\narchitecture. Secondly, a pruning method is applied to compress the CNN,\nhelping to reduce the runtime and memory usage by a factor of 2 without\nlowering the performances significantly. Finally, several embedded optimization\ntechniques such as mixed-quantization format usage and efficient data transfers\nbetween different memory areas are proposed to ensure real-time execution and\navoid bandwidth bottlenecks. The approach is evaluated on the hardware\nplatform, considering embedded detection performances, runtime and memory\nbandwidth. Unlike most works from the literature that focus on classification\ntask, we aim here to study the effect of pruning and quantization on a compact\nmulti-task network with object detection, semantic segmentation and soiling\ndetection tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:29:38 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Dellinger", "Flora", ""], ["Boulay", "Thomas", ""], ["Barrenechea", "Diego Mendoza", ""], ["El-Hachimi", "Said", ""], ["Leang", "Isabelle", ""], ["B\u00fcrger", "Fabian", ""]]}, {"id": "2101.07832", "submitter": "Xingyi Li", "authors": "Xingyi Li, Wenxuan Wu, Xiaoli Z. Fern, and Li Fuxin", "title": "The Devils in the Point Clouds: Studying the Robustness of Point Cloud\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been a significant interest in performing convolution\nover irregularly sampled point clouds. Since point clouds are very different\nfrom regular raster images, it is imperative to study the generalization of the\nconvolution networks more closely, especially their robustness under variations\nin scale and rotations of the input data. This paper investigates different\nvariants of PointConv, a convolution network on point clouds, to examine their\nrobustness to input scale and rotation changes. Of the variants we explored,\ntwo are novel and generated significant improvements. The first is replacing\nthe multilayer perceptron based weight function with much simpler third degree\npolynomials, together with a Sobolev norm regularization. Secondly, for 3D\ndatasets, we derive a novel viewpoint-invariant descriptor by utilizing 3D\ngeometric properties as the input to PointConv, in addition to the regular 3D\ncoordinates. We have also explored choices of activation functions,\nneighborhood, and subsampling methods. Experiments are conducted on the 2D\nMNIST & CIFAR-10 datasets as well as the 3D SemanticKITTI & ScanNet datasets.\nResults reveal that on 2D, using third degree polynomials greatly improves\nPointConv's robustness to scale changes and rotations, even surpassing\ntraditional 2D CNNs for the MNIST dataset. On 3D datasets, the novel\nviewpoint-invariant descriptor significantly improves the performance as well\nas robustness of PointConv. We achieve the state-of-the-art semantic\nsegmentation performance on the SemanticKITTI dataset, as well as comparable\nperformance with the current highest framework on the ScanNet dataset among\npoint-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:32:38 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 19:31:58 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Li", "Xingyi", ""], ["Wu", "Wenxuan", ""], ["Fern", "Xiaoli Z.", ""], ["Fuxin", "Li", ""]]}, {"id": "2101.07848", "submitter": "Farzam Hejazi Kookamari", "authors": "Farzam Hejazi, Katarina Vuckovic, Nazanin Rahnavard", "title": "DyLoc: Dynamic Localization for Massive MIMO Using Predictive Recurrent\n  Neural Networks", "comments": "Source Code: https://github.com/FarzamHejaziK/DyLoc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a data-driven localization framework with high precision\nin time-varying complex multipath environments, such as dense urban areas and\nindoors, where GPS and model-based localization techniques come short. We\nconsider the angle-delay profile (ADP), a linear transformation of channel\nstate information (CSI), in massive MIMO systems and show that ADPs preserve\nusers' motion when stacked temporally. We discuss that given a static\nenvironment, future frames of ADP time-series are predictable employing a video\nframe prediction algorithm. We express that a deep convolutional neural network\n(DCNN) can be employed to learn the background static scattering environment.\nTo detect foreground changes in the environment, corresponding to path blockage\nor addition, we introduce an algorithm taking advantage of the trained DCNN.\nFurthermore, we present DyLoc, a data-driven framework to recover distorted\nADPs due to foreground changes and to obtain precise location estimations. We\nevaluate the performance of DyLoc in several dynamic scenarios employing\nDeepMIMO dataset to generate geo-tagged CSI datasets for indoor and outdoor\nenvironments. We show that previous DCNN-based techniques fail to perform with\ndesirable accuracy in dynamic environments, while DyLoc pursues localization\nprecisely. Moreover, simulations show that as the environment gets richer in\nterms of the number of multipath, DyLoc gets more robust to foreground changes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 20:15:34 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 15:47:51 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Hejazi", "Farzam", ""], ["Vuckovic", "Katarina", ""], ["Rahnavard", "Nazanin", ""]]}, {"id": "2101.07855", "submitter": "Furkan Gursoy", "authors": "Mahsun Alt{\\i}n, Furkan G\\\"ursoy, Lina Xu", "title": "Machine-Generated Hierarchical Structure of Human Activities to Reveal\n  How Machines Think", "comments": null, "journal-ref": "IEEE Access, vol. 9, pp. 18307-18317, 2021", "doi": "10.1109/ACCESS.2021.3053084", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning based computer vision models have proved themselves to be\nground-breaking approaches to human activity recognition (HAR). However, most\nexisting works are dedicated to improve the prediction accuracy through either\ncreating new model architectures, increasing model complexity, or refining\nmodel parameters by training on larger datasets. Here, we propose an\nalternative idea, differing from existing work, to increase model accuracy and\nalso to shape model predictions to align with human understandings through\nautomatically creating higher-level summarizing labels for similar groups of\nhuman activities. First, we argue the importance and feasibility of\nconstructing a hierarchical labeling system for human activity recognition.\nThen, we utilize the predictions of a black box HAR model to identify\nsimilarities between different activities. Finally, we tailor hierarchical\nclustering methods to automatically generate hierarchical trees of activities\nand conduct experiments. In this system, the activity labels on the same level\nwill have a designed magnitude of accuracy and reflect a specific amount of\nactivity details. This strategy enables a trade-off between the extent of the\ndetails in the recognized activity and the user privacy by masking some\nsensitive predictions; and also provides possibilities for the use of formerly\nprohibited invasive models in privacy-concerned scenarios. Since the hierarchy\nis generated from the machine's perspective, the predictions at the upper\nlevels provide better accuracy, which is especially useful when there are too\ndetailed labels in the training set that are rather trivial to the final\nprediction goal. Moreover, the analysis of the structure of these trees can\nreveal the biases in the prediction model and guide future data collection\nstrategies.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 20:40:22 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Alt\u0131n", "Mahsun", ""], ["G\u00fcrsoy", "Furkan", ""], ["Xu", "Lina", ""]]}, {"id": "2101.07866", "submitter": "Weihan Zhang", "authors": "Weihan Zhang, Bryan Pogorelsky, Mark Loveland, Trevor Wolf", "title": "Classification of COVID-19 X-ray Images Using a Combination of Deep and\n  Handcrafted Features", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Coronavirus Disease 2019 (COVID-19) demonstrated the need for accurate and\nfast diagnosis methods for emergent viral diseases. Soon after the emergence of\nCOVID-19, medical practitioners used X-ray and computed tomography (CT) images\nof patients' lungs to detect COVID-19. Machine learning methods are capable of\nimproving the identification accuracy of COVID-19 in X-ray and CT images,\ndelivering near real-time results, while alleviating the burden on medical\npractitioners. In this work, we demonstrate the efficacy of a support vector\nmachine (SVM) classifier, trained with a combination of deep convolutional and\nhandcrafted features extracted from X-ray chest scans. We use this combination\nof features to discriminate between healthy, common pneumonia, and COVID-19\npatients. The performance of the combined feature approach is compared with a\nstandard convolutional neural network (CNN) and the SVM trained with\nhandcrafted features. We find that combining the features in our novel\nframework improves the performance of the classification task compared to the\nindependent application of convolutional and handcrafted features.\nSpecifically, we achieve an accuracy of 0.988 in the classification task with\nour combined approach compared to 0.963 and 0.983 accuracy for the handcrafted\nfeatures with SVM and CNN respectively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 21:09:46 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 17:21:01 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Weihan", ""], ["Pogorelsky", "Bryan", ""], ["Loveland", "Mark", ""], ["Wolf", "Trevor", ""]]}, {"id": "2101.07889", "submitter": "Mikaela Angelina Uy", "authors": "Mikaela Angelina Uy, Vladimir G. Kim, Minhyuk Sung, Noam Aigerman,\n  Siddhartha Chaudhuri, Leonidas Guibas", "title": "Joint Learning of 3D Shape Retrieval and Deformation", "comments": "CVPR '21 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique for producing high-quality 3D models that match\na given target object image or scan. Our method is based on retrieving an\nexisting shape from a database of 3D models and then deforming its parts to\nmatch the target shape. Unlike previous approaches that independently focus on\neither shape retrieval or deformation, we propose a joint learning procedure\nthat simultaneously trains the neural deformation module along with the\nembedding space used by the retrieval module. This enables our network to learn\na deformation-aware embedding space, so that retrieved models are more amenable\nto match the target after an appropriate deformation. In fact, we use the\nembedding space to guide the shape pairs used to train the deformation module,\nso that it invests its capacity in learning deformations between meaningful\nshape pairs. Furthermore, our novel part-aware deformation module can work with\ninconsistent and diverse part-structures on the source shapes. We demonstrate\nthe benefits of our joint training not only on our novel framework, but also on\nother state-of-the-art neural deformation modules proposed in recent years.\nLastly, we also show that our jointly-trained method outperforms various\nnon-joint baselines.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 22:49:41 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 12:10:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Uy", "Mikaela Angelina", ""], ["Kim", "Vladimir G.", ""], ["Sung", "Minhyuk", ""], ["Aigerman", "Noam", ""], ["Chaudhuri", "Siddhartha", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2101.07891", "submitter": "Homagni Saha", "authors": "Homagni Saha, Fateme Fotouhif, Qisai Liu, Soumik Sarkar", "title": "A modular vision language navigation and manipulation framework for long\n  horizon compositional tasks in indoor environment", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new framework - MoViLan (Modular Vision and\nLanguage) for execution of visually grounded natural language instructions for\nday to day indoor household tasks. While several data-driven, end-to-end\nlearning frameworks have been proposed for targeted navigation tasks based on\nthe vision and language modalities, performance on recent benchmark data sets\nrevealed the gap in developing comprehensive techniques for long horizon,\ncompositional tasks (involving manipulation and navigation) with diverse object\ncategories, realistic instructions and visual scenarios with non-reversible\nstate changes. We propose a modular approach to deal with the combined\nnavigation and object interaction problem without the need for strictly aligned\nvision and language training data (e.g., in the form of expert demonstrated\ntrajectories). Such an approach is a significant departure from the traditional\nend-to-end techniques in this space and allows for a more tractable training\nprocess with separate vision and language data sets. Specifically, we propose a\nnovel geometry-aware mapping technique for cluttered indoor environments, and a\nlanguage understanding model generalized for household instruction following.\nWe demonstrate a significant increase in success rates for long-horizon,\ncompositional tasks over the baseline on the recently released benchmark data\nset-ALFRED.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 23:05:43 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Saha", "Homagni", ""], ["Fotouhif", "Fateme", ""], ["Liu", "Qisai", ""], ["Sarkar", "Soumik", ""]]}, {"id": "2101.07899", "submitter": "Fupin Yao", "authors": "Fupin Yao", "title": "Cross-domain few-shot learning with unlabelled data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few shot learning aims to solve the data scarcity problem. If there is a\ndomain shift between the test set and the training set, their performance will\ndecrease a lot. This setting is called Cross-domain few-shot learning. However,\nthis is very challenging because the target domain is unseen during training.\nThus we propose a new setting some unlabelled data from the target domain is\nprovided, which can bridge the gap between the source domain and the target\ndomain. A benchmark for this setting is constructed using DomainNet\n\\cite{peng2018oment}. We come up with a self-supervised learning method to\nfully utilize the knowledge in the labeled training set and the unlabelled set.\nExtensive experiments show that our methods outperforms several baseline\nmethods by a large margin. We also carefully design an episodic training\npipeline which yields a significant performance boost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 23:41:57 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Yao", "Fupin", ""]]}, {"id": "2101.07905", "submitter": "Ryota Ikedo", "authors": "Ryota Ikedo, Kazuhiro Hotta", "title": "Feature Sharing Cooperative Network for Semantic Segmentation", "comments": "computer vision and pattern recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks have achieved high ac-curacy in the\nfield of image recognition. By inspired from human learning method, we propose\na semantic segmentation method using cooperative learning which shares the\ninformation resembling a group learning. We use two same networks and paths for\nsending feature maps between two networks. Two networks are trained\nsimultaneously. By sharing feature maps, one of two networks can obtain the\ninformation that cannot be obtained by a single network. In addition, in order\nto enhance the degree of cooperation, we propose two kinds of methods that\nconnect only the same layer and multiple layers. We evaluated our proposed idea\non two kinds of networks. One is Dual Attention Network (DANet) and the other\none is DeepLabv3+. The proposed method achieved better segmentation accuracy\nthan the conventional single network and ensemble of networks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 00:22:00 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ikedo", "Ryota", ""], ["Hotta", "Kazuhiro", ""]]}, {"id": "2101.07907", "submitter": "Sergio Casas", "authors": "Sergio Casas, Wenjie Luo, Raquel Urtasun", "title": "IntentNet: Learning to Predict Intention from Raw Sensor Data", "comments": "CoRL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to plan a safe maneuver, self-driving vehicles need to understand\nthe intent of other traffic participants. We define intent as a combination of\ndiscrete high-level behaviors as well as continuous trajectories describing\nfuture motion. In this paper, we develop a one-stage detector and forecaster\nthat exploits both 3D point clouds produced by a LiDAR sensor as well as\ndynamic maps of the environment. Our multi-task model achieves better accuracy\nthan the respective separate modules while saving computation, which is\ncritical to reducing reaction time in self-driving applications.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 00:31:52 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Casas", "Sergio", ""], ["Luo", "Wenjie", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.07922", "submitter": "Valeriia Cherepanova", "authors": "Valeriia Cherepanova, Micah Goldblum, Harrison Foley, Shiyuan Duan,\n  John Dickerson, Gavin Taylor, Tom Goldstein", "title": "LowKey: Leveraging Adversarial Attacks to Protect Social Media Users\n  from Facial Recognition", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial recognition systems are increasingly deployed by private corporations,\ngovernment agencies, and contractors for consumer services and mass\nsurveillance programs alike. These systems are typically built by scraping\nsocial media profiles for user images. Adversarial perturbations have been\nproposed for bypassing facial recognition systems. However, existing methods\nfail on full-scale systems and commercial APIs. We develop our own adversarial\nfilter that accounts for the entire image processing pipeline and is\ndemonstrably effective against industrial-grade pipelines that include face\ndetection and large scale databases. Additionally, we release an easy-to-use\nwebtool that significantly degrades the accuracy of Amazon Rekognition and the\nMicrosoft Azure Face Recognition API, reducing the accuracy of each to below\n1%.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 01:40:06 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 04:23:22 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Cherepanova", "Valeriia", ""], ["Goldblum", "Micah", ""], ["Foley", "Harrison", ""], ["Duan", "Shiyuan", ""], ["Dickerson", "John", ""], ["Taylor", "Gavin", ""], ["Goldstein", "Tom", ""]]}, {"id": "2101.07927", "submitter": "Yuanhao Gong", "authors": "Yuanhao Gong, Wenming Tang, Lebin Zhou, Lantao Yu, Guoping Qiu", "title": "A Discrete Scheme for Computing Image's Weighted Gaussian Curvature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted Gaussian Curvature is an important measurement for images. However,\nits conventional computation scheme has low performance, low accuracy and\nrequires that the input image must be second order differentiable. To tackle\nthese three issues, we propose a novel discrete computation scheme for the\nweighted Gaussian curvature. Our scheme does not require the second order\ndifferentiability. Moreover, our scheme is more accurate, has smaller support\nregion and computationally more efficient than the conventional schemes.\nTherefore, our scheme holds promise for a large range of applications where the\nweighted Gaussian curvature is needed, for example, image smoothing, cartoon\ntexture decomposition, optical flow estimation, etc.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 02:15:51 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Gong", "Yuanhao", ""], ["Tang", "Wenming", ""], ["Zhou", "Lebin", ""], ["Yu", "Lantao", ""], ["Qiu", "Guoping", ""]]}, {"id": "2101.07929", "submitter": "Ruibing Jin", "authors": "Ruibing Jin, Guosheng Lin, and Changyun Wen", "title": "Online Active Proposal Set Generation for Weakly Supervised Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce the manpower consumption on box-level annotations, many weakly\nsupervised object detection methods which only require image-level annotations,\nhave been proposed recently. The training process in these methods is\nformulated into two steps. They firstly train a neural network under weak\nsupervision to generate pseudo ground truths (PGTs). Then, these PGTs are used\nto train another network under full supervision. Compared with fully supervised\nmethods, the training process in weakly supervised methods becomes more complex\nand time-consuming. Furthermore, overwhelming negative proposals are involved\nat the first step. This is neglected by most methods, which makes the training\nnetwork biased towards to negative proposals and thus degrades the quality of\nthe PGTs, limiting the training network performance at the second step. Online\nproposal sampling is an intuitive solution to these issues. However, lacking of\nadequate labeling, a simple online proposal sampling may make the training\nnetwork stuck into local minima. To solve this problem, we propose an Online\nActive Proposal Set Generation (OPG) algorithm. Our OPG algorithm consists of\ntwo parts: Dynamic Proposal Constraint (DPC) and Proposal Partition (PP). DPC\nis proposed to dynamically determine different proposal sampling strategy\naccording to the current training state. PP is used to score each proposal,\npart proposals into different sets and generate an active proposal set for the\nnetwork optimization. Through experiments, our proposed OPG shows consistent\nand significant improvement on both datasets PASCAL VOC 2007 and 2012, yielding\ncomparable performance to the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 02:20:48 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Jin", "Ruibing", ""], ["Lin", "Guosheng", ""], ["Wen", "Changyun", ""]]}, {"id": "2101.07933", "submitter": "Yuanhao Gong", "authors": "Yuanhao Gong, Wenming Tang, Lebin Zhou, Lantao Yu, Guoping Qiu", "title": "Quarter Laplacian Filter for Edge Aware Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a quarter Laplacian filter that can preserve corners and\nedges during image smoothing. Its support region is $2\\times2$, which is\nsmaller than the $3\\times3$ support region of Laplacian filter. Thus, it is\nmore local. Moreover, this filter can be implemented via the classical box\nfilter, leading to high performance for real time applications. Finally, we\nshow its edge preserving property in several image processing tasks, including\nimage smoothing, texture enhancement, and low-light image enhancement. The\nproposed filter can be adopted in a wide range of image processing\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 02:29:54 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Gong", "Yuanhao", ""], ["Tang", "Wenming", ""], ["Zhou", "Lebin", ""], ["Yu", "Lantao", ""], ["Qiu", "Guoping", ""]]}, {"id": "2101.07945", "submitter": "Tao Wei", "authors": "Tao Wei, Angelica I Aviles-Rivero, Shuo Wang, Yuan Huang, Fiona J\n  Gilbert, Carola-Bibiane Sch\\\"onlieb, Chang Wen Chen", "title": "Beyond Fine-tuning: Classifying High Resolution Mammograms using\n  Function-Preserving Transformations", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of classifying mammograms is very challenging because the lesion is\nusually small in the high resolution image. The current state-of-the-art\napproaches for medical image classification rely on using the de-facto method\nfor ConvNets - fine-tuning. However, there are fundamental differences between\nnatural images and medical images, which based on existing evidence from the\nliterature, limits the overall performance gain when designed with algorithmic\napproaches. In this paper, we propose to go beyond fine-tuning by introducing a\nnovel framework called MorphHR, in which we highlight a new transfer learning\nscheme. The idea behind the proposed framework is to integrate\nfunction-preserving transformations, for any continuous non-linear activation\nneurons, to internally regularise the network for improving mammograms\nclassification. The proposed solution offers two major advantages over the\nexisting techniques. Firstly and unlike fine-tuning, the proposed approach\nallows for modifying not only the last few layers but also several of the first\nones on a deep ConvNet. By doing this, we can design the network front to be\nsuitable for learning domain specific features. Secondly, the proposed scheme\nis scalable to hardware. Therefore, one can fit high resolution images on\nstandard GPU memory. We show that by using high resolution images, one prevents\nlosing relevant information. We demonstrate, through numerical and visual\nexperiments, that the proposed approach yields to a significant improvement in\nthe classification performance over state-of-the-art techniques, and is indeed\non a par with radiology experts. Moreover and for generalisation purposes, we\nshow the effectiveness of the proposed learning scheme on another large\ndataset, the ChestX-ray14, surpassing current state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 03:04:07 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Wei", "Tao", ""], ["Aviles-Rivero", "Angelica I", ""], ["Wang", "Shuo", ""], ["Huang", "Yuan", ""], ["Gilbert", "Fiona J", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Chen", "Chang Wen", ""]]}, {"id": "2101.07959", "submitter": "Long Chen", "authors": "Long Chen, Junyu Dong and Huiyu Zhou", "title": "Class balanced underwater object detection dataset generated by\n  class-wise style augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater object detection technique is of great significance for various\napplications in underwater the scenes. However, class imbalance issue is still\nan unsolved bottleneck for current underwater object detection algorithms. It\nleads to large precision discrepancies among different classes that the\ndominant classes with more training data achieve higher detection precisions\nwhile the minority classes with fewer training data achieves much lower\ndetection precisions. In this paper, we propose a novel class-wise style\naugmentation (CWSA) algorithm to generate a class-balanced underwater dataset\nBalance18 from the public contest underwater dataset URPC2018. CWSA is a new\nkind of data augmentation technique which augments the training data for the\nminority classes by generating various colors, textures and contrasts for the\nminority classes. Compare with previous data augmentation algorithms such\nflipping, cropping and rotations, CWSA is able to generate a class balanced\nunderwater dataset with diverse color distortions and haze-effects.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 04:37:27 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Chen", "Long", ""], ["Dong", "Junyu", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2101.07974", "submitter": "Rohit Gupta", "authors": "Ishan Dave, Rohit Gupta, Mamshad Nayeem Rizve and Mubarak Shah", "title": "TCLR: Temporal Contrastive Learning for Video Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has nearly closed the gap between supervised and\nself-supervised learning of image representations, and has also been explored\nfor videos. However, prior work on contrastive learning for video data has not\nexplored the effect of explicitly encouraging the features to be distinct\nacross the temporal dimension. We develop a new temporal contrastive learning\nframework consisting of two novel losses to improve upon existing contrastive\nself-supervised video representation learning methods. The local-local temporal\ncontrastive loss adds the task of discriminating between non-overlapping clips\nfrom the same video, whereas the global-local temporal contrastive aims to\ndiscriminate between timesteps of the feature map of an input clip in order to\nincrease the temporal diversity of the learned features. Our proposed temporal\ncontrastive learning framework achieves significant improvement over the\nstate-of-the-art results in various downstream video understanding tasks such\nas action recognition, limited-label action classification, and\nnearest-neighbor video retrieval on multiple video datasets and backbones. We\nalso demonstrate significant improvement in fine-grained action classification\nfor visually similar classes. With the commonly used 3D ResNet-18 architecture,\nwe achieve 82.4% (+5.1% increase over the previous best) top-1 accuracy on\nUCF101 and 52.9% (+5.4% increase) on HMDB51 action classification, and 56.2%\n(+11.7% increase) Top-1 Recall on UCF101 nearest neighbor video retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:38:16 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 13:28:27 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 15:39:49 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Dave", "Ishan", ""], ["Gupta", "Rohit", ""], ["Rizve", "Mamshad Nayeem", ""], ["Shah", "Mubarak", ""]]}, {"id": "2101.07978", "submitter": "Zhi Chen", "authors": "Zhi Chen, Ruihong Qiu, Sen Wang, Zi Huang, Jingjing Li, Zheng Zhang", "title": "Semantic Disentangling Generalized Zero-Shot Learning", "comments": "This is not the final version of this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Zero-Shot Learning (GZSL) aims to recognize images from both seen\nand unseen categories. Most GZSL methods typically learn to synthesize CNN\nvisual features for the unseen classes by leveraging entire semantic\ninformation, e.g., tags and attributes, and the visual features of the seen\nclasses. Within the visual features, we define two types of features that\nsemantic-consistent and semantic-unrelated to represent the characteristics of\nimages annotated in attributes and less informative features of images\nrespectively. Ideally, the semantic-unrelated information is impossible to\ntransfer by semantic-visual relationship from seen classes to unseen classes,\nas the corresponding characteristics are not annotated in the semantic\ninformation. Thus, the foundation of the visual feature synthesis is not always\nsolid as the features of the seen classes may involve semantic-unrelated\ninformation that could interfere with the alignment between semantic and visual\nmodalities. To address this issue, in this paper, we propose a novel feature\ndisentangling approach based on an encoder-decoder architecture to factorize\nvisual features of images into these two latent feature spaces to extract\ncorresponding representations. Furthermore, a relation module is incorporated\ninto this architecture to learn semantic-visual relationship, whilst a total\ncorrelation penalty is applied to encourage the disentanglement of two latent\nrepresentations. The proposed model aims to distill quality semantic-consistent\nrepresentations that capture intrinsic features of seen images, which are\nfurther taken as the generation target for unseen classes. Extensive\nexperiments conducted on seven GZSL benchmark datasets have verified the\nstate-of-the-art performance of the proposal.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:46:21 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 02:28:44 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 01:56:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Chen", "Zhi", ""], ["Qiu", "Ruihong", ""], ["Wang", "Sen", ""], ["Huang", "Zi", ""], ["Li", "Jingjing", ""], ["Zhang", "Zheng", ""]]}, {"id": "2101.07983", "submitter": "Takamasa Ando", "authors": "Takamasa Ando, Kazuhiro Hotta", "title": "Cell image segmentation by Feature Random Enhancement Module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to extract good features using an encoder to realize semantic\nsegmentation with high accuracy. Although loss function is optimized in\ntraining deep neural network, far layers from the layers for computing loss\nfunction are difficult to train. Skip connection is effective for this problem\nbut there are still far layers from the loss function. In this paper, we\npropose the Feature Random Enhancement Module which enhances the features\nrandomly in only training. By emphasizing the features at far layers from loss\nfunction, we can train those layers well and the accuracy was improved. In\nexperiments, we evaluated the proposed module on two kinds of cell image\ndatasets, and our module improved the segmentation accuracy without increasing\ncomputational cost in test phase.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:16:46 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ando", "Takamasa", ""], ["Hotta", "Kazuhiro", ""]]}, {"id": "2101.07985", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Shaojie Li, Yan Wang, Yongjian Wu, Feiyue\n  Huang, Qixiang Ye", "title": "Network Pruning using Adaptive Exemplar Filters", "comments": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (IEEE TNNLS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular network pruning algorithms reduce redundant information by optimizing\nhand-crafted models, and may cause suboptimal performance and long time in\nselecting filters. We innovatively introduce adaptive exemplar filters to\nsimplify the algorithm design, resulting in an automatic and efficient pruning\napproach called EPruner. Inspired by the face recognition community, we use a\nmessage passing algorithm Affinity Propagation on the weight matrices to obtain\nan adaptive number of exemplars, which then act as the preserved filters.\nEPruner breaks the dependency on the training data in determining the\n\"important\" filters and allows the CPU implementation in seconds, an order of\nmagnitude faster than GPU based SOTAs. Moreover, we show that the weights of\nexemplars provide a better initialization for the fine-tuning. On VGGNet-16,\nEPruner achieves a 76.34%-FLOPs reduction by removing 88.80% parameters, with\n0.06% accuracy improvement on CIFAR-10. In ResNet-152, EPruner achieves a\n65.12%-FLOPs reduction by removing 64.18% parameters, with only 0.71% top-5\naccuracy loss on ILSVRC-2012. Our code can be available at\nhttps://github.com/lmbxmu/EPruner.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:18:38 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 08:44:26 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 07:04:01 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 16:08:22 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Li", "Shaojie", ""], ["Wang", "Yan", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""], ["Ye", "Qixiang", ""]]}, {"id": "2101.07988", "submitter": "Olga Moskvyak", "authors": "Olga Moskvyak, Frederic Maire, Feras Dayoub, Mahsa Baktashmotlagh", "title": "Semi-supervised Keypoint Localization", "comments": "accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge about the locations of keypoints of an object in an image can\nassist in fine-grained classification and identification tasks, particularly\nfor the case of objects that exhibit large variations in poses that greatly\ninfluence their visual appearance, such as wild animals. However, supervised\ntraining of a keypoint detection network requires annotating a large image\ndataset for each animal species, which is a labor-intensive task. To reduce the\nneed for labeled data, we propose to learn simultaneously keypoint heatmaps and\npose invariant keypoint representations in a semi-supervised manner using a\nsmall set of labeled images along with a larger set of unlabeled images.\nKeypoint representations are learnt with a semantic keypoint consistency\nconstraint that forces the keypoint detection network to learn similar features\nfor the same keypoint across the dataset. Pose invariance is achieved by making\nkeypoint representations for the image and its augmented copies closer together\nin feature space. Our semi-supervised approach significantly outperforms\nprevious methods on several benchmarks for human and animal body landmark\nlocalization.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:23:08 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Moskvyak", "Olga", ""], ["Maire", "Frederic", ""], ["Dayoub", "Feras", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "2101.07995", "submitter": "Yaoxin Zhuo", "authors": "Yaoxin Zhuo, Baoxin Li", "title": "FedNS: Improving Federated Learning for collaborative image\n  classification on mobile clients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a paradigm that aims to support loosely connected\nclients in learning a global model collaboratively with the help of a\ncentralized server. The most popular FL algorithm is Federated Averaging\n(FedAvg), which is based on taking weighted average of the client models, with\nthe weights determined largely based on dataset sizes at the clients. In this\npaper, we propose a new approach, termed Federated Node Selection (FedNS), for\nthe server's global model aggregation in the FL setting. FedNS filters and\nre-weights the clients' models at the node/kernel level, hence leading to a\npotentially better global model by fusing the best components of the clients.\nUsing collaborative image classification as an example, we show with\nexperiments from multiple datasets and networks that FedNS can consistently\nachieve improved performance over FedAvg.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:45:46 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zhuo", "Yaoxin", ""], ["Li", "Baoxin", ""]]}, {"id": "2101.07996", "submitter": "Xin Liu", "authors": "Xin Liu, Yuang Li, Josh Fromm, Yuntao Wang, Ziheng Jiang, Alex\n  Mariakakis, Shwetak Patel", "title": "SplitSR: An End-to-End Approach to Super-Resolution on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) is a coveted image processing technique for mobile apps\nranging from the basic camera apps to mobile health. Existing SR algorithms\nrely on deep learning models with significant memory requirements, so they have\nyet to be deployed on mobile devices and instead operate in the cloud to\nachieve feasible inference time. This shortcoming prevents existing SR methods\nfrom being used in applications that require near real-time latency. In this\nwork, we demonstrate state-of-the-art latency and accuracy for on-device\nsuper-resolution using a novel hybrid architecture called SplitSR and a novel\nlightweight residual block called SplitSRBlock. The SplitSRBlock supports\nchannel-splitting, allowing the residual blocks to retain spatial information\nwhile reducing the computation in the channel dimension. SplitSR has a hybrid\ndesign consisting of standard convolutional blocks and lightweight residual\nblocks, allowing people to tune SplitSR for their computational budget. We\nevaluate our system on a low-end ARM CPU, demonstrating both higher accuracy\nand up to 5 times faster inference than previous approaches. We then deploy our\nmodel onto a smartphone in an app called ZoomSR to demonstrate the first-ever\ninstance of on-device, deep learning-based SR. We conducted a user study with\n15 participants to have them assess the perceived quality of images that were\npost-processed by SplitSR. Relative to bilinear interpolation -- the existing\nstandard for on-device SR -- participants showed a statistically significant\npreference when looking at both images (Z=-9.270, p<0.01) and text (Z=-6.486,\np<0.01).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:47:41 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Liu", "Xin", ""], ["Li", "Yuang", ""], ["Fromm", "Josh", ""], ["Wang", "Yuntao", ""], ["Jiang", "Ziheng", ""], ["Mariakakis", "Alex", ""], ["Patel", "Shwetak", ""]]}, {"id": "2101.08000", "submitter": "Zhangzi Zhu", "authors": "Zhangzi Zhu, Tianlei Wang, and Hong Qu", "title": "Macroscopic Control of Text Generation for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that image captioning models have been able to generate\nimpressive descriptions for a given image, challenges remain: (1) the\ncontrollability and diversity of existing models are still far from\nsatisfactory; (2) models sometimes may produce extremely poor-quality captions.\nIn this paper, two novel methods are introduced to solve the problems\nrespectively. Specifically, for the former problem, we introduce a control\nsignal which can control the macroscopic sentence attributes, such as sentence\nquality, sentence length, sentence tense and number of nouns etc. With such a\ncontrol signal, the controllability and diversity of existing captioning models\nare enhanced. For the latter problem, we innovatively propose a strategy that\nan image-text matching model is trained to measure the quality of sentences\ngenerated in both forward and backward directions and finally choose the better\none. As a result, this strategy can effectively reduce the proportion of\npoorquality sentences. Our proposed methods can be easily applie on most image\ncaptioning models to improve their overall performance. Based on the Up-Down\nmodel, the experimental results show that our methods achieve BLEU-\n4/CIDEr/SPICE scores of 37.5/120.3/21.5 on MSCOCO Karpathy test split with\ncross-entropy training, which surpass the results of other state-of-the-art\nmethods trained by cross-entropy loss.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 07:20:07 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zhu", "Zhangzi", ""], ["Wang", "Tianlei", ""], ["Qu", "Hong", ""]]}, {"id": "2101.08018", "submitter": "Zheng Fang", "authors": "Xingyin Fu, Zheng Fang, Xizhen Xiao, Yijia He, Xiao Liu", "title": "Improved Signed Distance Function for 2D Real-time SLAM and Accurate\n  Localization", "comments": "7 pages, 9 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate mapping and localization are very important for many industrial\nrobotics applications. In this paper, we propose an improved Signed Distance\nFunction (SDF) for both 2D SLAM and pure localization to improve the accuracy\nof mapping and localization. To achieve this goal, firstly we improved the\nback-end mapping to build a more accurate SDF map by extending the update range\nand building free space, etc. Secondly, to get more accurate pose estimation\nfor the front-end, we proposed a new iterative registration method to align the\ncurrent scan to the SDF submap by removing random outliers of laser scanners.\nThirdly, we merged all the SDF submaps to produce an integrated SDF map for\nhighly accurate pure localization. Experimental results show that based on the\nmerged SDF map, a localization accuracy of a few millimeters (5mm) can be\nachieved globally within the map. We believe that this method is important for\nmobile robots working in scenarios where high localization accuracy matters.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 08:28:19 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Fu", "Xingyin", ""], ["Fang", "Zheng", ""], ["Xiao", "Xizhen", ""], ["He", "Yijia", ""], ["Liu", "Xiao", ""]]}, {"id": "2101.08024", "submitter": "Zhonghao Zhang", "authors": "Zhonghao Zhang and Yipeng Liu and Xingyu Cao and Fei Wen and Ce Zhu", "title": "Scalable Deep Compressive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been used to image compressive sensing (CS) for enhanced\nreconstruction performance. However, most existing deep learning methods train\ndifferent models for different subsampling ratios, which brings additional\nhardware burden. In this paper, we develop a general framework named scalable\ndeep compressive sensing (SDCS) for the scalable sampling and reconstruction\n(SSR) of all existing end-to-end-trained models. In the proposed way, images\nare measured and initialized linearly. Two sampling masks are introduced to\nflexibly control the subsampling ratios used in sampling and reconstruction,\nrespectively. To make the reconstruction model adapt to any subsampling ratio,\na training strategy dubbed scalable training is developed. In scalable\ntraining, the model is trained with the sampling matrix and the initialization\nmatrix at various subsampling ratios by integrating different sampling matrix\nmasks. Experimental results show that models with SDCS can achieve SSR without\nchanging their structure while maintaining good performance, and SDCS\noutperforms other SSR methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 08:42:50 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 02:53:03 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zhang", "Zhonghao", ""], ["Liu", "Yipeng", ""], ["Cao", "Xingyu", ""], ["Wen", "Fei", ""], ["Zhu", "Ce", ""]]}, {"id": "2101.08039", "submitter": "Chang Liu", "authors": "Zhuqing Jiang, Chang Liu, Ya'nan Wang, Kai Li, Aidong Men, Haiying\n  Wang, Haiyong Luo", "title": "Bridge the Vision Gap from Field to Command: A Deep Learning Network\n  Enhancing Illumination and Details", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the goal of tuning up the brightness, low-light image enhancement enjoys\nnumerous applications, such as surveillance, remote sensing and computational\nphotography. Images captured under low-light conditions often suffer from poor\nvisibility and blur. Solely brightening the dark regions will inevitably\namplify the blur, thus may lead to detail loss. In this paper, we propose a\nsimple yet effective two-stream framework named NEID to tune up the brightness\nand enhance the details simultaneously without introducing many computational\ncosts. Precisely, the proposed method consists of three parts: Light\nEnhancement (LE), Detail Refinement (DR) and Feature Fusing (FF) module, which\ncan aggregate composite features oriented to multiple tasks based on channel\nattention mechanism. Extensive experiments conducted on several benchmark\ndatasets demonstrate the efficacy of our method and its superiority over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 09:39:57 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Jiang", "Zhuqing", ""], ["Liu", "Chang", ""], ["Wang", "Ya'nan", ""], ["Li", "Kai", ""], ["Men", "Aidong", ""], ["Wang", "Haiying", ""], ["Luo", "Haiyong", ""]]}, {"id": "2101.08040", "submitter": "Jiasheng Tang", "authors": "Fei Du, Bo Xu, Jiasheng Tang, Yuqi Zhang, Fan Wang, and Hao Li", "title": "1st Place Solution to ECCV-TAO-2020: Detect and Represent Any Object for\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the classical tracking-by-detection paradigm to this\ntracking-any-object task. Solid detection results are first extracted from TAO\ndataset. Some state-of-the-art techniques like \\textbf{BA}lanced-\\textbf{G}roup\n\\textbf{S}oftmax (\\textbf{BAGS}\\cite{li2020overcoming}) and\nDetectoRS\\cite{qiao2020detectors} are integrated during detection. Then we\nlearned appearance features to represent any object by training feature\nlearning networks. We ensemble several models for improving detection and\nfeature representation. Simple linking strategies with most similar appearance\nfeatures and tracklet-level post association module are finally applied to\ngenerate final tracking results. Our method is submitted as \\textbf{AOA} on the\nchallenge website. Code is available at\nhttps://github.com/feiaxyt/Winner_ECCV20_TAO.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 09:42:32 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 08:38:31 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Du", "Fei", ""], ["Xu", "Bo", ""], ["Tang", "Jiasheng", ""], ["Zhang", "Yuqi", ""], ["Wang", "Fan", ""], ["Li", "Hao", ""]]}, {"id": "2101.08052", "submitter": "Kimberley Timmins", "authors": "Kimberley M. Timmins, Irene C. van der Schaaf, Ynte M. Ruigrok,\n  Birgitta K. Velthuis, Hugo J. Kuijf", "title": "Variational Autoencoders with a Structural Similarity Loss in Time of\n  Flight MRAs", "comments": "SPIE Medical Imaging 2021", "journal-ref": null, "doi": "10.1117/12.2580705", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time-of-Flight Magnetic Resonance Angiographs (TOF-MRAs) enable visualization\nand analysis of cerebral arteries. This analysis may indicate normal variation\nof the configuration of the cerebrovascular system or vessel abnormalities,\nsuch as aneurysms. A model would be useful to represent normal cerebrovascular\nstructure and variabilities in a healthy population and to differentiate from\nabnormalities. Current anomaly detection using autoencoding convolutional\nneural networks usually use a voxelwise mean-error for optimization. We propose\noptimizing a variational-autoencoder (VAE) with structural similarity loss\n(SSIM) for TOF-MRA reconstruction. A patch-trained 2D fully-convolutional VAE\nwas optimized for TOF-MRA reconstruction by comparing vessel segmentations of\noriginal and reconstructed MRAs. The method was trained and tested on two\ndatasets: the IXI dataset, and a subset from the ADAM challenge. Both trained\nnetworks were tested on a dataset including subjects with aneurysms. We\ncompared VAE optimization with L2-loss and SSIM-loss. Performance was evaluated\nbetween original and reconstructed MRAs using mean square error, mean-SSIM,\npeak-signal-to-noise-ratio and dice similarity index (DSI) of segmented\nvessels. The L2-optimized VAE outperforms SSIM, with improved reconstruction\nmetrics and DSIs for both datasets. Optimization using SSIM performed best for\nvisual image quality, but with discrepancy in quantitative reconstruction and\nvascular segmentation. The larger, more diverse IXI dataset had overall better\nperformance. Reconstruction metrics, including SSIM, were lower for MRAs\nincluding aneurysms. A SSIM-optimized VAE improved the visual perceptive image\nquality of TOF-MRA reconstructions. A L2-optimized VAE performed best for\nTOF-MRA reconstruction, where the vascular segmentation is important. SSIM is a\npotential metric for anomaly detection of MRAs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:13:57 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Timmins", "Kimberley M.", ""], ["van der Schaaf", "Irene C.", ""], ["Ruigrok", "Ynte M.", ""], ["Velthuis", "Birgitta K.", ""], ["Kuijf", "Hugo J.", ""]]}, {"id": "2101.08063", "submitter": "Benjamin Perret", "authors": "Benjamin Perret (LIGM), Jean Cousty (LIGM)", "title": "Component Tree Loss Function: Definition and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a method to design loss functions based on\ncomponent trees which can be optimized by gradient descent algorithms and which\nare therefore usable in conjunction with recent machine learning approaches\nsuch as neural networks. We show how the altitudes associated to the nodes of\nsuch hierarchical image representations can be differentiated with respect to\nthe image pixel values. This feature is used to design a generic loss function\nthat can select or discard image maxima based on various attributes such as\nextinction values. The possibilities of the proposed method are demonstrated on\nsimulated and real image filtering.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:55:37 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Perret", "Benjamin", "", "LIGM"], ["Cousty", "Jean", "", "LIGM"]]}, {"id": "2101.08085", "submitter": "Xiatian Zhu", "authors": "Xiatian Zhu and Antoine Toisoul and Juan-Manuel Perez-Rua and Li Zhang\n  and Brais Martinez and Tao Xiang", "title": "Few-shot Action Recognition with Prototype-centered Attentive Learning", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Few-shot action recognition aims to recognize action classes with few\ntraining samples. Most existing methods adopt a meta-learning approach with\nepisodic training. In each episode, the few samples in a meta-training task are\nsplit into support and query sets. The former is used to build a classifier,\nwhich is then evaluated on the latter using a query-centered loss for model\nupdating. There are however two major limitations: lack of data efficiency due\nto the query-centered only loss design and inability to deal with the support\nset outlying samples and inter-class distribution overlapping problems. In this\npaper, we overcome both limitations by proposing a new Prototype-centered\nAttentive Learning (PAL) model composed of two novel components. First, a\nprototype-centered contrastive learning loss is introduced to complement the\nconventional query-centered learning objective, in order to make full use of\nthe limited training samples in each episode. Second, PAL further integrates a\nhybrid attentive learning mechanism that can minimize the negative impacts of\noutliers and promote class separation. Extensive experiments on four standard\nfew-shot action benchmarks show that our method clearly outperforms previous\nstate-of-the-art methods, with the improvement particularly significant (10+\\%)\non the most challenging fine-grained action recognition benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 11:48:12 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 23:39:54 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 16:22:37 GMT"}, {"version": "v4", "created": "Sun, 28 Mar 2021 17:15:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhu", "Xiatian", ""], ["Toisoul", "Antoine", ""], ["Perez-Rua", "Juan-Manuel", ""], ["Zhang", "Li", ""], ["Martinez", "Brais", ""], ["Xiang", "Tao", ""]]}, {"id": "2101.08122", "submitter": "Devis Tuia", "authors": "Marrit Leenstra, Diego Marcos, Francesca Bovolo, Devis Tuia", "title": "Self-supervised pre-training enhances change detection in Sentinel-2\n  imagery", "comments": "Presented at the Pattern Recognition and Remote Sensing (PRRS)\n  workshop in ICPR, 2021", "journal-ref": "Part of the Lecture Notes in Computer Science book series (LNCS,\n  volume 12667), 2021", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While annotated images for change detection using satellite imagery are\nscarce and costly to obtain, there is a wealth of unlabeled images being\ngenerated every day. In order to leverage these data to learn an image\nrepresentation more adequate for change detection, we explore methods that\nexploit the temporal consistency of Sentinel-2 times series to obtain a usable\nself-supervised learning signal. For this, we build and make publicly available\n(https://zenodo.org/record/4280482) the Sentinel-2 Multitemporal Cities Pairs\n(S2MTCP) dataset, containing multitemporal image pairs from 1520 urban areas\nworldwide. We test the results of multiple self-supervised learning methods for\npre-training models for change detection and apply it on a public change\ndetection dataset made of Sentinel-2 image pairs (OSCD).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 13:47:25 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 20:43:10 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Leenstra", "Marrit", ""], ["Marcos", "Diego", ""], ["Bovolo", "Francesca", ""], ["Tuia", "Devis", ""]]}, {"id": "2101.08154", "submitter": "Xiaopei Zhu", "authors": "Xiaopei Zhu, Xiao Li, Jianmin Li, Zheyao Wang, Xiaolin Hu", "title": "Fooling thermal infrared pedestrian detectors in real world using small\n  bulbs", "comments": "accepted by AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal infrared detection systems play an important role in many areas such\nas night security, autonomous driving, and body temperature detection. They\nhave the unique advantages of passive imaging, temperature sensitivity and\npenetration. But the security of these systems themselves has not been fully\nexplored, which poses risks in applying these systems. We propose a physical\nattack method with small bulbs on a board against the state of-the-art\npedestrian detectors. Our goal is to make infrared pedestrian detectors unable\nto detect real-world pedestrians. Towards this goal, we first showed that it is\npossible to use two kinds of patches to attack the infrared pedestrian detector\nbased on YOLOv3. The average precision (AP) dropped by 64.12% in the digital\nworld, while a blank board with the same size caused the AP to drop by 29.69%\nonly. After that, we designed and manufactured a physical board and\nsuccessfully attacked YOLOv3 in the real world. In recorded videos, the\nphysical board caused AP of the target detector to drop by 34.48%, while a\nblank board with the same size caused the AP to drop by 14.91% only. With the\nensemble attack techniques, the designed physical board had good\ntransferability to unseen detectors.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:26:09 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zhu", "Xiaopei", ""], ["Li", "Xiao", ""], ["Li", "Jianmin", ""], ["Wang", "Zheyao", ""], ["Hu", "Xiaolin", ""]]}, {"id": "2101.08158", "submitter": "Yifan Zhang", "authors": "Yi-Fan Zhang, Weiqiang Ren, Zhang Zhang, Zhen Jia, Liang Wang, Tieniu\n  Tan", "title": "Focal and Efficient IOU Loss for Accurate Bounding Box Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In object detection, bounding box regression (BBR) is a crucial step that\ndetermines the object localization performance. However, we find that most\nprevious loss functions for BBR have two main drawbacks: (i) Both $\\ell_n$-norm\nand IOU-based loss functions are inefficient to depict the objective of BBR,\nwhich leads to slow convergence and inaccurate regression results. (ii) Most of\nthe loss functions ignore the imbalance problem in BBR that the large number of\nanchor boxes which have small overlaps with the target boxes contribute most to\nthe optimization of BBR. To mitigate the adverse effects caused thereby, we\nperform thorough studies to exploit the potential of BBR losses in this paper.\nFirstly, an Efficient Intersection over Union (EIOU) loss is proposed, which\nexplicitly measures the discrepancies of three geometric factors in BBR, i.e.,\nthe overlap area, the central point and the side length. After that, we state\nthe Effective Example Mining (EEM) problem and propose a regression version of\nfocal loss to make the regression process focus on high-quality anchor boxes.\nFinally, the above two parts are combined to obtain a new loss function, namely\nFocal-EIOU loss. Extensive experiments on both synthetic and real datasets are\nperformed. Notable superiorities on both the convergence speed and the\nlocalization accuracy can be achieved over other BBR losses.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:33:58 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zhang", "Yi-Fan", ""], ["Ren", "Weiqiang", ""], ["Zhang", "Zhang", ""], ["Jia", "Zhen", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "2101.08165", "submitter": "Wentao Xie", "authors": "Wentao Xie, Guanghui Ren, Si Liu", "title": "Video Relation Detection with Trajectory-aware Multi-modal Features", "comments": null, "journal-ref": null, "doi": "10.1145/3394171.3416284", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video relation detection problem refers to the detection of the relationship\nbetween different objects in videos, such as spatial relationship and action\nrelationship. In this paper, we present video relation detection with\ntrajectory-aware multi-modal features to solve this task.\n  Considering the complexity of doing visual relation detection in videos, we\ndecompose this task into three sub-tasks: object detection, trajectory proposal\nand relation prediction. We use the state-of-the-art object detection method to\nensure the accuracy of object trajectory detection and multi-modal feature\nrepresentation to help the prediction of relation between objects. Our method\nwon the first place on the video relation detection task of Video Relation\nUnderstanding Grand Challenge in ACM Multimedia 2020 with 11.74\\% mAP, which\nsurpasses other methods by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:49:02 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Xie", "Wentao", ""], ["Ren", "Guanghui", ""], ["Liu", "Si", ""]]}, {"id": "2101.08211", "submitter": "Xinwei Yu", "authors": "Xinwei Yu, Matthew S. Creamer, Francesco Randi, Anuj K. Sharma, Scott\n  W. Linderman, Andrew M. Leifer", "title": "Fast deep learning correspondence for neuron tracking and identification\n  in C.elegans using synthetic training", "comments": "5 figures", "journal-ref": "eLife 2021;10:e66410", "doi": "10.7554/eLife.66410", "report-no": null, "categories": "q-bio.QM cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present an automated method to track and identify neurons in C. elegans,\ncalled \"fast Deep Learning Correspondence\" or fDLC, based on the transformer\nnetwork architecture. The model is trained once on empirically derived\nsynthetic data and then predicts neural correspondence across held-out real\nanimals via transfer learning. The same pre-trained model both tracks neurons\nacross time and identifies corresponding neurons across individuals.\nPerformance is evaluated against hand-annotated datasets, including NeuroPAL\n[1]. Using only position information, the method achieves 80.0% accuracy at\ntracking neurons within an individual and 65.8% accuracy at identifying neurons\nacross individuals. Accuracy is even higher on a published dataset [2].\nAccuracy reaches 76.5% when using color information from NeuroPAL. Unlike\nprevious methods, fDLC does not require straightening or transforming the\nanimal into a canonical coordinate system. The method is fast and predicts\ncorrespondence in 10 ms making it suitable for future real-time applications.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:46:37 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Yu", "Xinwei", ""], ["Creamer", "Matthew S.", ""], ["Randi", "Francesco", ""], ["Sharma", "Anuj K.", ""], ["Linderman", "Scott W.", ""], ["Leifer", "Andrew M.", ""]]}, {"id": "2101.08215", "submitter": "Achala Shakya", "authors": "Achala Shakya, Mantosh Biswas, Mahesh Pal", "title": "SAR and Optical data fusion based on Anisotropic Diffusion with PCA and\n  Classification using Patch-based with LBP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAR (VV and VH polarization) and optical data are widely used in image fusion\nto use the complimentary information of each other and to obtain the\nbetter-quality image (in terms of spatial and spectral features) for the\nimproved classification results. This paper uses anisotropic diffusion with PCA\nfor the fusion of SAR and optical data and patch-based SVM Classification with\nLBP (LBP-PSVM). Fusion results with VV polarization performed better than VH\npolarization using considered fusion method. For classification, the\nperformance of LBP-PSVM using S1 (VV) with S2, S1 (VH) with S2 is compared with\nSVM classifier (without patch) and PSVM classifier (with patch), respectively.\nClassification results suggests that the LBP-PSVM classifier is more effective\nin comparison to SVM and PSVM classifiers for considered data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:59:00 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Shakya", "Achala", ""], ["Biswas", "Mantosh", ""], ["Pal", "Mahesh", ""]]}, {"id": "2101.08237", "submitter": "Huixiang Luo", "authors": "Huixiang Luo, Hao Cheng, Yuting Gao, Ke Li, Mengdan Zhang, Fanxu Meng,\n  Xiaowei Guo, Feiyue Huang, Xing Sun", "title": "On The Consistency Training for Open-Set Semi-Supervised Learning", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional semi-supervised learning (SSL) methods, e.g., MixMatch, achieve\ngreat performance when both labeled and unlabeled dataset are drawn from the\nsame distribution. However, these methods often suffer severe performance\ndegradation in a more realistic setting, where unlabeled dataset contains\nout-of-distribution (OOD) samples. Recent approaches mitigate the negative\ninfluence of OOD samples by filtering them out from the unlabeled data. Our\nstudies show that it is not necessary to get rid of OOD samples during\ntraining. On the contrary, the network can benefit from them if OOD samples are\nproperly utilized. We thoroughly study how OOD samples affect DNN training in\nboth low- and high-dimensional spaces, where two fundamental SSL methods are\nconsidered: Pseudo Labeling (PL) and Data Augmentation based Consistency\nTraining (DACT). Conclusion is twofold: (1) unlike PL that suffers performance\ndegradation, DACT brings improvement to model performance; (2) the improvement\nis closely related to class-wise distribution gap between the labeled and the\nunlabeled dataset. Motivated by this observation, we further improve the model\nperformance by bridging the gap between the labeled and the unlabeled datasets\n(containing OOD samples). Compared to previous algorithms paying much attention\nto distinguishing between ID and OOD samples, our method makes better use of\nOOD samples and achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:38:17 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Luo", "Huixiang", ""], ["Cheng", "Hao", ""], ["Gao", "Yuting", ""], ["Li", "Ke", ""], ["Zhang", "Mengdan", ""], ["Meng", "Fanxu", ""], ["Guo", "Xiaowei", ""], ["Huang", "Feiyue", ""], ["Sun", "Xing", ""]]}, {"id": "2101.08238", "submitter": "Ammarah Farooq", "authors": "Ammarah Farooq, Muhammad Awais, Josef Kittler, Syed Safwan Khalid", "title": "AXM-Net: Cross-Modal Context Sharing Attention Network for Person Re-ID", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal person re-identification (Re-ID) is critical for modern video\nsurveillance systems. The key challenge is to align inter-modality\nrepresentations according to semantic information present for a person and\nignore background information. In this work, we present AXM-Net, a novel CNN\nbased architecture designed for learning semantically aligned visual and\ntextual representations. The underlying building block consists of multiple\nstreams of feature maps coming from visual and textual modalities and a novel\nlearnable context sharing semantic alignment network. We also propose\ncomplementary intra modal attention learning mechanisms to focus on more\nfine-grained local details in the features along with a cross-modal affinity\nloss for robust feature matching. Our design is unique in its ability to\nimplicitly learn feature alignments from data. The entire AXM-Net can be\ntrained in an end-to-end manner. We report results on both person search and\ncross-modal Re-ID tasks. Extensive experimentation validates the proposed\nframework and demonstrates its superiority by outperforming the current\nstate-of-the-art methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:06:39 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 15:28:49 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Farooq", "Ammarah", ""], ["Awais", "Muhammad", ""], ["Kittler", "Josef", ""], ["Khalid", "Syed Safwan", ""]]}, {"id": "2101.08286", "submitter": "Matthew Colbrook", "authors": "Matthew J. Colbrook, Vegard Antun, Anders C. Hansen", "title": "Can stable and accurate neural networks be computed? -- On the barriers\n  of deep learning and Smale's 18th problem", "comments": "14 pages + SI Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA cs.NE math.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning (DL) has had unprecedented success and is now entering\nscientific computing with full force. However, current DL methods typically\nsuffer from instability, even when universal approximation properties guarantee\nthe existence of stable neural networks (NNs). We address this paradox by\ndemonstrating basic well-conditioned problems in scientific computing where one\ncan prove the existence of NNs with great approximation qualities, however,\nthere does not exist any algorithm, even randomised, that can train (or\ncompute) such a NN. For any positive integers $K > 2$ and $L$, there are cases\nwhere simultaneously: (a) no randomised training algorithm can compute a NN\ncorrect to $K$ digits with probability greater than $1/2$, (b) there exists a\ndeterministic training algorithm that computes a NN with $K-1$ correct digits,\nbut any such (even randomised) algorithm needs arbitrarily many training data,\n(c) there exists a deterministic training algorithm that computes a NN with\n$K-2$ correct digits using no more than $L$ training samples. These results\nimply a classification theory describing conditions under which (stable) NNs\nwith a given accuracy can be computed by an algorithm. We begin this theory by\nestablishing sufficient conditions for the existence of algorithms that compute\nstable NNs in inverse problems. We introduce Fast Iterative REstarted NETworks\n(FIRENETs), which we both prove and numerically verify are stable. Moreover, we\nprove that only $\\mathcal{O}(|\\log(\\epsilon)|)$ layers are needed for an\n$\\epsilon$-accurate solution to the inverse problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 19:04:17 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:09:49 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Colbrook", "Matthew J.", ""], ["Antun", "Vegard", ""], ["Hansen", "Anders C.", ""]]}, {"id": "2101.08299", "submitter": "Berat Kurar Barakat", "authors": "Berat Barakat, Ahmad Droby, Majeed Kassis and Jihad El-Sana", "title": "Text Line Segmentation for Challenging Handwritten Document Images Using\n  Fully Convolutional Network", "comments": "ICFHR 2018 16th International Conference on Frontiers in Handwriting\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a method for text line segmentation of challenging\nhistorical manuscript images. These manuscript images contain narrow interline\nspaces with touching components, interpenetrating vowel signs and inconsistent\nfont types and sizes. In addition, they contain curved, multi-skewed and\nmulti-directed side note lines within a complex page layout. Therefore,\nbounding polygon labeling would be very difficult and time consuming. Instead\nwe rely on line masks that connect the components on the same text line. Then\nthese line masks are predicted using a Fully Convolutional Network (FCN). In\nthe literature, FCN has been successfully used for text line segmentation of\nregular handwritten document images. The present paper shows that FCN is useful\nwith challenging manuscript images as well. Using a new evaluation metric that\nis sensitive to over segmentation as well as under segmentation, testing\nresults on a publicly available challenging handwritten dataset are comparable\nwith the results of a previous work on the same dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 19:51:26 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Barakat", "Berat", ""], ["Droby", "Ahmad", ""], ["Kassis", "Majeed", ""], ["El-Sana", "Jihad", ""]]}, {"id": "2101.08301", "submitter": "Laila Khalid", "authors": "Wei Gong, Laila Khalid", "title": "Aesthetics, Personalization and Recommendation: A survey on Deep\n  Learning in Fashion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Machine learning is completely changing the trends in the fashion industry.\nFrom big to small every brand is using machine learning techniques in order to\nimprove their revenue, increase customers and stay ahead of the trend. People\nare into fashion and they want to know what looks best and how they can improve\ntheir style and elevate their personality. Using Deep learning technology and\ninfusing it with Computer Vision techniques one can do so by utilizing\nBrain-inspired Deep Networks, and engaging into Neuroaesthetics, working with\nGANs and Training them, playing around with Unstructured Data,and infusing the\ntransformer architecture are just some highlights which can be touched with the\nFashion domain. Its all about designing a system that can tell us information\nregarding the fashion aspect that can come in handy with the ever growing\ndemand. Personalization is a big factor that impacts the spending choices of\ncustomers.The survey also shows remarkable approaches that encroach the subject\nof achieving that by divulging deep into how visual data can be interpreted and\nleveraged into different models and approaches. Aesthetics play a vital role in\nclothing recommendation as users' decision depends largely on whether the\nclothing is in line with their aesthetics, however the conventional image\nfeatures cannot portray this directly. For that the survey also highlights\nremarkable models like tensor factorization model, conditional random field\nmodel among others to cater the need to acknowledge aesthetics as an important\nfactor in Apparel recommendation.These AI inspired deep models can pinpoint\nexactly which certain style resonates best with their customers and they can\nhave an understanding of how the new designs will set in with the community.\nWith AI and machine learning your businesses can stay ahead of the fashion\ntrends.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 19:57:13 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gong", "Wei", ""], ["Khalid", "Laila", ""]]}, {"id": "2101.08309", "submitter": "Bal\\'azs Maga", "authors": "Bal\\'azs Maga", "title": "Chest X-ray lung and heart segmentation based on minimal training sets", "comments": "Preprint. arXiv admin note: text overlap with arXiv:2003.10304", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the COVID-19 pandemic aggravated the excessive workload of doctors\nglobally, the demand for computer aided methods in medical imaging analysis\nincreased even further. Such tools can result in more robust diagnostic\npipelines which are less prone to human errors. In our paper, we present a deep\nneural network to which we refer to as Attention BCDU-Net, and apply it to the\ntask of lung and heart segmentation from chest X-ray (CXR) images, a basic but\nardous step in the diagnostic pipeline, for instance for the detection of\ncardiomegaly. We show that the fine-tuned model exceeds previous\nstate-of-the-art results, reaching $98.1\\pm 0.1\\%$ Dice score and $95.2\\pm\n0.1\\%$ IoU score on the dataset of Japanese Society of Radiological Technology\n(JSRT). Besides that, we demonstrate the relative simplicity of the task by\nattaining surprisingly strong results with training sets of size 10 and 20: in\nterms of Dice score, $97.0\\pm 0.8\\%$ and $97.3\\pm 0.5$, respectively, while in\nterms of IoU score, $92.2\\pm 1.2\\%$ and $93.3\\pm 0.4\\%$, respectively. To\nachieve these scores, we capitalize on the mixup augmentation technique, which\nyields a remarkable gain above $4\\%$ IoU score in the size 10 setup.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 20:24:35 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Maga", "Bal\u00e1zs", ""]]}, {"id": "2101.08339", "submitter": "Lin Zhang", "authors": "Lin Zhang, Tiziano Portenier, Orcun Goksel", "title": "Learning Ultrasound Rendering from Cross-Sectional Model Slices for\n  Simulated Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. Given the high level of expertise required for navigation and\ninterpretation of ultrasound images, computational simulations can facilitate\nthe training of such skills in virtual reality. With ray-tracing based\nsimulations, realistic ultrasound images can be generated. However, due to\ncomputational constraints for interactivity, image quality typically needs to\nbe compromised.\n  Methods. We propose herein to bypass any rendering and simulation process at\ninteractive time, by conducting such simulations during a non-time-critical\noffline stage and then learning image translation from cross-sectional model\nslices to such simulated frames. We use a generative adversarial framework with\na dedicated generator architecture and input feeding scheme, which both\nsubstantially improve image quality without increase in network parameters.\nIntegral attenuation maps derived from cross-sectional model slices,\ntexture-friendly strided convolutions, providing stochastic noise and input\nmaps to intermediate layers in order to preserve locality are all shown herein\nto greatly facilitate such translation task.\n  Results. Given several quality metrics, the proposed method with only tissue\nmaps as input is shown to provide comparable or superior results to a\nstate-of-the-art that uses additional images of low-quality ultrasound\nrenderings. An extensive ablation study shows the need and benefits from the\nindividual contributions utilized in this work, based on qualitative examples\nand quantitative ultrasound similarity metrics. To that end, a local histogram\nstatistics based error metric is proposed and demonstrated for visualization of\nlocal dissimilarities between ultrasound images.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 21:58:19 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Lin", ""], ["Portenier", "Tiziano", ""], ["Goksel", "Orcun", ""]]}, {"id": "2101.08345", "submitter": "Giovanna Menardi", "authors": "Giovanna Menardi", "title": "Nonparametric clustering for image segmentation", "comments": null, "journal-ref": "Statistical Analysis and Data Mining, 13(1), 83-97 (2020)", "doi": "10.1002/sam.11444", "report-no": null, "categories": "cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation aims at identifying regions of interest within an image,\nby grouping pixels according to their properties. This task resembles the\nstatistical one of clustering, yet many standard clustering methods fail to\nmeet the basic requirements of image segmentation: segment shapes are often\nbiased toward predetermined shapes and their number is rarely determined\nautomatically. Nonparametric clustering is, in principle, free from these\nlimitations and turns out to be particularly suitable for the task of image\nsegmentation. This is also witnessed by several operational analogies, as, for\ninstance, the resort to topological data analysis and spatial tessellation in\nboth the frameworks. We discuss the application of nonparametric clustering to\nimage segmentation and provide an algorithm specific for this task. Pixel\nsimilarity is evaluated in terms of density of the color representation and the\nadjacency structure of the pixels is exploited to introduce a simple, yet\neffective method to identify image segments as disconnected high-density\nregions. The proposed method works both to segment an image and to detect its\nboundaries and can be seen as a generalization to color images of the class of\nthresholding methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 22:27:44 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Menardi", "Giovanna", ""]]}, {"id": "2101.08398", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Ghada Zamzmi, Fawwaz Batayneh", "title": "TDA-Net: Fusion of Persistent Homology and Deep Learning Features for\n  COVID-19 Detection in Chest X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) has emerged recently as a robust tool to\nextract and compare the structure of datasets. TDA identifies features in data\nsuch as connected components and holes and assigns a quantitative measure to\nthese features. Several studies reported that topological features extracted by\nTDA tools provide unique information about the data, discover new insights, and\ndetermine which feature is more related to the outcome. On the other hand, the\noverwhelming success of deep neural networks in learning patterns and\nrelationships has been proven on a vast array of data applications, images in\nparticular. To capture the characteristics of both powerful tools, we propose\n\\textit{TDA-Net}, a novel ensemble network that fuses topological and deep\nfeatures for the purpose of enhancing model generalizability and accuracy. We\napply the proposed \\textit{TDA-Net} to a critical application, which is the\nautomated detection of COVID-19 from CXR images. The experimental results\nshowed that the proposed network achieved excellent performance and suggests\nthe applicability of our method in practice.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:51:12 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 02:58:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hajij", "Mustafa", ""], ["Zamzmi", "Ghada", ""], ["Batayneh", "Fawwaz", ""]]}, {"id": "2101.08413", "submitter": "Hongjiang Wei", "authors": "Ruimin Feng, Jiayi Zhao, He Wang, Baofeng Yang, Jie Feng, Yuting Shi,\n  Ming Zhang, Chunlei Liu, Yuyao Zhang, Jie Zhuang, Hongjiang Wei", "title": "MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility\n  Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative susceptibility mapping (QSM) has demonstrated great potential in\nquantifying tissue susceptibility in various brain diseases. However, the\nintrinsic ill-posed inverse problem relating the tissue phase to the underlying\nsusceptibility distribution affects the accuracy for quantifying tissue\nsusceptibility. Recently, deep learning has shown promising results to improve\naccuracy by reducing the streaking artifacts. However, there exists a mismatch\nbetween the observed phase and the theoretical forward phase estimated by the\nsusceptibility label. In this study, we proposed a model-based deep learning\narchitecture that followed the STI (susceptibility tensor imaging) physical\nmodel, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the\nrelationship between STI-derived phase contrast induced by the susceptibility\ntensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The\nconvolution neural networks are embedded into the physical model to learn a\nregularization term containing prior information. ki33 and phase induced by\nki13 and ki23 terms were used as the labels for network training. Quantitative\nevaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed\ndeep learning QSM methods. The results showed that MoDL-QSM achieved superior\nperformance, demonstrating its potential for future applications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 02:52:05 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 07:00:52 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Feng", "Ruimin", ""], ["Zhao", "Jiayi", ""], ["Wang", "He", ""], ["Yang", "Baofeng", ""], ["Feng", "Jie", ""], ["Shi", "Yuting", ""], ["Zhang", "Ming", ""], ["Liu", "Chunlei", ""], ["Zhang", "Yuyao", ""], ["Zhuang", "Jie", ""], ["Wei", "Hongjiang", ""]]}, {"id": "2101.08415", "submitter": "Zhongxia Zhang", "authors": "Zhongxia Zhang, Mingwen Wang", "title": "Finger Vein Recognition by Generating Code", "comments": "11pages,1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Finger vein recognition has drawn increasing attention as one of the most\npopular and promising biometrics due to its high distinguishes ability,\nsecurity and non-invasive procedure. The main idea of traditional schemes is to\ndirectly extract features from finger vein images or patterns and then compare\nfeatures to find the best match. However, the features extracted from images\ncontain much redundant data, while the features extracted from patterns are\ngreatly influenced by image segmentation methods. To tack these problems, this\npaper proposes a new finger vein recognition by generating code. The proposed\nmethod does not require an image segmentation algorithm, is simple to calculate\nand has a small amount of data. Firstly, the finger vein images were divided\ninto blocks to calculate the mean value. Then the centrosymmetric coding is\nperformed by using the generated eigenmatrix. The obtained codewords are\nconcatenated as the feature codewords of the image. The similarity between vein\ncodes is measured by the ratio of minimum Hamming distance to codeword length.\nExtensive experiments on two public finger vein databases verify the\neffectiveness of the proposed method. The results indicate that our method\noutperforms the state-of-theart methods and has competitive potential in\nperforming the matching task.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 03:01:56 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Zhongxia", ""], ["Wang", "Mingwen", ""]]}, {"id": "2101.08418", "submitter": "Yuxiang Zhang", "authors": "Yuxiang Zhang, Sachin Mehta, Anat Caspi", "title": "Rethinking Semantic Segmentation Evaluation for Explainability and Model\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation aims to robustly predict coherent class labels for\nentire regions of an image. It is a scene understanding task that powers\nreal-world applications (e.g., autonomous navigation). One important\napplication, the use of imagery for automated semantic understanding of\npedestrian environments, provides remote mapping of accessibility features in\nstreet environments. This application (and others like it) require detailed\ngeometric information of geographical objects. Semantic segmentation is a\nprerequisite for this task since it maps contiguous regions of the same class\nas single entities. Importantly, semantic segmentation uses like ours are not\npixel-wise outcomes; however, most of their quantitative evaluation metrics\n(e.g., mean Intersection Over Union) are based on pixel-wise similarities to a\nground-truth, which fails to emphasize over- and under-segmentation properties\nof a segmentation model. Here, we introduce a new metric to assess region-based\nover- and under-segmentation. We analyze and compare it to other metrics,\ndemonstrating that the use of our metric lends greater explainability to\nsemantic segmentation model performance in real-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 03:12:43 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Yuxiang", ""], ["Mehta", "Sachin", ""], ["Caspi", "Anat", ""]]}, {"id": "2101.08427", "submitter": "Suemin Lee", "authors": "Suemin Lee and Ivan V. Baji\\'c", "title": "Analysis of Information Flow Through U-Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have become ubiquitous in medical image\nprocessing and analysis. Among them, U-Nets are very popular in various image\nsegmentation tasks. Yet, little is known about how information flows through\nthese networks and whether they are indeed properly designed for the tasks they\nare being proposed for. In this paper, we employ information-theoretic tools in\norder to gain insight into information flow through U-Nets. In particular, we\nshow how mutual information between input/output and an intermediate layer can\nbe a useful tool to understand information flow through various portions of a\nU-Net, assess its architectural efficiency, and even propose more efficient\ndesigns.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 03:53:42 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 05:00:24 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Lee", "Suemin", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "2101.08430", "submitter": "Xiangyu He", "authors": "Xiangyu He, Qinghao Hu, Peisong Wang, Jian Cheng", "title": "Generative Zero-shot Network Quantization", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are able to learn realistic image priors from\nnumerous training samples in low-level image generation and restoration. We\nshow that, for high-level image recognition tasks, we can further reconstruct\n\"realistic\" images of each category by leveraging intrinsic Batch Normalization\n(BN) statistics without any training data. Inspired by the popular VAE/GAN\nmethods, we regard the zero-shot optimization process of synthetic images as\ngenerative modeling to match the distribution of BN statistics. The generated\nimages serve as a calibration set for the following zero-shot network\nquantizations. Our method meets the needs for quantizing models based on\nsensitive information, \\textit{e.g.,} due to privacy concerns, no data is\navailable. Extensive experiments on benchmark datasets show that, with the help\nof generated data, our approach consistently outperforms existing data-free\nquantization methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 04:10:04 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["He", "Xiangyu", ""], ["Hu", "Qinghao", ""], ["Wang", "Peisong", ""], ["Cheng", "Jian", ""]]}, {"id": "2101.08434", "submitter": "Varad Bhatnagar", "authors": "Ravi Raj, Varad Bhatnagar, Aman Kumar Singh, Sneha Mane and Nilima\n  Walde", "title": "Video Summarization: Study of various techniques", "comments": null, "journal-ref": "Video Summarization: Study of Various Techniques Proceedings of\n  IRAJ International Conference, 26th May, 2019, Pune, India", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A comparative study of various techniques which can be used for summarization\nof Videos i.e. Video to Video conversion is presented along with respective\narchitecture, results, strengths and shortcomings. In all approaches, a lengthy\nvideo is converted into a shorter video which aims to capture all important\nevents that are present in the original video. The definition of 'important\nevent' may vary according to the context, such as a sports video and a\ndocumentary may have different events which are classified as important.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 04:45:57 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Raj", "Ravi", ""], ["Bhatnagar", "Varad", ""], ["Singh", "Aman Kumar", ""], ["Mane", "Sneha", ""], ["Walde", "Nilima", ""]]}, {"id": "2101.08446", "submitter": "Bowen Li", "authors": "Bowen Li, Changhong Fu, Fangqiang Ding, Junjie Ye, Fuling Lin", "title": "All-Day Object Tracking for Unmanned Aerial Vehicle", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual object tracking, which is representing a major interest in image\nprocessing field, has facilitated numerous real world applications. Among them,\nequipping unmanned aerial vehicle (UAV) with real time robust visual trackers\nfor all day aerial maneuver, is currently attracting incremental attention and\nhas remarkably broadened the scope of applications of object tracking. However,\nprior tracking methods have merely focused on robust tracking in the\nwell-illuminated scenes, while ignoring trackers' capabilities to be deployed\nin the dark. In darkness, the conditions can be more complex and harsh, easily\nposing inferior robust tracking or even tracking failure. To this end, this\nwork proposed a novel discriminative correlation filter based tracker with\nillumination adaptive and anti dark capability, namely ADTrack. ADTrack firstly\nexploits image illuminance information to enable adaptability of the model to\nthe given light condition. Then, by virtue of an efficient and effective image\nenhancer, ADTrack carries out image pretreatment, where a target aware mask is\ngenerated. Benefiting from the mask, ADTrack aims to solve a dual regression\nproblem where dual filters, i.e., the context filter and target focused filter,\nare trained with mutual constraint. Thus ADTrack is able to maintain\ncontinuously favorable performance in all-day conditions. Besides, this work\nalso constructed one UAV nighttime tracking benchmark UAVDark135, comprising of\nmore than 125k manually annotated frames, which is also very first UAV\nnighttime tracking benchmark. Exhaustive experiments are extended on\nauthoritative daytime benchmarks, i.e., UAV123 10fps, DTB70, and the newly\nbuilt dark benchmark UAVDark135, which have validated the superiority of\nADTrack in both bright and dark conditions on a single CPU.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 05:12:37 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 13:05:07 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Li", "Bowen", ""], ["Fu", "Changhong", ""], ["Ding", "Fangqiang", ""], ["Ye", "Junjie", ""], ["Lin", "Fuling", ""]]}, {"id": "2101.08459", "submitter": "Debarati Chakraborty Dr.", "authors": "Debarati B. Chakrabortya, Vinay Detania and Shah Parshv Jigneshkumar", "title": "Fire Threat Detection From Videos with Q-Rough Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article defines new methods for unsupervised fire region segmentation\nand fire threat detection from video stream. Fire in control serves a number of\npurposes to human civilization, but it could simultaneously be a threat once\nits spread becomes uncontrolled. There exists many methods on fire region\nsegmentation and fire non-fire classification. But the approaches to determine\nthe threat associated with fire is relatively scare, and no such unsupervised\nmethod has been formulated yet. Here we focus on developing an unsupervised\nmethod with which the threat of fire can be quantified and accordingly generate\nan alarm in automated surveillance systems in indoor as well as in outdoors.\nFire region segmentation without any manual intervention/ labelled data set is\na major challenge while formulating such a method. Here we have used rough\napproximations to approximate the fire region, and to manage the incompleteness\nof the knowledge base, due to absence of any prior information. Utility\nmaximization of Q-learning has been used to minimize ambiguities in the rough\napproximations. The new set approximation method, thus developed here, is named\nas Q-rough set. It is used for fire region segmentation from video frames. The\nthreat index of fire flame over the input video stream has been defined in sync\nwith the relative growth in the fire segments on the recent frames. All\ntheories and indices defined here have been experimentally validated with\ndifferent types of fire videos, through demonstrations and comparisons, as\nsuperior to the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 06:29:36 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Chakrabortya", "Debarati B.", ""], ["Detania", "Vinay", ""], ["Jigneshkumar", "Shah Parshv", ""]]}, {"id": "2101.08461", "submitter": "Enze Xie", "authors": "Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang,\n  Ping Luo", "title": "Segmenting Transparent Object in the Wild with Transformer", "comments": "Tech. Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents a new fine-grained transparent object segmentation\ndataset, termed Trans10K-v2, extending Trans10K-v1, the first large-scale\ntransparent object segmentation dataset. Unlike Trans10K-v1 that only has two\nlimited categories, our new dataset has several appealing benefits. (1) It has\n11 fine-grained categories of transparent objects, commonly occurring in the\nhuman domestic environment, making it more practical for real-world\napplication. (2) Trans10K-v2 brings more challenges for the current advanced\nsegmentation methods than its former version. Furthermore, a novel\ntransformer-based segmentation pipeline termed Trans2Seg is proposed. Firstly,\nthe transformer encoder of Trans2Seg provides the global receptive field in\ncontrast to CNN's local receptive field, which shows excellent advantages over\npure CNN architectures. Secondly, by formulating semantic segmentation as a\nproblem of dictionary look-up, we design a set of learnable prototypes as the\nquery of Trans2Seg's transformer decoder, where each prototype learns the\nstatistics of one category in the whole dataset. We benchmark more than 20\nrecent semantic segmentation methods, demonstrating that Trans2Seg\nsignificantly outperforms all the CNN-based methods, showing the proposed\nalgorithm's potential ability to solve transparent object segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 06:41:00 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 05:14:35 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 13:23:16 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Xie", "Enze", ""], ["Wang", "Wenjia", ""], ["Wang", "Wenhai", ""], ["Sun", "Peize", ""], ["Xu", "Hang", ""], ["Liang", "Ding", ""], ["Luo", "Ping", ""]]}, {"id": "2101.08463", "submitter": "Debarati Chakraborty Dr.", "authors": "Deesha Chavan, Dev Saad and Debarati B. Chakraborty", "title": "COLLIDE-PRED: Prediction of On-Road Collision From Surveillance Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting on-road abnormalities such as road accidents or traffic violations\nis a challenging task in traffic surveillance. If such predictions can be done\nin advance, many damages can be controlled. Here in our wok, we tried to\nformulate a solution for automated collision prediction in traffic surveillance\nvideos with computer vision and deep networks. It involves object detection,\ntracking, trajectory estimation, and collision prediction. We propose an\nend-to-end collision prediction system, named as COLLIDE-PRED, that\nintelligently integrates the information of past and future trajectories of\nmoving objects to predict collisions in videos. It is a pipeline that starts\nwith object detection, which is used for object tracking, and then trajectory\nprediction is performed which concludes by collision detection. The probable\nplace of collision, and the objects those may cause the collision, both can be\nidentified correctly with COLLIDE-PRED. The proposed method is experimentally\nvalidated with a number of different videos and proves to be effective in\nidentifying accident in advance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 06:45:56 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Chavan", "Deesha", ""], ["Saad", "Dev", ""], ["Chakraborty", "Debarati B.", ""]]}, {"id": "2101.08465", "submitter": "Cong Wang", "authors": "Cong Wang, Yan Huang, Yuexian Zou, Yong Xu", "title": "FWB-Net:Front White Balance Network for Color Shift Correction in Single\n  Image Dehazing via Atmospheric Light Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, single image dehazing deep models based on Atmospheric\nScattering Model (ASM) have achieved remarkable results. But the dehazing\noutputs of those models suffer from color shift. Analyzing the ASM model shows\nthat the atmospheric light factor (ALF) is set as a scalar which indicates ALF\nis constant for whole image. However, for images taken in real-world, the\nillumination is not uniformly distributed over whole image which brings model\nmismatch and possibly results in color shift of the deep models using ASM.\nBearing this in mind, in this study, first, a new non-homogeneous atmospheric\nscattering model (NH-ASM) is proposed for improving image modeling of hazy\nimages taken under complex illumination conditions. Second, a new U-Net based\nfront white balance module (FWB-Module) is dedicatedly designed to correct\ncolor shift before generating dehazing result via atmospheric light estimation.\nThird, a new FWB loss is innovatively developed for training FWB-Module, which\nimposes penalty on color shift. In the end, based on NH-ASM and front white\nbalance technology, an end-to-end CNN-based color-shift-restraining dehazing\nnetwork is developed, termed as FWB-Net. Experimental results demonstrate the\neffectiveness and superiority of our proposed FWB-Net for dehazing on both\nsynthetic and real-world images.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 06:53:44 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Wang", "Cong", ""], ["Huang", "Yan", ""], ["Zou", "Yuexian", ""], ["Xu", "Yong", ""]]}, {"id": "2101.08466", "submitter": "Xiaoke Peng", "authors": "Nan Jiang, Kuiran Wang, Xiaoke Peng, Xuehui Yu, Qiang Wang, Junliang\n  Xing, Guorong Li, Jian Zhao, Guodong Guo, Zhenjun Han", "title": "Anti-UAV: A Large Multi-Modal Benchmark for UAV Tracking", "comments": "13 pages, 8 figures, submitted to IEEE T-MM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned Aerial Vehicle (UAV) offers lots of applications in both commerce\nand recreation. With this, monitoring the operation status of UAVs is crucially\nimportant. In this work, we consider the task of tracking UAVs, providing rich\ninformation such as location and trajectory. To facilitate research on this\ntopic, we propose a dataset, Anti-UAV, with more than 300 video pairs\ncontaining over 580k manually annotated bounding boxes. The releasing of such a\nlarge-scale dataset could be a useful initial step in research of tracking\nUAVs. Furthermore, the advancement of addressing research challenges in\nAnti-UAV can help the design of anti-UAV systems, leading to better\nsurveillance of UAVs. Besides, a novel approach named dual-flow semantic\nconsistency (DFSC) is proposed for UAV tracking. Modulated by the semantic flow\nacross video sequences, the tracker learns more robust class-level semantic\ninformation and obtains more discriminative instance-level features.\nExperimental results demonstrate that Anti-UAV is very challenging, and the\nproposed method can effectively improve the tracker's performance. The Anti-UAV\nbenchmark and the code of the proposed approach will be publicly available at\nhttps://github.com/ucas-vg/Anti-UAV.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:00:15 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 00:18:16 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 02:01:55 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Jiang", "Nan", ""], ["Wang", "Kuiran", ""], ["Peng", "Xiaoke", ""], ["Yu", "Xuehui", ""], ["Wang", "Qiang", ""], ["Xing", "Junliang", ""], ["Li", "Guorong", ""], ["Zhao", "Jian", ""], ["Guo", "Guodong", ""], ["Han", "Zhenjun", ""]]}, {"id": "2101.08467", "submitter": "Chaoyou Fu", "authors": "Chaoyou Fu, Yibo Hu, Xiang Wu, Hailin Shi, Tao Mei, Ran He", "title": "CM-NAS: Cross-Modality Neural Architecture Search for Visible-Infrared\n  Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible-Infrared person re-identification (VI-ReID) aims to match\ncross-modality pedestrian images, breaking through the limitation of\nsingle-modality person ReID in dark environment. In order to mitigate the\nimpact of large modality discrepancy, existing works manually design various\ntwo-stream architectures to separately learn modality-specific and\nmodality-sharable representations. Such a manual design routine, however,\nhighly depends on massive experiments and empirical practice, which is time\nconsuming and labor intensive. In this paper, we systematically study the\nmanually designed architectures, and identify that appropriately separating\nBatch Normalization (BN) layers is the key to bring a great boost towards\ncross-modality matching. Based on this observation, the essential objective is\nto find the optimal separation scheme for each BN layer. To this end, we\npropose a novel method, named Cross-Modality Neural Architecture Search\n(CM-NAS). It consists of a BN-oriented search space in which the standard\noptimization can be fulfilled subject to the cross-modality task. Equipped with\nthe searched architecture, our method outperforms state-of-the-art counterparts\nin both two benchmarks, improving the Rank-1/mAP by 6.70%/6.13% on SYSU-MM01\nand by 12.17%/11.23% on RegDB. In light of its simplicity and effectiveness, we\nexpect CM-NAS will serve as a strong baseline for future research. Code will be\nmade available.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:07:00 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 07:48:02 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Fu", "Chaoyou", ""], ["Hu", "Yibo", ""], ["Wu", "Xiang", ""], ["Shi", "Hailin", ""], ["Mei", "Tao", ""], ["He", "Ran", ""]]}, {"id": "2101.08482", "submitter": "Zhaowei Cai", "authors": "Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes,\n  Zhuowen Tu, Stefano Soatto", "title": "Exponential Moving Average Normalization for Self-supervised and\n  Semi-supervised Learning", "comments": "accepted by CVPR21 as Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a plug-in replacement for batch normalization (BN) called\nexponential moving average normalization (EMAN), which improves the performance\nof existing student-teacher based self- and semi-supervised learning\ntechniques. Unlike the standard BN, where the statistics are computed within\neach batch, EMAN, used in the teacher, updates its statistics by exponential\nmoving average from the BN statistics of the student. This design reduces the\nintrinsic cross-sample dependency of BN and enhances the generalization of the\nteacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2\npoints and semi-supervised learning by about 7/2 points, when 1%/10% supervised\nlabels are available on ImageNet. These improvements are consistent across\nmethods, network architectures, training duration, and datasets, demonstrating\nthe general effectiveness of this technique. The code is available at\nhttps://github.com/amazon-research/exponential-moving-average-normalization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:45:37 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 08:16:27 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Cai", "Zhaowei", ""], ["Ravichandran", "Avinash", ""], ["Maji", "Subhransu", ""], ["Fowlkes", "Charless", ""], ["Tu", "Zhuowen", ""], ["Soatto", "Stefano", ""]]}, {"id": "2101.08502", "submitter": "Shadrokh Samavi", "authors": "Maedeh Jamali, Nader Karimi, Shadrokh Samavi", "title": "Weighted Fuzzy-Based PSNR for Watermarking", "comments": "Five pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the problems of conventional visual quality evaluation criteria such\nas PSNR and MSE is the lack of appropriate standards based on the human visual\nsystem (HVS). They are calculated based on the difference of the corresponding\npixels in the original and manipulated image. Hence, they practically do not\nprovide a correct understanding of the image quality. Watermarking is an image\nprocessing application in which the image's visual quality is an essential\ncriterion for its evaluation. Watermarking requires a criterion based on the\nHVS that provides more accurate values than conventional measures such as PSNR.\nThis paper proposes a weighted fuzzy-based criterion that tries to find\nessential parts of an image based on the HVS. Then these parts will have larger\nweights in computing the final value of PSNR. We compare our results against\nstandard PSNR, and our experiments show considerable consequences.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:41:05 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Jamali", "Maedeh", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2101.08515", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka and Kazushige Okayasu and Asato Matsumoto and Eisuke\n  Yamagata and Ryosuke Yamada and Nakamasa Inoue and Akio Nakamura and Yutaka\n  Satoh", "title": "Pre-training without Natural Images", "comments": "ACCV 2020 Best Paper Honorable Mention Award, Codes are publicly\n  available:\n  https://github.com/hirokatsukataoka16/FractalDB-Pretrained-ResNet-PyTorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to use convolutional neural networks pre-trained without any\nnatural images to assist natural image understanding? The paper proposes a\nnovel concept, Formula-driven Supervised Learning. We automatically generate\nimage patterns and their category labels by assigning fractals, which are based\non a natural law existing in the background knowledge of the real world.\nTheoretically, the use of automatically generated images instead of natural\nimages in the pre-training phase allows us to generate an infinite scale\ndataset of labeled images. Although the models pre-trained with the proposed\nFractal DataBase (FractalDB), a database without natural images, does not\nnecessarily outperform models pre-trained with human annotated datasets at all\nsettings, we are able to partially surpass the accuracy of ImageNet/Places\npre-trained models. The image representation with the proposed FractalDB\ncaptures a unique feature in the visualization of convolutional layers and\nattentions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 09:47:32 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Okayasu", "Kazushige", ""], ["Matsumoto", "Asato", ""], ["Yamagata", "Eisuke", ""], ["Yamada", "Ryosuke", ""], ["Inoue", "Nakamasa", ""], ["Nakamura", "Akio", ""], ["Satoh", "Yutaka", ""]]}, {"id": "2101.08524", "submitter": "Mercedes Garcia-Salguero", "authors": "Mercedes Garcia-Salguero and Javier Gonzalez-Jimenez", "title": "Fast and Robust Certifiable Estimation of the Relative Pose Between Two\n  Calibrated Cameras", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-021-01044-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work contributes an efficient algorithm to compute the Relative Pose\nproblem (RPp) between calibrated cameras and certify the optimality of the\nsolution, given a set of pair-wise feature correspondences affected by noise\nand probably corrupted by wrong matches. We propose a family of certifiers that\nis shown to increase the ratio of detected optimal solutions. This set of\ncertifiers is incorporated into a fast essential matrix estimation pipeline\nthat, given any initial guess for the RPp, refines it iteratively on the\nproduct space of 3D rotations and 2-sphere. In addition, this fast certifiable\npipeline is integrated into a robust framework that combines Graduated\nNon-convexity and the Black-Rangarajan duality between robust functions and\nline processes.\n  We proved through extensive experiments on synthetic and real data that the\nproposed framework provides a fast and robust relative pose estimation. We make\nthe code publicly available\n\\url{https://github.com/mergarsal/FastCertRelPose.git}.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:07:05 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 18:19:59 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Garcia-Salguero", "Mercedes", ""], ["Gonzalez-Jimenez", "Javier", ""]]}, {"id": "2101.08525", "submitter": "Ying Nie", "authors": "Ying Nie, Kai Han, Zhenhua Liu, An Xiao, Yiping Deng, Chunjing Xu,\n  Yunhe Wang", "title": "GhostSR: Learning Ghost Features for Efficient Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern single image super-resolution (SISR) system based on convolutional\nneural networks (CNNs) achieves fancy performance while requires huge\ncomputational costs. The problem on feature redundancy is well studied in\nvisual recognition task, but rarely discussed in SISR. Based on the observation\nthat many features in SISR models are also similar to each other, we propose to\nuse shift operation to generate the redundant features (i.e., Ghost features).\nCompared with depth-wise convolution which is not friendly to GPUs or NPUs,\nshift operation can bring practical inference acceleration for CNNs on common\nhardware. We analyze the benefits of shift operation for SISR and make the\nshift orientation learnable based on Gumbel-Softmax trick. For a given\npre-trained model, we first cluster all filters in each convolutional layer to\nidentify the intrinsic ones for generating intrinsic features. Ghost features\nwill be derived by moving these intrinsic features along a specific\norientation. The complete output features are constructed by concatenating the\nintrinsic and ghost features together. Extensive experiments on several\nbenchmark models and datasets demonstrate that both the non-compact and\nlightweight SISR models embedded in our proposed module can achieve comparable\nperformance to that of their baselines with large reduction of parameters,\nFLOPs and GPU latency. For instance, we reduce the parameters by 47%, FLOPs by\n46% and GPU latency by 41% of EDSR x2 network without significant performance\ndegradation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:09:47 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Nie", "Ying", ""], ["Han", "Kai", ""], ["Liu", "Zhenhua", ""], ["Xiao", "An", ""], ["Deng", "Yiping", ""], ["Xu", "Chunjing", ""], ["Wang", "Yunhe", ""]]}, {"id": "2101.08527", "submitter": "Tian Zhang", "authors": "Tian Zhang, Dongliang Chang, Zhanyu Ma and Jun Guo", "title": "Progressive Co-Attention Network for Fine-grained Visual Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-grained visual classification aims to recognize images belonging to\nmultiple sub-categories within a same category. It is a challenging task due to\nthe inherently subtle variations among highly-confused categories. Most\nexisting methods only take individual image as input, which may limit the\nability of models to recognize contrastive clues from different images. In this\npaper, we propose an effective method called progressive co-attention network\n(PCA-Net) to tackle this problem. Specifically, we calculate the channel-wise\nsimilarity by interacting the feature channels within same-category images to\ncapture the common discriminative features. Considering that complementary\nimformation is also crucial for recognition, we erase the prominent areas\nenhanced by the channel interaction to force the network to focus on other\ndiscriminative regions. The proposed model can be trained in an end-to-end\nmanner, and only requires image-level label supervision. It has achieved\ncompetitive results on three fine-grained visual classification benchmark\ndatasets: CUB-200-2011, Stanford Cars, and FGVC Aircraft.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:19:02 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Tian", ""], ["Chang", "Dongliang", ""], ["Ma", "Zhanyu", ""], ["Guo", "Jun", ""]]}, {"id": "2101.08533", "submitter": "Yunpeng Gong", "authors": "Yunpeng Gong", "title": "A general multi-modal data learning method for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a general multi-modal data learning method, which\nincludes Global Homogeneous Transformation, Local Homogeneous Transformation\nand their combination. During ReID model training, on the one hand, it randomly\nselected a rectangular area in the RGB image and replace its color with the\nsame rectangular area in corresponding homogeneous image, thus it generate a\ntraining image with different homogeneous areas; On the other hand, it convert\nan image into a homogeneous image. These two methods help the model to directly\nlearn the relationship between different modalities in the Special ReID task.\nIn single-modal ReID tasks, it can be used as an effective data augmentation.\nThe experimental results show that our method achieves a performance\nimprovement of up to 3.3% in single modal ReID task, and performance\nimprovement in the Sketch Re-identification more than 8%. In addition, our\nexperiments also show that this method is also very useful in adversarial\ntraining for adversarial defense. It can help the model learn faster and better\nfrom adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:33:02 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 08:26:49 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 15:15:14 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 01:30:13 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gong", "Yunpeng", ""]]}, {"id": "2101.08540", "submitter": "Megha Nawhal", "authors": "Megha Nawhal, Greg Mori", "title": "Activity Graph Transformer for Temporal Action Localization", "comments": "Project webpage: https://www.sfu.ca/~mnawhal/projects/agt.html; Code\n  available at https://github.com/Nmegha2601/activitygraph_transformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Activity Graph Transformer, an end-to-end learnable model for\ntemporal action localization, that receives a video as input and directly\npredicts a set of action instances that appear in the video. Detecting and\nlocalizing action instances in untrimmed videos requires reasoning over\nmultiple action instances in a video. The dominant paradigms in the literature\nprocess videos temporally to either propose action regions or directly produce\nframe-level detections. However, sequential processing of videos is problematic\nwhen the action instances have non-sequential dependencies and/or non-linear\ntemporal ordering, such as overlapping action instances or re-occurrence of\naction instances over the course of the video. In this work, we capture this\nnon-linear temporal structure by reasoning over the videos as non-sequential\nentities in the form of graphs. We evaluate our model on challenging datasets:\nTHUMOS14, Charades, and EPIC-Kitchens-100. Our results show that our proposed\nmodel outperforms the state-of-the-art by a considerable margin.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:42:48 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 12:14:19 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Nawhal", "Megha", ""], ["Mori", "Greg", ""]]}, {"id": "2101.08567", "submitter": "Sovan Biswas", "authors": "Sovan Biswas and Juergen Gall", "title": "Discovering Multi-Label Actor-Action Association in a Weakly Supervised\n  Setting", "comments": "Accepted in ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since collecting and annotating data for spatio-temporal action detection is\nvery expensive, there is a need to learn approaches with less supervision.\nWeakly supervised approaches do not require any bounding box annotations and\ncan be trained only from labels that indicate whether an action occurs in a\nvideo clip. Current approaches, however, cannot handle the case when there are\nmultiple persons in a video that perform multiple actions at the same time. In\nthis work, we address this very challenging task for the first time. We propose\na baseline based on multi-instance and multi-label learning. Furthermore, we\npropose a novel approach that uses sets of actions as representation instead of\nmodeling individual action classes. Since computing, the probabilities for the\nfull power set becomes intractable as the number of action classes increases,\nwe assign an action set to each detected person under the constraint that the\nassignment is consistent with the annotation of the video clip. We evaluate the\nproposed approach on the challenging AVA dataset where the proposed approach\noutperforms the MIML baseline and is competitive to fully supervised\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 11:59:47 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Biswas", "Sovan", ""], ["Gall", "Juergen", ""]]}, {"id": "2101.08581", "submitter": "Sovan Biswas", "authors": "Sovan Biswas, Yaser Souri and Juergen Gall", "title": "Hierarchical Graph-RNNs for Action Detection of Multiple Activities", "comments": "Accepted at ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach that spatially localizes the activities\nin a video frame where each person can perform multiple activities at the same\ntime. Our approach takes the temporal scene context as well as the relations of\nthe actions of detected persons into account. While the temporal context is\nmodeled by a temporal recurrent neural network (RNN), the relations of the\nactions are modeled by a graph RNN. Both networks are trained together and the\nproposed approach achieves state of the art results on the AVA dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 12:50:02 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Biswas", "Sovan", ""], ["Souri", "Yaser", ""], ["Gall", "Juergen", ""]]}, {"id": "2101.08609", "submitter": "Jinhai Yang", "authors": "Jinhai Yang, Hua Yang", "title": "MPASNET: Motion Prior-Aware Siamese Network for Unsupervised Deep Crowd\n  Segmentation in Video Scenes", "comments": "ICIP 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd segmentation is a fundamental task serving as the basis of crowded\nscene analysis, and it is highly desirable to obtain refined pixel-level\nsegmentation maps. However, it remains a challenging problem, as existing\napproaches either require dense pixel-level annotations to train deep learning\nmodels or merely produce rough segmentation maps from optical or particle flows\nwith physical models. In this paper, we propose the Motion Prior-Aware Siamese\nNetwork (MPASNET) for unsupervised crowd semantic segmentation. This model not\nonly eliminates the need for annotation but also yields high-quality\nsegmentation maps. Specially, we first analyze the coherent motion patterns\nacross the frames and then apply a circular region merging strategy on the\ncollective particles to generate pseudo-labels. Moreover, we equip MPASNET with\nsiamese branches for augmentation-invariant regularization and siamese feature\naggregation. Experiments over benchmark datasets indicate that our model\noutperforms the state-of-the-arts by more than 12% in terms of mIoU.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:55:29 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 05:02:45 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yang", "Jinhai", ""], ["Yang", "Hua", ""]]}, {"id": "2101.08629", "submitter": "Yingxue Pang", "authors": "Yingxue Pang, Jianxin Lin, Tao Qin, and Zhibo Chen", "title": "Image-to-Image Translation: Methods and Applications", "comments": "24 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation (I2I) aims to transfer images from a source domain\nto a target domain while preserving the content representations. I2I has drawn\nincreasing attention and made tremendous progress in recent years because of\nits wide range of applications in many computer vision and image processing\nproblems, such as image synthesis, segmentation, style transfer, restoration,\nand pose estimation. In this paper, we provide an overview of the I2I works\ndeveloped in recent years. We will analyze the key techniques of the existing\nI2I works and clarify the main progress the community has made. Additionally,\nwe will elaborate on the effect of I2I on the research and industry community\nand point out remaining challenges in related fields.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 14:20:26 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 11:07:40 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Pang", "Yingxue", ""], ["Lin", "Jianxin", ""], ["Qin", "Tao", ""], ["Chen", "Zhibo", ""]]}, {"id": "2101.08647", "submitter": "Hongxiang Hao", "authors": "Hongxiang Hao., Hanlin Mo., Hua Li", "title": "Geometric Moment Invariants to Motion Blur", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on removing interference of motion blur by the\nderivation of motion blur invariants.Unlike earlier work, we don't restore any\nblurred image. Based on geometric moment and mathematical model of motion blur,\nwe prove that geometric moments of blurred image and original image are\nlinearly related. Depending on this property, we can analyse whether an\nexisting moment-based feature is invariant to motion blur. Surprisingly, we\nfind some geometric moment invariants are invariants to not only spatial\ntransform but also motion blur. Meanwhile, we test invariance and robustness of\nthese invariants using synthetic and real blur image datasets. And the results\nshow these invariants outperform some widely used blur moment invariants and\nnon-moment image features in image retrieval, classification and template\nmatching.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 14:50:34 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 02:35:03 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hao.", "Hongxiang", ""], ["Mo.", "Hanlin", ""], ["Li", "Hua", ""]]}, {"id": "2101.08661", "submitter": "Thomas Oberlin", "authors": "Thomas Oberlin and Mathieu Verm", "title": "Regularization via deep generative models: an analysis point of view", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper proposes a new way of regularizing an inverse problem in imaging\n(e.g., deblurring or inpainting) by means of a deep generative neural network.\nCompared to end-to-end models, such approaches seem particularly interesting\nsince the same network can be used for many different problems and experimental\nconditions, as soon as the generative model is suited to the data. Previous\nworks proposed to use a synthesis framework, where the estimation is performed\non the latent vector, the solution being obtained afterwards via the decoder.\nInstead, we propose an analysis formulation where we directly optimize the\nimage itself and penalize the latent vector. We illustrate the interest of such\na formulation by running experiments of inpainting, deblurring and\nsuper-resolution. In many cases our technique achieves a clear improvement of\nthe performance and seems to be more robust, in particular with respect to\ninitialization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:04:57 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Oberlin", "Thomas", ""], ["Verm", "Mathieu", ""]]}, {"id": "2101.08674", "submitter": "Edwin Arkel Rios", "authors": "Edwin Arkel Rios, Wen-Huang Cheng, Bo-Cheng Lai", "title": "DAF:re: A Challenging, Crowd-Sourced, Large-Scale, Long-Tailed Dataset\n  For Anime Character Recognition", "comments": "5 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we tackle the challenging problem of anime character\nrecognition. Anime, referring to animation produced within Japan and work\nderived or inspired from it. For this purpose we present DAF:re\n(DanbooruAnimeFaces:revamped), a large-scale, crowd-sourced, long-tailed\ndataset with almost 500 K images spread across more than 3000 classes.\nAdditionally, we conduct experiments on DAF:re and similar datasets using a\nvariety of classification models, including CNN based ResNets and\nself-attention based Vision Transformer (ViT). Our results give new insights\ninto the generalization and transfer learning properties of ViT models on\nsubstantially different domain datasets from those used for the upstream\npre-training, including the influence of batch and image size in their\ntraining. Additionally, we share our dataset, source-code, pre-trained\ncheckpoints and results, as Animesion, the first end-to-end framework for\nlarge-scale anime character recognition: https://github.com/arkel23/animesion\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:40:45 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Rios", "Edwin Arkel", ""], ["Cheng", "Wen-Huang", ""], ["Lai", "Bo-Cheng", ""]]}, {"id": "2101.08684", "submitter": "Minh-Quan Dao", "authors": "Minh-Quan Dao, Vincent Fr\\'emont", "title": "A two-stage data association approach for 3D Multi-object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-object tracking (MOT) is an integral part of any autonomous driving\npipelines because itproduces trajectories which has been taken by other moving\nobjects in the scene and helps predicttheir future motion. Thanks to the recent\nadvances in 3D object detection enabled by deep learning,track-by-detection has\nbecome the dominant paradigm in 3D MOT. In this paradigm, a MOT systemis\nessentially made of an object detector and a data association algorithm which\nestablishes track-to-detection correspondence. While 3D object detection has\nbeen actively researched, associationalgorithms for 3D MOT seem to settle at a\nbipartie matching formulated as a linear assignmentproblem (LAP) and solved by\nthe Hungarian algorithm. In this paper, we adapt a two-stage dataassociation\nmethod which was successful in image-based tracking to the 3D setting, thus\nprovidingan alternative for data association for 3D MOT. Our method outperforms\nthe baseline using one-stagebipartie matching for data association by achieving\n0.587 AMOTA in NuScenes validation set.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:50:17 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Dao", "Minh-Quan", ""], ["Fr\u00e9mont", "Vincent", ""]]}, {"id": "2101.08685", "submitter": "Thomas Pfeil", "authors": "Thomas Pfeil", "title": "ItNet: iterative neural networks with small graphs for accurate and\n  efficient anytime prediction", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have usually to be compressed and accelerated for their\nusage in low-power, e.g. mobile, devices. Recently, massively-parallel hardware\naccelerators were developed that offer high throughput and low latency at low\npower by utilizing in-memory computation. However, to exploit these benefits\nthe computational graph of a neural network has to fit into the in-computation\nmemory of these hardware systems that is usually rather limited in size. In\nthis study, we introduce a class of network models that have a small memory\nfootprint in terms of their computational graphs. To this end, the graph is\ndesigned to contain loops by iteratively executing a single network building\nblock. Furthermore, the trade-off between accuracy and latency of these\nso-called iterative neural networks is improved by adding multiple intermediate\noutputs both during training and inference. We show state-of-the-art results\nfor semantic segmentation on the CamVid and Cityscapes datasets that are\nespecially demanding in terms of computational resources. In ablation studies,\nthe improvement of network training by intermediate network outputs as well as\nthe trade-off between weight sharing over iterations and the network size are\ninvestigated.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:56:29 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 14:25:35 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Pfeil", "Thomas", ""]]}, {"id": "2101.08692", "submitter": "Andrew Brock", "authors": "Andrew Brock, Soham De, Samuel L. Smith", "title": "Characterizing signal propagation to close the performance gap in\n  unnormalized ResNets", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization is a key component in almost all state-of-the-art image\nclassifiers, but it also introduces practical challenges: it breaks the\nindependence between training examples within a batch, can incur compute and\nmemory overhead, and often results in unexpected bugs. Building on recent\ntheoretical analyses of deep ResNets at initialization, we propose a simple set\nof analysis tools to characterize signal propagation on the forward pass, and\nleverage these tools to design highly performant ResNets without activation\nnormalization layers. Crucial to our success is an adapted version of the\nrecently proposed Weight Standardization. Our analysis tools show how this\ntechnique preserves the signal in networks with ReLU or Swish activation\nfunctions by ensuring that the per-channel activation means do not grow with\ndepth. Across a range of FLOP budgets, our networks attain performance\ncompetitive with the state-of-the-art EfficientNets on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:07:06 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 11:28:41 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Brock", "Andrew", ""], ["De", "Soham", ""], ["Smith", "Samuel L.", ""]]}, {"id": "2101.08715", "submitter": "Edward Stow", "authors": "Edward Stow, Riku Murai, Sajad Saeedi, Paul H. J. Kelly", "title": "Cain: Automatic Code Generation for Simultaneous Convolutional Kernels\n  on Focal-plane Sensor-processors", "comments": "17 pages, 4 figures, Accepted at LCPC 2020 to be published by\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Focal-plane Sensor-processors (FPSPs) are a camera technology that enable low\npower, high frame rate computation, making them suitable for edge computation.\nUnfortunately, these devices' limited instruction sets and registers make\ndeveloping complex algorithms difficult. In this work, we present Cain - a\ncompiler that targets SCAMP-5, a general-purpose FPSP - which generates code\nfrom multiple convolutional kernels. As an example, given the convolutional\nkernels for an MNIST digit recognition neural network, Cain produces code that\nis half as long, when compared to the other available compilers for SCAMP-5.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:48:28 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Stow", "Edward", ""], ["Murai", "Riku", ""], ["Saeedi", "Sajad", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "2101.08717", "submitter": "Jacson Rodrigues Correia-Silva", "authors": "Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue,\n  Alberto F. De Souza, Thiago Oliveira-Santos", "title": "Copycat CNN: Are Random Non-Labeled Data Enough to Steal Knowledge from\n  Black-box Models?", "comments": "The code is available at https://github.com/jeiks/Stealing_DL_Models", "journal-ref": "Pattern Recognition 113 (2021) 107830", "doi": "10.1016/j.patcog.2021.107830", "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural networks have been successful lately enabling companies\nto develop neural-based products, which demand an expensive process, involving\ndata acquisition and annotation; and model generation, usually requiring\nexperts. With all these costs, companies are concerned about the security of\ntheir models against copies and deliver them as black-boxes accessed by APIs.\nNonetheless, we argue that even black-box models still have some\nvulnerabilities. In a preliminary work, we presented a simple, yet powerful,\nmethod to copy black-box models by querying them with natural random images. In\nthis work, we consolidate and extend the copycat method: (i) some constraints\nare waived; (ii) an extensive evaluation with several problems is performed;\n(iii) models are copied between different architectures; and, (iv) a deeper\nanalysis is performed by looking at the copycat behavior. Results show that\nnatural random images are effective to generate copycats for several problems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:55:14 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Correia-Silva", "Jacson Rodrigues", ""], ["Berriel", "Rodrigo F.", ""], ["Badue", "Claudine", ""], ["De Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "2101.08732", "submitter": "Lang Huang", "authors": "Lang Huang, Chao Zhang and Hongyang Zhang", "title": "Self-Adaptive Training: Bridging the Supervised and Self-Supervised\n  Learning", "comments": "Journal version of arXiv:2002.10319 [cs.LG] (NeurIPS2020). 19 pages,\n  15 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose self-adaptive training -- a unified training algorithm that\ndynamically calibrates and enhances training process by model predictions\nwithout incurring extra computational cost -- to advance both supervised and\nself-supervised learning of deep neural networks. We analyze the training\ndynamics of deep networks on training data that are corrupted by, e.g., random\nnoise and adversarial examples. Our analysis shows that model predictions are\nable to magnify useful underlying information in data and this phenomenon\noccurs broadly even in the absence of \\emph{any} label information,\nhighlighting that model predictions could substantially benefit the training\nprocess: self-adaptive training improves the generalization of deep networks\nunder noise and enhances the self-supervised representation learning. The\nanalysis also sheds light on understanding deep learning, e.g., a potential\nexplanation of the recently-discovered double-descent phenomenon in empirical\nrisk minimization and the collapsing issue of the state-of-the-art\nself-supervised learning algorithms. Experiments on the CIFAR, STL and ImageNet\ndatasets verify the effectiveness of our approach in three applications:\nclassification with label noise, selective classification and linear\nevaluation. To facilitate future research, the code has been made public\navailable at https://github.com/LayneH/self-adaptive-training.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:17:30 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Huang", "Lang", ""], ["Zhang", "Chao", ""], ["Zhang", "Hongyang", ""]]}, {"id": "2101.08757", "submitter": "Chao Li", "authors": "Chao Li, Wenjian Huang, Xi Chen, Yiran Wei, Stephen J. Price,\n  Carola-Bibiane Sch\\\"onlieb", "title": "Expectation-Maximization Regularized Deep Learning for Weakly Supervised\n  Tumor Segmentation for Glioblastoma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present an Expectation-Maximization (EM) Regularized Deep Learning\n(EMReDL) model for weakly supervised tumor segmentation. The proposed framework\nis tailored to glioblastoma, a type of malignant tumor characterized by its\ndiffuse infiltration into the surrounding brain tissue, which poses significant\nchallenge to treatment target and tumor burden estimation using conventional\nstructural MRI. Although physiological MRI provides more specific information\nregarding tumor infiltration, the relatively low resolution hinders a precise\nfull annotation. This has motivated us to develop a weakly supervised deep\nlearning solution that exploits the partial labelled tumor regions.\n  EMReDL contains two components: a physiological prior prediction model and\nEM-regularized segmentation model. The physiological prior prediction model\nexploits the physiological MRI by training a classifier to generate a\nphysiological prior map. This map is passed to the segmentation model for\nregularization using the EM algorithm. We evaluated the model on a glioblastoma\ndataset with the pre-operative multiparametric and recurrence MRI available.\nEMReDL showed to effectively segment the infiltrated tumor from the partially\nlabelled region of potential infiltration. The segmented core tumor and\ninfiltrated tumor demonstrated high consistency with the tumor burden labelled\nby experts. The performance comparisons showed that EMReDL achieved higher\naccuracy than published state-of-the-art models. On MR spectroscopy, the\nsegmented region displayed more aggressive features than other partial labelled\nregion. The proposed model can be generalized to other segmentation tasks that\nrely on partial labels, with the CNN architecture flexible in the framework.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:14:43 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 10:58:15 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 21:08:01 GMT"}, {"version": "v4", "created": "Thu, 15 Jul 2021 16:44:07 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Li", "Chao", ""], ["Huang", "Wenjian", ""], ["Chen", "Xi", ""], ["Wei", "Yiran", ""], ["Price", "Stephen J.", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2101.08779", "submitter": "Ruilong Li", "authors": "Ruilong Li, Shan Yang, David A. Ross, Angjoo Kanazawa", "title": "Learn to Dance with AIST++: Music Conditioned 3D Dance Generation", "comments": "Project page: https://google.github.io/aichoreographer/; Dataset\n  page: https://google.github.io/aistplusplus_dataset/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a transformer-based learning framework for 3D dance\ngeneration conditioned on music. We carefully design our network architecture\nand empirically study the keys for obtaining qualitatively pleasing results.\nThe critical components include a deep cross-modal transformer, which well\nlearns the correlation between the music and dance motion; and the\nfull-attention with future-N supervision mechanism which is essential in\nproducing long-range non-freezing motion. In addition, we propose a new dataset\nof paired 3D motion and music called AIST++, which we reconstruct from the AIST\nmulti-view dance videos. This dataset contains 1.1M frames of 3D dance motion\nin 1408 sequences, covering 10 genres of dance choreographies and accompanied\nwith multi-view camera parameters. To our knowledge it is the largest dataset\nof this kind. Rich experiments on AIST++ demonstrate our method produces much\nbetter results than the state-of-the-art methods both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:59:22 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 05:23:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Li", "Ruilong", ""], ["Yang", "Shan", ""], ["Ross", "David A.", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2101.08783", "submitter": "Yunpeng Gong", "authors": "Yunpeng Gong and Zhiyong Zeng and Liwen Chen and Yifan Luo and Bin\n  Weng and Feng Ye", "title": "A Person Re-identification Data Augmentation Method with Adversarial\n  Defense Effect", "comments": "arXiv admin note: text overlap with arXiv:2101.08533", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The security of the Person Re-identification(ReID) model plays a decisive\nrole in the application of ReID. However, deep neural networks have been shown\nto be vulnerable, and adding undetectable adversarial perturbations to clean\nimages can trick deep neural networks that perform well in clean images. We\npropose a ReID multi-modal data augmentation method with adversarial defense\neffect: 1) Grayscale Patch Replacement, it consists of Local Grayscale Patch\nReplacement(LGPR) and Global Grayscale Patch Replacement(GGPR). This method can\nnot only improve the accuracy of the model, but also help the model defend\nagainst adversarial examples; 2) Multi-Modal Defense, it integrates three\nhomogeneous modal images of visible, grayscale and sketch, and further\nstrengthens the defense ability of the model. These methods fuse different\nmodalities of homogeneous images to enrich the input sample variety, the\nvariaty of samples will reduce the over-fitting of the ReID model to color\nvariations and make the adversarial space of the dataset that the attack method\ncan find difficult to align, thus the accuracy of model is improved, and the\nattack effect is greatly reduced. The more modal homogeneous images are fused,\nthe stronger the defense capabilities is . The proposed method performs well on\nmultiple datasets, and successfully defends the attack of MS-SSIM proposed by\nCVPR2020 against ReID [10], and increases the accuracy by 467 times(0.2% to\n93.3%).The code is available at\nhttps://github.com/finger-monkey/ReID_Adversarial_Defense.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 12:07:57 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 07:41:10 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 13:27:38 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Gong", "Yunpeng", ""], ["Zeng", "Zhiyong", ""], ["Chen", "Liwen", ""], ["Luo", "Yifan", ""], ["Weng", "Bin", ""], ["Ye", "Feng", ""]]}, {"id": "2101.08833", "submitter": "Brendan Duke", "authors": "Brendan Duke and Abdalla Ahmed and Christian Wolf and Parham Aarabi\n  and Graham W. Taylor", "title": "SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation", "comments": "CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a Transformer-based approach to video object\nsegmentation (VOS). To address compounding error and scalability issues of\nprior work, we propose a scalable, end-to-end method for VOS called Sparse\nSpatiotemporal Transformers (SST). SST extracts per-pixel representations for\neach object in a video using sparse attention over spatiotemporal features. Our\nattention-based formulation for VOS allows a model to learn to attend over a\nhistory of multiple frames and provides suitable inductive bias for performing\ncorrespondence-like computations necessary for solving motion segmentation. We\ndemonstrate the effectiveness of attention-based over recurrent networks in the\nspatiotemporal domain. Our method achieves competitive results on YouTube-VOS\nand DAVIS 2017 with improved scalability and robustness to occlusions compared\nwith the state of the art. Code is available at\nhttps://github.com/dukebw/SSTVOS.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 20:06:12 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 00:59:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Duke", "Brendan", ""], ["Ahmed", "Abdalla", ""], ["Wolf", "Christian", ""], ["Aarabi", "Parham", ""], ["Taylor", "Graham W.", ""]]}, {"id": "2101.08845", "submitter": "Kaziwa Saleh", "authors": "Kaziwa Saleh, S\\'andor Sz\\'en\\'asi, Zolt\\'an V\\'amossy", "title": "Occlusion Handling in Generic Object Detection: A Review", "comments": "To be published in the proceedings of IEEE 19th World Symposium on\n  Applied Machine Intelligence and Informatics", "journal-ref": null, "doi": "10.1109/SAMI50585.2021.9378657", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant power of deep learning networks has led to enormous\ndevelopment in object detection. Over the last few years, object detector\nframeworks have achieved tremendous success in both accuracy and efficiency.\nHowever, their ability is far from that of human beings due to several factors,\nocclusion being one of them. Since occlusion can happen in various locations,\nscale, and ratio, it is very difficult to handle. In this paper, we address the\nchallenges in occlusion handling in generic object detection in both outdoor\nand indoor scenes, then we refer to the recent works that have been carried out\nto overcome these challenges. Finally, we discuss some possible future\ndirections of research.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 20:48:59 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Saleh", "Kaziwa", ""], ["Sz\u00e9n\u00e1si", "S\u00e1ndor", ""], ["V\u00e1mossy", "Zolt\u00e1n", ""]]}, {"id": "2101.08850", "submitter": "Wei Wang", "authors": "Shibo Zhou, Wei Wang, Xiaohua Li, Zhanpeng Jin", "title": "A Spike Learning System for Event-driven Object Recognition", "comments": "Shibo Zhou and Wei Wang contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-driven sensors such as LiDAR and dynamic vision sensor (DVS) have found\nincreased attention in high-resolution and high-speed applications. A lot of\nwork has been conducted to enhance recognition accuracy. However, the essential\ntopic of recognition delay or time efficiency is largely under-explored. In\nthis paper, we present a spiking learning system that uses the spiking neural\nnetwork (SNN) with a novel temporal coding for accurate and fast object\nrecognition. The proposed temporal coding scheme maps each event's arrival time\nand data into SNN spike time so that asynchronously-arrived events are\nprocessed immediately without delay. The scheme is integrated nicely with the\nSNN's asynchronous processing capability to enhance time efficiency. A key\nadvantage over existing systems is that the event accumulation time for each\nrecognition task is determined automatically by the system rather than pre-set\nby the user. The system can finish recognition early without waiting for all\nthe input events. Extensive experiments were conducted over a list of 7 LiDAR\nand DVS datasets. The results demonstrated that the proposed system had\nstate-of-the-art recognition accuracy while achieving remarkable time\nefficiency. Recognition delay was shown to reduce by 56.3% to 91.7% in various\nexperiment settings over the popular KITTI dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 20:57:53 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zhou", "Shibo", ""], ["Wang", "Wei", ""], ["Li", "Xiaohua", ""], ["Jin", "Zhanpeng", ""]]}, {"id": "2101.08851", "submitter": "Alban Main de Boissiere", "authors": "Alban Main de Boissiere, Rita Noumeir", "title": "Bridging the gap between Human Action Recognition and Online Action\n  Detection", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Action recognition, early prediction, and online action detection are\ncomplementary disciplines that are often studied independently. Most online\naction detection networks use a pre-trained feature extractor, which might not\nbe optimal for its new task. We address the task-specific feature extraction\nwith a teacher-student framework between the aforementioned disciplines, and a\nnovel training strategy. Our network, Online Knowledge Distillation Action\nDetection network (OKDAD), embeds online early prediction and online temporal\nsegment proposal subnetworks in parallel. Low interclass and high intraclass\nsimilarity are encouraged during teacher training. Knowledge distillation to\nthe OKDAD network is ensured via layer reuse and cosine similarity between\nteacher-student feature vectors. Layer reuse and similarity learning\nsignificantly improve our baseline which uses a generic feature extractor. We\nevaluate our framework on infrared videos from two popular datasets, NTU RGB+D\n(action recognition, early prediction) and PKU MMD (action detection). Unlike\nprevious attempts on those datasets, our student networks perform without any\nknowledge of the future. Even with this added difficulty, we achieve\nstate-of-the-art results on both datasets. Moreover, our networks use infrared\nfrom RGB-D cameras, which we are the first to use for online action detection,\nto our knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:01:46 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["de Boissiere", "Alban Main", ""], ["Noumeir", "Rita", ""]]}, {"id": "2101.08888", "submitter": "Tinu Theckel Joy", "authors": "Tinu Theckel Joy, Suman Sedai, Rahil Garnavi", "title": "Analyzing Epistemic and Aleatoric Uncertainty for Drusen Segmentation in\n  Optical Coherence Tomography Images", "comments": "Accepted in AAAI 2021 Workshop on Trustworthy AI for Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age-related macular degeneration (AMD) is one of the leading causes of\npermanent vision loss in people aged over 60 years. Accurate segmentation of\nbiomarkers such as drusen that points to the early stages of AMD is crucial in\npreventing further vision impairment. However, segmenting drusen is extremely\nchallenging due to their varied sizes and appearances, low contrast and noise\nresemblance. Most existing literature, therefore, have focused on size\nestimation of drusen using classification, leaving the challenge of accurate\nsegmentation less tackled. Additionally, obtaining the pixel-wise annotations\nis extremely costly and such labels can often be noisy, suffering from\ninter-observer and intra-observer variability. Quantification of uncertainty\nassociated with segmentation tasks offers principled measures to inspect the\nsegmentation output. Realizing its utility in identifying erroneous\nsegmentation and the potential applications in clinical decision making, here\nwe develop a U-Net based drusen segmentation model and quantify the\nsegmentation uncertainty. We investigate epistemic and aleatoric uncertainty\ncapturing model confidence and data uncertainty respectively. We present\nsegmentation results and show how uncertainty can help formulate robust\nevaluation strategies. We visually inspect the pixel-wise uncertainty and\nsegmentation results on test images. We finally analyze the correlation between\nsegmentation uncertainty and accuracy. Our results demonstrate the utility of\nleveraging uncertainties in developing and explaining segmentation models for\nmedical image analysis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:34:29 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 03:45:31 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Joy", "Tinu Theckel", ""], ["Sedai", "Suman", ""], ["Garnavi", "Rahil", ""]]}, {"id": "2101.08894", "submitter": "Chandan Gautam", "authors": "Chandan Gautam, Sethupathy Parameswaran, Ashish Mishra, Suresh\n  Sundaram", "title": "Generative Replay-based Continual Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning is a new paradigm to classify objects from classes that\nare not available at training time. Zero-shot learning (ZSL) methods have\nattracted considerable attention in recent years because of their ability to\nclassify unseen/novel class examples. Most of the existing approaches on ZSL\nworks when all the samples from seen classes are available to train the model,\nwhich does not suit real life. In this paper, we tackle this hindrance by\ndeveloping a generative replay-based continual ZSL (GRCZSL). The proposed\nmethod endows traditional ZSL to learn from streaming data and acquire new\nknowledge without forgetting the previous tasks' gained experience. We handle\ncatastrophic forgetting in GRCZSL by replaying the synthetic samples of seen\nclasses, which have appeared in the earlier tasks. These synthetic samples are\nsynthesized using the trained conditional variational autoencoder (VAE) over\nthe immediate past task. Moreover, we only require the current and immediate\nprevious VAE at any time for training and testing. The proposed GRZSL method is\ndeveloped for a single-head setting of continual learning, simulating a\nreal-world problem setting. In this setting, task identity is given during\ntraining but unavailable during testing. GRCZSL performance is evaluated on\nfive benchmark datasets for the generalized setup of ZSL with fixed and dynamic\n(incremental class) settings of continual learning. The existing class setting\npresented recently in the literature is not suitable for a class-incremental\nsetting. Therefore, this paper proposes a new setting to address this issue.\nExperimental results show that the proposed method significantly outperforms\nthe baseline and the state-of-the-art method and makes it more suitable for\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 00:03:34 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 00:44:37 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gautam", "Chandan", ""], ["Parameswaran", "Sethupathy", ""], ["Mishra", "Ashish", ""], ["Sundaram", "Suresh", ""]]}, {"id": "2101.08895", "submitter": "Gerard Kennedy", "authors": "Gerard Kennedy, Zheyu Zhuang, Xin Yu, Robert Mahony", "title": "Iterative Optimisation with an Innovation CNN for Pose Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object pose estimation from a single RGB image is a challenging problem due\nto variable lighting conditions and viewpoint changes. The most accurate pose\nestimation networks implement pose refinement via reprojection of a known,\ntextured 3D model, however, such methods cannot be applied without high quality\n3D models of the observed objects. In this work we propose an approach, namely\nan Innovation CNN, to object pose estimation refinement that overcomes the\nrequirement for reprojecting a textured 3D model. Our approach improves initial\npose estimation progressively by applying the Innovation CNN iteratively in a\nstochastic gradient descent (SGD) framework. We evaluate our method on the\npopular LINEMOD and Occlusion LINEMOD datasets and obtain state-of-the-art\nperformance on both datasets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 00:12:12 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Kennedy", "Gerard", ""], ["Zhuang", "Zheyu", ""], ["Yu", "Xin", ""], ["Mahony", "Robert", ""]]}, {"id": "2101.08910", "submitter": "Heng Wang", "authors": "Heng Wang, Yang Song, Chaoyi Zhang, Jianhui Yu, Siqi Liu, Hanchuan\n  Peng, Weidong Cai", "title": "Single Neuron Segmentation using Graph-based Global Reasoning with\n  Auxiliary Skeleton Loss from 3D Optical Microscope Images", "comments": "5 pages, 3 figures, 2 tables, ISBI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the critical steps in improving accurate single neuron reconstruction\nfrom three-dimensional (3D) optical microscope images is the neuronal structure\nsegmentation. However, they are always hard to segment due to the lack in\nquality. Despite a series of attempts to apply convolutional neural networks\n(CNNs) on this task, noise and disconnected gaps are still challenging to\nalleviate with the neglect of the non-local features of graph-like tubular\nneural structures. Hence, we present an end-to-end segmentation network by\njointly considering the local appearance and the global geometry traits through\ngraph reasoning and a skeleton-based auxiliary loss. The evaluation results on\nthe Janelia dataset from the BigNeuron project demonstrate that our proposed\nmethod exceeds the counterpart algorithms in performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 01:27:14 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Wang", "Heng", ""], ["Song", "Yang", ""], ["Zhang", "Chaoyi", ""], ["Yu", "Jianhui", ""], ["Liu", "Siqi", ""], ["Peng", "Hanchuan", ""], ["Cai", "Weidong", ""]]}, {"id": "2101.08923", "submitter": "Lizhi Wang", "authors": "Niankai Cheng, Hua Huang, Lei Zhang, and Lizhi Wang", "title": "Snapshot Hyperspectral Imaging Based on Weighted High-order Singular\n  Value Regularization", "comments": "This paper has been accepted by IEEE ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Snapshot hyperspectral imaging can capture the 3D hyperspectral image (HSI)\nwith a single 2D measurement and has attracted increasing attention recently.\nRecovering the underlying HSI from the compressive measurement is an ill-posed\nproblem and exploiting the image prior is essential for solving this ill-posed\nproblem. However, existing reconstruction methods always start from modeling\nimage prior with the 1D vector or 2D matrix and cannot fully exploit the\nstructurally spectral-spatial nature in 3D HSI, thus leading to a poor\nfidelity. In this paper, we propose an effective high-order tensor optimization\nbased method to boost the reconstruction fidelity for snapshot hyperspectral\nimaging. We first build high-order tensors by exploiting the spatial-spectral\ncorrelation in HSI. Then, we propose a weight high-order singular value\nregularization (WHOSVR) based low-rank tensor recovery model to characterize\nthe structure prior of HSI. By integrating the structure prior in WHOSVR with\nthe system imaging process, we develop an optimization framework for HSI\nreconstruction, which is finally solved via the alternating minimization\nalgorithm. Extensive experiments implemented on two representative systems\ndemonstrate that our method outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 02:54:55 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Cheng", "Niankai", ""], ["Huang", "Hua", ""], ["Zhang", "Lei", ""], ["Wang", "Lizhi", ""]]}, {"id": "2101.08926", "submitter": "Shuai Li", "authors": "Chuankun Li, Shuai Li, Yanbo Gao, Xiang Zhang, Wanqing Li", "title": "A Two-stream Neural Network for Pose-based Hand Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pose based hand gesture recognition has been widely studied in the recent\nyears. Compared with full body action recognition, hand gesture involves joints\nthat are more spatially closely distributed with stronger collaboration. This\nnature requires a different approach from action recognition to capturing the\ncomplex spatial features. Many gesture categories, such as \"Grab\" and \"Pinch\",\nhave very similar motion or temporal patterns posing a challenge on temporal\nprocessing. To address these challenges, this paper proposes a two-stream\nneural network with one stream being a self-attention based graph convolutional\nnetwork (SAGCN) extracting the short-term temporal information and hierarchical\nspatial information, and the other being a residual-connection enhanced\nbidirectional Independently Recurrent Neural Network (RBi-IndRNN) for\nextracting long-term temporal information. The self-attention based graph\nconvolutional network has a dynamic self-attention mechanism to adaptively\nexploit the relationships of all hand joints in addition to the fixed topology\nand local feature extraction in the GCN. On the other hand, the\nresidual-connection enhanced Bi-IndRNN extends an IndRNN with the capability of\nbidirectional processing for temporal modelling. The two streams are fused\ntogether for recognition. The Dynamic Hand Gesture dataset and First-Person\nHand Action dataset are used to validate its effectiveness, and our method\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 03:22:26 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Li", "Chuankun", ""], ["Li", "Shuai", ""], ["Gao", "Yanbo", ""], ["Zhang", "Xiang", ""], ["Li", "Wanqing", ""]]}, {"id": "2101.08934", "submitter": "Hengrong Lan", "authors": "Mengjie Guo, Hengrong Lan, Changchun Yang, and Fei Gao", "title": "AS-Net: Fast Photoacoustic Reconstruction with Multi-feature Fusion from\n  Sparse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoacoustic (PA) imaging is a biomedical imaging modality capable of\nacquiring high-contrast images of optical absorption at depths much greater\nthan traditional optical imaging techniques. However, practical instrumentation\nand geometry limit the number of available acoustic sensors surrounding the\nimaging target, which results in the sparsity of sensor data. Conventional PA\nimage reconstruction methods give severe artifacts when they are applied\ndirectly to the sparse PA data. In this paper, we firstly propose to employ a\nnovel signal processing method to make sparse PA raw data more suitable for the\nneural network, concurrently speeding up image reconstruction. Then we propose\nAttention Steered Network (AS-Net) for PA reconstruction with multi-feature\nfusion. AS-Net is validated on different datasets, including simulated\nphotoacoustic data from fundus vasculature phantoms and experimental data from\nin vivo fish and mice. Notably, the method is also able to eliminate some\nartifacts present in the ground truth for in vivo data. Results demonstrated\nthat our method provides superior reconstructions at a faster speed.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 03:49:30 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 01:55:16 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Guo", "Mengjie", ""], ["Lan", "Hengrong", ""], ["Yang", "Changchun", ""], ["Gao", "Fei", ""]]}, {"id": "2101.08940", "submitter": "Amir Gholami", "authors": "Shixing Yu, Zhewei Yao, Amir Gholami, Zhen Dong, Sehoon Kim, Michael W\n  Mahoney, Kurt Keutzer", "title": "Hessian-Aware Pruning and Optimal Neural Implant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning is an effective method to reduce the memory footprint and FLOPs\nassociated with neural network models. However, existing structured-pruning\nmethods often result in significant accuracy degradation for moderate pruning\nlevels. To address this problem, we introduce a new Hessian Aware Pruning (HAP)\nmethod coupled with a Neural Implant approach that uses second-order\nsensitivity as a metric for structured pruning. The basic idea is to prune\ninsensitive components and to use a Neural Implant for moderately sensitive\ncomponents, instead of completely pruning them. For the latter approach, the\nmoderately sensitive components are replaced with with a low rank implant that\nis smaller and less computationally expensive than the original component. We\nuse the relative Hessian trace to measure sensitivity, as opposed to the\nmagnitude based sensitivity metric commonly used in the literature. We test HAP\nfor both computer vision tasks and natural language tasks, and we achieve new\nstate-of-the-art results. Specifically, HAP achieves less than $0.1\\%$/$0.5\\%$\ndegradation on PreResNet29/ResNet50 (CIFAR-10/ImageNet) with more than\n70\\%/50\\% of parameters pruned. Meanwhile, HAP also achieves significantly\nbetter performance (up to 0.8\\% with 60\\% of parameters pruned) as compared to\ngradient based method for head pruning on transformer-based models. The\nframework has been open sourced and available online.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 04:08:03 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 21:11:04 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 21:41:39 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Yu", "Shixing", ""], ["Yao", "Zhewei", ""], ["Gholami", "Amir", ""], ["Dong", "Zhen", ""], ["Kim", "Sehoon", ""], ["Mahoney", "Michael W", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2101.08967", "submitter": "Dong-Gyu Lee Ph.D.", "authors": "Dong-Gyu Lee, Seong-Whan Lee", "title": "Human Interaction Recognition Framework based on Interacting Body Part\n  Attention", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition in videos has been widely studied and has recently\ngained significant advances with deep learning approaches; however, it remains\na challenging task. In this paper, we propose a novel framework that\nsimultaneously considers both implicit and explicit representations of human\ninteractions by fusing information of local image where the interaction\nactively occurred, primitive motion with the posture of individual subject's\nbody parts, and the co-occurrence of overall appearance change. Human\ninteractions change, depending on how the body parts of each human interact\nwith the other. The proposed method captures the subtle difference between\ndifferent interactions using interacting body part attention. Semantically\nimportant body parts that interact with other objects are given more weight\nduring feature representation. The combined feature of interacting body part\nattention-based individual representation and the co-occurrence descriptor of\nthe full-body appearance change is fed into long short-term memory to model the\ntemporal dynamics over time in a single framework. We validate the\neffectiveness of the proposed method using four widely used public datasets by\noutperforming the competing state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 06:52:42 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lee", "Dong-Gyu", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2101.08978", "submitter": "Dong-Gyu Lee Ph.D.", "authors": "Jung-Jun Kim, Dong-Gyu Lee, Jialin Wu, Hong-Gyu Jung, Seong-Whan Lee", "title": "Visual Question Answering based on Local-Scene-Aware Referring\n  Expression Generation", "comments": "32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering requires a deep understanding of both images and\nnatural language. However, most methods mainly focus on visual concept; such as\nthe relationships between various objects. The limited use of object categories\ncombined with their relationships or simple question embedding is insufficient\nfor representing complex scenes and explaining decisions. To address this\nlimitation, we propose the use of text expressions generated for images,\nbecause such expressions have few structural constraints and can provide richer\ndescriptions of images. The generated expressions can be incorporated with\nvisual features and question embedding to obtain the question-relevant answer.\nA joint-embedding multi-head attention network is also proposed to model three\ndifferent information modalities with co-attention. We quantitatively and\nqualitatively evaluated the proposed method on the VQA v2 dataset and compared\nit with state-of-the-art methods in terms of answer prediction. The quality of\nthe generated expressions was also evaluated on the RefCOCO, RefCOCO+, and\nRefCOCOg datasets. Experimental results demonstrate the effectiveness of the\nproposed method and reveal that it outperformed all of the competing methods in\nterms of both quantitative and qualitative results.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 07:28:28 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Kim", "Jung-Jun", ""], ["Lee", "Dong-Gyu", ""], ["Wu", "Jialin", ""], ["Jung", "Hong-Gyu", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2101.08987", "submitter": "Seobin Park", "authors": "Seobin Park and Tae Hyun Kim", "title": "Image Restoration by Solving IVP", "comments": "Revision on the abstract and main text; Remove first figure, table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research on image restoration have achieved great success with the aid\nof deep learning technologies, but, many of them are limited to dealing SR with\nrealistic settings. To alleviate this problem, we introduce a new formulation\nfor image super-resolution to solve arbitrary scale image super-resolution\nmethods. Based on the proposed new SR formulation, we can not only\nsuper-resolve images with multiple scales, but also find a new way to analyze\nthe performance of super-resolving process. We demonstrate that the proposed\nmethod can generate high-quality images unlike conventional SR methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 07:59:48 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 11:13:53 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 03:33:12 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Park", "Seobin", ""], ["Kim", "Tae Hyun", ""]]}, {"id": "2101.08992", "submitter": "Gangming Zhao", "authors": "Gangming Zhao, Baolian Qi, Jinpeng Li", "title": "Cross Chest Graph for Disease Diagnosis with Structural Relational\n  Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Locating lesions is important in the computer-aided diagnosis of X-ray\nimages. However, box-level annotation is time-consuming and laborious. How to\nlocate lesions accurately with few, or even without careful annotations is an\nurgent problem. Although several works have approached this problem with\nweakly-supervised methods, the performance needs to be improved. One obstacle\nis that general weakly-supervised methods have failed to consider the\ncharacteristics of X-ray images, such as the highly-structural attribute. We\ntherefore propose the Cross-chest Graph (CCG), which improves the performance\nof automatic lesion detection by imitating doctor's training and\ndecision-making process. CCG models the intra-image relationship between\ndifferent anatomical areas by leveraging the structural information to simulate\nthe doctor's habit of observing different areas. Meanwhile, the relationship\nbetween any pair of images is modeled by a knowledge-reasoning module to\nsimulate the doctor's habit of comparing multiple images. We integrate\nintra-image and inter-image information into a unified end-to-end framework.\nExperimental results on the NIH Chest-14 database (112,120 frontal-view X-ray\nimages with 14 diseases) demonstrate that the proposed method achieves\nstate-of-the-art performance in weakly-supervised localization of lesions by\nabsorbing professional knowledge in the medical field.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 08:24:04 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 06:51:14 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhao", "Gangming", ""], ["Qi", "Baolian", ""], ["Li", "Jinpeng", ""]]}, {"id": "2101.08993", "submitter": "Vivian Wen Hui Wong", "authors": "Vivian Wen Hui Wong, Max Ferguson, Kincho H. Law, Yung-Tsun Tina Lee,\n  Paul Witherell", "title": "Automatic Volumetric Segmentation of Additive Manufacturing Defects with\n  3D U-Net", "comments": "Accepted by AAAI 2020 Spring Symposia", "journal-ref": "AAAI 2020 Spring Symposia, Stanford, CA, USA, Mar 23-25, 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of additive manufacturing (AM) defects in X-ray Computed\nTomography (XCT) images is challenging, due to the poor contrast, small sizes\nand variation in appearance of defects. Automatic segmentation can, however,\nprovide quality control for additive manufacturing. Over recent years,\nthree-dimensional convolutional neural networks (3D CNNs) have performed well\nin the volumetric segmentation of medical images. In this work, we leverage\ntechniques from the medical imaging domain and propose training a 3D U-Net\nmodel to automatically segment defects in XCT images of AM samples. This work\nnot only contributes to the use of machine learning for AM defect detection but\nalso demonstrates for the first time 3D volumetric segmentation in AM. We train\nand test with three variants of the 3D U-Net on an AM dataset, achieving a mean\nintersection of union (IOU) value of 88.4%.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 08:24:54 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Wong", "Vivian Wen Hui", ""], ["Ferguson", "Max", ""], ["Law", "Kincho H.", ""], ["Lee", "Yung-Tsun Tina", ""], ["Witherell", "Paul", ""]]}, {"id": "2101.09014", "submitter": "Gongyang Li", "authors": "Gongyang Li and Zhi Liu and Ran Shi and Zheng Hu and Weijie Wei and\n  Yong Wu and Mengke Huang and Haibin Ling", "title": "Personal Fixations-Based Object Segmentation with Object Localization\n  and Boundary Preservation", "comments": "Accepted by IEEE TIP. Code: https://github.com/MathLee/OLBPNet4PFOS", "journal-ref": null, "doi": "10.1109/TIP.2020.3044440", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a natural way for human-computer interaction, fixation provides a\npromising solution for interactive image segmentation. In this paper, we focus\non Personal Fixations-based Object Segmentation (PFOS) to address issues in\nprevious studies, such as the lack of appropriate dataset and the ambiguity in\nfixations-based interaction. In particular, we first construct a new PFOS\ndataset by carefully collecting pixel-level binary annotation data over an\nexisting fixation prediction dataset, such dataset is expected to greatly\nfacilitate the study along the line. Then, considering characteristics of\npersonal fixations, we propose a novel network based on Object Localization and\nBoundary Preservation (OLBP) to segment the gazed objects. Specifically, the\nOLBP network utilizes an Object Localization Module (OLM) to analyze personal\nfixations and locates the gazed objects based on the interpretation. Then, a\nBoundary Preservation Module (BPM) is designed to introduce additional boundary\ninformation to guard the completeness of the gazed objects. Moreover, OLBP is\norganized in the mixed bottom-up and top-down manner with multiple types of\ndeep supervision. Extensive experiments on the constructed PFOS dataset show\nthe superiority of the proposed OLBP network over 17 state-of-the-art methods,\nand demonstrate the effectiveness of the proposed OLM and BPM components. The\nconstructed PFOS dataset and the proposed OLBP network are available at\nhttps://github.com/MathLee/OLBPNet4PFOS.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:20:47 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Li", "Gongyang", ""], ["Liu", "Zhi", ""], ["Shi", "Ran", ""], ["Hu", "Zheng", ""], ["Wei", "Weijie", ""], ["Wu", "Yong", ""], ["Huang", "Mengke", ""], ["Ling", "Haibin", ""]]}, {"id": "2101.09021", "submitter": "Hoang Trinh Man", "authors": "Trinh Man Hoang, Jinjia Zhou", "title": "B-DRRN: A Block Information Constrained Deep Recursive Residual Network\n  for Video Compression Artifacts Reduction", "comments": null, "journal-ref": "2019 Picture Coding Symposium (PCS), Ningbo, China, 2019, pp. 1-5", "doi": "10.1109/PCS48520.2019.8954521", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the video compression ratio nowadays becomes higher, the video\ncoders such as H.264/AVC, H.265/HEVC, H.266/VVC always suffer from the video\nartifacts. In this paper, we design a neural network to enhance the quality of\nthe compressed frame by leveraging the block information, called B-DRRN (Deep\nRecursive Residual Network with Block information). Firstly, an extra network\nbranch is designed for leveraging the block information of the coding unit\n(CU). Moreover, to avoid a great increase in the network size, Recursive\nResidual structure and sharing weight techniques are applied. We also conduct a\nnew large-scale dataset with 209,152 training samples. Experimental results\nshow that the proposed B-DRRN can reduce 6.16% BD-rate compared to the HEVC\nstandard. After efficiently adding an extra network branch, this work can\nimprove the performance of the main network without increasing any memory for\nstoring.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:35:44 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 05:52:08 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hoang", "Trinh Man", ""], ["Zhou", "Jinjia", ""]]}, {"id": "2101.09028", "submitter": "Yitian Yuan", "authors": "Yitian Yuan, Xiaohan Lan, Long Chen, Wei Liu, Xin Wang, Wenwu Zhu", "title": "A Closer Look at Temporal Sentence Grounding in Videos: Datasets and\n  Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite Temporal Sentence Grounding in Videos (TSGV) has realized impressive\nprogress over the last few years, current TSGV models tend to capture the\nmoment annotation biases and fail to take full advantage of multi-modal inputs.\nMiraculously, some extremely simple TSGV baselines even without training can\nalso achieve state-of-the-art performance. In this paper, we first take a\ncloser look at the existing evaluation protocol, and argue that both the\nprevailing datasets and metrics are the devils to cause the unreliable\nbenchmarking. To this end, we propose to re-organize two widely-used TSGV\ndatasets (Charades-STA and ActivityNet Captions), and deliberately\n\\textbf{C}hange the moment annotation \\textbf{D}istribution of the test split\nto make it different from the training split, dubbed as Charades-CD and\nActivityNet-CD, respectively. Meanwhile, we further introduce a new evaluation\nmetric \"dR@$n$,IoU@$m$\" to calibrate the basic IoU scores by penalizing more on\nthe over-long moment predictions and reduce the inflating performance caused by\nthe moment annotation biases. Under this new evaluation protocol, we conduct\nextensive experiments and ablation studies on eight state-of-the-art TSGV\nmodels. All the results demonstrate that the re-organized datasets and new\nmetric can better monitor the progress in TSGV, which is still far from\nsatisfactory. The repository of this work is at\n\\url{https://github.com/yytzsy/grounding_changing_distribution}.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:59:30 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 07:19:07 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yuan", "Yitian", ""], ["Lan", "Xiaohan", ""], ["Chen", "Long", ""], ["Liu", "Wei", ""], ["Wang", "Xin", ""], ["Zhu", "Wenwu", ""]]}, {"id": "2101.09057", "submitter": "Ziyuan Zhao", "authors": "Ziyuan Zhao, Zeng Zeng, Kaixin Xu, Cen Chen, Cuntai Guan", "title": "DSAL: Deeply Supervised Active Learning from Strong and Weak Labelers\n  for Biomedical Image Segmentation", "comments": "Published as a journal paper at IEEE J-BHI", "journal-ref": null, "doi": "10.1109/JBHI.2021.3052320", "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is one of the most essential biomedical image processing\nproblems for different imaging modalities, including microscopy and X-ray in\nthe Internet-of-Medical-Things (IoMT) domain. However, annotating biomedical\nimages is knowledge-driven, time-consuming, and labor-intensive, making it\ndifficult to obtain abundant labels with limited costs. Active learning\nstrategies come into ease the burden of human annotation, which queries only a\nsubset of training data for annotation. Despite receiving attention, most of\nactive learning methods generally still require huge computational costs and\nutilize unlabeled data inefficiently. They also tend to ignore the intermediate\nknowledge within networks. In this work, we propose a deep active\nsemi-supervised learning framework, DSAL, combining active learning and\nsemi-supervised learning strategies. In DSAL, a new criterion based on deep\nsupervision mechanism is proposed to select informative samples with high\nuncertainties and low uncertainties for strong labelers and weak labelers\nrespectively. The internal criterion leverages the disagreement of intermediate\nfeatures within the deep learning network for active sample selection, which\nsubsequently reduces the computational costs. We use the proposed criteria to\nselect samples for strong and weak labelers to produce oracle labels and pseudo\nlabels simultaneously at each active learning iteration in an ensemble learning\nmanner, which can be examined with IoMT Platform. Extensive experiments on\nmultiple medical image datasets demonstrate the superiority of the proposed\nmethod over state-of-the-art active learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:31:33 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zhao", "Ziyuan", ""], ["Zeng", "Zeng", ""], ["Xu", "Kaixin", ""], ["Chen", "Cen", ""], ["Guan", "Cuntai", ""]]}, {"id": "2101.09060", "submitter": "Francesco Cappio Borlino", "authors": "Francesco Cappio Borlino, Antonio D'Innocente, Tatiana Tommasi", "title": "Rethinking Domain Generalization Baselines", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being very powerful in standard learning settings, deep learning\nmodels can be extremely brittle when deployed in scenarios different from those\non which they were trained. Domain generalization methods investigate this\nproblem and data augmentation strategies have shown to be helpful tools to\nincrease data variability, supporting model robustness across domains. In our\nwork we focus on style transfer data augmentation and we present how it can be\nimplemented with a simple and inexpensive strategy to improve generalization.\nMoreover, we analyze the behavior of current state of the art domain\ngeneralization methods when integrated with this augmentation solution: our\nthorough experimental evaluation shows that their original effect almost always\ndisappears with respect to the augmented baseline. This issue open new\nscenarios for domain generalization research, highlighting the need of novel\nmethods properly able to take advantage of the introduced data variability.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:35:58 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 10:41:53 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Borlino", "Francesco Cappio", ""], ["D'Innocente", "Antonio", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "2101.09095", "submitter": "Henghui Ding", "authors": "Chang Liu, Henghui Ding, Xudong Jiang", "title": "Towards Enhancing Fine-grained Details for Image Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep natural image matting has been rapidly evolved by\nextracting high-level contextual features into the model. However, most current\nmethods still have difficulties with handling tiny details, like hairs or furs.\nIn this paper, we argue that recovering these microscopic details relies on\nlow-level but high-definition texture features. However, {these features are\ndownsampled in a very early stage in current encoder-decoder-based models,\nresulting in the loss of microscopic details}. To address this issue, we design\na deep image matting model {to enhance fine-grained details. Our model consists\nof} two parallel paths: a conventional encoder-decoder Semantic Path and an\nindependent downsampling-free Textural Compensate Path (TCP). The TCP is\nproposed to extract fine-grained details such as lines and edges in the\noriginal image size, which greatly enhances the fineness of prediction.\nMeanwhile, to leverage the benefits of high-level context, we propose a feature\nfusion unit(FFU) to fuse multi-scale features from the semantic path and inject\nthem into the TCP. In addition, we have observed that poorly annotated trimaps\nseverely affect the performance of the model. Thus we further propose a novel\nterm in loss function and a trimap generation method to improve our model's\nrobustness to the trimaps. The experiments show that our method outperforms\nprevious start-of-the-art methods on the Composition-1k dataset.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 13:20:23 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Liu", "Chang", ""], ["Ding", "Henghui", ""], ["Jiang", "Xudong", ""]]}, {"id": "2101.09116", "submitter": "Yu Chen", "authors": "Yu Chen and Ji Zhao and Laurent Kneip", "title": "Hybrid Rotation Averaging: A Fast and Robust Rotation Averaging Approach", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address rotation averaging (RA) and its application to real-world 3D\nreconstruction. Local optimisation based approaches are the de facto choice,\nthough they only guarantee a local optimum. Global optimisers ensure global\noptimality in low noise conditions, but they are inefficient and may easily\ndeviate under the influence of outliers or elevated noise levels. We push the\nenvelope of rotation averaging by leveraging the advantages of a global RA\nmethod and a local RA method. Combined with a fast view graph filtering as\npreprocessing, the proposed hybrid approach is robust to outliers. We further\napply the proposed hybrid rotation averaging approach to incremental Structure\nfrom Motion (SfM), the accuracy and robustness of SfM are both improved by\nadding the resulting global rotations as regularisers to bundle adjustment.\nOverall, we demonstrate high practicality of the proposed method as bad camera\nposes are effectively corrected and drift is reduced.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:11:19 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 11:50:58 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 13:01:35 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Yu", ""], ["Zhao", "Ji", ""], ["Kneip", "Laurent", ""]]}, {"id": "2101.09122", "submitter": "Simone Cammarasana Ing", "authors": "Simone Cammarasana, Paolo Nicolardi, Giuseppe Patan\\`e", "title": "A Universal Deep Learning Framework for Real-Time Denoising of\n  Ultrasound Images", "comments": "21 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound images are widespread in medical diagnosis for muscle-skeletal,\ncardiac, and obstetrical diseases, due to the efficiency and non-invasiveness\nof the acquisition methodology. However, ultrasound acquisition introduces a\nspeckle noise in the signal, that corrupts the resulting image and affects\nfurther processing operations, and the visual analysis that medical experts\nconduct to estimate patient diseases. Our main goal is to define a universal\ndeep learning framework for real-time denoising of ultrasound images. We\nanalyse and compare state-of-the-art methods for the smoothing of ultrasound\nimages (e.g., spectral, low-rank, and deep learning denoising algorithms), in\norder to select the best one in terms of accuracy, preservation of anatomical\nfeatures, and computational cost. Then, we propose a tuned version of the\nselected state-of-the-art denoising methods (e.g., WNNM), to improve the\nquality of the denoised images, and extend its applicability to ultrasound\nimages. To handle large data sets of ultrasound images with respect to\napplications and industrial requirements, we introduce a denoising framework\nthat exploits deep learning and HPC tools, and allows us to replicate the\nresults of state-of-the-art denoising methods in a real-time execution.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:18:47 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Cammarasana", "Simone", ""], ["Nicolardi", "Paolo", ""], ["Patan\u00e8", "Giuseppe", ""]]}, {"id": "2101.09129", "submitter": "Nicola Messina", "authors": "Nicola Messina, Giuseppe Amato, Fabio Carrara, Claudio Gennaro,\n  Fabrizio Falchi", "title": "Solving the Same-Different Task with Convolutional Neural Networks", "comments": "Preprint of the paper published in Patter Recognition Letters\n  (Elsevier)", "journal-ref": "Pattern Recognition Letters, Volume 143, March 2021, Pages 75-80", "doi": "10.1016/j.patrec.2020.12.019", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning demonstrated major abilities in solving many kinds of different\nreal-world problems in computer vision literature. However, they are still\nstrained by simple reasoning tasks that humans consider easy to solve. In this\nwork, we probe current state-of-the-art convolutional neural networks on a\ndifficult set of tasks known as the same-different problems. All the problems\nrequire the same prerequisite to be solved correctly: understanding if two\nrandom shapes inside the same image are the same or not. With the experiments\ncarried out in this work, we demonstrate that residual connections, and more\ngenerally the skip connections, seem to have only a marginal impact on the\nlearning of the proposed problems. In particular, we experiment with DenseNets,\nand we examine the contribution of residual and recurrent connections in\nalready tested architectures, ResNet-18, and CorNet-S respectively. Our\nexperiments show that older feed-forward networks, AlexNet and VGG, are almost\nunable to learn the proposed problems, except in some specific scenarios. We\nshow that recently introduced architectures can converge even in the cases\nwhere the important parts of their architecture are removed. We finally carry\nout some zero-shot generalization tests, and we discover that in these\nscenarios residual and recurrent connections can have a stronger impact on the\noverall test accuracy. On four difficult problems from the SVRT dataset, we can\nreach state-of-the-art results with respect to the previous approaches,\nobtaining super-human performances on three of the four problems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:35:33 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Messina", "Nicola", ""], ["Amato", "Giuseppe", ""], ["Carrara", "Fabio", ""], ["Gennaro", "Claudio", ""], ["Falchi", "Fabrizio", ""]]}, {"id": "2101.09154", "submitter": "Lukas Winiwarter", "authors": "Lukas Winiwarter, Alberto Manuel Esmor\\'is Pena, Hannah Weiser,\n  Katharina Anders, Jorge Mart\\'inez Sanchez, Mark Searle, Bernhard H\\\"ofle", "title": "Virtual laser scanning with HELIOS++: A novel take on ray tracing-based\n  simulation of topographic 3D laser scanning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topographic laser scanning is a remote sensing method to create detailed 3D\npoint cloud representations of the Earth's surface. Since data acquisition is\nexpensive, simulations can complement real data given certain premises are\navailable: i) a model of 3D scene and scanner, ii) a model of the beam-scene\ninteraction, simplified to a computationally feasible while physically\nrealistic level, and iii) an application for which simulated data is fit for\nuse. A number of laser scanning simulators for different purposes exist, which\nwe enrich by presenting HELIOS++. HELIOS++ is an open-source simulation\nframework for terrestrial static, mobile, UAV-based and airborne laser scanning\nimplemented in C++. The HELIOS++ concept provides a flexible solution for the\ntrade-off between physical accuracy (realism) and computational complexity\n(runtime, memory footprint), as well as ease of use and of configuration.\nUnique features of HELIOS++ include the availability of Python bindings\n(pyhelios) for controlling simulations, and a range of model types for 3D scene\nrepresentation. HELIOS++ further allows the simulation of beam divergence using\na subsampling strategy, and is able to create full-waveform outputs as a basis\nfor detailed analysis. As generation and analysis of waveforms can strongly\nimpact runtimes, the user may set the level of detail for the subsampling, or\noptionally disable full-waveform output altogether. A detailed assessment of\ncomputational considerations and a comparison of HELIOS++ to its predecessor,\nHELIOS, reveal reduced runtimes by up to 83 %. At the same time, memory\nrequirements are reduced by up to 94 %, allowing for much larger (i.e. more\ncomplex) 3D scenes to be loaded into memory and hence to be virtually acquired\nby laser scanning simulation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:39:38 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Winiwarter", "Lukas", ""], ["Pena", "Alberto Manuel Esmor\u00eds", ""], ["Weiser", "Hannah", ""], ["Anders", "Katharina", ""], ["Sanchez", "Jorge Mart\u00ednez", ""], ["Searle", "Mark", ""], ["H\u00f6fle", "Bernhard", ""]]}, {"id": "2101.09176", "submitter": "Luis Leiva", "authors": "Luis A. Leiva, Yunfei Xue, Avya Bansal, Hamed R. Tavakoli,\n  Tu\\u{g}\\c{c}e K\\\"oro\\u{g}lu, Niraj R. Dayama, Antti Oulasvirta", "title": "Understanding Visual Saliency in Mobile User Interfaces", "comments": null, "journal-ref": "Proceedings of the 22nd Intl. Conf. on Human-Computer Interaction\n  with Mobile Devices and Services (MobileHCI), 2020", "doi": "10.1145/3379503.3403557", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For graphical user interface (UI) design, it is important to understand what\nattracts visual attention. While previous work on saliency has focused on\ndesktop and web-based UIs, mobile app UIs differ from these in several\nrespects. We present findings from a controlled study with 30 participants and\n193 mobile UIs. The results speak to a role of expectations in guiding where\nusers look at. Strong bias toward the top-left corner of the display, text, and\nimages was evident, while bottom-up features such as color or size affected\nsaliency less. Classic, parameter-free saliency models showed a weak fit with\nthe data, and data-driven models improved significantly when trained\nspecifically on this dataset (e.g., NSS rose from 0.66 to 0.84). We also\nrelease the first annotated dataset for investigating visual saliency in mobile\nUIs.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:45:13 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Leiva", "Luis A.", ""], ["Xue", "Yunfei", ""], ["Bansal", "Avya", ""], ["Tavakoli", "Hamed R.", ""], ["K\u00f6ro\u011flu", "Tu\u011f\u00e7e", ""], ["Dayama", "Niraj R.", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "2101.09193", "submitter": "Petra Bevandi\\'c", "authors": "Petra Bevandi\\'c, Ivan Kre\\v{s}o, Marin Or\\v{s}i\\'c, Sini\\v{s}a\n  \\v{S}egvi\\'c", "title": "Dense outlier detection and open-set recognition based on training with\n  noisy negative images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional models often produce inadequate predictions for inputs\nforeign to the training distribution. Consequently, the problem of detecting\noutlier images has recently been receiving a lot of attention. Unlike most\nprevious work, we address this problem in the dense prediction context in order\nto be able to locate outlier objects in front of in-distribution background.\nOur approach is based on two reasonable assumptions. First, we assume that the\ninlier dataset is related to some narrow application field (e.g.~road driving).\nSecond, we assume that there exists a general-purpose dataset which is much\nmore diverse than the inlier dataset (e.g.~ImageNet-1k). We consider pixels\nfrom the general-purpose dataset as noisy negative training samples since most\n(but not all) of them are outliers. We encourage the model to recognize borders\nbetween known and unknown by pasting jittered negative patches over inlier\ntraining images. Our experiments target two dense open-set recognition\nbenchmarks (WildDash 1 and Fishyscapes) and one dense open-set recognition\ndataset (StreetHazard). Extensive performance evaluation indicates competitive\npotential of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:31:36 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Bevandi\u0107", "Petra", ""], ["Kre\u0161o", "Ivan", ""], ["Or\u0161i\u0107", "Marin", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "2101.09231", "submitter": "Fabio Valerio Massoli", "authors": "Donato Cafarelli, Fabio Valerio Massoli, Fabrizio Falchi, Claudio\n  Gennaro, Giuseppe Amato", "title": "Expression Recognition Analysis in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Facial Expression Recognition(FER) is one of the most important topic in\nHuman-Computer interactions(HCI). In this work we report details and\nexperimental results about a facial expression recognition method based on\nstate-of-the-art methods. We fine-tuned a SeNet deep learning architecture\npre-trained on the well-known VGGFace2 dataset, on the AffWild2 facial\nexpression recognition dataset. The main goal of this work is to define a\nbaseline for a novel method we are going to propose in the near future. This\npaper is also required by the Affective Behavior Analysis in-the-wild (ABAW)\ncompetition in order to evaluate on the test set this approach. The results\nreported here are on the validation set and are related on the Expression\nChallenge part (seven basic emotion recognition) of the competition. We will\nupdate them as soon as the actual results on the test set will be published on\nthe leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 17:28:31 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Cafarelli", "Donato", ""], ["Massoli", "Fabio Valerio", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Amato", "Giuseppe", ""]]}, {"id": "2101.09253", "submitter": "Vera De Vos", "authors": "V. de Vos, K.M. Timmins, I.C. van der Schaaf, Y. Ruigrok, B.K.\n  Velthuis, H.J. Kuijf", "title": "Automatic Cerebral Vessel Extraction in TOF-MRA Using Deep Learning", "comments": "Preprint for the SPIE Medical Imaging, Image Processing Conference\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning approaches may help radiologists in the early diagnosis and\ntimely treatment of cerebrovascular diseases. Accurate cerebral vessel\nsegmentation of Time-of-Flight Magnetic Resonance Angiographs (TOF-MRAs) is an\nessential step in this process. This study investigates deep learning\napproaches for automatic, fast and accurate cerebrovascular segmentation for\nTOF-MRAs. The performance of several data augmentation and selection methods\nfor training a 2D and 3D U-Net for vessel segmentation was investigated in five\nexperiments: a) without augmentation, b) Gaussian blur, c) rotation and\nflipping, d) Gaussian blur, rotation and flipping and e) different input patch\nsizes. All experiments were performed by patch-training both a 2D and 3D U-Net\nand predicted on a test set of MRAs. Ground truth was manually defined using an\ninteractive threshold and region growing method. The performance was evaluated\nusing the Dice Similarity Coefficient (DSC), Modified Hausdorff Distance and\nVolumetric Similarity, between the predicted images and the interactively\ndefined ground truth. The segmentation performance of all trained networks on\nthe test set was found to be good, with DSC scores ranging from 0.72 to 0.83.\nBoth the 2D and 3D U-Net had the best segmentation performance with Gaussian\nblur, rotation and flipping compared to other experiments without augmentation\nor only one of those augmentation techniques. Additionally, training on larger\npatches or slices gave optimal segmentation results. In conclusion, vessel\nsegmentation can be optimally performed on TOF-MRAs using a trained 3D U-Net on\nlarger patches, where data augmentation including Gaussian blur, rotation and\nflipping was performed on the training data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 18:11:34 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["de Vos", "V.", ""], ["Timmins", "K. M.", ""], ["van der Schaaf", "I. C.", ""], ["Ruigrok", "Y.", ""], ["Velthuis", "B. K.", ""], ["Kuijf", "H. J.", ""]]}, {"id": "2101.09318", "submitter": "F. Patricia Medina", "authors": "F. Patricia Medina, Randy Paffenroth", "title": "Machine Learning in LiDAR 3D point clouds", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR point clouds contain measurements of complicated natural scenes and can\nbe used to update digital elevation models, glacial monitoring, detecting\nfaults and measuring uplift detecting, forest inventory, detect shoreline and\nbeach volume changes, landslide risk analysis, habitat mapping, and urban\ndevelopment, among others. A very important application is the classification\nof the 3D cloud into elementary classes. For example, it can be used to\ndifferentiate between vegetation, man-made structures, and water. Our goal is\nto present a preliminary comparison study for the classification of 3D point\ncloud LiDAR data that includes several types of feature engineering. In\nparticular, we demonstrate that providing context by augmenting each point in\nthe LiDAR point cloud with information about its neighboring points can improve\nthe performance of downstream learning algorithms. We also experiment with\nseveral dimension reduction strategies, ranging from Principal Component\nAnalysis (PCA) to neural network-based auto-encoders, and demonstrate how they\naffect classification performance in LiDAR point clouds. For instance, we\nobserve that combining feature engineering with a dimension reduction a method\nsuch as PCA, there is an improvement in the accuracy of the classification with\nrespect to doing a straightforward classification with the raw data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:23:23 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Medina", "F. Patricia", ""], ["Paffenroth", "Randy", ""]]}, {"id": "2101.09321", "submitter": "Maria A. Zuluaga", "authors": "Vien Ngoc Dang and Francesco Galati and Rosa Cortese and Giuseppe Di\n  Giacomo and Viola Marconetto and Prateek Mathur and Karim Lekadir and Marco\n  Lorenzi and Ferran Prados and Maria A. Zuluaga", "title": "Vessel-CAPTCHA: an efficient learning framework for vessel annotation\n  and segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques for 3D brain vessel image segmentation have not been\nas successful as in the segmentation of other organs and tissues. This can be\nexplained by two factors. First, deep learning techniques tend to show poor\nperformances at the segmentation of relatively small objects compared to the\nsize of the full image. Second, due to the complexity of vascular trees and the\nsmall size of vessels, it is challenging to obtain the amount of annotated\ntraining data typically needed by deep learning methods. To address these\nproblems, we propose a novel annotation-efficient deep learning vessel\nsegmentation framework. The framework avoids pixel-wise annotations, only\nrequiring weak patch-level labels to discriminate between vessel and non-vessel\n2D patches in the training set, in a setup similar to the CAPTCHAs used to\ndifferentiate humans from bots in web applications. The user-provided weak\nannotations are used for two tasks: 1) to synthesize pixel-wise pseudo-labels\nfor vessels and background in each patch, which are used to train a\nsegmentation network, and 2) to train a classifier network. The classifier\nnetwork allows to generate additional weak patch labels, further reducing the\nannotation burden, and it acts as a noise filter for poor quality images. We\nuse this framework for the segmentation of the cerebrovascular tree in\nTime-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). The\nresults show that the framework achieves state-of-the-art accuracy, while\nreducing the annotation time by ~77% w.r.t. learning-based segmentation methods\nusing pixel-wise labels for training.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:29:23 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 14:09:45 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 09:44:51 GMT"}, {"version": "v4", "created": "Tue, 20 Jul 2021 12:39:31 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Dang", "Vien Ngoc", ""], ["Galati", "Francesco", ""], ["Cortese", "Rosa", ""], ["Di Giacomo", "Giuseppe", ""], ["Marconetto", "Viola", ""], ["Mathur", "Prateek", ""], ["Lekadir", "Karim", ""], ["Lorenzi", "Marco", ""], ["Prados", "Ferran", ""], ["Zuluaga", "Maria A.", ""]]}, {"id": "2101.09376", "submitter": "Aaron Hertzmann", "authors": "Aaron Hertzmann", "title": "The Role of Edges in Line Drawing Perception", "comments": "Accepted to _Perception_", "journal-ref": "Perception. 2021;50(3):266-275", "doi": "10.1177/0301006621994407", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has often been conjectured that the effectiveness of line drawings can be\nexplained by the similarity of edge images to line drawings. This paper\npresents several problems with explaining line drawing perception in terms of\nedges, and how the recently-proposed Realism Hypothesis of Hertzmann (2020)\nresolves these problems. There is nonetheless existing evidence that edges are\noften the best features for predicting where people draw lines; this paper\ndescribes how the Realism Hypothesis can explain this evidence.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 23:22:05 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Hertzmann", "Aaron", ""]]}, {"id": "2101.09392", "submitter": "Kai Han", "authors": "Kai Han and Miaomiao Liu and Dirk Schnieders and Kwan-Yee K. Wong", "title": "Fixed Viewpoint Mirror Surface Reconstruction under an Uncalibrated\n  Camera", "comments": "IEEE Transactions on Image Processing (TIP). Code available at\n  https://github.com/k-han/mirror", "journal-ref": null, "doi": "10.1109/TIP.2021.3049946", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of mirror surface reconstruction, and\nproposes a solution based on observing the reflections of a moving reference\nplane on the mirror surface. Unlike previous approaches which require tedious\ncalibration, our method can recover the camera intrinsics, the poses of the\nreference plane, as well as the mirror surface from the observed reflections of\nthe reference plane under at least three unknown distinct poses. We first show\nthat the 3D poses of the reference plane can be estimated from the reflection\ncorrespondences established between the images and the reference plane. We then\nform a bunch of 3D lines from the reflection correspondences, and derive an\nanalytical solution to recover the line projection matrix. We transform the\nline projection matrix to its equivalent camera projection matrix, and propose\na cross-ratio based formulation to optimize the camera projection matrix by\nminimizing reprojection errors. The mirror surface is then reconstructed based\non the optimized cross-ratio constraint. Experimental results on both synthetic\nand real data are presented, which demonstrate the feasibility and accuracy of\nour method.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 01:20:55 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Han", "Kai", ""], ["Liu", "Miaomiao", ""], ["Schnieders", "Dirk", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "2101.09397", "submitter": "Juan Irving Vasquez-Gomez", "authors": "J. Irving Vasquez-Gomez and David Troncoso and Israel Becerra and\n  Enrique Sucar and Rafael Murrieta-Cid", "title": "Next-best-view Regression using a 3D Convolutional Neural Network", "comments": "Accepted to Machine Vision and Applications", "journal-ref": "Machine Vision and Applications 32, 42 (2021)", "doi": "10.1007/s00138-020-01166-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated three-dimensional (3D) object reconstruction is the task of\nbuilding a geometric representation of a physical object by means of sensing\nits surface. Even though new single view reconstruction techniques can predict\nthe surface, they lead to incomplete models, specially, for non commons objects\nsuch as antique objects or art sculptures. Therefore, to achieve the task's\ngoals, it is essential to automatically determine the locations where the\nsensor will be placed so that the surface will be completely observed. This\nproblem is known as the next-best-view problem. In this paper, we propose a\ndata-driven approach to address the problem. The proposed approach trains a 3D\nconvolutional neural network (3D CNN) with previous reconstructions in order to\nregress the \\btxt{position of the} next-best-view. To the best of our\nknowledge, this is one of the first works that directly infers the\nnext-best-view in a continuous space using a data-driven approach for the 3D\nobject reconstruction task. We have validated the proposed approach making use\nof two groups of experiments. In the first group, several variants of the\nproposed architecture are analyzed. Predicted next-best-views were observed to\nbe closely positioned to the ground truth. In the second group of experiments,\nthe proposed approach is requested to reconstruct several unseen objects,\nnamely, objects not considered by the 3D CNN during training nor validation.\nCoverage percentages of up to 90 \\% were observed. With respect to current\nstate-of-the-art methods, the proposed approach improves the performance of\nprevious next-best-view classification approaches and it is quite fast in\nrunning time (3 frames per second), given that it does not compute the\nexpensive ray tracing required by previous information metrics.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 01:50:26 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Vasquez-Gomez", "J. Irving", ""], ["Troncoso", "David", ""], ["Becerra", "Israel", ""], ["Sucar", "Enrique", ""], ["Murrieta-Cid", "Rafael", ""]]}, {"id": "2101.09401", "submitter": "Ningshan Xu", "authors": "Ningshan Xu", "title": "Adaptively Sparse Regularization for Blind Image Restoration", "comments": "10 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image quality is the basis of image communication and understanding tasks.\nDue to the blur and noise effects caused by imaging, transmission and other\nprocesses, the image quality is degraded. Blind image restoration is widely\nused to improve image quality, where the main goal is to faithfully estimate\nthe blur kernel and the latent sharp image. In this study, based on\nexperimental observation and research, an adaptively sparse regularized\nminimization method is originally proposed. The high-order gradients combine\nwith low-order ones to form a hybrid regularization term, and an adaptive\noperator derived from the image entropy is introduced to maintain a good\nconvergence. Extensive experiments were conducted on different blur kernels and\nimages. Compared with existing state-of-the-art blind deblurring methods, our\nmethod demonstrates superiority on the recovery accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 02:40:01 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Xu", "Ningshan", ""]]}, {"id": "2101.09403", "submitter": "Hamid Laga", "authors": "Hamid Laga, Marcel Padilla, Ian H. Jermyn, Sebastian Kurtek, Mohammed\n  Bennamoun, Anuj Srivastava", "title": "4D Atlas: Statistical Analysis of the Spatiotemporal Variability in\n  Longitudinal 3D Shape Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel framework to learn the spatiotemporal variability in\nlongitudinal 3D shape data sets, which contain observations of subjects that\nevolve and deform over time. This problem is challenging since surfaces come\nwith arbitrary spatial and temporal parameterizations. Thus, they need to be\nspatially registered and temporally aligned onto each other. We solve this\nspatiotemporal registration problem using a Riemannian approach. We treat a 3D\nsurface as a point in a shape space equipped with an elastic metric that\nmeasures the amount of bending and stretching that the surfaces undergo. A 4D\nsurface can then be seen as a trajectory in this space. With this formulation,\nthe statistical analysis of 4D surfaces becomes the problem of analyzing\ntrajectories embedded in a nonlinear Riemannian manifold. However, computing\nspatiotemporal registration and statistics on nonlinear spaces relies on\ncomplex nonlinear optimizations. Our core contribution is the mapping of the\nsurfaces to the space of Square-Root Normal Fields (SRNF) where the L2 metric\nis equivalent to the partial elastic metric in the space of surfaces. By\nsolving the spatial registration in the SRNF space, analyzing 4D surfaces\nbecomes the problem of analyzing trajectories embedded in the SRNF space, which\nis Euclidean. Here, we develop the building blocks that enable such analysis.\nThese include the spatiotemporal registration of arbitrarily parameterized 4D\nsurfaces even in the presence of large elastic deformations and large\nvariations in their execution rates, the computation of geodesics between 4D\nsurfaces, the computation of statistical summaries, such as means and modes of\nvariation, and the synthesis of random 4D surfaces. We demonstrate the\nperformance of the proposed framework using 4D facial surfaces and 4D human\nbody shapes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 02:59:55 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Laga", "Hamid", ""], ["Padilla", "Marcel", ""], ["Jermyn", "Ian H.", ""], ["Kurtek", "Sebastian", ""], ["Bennamoun", "Mohammed", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2101.09412", "submitter": "Yazhou Yao", "authors": "Huafeng Liu, Chuanyi Zhang, Yazhou Yao, Xiushen Wei, Fumin Shen, Jian\n  Zhang, and Zhenmin Tang", "title": "Exploiting Web Images for Fine-Grained Visual Recognition by Eliminating\n  Noisy Samples and Utilizing Hard Ones", "comments": null, "journal-ref": "IEEE Transactions on Multimedia, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling objects at a subordinate level typically requires expert knowledge,\nwhich is not always available when using random annotators. As such, learning\ndirectly from web images for fine-grained recognition has attracted broad\nattention. However, the presence of label noise and hard examples in web images\nare two obstacles for training robust fine-grained recognition models.\nTherefore, in this paper, we propose a novel approach for removing irrelevant\nsamples from real-world web images during training, while employing useful hard\nexamples to update the network. Thus, our approach can alleviate the harmful\neffects of irrelevant noisy web images and hard examples to achieve better\nperformance. Extensive experiments on three commonly used fine-grained datasets\ndemonstrate that our approach is far superior to current state-of-the-art\nweb-supervised methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 03:58:10 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Liu", "Huafeng", ""], ["Zhang", "Chuanyi", ""], ["Yao", "Yazhou", ""], ["Wei", "Xiushen", ""], ["Shen", "Fumin", ""], ["Zhang", "Jian", ""], ["Tang", "Zhenmin", ""]]}, {"id": "2101.09420", "submitter": "Li Yaning", "authors": "Yaning Li, Xue Wang, Guoqing Zhou, and Qing Wang", "title": "Deep Anti-aliasing of Whole Focal Stack Using its Slice Spectrum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper aims at removing the aliasing effects for the whole focal stack\ngenerated from a sparse 3D light field, while keeping the consistency across\nall the focal layers.We first explore the structural characteristics embedded\nin the focal stack slice and its corresponding frequency-domain representation,\ni.e., the focal stack spectrum (FSS). We also observe that the energy\ndistribution of FSS always locates within the same triangular area under\ndifferent angular sampling rates, additionally the continuity of point spread\nfunction (PSF) is intrinsically maintained in the FSS. Based on these two\nfindings, we propose a learning-based FSS reconstruction approach for one-time\naliasing removing over the whole focal stack. What's more, a novel\nconjugate-symmetric loss function is proposed for the optimization. Compared to\nprevious works, our method avoids an explicit depth estimation, and can handle\nchallenging large-disparity scenarios. Experimental results on both synthetic\nand real light field datasets show the superiority of the proposed approach for\ndifferent scenes and various angular sampling rates.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 05:14:49 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Li", "Yaning", ""], ["Wang", "Xue", ""], ["Zhou", "Guoqing", ""], ["Wang", "Qing", ""]]}, {"id": "2101.09433", "submitter": "Jinyeong Chae", "authors": "Jinyeong Chae, Ki Yong Hong, Jihie Kim", "title": "A Pressure Ulcer Care System For Remote Medical Assistance: Residual\n  U-Net with an Attention Model Based for Wound Area Segmentation", "comments": "Accepted by AAAI 2021 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increasing numbers of patients with disabilities or elderly people with\nmobility issues often suffer from a pressure ulcer. The affected areas need\nregular checks, but they have a difficulty in accessing a hospital. Some remote\ndiagnosis systems are being used for them, but there are limitations in\nchecking a patient's status regularly. In this paper, we present a remote\nmedical assistant that can help pressure ulcer management with image processing\ntechniques. The proposed system includes a mobile application with a deep\nlearning model for wound segmentation and analysis. As there are not enough\ndata to train the deep learning model, we make use of a pretrained model from a\nrelevant domain and data augmentation that is appropriate for this task. First\nof all, an image preprocessing method using bilinear interpolation is used to\nresize images and normalize the images. Second, for data augmentation, we use\nrotation, reflection, and a watershed algorithm. Third, we use a pretrained\ndeep learning model generated from skin wound images similar to pressure ulcer\nimages. Finally, we added an attention module that can provide hints on the\npressure ulcer image features. The resulting model provides an accuracy of\n99.0%, an intersection over union (IoU) of 99.99%, and a dice similarity\ncoefficient (DSC) of 93.4% for pressure ulcer segmentation, which is better\nthan existing results.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 06:45:52 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 10:24:09 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Chae", "Jinyeong", ""], ["Hong", "Ki Yong", ""], ["Kim", "Jihie", ""]]}, {"id": "2101.09436", "submitter": "Xudong Sun", "authors": "Xudong Sun, Florian Buettner", "title": "Hierarchical Variational Auto-Encoding for Unsupervised Domain\n  Generalization", "comments": "Presented at ICLR 2021 RobustML Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of domain generalization, where the goal is to train a\npredictive model such that it is able to generalize to a new, previously unseen\ndomain. We choose a hierarchical generative approach within the framework of\nvariational autoencoders and propose a domain-unsupervised algorithm that is\nable to generalize to new domains without domain supervision. We show that our\nmethod is able to learn representations that disentangle domain-specific\ninformation from class-label specific information even in complex settings\nwhere domain structure is not observed during training. Our interpretable\nmethod outperforms previously proposed generative algorithms for domain\ngeneralization as well as other non-generative state-of-the-art approaches in\nseveral hierarchical domain settings including sequential overlapped near\ncontinuous domain shift. It also achieves competitive performance on the\nstandard domain generalization benchmark dataset PACS compared to\nstate-of-the-art approaches which rely on observing domain-specific information\nduring training, as well as another domain unsupervised method. Additionally,\nwe proposed model selection purely based on Evidence Lower Bound (ELBO) and\nalso proposed weak domain supervision where implicit domain information can be\nadded into the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:09:59 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 19:00:48 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2021 13:35:03 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 17:36:04 GMT"}, {"version": "v5", "created": "Fri, 14 May 2021 20:51:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sun", "Xudong", ""], ["Buettner", "Florian", ""]]}, {"id": "2101.09451", "submitter": "Shao-Yuan Lo", "authors": "Shao-Yuan Lo and Vishal M. Patel", "title": "Error Diffusion Halftoning Against Adversarial Examples", "comments": "Accepted at IEEE International Conference on Image Processing (ICIP)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples contain carefully crafted perturbations that can fool\ndeep neural networks (DNNs) into making wrong predictions. Enhancing the\nadversarial robustness of DNNs has gained considerable interest in recent\nyears. Although image transformation-based defenses were widely considered at\nan earlier time, most of them have been defeated by adaptive attacks. In this\npaper, we propose a new image transformation defense based on error diffusion\nhalftoning, and combine it with adversarial training to defend against\nadversarial examples. Error diffusion halftoning projects an image into a 1-bit\nspace and diffuses quantization error to neighboring pixels. This process can\nremove adversarial perturbations from a given image while maintaining\nacceptable image quality in the meantime in favor of recognition. Experimental\nresults demonstrate that the proposed method is able to improve adversarial\nrobustness even under advanced adaptive attacks, while most of the other image\ntransformation-based defenses do not. We show that a proper image\ntransformation can still be an effective defense approach. Code:\nhttps://github.com/shaoyuanlo/Halftoning-Defense\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:55:02 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 23:03:02 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 06:59:58 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Lo", "Shao-Yuan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2101.09461", "submitter": "Gennaro Vessio Dr.", "authors": "Moises Diaz, Momina Moetesum, Imran Siddiqi, Gennaro Vessio", "title": "Sequence-based Dynamic Handwriting Analysis for Parkinson's Disease\n  Detection with One-dimensional Convolutions and BiGRUs", "comments": null, "journal-ref": "Expert Systems with Applications, Volume 168, 15 April 2021,\n  114405", "doi": "10.1016/j.eswa.2020.114405", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Parkinson's disease (PD) is commonly characterized by several motor symptoms,\nsuch as bradykinesia, akinesia, rigidity, and tremor. The analysis of patients'\nfine motor control, particularly handwriting, is a powerful tool to support PD\nassessment. Over the years, various dynamic attributes of handwriting, such as\npen pressure, stroke speed, in-air time, etc., which can be captured with the\nhelp of online handwriting acquisition tools, have been evaluated for the\nidentification of PD. Motion events, and their associated spatio-temporal\nproperties captured in online handwriting, enable effective classification of\nPD patients through the identification of unique sequential patterns. This\npaper proposes a novel classification model based on one-dimensional\nconvolutions and Bidirectional Gated Recurrent Units (BiGRUs) to assess the\npotential of sequential information of handwriting in identifying Parkinsonian\nsymptoms. One-dimensional convolutions are applied to raw sequences as well as\nderived features; the resulting sequences are then fed to BiGRU layers to\nachieve the final classification. The proposed method outperformed\nstate-of-the-art approaches on the PaHaW dataset and achieved competitive\nresults on the NewHandPD dataset.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 09:25:13 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Diaz", "Moises", ""], ["Moetesum", "Momina", ""], ["Siddiqi", "Imran", ""], ["Vessio", "Gennaro", ""]]}, {"id": "2101.09499", "submitter": "Yizhao Gao", "authors": "Yizhao Gao, Nanyi Fei, Guangzhen Liu, Zhiwu Lu, Tao Xiang, Songfang\n  Huang", "title": "Contrastive Prototype Learning with Augmented Embeddings for Few-Shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent few-shot learning (FSL) methods are based on meta-learning with\nepisodic training. In each meta-training episode, a discriminative feature\nembedding and/or classifier are first constructed from a support set in an\ninner loop, and then evaluated in an outer loop using a query set for model\nupdating. This query set sample centered learning objective is however\nintrinsically limited in addressing the lack of training data problem in the\nsupport set. In this paper, a novel contrastive prototype learning with\naugmented embeddings (CPLAE) model is proposed to overcome this limitation.\nFirst, data augmentations are introduced to both the support and query sets\nwith each sample now being represented as an augmented embedding (AE) composed\nof concatenated embeddings of both the original and augmented versions. Second,\na novel support set class prototype centered contrastive loss is proposed for\ncontrastive prototype learning (CPL). With a class prototype as an anchor, CPL\naims to pull the query samples of the same class closer and those of different\nclasses further away. This support set sample centered loss is highly\ncomplementary to the existing query centered loss, fully exploiting the limited\ntraining data in each episode. Extensive experiments on several benchmarks\ndemonstrate that our proposed CPLAE achieves new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 13:22:44 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Gao", "Yizhao", ""], ["Fei", "Nanyi", ""], ["Liu", "Guangzhen", ""], ["Lu", "Zhiwu", ""], ["Xiang", "Tao", ""], ["Huang", "Songfang", ""]]}, {"id": "2101.09536", "submitter": "James Smith", "authors": "James Smith, Jonathan Balloch, Yen-Chang Hsu, Zsolt Kira", "title": "Memory-Efficient Semi-Supervised Continual Learning: The World is its\n  Own Replay Buffer", "comments": "Accepted by the 2021 International Joint Conference on Neural\n  Networks (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rehearsal is a critical component for class-incremental continual learning,\nyet it requires a substantial memory budget. Our work investigates whether we\ncan significantly reduce this memory budget by leveraging unlabeled data from\nan agent's environment in a realistic and challenging continual learning\nparadigm. Specifically, we explore and formalize a novel semi-supervised\ncontinual learning (SSCL) setting, where labeled data is scarce yet non-i.i.d.\nunlabeled data from the agent's environment is plentiful. Importantly, data\ndistributions in the SSCL setting are realistic and therefore reflect object\nclass correlations between, and among, the labeled and unlabeled data\ndistributions. We show that a strategy built on pseudo-labeling, consistency\nregularization, Out-of-Distribution (OoD) detection, and knowledge distillation\nreduces forgetting in this setting. Our approach, DistillMatch, increases\nperformance over the state-of-the-art by no less than 8.7% average task\naccuracy and up to 54.5% average task accuracy in SSCL CIFAR-100 experiments.\nMoreover, we demonstrate that DistillMatch can save up to 0.23 stored images\nper processed unlabeled image compared to the next best method which only saves\n0.08. Our results suggest that focusing on realistic correlated distributions\nis a significantly new perspective, which accentuates the importance of\nleveraging the world's structure as a continual learning strategy.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 17:23:08 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 17:55:20 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Smith", "James", ""], ["Balloch", "Jonathan", ""], ["Hsu", "Yen-Chang", ""], ["Kira", "Zsolt", ""]]}, {"id": "2101.09552", "submitter": "Bahjat Kawar", "authors": "Bahjat Kawar, Gregory Vaksman, Michael Elad", "title": "Stochastic Image Denoising by Sampling from the Posterior Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is a well-known and well studied problem, commonly targeting\na minimization of the mean squared error (MSE) between the outcome and the\noriginal image. Unfortunately, especially for severe noise levels, such Minimum\nMSE (MMSE) solutions may lead to blurry output images. In this work we propose\na novel stochastic denoising approach that produces viable and high perceptual\nquality results, while maintaining a small MSE. Our method employs Langevin\ndynamics that relies on a repeated application of any given MMSE denoiser,\nobtaining the reconstructed image by effectively sampling from the posterior\ndistribution. Due to its stochasticity, the proposed algorithm can produce a\nvariety of high-quality outputs for a given noisy input, all shown to be\nlegitimate denoising results. In addition, we present an extension of our\nalgorithm for handling the inpainting problem, recovering missing pixels while\nremoving noise from partially given data.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 18:28:19 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 12:46:50 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Kawar", "Bahjat", ""], ["Vaksman", "Gregory", ""], ["Elad", "Michael", ""]]}, {"id": "2101.09553", "submitter": "Kevin Black", "authors": "Kevin Black, Shrivu Shankar, Daniel Fonseka, Jacob Deutsch, Abhimanyu\n  Dhir, and Maruthi R. Akella", "title": "Real-Time, Flight-Ready, Non-Cooperative Spacecraft Pose Estimation\n  Using Monocular Imagery", "comments": "Presented at the 31st AAS/AIAA Space Flight Mechanics Meeting,\n  February 2021. 16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": "AAS 21-283", "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for autonomous on-orbit proximity operations is the\nestimation of a target spacecraft's relative pose (position and orientation).\nIt is desirable to employ monocular cameras for this problem due to their low\ncost, weight, and power requirements. This work presents a novel convolutional\nneural network (CNN)-based monocular pose estimation system that achieves\nstate-of-the-art accuracy with low computational demand. In combination with a\nBlender-based synthetic data generation scheme, the system demonstrates the\nability to generalize from purely synthetic training data to real in-space\nimagery of the Northrop Grumman Enhanced Cygnus spacecraft. Additionally, the\nsystem achieves real-time performance on low-power flight-like hardware.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 18:40:08 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Black", "Kevin", ""], ["Shankar", "Shrivu", ""], ["Fonseka", "Daniel", ""], ["Deutsch", "Jacob", ""], ["Dhir", "Abhimanyu", ""], ["Akella", "Maruthi R.", ""]]}, {"id": "2101.09560", "submitter": "Shuhang Wang", "authors": "Shuhang Wang, Vivek Kumar Singh, Alex Benjamin, Mercy Asiedu, Elham\n  Yousef Kalafi, Eugene Cheah, Viksit Kumar, Anthony Samir", "title": "Network-Agnostic Knowledge Transfer for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional transfer learning leverages weights of pre-trained networks, but\nmandates the need for similar neural architectures. Alternatively, knowledge\ndistillation can transfer knowledge between heterogeneous networks but often\nrequires access to the original training data or additional generative\nnetworks. Knowledge transfer between networks can be improved by being agnostic\nto the choice of network architecture and reducing the dependence on original\ntraining data. We propose a knowledge transfer approach from a teacher to a\nstudent network wherein we train the student on an independent transferal\ndataset, whose annotations are generated by the teacher. Experiments were\nconducted on five state-of-the-art networks for semantic segmentation and seven\ndatasets across three imaging modalities. We studied knowledge transfer from a\nsingle teacher, combination of knowledge transfer and fine-tuning, and\nknowledge transfer from multiple teachers. The student model with a single\nteacher achieved similar performance as the teacher; and the student model with\nmultiple teachers achieved better performance than the teachers. The salient\nfeatures of our algorithm include: 1)no need for original training data or\ngenerative networks, 2) knowledge transfer between different architectures, 3)\nease of implementation for downstream tasks by using the downstream task\ndataset as the transferal dataset, 4) knowledge transfer of an ensemble of\nmodels, trained independently, into one student model. Extensive experiments\ndemonstrate that the proposed algorithm is effective for knowledge transfer and\neasily tunable.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 19:06:14 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Shuhang", ""], ["Singh", "Vivek Kumar", ""], ["Benjamin", "Alex", ""], ["Asiedu", "Mercy", ""], ["Kalafi", "Elham Yousef", ""], ["Cheah", "Eugene", ""], ["Kumar", "Viksit", ""], ["Samir", "Anthony", ""]]}, {"id": "2101.09568", "submitter": "Xinwei Zhao", "authors": "Xinwei Zhao, Chen Chen, Matthew C. Stamm", "title": "A Transferable Anti-Forensic Attack on Forensic CNNs Using A Generative\n  Adversarial Network", "comments": "Submitted to IEEE Transactions on Information Forensics and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of deep learning, convolutional neural networks (CNNs)\nhave become widely used in multimedia forensics for tasks such as detecting and\nidentifying image forgeries. Meanwhile, anti-forensic attacks have been\ndeveloped to fool these CNN-based forensic algorithms. Previous anti-forensic\nattacks often were designed to remove forgery traces left by a single\nmanipulation operation as opposed to a set of manipulations. Additionally,\nrecent research has shown that existing anti-forensic attacks against forensic\nCNNs have poor transferability, i.e. they are unable to fool other forensic\nCNNs that were not explicitly used during training. In this paper, we propose a\nnew anti-forensic attack framework designed to remove forensic traces left by a\nvariety of manipulation operations. This attack is transferable, i.e. it can be\nused to attack forensic CNNs are unknown to the attacker, and it introduces\nonly minimal distortions that are imperceptible to human eyes. Our proposed\nattack utilizes a generative adversarial network (GAN) to build a generator\nthat can attack color images of any size. We achieve attack transferability\nthrough the use of a new training strategy and loss function. We conduct\nextensive experiment to demonstrate that our attack can fool many state-of-art\nforensic CNNs with varying levels of knowledge available to the attacker.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 19:31:59 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhao", "Xinwei", ""], ["Chen", "Chen", ""], ["Stamm", "Matthew C.", ""]]}, {"id": "2101.09569", "submitter": "Mokshith Voodarla", "authors": "Mokshith Voodarla, Shubham Shrivastava, Sagar Manglani, Ankit Vora,\n  Siddharth Agarwal, Punarjay Chakravarty", "title": "S-BEV: Semantic Birds-Eye View Representation for Weather and Lighting\n  Invariant 3-DoF Localization", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a light-weight, weather and lighting invariant, Semantic Bird's\nEye View (S-BEV) signature for vision-based vehicle re-localization. A\ntopological map of S-BEV signatures is created during the first traversal of\nthe route, which are used for coarse localization in subsequent route\ntraversal. A fine-grained localizer is then trained to output the global 3-DoF\npose of the vehicle using its S-BEV and its coarse localization. We conduct\nexperiments on vKITTI2 virtual dataset and show the potential of the S-BEV to\nbe robust to weather and lighting. We also demonstrate results with 2 vehicles\non a 22 km long highway route in the Ford AV dataset.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 19:37:09 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Voodarla", "Mokshith", ""], ["Shrivastava", "Shubham", ""], ["Manglani", "Sagar", ""], ["Vora", "Ankit", ""], ["Agarwal", "Siddharth", ""], ["Chakravarty", "Punarjay", ""]]}, {"id": "2101.09585", "submitter": "Ozan Tezcan", "authors": "M. Ozan Tezcan, Prakash Ishwar, Janusz Konrad", "title": "BSUV-Net 2.0: Spatio-Temporal Data Augmentations for Video-Agnostic\n  Supervised Background Subtraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background subtraction (BGS) is a fundamental video processing task which is\na key component of many applications. Deep learning-based supervised algorithms\nachieve very good perforamnce in BGS, however, most of these algorithms are\noptimized for either a specific video or a group of videos, and their\nperformance decreases dramatically when applied to unseen videos. Recently,\nseveral papers addressed this problem and proposed video-agnostic supervised\nBGS algorithms. However, nearly all of the data augmentations used in these\nalgorithms are limited to the spatial domain and do not account for temporal\nvariations that naturally occur in video data. In this work, we introduce\nspatio-temporal data augmentations and apply them to one of the leading\nvideo-agnostic BGS algorithms, BSUV-Net. We also introduce a new\ncross-validation training and evaluation strategy for the CDNet-2014 dataset\nthat makes it possible to fairly and easily compare the performance of various\nvideo-agnostic supervised BGS algorithms. Our new model trained using the\nproposed data augmentations, named BSUV-Net 2.0, significantly outperforms\nstate-of-the-art algorithms evaluated on unseen videos of CDNet-2014. We also\nevaluate the cross-dataset generalization capacity of BSUV-Net 2.0 by training\nit solely on CDNet-2014 videos and evaluating its performance on LASIESTA\ndataset. Overall, BSUV-Net 2.0 provides a ~5% improvement in the F-score over\nstate-of-the-art methods on unseen videos of CDNet-2014 and LASIESTA datasets.\nFurthermore, we develop a real-time variant of our model, that we call Fast\nBSUV-Net 2.0, whose performance is close to the state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 21:11:29 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 19:51:05 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Tezcan", "M. Ozan", ""], ["Ishwar", "Prakash", ""], ["Konrad", "Janusz", ""]]}, {"id": "2101.09606", "submitter": "Xiaoyu Lin", "authors": "Xiaoyu Lin", "title": "Learning degraded image classification with restoration data fidelity", "comments": "semester project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based methods especially with convolutional neural networks (CNN)\nare continuously showing superior performance in computer vision applications,\nranging from image classification to restoration. For image classification,\nmost existing works focus on very clean images such as images in Caltech-256\nand ImageNet datasets. However, in most realistic scenarios, the acquired\nimages may suffer from degradation. One important and interesting problem is to\ncombine image classification and restoration tasks to improve the performance\nof CNN-based classification networks on degraded images. In this report, we\nexplore the influence of degradation types and levels on four widely-used\nclassification networks, and the use of a restoration network to eliminate the\ndegradation's influence. We also propose a novel method leveraging a fidelity\nmap to calibrate the image features obtained by pre-trained classification\nnetworks. We empirically demonstrate that our proposed method consistently\noutperforms the pre-trained networks under all degradation levels and types\nwith additive white Gaussian noise (AWGN), and it even outperforms the\nre-trained networks for degraded images under low degradation levels. We also\nshow that the proposed method is a model-agnostic approach that benefits\ndifferent classification networks. Our results reveal that the proposed method\nis a promising solution to mitigate the effect caused by image degradation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 23:47:03 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lin", "Xiaoyu", ""]]}, {"id": "2101.09617", "submitter": "Aishan Liu", "authors": "Aishan Liu, Xianglong Liu, Jun Guo, Jiakai Wang, Yuqing Ma, Ze Zhao,\n  Xinghai Gao, Gang Xiao", "title": "A Comprehensive Evaluation Framework for Deep Model Robustness", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved remarkable performance across a\nwide area of applications. However, they are vulnerable to adversarial\nexamples, which motivates the adversarial defense. By adopting simple\nevaluation metrics, most of the current defenses only conduct incomplete\nevaluations, which are far from providing comprehensive understandings of the\nlimitations of these defenses. Thus, most proposed defenses are quickly shown\nto be attacked successfully, which result in the \"arm race\" phenomenon between\nattack and defense. To mitigate this problem, we establish a model robustness\nevaluation framework containing a comprehensive, rigorous, and coherent set of\nevaluation metrics, which could fully evaluate model robustness and provide\ndeep insights into building robust models. With 23 evaluation metrics in total,\nour framework primarily focuses on the two key factors of adversarial learning\n(\\ie, data and model). Through neuron coverage and data imperceptibility, we\nuse data-oriented metrics to measure the integrity of test examples; by delving\ninto model structure and behavior, we exploit model-oriented metrics to further\nevaluate robustness in the adversarial setting. To fully demonstrate the\neffectiveness of our framework, we conduct large-scale experiments on multiple\ndatasets including CIFAR-10 and SVHN using different models and defenses with\nour open-source platform AISafety. Overall, our paper aims to provide a\ncomprehensive evaluation framework which could demonstrate detailed inspections\nof the model robustness, and we hope that our paper can inspire further\nimprovement to the model robustness.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 01:04:25 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Liu", "Aishan", ""], ["Liu", "Xianglong", ""], ["Guo", "Jun", ""], ["Wang", "Jiakai", ""], ["Ma", "Yuqing", ""], ["Zhao", "Ze", ""], ["Gao", "Xinghai", ""], ["Xiao", "Gang", ""]]}, {"id": "2101.09637", "submitter": "Vasudevan Lakshminarayanan", "authors": "Yuliana Jim\\'enez Gaona, Mar\\'ia Jos\\'e Rodriguez-Alvarez, Hector\n  Espin\\'o Morat\\'o, Darwin Castillo Malla, and Vasudevan Lakshminarayanan", "title": "DenseNet for Breast Tumor Classification in Mammographic Images", "comments": "to be submitted to The 2nd International Conference on Medical\n  Imaging and Computer-Aided Diagnosis (MICAD2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer is the most common invasive cancer in women, and the second\nmain cause of death. Breast cancer screening is an efficient method to detect\nindeterminate breast lesions early. The common approaches of screening for\nwomen are tomosynthesis and mammography images. However, the traditional manual\ndiagnosis requires an intense workload by pathologists, who are prone to\ndiagnostic errors. Thus, the aim of this study is to build a deep convolutional\nneural network method for automatic detection, segmentation, and classification\nof breast lesions in mammography images. Based on deep learning the Mask-CNN\n(RoIAlign) method was developed to features selection and extraction; and the\nclassification was carried out by DenseNet architecture. Finally, the precision\nand accuracy of the model is evaluated by cross validation matrix and AUC\ncurve. To summarize, the findings of this study may provide a helpful to\nimprove the diagnosis and efficiency in the automatic tumor localization\nthrough the medical image classification.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 03:30:59 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Gaona", "Yuliana Jim\u00e9nez", ""], ["Rodriguez-Alvarez", "Mar\u00eda Jos\u00e9", ""], ["Morat\u00f3", "Hector Espin\u00f3", ""], ["Malla", "Darwin Castillo", ""], ["Lakshminarayanan", "Vasudevan", ""]]}, {"id": "2101.09639", "submitter": "Sergiu Mocanu", "authors": "Sergiu Mocanu, Alan R. Moody, April Khademi", "title": "FlowReg: Fast Deformable Unsupervised Medical Image Registration using\n  Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose FlowReg, a deep learning-based framework for unsupervised image\nregistration for neuroimaging applications. The system is composed of two\narchitectures that are trained sequentially: FlowReg-A which affinely corrects\nfor gross differences between moving and fixed volumes in 3D followed by\nFlowReg-O which performs pixel-wise deformations on a slice-by-slice basis for\nfine tuning in 2D. The affine network regresses the 3D affine matrix based on a\ncorrelation loss function that enforces global similarity. The deformable\nnetwork operates on 2D image slices based on the optical flow network\nFlowNet-Simple but with three loss components. The photometric loss minimizes\npixel intensity differences differences, the smoothness loss encourages similar\nmagnitudes between neighbouring vectors, and a correlation loss that is used to\nmaintain the intensity similarity between fixed and moving image slices. The\nproposed method is compared to four open source registration techniques ANTs,\nDemons, SE, and Voxelmorph. In total, 4643 FLAIR MR imaging volumes are used\nfrom dementia and vascular disease cohorts, acquired from over 60 international\ncentres with varying acquisition parameters. A battery of quantitative novel\nregistration validation metrics are proposed that focus on the structural\nintegrity of tissues, spatial alignment, and intensity similarity. Experimental\nresults show FlowReg (FlowReg-A+O) performs better than iterative-based\nregistration algorithms for intensity and spatial alignment metrics with a\nPixelwise Agreement of 0.65, correlation coefficient of 0.80, and Mutual\nInformation of 0.29. Among the deep learning frameworks, FlowReg-A or\nFlowReg-A+O provided the highest performance over all but one of the metrics.\nResults show that FlowReg is able to obtain high intensity and spatial\nsimilarity while maintaining the shape and structure of anatomy and pathology.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 03:51:34 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mocanu", "Sergiu", ""], ["Moody", "Alan R.", ""], ["Khademi", "April", ""]]}, {"id": "2101.09641", "submitter": "Nannan Qin", "authors": "Nannan Qin, Weikai Tan, Lingfei Ma, Dedong Zhang, Jonathan Li", "title": "OpenGF: An Ultra-Large-Scale Ground Filtering Dataset Built Upon Open\n  ALS Point Clouds Around the World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground filtering has remained a widely studied but incompletely resolved\nbottleneck for decades in the automatic generation of high-precision digital\nelevation model, due to the dramatic changes of topography and the complex\nstructures of objects. The recent breakthrough of supervised deep learning\nalgorithms in 3D scene understanding brings new solutions for better solving\nsuch problems. However, there are few large-scale and scene-rich public\ndatasets dedicated to ground extraction, which considerably limits the\ndevelopment of effective deep-learning-based ground filtering methods. To this\nend, we present OpenGF, first Ultra-Large-Scale Ground Filtering dataset\ncovering over 47 $km^2$ of 9 different typical terrain scenes built upon open\nALS point clouds of 4 different countries around the world. OpenGF contains\nmore than half a billion finely labeled ground and non-ground points, thousands\nof times the number of labeled points than the de facto standard ISPRS\nfiltertest dataset. We extensively evaluate the performance of state-of-the-art\nrule-based algorithms and 3D semantic segmentation networks on our dataset and\nprovide a comprehensive analysis. The results have confirmed the capability of\nOpenGF to train deep learning models effectively. This dataset is released at\nhttps://github.com/Nathan-UW/OpenGF to promote more advancing research for\nground filtering and large-scale 3D geographic environment understanding.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 04:07:35 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 13:15:22 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Qin", "Nannan", ""], ["Tan", "Weikai", ""], ["Ma", "Lingfei", ""], ["Zhang", "Dedong", ""], ["Li", "Jonathan", ""]]}, {"id": "2101.09642", "submitter": "Hoang Trinh Man", "authors": "Trinh Man Hoang, Jinjia Zhou, Yibo Fan", "title": "Image Compression with Encoder-Decoder Matched Semantic Segmentation", "comments": null, "journal-ref": "2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW), Seattle, WA, USA, 2020, pp. 619-623", "doi": "10.1109/CVPRW50498.2020.00088", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, layered image compression is demonstrated to be a promising\ndirection, which encodes a compact representation of the input image and apply\nan up-sampling network to reconstruct the image. To further improve the quality\nof the reconstructed image, some works transmit the semantic segment together\nwith the compressed image data. Consequently, the compression ratio is also\ndecreased because extra bits are required for transmitting the semantic\nsegment. To solve this problem, we propose a new layered image compression\nframework with encoder-decoder matched semantic segmentation (EDMS). And then,\nfollowed by the semantic segmentation, a special convolution neural network is\nused to enhance the inaccurate semantic segment. As a result, the accurate\nsemantic segment can be obtained in the decoder without requiring extra bits.\nThe experimental results show that the proposed EDMS framework can get up to\n35.31% BD-rate reduction over the HEVC-based (BPG) codec, 5% bitrate, and 24%\nencoding time saving compare to the state-of-the-art semantic-based image\ncodec.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 04:11:05 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 05:50:57 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hoang", "Trinh Man", ""], ["Zhou", "Jinjia", ""], ["Fan", "Yibo", ""]]}, {"id": "2101.09643", "submitter": "Yu Fu", "authors": "Yu Fu, Xiao-Jun Wu", "title": "A Dual-branch Network for Infrared and Visible Image Fusion", "comments": null, "journal-ref": "25th International Conference on Pattern Recognition (ICPR2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning is a rapidly developing approach in the field of infrared and\nvisible image fusion. In this context, the use of dense blocks in deep networks\nsignificantly improves the utilization of shallow information, and the\ncombination of the Generative Adversarial Network (GAN) also improves the\nfusion performance of two source images. We propose a new method based on dense\nblocks and GANs , and we directly insert the input image-visible light image in\neach layer of the entire network. We use SSIM and gradient loss functions that\nare more consistent with perception instead of mean square error loss. After\nthe adversarial training between the generator and the discriminator, we show\nthat a trained end-to-end fusion network -- the generator network -- is finally\nobtained. Our experiments show that the fused images obtained by our approach\nachieve good score based on multiple evaluation indicators. Further, our fused\nimages have better visual effects in multiple sets of contrasts, which are more\nsatisfying to human visual perception.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 04:18:32 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Fu", "Yu", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "2101.09657", "submitter": "Zijie Jiang", "authors": "Zijie Jiang, Hajime Taira, Naoyuki Miyashita, Masatoshi Okutomi", "title": "VIO-Aided Structure from Motion Under Challenging Environments", "comments": "This manuscript was accepted and presented in the 22th IEEE\n  International Conference on Industrial Technology (ICIT2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a robust and efficient Structure from Motion\npipeline for accurate 3D reconstruction under challenging environments by\nleveraging the camera pose information from a visual-inertial odometry.\nSpecifically, we propose a geometric verification method to filter out\nmismatches by considering the prior geometric configuration of candidate image\npairs. Furthermore, we introduce an efficient and scalable reconstruction\napproach that relies on batched image registration and robust bundle\nadjustment, both leveraging the reliable local odometry estimation. Extensive\nexperimental results show that our pipeline performs better than the\nstate-of-the-art SfM approaches in terms of reconstruction accuracy and\nrobustness for challenging sequential image collections.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 06:35:52 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 11:40:47 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Jiang", "Zijie", ""], ["Taira", "Hajime", ""], ["Miyashita", "Naoyuki", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2101.09658", "submitter": "Nilanjan Sinhababu", "authors": "Nilanjan Sinhababu, Monalisa Sarma and Debasis Samanta", "title": "Computational Intelligence Approach to Improve the Classification\n  Accuracy of Brain Neoplasm in MRI Data", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic detection of brain neoplasm in Magnetic Resonance Imaging (MRI) is\ngaining importance in many medical diagnostic applications. This report\npresents two improvements for brain neoplasm detection in MRI data: an advanced\npreprocessing technique is proposed to improve the area of interest in MRI data\nand a hybrid technique using Convolutional Neural Network (CNN) for feature\nextraction followed by Support Vector Machine (SVM) for classification. The\nlearning algorithm for SVM is modified with the addition of cost function to\nminimize false positive prediction addressing the errors in MRI data diagnosis.\nThe proposed approach can effectively detect the presence of neoplasm and also\npredict whether it is cancerous (malignant) or non-cancerous (benign). To check\nthe effectiveness of the proposed preprocessing technique, it is inspected\nvisually and evaluated using training performance metrics. A comparison study\nbetween the proposed classification technique and the existing techniques was\nperformed. The result showed that the proposed approach outperformed in terms\nof accuracy and can handle errors in classification better than the existing\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 06:45:26 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sinhababu", "Nilanjan", ""], ["Sarma", "Monalisa", ""], ["Samanta", "Debasis", ""]]}, {"id": "2101.09666", "submitter": "Xushuai Shuai", "authors": "Shuai Xu, Dongliang Chang, Jiyang Xie, and Zhanyu Ma", "title": "Grad-CAM guided channel-spatial attention module for fine-grained visual\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fine-grained visual classification (FGVC) is becoming an important research\nfield, due to its wide applications and the rapid development of computer\nvision technologies. The current state-of-the-art (SOTA) methods in the FGVC\nusually employ attention mechanisms to first capture the semantic parts and\nthen discover their subtle differences between distinct classes. The\nchannel-spatial attention mechanisms, which focus on the discriminative\nchannels and regions simultaneously, have significantly improved the\nclassification performance. However, the existing attention modules are poorly\nguided since part-based detectors in the FGVC depend on the network learning\nability without the supervision of part annotations. As obtaining such part\nannotations is labor-intensive, some visual localization and explanation\nmethods, such as gradient-weighted class activation mapping (Grad-CAM), can be\nutilized for supervising the attention mechanism. We propose a Grad-CAM guided\nchannel-spatial attention module for the FGVC, which employs the Grad-CAM to\nsupervise and constrain the attention weights by generating the coarse\nlocalization maps. To demonstrate the effectiveness of the proposed method, we\nconduct comprehensive experiments on three popular FGVC datasets, including\nCUB-$200$-$2011$, Stanford Cars, and FGVC-Aircraft datasets. The proposed\nmethod outperforms the SOTA attention modules in the FGVC task. In addition,\nvisualizations of feature maps also demonstrate the superiority of the proposed\nmethod against the SOTA approaches.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 07:28:14 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Xu", "Shuai", ""], ["Chang", "Dongliang", ""], ["Xie", "Jiyang", ""], ["Ma", "Zhanyu", ""]]}, {"id": "2101.09671", "submitter": "Tailin Liang", "authors": "Tailin Liang, John Glossner, Lei Wang, Shaobo Shi and Xiaotong Zhang", "title": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been applied in many applications exhibiting\nextraordinary abilities in the field of computer vision. However, complex\nnetwork architectures challenge efficient real-time deployment and require\nsignificant computation resources and energy costs. These challenges can be\novercome through optimizations such as network compression. Network compression\ncan often be realized with little loss of accuracy. In some cases accuracy may\neven improve. This paper provides a survey on two types of network compression:\npruning and quantization. Pruning can be categorized as static if it is\nperformed offline or dynamic if it is performed at run-time. We compare pruning\ntechniques and describe criteria used to remove redundant computations. We\ndiscuss trade-offs in element-wise, channel-wise, shape-wise, filter-wise,\nlayer-wise and even network-wise pruning. Quantization reduces computations by\nreducing the precision of the datatype. Weights, biases, and activations may be\nquantized typically to 8-bit integers although lower bit width implementations\nare also discussed including binary neural networks. Both pruning and\nquantization can be used independently or combined. We compare current\ntechniques, analyze their strengths and weaknesses, present compressed network\naccuracy results on a number of frameworks, and provide practical guidance for\ncompressing networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 08:21:04 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 03:39:39 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 07:15:40 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Liang", "Tailin", ""], ["Glossner", "John", ""], ["Wang", "Lei", ""], ["Shi", "Shaobo", ""], ["Zhang", "Xiaotong", ""]]}, {"id": "2101.09698", "submitter": "Longteng Guo", "authors": "Longteng Guo, Jing Liu, Xinxin Zhu, Hanqing Lu", "title": "Fast Sequence Generation with Multi-Agent Reinforcement Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.04690", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autoregressive sequence Generation models have achieved state-of-the-art\nperformance in areas like machine translation and image captioning. These\nmodels are autoregressive in that they generate each word by conditioning on\npreviously generated words, which leads to heavy latency during inference.\nRecently, non-autoregressive decoding has been proposed in machine translation\nto speed up the inference time by generating all words in parallel. Typically,\nthese models use the word-level cross-entropy loss to optimize each word\nindependently. However, such a learning process fails to consider the\nsentence-level consistency, thus resulting in inferior generation quality of\nthese non-autoregressive models. In this paper, we propose a simple and\nefficient model for Non-Autoregressive sequence Generation (NAG) with a novel\ntraining paradigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL\nformulates NAG as a multi-agent reinforcement learning system where element\npositions in the target sequence are viewed as agents that learn to\ncooperatively maximize a sentence-level reward. On MSCOCO image captioning\nbenchmark, our NAG method achieves a performance comparable to state-of-the-art\nautoregressive models, while brings 13.9x decoding speedup. On WMT14 EN-DE\nmachine translation dataset, our method outperforms cross-entropy trained\nbaseline by 6.0 BLEU points while achieves the greatest decoding speedup of\n17.46x.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:16:45 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Guo", "Longteng", ""], ["Liu", "Jing", ""], ["Zhu", "Xinxin", ""], ["Lu", "Hanqing", ""]]}, {"id": "2101.09710", "submitter": "Gerrit Ecke", "authors": "Gerrit A. Ecke, Harald M. Papp, Hanspeter A. Mallot", "title": "Exploitation of Image Statistics with Sparse Coding in the Case of\n  Stereo Vision", "comments": "Author's accepted manuscript", "journal-ref": "Neural Networks, Volume 135, 2021, Pages 158-176", "doi": "10.1016/j.neunet.2020.12.016", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The sparse coding algorithm has served as a model for early processing in\nmammalian vision. It has been assumed that the brain uses sparse coding to\nexploit statistical properties of the sensory stream. We hypothesize that\nsparse coding discovers patterns from the data set, which can be used to\nestimate a set of stimulus parameters by simple readout. In this study, we\nchose a model of stereo vision to test our hypothesis. We used the Locally\nCompetitive Algorithm (LCA), followed by a na\\\"ive Bayes classifier, to infer\nstereo disparity. From the results we report three observations. First,\ndisparity inference was successful with this naturalistic processing pipeline.\nSecond, an expanded, highly redundant representation is required to robustly\nidentify the input patterns. Third, the inference error can be predicted from\nthe number of active coefficients in the LCA representation. We conclude that\nsparse coding can generate a suitable general representation for subsequent\ninference tasks. Keywords: Sparse coding; Locally Competitive Algorithm (LCA);\nEfficient coding; Compact code; Probabilistic inference; Stereo vision\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:45:25 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 22:24:16 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ecke", "Gerrit A.", ""], ["Papp", "Harald M.", ""], ["Mallot", "Hanspeter A.", ""]]}, {"id": "2101.09744", "submitter": "Nati Ofir", "authors": "Nati Ofir and Jean-Christophe Nebel", "title": "Classic versus deep approaches to address computer vision challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision and image processing address many challenging applications.\nWhile the last decade has seen deep neural network architectures\nrevolutionizing those fields, early methods relied on 'classic', i.e.,\nnon-learned approaches. In this study, we explore the differences between\nclassic and deep learning (DL) algorithms to gain new insight regarding which\nis more suitable for a given application. The focus is on two challenging\nill-posed problems, namely faint edge detection and multispectral image\nregistration, studying recent state-of-the-art DL and classic solutions. While\nthose DL algorithms outperform classic methods in terms of accuracy and\ndevelopment time, they tend to have higher resource requirements and are unable\nto perform outside their training space. Moreover, classic algorithms are more\ntransparent, which facilitates their adoption for real-life applications. As\nboth classes of approaches have unique strengths and limitations, the choice of\na solution is clearly application dependent.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 16:27:23 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ofir", "Nati", ""], ["Nebel", "Jean-Christophe", ""]]}, {"id": "2101.09745", "submitter": "Julian Tanke", "authors": "Julian Tanke, Juergen Gall", "title": "Iterative Greedy Matching for 3D Human Pose Tracking from Multiple Views", "comments": "German Conference on Pattern Recognition 2019", "journal-ref": "GCPR 2019, pages 537--550", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we propose an approach for estimating 3D human poses of multiple\npeople from a set of calibrated cameras. Estimating 3D human poses from\nmultiple views has several compelling properties: human poses are estimated\nwithin a global coordinate space and multiple cameras provide an extended field\nof view which helps in resolving ambiguities, occlusions and motion blur. Our\napproach builds upon a real-time 2D multi-person pose estimation system and\ngreedily solves the association problem between multiple views. We utilize\nbipartite matching to track multiple people over multiple frames. This proofs\nto be especially efficient as problems associated with greedy matching such as\nocclusion can be easily resolved in 3D. Our approach achieves state-of-the-art\nresults on popular benchmarks and may serve as a baseline for future work.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 16:28:10 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tanke", "Julian", ""], ["Gall", "Juergen", ""]]}, {"id": "2101.09752", "submitter": "Sibendu Paul", "authors": "Sibendu Paul, Utsav Drolia, Y. Charlie Hu, Srimat T. Chakradhar", "title": "AQuA: Analytical Quality Assessment for Optimizing Video Analytics\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of cameras at edge are being deployed to power a variety of\ndifferent deep learning applications. However, the frames captured by these\ncameras are not always pristine - they can be distorted due to lighting issues,\nsensor noise, compression etc. Such distortions not only deteriorate visual\nquality, they impact the accuracy of deep learning applications that process\nsuch video streams. In this work, we introduce AQuA, to protect application\naccuracy against such distorted frames by scoring the level of distortion in\nthe frames. It takes into account the analytical quality of frames, not the\nvisual quality, by learning a novel metric, classifier opinion score, and uses\na lightweight, CNN-based, object-independent feature extractor. AQuA accurately\nscores distortion levels of frames and generalizes to multiple different deep\nlearning applications. When used for filtering poor quality frames at edge, it\nreduces high-confidence errors for analytics applications by 17%. Through\nfiltering, and due to its low overhead (14ms), AQuA can also reduce computation\ntime and average bandwidth usage by 25%.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 16:56:59 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Paul", "Sibendu", ""], ["Drolia", "Utsav", ""], ["Hu", "Y. Charlie", ""], ["Chakradhar", "Srimat T.", ""]]}, {"id": "2101.09781", "submitter": "Luca Guarnera", "authors": "Oliver Giudice (1), Luca Guarnera (1 and 2), Sebastiano Battiato (1\n  and 2) ((1) University of Catania, (2) iCTLab s.r.l. - Spin-off of University\n  of Catania)", "title": "Fighting deepfakes by detecting GAN DCT anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic multimedia contents created through AI technologies, such as\nGenerative Adversarial Networks (GAN), applied to human faces can have serious\nsocial and political consequences. State-of-the-art algorithms employ deep\nneural networks to detect fake contents but, unfortunately, almost all\napproaches appear to be neither generalizable nor explainable. In this paper, a\nnew fast detection method able to discriminate Deepfake images with high\nprecision is proposed. By employing Discrete Cosine Transform (DCT), anomalous\nfrequencies in real and Deepfake image datasets were analyzed. The \\beta\nstatistics inferred by the distribution of AC coefficients have been the key to\nrecognize GAN-engine generated images. The proposed technique has been\nvalidated on pristine high quality images of faces synthesized by different GAN\narchitectures. Experiments carried out show that the method is innovative,\nexceeds the state-of-the-art and also gives many insights in terms of\nexplainability.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 19:45:11 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 13:24:33 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 10:07:55 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Giudice", "Oliver", "", "1 and 2"], ["Guarnera", "Luca", "", "1 and 2"], ["Battiato", "Sebastiano", "", "1\n  and 2"]]}, {"id": "2101.09782", "submitter": "Pramuditha Perera", "authors": "Pramuditha Perera, Vishal Patel", "title": "A Joint Representation Learning and Feature Modeling Approach for\n  One-class Recognition", "comments": "ICPR Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class recognition is traditionally approached either as a representation\nlearning problem or a feature modeling problem. In this work, we argue that\nboth of these approaches have their own limitations; and a more effective\nsolution can be obtained by combining the two. The proposed approach is based\non the combination of a generative framework and a one-class classification\nmethod. First, we learn generative features using the one-class data with a\ngenerative framework. We augment the learned features with the corresponding\nreconstruction errors to obtain augmented features. Then, we qualitatively\nidentify a suitable feature distribution that reduces the redundancy in the\nchosen classifier space. Finally, we force the augmented features to take the\nform of this distribution using an adversarial framework. We test the\neffectiveness of the proposed method on three one-class classification tasks\nand obtain state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 19:51:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Perera", "Pramuditha", ""], ["Patel", "Vishal", ""]]}, {"id": "2101.09793", "submitter": "Pranjal Singh Rajput", "authors": "Pranjal Singh Rajput, Kanya Satis, Sonnya Dellarosa, Wenxuan Huang,\n  Obinna Agba", "title": "cGANs for Cartoon to Real-life Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image-to-image translation is a learning task to establish a visual\nmapping between an input and output image. The task has several variations\ndifferentiated based on the purpose of the translation, such as synthetic to\nreal translation, photo to caricature translation, and many others. The problem\nhas been tackled using different approaches, either through traditional\ncomputer vision methods, as well as deep learning approaches in recent trends.\nOne approach currently deemed popular and effective is using the conditional\ngenerative adversarial network, also known shortly as cGAN. It is adapted to\nperform image-to-image translation tasks with typically two networks: a\ngenerator and a discriminator. This project aims to evaluate the robustness of\nthe Pix2Pix model by applying the Pix2Pix model to datasets consisting of\ncartoonized images. Using the Pix2Pix model, it should be possible to train the\nnetwork to generate real-life images from the cartoonized images.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 20:26:31 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rajput", "Pranjal Singh", ""], ["Satis", "Kanya", ""], ["Dellarosa", "Sonnya", ""], ["Huang", "Wenxuan", ""], ["Agba", "Obinna", ""]]}, {"id": "2101.09798", "submitter": "Xiaoqi Ma", "authors": "Xiaoqi Ma", "title": "Exploring ensembles and uncertainty minimization in denoising networks", "comments": "Semester project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of neural networks has greatly improved the performance in\nvarious computer vision tasks. In the filed of image denoising, convolutional\nneural network based methods such as DnCNN break through the limits of\nclassical methods, achieving better quantitative results. However, the\nepistemic uncertainty existing in neural networks limits further improvements\nin their performance over denoising tasks. Therefore, we develop and study\ndifferent solutions to minimize uncertainty and further improve the removal of\nnoise. From the perspective of ensemble learning, we implement manipulations to\nnoisy images from the point of view of spatial and frequency domains and then\ndenoise them using pre-trained denoising networks. We propose a fusion model\nconsisting of two attention modules, which focus on assigning the proper\nweights to pixels and channels. The experimental results show that our model\nachieves better performance on top of the baseline of regular pre-trained\ndenoising networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 20:48:18 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ma", "Xiaoqi", ""]]}, {"id": "2101.09819", "submitter": "Shubham Shrivastava", "authors": "Edwin Pan and Pankaj Rajak and Shubham Shrivastava", "title": "Meta-Regularization by Enforcing Mutual-Exclusiveness", "comments": "12 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning models have two objectives. First, they need to be able to make\npredictions over a range of task distributions while utilizing only a small\namount of training data. Second, they also need to adapt to new novel unseen\ntasks at meta-test time again by using only a small amount of training data\nfrom that task. It is the second objective where meta-learning models fail for\nnon-mutually exclusive tasks due to task overfitting. Given that guaranteeing\nmutually exclusive tasks is often difficult, there is a significant need for\nregularization methods that can help reduce the impact of task-memorization in\nmeta-learning. For example, in the case of N-way, K-shot classification\nproblems, tasks becomes non-mutually exclusive when the labels associated with\neach task is fixed. Under this design, the model will simply memorize the class\nlabels of all the training tasks, and thus will fail to recognize a new task\n(class) at meta-test time. A direct observable consequence of this memorization\nis that the meta-learning model simply ignores the task-specific training data\nin favor of directly classifying based on the test-data input. In our work, we\npropose a regularization technique for meta-learning models that gives the\nmodel designer more control over the information flow during meta-training. Our\nmethod consists of a regularization function that is constructed by maximizing\nthe distance between task-summary statistics, in the case of black-box models\nand task specific network parameters in the case of optimization based models\nduring meta-training. Our proposed regularization function shows an accuracy\nboost of $\\sim$ $36\\%$ on the Omniglot dataset for 5-way, 1-shot classification\nusing black-box method and for 20-way, 1-shot classification problem using\noptimization-based method.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 22:57:19 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pan", "Edwin", ""], ["Rajak", "Pankaj", ""], ["Shrivastava", "Shubham", ""]]}, {"id": "2101.09825", "submitter": "Guillaume Lagrange", "authors": "Nathaniel Simard and Guillaume Lagrange", "title": "Improving Few-Shot Learning with Auxiliary Self-Supervised Pretext Tasks", "comments": "Research project report for graduate class IFT 6268-A2020 on\n  Self-supervised Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work on few-shot learning \\cite{tian2020rethinking} showed that\nquality of learned representations plays an important role in few-shot\nclassification performance. On the other hand, the goal of self-supervised\nlearning is to recover useful semantic information of the data without the use\nof class labels. In this work, we exploit the complementarity of both paradigms\nvia a multi-task framework where we leverage recent self-supervised methods as\nauxiliary tasks. We found that combining multiple tasks is often beneficial,\nand that solving them simultaneously can be done efficiently. Our results\nsuggest that self-supervised auxiliary tasks are effective data-dependent\nregularizers for representation learning. Our code is available at:\n\\url{https://github.com/nathanielsimard/improving-fs-ssl}.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 23:21:43 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Simard", "Nathaniel", ""], ["Lagrange", "Guillaume", ""]]}, {"id": "2101.09840", "submitter": "Yurong Guo", "authors": "Yurong Guo, Zhanyu Ma, Xiaoxu Li, and Yuan Dong", "title": "ATRM: Attention-based Task-level Relation Module for GNN-based Few-shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph neural networks (GNNs) have shown powerful ability to handle\nfew-shot classification problem, which aims at classifying unseen samples when\ntrained with limited labeled samples per class. GNN-based few-shot learning\narchitectures mostly replace traditional metric with a learnable GNN. In the\nGNN, the nodes are set as the samples embedding, and the relationship between\ntwo connected nodes can be obtained by a network, the input of which is the\ndifference of their embedding features. We consider this method of measuring\nrelation of samples only models the sample-to-sample relation, while neglects\nthe specificity of different tasks. That is, this method of measuring relation\ndoes not take the task-level information into account. To this end, we propose\na new relation measure method, namely the attention-based task-level relation\nmodule (ATRM), to explicitly model the task-level relation of one sample to all\nthe others. The proposed module captures the relation representations between\nnodes by considering the sample-to-task instead of sample-to-sample embedding\nfeatures. We conducted extensive experiments on four benchmark datasets:\nmini-ImageNet, tiered-ImageNet, CUB-200-2011, and CIFAR-FS. Experimental\nresults demonstrate that the proposed module is effective for GNN-based\nfew-shot learning.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 00:53:04 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Guo", "Yurong", ""], ["Ma", "Zhanyu", ""], ["Li", "Xiaoxu", ""], ["Dong", "Yuan", ""]]}, {"id": "2101.09848", "submitter": "Chen Zhao", "authors": "Chen Zhao, Haipeng Tang, Daniel McGonigle, Zhuo He, Chaoyang Zhang,\n  Yu-Ping Wang, Hong-Wen Deng, Robert Bober, Weihua Zhou", "title": "A new approach to extracting coronary arteries and detecting stenosis in\n  invasive coronary angiograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stable coronary artery disease (CAD), reduction in mortality and/or\nmyocardial infarction with revascularization over medical therapy has not been\nreliably achieved. Coronary arteries are usually extracted to perform stenosis\ndetection. We aim to develop an automatic algorithm by deep learning to extract\ncoronary arteries from ICAs.In this study, a multi-input and multi-scale (MIMS)\nU-Net with a two-stage recurrent training strategy was proposed for the\nautomatic vessel segmentation. Incorporating features such as the Inception\nresidual module with depth-wise separable convolutional layers, the proposed\nmodel generated a refined prediction map with the following two training\nstages: (i) Stage I coarsely segmented the major coronary arteries from\npre-processed single-channel ICAs and generated the probability map of vessels;\n(ii) during the Stage II, a three-channel image consisting of the original\npreprocessed image, a generated probability map, and an edge-enhanced image\ngenerated from the preprocessed image was fed to the proposed MIMS U-Net to\nproduce the final segmentation probability map. During the training stage, the\nprobability maps were iteratively and recurrently updated by feeding into the\nneural network. After segmentation, an arterial stenosis detection algorithm\nwas developed to extract vascular centerlines and calculate arterial diameters\nto evaluate stenotic level. Experimental results demonstrated that the proposed\nmethod achieved an average Dice score of 0.8329, an average sensitivity of\n0.8281, and an average specificity of 0.9979 in our dataset with 294 ICAs\nobtained from 73 patient. Moreover, our stenosis detection algorithm achieved a\ntrue positive rate of 0.6668 and a positive predictive value of 0.7043.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 01:48:27 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhao", "Chen", ""], ["Tang", "Haipeng", ""], ["McGonigle", "Daniel", ""], ["He", "Zhuo", ""], ["Zhang", "Chaoyang", ""], ["Wang", "Yu-Ping", ""], ["Deng", "Hong-Wen", ""], ["Bober", "Robert", ""], ["Zhou", "Weihua", ""]]}, {"id": "2101.09858", "submitter": "Gnana Praveen Rajasekar", "authors": "Gnana Praveen R, Eric Granger, Patrick Cardinal", "title": "Weakly Supervised Learning for Facial Behavior Analysis : A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, there has been a shift in facial behavior analysis from\nthe laboratory-controlled conditions to the challenging in-the-wild conditions\ndue to the superior performance of deep learning based approaches for many real\nworld applications.However, the performance of deep learning approaches relies\non the amount of training data. One of the major problems with data acquisition\nis the requirement of annotations for large amount of training data. Labeling\nprocess of huge training data demands lot of human support with strong domain\nexpertise for facial expressions or action units, which is difficult to obtain\nin real-time environments.Moreover, labeling process is highly vulnerable to\nambiguity of expressions or action units, especially for intensities due to the\nbias induced by the domain experts. Therefore, there is an imperative need to\naddress the problem of facial behavior analysis with weak annotations. In this\npaper, we provide a comprehensive review of weakly supervised learning (WSL)\napproaches for facial behavior analysis with both categorical as well as\ndimensional labels along with the challenges and potential research directions\nassociated with it. First, we introduce various types of weak annotations in\nthe context of facial behavior analysis and the corresponding challenges\nassociated with it. We then systematically review the existing state-of-the-art\napproaches and provide a taxonomy of these approaches along with their insights\nand limitations. In addition, widely used data-sets in the reviewed literature\nand the performance of these approaches along with evaluation principles are\nsummarized. Finally, we discuss the remaining challenges and opportunities\nalong with the potential research directions in order to apply facial behavior\nanalysis with weak labels in real life situations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:31:49 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["R", "Gnana Praveen", ""], ["Granger", "Eric", ""], ["Cardinal", "Patrick", ""]]}, {"id": "2101.09864", "submitter": "Wang Bo", "authors": "Tao Li and Wang Bo and Chunyu Hu and Hong Kang and Hanruo Liu and Kai\n  Wang and Huazhu Fu", "title": "Applications of Deep Learning in Fundus Images: A Review", "comments": null, "journal-ref": "Medical Image Analysis 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The use of fundus images for the early screening of eye diseases is of great\nclinical importance. Due to its powerful performance, deep learning is becoming\nmore and more popular in related applications, such as lesion segmentation,\nbiomarkers segmentation, disease diagnosis and image synthesis. Therefore, it\nis very necessary to summarize the recent developments in deep learning for\nfundus images with a review paper. In this review, we introduce 143 application\npapers with a carefully designed hierarchy. Moreover, 33 publicly available\ndatasets are presented. Summaries and analyses are provided for each task.\nFinally, limitations common to all tasks are revealed and possible solutions\nare given. We will also release and regularly update the state-of-the-art\nresults and newly-released datasets at https://github.com/nkicsl/Fundus Review\nto adapt to the rapid development of this field.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:39:40 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Li", "Tao", ""], ["Bo", "Wang", ""], ["Hu", "Chunyu", ""], ["Kang", "Hong", ""], ["Liu", "Hanruo", ""], ["Wang", "Kai", ""], ["Fu", "Huazhu", ""]]}, {"id": "2101.09865", "submitter": "Yufei Wang", "authors": "Yufei Wang and Ian D. Wood and Stephen Wan and Mark Johnson", "title": "ECOL-R: Encouraging Copying in Novel Object Captioning with\n  Reinforcement Learning", "comments": "long paper accepted @ EACL-2021 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Novel Object Captioning is a zero-shot Image Captioning task requiring\ndescribing objects not seen in the training captions, but for which information\nis available from external object detectors. The key challenge is to select and\ndescribe all salient detected novel objects in the input images. In this paper,\nwe focus on this challenge and propose the ECOL-R model (Encouraging Copying of\nObject Labels with Reinforced Learning), a copy-augmented transformer model\nthat is encouraged to accurately describe the novel object labels. This is\nachieved via a specialised reward function in the SCST reinforcement learning\nframework (Rennie et al., 2017) that encourages novel object mentions while\nmaintaining the caption quality. We further restrict the SCST training to the\nimages where detected objects are mentioned in reference captions to train the\nECOL-R model. We additionally improve our copy mechanism via Abstract Labels,\nwhich transfer knowledge from known to novel object types, and a Morphological\nSelector, which determines the appropriate inflected forms of novel object\nlabels. The resulting model sets new state-of-the-art on the nocaps (Agrawal et\nal., 2019) and held-out COCO (Hendricks et al., 2016) benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:41:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Yufei", ""], ["Wood", "Ian D.", ""], ["Wan", "Stephen", ""], ["Johnson", "Mark", ""]]}, {"id": "2101.09866", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Yi Yang, Shih-En Wei, Xinshuo Weng, Yaser Sheikh, Shoou-I\n  Yu", "title": "Supervision by Registration and Triangulation for Landmark Detection", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2020", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2983935", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Supervision by Registration and Triangulation (SRT), an\nunsupervised approach that utilizes unlabeled multi-view video to improve the\naccuracy and precision of landmark detectors. Being able to utilize unlabeled\ndata enables our detectors to learn from massive amounts of unlabeled data\nfreely available and not be limited by the quality and quantity of manual human\nannotations. To utilize unlabeled data, there are two key observations: (1) the\ndetections of the same landmark in adjacent frames should be coherent with\nregistration, i.e., optical flow. (2) the detections of the same landmark in\nmultiple synchronized and geometrically calibrated views should correspond to a\nsingle 3D point, i.e., multi-view consistency. Registration and multi-view\nconsistency are sources of supervision that do not require manual labeling,\nthus it can be leveraged to augment existing training data during detector\ntraining. End-to-end training is made possible by differentiable registration\nand 3D triangulation modules. Experiments with 11 datasets and a newly proposed\nmetric to measure precision demonstrate accuracy and precision improvements in\nlandmark detection on both images and video. Code is available at\nhttps://github.com/D-X-Y/landmark-detection.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:48:21 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Dong", "Xuanyi", ""], ["Yang", "Yi", ""], ["Wei", "Shih-En", ""], ["Weng", "Xinshuo", ""], ["Sheikh", "Yaser", ""], ["Yu", "Shoou-I", ""]]}, {"id": "2101.09870", "submitter": "Shi Guo", "authors": "Shi Guo, Zhetong Liang, Lei Zhang", "title": "Joint Denoising and Demosaicking with Green Channel Prior for Real-world\n  Burst Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising and demosaicking are essential yet correlated steps to reconstruct\na full color image from the raw color filter array (CFA) data. By learning a\ndeep convolutional neural network (CNN), significant progress has been achieved\nto perform denoising and demosaicking jointly. However, most existing CNN-based\njoint denoising and demosaicking (JDD) methods work on a single image while\nassuming additive white Gaussian noise, which limits their performance on\nreal-world applications. In this work, we study the JDD problem for real-world\nburst images, namely JDD-B. Considering the fact that the green channel has\ntwice the sampling rate and better quality than the red and blue channels in\nCFA raw data, we propose to use this green channel prior (GCP) to build a\nGCP-Net for the JDD-B task. In GCP-Net, the GCP features extracted from green\nchannels are utilized to guide the feature extraction and feature upsampling of\nthe whole image. To compensate for the shift between frames, the offset is also\nestimated from GCP features to reduce the impact of noise. Our GCP-Net can\npreserve more image structures and details than other JDD methods while\nremoving noise. Experiments on synthetic and real-world noisy images\ndemonstrate the effectiveness of GCP-Net quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 03:08:25 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Guo", "Shi", ""], ["Liang", "Zhetong", ""], ["Zhang", "Lei", ""]]}, {"id": "2101.09892", "submitter": "Hongxin Xiang", "authors": "Cheng Xie, Hongxin Xiang, Ting Zeng, Yun Yang, Beibei Yu and Qing Liu", "title": "Cross Knowledge-based Generative Zero-Shot Learning Approach with\n  Taxonomy Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although zero-shot learning (ZSL) has an inferential capability of\nrecognizing new classes that have never been seen before, it always faces two\nfundamental challenges of the cross modality and crossdomain challenges. In\norder to alleviate these problems, we develop a generative network-based ZSL\napproach equipped with the proposed Cross Knowledge Learning (CKL) scheme and\nTaxonomy Regularization (TR). In our approach, the semantic features are taken\nas inputs, and the output is the synthesized visual features generated from the\ncorresponding semantic features. CKL enables more relevant semantic features to\nbe trained for semantic-to-visual feature embedding in ZSL, while Taxonomy\nRegularization (TR) significantly improves the intersections with unseen images\nwith more generalized visual features generated from generative network.\nExtensive experiments on several benchmark datasets (i.e., AwA1, AwA2, CUB, NAB\nand aPY) show that our approach is superior to these state-of-the-art methods\nin terms of ZSL image classification and retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 04:38:18 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Xie", "Cheng", ""], ["Xiang", "Hongxin", ""], ["Zeng", "Ting", ""], ["Yang", "Yun", ""], ["Yu", "Beibei", ""], ["Liu", "Qing", ""]]}, {"id": "2101.09895", "submitter": "Jae-Yeul Kim", "authors": "Jae-Yeul Kim, Jong-Eun Ha", "title": "Spatio-temporal Data Augmentation for Visual Surveillance", "comments": "22 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual surveillance aims to stably detect a foreground object using a\ncontinuous image acquired from a fixed camera. Recent deep learning methods\nbased on supervised learning show superior performance compared to classical\nbackground subtraction algorithms. However, there is still a room for\nimprovement in static foreground, dynamic background, hard shadow, illumination\nchanges, camouflage, etc. In addition, most of the deep learning-based methods\noperates well on environments similar to training. If the testing environments\nare different from training ones, their performance degrades. As a result,\nadditional training on those operating environments is required to ensure a\ngood performance. Our previous work which uses spatio-temporal input data\nconsisted of a number of past images, background images and current image\nshowed promising results in different environments from training, although it\nuses a simple U-NET structure. In this paper, we propose a data augmentation\ntechnique suitable for visual surveillance for additional performance\nimprovement using the same network used in our previous work. In deep learning,\nmost data augmentation techniques deal with spatial-level data augmentation\ntechniques for use in image classification and object detection. In this paper,\nwe propose a new method of data augmentation in the spatio-temporal dimension\nsuitable for our previous work. Two data augmentation methods of adjusting\nbackground model images and past images are proposed. Through this, it is shown\nthat performance can be improved in difficult areas such as static foreground\nand ghost objects, compared to previous studies. Through quantitative and\nqualitative evaluation using SBI, LASIESTA, and our own dataset, we show that\nit gives superior performance compared to deep learning-based algorithms and\nbackground subtraction algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 04:55:10 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 03:39:15 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 02:58:00 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kim", "Jae-Yeul", ""], ["Ha", "Jong-Eun", ""]]}, {"id": "2101.09899", "submitter": "Jing Xu", "authors": "Jing Xu, Tszhang Guo, Yong Xu, Zenglin Xu, Kun Bai", "title": "MultiFace: A Generic Training Mechanism for Boosting Face Recognition\n  Performance", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) and their variants have been\nwidely used in large scale face recognition(FR) recently. Existing methods have\nachieved good performance on many FR benchmarks. However, most of them suffer\nfrom two major problems. First, these methods converge quite slowly since they\noptimize the loss functions in a high-dimensional and sparse Gaussian Sphere.\nSecond, the high dimensionality of features, despite the powerful descriptive\nability, brings difficulty to the optimization, which may lead to a sub-optimal\nlocal optimum. To address these problems, we propose a simple yet efficient\ntraining mechanism called MultiFace, where we approximate the original\nhigh-dimensional features by the ensemble of low-dimensional features. The\nproposed mechanism is also generic and can be easily applied to many advanced\nFR models. Moreover, it brings the benefits of good interpretability to FR\nmodels via the clustering effect. In detail, the ensemble of these\nlow-dimensional features can capture complementary yet discriminative\ninformation, which can increase the intra-class compactness and inter-class\nseparability. Experimental results show that the proposed mechanism can\naccelerate 2-3 times with the softmax loss and 1.2-1.5 times with Arcface or\nCosface, while achieving state-of-the-art performances in several benchmark\ndatasets. Especially, the significant improvements on large-scale\ndatasets(e.g., IJB and MageFace) demonstrate the flexibility of our new\ntraining mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 05:18:51 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 13:05:46 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 06:08:49 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Xu", "Jing", ""], ["Guo", "Tszhang", ""], ["Xu", "Yong", ""], ["Xu", "Zenglin", ""], ["Bai", "Kun", ""]]}, {"id": "2101.09903", "submitter": "Weixin Jiang", "authors": "Weixin Jiang, Eric Schwenker, Trevor Spreadbury, Nicola Ferrier, Maria\n  K.Y. Chan, Oliver Cossairt", "title": "A Two-stage Framework for Compound Figure Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific literature contains large volumes of complex, unstructured figures\nthat are compound in nature (i.e. composed of multiple images, graphs, and\ndrawings). Separation of these compound figures is critical for information\nretrieval from these figures. In this paper, we propose a new strategy for\ncompound figure separation, which decomposes the compound figures into\nconstituent subfigures while preserving the association between the subfigures\nand their respective caption components. We propose a two-stage framework to\naddress the proposed compound figure separation problem. In particular, the\nsubfigure label detection module detects all subfigure labels in the first\nstage. Then, in the subfigure detection module, the detected subfigure labels\nhelp to detect the subfigures by optimizing the feature selection process and\nproviding the global layout information as extra features. Extensive\nexperiments are conducted to validate the effectiveness and superiority of the\nproposed framework, which improves the detection precision by 9%.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 05:43:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jiang", "Weixin", ""], ["Schwenker", "Eric", ""], ["Spreadbury", "Trevor", ""], ["Ferrier", "Nicola", ""], ["Chan", "Maria K. Y.", ""], ["Cossairt", "Oliver", ""]]}, {"id": "2101.09915", "submitter": "Hong-Gyu Jung", "authors": "Hyun-Woo Kim, Hong-Gyu Jung, Seong-Whan Lee", "title": "Weakly Supervised Thoracic Disease Localization via Disease Masks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable a deep learning-based system to be used in the medical domain as a\ncomputer-aided diagnosis system, it is essential to not only classify diseases\nbut also present the locations of the diseases. However, collecting\ninstance-level annotations for various thoracic diseases is expensive.\nTherefore, weakly supervised localization methods have been proposed that use\nonly image-level annotation. While the previous methods presented the disease\nlocation as the most discriminative part for classification, this causes a deep\nnetwork to localize wrong areas for indistinguishable X-ray images. To solve\nthis issue, we propose a spatial attention method using disease masks that\ndescribe the areas where diseases mainly occur. We then apply the spatial\nattention to find the precise disease area by highlighting the highest\nprobability of disease occurrence. Meanwhile, the various sizes, rotations and\nnoise in chest X-ray images make generating the disease masks challenging. To\nreduce the variation among images, we employ an alignment module to transform\nan input X-ray image into a generalized image. Through extensive experiments on\nthe NIH-Chest X-ray dataset with eight kinds of diseases, we show that the\nproposed method results in superior localization performances compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 06:52:57 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kim", "Hyun-Woo", ""], ["Jung", "Hong-Gyu", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2101.09976", "submitter": "Keno Bressem", "authors": "Keno K. Bressem, Stefan M. Niehues, Bernd Hamm, Marcus R. Makowski,\n  Janis L. Vahldiek, Lisa C. Adams", "title": "3D U-Net for segmentation of COVID-19 associated pulmonary infiltrates\n  using transfer learning: State-of-the-art results on affordable hardware", "comments": "8 Pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Segmentation of pulmonary infiltrates can help assess severity of COVID-19,\nbut manual segmentation is labor and time-intensive. Using neural networks to\nsegment pulmonary infiltrates would enable automation of this task. However,\ntraining a 3D U-Net from computed tomography (CT) data is time- and\nresource-intensive. In this work, we therefore developed and tested a solution\non how transfer learning can be used to train state-of-the-art segmentation\nmodels on limited hardware and in shorter time. We use the recently published\nRSNA International COVID-19 Open Radiology Database (RICORD) to train a fully\nthree-dimensional U-Net architecture using an 18-layer 3D ResNet, pretrained on\nthe Kinetics-400 dataset as encoder. The generalization of the model was then\ntested on two openly available datasets of patients with COVID-19, who received\nchest CTs (Corona Cases and MosMed datasets). Our model performed comparable to\npreviously published 3D U-Net architectures, achieving a mean Dice score of\n0.679 on the tuning dataset, 0.648 on the Coronacases dataset and 0.405 on the\nMosMed dataset. Notably, these results were achieved with shorter training time\non a single GPU with less memory available than the GPUs used in previous\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:37:32 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Bressem", "Keno K.", ""], ["Niehues", "Stefan M.", ""], ["Hamm", "Bernd", ""], ["Makowski", "Marcus R.", ""], ["Vahldiek", "Janis L.", ""], ["Adams", "Lisa C.", ""]]}, {"id": "2101.09978", "submitter": "Tianming Zhao", "authors": "Tianming Zhao (1), Chunyang Chen (2), Yuanning Liu (1), Xiaodong Zhu\n  (1) ((1) Jilin University, (2) Monash University)", "title": "GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial\n  Networks", "comments": "13 pages, 10 figures, accepted for publication at ICSE2021 Technical\n  Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical User Interface (GUI) is ubiquitous in almost all modern desktop\nsoftware, mobile applications, and online websites. A good GUI design is\ncrucial to the success of the software in the market, but designing a good GUI\nwhich requires much innovation and creativity is difficult even to well-trained\ndesigners. Besides, the requirement of the rapid development of GUI design also\naggravates designers' working load. So, the availability of various automated\ngenerated GUIs can help enhance the design personalization and specialization\nas they can cater to the taste of different designers. To assist designers, we\ndevelop a model GUIGAN to automatically generate GUI designs. Different from\nconventional image generation models based on image pixels, our GUIGAN is to\nreuse GUI components collected from existing mobile app GUIs for composing a\nnew design that is similar to natural-language generation. Our GUIGAN is based\non SeqGAN by modeling the GUI component style compatibility and GUI structure.\nThe evaluation demonstrates that our model significantly outperforms the best\nof the baseline methods by 30.77% in Frechet Inception distance (FID) and\n12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we\nprovide initial evidence of the usefulness of our approach for generating\nacceptable brand new GUI designs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:42:58 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 04:42:42 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhao", "Tianming", "", "Jilin University"], ["Chen", "Chunyang", "", "Monash University"], ["Liu", "Yuanning", "", "Jilin University"], ["Zhu", "Xiaodong", "", "Jilin University"]]}, {"id": "2101.09983", "submitter": "Stanislav Frolov", "authors": "Stanislav Frolov, Tobias Hinz, Federico Raue, J\\\"orn Hees, Andreas\n  Dengel", "title": "Adversarial Text-to-Image Synthesis: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the advent of generative adversarial networks, synthesizing images from\ntextual descriptions has recently become an active research area. It is a\nflexible and intuitive way for conditional image generation with significant\nprogress in the last years regarding visual realism, diversity, and semantic\nalignment. However, the field still faces several challenges that require\nfurther research efforts such as enabling the generation of high-resolution\nimages with multiple objects, and developing suitable and reliable evaluation\nmetrics that correlate with human judgement. In this review, we contextualize\nthe state of the art of adversarial text-to-image synthesis models, their\ndevelopment since their inception five years ago, and propose a taxonomy based\non the level of supervision. We critically examine current strategies to\nevaluate text-to-image synthesis models, highlight shortcomings, and identify\nnew areas of research, ranging from the development of better datasets and\nevaluation metrics to possible improvements in architectural design and model\ntraining. This review complements previous surveys on generative adversarial\nnetworks with a focus on text-to-image synthesis which we believe will help\nresearchers to further advance the field.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:58:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Frolov", "Stanislav", ""], ["Hinz", "Tobias", ""], ["Raue", "Federico", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "2101.09987", "submitter": "Erkan Bostanci", "authors": "K. E. Sengun, Y. T. Cetin, M.S Guzel, S. Can and E. Bostanci", "title": "Automatic Liver Segmentation from CT Images Using Deep Learning\n  Algorithms: A Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging has been employed to support medical diagnosis and treatment.\nIt may also provide crucial information to surgeons to facilitate optimal\nsurgical preplanning and perioperative management. Essentially, semi-automatic\norgan and tumor segmentation has been studied by many researchers. Recently,\nwith the development of Deep Learning (DL) algorithms, automatic organ\nsegmentation has been gathered lots of attention from the researchers. This\npaper addresses to propose the most efficient DL architectures for Liver\nsegmentation by adapting and comparing state-of-the-art DL frameworks, studied\nin different disciplines. These frameworks are implemented and adapted into a\nCommercial software, 'LiverVision'. It is aimed to reveal the most effective\nand accurate DL architecture for fully automatic liver segmentation. Equal\nconditions were provided to all architectures in the experiments so as to\nmeasure the effectiveness of algorithms accuracy, and Dice coefficient metrics\nwere also employed to support comparative analysis. Experimental results prove\nthat 'U-Net' and 'SegNet' have been superior in line with the experiments\nconducted considering the concepts of time, cost, and effectiveness.\nConsidering both architectures, 'SegNet' was observed to be more successful in\neliminating false-positive values. Besides, it was seen that the accuracy\nmetric used to measure effectiveness in image segmentation alone was not\nenough. Results reveal that DL algorithms are able to automate organ\nsegmentation from DICOM images with high accuracy. This contribution is\ncritical for surgical preplanning and motivates author to apply this approach\nto the different organs and field of medicine.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:05:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sengun", "K. E.", ""], ["Cetin", "Y. T.", ""], ["Guzel", "M. S", ""], ["Can", "S.", ""], ["Bostanci", "E.", ""]]}, {"id": "2101.09991", "submitter": "Carlo Alberto Barbano", "authors": "Carlo Alberto Barbano, Daniele Perlo, Enzo Tartaglione, Attilio\n  Fiandrotti, Luca Bertero, Paola Cassoni, Marco Grangetto", "title": "UniToPatho, a labeled histopathological dataset for colorectal polyps\n  classification and adenoma dysplasia grading", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Histopathological characterization of colorectal polyps allows to tailor\npatients' management and follow up with the ultimate aim of avoiding or\npromptly detecting an invasive carcinoma. Colorectal polyps characterization\nrelies on the histological analysis of tissue samples to determine the polyps\nmalignancy and dysplasia grade. Deep neural networks achieve outstanding\naccuracy in medical patterns recognition, however they require large sets of\nannotated training images. We introduce UniToPatho, an annotated dataset of\n9536 hematoxylin and eosin (H&E) stained patches extracted from 292 whole-slide\nimages, meant for training deep neural networks for colorectal polyps\nclassification and adenomas grading. We present our dataset and provide\ninsights on how to tackle the problem of automatic colorectal polyps\ncharacterization.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:18:44 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 09:23:19 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Barbano", "Carlo Alberto", ""], ["Perlo", "Daniele", ""], ["Tartaglione", "Enzo", ""], ["Fiandrotti", "Attilio", ""], ["Bertero", "Luca", ""], ["Cassoni", "Paola", ""], ["Grangetto", "Marco", ""]]}, {"id": "2101.09992", "submitter": "Nadia Brancati", "authors": "Nadia Brancati, Giuseppe De Pietro, Daniel Riccio, Maria Frucci", "title": "Gigapixel Histopathological Image Analysis using Attention-based Neural\n  Networks", "comments": "The manuscript was submitted to a peer-review journal on January 27th", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3086892", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although CNNs are widely considered as the state-of-the-art models in various\napplications of image analysis, one of the main challenges still open is the\ntraining of a CNN on high resolution images. Different strategies have been\nproposed involving either a rescaling of the image or an individual processing\nof parts of the image. Such strategies cannot be applied to images, such as\ngigapixel histopathological images, for which a high reduction in resolution\ninherently effects a loss of discriminative information, and in respect of\nwhich the analysis of single parts of the image suffers from a lack of global\ninformation or implies a high workload in terms of annotating the training\nimages in such a way as to select significant parts. We propose a method for\nthe analysis of gigapixel histopathological images solely by using weak\nimage-level labels. In particular, two analysis tasks are taken into account: a\nbinary classification and a prediction of the tumor proliferation score. Our\nmethod is based on a CNN structure consisting of a compressing path and a\nlearning path. In the compressing path, the gigapixel image is packed into a\ngrid-based feature map by using a residual network devoted to the feature\nextraction of each patch into which the image has been divided. In the learning\npath, attention modules are applied to the grid-based feature map, taking into\naccount spatial correlations of neighboring patch features to find regions of\ninterest, which are then used for the final whole slide analysis. Our method\nintegrates both global and local information, is flexible with regard to the\nsize of the input images and only requires weak image-level labels. Comparisons\nwith different methods of the state-of-the-art on two well known datasets,\nCamelyon16 and TUPAC16, have been made to confirm the validity of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:18:52 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 16:49:24 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Brancati", "Nadia", ""], ["De Pietro", "Giuseppe", ""], ["Riccio", "Daniel", ""], ["Frucci", "Maria", ""]]}, {"id": "2101.10011", "submitter": "Sebastian K\\\"ohler", "authors": "Sebastian K\\\"ohler, Giulio Lovisotto, Simon Birnbach, Richard Baker,\n  Ivan Martinovic", "title": "They See Me Rollin': Inherent Vulnerability of the Rolling Shutter in\n  CMOS Image Sensors", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras have become a fundamental component of vision-based intelligent\nsystems. As a balance between production costs and image quality, most modern\ncameras use Complementary Metal-Oxide Semiconductor image sensors that\nimplement an electronic rolling shutter mechanism, where image rows are\ncaptured consecutively rather than all-at-once.\n  In this paper, we describe how the electronic rolling shutter can be\nexploited using a bright, modulated light source (e.g., an inexpensive,\noff-the-shelf laser), to inject fine-grained image disruptions. These\ndisruptions substantially affect camera-based computer vision systems, where\nhigh-frequency data is crucial in extracting informative features from objects.\n  We study the fundamental factors affecting a rolling shutter attack, such as\nenvironmental conditions, angle of the incident light, laser to camera\ndistance, and aiming precision. We demonstrate how these factors affect the\nintensity of the injected distortion and how an adversary can take them into\naccount by modeling the properties of the camera. We introduce a general\npipeline of a practical attack, which consists of: (i) profiling several\nproperties of the target camera and (ii) partially simulating the attack to\nfind distortions that satisfy the adversary's goal. Then, we instantiate the\nattack to the scenario of object detection, where the adversary's goal is to\nmaximally disrupt the detection of objects in the image. We show that the\nadversary can modulate the laser to hide up to 75% of objects perceived by\nstate-of-the-art detectors while controlling the amount of perturbation to keep\nthe attack inconspicuous. Our results indicate that rolling shutter attacks can\nsubstantially reduce the performance and reliability of vision-based\nintelligent systems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 11:14:25 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["K\u00f6hler", "Sebastian", ""], ["Lovisotto", "Giulio", ""], ["Birnbach", "Simon", ""], ["Baker", "Richard", ""], ["Martinovic", "Ivan", ""]]}, {"id": "2101.10015", "submitter": "Kai Han", "authors": "Yunhe Wang, Mingqiang Huang, Kai Han, Hanting Chen, Wei Zhang,\n  Chunjing Xu, Dacheng Tao", "title": "AdderNet and its Minimalist Hardware Design for Energy-Efficient\n  Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have been widely used for boosting the\nperformance of many machine intelligence tasks. However, the CNN models are\nusually computationally intensive and energy consuming, since they are often\ndesigned with numerous multiply-operations and considerable parameters for the\naccuracy reason. Thus, it is difficult to directly apply them in the\nresource-constrained environments such as 'Internet of Things' (IoT) devices\nand smart phones. To reduce the computational complexity and energy burden,\nhere we present a novel minimalist hardware architecture using adder\nconvolutional neural network (AdderNet), in which the original convolution is\nreplaced by adder kernel using only additions. To maximally excavate the\npotential energy consumption, we explore the low-bit quantization algorithm for\nAdderNet with shared-scaling-factor method, and we design both specific and\ngeneral-purpose hardware accelerators for AdderNet. Experimental results show\nthat the adder kernel with int8/int16 quantization also exhibits high\nperformance, meanwhile consuming much less resources (theoretically ~81% off).\nIn addition, we deploy the quantized AdderNet on FPGA (Field Programmable Gate\nArray) platform. The whole AdderNet can practically achieve 16% enhancement in\nspeed, 67.6%-71.4% decrease in logic resource utilization and 47.85%-77.9%\ndecrease in power consumption compared to CNN under the same circuit\narchitecture. With a comprehensive comparison on the performance, power\nconsumption, hardware resource consumption and network generalization\ncapability, we conclude the AdderNet is able to surpass all the other\ncompetitors including the classical CNN, novel memristor-network, XNOR-Net and\nthe shift-kernel based network, indicating its great potential in future high\nperformance and energy-efficient artificial intelligence applications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 11:31:52 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 06:48:54 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Wang", "Yunhe", ""], ["Huang", "Mingqiang", ""], ["Han", "Kai", ""], ["Chen", "Hanting", ""], ["Zhang", "Wei", ""], ["Xu", "Chunjing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2101.10027", "submitter": "Tuan Anh Bui", "authors": "Anh Bui, Trung Le, He Zhao, Paul Montague, Seyit Camtepe, Dinh Phung", "title": "Understanding and Achieving Efficient Robustness with Adversarial\n  Supervised Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Contrastive learning (CL) has recently emerged as an effective approach to\nlearning representation in a range of downstream tasks. Central to this\napproach is the selection of positive (similar) and negative (dissimilar) sets\nto provide the model the opportunity to `contrast' between data and class\nrepresentation in the latent space. In this paper, we investigate CL for\nimproving model robustness using adversarial samples. We first designed and\nperformed a comprehensive study to understand how adversarial vulnerability\nbehaves in the latent space. Based on these empirical evidences, we propose an\neffective and efficient supervised contrastive learning to achieve model\nrobustness against adversarial attacks. Moreover, we propose a new sample\nselection strategy that optimizes the positive/negative sets by removing\nredundancy and improving correlation with the anchor. Experiments conducted on\nbenchmark datasets show that our Adversarial Supervised Contrastive Learning\n(ASCL) approach outperforms the state-of-the-art defenses by $2.6\\%$ in terms\nof the robust accuracy, whilst our ASCL with the proposed selection strategy\ncan further gain $1.4\\%$ improvement with only $42.8\\%$ positives and $6.3\\%$\nnegatives compared with ASCL without a selection strategy.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 11:57:52 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 03:46:14 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bui", "Anh", ""], ["Le", "Trung", ""], ["Zhao", "He", ""], ["Montague", "Paul", ""], ["Camtepe", "Seyit", ""], ["Phung", "Dinh", ""]]}, {"id": "2101.10030", "submitter": "Yu Tian", "authors": "Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W.\n  Verjans, Gustavo Carneiro", "title": "Weakly-supervised Video Anomaly Detection with Robust Temporal Feature\n  Magnitude Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection with weakly supervised video-level labels is typically\nformulated as a multiple instance learning (MIL) problem, in which we aim to\nidentify snippets containing abnormal events, with each video represented as a\nbag of video snippets. Although current methods show effective detection\nperformance, their recognition of the positive instances, i.e., rare abnormal\nsnippets in the abnormal videos, is largely biased by the dominant negative\ninstances, especially when the abnormal events are subtle anomalies that\nexhibit only small differences compared with normal events. This issue is\nexacerbated in many methods that ignore important video temporal dependencies.\nTo address this issue, we introduce a novel and theoretically sound method,\nnamed Robust Temporal Feature Magnitude learning (RTFM), which trains a feature\nmagnitude learning function to effectively recognise the positive instances,\nsubstantially improving the robustness of the MIL approach to the negative\ninstances from abnormal videos. RTFM also adapts dilated convolutions and\nself-attention mechanisms to capture long- and short-range temporal\ndependencies to learn the feature magnitude more faithfully. Extensive\nexperiments show that the RTFM-enabled MIL model (i) outperforms several\nstate-of-the-art methods by a large margin on three benchmark data sets\n(ShanghaiTech, UCF-Crime and XD-Violence) and (ii) achieves significantly\nimproved subtle anomaly discriminability and sample efficiency. Code is\navailable at https://github.com/tianyu0207/RTFM.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:04:00 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 11:44:05 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tian", "Yu", ""], ["Pang", "Guansong", ""], ["Chen", "Yuanhong", ""], ["Singh", "Rajvinder", ""], ["Verjans", "Johan W.", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2101.10033", "submitter": "Florian Jug", "authors": "Manan Lalit, Pavel Tomancak, Florian Jug", "title": "Embedding-based Instance Segmentation in Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic detection and segmentation of objects in 2D and 3D microscopy data\nis important for countless biomedical applications. In the natural image\ndomain, spatial embedding-based instance segmentation methods are known to\nyield high-quality results, but their utility for segmenting microscopy data is\ncurrently little researched. Here we introduce EmbedSeg, an embedding-based\ninstance segmentation method which outperforms existing state-of-the-art\nbaselines on 2D as well as 3D microscopy datasets. Additionally, we show that\nEmbedSeg has a GPU memory footprint small enough to train even on laptop GPUs,\nmaking it accessible to virtually everyone. Finally, we introduce four new 3D\nmicroscopy datasets, which we make publicly available alongside ground truth\ntraining labels. Our open-source implementation is available at\nhttps://github.com/juglab/EmbedSeg.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:06:44 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 11:13:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Lalit", "Manan", ""], ["Tomancak", "Pavel", ""], ["Jug", "Florian", ""]]}, {"id": "2101.10039", "submitter": "Mansi Sharma", "authors": "Balasubramanyam Appina, Mansi Sharma, Santosh Kumar", "title": "Latent Factor Modeling of Users Subjective Perception for Stereoscopic\n  3D Video Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous stereoscopic 3D movies are released every year to theaters and\ncreated large revenues. Despite the improvement in stereo capturing and 3D\nvideo post-production technology, stereoscopic artifacts which cause viewer\ndiscomfort continue to appear even in high-budget films. Existing automatic 3D\nvideo quality measurement tools can detect distortions in stereoscopic images\nor videos, but they fail to consider the viewer's subjective perception of\nthose artifacts, and how these distortions affect their choices. In this paper,\nwe introduce a novel recommendation system for stereoscopic 3D movies based on\na latent factor model that meticulously analyse the viewer's subjective ratings\nand influence of 3D video distortions on their preferences. To the best of our\nknowledge, this is a first-of-its-kind model that recommends 3D movies based on\nstereo-film quality ratings accounting correlation between the viewer's visual\ndiscomfort and stereoscopic-artifact perception. The proposed model is trained\nand tested on benchmark Nama3ds1-cospad1 and LFOVIAS3DPh2 S3D video quality\nassessment datasets. The experiments revealed that resulting\nmatrix-factorization based recommendation system is able to generalize\nconsiderably better for the viewer's subjective ratings.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:13:32 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Appina", "Balasubramanyam", ""], ["Sharma", "Mansi", ""], ["Kumar", "Santosh", ""]]}, {"id": "2101.10043", "submitter": "Yu Tian", "authors": "Yuanhong Chen, Yu Tian, Guansong Pang, Gustavo Carneiro", "title": "Unsupervised Anomaly Detection with Multi-scale Interpolated Gaussian\n  Descriptors", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current unsupervised anomaly detection and pixel-wise anomaly localisation\nsystems are commonly formulated as one-class classifiers that depend on an\neffective estimation of the distribution of normal images and robust criteria\nto identify anomalies. However, the distribution of normal images estimated by\ncurrent systems tends to be unstable for classes of normal images that are\nunder-represented in the training set, and the anomaly identification criteria\ncommonly explored in the field does not work well for multi-scale structural\nand non-structural anomalies. In this paper, we introduce a new unsupervised\nanomaly detection and localisation method designed to address these two issues.\nMore specifically, we introduce a normal image distribution estimation method\nthat is robust to under-represented classes of normal images -- this method is\nbased on adversarially interpolated descriptors from training images and a\nGaussian classifier. We also propose a new anomaly identification criterion\nthat can accurately detect and localise multi-scale structural and\nnon-structural anomalies. In extensive experiments on MNIST, Fashion MNIST,\nCIFAR10, MVTec AD and two medical datasets, our approach shows better results\nthan the current state of the art in the standard experimental setup for\nunsupervised anomaly detection and localisation. Code is available at\nhttps://github.com/tianyu0207/IGD.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:38:51 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 12:05:05 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 03:52:17 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chen", "Yuanhong", ""], ["Tian", "Yu", ""], ["Pang", "Guansong", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2101.10044", "submitter": "Ozan Caglayan", "authors": "Ozan Caglayan, Menekse Kuyu, Mustafa Sercan Amac, Pranava Madhyastha,\n  Erkut Erdem, Aykut Erdem, Lucia Specia", "title": "Cross-lingual Visual Pre-training for Multimodal Machine Translation", "comments": "Accepted to EACL 2021 (Camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models have been shown to improve performance in many\nnatural language tasks substantially. Although the early focus of such models\nwas single language pre-training, recent advances have resulted in\ncross-lingual and visual pre-training methods. In this paper, we combine these\ntwo approaches to learn visually-grounded cross-lingual representations.\nSpecifically, we extend the translation language modelling (Lample and Conneau,\n2019) with masked region classification and perform pre-training with three-way\nparallel vision & language corpora. We show that when fine-tuned for multimodal\nmachine translation, these models obtain state-of-the-art performance. We also\nprovide qualitative insights into the usefulness of the learned grounded\nrepresentations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:46:41 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 19:11:47 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Caglayan", "Ozan", ""], ["Kuyu", "Menekse", ""], ["Amac", "Mustafa Sercan", ""], ["Madhyastha", "Pranava", ""], ["Erdem", "Erkut", ""], ["Erdem", "Aykut", ""], ["Specia", "Lucia", ""]]}, {"id": "2101.10075", "submitter": "Baoliang Chen", "authors": "Baoliang Chen, Wenhan Yang, Haoliang Li, Shiqi Wang and Sam Kwong", "title": "Camera Invariant Feature Learning for Generalized Face Anti-spoofing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There has been an increasing consensus in learning based face anti-spoofing\nthat the divergence in terms of camera models is causing a large domain gap in\nreal application scenarios. We describe a framework that eliminates the\ninfluence of inherent variance from acquisition cameras at the feature level,\nleading to the generalized face spoofing detection model that could be highly\nadaptive to different acquisition devices. In particular, the framework is\ncomposed of two branches. The first branch aims to learn the camera invariant\nspoofing features via feature level decomposition in the high frequency domain.\nMotivated by the fact that the spoofing features exist not only in the high\nfrequency domain, in the second branch the discrimination capability of\nextracted spoofing features is further boosted from the enhanced image based on\nthe recomposition of the high-frequency and low-frequency information. Finally,\nthe classification results of the two branches are fused together by a\nweighting strategy. Experiments show that the proposed method can achieve\nbetter performance in both intra-dataset and cross-dataset settings,\ndemonstrating the high generalization capability in various application\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 13:40:43 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chen", "Baoliang", ""], ["Yang", "Wenhan", ""], ["Li", "Haoliang", ""], ["Wang", "Shiqi", ""], ["Kwong", "Sam", ""]]}, {"id": "2101.10141", "submitter": "George Papakostas Prof.", "authors": "K.D. Apostolidis, P.S. Amanatidis, G.A. Papakostas", "title": "Performance Evaluation of Convolutional Neural Networks for Gait\n  Recognition", "comments": "6 pages, 15 figures, to be published in proceedings of the 24th\n  Pan-Hellenic Conference on Informatics (PCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, a performance evaluation of well-known deep learning models in\ngait recognition is presented. For this purpose, the transfer learning scheme\nis adopted to pre-trained models in order to fit the models to the CASIA-B\ndataset for solving a gait recognition task. In this context, 18 popular\nConvolutional Neural Networks (CNNs), were re-trained using Gait Energy Images\n(GEIs) of CASIA-B containing almost 14000 images of 124 classes under various\nconditions, and their performance was studied in terms of accuracy. Moreover,\nthe performance of the studied models is managed to be explained by examining\nthe parts of the images being considered by the models towards providing their\ndecisions. The experimental results are very promising since almost all the\nmodels achieved a high accuracy of over 90%, which is robust to the increasing\nnumber of classes. Furthermore, an important outcome of this study is the fact\nthat a recognition problem can be effectively solved by using CNNs pre-trained\nto different problems, thus eliminating the need for customized model design.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:44:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Apostolidis", "K. D.", ""], ["Amanatidis", "P. S.", ""], ["Papakostas", "G. A.", ""]]}, {"id": "2101.10143", "submitter": "Nergis Tomen", "authors": "Nergis Tomen, Jan van Gemert", "title": "Spectral Leakage and Rethinking the Kernel Size in CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional layers in CNNs implement linear filters which decompose the\ninput into different frequency bands. However, most modern architectures\nneglect standard principles of filter design when optimizing their model\nchoices regarding the size and shape of the convolutional kernel. In this work,\nwe consider the well-known problem of spectral leakage caused by windowing\nartifacts in filtering operations in the context of CNNs. We show that the\nsmall size of CNN kernels make them susceptible to spectral leakage, which may\ninduce performance-degrading artifacts. To address this issue, we propose the\nuse of larger kernel sizes along with the Hamming window function to alleviate\nleakage in CNN architectures. We demonstrate improved classification accuracy\non multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and\nImageNet with the simple use of a standard window function in convolutional\nlayers. Finally, we show that CNNs employing the Hamming window display\nincreased robustness against various adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:49:29 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 10:30:21 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Tomen", "Nergis", ""], ["van Gemert", "Jan", ""]]}, {"id": "2101.10156", "submitter": "Ying Chen", "authors": "Ying Chen, Xu Ouyang, Kaiyue Zhu, Gady Agam", "title": "Mask-based Data Augmentation for Semi-supervised Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation using convolutional neural networks (CNN) is a crucial\ncomponent in image analysis. Training a CNN to perform semantic segmentation\nrequires a large amount of labeled data, where the production of such labeled\ndata is both costly and labor intensive. Semi-supervised learning algorithms\naddress this issue by utilizing unlabeled data and so reduce the amount of\nlabeled data needed for training. In particular, data augmentation techniques\nsuch as CutMix and ClassMix generate additional training data from existing\nlabeled data. In this paper we propose a new approach for data augmentation,\ntermed ComplexMix, which incorporates aspects of CutMix and ClassMix with\nimproved performance. The proposed approach has the ability to control the\ncomplexity of the augmented data while attempting to be semantically-correct\nand address the tradeoff between complexity and correctness. The proposed\nComplexMix approach is evaluated on a standard dataset for semantic\nsegmentation and compared to other state-of-the-art techniques. Experimental\nresults show that our method yields improvement over state-of-the-art methods\non standard datasets for semantic image segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 15:09:34 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chen", "Ying", ""], ["Ouyang", "Xu", ""], ["Zhu", "Kaiyue", ""], ["Agam", "Gady", ""]]}, {"id": "2101.10165", "submitter": "Yuanzhuo Li", "authors": "Yuanzhuo Li, Yunan Zheng, Jie Chen, Zhenyu Xu, Yiguang Liu", "title": "Learning Structral coherence Via Generative Adversarial Network for\n  Single Image Super-Resolution", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among the major remaining challenges for single image super resolution (SISR)\nis the capacity to recover coherent images with global shapes and local details\nconforming to human vision system. Recent generative adversarial network (GAN)\nbased SISR methods have yielded overall realistic SR images, however, there are\nalways unpleasant textures accompanied with structural distortions in local\nregions. To target these issues, we introduce the gradient branch into the\ngenerator to preserve structural information by restoring high-resolution\ngradient maps in SR process. In addition, we utilize a U-net based\ndiscriminator to consider both the whole image and the detailed per-pixel\nauthenticity, which could encourage the generator to maintain overall coherence\nof the reconstructed images. Moreover, we have studied objective functions and\nLPIPS perceptual loss is added to generate more realistic and natural details.\nExperimental results show that our proposed method outperforms state-of-the-art\nperceptual-driven SR methods in perception index (PI), and obtains more\ngeometrically consistent and visually pleasing textures in natural image\nrestoration.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 15:26:23 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Li", "Yuanzhuo", ""], ["Zheng", "Yunan", ""], ["Chen", "Jie", ""], ["Xu", "Zhenyu", ""], ["Liu", "Yiguang", ""]]}, {"id": "2101.10173", "submitter": "Arnaud Boutillon", "authors": "Arnaud Boutillon, Bhushan Borotikar, Christelle Pons, Val\\'erie\n  Burdin, Pierre-Henri Conze", "title": "Multi-Structure Deep Segmentation with Shape Priors and Latent\n  Adversarial Regularization", "comments": "4 pages, 3 figures; 1 table, accepted at 2021 IEEE 18th International\n  Symposium on Biomedical Imaging (ISBI)", "journal-ref": null, "doi": "10.1109/ISBI48211.2021.9434104", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of the musculoskeletal system in pediatric magnetic\nresonance (MR) images is a challenging but crucial task for morphological\nevaluation in clinical practice. We propose a deep learning-based regularized\nsegmentation method for multi-structure bone delineation in MR images, designed\nto overcome the inherent scarcity and heterogeneity of pediatric data. Based on\na newly devised shape code discriminator, our adversarial regularization scheme\nenforces the deep network to follow a learnt shape representation of the\nanatomy. The novel shape priors based adversarial regularization (SPAR)\nexploits latent shape codes arising from ground truth and predicted masks to\nguide the segmentation network towards more consistent and plausible\npredictions. Our contribution is compared to state-of-the-art regularization\nmethods on two pediatric musculoskeletal imaging datasets from ankle and\nshoulder joints.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 15:43:40 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Boutillon", "Arnaud", ""], ["Borotikar", "Bhushan", ""], ["Pons", "Christelle", ""], ["Burdin", "Val\u00e9rie", ""], ["Conze", "Pierre-Henri", ""]]}, {"id": "2101.10200", "submitter": "Ngoc Long Nguyen", "authors": "Ngoc Long Nguyen, J\\'er\\'emy Anger, Axel Davy, Pablo Arias, Gabriele\n  Facciolo", "title": "Proba-V-ref: Repurposing the Proba-V challenge for reference-aware super\n  resolution", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The PROBA-V Super-Resolution challenge distributes real low-resolution image\nseries and corresponding high-resolution targets to advance research on\nMulti-Image Super Resolution (MISR) for satellite images. However, in the\nPROBA-V dataset the low-resolution image corresponding to the high-resolution\ntarget is not identified. We argue that in doing so, the challenge ranks the\nproposed methods not only by their MISR performance, but mainly by the\nheuristics used to guess which image in the series is the most similar to the\nhigh-resolution target. We demonstrate this by improving the performance\nobtained by the two winners of the challenge only by using a different\nreference image, which we compute following a simple heuristic. Based on this,\nwe propose PROBA-V-REF a variant of the PROBA-V dataset, in which the reference\nimage in the low-resolution series is provided, and show that the ranking\nbetween the methods changes in this setting. This is relevant to many practical\nuse cases of MISR where the goal is to super-resolve a specific image of the\nseries, i.e. the reference is known. The proposed PROBA-V-REF should better\nreflect the performance of the different methods for this reference-aware MISR\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:09:21 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 08:32:46 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Nguyen", "Ngoc Long", ""], ["Anger", "J\u00e9r\u00e9my", ""], ["Davy", "Axel", ""], ["Arias", "Pablo", ""], ["Facciolo", "Gabriele", ""]]}, {"id": "2101.10203", "submitter": "Eli Schwartz", "authors": "Eli Schwartz, Alex Bronstein, Raja Giryes", "title": "ISP Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nowadays, many of the images captured are \"observed\" by machines only and not\nby humans, for example, robots' or autonomous cars' cameras. High-level machine\nvision models, such as object recognition, assume images are transformed to\nsome canonical image space by the camera ISP. However, the camera ISP is\noptimized for producing visually pleasing images to human observers and not for\nmachines, thus, one may spare the ISP compute time and apply the vision models\ndirectly to the raw data. Yet, it has been shown that training such models\ndirectly on the RAW images results in a performance drop. To mitigate this drop\nin performance (without the need to annotate RAW data), we use a dataset of RAW\nand RGB image pairs, which can be easily acquired with no human labeling. We\nthen train a model that is applied directly to the RAW data by using knowledge\ndistillation such that the model predictions for RAW images will be aligned\nwith the predictions of an off-the-shelf pre-trained model for processed RGB\nimages. Our experiments show that our performance on RAW images is\nsignificantly better than a model trained on labeled RAW images. It also\nreasonably matches the predictions of a pre-trained model on processed RGB\nimages, while saving the ISP compute overhead.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:12:24 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Schwartz", "Eli", ""], ["Bronstein", "Alex", ""], ["Giryes", "Raja", ""]]}, {"id": "2101.10223", "submitter": "Carlo Alberto Barbano", "authors": "Carlo Alberto Barbano, Enzo Tartaglione, Claudio Berzovini, Marco\n  Calandri, Marco Grangetto", "title": "A two-step explainable approach for COVID-19 computer-aided diagnosis\n  from chest x-ray images", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early screening of patients is a critical issue in order to assess immediate\nand fast responses against the spread of COVID-19. The use of nasopharyngeal\nswabs has been considered the most viable approach; however, the result is not\nimmediate or, in the case of fast exams, sufficiently accurate. Using Chest\nX-Ray (CXR) imaging for early screening potentially provides faster and more\naccurate response; however, diagnosing COVID from CXRs is hard and we should\nrely on deep learning support, whose decision process is, on the other hand,\n\"black-boxed\" and, for such reason, untrustworthy. We propose an explainable\ntwo-step diagnostic approach, where we first detect known pathologies\n(anomalies) in the lungs, on top of which we diagnose the illness. Our approach\nachieves promising performance in COVID detection, compatible with expert human\nradiologists. All of our experiments have been carried out bearing in mind\nthat, especially for clinical applications, explainability plays a major role\nfor building trust in machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:35:44 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Barbano", "Carlo Alberto", ""], ["Tartaglione", "Enzo", ""], ["Berzovini", "Claudio", ""], ["Calandri", "Marco", ""], ["Grangetto", "Marco", ""]]}, {"id": "2101.10226", "submitter": "Hu Cao", "authors": "Hu Cao, Guang Chen, Zhijun Li, Jianjie Lin, Alois Knoll", "title": "Lightweight Convolutional Neural Network with Gaussian-based Grasping\n  Representation for Robotic Grasping Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The method of deep learning has achieved excellent results in improving the\nperformance of robotic grasping detection. However, the deep learning methods\nused in general object detection are not suitable for robotic grasping\ndetection. Current modern object detectors are difficult to strike a balance\nbetween high accuracy and fast inference speed. In this paper, we present an\nefficient and robust fully convolutional neural network model to perform\nrobotic grasping pose estimation from an n-channel input image of the real\ngrasping scene. The proposed network is a lightweight generative architecture\nfor grasping detection in one stage. Specifically, a grasping representation\nbased on Gaussian kernel is introduced to encode training samples, which\nembodies the principle of maximum central point grasping confidence. Meanwhile,\nto extract multi-scale information and enhance the feature discriminability, a\nreceptive field block (RFB) is assembled to the bottleneck of our grasping\ndetection architecture. Besides, pixel attention and channel attention are\ncombined to automatically learn to focus on fusing context information of\nvarying shapes and sizes by suppressing the noise feature and highlighting the\ngrasping object feature. Extensive experiments on two public grasping datasets,\nCornell and Jacquard demonstrate the state-of-the-art performance of our method\nin balancing accuracy and inference speed. The network is an order of magnitude\nsmaller than other excellent algorithms while achieving better performance with\nan accuracy of 98.9$\\%$ and 95.6$\\%$ on the Cornell and Jacquard datasets,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:36:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Cao", "Hu", ""], ["Chen", "Guang", ""], ["Li", "Zhijun", ""], ["Lin", "Jianjie", ""], ["Knoll", "Alois", ""]]}, {"id": "2101.10241", "submitter": "Qian Chen", "authors": "Qian Chen, Ze Liu, Yi Zhang, Keren Fu, Qijun Zhao, Hongwei Du", "title": "RGB-D Salient Object Detection via 3D Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-D salient object detection (SOD) recently has attracted increasing\nresearch interest and many deep learning methods based on encoder-decoder\narchitectures have emerged. However, most existing RGB-D SOD models conduct\nfeature fusion either in the single encoder or the decoder stage, which hardly\nguarantees sufficient cross-modal fusion ability. In this paper, we make the\nfirst attempt in addressing RGB-D SOD through 3D convolutional neural networks.\nThe proposed model, named RD3D, aims at pre-fusion in the encoder stage and\nin-depth fusion in the decoder stage to effectively promote the full\nintegration of RGB and depth streams. Specifically, RD3D first conducts\npre-fusion across RGB and depth modalities through an inflated 3D encoder, and\nlater provides in-depth feature fusion by designing a 3D decoder equipped with\nrich back-projection paths (RBPP) for leveraging the extensive aggregation\nability of 3D convolutions. With such a progressive fusion strategy involving\nboth the encoder and decoder, effective and thorough interaction between the\ntwo modalities can be exploited and boost the detection accuracy. Extensive\nexperiments on six widely used benchmark datasets demonstrate that RD3D\nperforms favorably against 14 state-of-the-art RGB-D SOD approaches in terms of\nfour key evaluation metrics. Our code will be made publicly available:\nhttps://github.com/PPOLYpubki/RD3D.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:03:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chen", "Qian", ""], ["Liu", "Ze", ""], ["Zhang", "Yi", ""], ["Fu", "Keren", ""], ["Zhao", "Qijun", ""], ["Du", "Hongwei", ""]]}, {"id": "2101.10248", "submitter": "Jian-Qing Zheng", "authors": "Jian-Qing Zheng, Ngee Han Lim, Bartlomiej W. Papiez", "title": "D-Net: Siamese based Network with Mutual Attention for Volume Alignment", "comments": "this uploaded manuscript is another version of which published in:\n  International Workshop on Shape in Medical Imaging, Springer, 2020, pp. 73-84", "journal-ref": "in: International Workshop on Shape in Medical Imaging, Springer,\n  2020, pp. 73-84", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alignment of contrast and non-contrast-enhanced imaging is essential for the\nquantification of changes in several biomedical applications. In particular,\nthe extraction of cartilage shape from contrast-enhanced Computed Tomography\n(CT) of tibiae requires accurate alignment of the bone, currently performed\nmanually. Existing deep learning-based methods for alignment require a common\ntemplate or are limited in rotation range. Therefore, we present a novel\nnetwork, D-net, to estimate arbitrary rotation and translation between 3D CT\nscans that additionally does not require a prior standard template. D-net is an\nextension to the branched Siamese encoder-decoder structure connected by new\nmutual non-local links, which efficiently capture long-range connections of\nsimilar features between two branches. The 3D supervised network is trained and\nvalidated using preclinical CT scans of mouse tibiae with and without contrast\nenhancement in cartilage. The presented results show a significant improvement\nin the estimation of CT alignment, outperforming the current comparable\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:24:16 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zheng", "Jian-Qing", ""], ["Lim", "Ngee Han", ""], ["Papiez", "Bartlomiej W.", ""]]}, {"id": "2101.10253", "submitter": "Daniela Mihai", "authors": "Daniela Mihai and Jonathon Hare", "title": "The emergence of visual semantics through communication games", "comments": "arXiv admin note: text overlap with arXiv:1911.05546", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of communication systems between agents which learn to play\nreferential signalling games with realistic images has attracted a lot of\nattention recently. The majority of work has focused on using fixed, pretrained\nimage feature extraction networks which potentially bias the information the\nagents learn to communicate. In this work, we consider a signalling game\nsetting in which a `sender' agent must communicate the information about an\nimage to a `receiver' who must select the correct image from many distractors.\nWe investigate the effect of the feature extractor's weights and of the task\nbeing solved on the visual semantics learned by the models. We first\ndemonstrate to what extent the use of pretrained feature extraction networks\ninductively bias the visual semantics conveyed by emergent communication\nchannel and quantify the visual semantics that are induced.\n  We then go on to explore ways in which inductive biases can be introduced to\nencourage the emergence of semantically meaningful communication without the\nneed for any form of supervised pretraining of the visual feature extractor. We\nimpose various augmentations to the input images and additional tasks in the\ngame with the aim to induce visual representations which capture conceptual\nproperties of images. Through our experiments, we demonstrate that\ncommunication systems which capture visual semantics can be learned in a\ncompletely self-supervised manner by playing the right types of game. Our work\nbridges a gap between emergent communication research and self-supervised\nfeature learning.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:43:37 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "2101.10260", "submitter": "Matthew Ehrler", "authors": "Matthew Ehrler and Neil Ernst", "title": "VConstruct: Filling Gaps in Chl-a Data Using a Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Remote sensing of Chlorophyll-a is vital in monitoring climate change.\nChlorphyll-a measurements give us an idea of the algae concentrations in the\nocean, which lets us monitor ocean health. However, a common problem is that\nthe satellites used to gather the data are commonly obstructed by clouds and\nother artifacts. This means that time series data from satellites can suffer\nfrom spatial data loss.\n  There are a number of algorithms that are able to reconstruct the missing\nparts of these images to varying degrees of accuracy, with Data INterpolating\nEmpirical Orthogonal Functions (DINEOF) being the current standard. However,\nDINEOF is slow, suffers from accuracy loss in temporally homogenous waters,\nreliant on temporal data, and only able to generate a single potential\nreconstruction.\n  We propose a machine learning approach to reconstruction of Chlorophyll-a\ndata using a Variational Autoencoder (VAE). Our accuracy results to date are\ncompetitive with but slightly less accurate than DINEOF. We show the benefits\nof our method including vastly decreased computation time and ability to\ngenerate multiple potential reconstructions. Lastly, we outline our planned\nimprovements and future work.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:49:42 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ehrler", "Matthew", ""], ["Ernst", "Neil", ""]]}, {"id": "2101.10292", "submitter": "Xiaoqian Wu", "authors": "Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Xijie Huang, Liang Xu, Cewu Lu", "title": "Transferable Interactiveness Knowledge for Human-Object Interaction\n  Detection", "comments": "TPAMI version of our CVPR2019 paper with a new benchmark\n  PaStaNet-HOI. Code:\n  https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network. arXiv\n  admin note: substantial text overlap with arXiv:1811.08264", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3054048", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object Interaction (HOI) detection is an important problem to\nunderstand how humans interact with objects. In this paper, we explore\ninteractiveness knowledge which indicates whether a human and an object\ninteract with each other or not. We found that interactiveness knowledge can be\nlearned across HOI datasets and bridge the gap between diverse HOI category\nsettings. Our core idea is to exploit an interactiveness network to learn the\ngeneral interactiveness knowledge from multiple HOI datasets and perform\nNon-Interaction Suppression (NIS) before HOI classification in inference. On\naccount of the generalization ability of interactiveness, interactiveness\nnetwork is a transferable knowledge learner and can be cooperated with any HOI\ndetection models to achieve desirable results. We utilize the human instance\nand body part features together to learn the interactiveness in hierarchical\nparadigm, i.e., instance-level and body part-level interactivenesses.\nThereafter, a consistency task is proposed to guide the learning and extract\ndeeper interactive visual clues. We extensively evaluate the proposed method on\nHICO-DET, V-COCO, and a newly constructed PaStaNet-HOI dataset. With the\nlearned interactiveness, our method outperforms state-of-the-art HOI detection\nmethods, verifying its efficacy and flexibility. Code is available at\nhttps://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:21:07 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 04:21:24 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 10:04:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Yong-Lu", ""], ["Liu", "Xinpeng", ""], ["Wu", "Xiaoqian", ""], ["Huang", "Xijie", ""], ["Xu", "Liang", ""], ["Lu", "Cewu", ""]]}, {"id": "2101.10353", "submitter": "Yiming Luo", "authors": "Yiming Luo, Zhenxing Mi, Wenbing Tao", "title": "DeepDT: Learning Geometry From Delaunay Triangulation for Surface\n  Reconstruction", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel learning-based network, named DeepDT, is proposed to\nreconstruct the surface from Delaunay triangulation of point cloud. DeepDT\nlearns to predict inside/outside labels of Delaunay tetrahedrons directly from\na point cloud and corresponding Delaunay triangulation. The local geometry\nfeatures are first extracted from the input point cloud and aggregated into a\ngraph deriving from the Delaunay triangulation. Then a graph filtering is\napplied on the aggregated features in order to add structural regularization to\nthe label prediction of tetrahedrons. Due to the complicated spatial relations\nbetween tetrahedrons and the triangles, it is impossible to directly generate\nground truth labels of tetrahedrons from ground truth surface. Therefore, we\npropose a multi-label supervision strategy which votes for the label of a\ntetrahedron with labels of sampling locations inside it. The proposed DeepDT\ncan maintain abundant geometry details without generating overly complex\nsurfaces, especially for inner surfaces of open scenes. Meanwhile, the\ngeneralization ability and time consumption of the proposed method is\nacceptable and competitive compared with the state-of-the-art methods.\nExperiments demonstrate the superior performance of the proposed DeepDT.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 19:04:27 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 14:09:43 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 09:06:03 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Luo", "Yiming", ""], ["Mi", "Zhenxing", ""], ["Tao", "Wenbing", ""]]}, {"id": "2101.10371", "submitter": "Juan Pablo Rodr\\'iguez G\\'omez", "authors": "J.P. Rodr\\'iguez-G\\'omez, R. Tapia, J. L. Paneque, P. Grau, A. G\\'omez\n  Egu\\'iluz, J.R. Mart\\'inez-de Dios and A. Ollero", "title": "The GRIFFIN Perception Dataset: Bridging the Gap Between Flapping-Wing\n  Flight and Robotic Perception", "comments": "8 pages, 22 figures, Video: \"this https URL\n  https://www.youtube.com/watch?v=ymCRnlWxX24&t=35s\"", "journal-ref": "IEEE Robotics and Automation Letters (RA-L), 2021", "doi": "10.1109/LRA.2021.3056348", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of automatic perception systems and techniques for\nbio-inspired flapping-wing robots is severely hampered by the high technical\ncomplexity of these platforms and the installation of onboard sensors and\nelectronics. Besides, flapping-wing robot perception suffers from high\nvibration levels and abrupt movements during flight, which cause motion blur\nand strong changes in lighting conditions. This paper presents a perception\ndataset for bird-scale flapping-wing robots as a tool to help alleviate the\naforementioned problems. The presented data include measurements from onboard\nsensors widely used in aerial robotics and suitable to deal with the perception\nchallenges of flapping-wing robots, such as an event camera, a conventional\ncamera, and two Inertial Measurement Units (IMUs), as well as ground truth\nmeasurements from a laser tracker or a motion capture system. A total of 21\ndatasets of different types of flights were collected in three different\nscenarios (one indoor and two outdoor). To the best of the authors' knowledge\nthis is the first dataset for flapping-wing robot perception.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 19:42:13 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 18:31:48 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Rodr\u00edguez-G\u00f3mez", "J. P.", ""], ["Tapia", "R.", ""], ["Paneque", "J. L.", ""], ["Grau", "P.", ""], ["Egu\u00edluz", "A. G\u00f3mez", ""], ["Dios", "J. R. Mart\u00ednez-de", ""], ["Ollero", "A.", ""]]}, {"id": "2101.10382", "submitter": "Radu Tudor Ionescu", "authors": "Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe", "title": "Curriculum Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:08:32 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Soviany", "Petru", ""], ["Ionescu", "Radu Tudor", ""], ["Rota", "Paolo", ""], ["Sebe", "Nicu", ""]]}, {"id": "2101.10391", "submitter": "Hyeonwoo Yu", "authors": "Hyeonwoo Yu and Jean Oh", "title": "Adaptive Data Over-Compression Method for 3D Object Reconstruction using\n  Multi-modal Variational Autoencoder", "comments": "submitted to IROS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For effective human-robot teaming, it is important for the robots to be able\nto share their visual perception with the human operators. In a harsh remote\ncollaboration setting, however, it is especially challenging to transfer a\nlarge amount of sensory data over a low-bandwidth network in real-time, e.g.,\nfor the task of 3D shape reconstruction given 2D camera images. To reduce the\nburden of data transferring, data compression techniques such as autoencoder\ncan be utilized to obtain and transmit the data in terms of latent variables in\na compact form. However, to overcome the lower-bandwidth limitation and achieve\nfaster transmission, an adaptive and flexible over-compression method is\nnecessary that can exploit only partial elements of the latent variables. To\nhandle these cases, we propose a method for imputation of latent variables\nwhose elements are partially excluded for additional compression. To perform\nimputation with only some dimensions of variables, exploiting prior information\nof the category- or instance-level is essential. In general, a prior\ndistribution used in variational autoencoders is achieved from all of the\ntraining datapoints regardless of their labels. This type of flattened prior\nmakes it difficult to perform imputation from the category- or instance-level\ndistributions. We overcome this limitation by exploiting a category-specific\nmulti-modal prior distribution in the latent space. By finding a modal in a\nlatent space according to the remaining elements of the latent variables, the\nmissing elements can be sampled. Based on the experiments on the ModelNet and\nPascal3D datasets, the proposed approach shows a consistently superior\nperformance over autoencoder and variational autoencoder up to 50\\% data loss.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:23:33 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 18:46:49 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yu", "Hyeonwoo", ""], ["Oh", "Jean", ""]]}, {"id": "2101.10396", "submitter": "Aakanksha Rana", "authors": "Cagri Ozcinar and Aakanksha Rana", "title": "Quality Assessment of Super-Resolved Omnidirectional Image Quality Using\n  Tangential Views", "comments": "Paper Accepted at Electronic Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Omnidirectional images (ODIs), also known as 360-degree images, enable\nviewers to explore all directions of a given 360-degree scene from a fixed\npoint. Designing an immersive imaging system with ODI is challenging as such\nsystems require very large resolution coverage of the entire 360 viewing space\nto provide an enhanced quality of experience (QoE). Despite remarkable progress\non single image super-resolution (SISR) methods with deep-learning techniques,\nno study for quality assessments of super-resolved ODIs exists to analyze the\nquality of such SISR techniques. This paper proposes an objective,\nfull-reference quality assessment framework which studies quality measurement\nfor ODIs generated by GAN-based and CNN-based SISR methods. The quality\nassessment framework offers to utilize tangential views to cope with the\nspherical nature of a given ODIs. The generated tangential views are\ndistortion-free and can be efficiently scaled to high-resolution spherical data\nfor SISR quality measurement. We extensively evaluate two state-of-the-art SISR\nmethods using widely used full-reference SISR quality metrics adapted to our\ndesigned framework. In addition, our study reveals that most objective metric\nshow high performance over CNN based SISR, while subjective tests favors\nGAN-based architectures.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:30:57 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ozcinar", "Cagri", ""], ["Rana", "Aakanksha", ""]]}, {"id": "2101.10399", "submitter": "Hyeonwoo Yu", "authors": "Hyeonwoo Yu and Jean Oh", "title": "Anchor Distance for 3D Multi-Object Distance Estimation from 2D Single\n  Shot", "comments": "submitted to RA-letter with ICRA2021 option", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual perception of the objects in a 3D environment is a key to successful\nperformance in autonomous driving and simultaneous localization and mapping\n(SLAM). In this paper, we present a real time approach for estimating the\ndistances to multiple objects in a scene using only a single-shot image. Given\na 2D Bounding Box (BBox) and object parameters, a 3D distance to the object can\nbe calculated directly using 3D reprojection; however, such methods are prone\nto significant errors because an error from the 2D detection can be amplified\nin 3D. In addition, it is also challenging to apply such methods to a real-time\nsystem due to the computational burden. In the case of the traditional\nmulti-object detection methods, %they mostly pay attention to existing works\nhave been developed for specific tasks such as object segmentation or 2D BBox\nregression. These methods introduce the concept of anchor BBox for elaborate 2D\nBBox estimation, and predictors are specialized and trained for specific 2D\nBBoxes. In order to estimate the distances to the 3D objects from a single 2D\nimage, we introduce the notion of \\textit{anchor distance} based on an object's\nlocation and propose a method that applies the anchor distance to the\nmulti-object detector structure. We let the predictors catch the distance prior\nusing anchor distance and train the network based on the distance. The\npredictors can be characterized to the objects located in a specific distance\nrange. By propagating the distance prior using a distance anchor to the\npredictors, it is feasible to perform the precise distance estimation and\nreal-time execution simultaneously. The proposed method achieves about 30 FPS\nspeed, and shows the lowest RMSE compared to the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:33:05 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 17:57:39 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Yu", "Hyeonwoo", ""], ["Oh", "Jean", ""]]}, {"id": "2101.10402", "submitter": "Guoxiang Zhang", "authors": "Guoxiang Zhang and YangQuan Chen", "title": "A metric for evaluating 3D reconstruction and mapping performance with\n  no ground truthing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not easy when evaluating 3D mapping performance because existing\nmetrics require ground truth data that can only be collected with special\ninstruments. In this paper, we propose a metric, dense map posterior (DMP), for\nthis evaluation. It can work without any ground truth data. Instead, it\ncalculates a comparable value, reflecting a map posterior probability, from\ndense point cloud observations. In our experiments, the proposed DMP is\nbenchmarked against ground truth-based metrics. Results show that DMP can\nprovide a similar evaluation capability. The proposed metric makes evaluating\ndifferent methods more flexible and opens many new possibilities, such as\nself-supervised methods and more available datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:37:17 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 21:36:33 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhang", "Guoxiang", ""], ["Chen", "YangQuan", ""]]}, {"id": "2101.10423", "submitter": "Zheda Mai", "authors": "Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, Scott\n  Sanner", "title": "Online Continual Learning in Image Classification: An Empirical Survey", "comments": "Submitted to Neurocomputing. Codes available at\n  https://github.com/RaptorMai/online-continual-learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online continual learning for image classification studies the problem of\nlearning to classify images from an online stream of data and tasks, where\ntasks may include new classes (class incremental) or data nonstationarity\n(domain incremental). One of the key challenges of continual learning is to\navoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence\nof more recent tasks. Over the past few years, many methods and tricks have\nbeen introduced to address this problem, but many have not been fairly and\nsystematically compared under a variety of realistic and practical settings. To\nbetter understand the relative advantages of various approaches and the\nsettings where they work best, this survey aims to (1) compare state-of-the-art\nmethods such as MIR, iCARL, and GDumb and determine which works best at\ndifferent experimental settings; (2) determine if the best class incremental\nmethods are also competitive in domain incremental setting; (3) evaluate the\nperformance of 7 simple but effective trick such as \"review\" trick and nearest\nclass mean (NCM) classifier to assess their relative impact. Regarding (1), we\nobserve iCaRL remains competitive when the memory buffer is small; GDumb\noutperforms many recently proposed methods in medium-size datasets and MIR\nperforms the best in larger-scale datasets. For (2), we note that GDumb\nperforms quite poorly while MIR -- already competitive for (1) -- is also\nstrongly competitive in this very different but important setting. Overall,\nthis allows us to conclude that MIR is overall a strong and versatile method\nacross a wide variety of settings. For (3), we find that all 7 tricks are\nbeneficial, and when augmented with the \"review\" trick and NCM classifier, MIR\nproduces performance levels that bring online continual learning much closer to\nits ultimate goal of matching offline training.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:20:02 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 21:07:30 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mai", "Zheda", ""], ["Li", "Ruiwen", ""], ["Jeong", "Jihwan", ""], ["Quispe", "David", ""], ["Kim", "Hyunwoo", ""], ["Sanner", "Scott", ""]]}, {"id": "2101.10443", "submitter": "Piduguralla Manaswini", "authors": "Piduguralla Manaswini, Jignesh S. Bhatt", "title": "Towards glass-box CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution neural networks (CNNs) are brain-inspired architectures popular\nfor their ability to train and relearn visually complex tasks. It is\nincremental and scalable; however, CNN is mostly treated as black-box and\ninvolves multiple trial & error runs. We observe that CNN constructs powerful\ninternal representations that help achieve state-of-the-art performance. Here\nwe propose three layer glass-box (analytical) CNN for two-class image\nclassifcation problems. First is a representation layer that encompasses both\nthe class information (group invariant) and symmetric transformations (group\nequivariant) of input images. It is then passed through dimension reduction\nlayer (PCA). Finally the compact yet complete representation is provided to a\nclassifer. Analytical machine learning classifers and multilayer perceptrons\nare used to assess sensitivity. Proposed glass-box CNN is compared with\nequivariance of AlexNet (CNN) internal representation for better understanding\nand dissemination of results. In future, we would like to construct glass-box\nCNN for multiclass visually complex tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:00:35 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Manaswini", "Piduguralla", ""], ["Bhatt", "Jignesh S.", ""]]}, {"id": "2101.10444", "submitter": "Baohua Sun", "authors": "Baohua Sun, Weixiong Lin, Hao Sha, Jiapeng Su", "title": "GnetSeg: Semantic Segmentation Model Optimized on a 224mW CNN\n  Accelerator Chip at the Speed of 318FPS", "comments": "7 pages, 3 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is the task to cluster pixels on an image belonging to\nthe same class. It is widely used in the real-world applications including\nautonomous driving, medical imaging analysis, industrial inspection, smartphone\ncamera for person segmentation and so on. Accelerating the semantic\nsegmentation models on the mobile and edge devices are practical needs for the\nindustry. Recent years have witnessed the wide availability of CNN\n(Convolutional Neural Networks) accelerators. They have the advantages on power\nefficiency, inference speed, which are ideal for accelerating the semantic\nsegmentation models on the edge devices. However, the CNN accelerator chips\nalso have the limitations on flexibility and memory. In addition, the CPU load\nis very critical because the CNN accelerator chip works as a co-processor with\na host CPU. In this paper, we optimize the semantic segmentation model in order\nto fully utilize the limited memory and the supported operators on the CNN\naccelerator chips, and at the same time reduce the CPU load of the CNN model to\nzero. The resulting model is called GnetSeg. Furthermore, we propose the\ninteger encoding for the mask of the GnetSeg model, which minimizes the latency\nof data transfer between the CNN accelerator and the host CPU. The experimental\nresult shows that the model running on the 224mW chip achieves the speed of\n318FPS with excellent accuracy for applications such as person segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 23:11:48 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Sun", "Baohua", ""], ["Lin", "Weixiong", ""], ["Sha", "Hao", ""], ["Su", "Jiapeng", ""]]}, {"id": "2101.10445", "submitter": "Rateb Jabbar Mr.", "authors": "Safa Ayadi, Ahmed ben said, Rateb Jabbar, Chafik Aloulou, Achraf\n  Chabbouh, and Ahmed Ben Achballah", "title": "Dairy Cow rumination detection: A deep learning approach", "comments": "17 pages, 6 figures, 4 tables", "journal-ref": "International Workshop on Distributed Computing for Emerging Smart\n  Networks. Springer, Cham, 2020", "doi": "10.1007/978-3-030-65810-6_7", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cattle activity is an essential index for monitoring health and welfare of\nthe ruminants. Thus, changes in the livestock behavior are a critical indicator\nfor early detection and prevention of several diseases. Rumination behavior is\na significant variable for tracking the development and yield of animal\nhusbandry. Therefore, various monitoring methods and measurement equipment have\nbeen used to assess cattle behavior. However, these modern attached devices are\ninvasive, stressful and uncomfortable for the cattle and can influence\nnegatively the welfare and diurnal behavior of the animal. Multiple research\nefforts addressed the problem of rumination detection by adopting new methods\nby relying on visual features. However, they only use few postures of the dairy\ncow to recognize the rumination or feeding behavior. In this study, we\nintroduce an innovative monitoring method using Convolution Neural Network\n(CNN)-based deep learning models. The classification process is conducted under\ntwo main labels: ruminating and other, using all cow postures captured by the\nmonitoring camera. Our proposed system is simple and easy-to-use which is able\nto capture long-term dynamics using a compacted representation of a video in a\nsingle 2D image. This method proved efficiency in recognizing the rumination\nbehavior with 95%, 98% and 98% of average accuracy, recall and precision,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:33:32 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ayadi", "Safa", ""], ["said", "Ahmed ben", ""], ["Jabbar", "Rateb", ""], ["Aloulou", "Chafik", ""], ["Chabbouh", "Achraf", ""], ["Achballah", "Ahmed Ben", ""]]}, {"id": "2101.10449", "submitter": "Pranjay Shyam", "authors": "Pranjay Shyam, Kuk-Jin Yoon and Kyung-Soo Kim", "title": "Towards Domain Invariant Single Image Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Presence of haze in images obscures underlying information, which is\nundesirable in applications requiring accurate environment information. To\nrecover such an image, a dehazing algorithm should localize and recover\naffected regions while ensuring consistency between recovered and its\nneighboring regions. However owing to fixed receptive field of convolutional\nkernels and non uniform haze distribution, assuring consistency between regions\nis difficult. In this paper, we utilize an encoder-decoder based network\narchitecture to perform the task of dehazing and integrate an spatially aware\nchannel attention mechanism to enhance features of interest beyond the\nreceptive field of traditional conventional kernels. To ensure performance\nconsistency across diverse range of haze densities, we utilize greedy localized\ndata augmentation mechanism. Synthetic datasets are typically used to ensure a\nlarge amount of paired training samples, however the methodology to generate\nsuch samples introduces a gap between them and real images while accounting for\nonly uniform haze distribution and overlooking more realistic scenario of\nnon-uniform haze distribution resulting in inferior dehazing performance when\nevaluated on real datasets. Despite this, the abundance of paired samples\nwithin synthetic datasets cannot be ignored. Thus to ensure performance\nconsistency across diverse datasets, we train the proposed network within an\nadversarial prior-guided framework that relies on a generated image along with\nits low and high frequency components to determine if properties of dehazed\nimages matches those of ground truth. We preform extensive experiments to\nvalidate the dehazing and domain invariance performance of proposed framework\nacross diverse domains and report state-of-the-art (SoTA) results.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 14:14:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Shyam", "Pranjay", ""], ["Yoon", "Kuk-Jin", ""], ["Kim", "Kyung-Soo", ""]]}, {"id": "2101.10450", "submitter": "Enkhtogtokh Togootogtokh", "authors": "Enkhtogtokh Togootogtokh, Christian Klasen", "title": "LAIF: AI, Deep Learning for Germany Suetterlin Letter Recognition and\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the successful early implementation of deep learning AI technology was\non letter recognition. With the recent breakthrough of artificial intelligence\n(AI) brings more solid technology for complex problems like handwritten letter\nrecognition and even automatic generation of them. In this research, we\nproposed deep learning framework called Ludwig AI Framework(LAIF) for Germany\nSuetterlin letter recognition and generation. To recognize Suetterlin letter,\nwe proposed deep convolutional neural network. Since lack of big amount of data\nto train for the deep models and huge cost to label existing hard copy of\nhandwritten letters, we also introduce the methodology with deep generative\nadversarial network to generate handwritten letters as synthetic data. Main\nsource code is in https://github.com/enkhtogtokh/LAIF repository.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 08:28:11 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Togootogtokh", "Enkhtogtokh", ""], ["Klasen", "Christian", ""]]}, {"id": "2101.10452", "submitter": "Renya Daimo", "authors": "Renya Daimo (1), Satoshi Ono (1), Takahiro Suzuki (1) ((1) Department\n  of Information Science and Biomedical Engineering, Graduate School of Science\n  and Engineering, Kagoshima University)", "title": "Black-box Adversarial Attacks on Monocular Depth Estimation Using\n  Evolutionary Multi-objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an adversarial attack method to deep neural networks\n(DNNs) for monocular depth estimation, i.e., estimating the depth from a single\nimage. Single image depth estimation has improved drastically in recent years\ndue to the development of DNNs. However, vulnerabilities of DNNs for image\nclassification have been revealed by adversarial attacks, and DNNs for\nmonocular depth estimation could contain similar vulnerabilities. Therefore,\nresearch on vulnerabilities of DNNs for monocular depth estimation has spread\nrapidly, but many of them assume white-box conditions where inside information\nof DNNs is available, or are transferability-based black-box attacks that\nrequire a substitute DNN model and a training dataset. Utilizing Evolutionary\nMulti-objective Optimization, the proposed method in this paper analyzes DNNs\nunder the black-box condition where only output depth maps are available. In\naddition, the proposed method does not require a substitute DNN that has a\nsimilar architecture to the target DNN nor any knowledge about training data\nused to train the target model. Experimental results showed that the proposed\nmethod succeeded in attacking two DNN-based methods that were trained with\nindoor and outdoor scenes respectively.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 14:01:11 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Daimo", "Renya", ""], ["Ono", "Satoshi", ""], ["Suzuki", "Takahiro", ""]]}, {"id": "2101.10492", "submitter": "Dayu Zhu", "authors": "Dayu Zhu, Wenshan Cai", "title": "Fast Non-line-of-sight Imaging with Two-step Deep Remapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional imaging only records photons directly sent from the object to\nthe detector, while non-line-of-sight (NLOS) imaging takes the indirect light\ninto account. Most NLOS solutions employ a transient scanning process, followed\nby a physical based algorithm to reconstruct the NLOS scenes. However, the\ntransient detection requires sophisticated apparatus, with long scanning time\nand low robustness to ambient environment, and the reconstruction algorithms\nare typically time-consuming and computationally expensive. Here we propose a\nnew NLOS solution to address the above defects, with innovations on both\nequipment and algorithm. We apply inexpensive commercial Lidar for detection,\nwith much higher scanning speed and better compatibility to real-world imaging.\nOur reconstruction framework is deep learning based, with a generative two-step\nremapping strategy to guarantee high reconstruction fidelity. The overall\ndetection and reconstruction process allows for millisecond responses, with\nreconstruction precision of millimeter level. We have experimentally tested the\nproposed solution on both synthetic and real objects, and further demonstrated\nour method to be applicable to full-color NLOS imaging.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 00:08:54 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 01:23:18 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zhu", "Dayu", ""], ["Cai", "Wenshan", ""]]}, {"id": "2101.10504", "submitter": "Ming Zhao", "authors": "Ming Zhao, Peter Anderson, Vihan Jain, Su Wang, Alexander Ku, Jason\n  Baldridge, Eugene Ie", "title": "On the Evaluation of Vision-and-Language Navigation Instructions", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vision-and-Language Navigation wayfinding agents can be enhanced by\nexploiting automatically generated navigation instructions. However, existing\ninstruction generators have not been comprehensively evaluated, and the\nautomatic evaluation metrics used to develop them have not been validated.\nUsing human wayfinders, we show that these generators perform on par with or\nonly slightly better than a template-based generator and far worse than human\ninstructors. Furthermore, we discover that BLEU, ROUGE, METEOR and CIDEr are\nineffective for evaluating grounded navigation instructions. To improve\ninstruction evaluation, we propose an instruction-trajectory compatibility\nmodel that operates without reference instructions. Our model shows the highest\ncorrelation with human wayfinding outcomes when scoring individual\ninstructions. For ranking instruction generation systems, if reference\ninstructions are available we recommend using SPICE.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:03:49 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zhao", "Ming", ""], ["Anderson", "Peter", ""], ["Jain", "Vihan", ""], ["Wang", "Su", ""], ["Ku", "Alexander", ""], ["Baldridge", "Jason", ""], ["Ie", "Eugene", ""]]}, {"id": "2101.10509", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "Continual Learning of Visual Concepts for Robots through Limited\n  Supervision", "comments": "Accepted at ACM/IEEE HRI 2021, Pioneers Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many real-world robotics applications, robots need to continually adapt\nand learn new concepts. Further, robots need to learn through limited data\nbecause of scarcity of labeled data in the real-world environments. To this\nend, my research focuses on developing robots that continually learn in dynamic\nunseen environments/scenarios, learn from limited human supervision, remember\npreviously learned knowledge and use that knowledge to learn new concepts. I\ndevelop machine learning models that not only produce State-of-the-results on\nbenchmark datasets but also allow robots to learn new objects and scenes in\nunconstrained environments which lead to a variety of novel robotics\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:26:07 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2101.10511", "submitter": "Mike Zheng Shou", "authors": "Mike Zheng Shou, Stan W. Lei, Weiyao Wang, Deepti Ghadiyaram, Matt\n  Feiszli", "title": "Generic Event Boundary Detection: A Benchmark for Event Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel task together with a new benchmark for detecting\ngeneric, taxonomy-free event boundaries that segment a whole video into chunks.\nConventional work in temporal video segmentation and action detection focuses\non localizing pre-defined action categories and thus does not scale to generic\nvideos. Cognitive Science has known since last century that humans consistently\nsegment videos into meaningful temporal chunks. This segmentation happens\nnaturally, without pre-defined event categories and without being explicitly\nasked to do so. Here, we repeat these cognitive experiments on mainstream CV\ndatasets; with our novel annotation guideline which addresses the complexities\nof taxonomy-free event boundary annotation, we introduce the task of Generic\nEvent Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our\nKinetics-GEBD has the largest number of boundaries (e.g. 32x of ActivityNet, 8x\nof EPIC-Kitchens-100) which are in-the-wild, open-vocabulary, cover generic\nevent change, and respect human perception diversity. We view GEBD as an\nimportant stepping stone towards understanding the video as a whole, and\nbelieve it has been previously neglected due to a lack of proper task\ndefinition and annotations. Through experiment and human study we demonstrate\nthe value of the annotations. Further, we benchmark supervised and\nun-supervised GEBD approaches on the TAPOS dataset and our Kinetics-GEBD,\ntogether with method design explorations that suggest future directions. We\nrelease our annotations and baseline codes at CVPR'21 LOVEU Challenge:\nhttps://sites.google.com/view/loveucvpr21\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:31:30 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 21:24:13 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 04:32:54 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Shou", "Mike Zheng", ""], ["Lei", "Stan W.", ""], ["Wang", "Weiyao", ""], ["Ghadiyaram", "Deepti", ""], ["Feiszli", "Matt", ""]]}, {"id": "2101.10514", "submitter": "Vishal Kaushal", "authors": "Vishal Kaushal, Suraj Kothawade, Anshul Tomar, Rishabh Iyer, Ganesh\n  Ramakrishnan", "title": "How Good is a Video Summary? A New Benchmarking Dataset and Evaluation\n  Framework Towards Realistic Video Summarization", "comments": "19 pages, 6 tables, 4 figures. arXiv admin note: substantial text\n  overlap with arXiv:2007.14560", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic video summarization is still an unsolved problem due to several\nchallenges. The currently available datasets either have very short videos or\nhave few long videos of only a particular type. We introduce a new benchmarking\nvideo dataset called VISIOCITY (VIdeo SummarIzatiOn based on Continuity, Intent\nand DiversiTY) which comprises of longer videos across six different categories\nwith dense concept annotations capable of supporting different flavors of video\nsummarization and other vision problems. For long videos, human reference\nsummaries necessary for supervised video summarization techniques are difficult\nto obtain. We explore strategies to automatically generate multiple reference\nsummaries from indirect ground truth present in VISIOCITY. We show that these\nsummaries are at par with human summaries. We also present a study of different\ndesired characteristics of a good summary and demonstrate how it is normal to\nhave two good summaries with different characteristics. Thus we argue that\nevaluating a summary against one or more human summaries and using a single\nmeasure has its shortcomings. We propose an evaluation framework for better\nquantitative assessment of summary quality which is closer to human judgment.\nLastly, we present insights into how a model can be enhanced to yield better\nsummaries. Sepcifically, when multiple diverse ground truth summaries can\nexist, learning from them individually and using a combination of loss\nfunctions measuring different characteristics is better than learning from a\nsingle combined (oracle) ground truth summary using a single loss function. We\ndemonstrate the effectiveness of doing so as compared to some of the\nrepresentative state of the art techniques tested on VISIOCITY. We release\nVISIOCITY as a benchmarking dataset and invite researchers to test the\neffectiveness of their video summarization algorithms on VISIOCITY.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:42:55 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Kaushal", "Vishal", ""], ["Kothawade", "Suraj", ""], ["Tomar", "Anshul", ""], ["Iyer", "Rishabh", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "2101.10531", "submitter": "Minyu Liao", "authors": "Delu Zeng, Minyu Liao, Mohammad Tavakolian, Yulan Guo, Bolei Zhou,\n  Dewen Hu, Matti Pietik\\\"ainen, Li Liu", "title": "Deep Learning for Scene Classification: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene classification, aiming at classifying a scene image to one of the\npredefined scene categories by comprehending the entire image, is a\nlongstanding, fundamental and challenging problem in computer vision. The rise\nof large-scale datasets, which constitute the corresponding dense sampling of\ndiverse real-world scenes, and the renaissance of deep learning techniques,\nwhich learn powerful feature representations directly from big raw data, have\nbeen bringing remarkable progress in the field of scene representation and\nclassification. To help researchers master needed advances in this field, the\ngoal of this paper is to provide a comprehensive survey of recent achievements\nin scene classification using deep learning. More than 200 major publications\nare included in this survey covering different aspects of scene classification,\nincluding challenges, benchmark datasets, taxonomy, and quantitative\nperformance comparisons of the reviewed methods. In retrospect of what has been\nachieved so far, this paper is also concluded with a list of promising research\nopportunities.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 03:06:50 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 04:39:10 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zeng", "Delu", ""], ["Liao", "Minyu", ""], ["Tavakolian", "Mohammad", ""], ["Guo", "Yulan", ""], ["Zhou", "Bolei", ""], ["Hu", "Dewen", ""], ["Pietik\u00e4inen", "Matti", ""], ["Liu", "Li", ""]]}, {"id": "2101.10532", "submitter": "Muhammad Ahmad", "authors": "Muhammad Ahmad, Sidrah Shabbir, Rana Aamir Raza, Manuel Mazzara,\n  Salvatore Distefano, Adil Mehmood Khan", "title": "Hyperspectral Image Classification: Artifacts of Dimension Reduction on\n  Hybrid CNN", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNN) has been extensively studied for\nHyperspectral Image Classification (HSIC) more specifically, 2D and 3D CNN\nmodels have proved highly efficient in exploiting the spatial and spectral\ninformation of Hyperspectral Images. However, 2D CNN only considers the spatial\ninformation and ignores the spectral information whereas 3D CNN jointly\nexploits spatial-spectral information at a high computational cost. Therefore,\nthis work proposed a lightweight CNN (3D followed by 2D-CNN) model which\nsignificantly reduces the computational cost by distributing spatial-spectral\nfeature extraction across a lighter model alongside a preprocessing that has\nbeen carried out to improve the classification results. Five benchmark\nHyperspectral datasets (i.e., SalinasA, Salinas, Indian Pines, Pavia\nUniversity, Pavia Center, and Botswana) are used for experimental evaluation.\nThe experimental results show that the proposed pipeline outperformed in terms\nof generalization performance, statistical significance, and computational\ncomplexity, as compared to the state-of-the-art 2D/3D CNN models except\ncommonly used computationally expensive design choices.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:43:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ahmad", "Muhammad", ""], ["Shabbir", "Sidrah", ""], ["Raza", "Rana Aamir", ""], ["Mazzara", "Manuel", ""], ["Distefano", "Salvatore", ""], ["Khan", "Adil Mehmood", ""]]}, {"id": "2101.10540", "submitter": "Nikolaos Athanasios Anagnostopoulos", "authors": "Nikolaos Athanasios Anagnostopoulos", "title": "Ear Recognition", "comments": "Submission to Biometrics in University of Twente under the auspices\n  of the EIT ICT Labs Master School in the academic year 2013-14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ear recognition can be described as a revived scientific field. Ear\nbiometrics were long believed to not be accurate enough and held a secondary\nplace in scientific research, being seen as only complementary to other types\nof biometrics, due to difficulties in measuring correctly the ear\ncharacteristics and the potential occlusion of the ear by hair, clothes and ear\njewellery. However, recent research has reinstated them as a vivid research\nfield, after having addressed these problems and proven that ear biometrics can\nprovide really accurate identification and verification results. Several 2D and\n3D imaging techniques, as well as acoustical techniques using sound emission\nand reflection, have been developed and studied for ear recognition, while\nthere have also been significant advances towards a fully automated recognition\nof the ear. Furthermore, ear biometrics have been proven to be mostly\nnon-invasive, adequately permanent and accurate, and hard to spoof and\ncounterfeit. Moreover, different ear recognition techniques have proven to be\nas effective as face recognition ones, thus providing the opportunity for ear\nrecognition to be used in identification and verification applications.\nFinally, even though some issues still remain open and require further\nresearch, the scientific field of ear biometrics has proven to be not only\nviable, but really thriving.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 03:26:00 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Anagnostopoulos", "Nikolaos Athanasios", ""]]}, {"id": "2101.10556", "submitter": "Wenliang Qian", "authors": "Wenliang Qian, Yang Xu, Wangmeng Zuo, Hui Li", "title": "Self Sparse Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "14", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) are an unsupervised generative model\nthat learns data distribution through adversarial training. However, recent\nexperiments indicated that GANs are difficult to train due to the requirement\nof optimization in the high dimensional parameter space and the zero gradient\nproblem. In this work, we propose a Self Sparse Generative Adversarial Network\n(Self-Sparse GAN) that reduces the parameter space and alleviates the zero\ngradient problem. In the Self-Sparse GAN, we design a Self-Adaptive Sparse\nTransform Module (SASTM) comprising the sparsity decomposition and feature-map\nrecombination, which can be applied on multi-channel feature maps to obtain\nsparse feature maps. The key idea of Self-Sparse GAN is to add the SASTM\nfollowing every deconvolution layer in the generator, which can adaptively\nreduce the parameter space by utilizing the sparsity in multi-channel feature\nmaps. We theoretically prove that the SASTM can not only reduce the search\nspace of the convolution kernel weight of the generator but also alleviate the\nzero gradient problem by maintaining meaningful features in the Batch\nNormalization layer and driving the weight of deconvolution layers away from\nbeing negative. The experimental results show that our method achieves the best\nFID scores for image generation compared with WGAN-GP on MNIST, Fashion-MNIST,\nCIFAR-10, STL-10, mini-ImageNet, CELEBA-HQ, and LSUN bedrooms, and the relative\ndecrease of FID is 4.76% ~ 21.84%.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 04:49:12 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Qian", "Wenliang", ""], ["Xu", "Yang", ""], ["Zuo", "Wangmeng", ""], ["Li", "Hui", ""]]}, {"id": "2101.10562", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Baptist Vandersmissen, Azarakhsh Jalalvand, Ivo\n  Couckuyt, Arnout Van Messem, Wesley De Neve", "title": "Investigating the significance of adversarial attacks and their relation\n  to interpretability for radar-based human activity recognition systems", "comments": "Accepted for publication on Computer Vision and Image Understanding,\n  Special issue on Adversarial Deep Learning in Biometrics & Forensics", "journal-ref": null, "doi": "10.1016/j.cviu.2020.103111", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given their substantial success in addressing a wide range of computer vision\nchallenges, Convolutional Neural Networks (CNNs) are increasingly being used in\nsmart home applications, with many of these applications relying on the\nautomatic recognition of human activities. In this context, low-power radar\ndevices have recently gained in popularity as recording sensors, given that the\nusage of these devices allows mitigating a number of privacy concerns, a key\nissue when making use of conventional video cameras. Another concern that is\noften cited when designing smart home applications is the resilience of these\napplications against cyberattacks. It is, for instance, well-known that the\ncombination of images and CNNs is vulnerable against adversarial examples,\nmischievous data points that force machine learning models to generate wrong\nclassifications during testing time. In this paper, we investigate the\nvulnerability of radar-based CNNs to adversarial attacks, and where these\nradar-based CNNs have been designed to recognize human gestures. Through\nexperiments with four unique threat models, we show that radar-based CNNs are\nsusceptible to both white- and black-box adversarial attacks. We also expose\nthe existence of an extreme adversarial attack case, where it is possible to\nchange the prediction made by the radar-based CNNs by only perturbing the\npadding of the inputs, without touching the frames where the action itself\noccurs. Moreover, we observe that gradient-based attacks exercise perturbation\nnot randomly, but on important features of the input data. We highlight these\nimportant features by making use of Grad-CAM, a popular neural network\ninterpretability method, hereby showing the connection between adversarial\nperturbation and prediction interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 05:16:16 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ozbulak", "Utku", ""], ["Vandersmissen", "Baptist", ""], ["Jalalvand", "Azarakhsh", ""], ["Couckuyt", "Ivo", ""], ["Van Messem", "Arnout", ""], ["De Neve", "Wesley", ""]]}, {"id": "2101.10578", "submitter": "Tajuddin Manhar Mohammed", "authors": "Tajuddin Manhar Mohammed, Lakshmanan Nataraj, Satish Chikkagoudar,\n  Shivkumar Chandrasekaran, B.S. Manjunath", "title": "Malware Detection Using Frequency Domain-Based Image Visualization and\n  Deep Learning", "comments": "Submitted version - Proceedings of the 54th Hawaii International\n  Conference on System Sciences (HICSS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel method to detect and visualize malware through image\nclassification. The executable binaries are represented as grayscale images\nobtained from the count of N-grams (N=2) of bytes in the Discrete Cosine\nTransform (DCT) domain and a neural network is trained for malware detection. A\nshallow neural network is trained for classification, and its accuracy is\ncompared with deep-network architectures such as ResNet that are trained using\ntransfer learning. Neither dis-assembly nor behavioral analysis of malware is\nrequired for these methods. Motivated by the visual similarity of these images\nfor different malware families, we compare our deep neural network models with\nstandard image features like GIST descriptors to evaluate the performance. A\njoint feature measure is proposed to combine different features using error\nanalysis to get an accurate ensemble model for improved classification\nperformance. A new dataset called MaleX which contains around 1 million malware\nand benign Windows executable samples is created for large-scale malware\ndetection and classification experiments. Experimental results are quite\npromising with 96% binary classification accuracy on MaleX. The proposed model\nis also able to generalize well on larger unseen malware samples and the\nresults compare favorably with state-of-the-art static analysis-based malware\ndetection algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:07:46 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Mohammed", "Tajuddin Manhar", ""], ["Nataraj", "Lakshmanan", ""], ["Chikkagoudar", "Satish", ""], ["Chandrasekaran", "Shivkumar", ""], ["Manjunath", "B. S.", ""]]}, {"id": "2101.10586", "submitter": "Haekyu Park", "authors": "Haekyu Park, Zijie J. Wang, Nilaksh Das, Anindya S. Paul, Pruthvi\n  Perumalla, Zhiyan Zhou, Duen Horng Chau", "title": "SkeletonVis: Interactive Visualization for Understanding Adversarial\n  Attacks on Human Action Recognition Models", "comments": "Accepted at AAAI'21 Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based human action recognition technologies are increasingly used in\nvideo based applications, such as home robotics, healthcare on aging\npopulation, and surveillance. However, such models are vulnerable to\nadversarial attacks, raising serious concerns for their use in safety-critical\napplications. To develop an effective defense against attacks, it is essential\nto understand how such attacks mislead the pose detection models into making\nincorrect predictions. We present SkeletonVis, the first interactive system\nthat visualizes how the attacks work on the models to enhance human\nunderstanding of attacks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:40:32 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Park", "Haekyu", ""], ["Wang", "Zijie J.", ""], ["Das", "Nilaksh", ""], ["Paul", "Anindya S.", ""], ["Perumalla", "Pruthvi", ""], ["Zhou", "Zhiyan", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2101.10589", "submitter": "Mehul S. Raval", "authors": "Snehal Rajput, Rupal Agravat, Mohendra Roy, Mehul S Raval", "title": "Glioblastoma Multiforme Patient Survival Prediction", "comments": "10 pages, 9 figures", "journal-ref": "2021 International Conference on Medical Imaging and\n  Computer-Aided Diagnosis (MICAD 2021)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Glioblastoma Multiforme is a very aggressive type of brain tumor. Due to\nspatial and temporal intra-tissue inhomogeneity, location and the extent of the\ncancer tissue, it is difficult to detect and dissect the tumor regions. In this\npaper, we propose survival prognosis models using four regressors operating on\nhandcrafted image-based and radiomics features. We hypothesize that the\nradiomics shape features have the highest correlation with survival prediction.\nThe proposed approaches were assessed on the Brain Tumor Segmentation\n(BraTS-2020) challenge dataset. The highest accuracy of image features with\nrandom forest regressor approach was 51.5\\% for the training and 51.7\\% for the\nvalidation dataset. The gradient boosting regressor with shape features gave an\naccuracy of 91.5\\% and 62.1\\% on training and validation datasets respectively.\nIt is better than the BraTS 2020 survival prediction challenge winners on the\ntraining and validation datasets. Our work shows that handcrafted features\nexhibit a strong correlation with survival prediction. The consensus based\nregressor with gradient boosting and radiomics shape features is the best\ncombination for survival prediction.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:47:14 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Rajput", "Snehal", ""], ["Agravat", "Rupal", ""], ["Roy", "Mohendra", ""], ["Raval", "Mehul S", ""]]}, {"id": "2101.10595", "submitter": "Pei Lv", "authors": "Pei Lv, Hui Wei, Tianxin Gu, Yuzhen Zhang, Xiaoheng Jiang, Bing Zhou\n  and Mingliang Xu", "title": "Probability Trajectory: One New Movement Description for Trajectory\n  Prediction", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction is a fundamental and challenging task for numerous\napplications, such as autonomous driving and intelligent robots. Currently,\nmost of existing work treat the pedestrian trajectory as a series of fixed\ntwo-dimensional coordinates. However, in real scenarios, the trajectory often\nexhibits randomness, and has its own probability distribution. Inspired by this\nobserved fact, also considering other movement characteristics of pedestrians,\nwe propose one simple and intuitive movement description, probability\ntrajectory, which maps the coordinate points of pedestrian trajectory into\ntwo-dimensional Gaussian distribution in images. Based on this unique\ndescription, we develop one novel trajectory prediction method, called social\nprobability. The method combines the new probability trajectory and powerful\nconvolution recurrent neural networks together. Both the input and output of\nour method are probability trajectories, which provide the recurrent neural\nnetwork with sufficient spatial and random information of moving pedestrians.\nAnd the social probability extracts spatio-temporal features directly on the\nnew movement description to generate robust and accurate predicted results. The\nexperiments on public benchmark datasets show the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 07:09:36 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 12:18:42 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Lv", "Pei", ""], ["Wei", "Hui", ""], ["Gu", "Tianxin", ""], ["Zhang", "Yuzhen", ""], ["Jiang", "Xiaoheng", ""], ["Zhou", "Bing", ""], ["Xu", "Mingliang", ""]]}, {"id": "2101.10599", "submitter": "Mehul S. Raval", "authors": "Rupal Agravat, Mehul S Raval", "title": "A Survey and Analysis on Automated Glioma Brain Tumor Segmentation and\n  Overall Patient Survival Prediction", "comments": "40 pages, 19 figures, 11 Tables", "journal-ref": "Archives of Computational Methods in Engineering, Springer, 2021", "doi": "10.1007/s11831-021-09559-w", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Glioma is the most deadly brain tumor with high mortality. Treatment planning\nby human experts depends on the proper diagnosis of physical symptoms along\nwith Magnetic Resonance(MR) image analysis. Highly variability of a brain tumor\nin terms of size, shape, location, and a high volume of MR images makes the\nanalysis time-consuming. Automatic segmentation methods achieve a reduction in\ntime with excellent reproducible results. The article aims to survey the\nadvancement of automated methods for Glioma brain tumor segmentation. It is\nalso essential to make an objective evaluation of various models based on the\nbenchmark. Therefore, the 2012 - 2019 BraTS challenges database evaluates\nstate-of-the-art methods. The complexity of tasks under the challenge has grown\nfrom segmentation (Task1) to overall survival prediction (Task 2) to\nuncertainty prediction for classification (Task 3). The paper covers the\ncomplete gamut of brain tumor segmentation using handcrafted features to deep\nneural network models for Task 1. The aim is to showcase a complete change of\ntrends in automated brain tumor models. The paper also covers end to end joint\nmodels involving brain tumor segmentation and overall survival prediction. All\nthe methods are probed, and parameters that affect performance are tabulated\nand analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 07:22:52 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 15:34:56 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Agravat", "Rupal", ""], ["Raval", "Mehul S", ""]]}, {"id": "2101.10620", "submitter": "Liang Lin", "authors": "Liang Lin and Yiming Gao and Ke Gong and Meng Wang and Xiaodan Liang", "title": "Graphonomy: Universal Image Parsing via Graph Reasoning and Transfer", "comments": "To appear in IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE\n  INTELLIGENCE (T-PAMI) 2021. We propose a graph reasoning and transfer\n  learning framework, which incorporates human knowledge and label taxonomy\n  into the intermediate graph representation learning beyond local\n  convolutions. arXiv admin note: substantial text overlap with\n  arXiv:1904.04536", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior highly-tuned image parsing models are usually studied in a certain\ndomain with a specific set of semantic labels and can hardly be adapted into\nother scenarios (e.g., sharing discrepant label granularity) without extensive\nre-training. Learning a single universal parsing model by unifying label\nannotations from different domains or at various levels of granularity is a\ncrucial but rarely addressed topic. This poses many fundamental learning\nchallenges, e.g., discovering underlying semantic structures among different\nlabel granularity or mining label correlation across relevant tasks. To address\nthese challenges, we propose a graph reasoning and transfer learning framework,\nnamed \"Graphonomy\", which incorporates human knowledge and label taxonomy into\nthe intermediate graph representation learning beyond local convolutions. In\nparticular, Graphonomy learns the global and structured semantic coherency in\nmultiple domains via semantic-aware graph reasoning and transfer, enforcing the\nmutual benefits of the parsing across domains (e.g., different datasets or\nco-related tasks). The Graphonomy includes two iterated modules: Intra-Graph\nReasoning and Inter-Graph Transfer modules. The former extracts the semantic\ngraph in each domain to improve the feature representation learning by\npropagating information with the graph; the latter exploits the dependencies\namong the graphs from different domains for bidirectional knowledge transfer.\nWe apply Graphonomy to two relevant but different image understanding research\ntopics: human parsing and panoptic segmentation, and show Graphonomy can handle\nboth of them well via a standard pipeline against current state-of-the-art\napproaches. Moreover, some extra benefit of our framework is demonstrated,\ne.g., generating the human parsing at various levels of granularity by unifying\nannotations across different datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 08:19:03 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lin", "Liang", ""], ["Gao", "Yiming", ""], ["Gong", "Ke", ""], ["Wang", "Meng", ""], ["Liang", "Xiaodan", ""]]}, {"id": "2101.10629", "submitter": "Gennaro Vessio Dr.", "authors": "Eufemia Lella, Gennaro Vessio", "title": "Ensembling complex network 'perspectives' for mild cognitive impairment\n  detection with artificial neural networks", "comments": null, "journal-ref": "Pattern Recognition Letters, Volume 136, August 2020, Pages\n  168-174", "doi": "10.1016/j.patrec.2020.06.001", "report-no": null, "categories": "cs.CV eess.IV q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose a novel method for mild cognitive impairment\ndetection based on jointly exploiting the complex network and the neural\nnetwork paradigm. In particular, the method is based on ensembling different\nbrain structural \"perspectives\" with artificial neural networks. On one hand,\nthese perspectives are obtained with complex network measures tailored to\ndescribe the altered brain connectivity. In turn, the brain reconstruction is\nobtained by combining diffusion-weighted imaging (DWI) data to tractography\nalgorithms. On the other hand, artificial neural networks provide a means to\nlearn a mapping from topological properties of the brain to the presence or\nabsence of cognitive decline. The effectiveness of the method is studied on a\nwell-known benchmark data set in order to evaluate if it can provide an\nautomatic tool to support the early disease diagnosis. Also, the effects of\nbalancing issues are investigated to further assess the reliability of the\ncomplex network approach to DWI data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 08:38:11 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lella", "Eufemia", ""], ["Vessio", "Gennaro", ""]]}, {"id": "2101.10633", "submitter": "Cui Jiequan", "authors": "Jiequan Cui, Shu Liu, Zhuotao Tian, Zhisheng Zhong, Jiaya Jia", "title": "ResLT: Residual Learning for Long-tailed Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning algorithms face great challenges with long-tailed data\ndistribution which, however, is quite a common case in real-world scenarios.\nPrevious methods tackle the problem from either the aspect of input space\n(re-sampling classes with different frequencies) or loss space (re-weighting\nclasses with different weights), suffering from heavy over-fitting to tail\nclasses or hard optimization during training. To alleviate these issues, we\npropose a more fundamental perspective for long-tailed recognition, {i.e., from\nthe aspect of parameter space, and aims to preserve specific capacity for\nclasses with low frequencies. From this perspective, the trivial solution\nutilizes different branches for the head, medium, tail classes respectively,\nand then sums their outputs as the final results is not feasible. Instead, we\ndesign the effective residual fusion mechanism -- with one main branch\noptimized to recognize images from all classes, another two residual branches\nare gradually fused and optimized to enhance images from medium+tail classes\nand tail classes respectively. Then the branches are aggregated into final\nresults by additive shortcuts. We test our method on several benchmarks, {i.e.,\nlong-tailed version of CIFAR-10, CIFAR-100, Places, ImageNet, and iNaturalist\n2018. Experimental results manifest that our method achieves new\nstate-of-the-art for long-tailed recognition. Code will be available at\n\\url{https://github.com/FPNAS/ResLT}.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 08:43:50 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 01:30:23 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Cui", "Jiequan", ""], ["Liu", "Shu", ""], ["Tian", "Zhuotao", ""], ["Zhong", "Zhisheng", ""], ["Jia", "Jiaya", ""]]}, {"id": "2101.10667", "submitter": "Xin He", "authors": "Xin He, Shihao Wang, Guohao Ying, Jiyong Zhang, Xiaowen Chu", "title": "Efficient Multi-objective Evolutionary 3D Neural Architecture Search for\n  COVID-19 Detection with Chest CT Scans", "comments": "Neural Architecture Search, Evolutionary Algorithm, COVID-19, CT", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 pandemic has spread globally for months. Due to its long incubation\nperiod and high testing cost, there is no clue showing its spread speed is\nslowing down, and hence a faster testing method is in dire need. This paper\nproposes an efficient Evolutionary Multi-objective neural ARchitecture Search\n(EMARS) framework, which can automatically search for 3D neural architectures\nbased on a well-designed search space for COVID-19 chest CT scan\nclassification. Within the framework, we use weight sharing strategy to\nsignificantly improve the search efficiency and finish the search process in 8\nhours. We also propose a new objective, namely potential, which is of benefit\nto improve the search process's robustness. With the objectives of accuracy,\npotential, and model size, we find a lightweight model (3.39 MB), which\noutperforms three baseline human-designed models, i.e., ResNet3D101 (325.21\nMB), DenseNet3D121 (43.06 MB), and MC3\\_18 (43.84 MB). Besides, our\nwell-designed search space enables the class activation mapping algorithm to be\neasily embedded into all searched models, which can provide the\ninterpretability for medical diagnosis by visualizing the judgment based on the\nmodels to locate the lesion areas.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:52:42 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["He", "Xin", ""], ["Wang", "Shihao", ""], ["Ying", "Guohao", ""], ["Zhang", "Jiyong", ""], ["Chu", "Xiaowen", ""]]}, {"id": "2101.10674", "submitter": "Benjamin Lambert", "authors": "Benjamin Lambert, Maxime Louis, Senan Doyle, Florence Forbes, Michel\n  Dojat, Alan Tucholka", "title": "Leveraging 3D Information in Unsupervised Brain MRI Segmentation", "comments": "Accepted for presentation at IEEE International Symposium on\n  Biomedical Imaging 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic segmentation of brain abnormalities is challenging, as they vary\nconsiderably from one pathology to another. Current methods are supervised and\nrequire numerous annotated images for each pathology, a strenuous task. To\ntackle anatomical variability, Unsupervised Anomaly Detection (UAD) methods are\nproposed, detecting anomalies as outliers of a healthy model learned using a\nVariational Autoencoder (VAE). Previous work on UAD adopted a 2D approach,\nmeaning that MRIs are processed as a collection of independent slices. Yet, it\ndoes not fully exploit the spatial information contained in MRI. Here, we\npropose to perform UAD in a 3D fashion and compare 2D and 3D VAEs. As a side\ncontribution, we present a new loss function guarantying a robust training.\nLearning is performed using a multicentric dataset of healthy brain MRIs, and\nsegmentation performances are estimated on White-Matter Hyperintensities and\ntumors lesions. Experiments demonstrate the interest of 3D methods which\noutperform their 2D counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:04:57 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lambert", "Benjamin", ""], ["Louis", "Maxime", ""], ["Doyle", "Senan", ""], ["Forbes", "Florence", ""], ["Dojat", "Michel", ""], ["Tucholka", "Alan", ""]]}, {"id": "2101.10696", "submitter": "Yaxiong Wang", "authors": "Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, Yi Yang", "title": "AINet: Association Implantation for Superpixel Segmentation", "comments": "Superpixel Segmentation, Computer Version, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, some approaches are proposed to harness deep convolutional networks\nto facilitate superpixel segmentation. The common practice is to first evenly\ndivide the image into a pre-defined number of grids and then learn to associate\neach pixel with its surrounding grids. However, simply applying a series of\nconvolution operations with limited receptive fields can only implicitly\nperceive the relations between the pixel and its surrounding grids.\nConsequently, existing methods often fail to provide an effective context when\ninferring the association map. To remedy this issue, we propose a novel\n\\textbf{A}ssociation \\textbf{I}mplantation (AI) module to enable the network to\nexplicitly capture the relations between the pixel and its surrounding grids.\nThe proposed AI module directly implants the features of grid cells to the\nsurrounding of its corresponding central pixel, and conducts convolution on the\npadded window to adaptively transfer knowledge between them. With such an\nimplantation operation, the network could explicitly harvest the pixel-grid\nlevel context, which is more in line with the target of superpixel segmentation\ncomparing to the pixel-wise relation. Furthermore, to pursue better boundary\nprecision, we design a boundary-perceiving loss to help the network\ndiscriminate the pixels around boundaries in hidden feature level, which could\nbenefit the subsequent inferring modules to accurately identify more boundary\npixels. Extensive experiments on BSDS500 and NYUv2 datasets show that our\nmethod could not only achieve state-of-the-art performance but maintain\nsatisfactory inference efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:40:13 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 03:57:21 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Yaxiong", ""], ["Wei", "Yunchao", ""], ["Qian", "Xueming", ""], ["Zhu", "Li", ""], ["Yang", "Yi", ""]]}, {"id": "2101.10710", "submitter": "Mohammad Naser Sabet Jahromi", "authors": "Satya M. Muddamsetty, Mohammad N. S. Jahromi, Andreea E. Ciontos,\n  Laura M. Fenoy, Thomas B. Moeslund", "title": "Introducing and assessing the explainable AI (XAI)method: SIDU", "comments": "Preprint-submitted to Journal of Pattern Recognition (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable Artificial Intelligence (XAI) has in recent years become a\nwell-suited framework to generate human understandable explanations of black\nbox models. In this paper, we present a novel XAI visual explanation algorithm\ndenoted SIDU that can effectively localize entire object regions responsible\nfor prediction in a full extend. We analyze its robustness and effectiveness\nthrough various computational and human subject experiments. In particular, we\nassess the SIDU algorithm using three different types of evaluations\n(Application, Human and Functionally-Grounded) to demonstrate its superior\nperformance. The robustness of SIDU is further studied in presence of\nadversarial attack on black box models to better understand its performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:13:50 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Muddamsetty", "Satya M.", ""], ["Jahromi", "Mohammad N. S.", ""], ["Ciontos", "Andreea E.", ""], ["Fenoy", "Laura M.", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "2101.10734", "submitter": "Stuart James", "authors": "Mohamed Dahy Elkhouly, Alessio Del Bue, Stuart James", "title": "Consistent Mesh Colors for Multi-View Reconstructed 3D Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of creating consistent mesh texture maps captured from\nscenes without color calibration. We find that the method for aggregation of\nthe multiple views is crucial for creating spatially consistent meshes without\nthe need to explicitly optimize for spatial consistency. We compute a color\nprior from the cross-correlation of observable view faces and the faces per\nview to identify an optimal per-face color. We then use this color in a\nre-weighting ratio for the best-view texture, which is identified by prior mesh\ntexturing work, to create a spatial consistent texture map. Despite our method\nnot explicitly handling spatial consistency, our results show qualitatively\nmore consistent results than other state-of-the-art techniques while being\ncomputationally more efficient. We evaluate on prior datasets and additionally\nMatterport3D showing qualitative improvements.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:59:23 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Elkhouly", "Mohamed Dahy", ""], ["Del Bue", "Alessio", ""], ["James", "Stuart", ""]]}, {"id": "2101.10747", "submitter": "Mazen Abdelfattah Mr", "authors": "Mazen Abdelfattah, Kaiwen Yuan, Z. Jane Wang, Rabab Ward", "title": "Towards Universal Physical Attacks On Cascaded Camera-Lidar 3D Object\n  Detection Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a universal and physically realizable adversarial attack on a\ncascaded multi-modal deep learning network (DNN), in the context of\nself-driving cars. DNNs have achieved high performance in 3D object detection,\nbut they are known to be vulnerable to adversarial attacks. These attacks have\nbeen heavily investigated in the RGB image domain and more recently in the\npoint cloud domain, but rarely in both domains simultaneously - a gap to be\nfilled in this paper. We use a single 3D mesh and differentiable rendering to\nexplore how perturbing the mesh's geometry and texture can reduce the\nrobustness of DNNs to adversarial attacks. We attack a prominent cascaded\nmulti-modal DNN, the Frustum-Pointnet model. Using the popular KITTI benchmark,\nwe showed that the proposed universal multi-modal attack was successful in\nreducing the model's ability to detect a car by nearly 73%. This work can aid\nin the understanding of what the cascaded RGB-point cloud DNN learns and its\nvulnerability to adversarial attacks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 12:40:34 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 18:40:27 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Abdelfattah", "Mazen", ""], ["Yuan", "Kaiwen", ""], ["Wang", "Z. Jane", ""], ["Ward", "Rabab", ""]]}, {"id": "2101.10760", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Muchen Li, Wenxiu Sun, Ming-Hsuan Yang", "title": "Learning Spatial and Spatio-Temporal Pixel Aggregations for Image and\n  Video Denoising", "comments": "Project page: https://sites.google.com/view/xiangyuxu/denoise_stpan.\n  arXiv admin note: substantial text overlap with arXiv:1904.06903", "journal-ref": "IEEE Transactions on Image Processing 29 (2020): 7153-7165", "doi": "10.1109/TIP.2020.2999209", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing denoising methods typically restore clear results by aggregating\npixels from the noisy input. Instead of relying on hand-crafted aggregation\nschemes, we propose to explicitly learn this process with deep neural networks.\nWe present a spatial pixel aggregation network and learn the pixel sampling and\naveraging strategies for image denoising. The proposed model naturally adapts\nto image structures and can effectively improve the denoised results.\nFurthermore, we develop a spatio-temporal pixel aggregation network for video\ndenoising to efficiently sample pixels across the spatio-temporal space. Our\nmethod is able to solve the misalignment issues caused by large motion in\ndynamic scenes. In addition, we introduce a new regularization term for\neffectively training the proposed video denoising model. We present extensive\nanalysis of the proposed method and demonstrate that our model performs\nfavorably against the state-of-the-art image and video denoising approaches on\nboth synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:00:46 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Xu", "Xiangyu", ""], ["Li", "Muchen", ""], ["Sun", "Wenxiu", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2101.10772", "submitter": "Stuart James", "authors": "Mohamed Dahy Elkhouly, Theodore Tsesmelis, Alessio Del Bue, Stuart\n  James", "title": "LIGHTS: LIGHT Specularity Dataset for specular detection in Multi-view", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specular highlights are commonplace in images, however, methods for detecting\nthem and in turn removing the phenomenon are particularly challenging. A reason\nfor this, is due to the difficulty of creating a dataset for training or\nevaluation, as in the real-world we lack the necessary control over the\nenvironment. Therefore, we propose a novel physically-based rendered LIGHT\nSpecularity (LIGHTS) Dataset for the evaluation of the specular highlight\ndetection task. Our dataset consists of 18 high quality architectural scenes,\nwhere each scene is rendered with multiple views. In total we have 2,603 views\nwith an average of 145 views per scene. Additionally we propose a simple\naggregation based method for specular highlight detection that outperforms\nprior work by 3.6% in two orders of magnitude less time on our dataset.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:26:49 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Elkhouly", "Mohamed Dahy", ""], ["Tsesmelis", "Theodore", ""], ["Del Bue", "Alessio", ""], ["James", "Stuart", ""]]}, {"id": "2101.10774", "submitter": "Fabian Herzog", "authors": "Fabian Herzog, Xunbo Ji, Torben Teepe, Stefan H\\\"ormann, Johannes\n  Gilg, Gerhard Rigoll", "title": "Lightweight Multi-Branch Network for Person Re-Identification", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification aims to retrieve person identities from images\ncaptured by multiple cameras or the same cameras in different time instances\nand locations. Because of its importance in many vision applications from\nsurveillance to human-machine interaction, person re-identification methods\nneed to be reliable and fast. While more and more deep architectures are\nproposed for increasing performance, those methods also increase overall model\ncomplexity. This paper proposes a lightweight network that combines global,\npart-based, and channel features in a unified multi-branch architecture that\nbuilds on the resource-efficient OSNet backbone. Using a well-founded\ncombination of training techniques and design choices, our final model achieves\nstate-of-the-art results on CUHK03 labeled, CUHK03 detected, and Market-1501\nwith 85.1% mAP / 87.2% rank1, 82.4% mAP / 84.9% rank1, and 91.5% mAP / 96.3%\nrank1, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:28:46 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Herzog", "Fabian", ""], ["Ji", "Xunbo", ""], ["Teepe", "Torben", ""], ["H\u00f6rmann", "Stefan", ""], ["Gilg", "Johannes", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "2101.10775", "submitter": "Leonardo Parisi", "authors": "Andrea Cavagna, Xiao Feng, Stefania Melillo, Leonardo Parisi, Lorena\n  Postiglione, Pablo Villegas", "title": "CoMo: A novel co-moving 3D camera system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the theoretical interest in reconstructing long 3D trajectories\nof individual birds in large flocks, we developed CoMo, a co-moving camera\nsystem of two synchronized high speed cameras coupled with rotational stages,\nwhich allow us to dynamically follow the motion of a target flock. With the\nrotation of the cameras we overcome the limitations of standard static systems\nthat restrict the duration of the collected data to the short interval of time\nin which targets are in the cameras common field of view, but at the same time\nwe change in time the external parameters of the system, which have then to be\ncalibrated frame-by-frame. We address the calibration of the external\nparameters measuring the position of the cameras and their three angles of yaw,\npitch and roll in the system \"home\" configuration (rotational stage at an angle\nequal to 0deg and combining this static information with the time dependent\nrotation due to the stages. We evaluate the robustness and accuracy of the\nsystem by comparing reconstructed and measured 3D distances in what we call 3D\ntests, which show a relative error of the order of 1%. The novelty of the work\npresented in this paper is not only on the system itself, but also on the\napproach we use in the tests, which we show to be a very powerful tool in\ndetecting and fixing calibration inaccuracies and that, for this reason, may be\nrelevant for a broad audience.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:29:13 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cavagna", "Andrea", ""], ["Feng", "Xiao", ""], ["Melillo", "Stefania", ""], ["Parisi", "Leonardo", ""], ["Postiglione", "Lorena", ""], ["Villegas", "Pablo", ""]]}, {"id": "2101.10777", "submitter": "Josip \\v{S}ari\\'c", "authors": "Josip \\v{S}ari\\'c and Sacha Vra\\v{z}i\\'c and Sini\\v{s}a \\v{S}egvi\\'c", "title": "Joint Forecasting of Features and Feature Motion for Dense Semantic\n  Future Prediction", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel dense semantic forecasting approach which is applicable to\na variety of architectures and tasks. The approach consists of two modules.\nFeature-to-motion (F2M) module forecasts a dense deformation field which warps\npast features into their future positions. Feature-to-feature (F2F) module\nregresses the future features directly and is therefore able to account for\nemergent scenery. The compound F2MF approach decouples effects of motion from\nthe effects of novelty in a task-agnostic manner. We aim to apply F2MF\nforecasting to the most subsampled and the most abstract representation of a\ndesired single-frame model. Our implementations take advantage of deformable\nconvolutions and pairwise correlation coefficients across neighbouring time\ninstants. We perform experiments on three dense prediction tasks: semantic\nsegmentation, instance-level segmentation, and panoptic segmentation. The\nresults reveal state-of-the-art forecasting accuracy across all three\nmodalities on the Cityscapes dataset.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:30:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["\u0160ari\u0107", "Josip", ""], ["Vra\u017ei\u0107", "Sacha", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "2101.10785", "submitter": "Michael Gresser", "authors": "Marc Franzen, Michael Stephan Gresser, Tobias M\\\"uller, Prof. Dr.\n  Sebastian Mauser", "title": "Developing emotion recognition for video conference software to support\n  people with autism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an emotion recognition software for the use with a video\nconference software for autistic individuals which are unable to recognize\nemotions properly. It can get an image out of the video stream, detect the\nemotion in it with the help of a neural network and display the prediction to\nthe user. The network is trained on facial landmark features. The software is\nfully modular to support adaption to different video conference software,\nprogramming languages and implementations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:54:36 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Franzen", "Marc", ""], ["Gresser", "Michael Stephan", ""], ["M\u00fcller", "Tobias", ""], ["Mauser", "Prof. Dr. Sebastian", ""]]}, {"id": "2101.10799", "submitter": "Xiaowei Xu", "authors": "Xiaowei Xu, Tianchen Wang, Jian Zhuang, Haiyun Yuan, Meiping Huang,\n  Jianzheng Cen, Qianjun Jia, Yuhao Dong, Yiyu Shi", "title": "ImageCHD: A 3D Computed Tomography Image Dataset for Classification of\n  Congenital Heart Disease", "comments": "11 pages, 6 figures, 2 tables, published at MICCAI 2020. The\n  diagnosis info of the dataset is updated (thanks to the help of Kadirbarut\n  from Bilgiuzayi)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Congenital heart disease (CHD) is the most common type of birth defect, which\noccurs 1 in every 110 births in the United States. CHD usually comes with\nsevere variations in heart structure and great artery connections that can be\nclassified into many types. Thus highly specialized domain knowledge and the\ntime-consuming human process is needed to analyze the associated medical\nimages. On the other hand, due to the complexity of CHD and the lack of\ndataset, little has been explored on the automatic diagnosis (classification)\nof CHDs. In this paper, we present ImageCHD, the first medical image dataset\nfor CHD classification. ImageCHD contains 110 3D Computed Tomography (CT)\nimages covering most types of CHD, which is of decent size Classification of\nCHDs requires the identification of large structural changes without any local\ntissue changes, with limited data. It is an example of a larger class of\nproblems that are quite difficult for current machine-learning-based vision\nmethods to solve. To demonstrate this, we further present a baseline framework\nfor the automatic classification of CHD, based on a state-of-the-art CHD\nsegmentation method. Experimental results show that the baseline framework can\nonly achieve a classification accuracy of 82.0\\% under a selective prediction\nscheme with 88.4\\% coverage, leaving big room for further improvement. We hope\nthat ImageCHD can stimulate further research and lead to innovative and generic\nsolutions that would have an impact in multiple domains. Our dataset is\nreleased to the public compared with existing medical imaging datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:15:31 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 02:07:17 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Xu", "Xiaowei", ""], ["Wang", "Tianchen", ""], ["Zhuang", "Jian", ""], ["Yuan", "Haiyun", ""], ["Huang", "Meiping", ""], ["Cen", "Jianzheng", ""], ["Jia", "Qianjun", ""], ["Dong", "Yuhao", ""], ["Shi", "Yiyu", ""]]}, {"id": "2101.10801", "submitter": "Sihan Chen", "authors": "Sihan Chen, Xinxin Zhu, Wei Liu, Xingjian He, Jing Liu", "title": "Global-Local Propagation Network for RGB-D Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth information matters in RGB-D semantic segmentation task for providing\nadditional geometric information to color images. Most existing methods exploit\na multi-stage fusion strategy to propagate depth feature to the RGB branch.\nHowever, at the very deep stage, the propagation in a simple element-wise\naddition manner can not fully utilize the depth information. We propose\nGlobal-Local propagation network (GLPNet) to solve this problem. Specifically,\na local context fusion module(L-CFM) is introduced to dynamically align both\nmodalities before element-wise fusion, and a global context fusion\nmodule(G-CFM) is introduced to propagate the depth information to the RGB\nbranch by jointly modeling the multi-modal global context features. Extensive\nexperiments demonstrate the effectiveness and complementarity of the proposed\nfusion modules. Embedding two fusion modules into a two-stream encoder-decoder\nstructure, our GLPNet achieves new state-of-the-art performance on two\nchallenging indoor scene segmentation datasets, i.e., NYU-Depth v2 and SUN-RGBD\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:26:07 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chen", "Sihan", ""], ["Zhu", "Xinxin", ""], ["Liu", "Wei", ""], ["He", "Xingjian", ""], ["Liu", "Jing", ""]]}, {"id": "2101.10803", "submitter": "Sangho Lee", "authors": "Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal\n  Chechik, Yale Song", "title": "Automatic Curation of Large-Scale Datasets for Audio-Visual\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale datasets are the cornerstone of representation learning. Existing\nself-supervised approaches extract learning signals by making certain\nassumptions about the data, e.g., spatio-temporal continuity and multimodal\ncorrespondence. However, finding large amounts of data that satisfy such\nassumptions is not straightforward, and this restricts the community to rely on\ndatasets collected through laborious annotation and/or manual filtering\nprocesses. In this paper, we propose a subset optimization approach for\nautomatic dataset curation. Focusing on audio-visual representation learning,\nwe find a subset that provides the maximum mutual information between audio and\nvisual channels in videos. We show that self-supervised models trained on our\ndata, despite being automatically constructed, achieve competitive downstream\nperformances compared to existing datasets that require annotation and/or\nmanual filtering. The most significant benefit of our approach is scalability.\nWe release a dataset of 100M videos with high audio-visual correspondence.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:27:47 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 13:19:45 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Lee", "Sangho", ""], ["Chung", "Jiwan", ""], ["Yu", "Youngjae", ""], ["Kim", "Gunhee", ""], ["Breuel", "Thomas", ""], ["Chechik", "Gal", ""], ["Song", "Yale", ""]]}, {"id": "2101.10804", "submitter": "Wei Liu", "authors": "Wei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu, Jing Liu", "title": "CPTR: Full Transformer Network for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the image captioning task from a new\nsequence-to-sequence prediction perspective and propose CaPtion TransformeR\n(CPTR) which takes the sequentialized raw images as the input to Transformer.\nCompared to the \"CNN+Transformer\" design paradigm, our model can model global\ncontext at every encoder layer from the beginning and is totally\nconvolution-free. Extensive experiments demonstrate the effectiveness of the\nproposed model and we surpass the conventional \"CNN+Transformer\" methods on the\nMSCOCO dataset. Besides, we provide detailed visualizations of the\nself-attention between patches in the encoder and the \"words-to-patches\"\nattention in the decoder thanks to the full Transformer architecture.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:29:52 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 13:10:00 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 04:38:38 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Liu", "Wei", ""], ["Chen", "Sihan", ""], ["Guo", "Longteng", ""], ["Zhu", "Xinxin", ""], ["Liu", "Jing", ""]]}, {"id": "2101.10807", "submitter": "Anthony Bourached", "authors": "Anthony Bourached, George Cann, Ryan-Rhys Griffiths, David G. Stork", "title": "Recovery of underdrawings and ghost-paintings via style transfer by deep\n  convolutional neural networks: A digital tool for art scholars", "comments": "Accepted to Electronic Imaging 2021: Computer Vision and image\n  Analysis of Art", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the application of convolutional neural network style transfer to\nthe problem of improved visualization of underdrawings and ghost-paintings in\nfine art oil paintings. Such underdrawings and hidden paintings are typically\nrevealed by x-ray or infrared techniques which yield images that are grayscale,\nand thus devoid of color and full style information. Past methods for inferring\ncolor in underdrawings have been based on physical x-ray fluorescence spectral\nimaging of pigments in ghost-paintings and are thus expensive, time consuming,\nand require equipment not available in most conservation studios. Our\nalgorithmic methods do not need such expensive physical imaging devices. Our\nproof-of-concept system, applied to works by Pablo Picasso and Leonardo, reveal\ncolors and designs that respect the natural segmentation in the ghost-painting.\nWe believe the computed images provide insight into the artist and associated\noeuvre not available by other means. Our results strongly suggest that future\napplications based on larger corpora of paintings for training will display\ncolor schemes and designs that even more closely resemble works of the artist.\nFor these reasons refinements to our methods should find wide use in art\nconservation, connoisseurship, and art analysis.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:24:11 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Bourached", "Anthony", ""], ["Cann", "George", ""], ["Griffiths", "Ryan-Rhys", ""], ["Stork", "David G.", ""]]}, {"id": "2101.10808", "submitter": "Kostiantyn Khabarlak", "authors": "Kostiantyn Khabarlak, Larysa Koriashkina", "title": "Fast Facial Landmark Detection and Applications: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we survey and analyze modern neural-network-based facial\nlandmark detection algorithms. We focus on approaches that have led to a\nsignificant increase in quality over the past few years on datasets with large\npose and emotion variability, high levels of face occlusions - all of which are\ntypical in real-world scenarios. We summarize the improvements into categories,\nprovide quality comparison on difficult and modern in-the-wild datasets: 300-W,\nAFLW, WFLW, COFW. Additionally, we compare algorithm speed on CPU, GPU and\nMobile devices. For completeness, we also briefly touch on established methods\nwith open implementations available. Besides, we cover applications and\nvulnerabilities of the landmark detection algorithms. Based on which, we raise\nproblems that as we hope will lead to further algorithm improvements.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:40:40 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 05:14:54 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Khabarlak", "Kostiantyn", ""], ["Koriashkina", "Larysa", ""]]}, {"id": "2101.10809", "submitter": "Nuredin Ali", "authors": "Nuredin Ali", "title": "Exploring Transfer Learning on Face Recognition of Dark Skinned, Low\n  Quality and Low Resource Face Data", "comments": "3 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a big difference in the tone of color of skin between dark and light\nskinned people. Despite this fact, most face recognition tasks almost all\nclassical state-of-the-art models are trained on datasets containing an\noverwhelming majority of light skinned face images. It is tedious to collect a\nhuge amount of data for dark skinned faces and train a model from scratch. In\nthis paper, we apply transfer learning on VGGFace to check how it works on\nrecognising dark skinned mainly Ethiopian faces. The dataset is of low quality\nand low resource. Our experimental results show above 95\\% accuracy which\nindicates that transfer learning in such settings works.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 19:50:15 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ali", "Nuredin", ""]]}, {"id": "2101.10811", "submitter": "Ju He", "authors": "Ju He, Enyu Zhou, Liusheng Sun, Fei Lei, Chenyang Liu, Wenxiu Sun", "title": "Semi-synthesis: A fast way to produce effective datasets for stereo\n  matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo matching is an important problem in computer vision which has drawn\ntremendous research attention for decades. Recent years, data-driven methods\nwith convolutional neural networks (CNNs) are continuously pushing stereo\nmatching to new heights. However, data-driven methods require large amount of\ntraining data, which is not an easy task for real stereo data due to the\nannotation difficulties of per-pixel ground-truth disparity. Though synthetic\ndataset is proposed to fill the gaps of large data demand, the fine-tuning on\nreal dataset is still needed due to the domain variances between synthetic data\nand real data. In this paper, we found that in synthetic datasets,\nclose-to-real-scene texture rendering is a key factor to boost up stereo\nmatching performance, while close-to-real-scene 3D modeling is less important.\nWe then propose semi-synthetic, an effective and fast way to synthesize large\namount of data with close-to-real-scene texture to minimize the gap between\nsynthetic data and real data. Extensive experiments demonstrate that models\ntrained with our proposed semi-synthetic datasets achieve significantly better\nperformance than with general synthetic datasets, especially on real data\nbenchmarks with limited training data. With further fine-tuning on the real\ndataset, we also achieve SOTA performance on Middlebury and competitive results\non KITTI and ETH3D datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:34:49 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["He", "Ju", ""], ["Zhou", "Enyu", ""], ["Sun", "Liusheng", ""], ["Lei", "Fei", ""], ["Liu", "Chenyang", ""], ["Sun", "Wenxiu", ""]]}, {"id": "2101.10815", "submitter": "Jun Ma", "authors": "Jun Ma", "title": "Loss Ensembles for Extremely Imbalanced Segmentation", "comments": "1st place in MICCAI ADAM Challenge segmentation task", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This short paper briefly presents our methodology details of automatic\nintracranial aneurysms segmentation from brain MR scans. We use ensembles of\nmultiple models trained from different loss functions. Our method ranked first\nplace in the ADAM challenge segmentation task. The code and trained models are\npublicly available at https://github.com/JunMa11/ADAM2020.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 12:13:27 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ma", "Jun", ""]]}, {"id": "2101.10832", "submitter": "Yulin Wang", "authors": "Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, Gao Huang", "title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end\n  Training", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the need to store the intermediate activations for back-propagation,\nend-to-end (E2E) training of deep networks usually suffers from high GPUs\nmemory footprint. This paper aims to address this problem by revisiting the\nlocally supervised learning, where a network is split into gradient-isolated\nmodules and trained with local supervision. We experimentally show that simply\ntraining local modules with E2E loss tends to collapse task-relevant\ninformation at early layers, and hence hurts the performance of the full model.\nTo avoid this issue, we propose an information propagation (InfoPro) loss,\nwhich encourages local modules to preserve as much useful information as\npossible, while progressively discard task-irrelevant information. As InfoPro\nloss is difficult to compute in its original form, we derive a feasible upper\nbound as a surrogate optimization objective, yielding a simple but effective\nalgorithm. In fact, we show that the proposed method boils down to minimizing\nthe combination of a reconstruction loss and a normal cross-entropy/contrastive\nterm. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10,\nImageNet and Cityscapes) validate that InfoPro is capable of achieving\ncompetitive performance with less than 40% memory footprint compared to E2E\ntraining, while allowing using training data with higher-resolution or larger\nbatch sizes under the same GPU memory constraint. Our method also enables\ntraining local modules asynchronously for potential training acceleration. Code\nis available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:02:18 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Wang", "Yulin", ""], ["Ni", "Zanlin", ""], ["Song", "Shiji", ""], ["Yang", "Le", ""], ["Huang", "Gao", ""]]}, {"id": "2101.10833", "submitter": "Amr Hilal", "authors": "Amr E Hilal, Ismail Arai, Samy El-Tawab", "title": "DataLoc+: A Data Augmentation Technique for Machine Learning in\n  Room-Level Indoor Localization", "comments": "7 pages, 7 figures, 1 table, 1 algorithm. Accepted at IEEE WCNC 2021,\n  and final version is submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor localization has been a hot area of research over the past two\ndecades. Since its advent, it has been steadily utilizing the emerging\ntechnologies to improve accuracy, and machine learning has been at the heart of\nthat. Machine learning has been increasingly used in fingerprint-based indoor\nlocalization to replace or emulate the radio map that is used to predict\nlocations given a location signature. The prediction quality of a machine\nlearning model primarily depends on how well the model was trained, which\nrelies on the amount and quality of data used to train it. Data augmentation\nhas been used to improve quality of the trained models by synthetically\nproducing more training data, and several approaches were used in the\nliterature that tackles the problem of lack of training data from different\nangles. In this paper, we propose DataLoc+, a data augmentation technique for\nroom-level indoor localization that combines different approaches in a simple\nalgorithm. We evaluate the technique by comparing it to the typical direct\nsnapshot approach using data collected from a field experiment conducted in a\nhospital. Our evaluation shows that the model trained using the proposed\ntechnique achieves higher accuracy. We also show that the technique adapts to\nlarger problems using a limited dataset while maintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:41:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Hilal", "Amr E", ""], ["Arai", "Ismail", ""], ["El-Tawab", "Samy", ""]]}, {"id": "2101.10837", "submitter": "Venkata Satya Sai Ajay Daliparthi", "authors": "Venkata Satya Sai Ajay Daliparthi", "title": "Ikshana: A Theory of Human Scene Understanding Mechanism", "comments": "24 pages, 19 figures, Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, deep neural networks (DNNs) achieved state-of-the-art\nperformance on many computer vision tasks. However, the one typical drawback of\nthese DNNs is the requirement of massive labeled data. Even though few-shot\nlearning methods addressed this problem through metric-learning and\nmeta-learning techniques, in this work, we address this problem from a\nneuroscience perspective. We propose a theory named Ikshana, to explain the\nfunctioning of the human brain, while humans understand an image. By following\nthe Ikshana theory, we propose a novel neural-inspired CNN architecture named\nIkshanaNet for semantic segmentation. The empirical results demonstrate the\neffectiveness of our method on few data samples, outperforming several\nbaselines, on the Cityscapes and the CamVid benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:30:56 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 05:01:26 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Daliparthi", "Venkata Satya Sai Ajay", ""]]}, {"id": "2101.10840", "submitter": "Ahmed Hamdy Hassan Mohamed Ahmed", "authors": "Ahmed Hamdy, Ahmed Elsherif, Saiid Shebl", "title": "Three-Dimensional Investigation of the Metric Properties of Parabolic\n  Double Projection Involving Catadioptric Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an analytical study for the metric properties of the\nparaboloidal double projection, i.e. central and orthogonal projections used in\nthe catadioptric camera system. Metric properties have not sufficiently studied\nin previous treatments of such system. These properties incorporate the\ndetermination of the true lengths of projected lines and areas bounded by\nprojected lines. The advantageous main gain of determining metric elements of\nthe paraboloidal double projection is studying distortion analysis and camera\ncalibration, which is considered an essential tool in testing camera accuracy.\nAlso, this may be considered as a significant utility in studying comparison\nanalysis between different cameras projection systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:20:14 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Hamdy", "Ahmed", ""], ["Elsherif", "Ahmed", ""], ["Shebl", "Saiid", ""]]}, {"id": "2101.10841", "submitter": "Yong-Goo Shin", "authors": "Seung Park, Yoon-Jae Yeo, and Yong-Goo Shin", "title": "Generative Adversarial Network using Perturbed-Convolutions", "comments": "Submitted to IEEE transactions on Neural networks and learning\n  systems. arXiv admin note: text overlap with arXiv:1911.10979", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite growing insights into the GAN training, it still suffers from\ninstability during the training procedure. To alleviate this problem, this\npaper presents a novel convolutional layer, called perturbed-convolution\n(PConv), which focuses on achieving two goals simultaneously: penalize the\ndiscriminator for training GAN stably and prevent the overfitting problem in\nthe discriminator. PConv generates perturbed features by randomly disturbing an\ninput tensor before performing the convolution operation. This approach is\nsimple but surprisingly effective. First, to reliably classify real and\ngenerated samples using the disturbed input tensor, the intermediate layers in\nthe discriminator should learn features having a small local Lipschitz value.\nSecond, due to the perturbed features in PConv, the discriminator is difficult\nto memorize the real images; this makes the discriminator avoid the overfitting\nproblem. To show the generalization ability of the proposed method, we\nconducted extensive experiments with various loss functions and datasets\nincluding CIFAR-10, CelebA-HQ, LSUN, and tiny-ImageNet. Quantitative\nevaluations demonstrate that WCL significantly improves the performance of GAN\nand conditional GAN in terms of Frechet inception distance (FID). For instance,\nthe proposed method improves FID scores on the tiny-ImageNet dataset from 58.59\nto 50.42.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 22:05:13 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 11:32:20 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Park", "Seung", ""], ["Yeo", "Yoon-Jae", ""], ["Shin", "Yong-Goo", ""]]}, {"id": "2101.10842", "submitter": "Masato Ishii", "authors": "Masato Ishii and Masashi Sugiyama", "title": "Source-free Domain Adaptation via Distributional Alignment by Matching\n  Batch Normalization Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel domain adaptation method for the\nsource-free setting. In this setting, we cannot access source data during\nadaptation, while unlabeled target data and a model pretrained with source data\nare given. Due to lack of source data, we cannot directly match the data\ndistributions between domains unlike typical domain adaptation algorithms. To\ncope with this problem, we propose utilizing batch normalization statistics\nstored in the pretrained model to approximate the distribution of unobserved\nsource data. Specifically, we fix the classifier part of the model during\nadaptation and only fine-tune the remaining feature encoder part so that batch\nnormalization statistics of the features extracted by the encoder match those\nstored in the fixed classifier. Additionally, we also maximize the mutual\ninformation between the features and the classifier's outputs to further boost\nthe classification performance. Experimental results with several benchmark\ndatasets show that our method achieves competitive performance with\nstate-of-the-art domain adaptation methods even though it does not require\naccess to source data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:22:33 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ishii", "Masato", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2101.10844", "submitter": "Mingkui Tan", "authors": "Zhuoman Liu, Wei Jia, Ming Yang, Peiyao Luo, Yong Guo, and Mingkui Tan", "title": "Deep View Synthesis via Self-Consistent Generative Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  View synthesis aims to produce unseen views from a set of views captured by\ntwo or more cameras at different positions. This task is non-trivial since it\nis hard to conduct pixel-level matching among different views. To address this\nissue, most existing methods seek to exploit the geometric information to match\npixels. However, when the distinct cameras have a large baseline (i.e., far\naway from each other), severe geometry distortion issues would occur and the\ngeometric information may fail to provide useful guidance, resulting in very\nblurry synthesized images. To address the above issues, in this paper, we\npropose a novel deep generative model, called Self-Consistent Generative\nNetwork (SCGN), which synthesizes novel views from the given input views\nwithout explicitly exploiting the geometric information. The proposed SCGN\nmodel consists of two main components, i.e., a View Synthesis Network (VSN) and\na View Decomposition Network (VDN), both employing an Encoder-Decoder\nstructure. Here, the VDN seeks to reconstruct input views from the synthesized\nnovel view to preserve the consistency of view synthesis. Thanks to VDN, SCGN\nis able to synthesize novel views without using any geometric rectification\nbefore encoding, making it easier for both training and applications. Finally,\nadversarial loss is introduced to improve the photo-realism of novel views.\nBoth qualitative and quantitative comparisons against several state-of-the-art\nmethods on two benchmark tasks demonstrated the superiority of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 10:56:00 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Liu", "Zhuoman", ""], ["Jia", "Wei", ""], ["Yang", "Ming", ""], ["Luo", "Peiyao", ""], ["Guo", "Yong", ""], ["Tan", "Mingkui", ""]]}, {"id": "2101.10845", "submitter": "Angelo Menezes", "authors": "Angelo G. Menezes", "title": "Analysis and evaluation of Deep Learning based Super-Resolution\n  algorithms to improve performance in Low-Resolution Face Recognition", "comments": "MSc Thesis under supervision of Carlos A. E. Montesco presented at\n  the Federal University of Sergipe, Brazil (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Surveillance scenarios are prone to several problems since they usually\ninvolve low-resolution footage, and there is no control of how far the subjects\nmay be from the camera in the first place. This situation is suitable for the\napplication of upsampling (super-resolution) algorithms since they may be able\nto recover the discriminant properties of the subjects involved. While general\nsuper-resolution approaches were proposed to enhance image quality for\nhuman-level perception, biometrics super-resolution methods seek the best\n\"computer perception\" version of the image since their focus is on improving\nautomatic recognition performance. Convolutional neural networks and deep\nlearning algorithms, in general, have been applied to computer vision tasks and\nare now state-of-the-art for several sub-domains, including image\nclassification, restoration, and super-resolution. However, no work has\nevaluated the effects that the latest proposed super-resolution methods may\nhave upon the accuracy and face verification performance in low-resolution\n\"in-the-wild\" data. This project aimed at evaluating and adapting different\ndeep neural network architectures for the task of face super-resolution driven\nby face recognition performance in real-world low-resolution images. The\nexperimental results in a real-world surveillance and attendance datasets\nshowed that general super-resolution architectures might enhance face\nverification performance of deep neural networks trained on high-resolution\nfaces. Also, since neural networks are function approximators and can be\ntrained based on specific objective functions, the use of a customized loss\nfunction optimized for feature extraction showed promising results for\nrecovering discriminant features in low-resolution face images.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:41:57 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Menezes", "Angelo G.", ""]]}, {"id": "2101.10857", "submitter": "Vinayak Elangovan", "authors": "Vinayak Elangovan", "title": "Indoor Group Activity Recognition using Multi-Layered HMMs", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": "Proceedings of Academics World International Conference,\n  Philadelphia, USA, 28th - 29th December, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Discovery and recognition of Group Activities (GA) based on imagery data\nprocessing have significant applications in persistent surveillance systems,\nwhich play an important role in some Internet services. The process is involved\nwith analysis of sequential imagery data with spatiotemporal associations.\nDiscretion of video imagery requires a proper inference system capable of\ndiscriminating and differentiating cohesive observations and interlinking them\nto known ontologies. We propose an Ontology based GAR with a proper inference\nmodel that is capable of identifying and classifying a sequence of events in\ngroup activities. A multi-layered Hidden Markov Model (HMM) is proposed to\nrecognize different levels of abstract GA. The multi-layered HMM consists of N\nlayers of HMMs where each layer comprises of M number of HMMs running in\nparallel. The number of layers depends on the order of information to be\nextracted. At each layer, by matching and correlating attributes of detected\ngroup events, the model attempts to associate sensory observations to known\nontology perceptions. This paper demonstrates and compares performance of three\ndifferent implementation of HMM, namely, concatenated N-HMM, cascaded C-HMM and\nhybrid H-HMM for building effective multi-layered HMM.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 22:02:12 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Elangovan", "Vinayak", ""]]}, {"id": "2101.10861", "submitter": "Lucas Prado Osco", "authors": "Lucas Prado Osco, Jos\\'e Marcato Junior, Ana Paula Marques Ramos,\n  L\\'ucio Andr\\'e de Castro Jorge, Sarah Narges Fatholahi, Jonathan de Andrade\n  Silva, Edson Takashi Matsubara, Hemerson Pistori, Wesley Nunes Gon\\c{c}alves,\n  Jonathan Li", "title": "A Review on Deep Learning in UAV Remote Sensing", "comments": "38 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep Neural Networks (DNNs) learn representation from data with an impressive\ncapability, and brought important breakthroughs for processing images,\ntime-series, natural language, audio, video, and many others. In the remote\nsensing field, surveys and literature revisions specifically involving DNNs\nalgorithms' applications have been conducted in an attempt to summarize the\namount of information produced in its subfields. Recently, Unmanned Aerial\nVehicles (UAV) based applications have dominated aerial sensing research.\nHowever, a literature revision that combines both \"deep learning\" and \"UAV\nremote sensing\" thematics has not yet been conducted. The motivation for our\nwork was to present a comprehensive review of the fundamentals of Deep Learning\n(DL) applied in UAV-based imagery. We focused mainly on describing\nclassification and regression techniques used in recent applications with\nUAV-acquired data. For that, a total of 232 papers published in international\nscientific journal databases was examined. We gathered the published material\nand evaluated their characteristics regarding application, sensor, and\ntechnique used. We relate how DL presents promising results and has the\npotential for processing tasks associated with UAV-based image data. Lastly, we\nproject future perspectives, commentating on prominent DL paths to be explored\nin the UAV remote sensing field. Our revision consists of a friendly-approach\nto introduce, commentate, and summarize the state-of-the-art in UAV-based image\napplications with DNNs algorithms in diverse subfields of remote sensing,\ngrouping it in the environmental, urban, and agricultural contexts.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:08:38 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 14:09:43 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Osco", "Lucas Prado", ""], ["Junior", "Jos\u00e9 Marcato", ""], ["Ramos", "Ana Paula Marques", ""], ["Jorge", "L\u00facio Andr\u00e9 de Castro", ""], ["Fatholahi", "Sarah Narges", ""], ["Silva", "Jonathan de Andrade", ""], ["Matsubara", "Edson Takashi", ""], ["Pistori", "Hemerson", ""], ["Gon\u00e7alves", "Wesley Nunes", ""], ["Li", "Jonathan", ""]]}, {"id": "2101.10862", "submitter": "Christian M. Dahl", "authors": "Christian M. Dahl, Torben Johansen, Emil N. S{\\o}rensen, Simon\n  Wittrock", "title": "HANA: A HAndwritten NAme Database for Offline Handwritten Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for linking individuals across historical data sets, typically in\ncombination with AI based transcription models, are developing rapidly.\nProbably the single most important identifier for linking is personal names.\nHowever, personal names are prone to enumeration and transcription errors and\nalthough modern linking methods are designed to handle such challenges these\nsources of errors are critical and should be minimized. For this purpose,\nimproved transcription methods and large-scale databases are crucial\ncomponents. This paper describes and provides documentation for HANA, a newly\nconstructed large-scale database which consists of more than 1.1 million images\nof handwritten word-groups. The database is a collection of personal names,\ncontaining more than 105 thousand unique names with a total of more than 3.3\nmillion examples. In addition, we present benchmark results for deep learning\nmodels that automatically can transcribe the personal names from the scanned\ndocuments. Focusing mainly on personal names, due to its vital role in linking,\nwe hope to foster more sophisticated, accurate, and robust models for\nhandwritten text recognition through making more challenging large-scale\ndatabases publicly available. This paper describes the data source, the\ncollection process, and the image-processing procedures and methods that are\ninvolved in extracting the handwritten personal names and handwritten text in\ngeneral from the forms.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:23:01 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Dahl", "Christian M.", ""], ["Johansen", "Torben", ""], ["S\u00f8rensen", "Emil N.", ""], ["Wittrock", "Simon", ""]]}, {"id": "2101.10876", "submitter": "Rasika Karkare", "authors": "Rasika Karkare, Randy Paffenroth and Gunjan Mahindre", "title": "Blind Image Denoising and Inpainting Using Robust Hadamard Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how deep autoencoders can be generalized to the\ncase of inpainting and denoising, even when no clean training data is\navailable. In particular, we show how neural networks can be trained to perform\nall of these tasks simultaneously. While, deep autoencoders implemented by way\nof neural networks have demonstrated potential for denoising and anomaly\ndetection, standard autoencoders have the drawback that they require access to\nclean data for training. However, recent work in Robust Deep Autoencoders\n(RDAEs) shows how autoencoders can be trained to eliminate outliers and noise\nin a dataset without access to any clean training data. Inspired by this work,\nwe extend RDAEs to the case where data are not only noisy and have outliers,\nbut also only partially observed. Moreover, the dataset we train the neural\nnetwork on has the properties that all entries have noise, some entries are\ncorrupted by large mistakes, and many entries are not even known. Given such an\nalgorithm, many standard tasks, such as denoising, image inpainting, and\nunobserved entry imputation can all be accomplished simultaneously within the\nsame framework. Herein we demonstrate these techniques on standard machine\nlearning tasks, such as image inpainting and denoising for the MNIST and\nCIFAR10 datasets. However, these approaches are not only applicable to image\nprocessing problems, but also have wide ranging impacts on datasets arising\nfrom real-world problems, such as manufacturing and network processing, where\nnoisy, partially observed data naturally arise.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:33:22 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Karkare", "Rasika", ""], ["Paffenroth", "Randy", ""], ["Mahindre", "Gunjan", ""]]}, {"id": "2101.10892", "submitter": "Pedro Vicente", "authors": "Gon\\c{c}alo Cunha, Pedro Vicente, Alexandre Bernardino, Ricardo\n  Ribeiro, Pl\\'inio Moreno", "title": "Online Body Schema Adaptation through Cost-Sensitive Active Learning", "comments": "6 pages, 7 figures. Submitted to Humanoids 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanoid robots have complex bodies and kinematic chains with several\nDegrees-of-Freedom (DoF) which are difficult to model. Learning the parameters\nof a kinematic model can be achieved by observing the position of the robot\nlinks during prospective motions and minimising the prediction errors. This\nwork proposes a movement efficient approach for estimating online the\nbody-schema of a humanoid robot arm in the form of Denavit-Hartenberg (DH)\nparameters. A cost-sensitive active learning approach based on the A-Optimality\ncriterion is used to select optimal joint configurations. The chosen joint\nconfigurations simultaneously minimise the error in the estimation of the body\nschema and minimise the movement between samples. This reduces energy\nconsumption, along with mechanical fatigue and wear, while not compromising the\nlearning accuracy. The work was implemented in a simulation environment, using\nthe 7DoF arm of the iCub robot simulator. The hand pose is measured with a\nsingle camera via markers placed in the palm and back of the robot's hand. A\nnon-parametric occlusion model is proposed to avoid choosing joint\nconfigurations where the markers are not visible, thus preventing worthless\nattempts. The results show cost-sensitive active learning has similar accuracy\nto the standard active learning approach, while reducing in about half the\nexecuted movement.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:01:02 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cunha", "Gon\u00e7alo", ""], ["Vicente", "Pedro", ""], ["Bernardino", "Alexandre", ""], ["Ribeiro", "Ricardo", ""], ["Moreno", "Pl\u00ednio", ""]]}, {"id": "2101.10894", "submitter": "Thamar Solorio", "authors": "Thamar Solorio, Mahsa Shafaei, Christos Smailis, Mona Diab, Theodore\n  Giannakopoulos, Heng Ji, Yang Liu, Rada Mihalcea, Smaranda Muresan, Ioannis\n  Kakadiaris", "title": "White Paper: Challenges and Considerations for the Creation of a Large\n  Labelled Repository of Online Videos with Questionable Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This white paper presents a summary of the discussions regarding critical\nconsiderations to develop an extensive repository of online videos annotated\nwith labels indicating questionable content. The main discussion points\ninclude: 1) the type of appropriate labels that will result in a valuable\nrepository for the larger AI community; 2) how to design the collection and\nannotation process, as well as the distribution of the corpus to maximize its\npotential impact; and, 3) what actions we can take to reduce risk of trauma to\nannotators.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:19:24 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Solorio", "Thamar", ""], ["Shafaei", "Mahsa", ""], ["Smailis", "Christos", ""], ["Diab", "Mona", ""], ["Giannakopoulos", "Theodore", ""], ["Ji", "Heng", ""], ["Liu", "Yang", ""], ["Mihalcea", "Rada", ""], ["Muresan", "Smaranda", ""], ["Kakadiaris", "Ioannis", ""]]}, {"id": "2101.10897", "submitter": "Yunxiang Zhao", "authors": "Yunxiang Zhao, Qiuhong Ke, Flip Korn, Jianzhong Qi, Rui Zhang", "title": "HexCNN: A Framework for Native Hexagonal Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hexagonal CNN models have shown superior performance in applications such as\nIACT data analysis and aerial scene classification due to their better rotation\nsymmetry and reduced anisotropy. In order to realize hexagonal processing,\nexisting studies mainly use the ZeroOut method to imitate hexagonal processing,\nwhich causes substantial memory and computation overheads. We address this\ndeficiency with a novel native hexagonal CNN framework named HexCNN. HexCNN\ntakes hexagon-shaped input and performs forward and backward propagation on the\noriginal form of the input based on hexagon-shaped filters, hence avoiding\ncomputation and memory overheads caused by imitation. For applications with\nrectangle-shaped input but require hexagonal processing, HexCNN can be applied\nby padding the input into hexagon-shape as preprocessing. In this case, we show\nthat the time and space efficiency of HexCNN still outperforms existing\nhexagonal CNN methods substantially. Experimental results show that compared\nwith the state-of-the-art models, which imitate hexagonal processing but using\nrectangle-shaped filters, HexCNN reduces the training time by up to 42.2%.\nMeanwhile, HexCNN saves the memory space cost by up to 25% and 41.7% for\nloading the input and performing convolution, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 08:23:39 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zhao", "Yunxiang", ""], ["Ke", "Qiuhong", ""], ["Korn", "Flip", ""], ["Qi", "Jianzhong", ""], ["Zhang", "Rui", ""]]}, {"id": "2101.10913", "submitter": "Min Yan", "authors": "Min Yan, Guoshan Zhang, Tong Zhang, Yueming Zhang", "title": "Nondiscriminatory Treatment: a straightforward framework for multi-human\n  parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-human parsing aims to segment every body part of every human instance.\nNearly all state-of-the-art methods follow the \"detection first\" or\n\"segmentation first\" pipelines. Different from them, we present an end-to-end\nand box-free pipeline from a new and more human-intuitive perspective. In\ntraining time, we directly do instance segmentation on humans and parts. More\nspecifically, we introduce a notion of \"indiscriminate objects with categorie\"\nwhich treats humans and parts without distinction and regards them both as\ninstances with categories. In the mask prediction, each binary mask is obtained\nby a combination of prototypes shared among all human and part categories. In\ninference time, we design a brand-new grouping post-processing method that\nrelates each part instance with one single human instance and groups them\ntogether to obtain the final human-level parsing result. We name our method as\nNondiscriminatory Treatment between Humans and Parts for Human Parsing (NTHP).\nExperiments show that our network performs superiorly against state-of-the-art\nmethods by a large margin on the MHP v2.0 and PASCAL-Person-Part datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:31:21 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yan", "Min", ""], ["Zhang", "Guoshan", ""], ["Zhang", "Tong", ""], ["Zhang", "Yueming", ""]]}, {"id": "2101.10914", "submitter": "Tristan Matthias Gottschalk", "authors": "Tristan M. Gottschalk, Andreas Maier, Florian Kordon, Bj\\\"orn W.\n  Kreher", "title": "Learning-Based Patch-Wise Metal Segmentation with Consistency Check", "comments": "Accepted for Bildverarbeitung f\\\"ur die Medizin, 07.-09.03.2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metal implants that are inserted into the patient's body during trauma\ninterventions cause heavy artifacts in 3D X-ray acquisitions. Metal Artifact\nReduction (MAR) methods, whose first step is always a segmentation of the\npresent metal objects, try to remove these artifacts. Thereby, the segmentation\nis a crucial task which has strong influence on the MAR's outcome. This study\nproposes and evaluates a learning-based patch-wise segmentation network and a\nnewly proposed Consistency Check as post-processing step. The combination of\nthe learned segmentation and Consistency Check reaches a high segmentation\nperformance with an average IoU score of 0.924 on the test set. Furthermore,\nthe Consistency Check proves the ability to significantly reduce false positive\nsegmentations whilst simultaneously ensuring consistent segmentations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:35:28 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gottschalk", "Tristan M.", ""], ["Maier", "Andreas", ""], ["Kordon", "Florian", ""], ["Kreher", "Bj\u00f6rn W.", ""]]}, {"id": "2101.10955", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu, Xiangxu Yu, Yilin Wang, Neil Birkbeck, Balu Adsumilli,\n  and Alan C. Bovik", "title": "RAPIQUE: Rapid and Accurate Video Quality Prediction of User Generated\n  Content", "comments": "13 pages, 13 figurs, 5 tables. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Blind or no-reference video quality assessment of user-generated content\n(UGC) has become a trending, challenging, unsolved problem. Accurate and\nefficient video quality predictors suitable for this content are thus in great\ndemand to achieve more intelligent analysis and processing of UGC videos.\nPrevious studies have shown that natural scene statistics and deep learning\nfeatures are both sufficient to capture spatial distortions, which contribute\nto a significant aspect of UGC video quality issues. However, these models are\neither incapable or inefficient for predicting the quality of complex and\ndiverse UGC videos in practical applications. Here we introduce an effective\nand efficient video quality model for UGC content, which we dub the Rapid and\nAccurate Video Quality Evaluator (RAPIQUE), which we show performs comparably\nto state-of-the-art (SOTA) models but with orders-of-magnitude faster runtime.\nRAPIQUE combines and leverages the advantages of both quality-aware scene\nstatistics features and semantics-aware deep convolutional features, allowing\nus to design the first general and efficient spatial and temporal (space-time)\nbandpass statistics model for video quality modeling. Our experimental results\non recent large-scale UGC video quality databases show that RAPIQUE delivers\ntop performances on all the datasets at a considerably lower computational\nexpense. We hope this work promotes and inspires further efforts towards\npractical modeling of video quality problems for potential real-time and\nlow-latency applications. To promote public usage, an implementation of RAPIQUE\nhas been made freely available online: \\url{https://github.com/vztu/RAPIQUE}.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:23:46 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Tu", "Zhengzhong", ""], ["Yu", "Xiangxu", ""], ["Wang", "Yilin", ""], ["Birkbeck", "Neil", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2101.10977", "submitter": "Lukas Brunke", "authors": "Lukas Brunke, Prateek Agrawal, Nikhil George", "title": "Evaluating Input Perturbation Methods for Interpreting CNNs and Saliency\n  Map Comparison", "comments": null, "journal-ref": "ECCV 2020: Computer Vision - ECCV 2020 Workshops pp 120-134", "doi": "10.1007/978-3-030-66415-2_8", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Input perturbation methods occlude parts of an input to a function and\nmeasure the change in the function's output. Recently, input perturbation\nmethods have been applied to generate and evaluate saliency maps from\nconvolutional neural networks. In practice, neutral baseline images are used\nfor the occlusion, such that the baseline image's impact on the classification\nprobability is minimal. However, in this paper we show that arguably neutral\nbaseline images still impact the generated saliency maps and their evaluation\nwith input perturbations. We also demonstrate that many choices of\nhyperparameters lead to the divergence of saliency maps generated by input\nperturbations. We experimentally reveal inconsistencies among a selection of\ninput perturbation methods and find that they lack robustness for generating\nsaliency maps and for evaluating saliency maps as saliency metrics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 18:11:06 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Brunke", "Lukas", ""], ["Agrawal", "Prateek", ""], ["George", "Nikhil", ""]]}, {"id": "2101.10979", "submitter": "Pan Zhang", "authors": "Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, Fang Wen", "title": "Prototypical Pseudo Label Denoising and Target Structure Learning for\n  Domain Adaptive Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-training is a competitive approach in domain adaptive segmentation,\nwhich trains the network with the pseudo labels on the target domain. However\ninevitably, the pseudo labels are noisy and the target features are dispersed\ndue to the discrepancy between source and target domains. In this paper, we\nrely on representative prototypes, the feature centroids of classes, to address\nthe two issues for unsupervised domain adaptation. In particular, we take one\nstep further and exploit the feature distances from prototypes that provide\nricher information than mere prototypes. Specifically, we use it to estimate\nthe likelihood of pseudo labels to facilitate online correction in the course\nof training. Meanwhile, we align the prototypical assignments based on relative\nfeature distances for two different views of the same target, producing a more\ncompact target feature space. Moreover, we find that distilling the already\nlearned knowledge to a self-supervised pretrained model further boosts the\nperformance. Our method shows tremendous performance advantage over\nstate-of-the-art methods. We will make the code publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 18:12:54 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 15:10:37 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhang", "Pan", ""], ["Zhang", "Bo", ""], ["Zhang", "Ting", ""], ["Chen", "Dong", ""], ["Wang", "Yong", ""], ["Wen", "Fang", ""]]}, {"id": "2101.10994", "submitter": "Towaki Takikawa", "authors": "Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles\n  Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler", "title": "Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D\n  Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural signed distance functions (SDFs) are emerging as an effective\nrepresentation for 3D shapes. State-of-the-art methods typically encode the SDF\nwith a large, fixed-size neural network to approximate complex shapes with\nimplicit surfaces. Rendering with these large networks is, however,\ncomputationally expensive since it requires many forward passes through the\nnetwork for every pixel, making these representations impractical for real-time\ngraphics. We introduce an efficient neural representation that, for the first\ntime, enables real-time rendering of high-fidelity neural SDFs, while achieving\nstate-of-the-art geometry reconstruction quality. We represent implicit\nsurfaces using an octree-based feature volume which adaptively fits shapes with\nmultiple discrete levels of detail (LODs), and enables continuous LOD with SDF\ninterpolation. We further develop an efficient algorithm to directly render our\nnovel neural SDF representation in real-time by querying only the necessary\nLODs with sparse octree traversal. We show that our representation is 2-3\norders of magnitude more efficient in terms of rendering speed compared to\nprevious works. Furthermore, it produces state-of-the-art reconstruction\nquality for complex shapes under both 3D geometric and 2D image-space metrics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 18:50:22 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Takikawa", "Towaki", ""], ["Litalien", "Joey", ""], ["Yin", "Kangxue", ""], ["Kreis", "Karsten", ""], ["Loop", "Charles", ""], ["Nowrouzezahrai", "Derek", ""], ["Jacobson", "Alec", ""], ["McGuire", "Morgan", ""], ["Fidler", "Sanja", ""]]}, {"id": "2101.10997", "submitter": "Goutam Bhat", "authors": "Goutam Bhat and Martin Danelljan and Luc Van Gool and Radu Timofte", "title": "Deep Burst Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While single-image super-resolution (SISR) has attracted substantial interest\nin recent years, the proposed approaches are limited to learning image priors\nin order to add high frequency details. In contrast, multi-frame\nsuper-resolution (MFSR) offers the possibility of reconstructing rich details\nby combining signal information from multiple shifted images. This key\nadvantage, along with the increasing popularity of burst photography, have made\nMFSR an important problem for real-world applications.\n  We propose a novel architecture for the burst super-resolution task. Our\nnetwork takes multiple noisy RAW images as input, and generates a denoised,\nsuper-resolved RGB image as output. This is achieved by explicitly aligning\ndeep embeddings of the input frames using pixel-wise optical flow. The\ninformation from all frames are then adaptively merged using an attention-based\nfusion module. In order to enable training and evaluation on real-world data,\nwe additionally introduce the BurstSR dataset, consisting of smartphone bursts\nand high-resolution DSLR ground-truth. We perform comprehensive experimental\nanalysis, demonstrating the effectiveness of the proposed architecture.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 18:57:21 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 16:38:41 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Bhat", "Goutam", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2101.11002", "submitter": "Evan Debenham", "authors": "Evan R.M. Debenham and Roberto Solis-Oba (The University of Western\n  Ontario, Canada)", "title": "New Algorithms for Computing Field of Vision over 2D Grids", "comments": "Presented at the 6th International Conference on Computer Science,\n  Engineering And Applications (CSEA 2020) 18 pages, 11 figures, 4 tables", "journal-ref": "6th International Conference on Computer Science, Engineering And\n  Applications (CSEA 2020), Volume 10, Number 18, December 2020, pg. 1-18", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The aim of this paper is to propose new algorithms for Field of Vision (FOV)\ncomputation which improve on existing work at high resolutions. FOV refers to\nthe set of locations that are visible from a specific position in a scene of a\ncomputer game.\n  We summarize existing algorithms for FOV computation, describe their\nlimitations, and present new algorithms which aim to address these limitations.\nWe first present an algorithm which makes use of spatial data structures in a\nway which is new for FOV calculation. We then present a novel technique which\nupdates a previously calculated FOV, rather than re-calculating an FOV from\nscratch.\n  We compare our algorithms to existing FOV algorithms and show they provide\nsubstantial improvements to running time. Our algorithms provide the largest\nimprovement over existing FOV algorithms at large grid sizes, thus allowing the\npossibility of the design of high resolution FOV-based video games.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 20:38:35 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Debenham", "Evan R. M.", "", "The University of Western\n  Ontario, Canada"], ["Solis-Oba", "Roberto", "", "The University of Western\n  Ontario, Canada"]]}, {"id": "2101.11058", "submitter": "Orchid Majumder", "authors": "Orchid Majumder, Avinash Ravichandran, Subhransu Maji, Alessandro\n  Achille, Marzia Polito, Stefano Soatto", "title": "Supervised Momentum Contrastive Learning for Few-Shot Classification", "comments": "V2 version; updated with new experiments and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning aims to transfer information from one task to enable\ngeneralization on novel tasks given a few examples. This information is present\nboth in the domain and the class labels. In this work we investigate the\ncomplementary roles of these two sources of information by combining\ninstance-discriminative contrastive learning and supervised learning in a\nsingle framework called Supervised Momentum Contrastive learning (SUPMOCO). Our\napproach avoids a problem observed in supervised learning where information in\nimages not relevant to the task is discarded, which hampers their\ngeneralization to novel tasks. We show that (self-supervised) contrastive\nlearning and supervised learning are mutually beneficial, leading to a new\nstate-of-the-art on the META-DATASET - a recently introduced benchmark for\nfew-shot learning. Our method is based on a simple modification of MOCO and\nscales better than prior work on combining supervised and self-supervised\nlearning. This allows us to easily combine data from multiple domains leading\nto further improvements.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:58:08 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 19:34:56 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Majumder", "Orchid", ""], ["Ravichandran", "Avinash", ""], ["Maji", "Subhransu", ""], ["Achille", "Alessandro", ""], ["Polito", "Marzia", ""], ["Soatto", "Stefano", ""]]}, {"id": "2101.11060", "submitter": "Xinwei Zhao", "authors": "Xinwei Zhao and Matthew C. Stamm", "title": "Defenses Against Multi-Sticker Physical Domain Attacks on Classifiers", "comments": null, "journal-ref": "This paper is published on European Conference on Computer Vision\n  2020, page 202-219, Springer", "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, physical domain adversarial attacks have drawn significant\nattention from the machine learning community. One important attack proposed by\nEykholt et al. can fool a classifier by placing black and white stickers on an\nobject such as a road sign. While this attack may pose a significant threat to\nvisual classifiers, there are currently no defenses designed to protect against\nthis attack. In this paper, we propose new defenses that can protect against\nmulti-sticker attacks. We present defensive strategies capable of operating\nwhen the defender has full, partial, and no prior information about the attack.\nBy conducting extensive experiments, we show that our proposed defenses can\noutperform existing defenses against physical attacks when presented with a\nmulti-sticker attack.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:59:28 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zhao", "Xinwei", ""], ["Stamm", "Matthew C.", ""]]}, {"id": "2101.11080", "submitter": "Peng Zhou", "authors": "Peng Zhou, Ning Yu, Zuxuan Wu, Larry S. Davis, Abhinav Shrivastava and\n  Ser-Nam Lim", "title": "Deep Video Inpainting Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies video inpainting detection, which localizes an inpainted\nregion in a video both spatially and temporally. In particular, we introduce\nVIDNet, Video Inpainting Detection Network, which contains a two-stream\nencoder-decoder architecture with attention module. To reveal artifacts encoded\nin compression, VIDNet additionally takes in Error Level Analysis frames to\naugment RGB frames, producing multimodal features at different levels with an\nencoder. Exploring spatial and temporal relationships, these features are\nfurther decoded by a Convolutional LSTM to predict masks of inpainted regions.\nIn addition, when detecting whether a pixel is inpainted or not, we present a\nquad-directional local attention module that borrows information from its\nsurrounding pixels from four directions. Extensive experiments are conducted to\nvalidate our approach. We demonstrate, among other things, that VIDNet not only\noutperforms by clear margins alternative inpainting detection methods but also\ngeneralizes well on novel videos that are unseen during training.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 20:53:49 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zhou", "Peng", ""], ["Yu", "Ning", ""], ["Wu", "Zuxuan", ""], ["Davis", "Larry S.", ""], ["Shrivastava", "Abhinav", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2101.11081", "submitter": "Xinwei Zhao", "authors": "Xinwei Zhao and Matthew C. Stamm", "title": "The Effect of Class Definitions on the Transferability of Adversarial\n  Attacks Against Forensic CNNs", "comments": null, "journal-ref": "Published at Electronic Imaging, Media Watermarking, Security, and\n  Forensics 2020, pp. 119-1-119-7(7)", "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, convolutional neural networks (CNNs) have been widely used\nby researchers to perform forensic tasks such as image tampering detection. At\nthe same time, adversarial attacks have been developed that are capable of\nfooling CNN-based classifiers. Understanding the transferability of adversarial\nattacks, i.e. an attacks ability to attack a different CNN than the one it was\ntrained against, has important implications for designing CNNs that are\nresistant to attacks. While attacks on object recognition CNNs are believed to\nbe transferrable, recent work by Barni et al. has shown that attacks on\nforensic CNNs have difficulty transferring to other CNN architectures or CNNs\ntrained using different datasets. In this paper, we demonstrate that\nadversarial attacks on forensic CNNs are even less transferrable than\npreviously thought even between virtually identical CNN architectures! We show\nthat several common adversarial attacks against CNNs trained to identify image\nmanipulation fail to transfer to CNNs whose only difference is in the class\ndefinitions (i.e. the same CNN architectures trained using the same data). We\nnote that all formulations of class definitions contain the unaltered class.\nThis has important implications for the future design of forensic CNNs that are\nrobust to adversarial and anti-forensic attacks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 20:59:37 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zhao", "Xinwei", ""], ["Stamm", "Matthew C.", ""]]}, {"id": "2101.11085", "submitter": "Hassan Muhammad", "authors": "Hassan Muhammad, Chensu Xie, Carlie S. Sigel, Michael Doukas, Lindsay\n  Alpert, William R. Jarnagin, Amber Simpson, and Thomas J. Fuchs", "title": "EPIC-Survival: End-to-end Part Inferred Clustering for Survival\n  Analysis, Featuring Prognostic Stratification Boosting", "comments": "co-first authors: Hassan Muhammad and Chensu Xie", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Histopathology-based survival modelling has two major hurdles. Firstly, a\nwell-performing survival model has minimal clinical application if it does not\ncontribute to the stratification of a cancer patient cohort into different risk\ngroups, preferably driven by histologic morphologies. In the clinical setting,\nindividuals are not given specific prognostic predictions, but are rather\npredicted to lie within a risk group which has a general survival trend. Thus,\nIt is imperative that a survival model produces well-stratified risk groups.\nSecondly, until now, survival modelling was done in a two-stage approach\n(encoding and aggregation). The massive amount of pixels in digitized whole\nslide images were never utilized to their fullest extent due to technological\nconstraints on data processing, forcing decoupled learning. EPIC-Survival\nbridges encoding and aggregation into an end-to-end survival modelling\napproach, while introducing stratification boosting to encourage the model to\nnot only optimize ranking, but also to discriminate between risk groups. In\nthis study we show that EPIC-Survival performs better than other approaches in\nmodelling intrahepatic cholangiocarcinoma, a historically difficult cancer to\nmodel. Further, we show that stratification boosting improves further improves\nmodel performance, resulting in a concordance-index of 0.880 on a held-out test\nset. Finally, we were able to identify specific histologic differences, not\ncommonly sought out in ICC, between low and high risk groups.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 21:11:45 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 21:30:38 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 15:27:26 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Muhammad", "Hassan", ""], ["Xie", "Chensu", ""], ["Sigel", "Carlie S.", ""], ["Doukas", "Michael", ""], ["Alpert", "Lindsay", ""], ["Jarnagin", "William R.", ""], ["Simpson", "Amber", ""], ["Fuchs", "Thomas J.", ""]]}, {"id": "2101.11111", "submitter": "Letian Yu", "authors": "Xin Yang, Zongliang Ma, Letian Yu, Ying Cao, Baocai Yin, Xiaopeng Wei,\n  Qiang Zhang, Rynson W.H. Lau", "title": "Automatic Comic Generation with Stylistic Multi-page Layouts and\n  Emotion-driven Text Balloon Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fully automatic system for generating comic books\nfrom videos without any human intervention. Given an input video along with its\nsubtitles, our approach first extracts informative keyframes by analyzing the\nsubtitles, and stylizes keyframes into comic-style images. Then, we propose a\nnovel automatic multi-page layout framework, which can allocate the images\nacross multiple pages and synthesize visually interesting layouts based on the\nrich semantics of the images (e.g., importance and inter-image relation).\nFinally, as opposed to using the same type of balloon as in previous works, we\npropose an emotion-aware balloon generation method to create different types of\nword balloons by analyzing the emotion of subtitles and audios. Our method is\nable to vary balloon shapes and word sizes in balloons in response to different\nemotions, leading to more enriched reading experience. Once the balloons are\ngenerated, they are placed adjacent to their corresponding speakers via speaker\ndetection. Our results show that our method, without requiring any user inputs,\ncan generate high-quality comic pages with visually rich layouts and balloons.\nOur user studies also demonstrate that users prefer our generated results over\nthose by state-of-the-art comic generation systems.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:15:15 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yang", "Xin", ""], ["Ma", "Zongliang", ""], ["Yu", "Letian", ""], ["Cao", "Ying", ""], ["Yin", "Baocai", ""], ["Wei", "Xiaopeng", ""], ["Zhang", "Qiang", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "2101.11135", "submitter": "Prabhakara Subramanya Jois", "authors": "Prabhakara Subramanya Jois, Aniketh Manjunath and Thomas Fevens", "title": "Boosting Segmentation Performance across datasets using histogram\n  specification with application to pelvic bone segmentation", "comments": "5 pages, 4 figures, 3 tables; Submitted To IEEE International\n  Conference on Image Processing, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate segmentation of the pelvic CTs is crucial for the clinical diagnosis\nof pelvic bone diseases and for planning patient-specific hip surgeries. With\nthe emergence and advancements of deep learning for digital healthcare, several\nmethodologies have been proposed for such segmentation tasks. But in a low data\nscenario, the lack of abundant data needed to train a Deep Neural Network is a\nsignificant bottle-neck. In this work, we propose a methodology based on\nmodulation of image tonal distributions and deep learning to boost the\nperformance of networks trained on limited data. The strategy involves\npre-processing of test data through histogram specification. This simple yet\neffective approach can be viewed as a style transfer methodology. The\nsegmentation task uses a U-Net configuration with an EfficientNet-B0 backbone,\noptimized using an augmented BCE-IoU loss function. This configuration is\nvalidated on a total of 284 images taken from two publicly available CT\ndatasets, TCIA (a cancer imaging archive) and the Visible Human Project. The\naverage performance measures for the Dice coefficient and Intersection over\nUnion are 95.7% and 91.9%, respectively, give strong evidence for the\neffectiveness of the approach, which is highly competitive with\nstate-of-the-art methodologies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 23:48:40 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Jois", "Prabhakara Subramanya", ""], ["Manjunath", "Aniketh", ""], ["Fevens", "Thomas", ""]]}, {"id": "2101.11164", "submitter": "Adam Byerly", "authors": "Adam Byerly, Tatiana Kalganova, Anthony J. Grichnik", "title": "On the Importance of Capturing a Sufficient Diversity of Perspective for\n  the Classification of micro-PCBs", "comments": "12 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dataset consisting of high-resolution images of 13 micro-PCBs\ncaptured in various rotations and perspectives relative to the camera, with\neach sample labeled for PCB type, rotation category, and perspective\ncategories. We then present the design and results of experimentation on\ncombinations of rotations and perspectives used during training and the\nresulting impact on test accuracy. We then show when and how well data\naugmentation techniques are capable of simulating rotations vs. perspectives\nnot present in the training data. We perform all experiments using CNNs with\nand without homogeneous vector capsules (HVCs) and investigate and show the\ncapsules' ability to better encode the equivariance of the sub-components of\nthe micro-PCBs. The results of our experiments lead us to conclude that\ntraining a neural network equipped with HVCs, capable of modeling equivariance\namong sub-components, coupled with training on a diversity of perspectives,\nachieves the greatest classification accuracy on micro-PCB data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 02:06:09 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Byerly", "Adam", ""], ["Kalganova", "Tatiana", ""], ["Grichnik", "Anthony J.", ""]]}, {"id": "2101.11183", "submitter": "Haipeng Li", "authors": "Haipeng Li, Shuaicheng Liu, Jue Wang", "title": "DeepOIS: Gyroscope-Guided Deep Optical Image Stabilizer Compensation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile captured images can be aligned using their gyroscope sensors. Optical\nimage stabilizer (OIS) terminates this possibility by adjusting the images\nduring the capturing. In this work, we propose a deep network that compensates\nthe motions caused by the OIS, such that the gyroscopes can be used for image\nalignment on the OIS cameras. To achieve this, first, we record both videos and\ngyroscopes with an OIS camera as training data. Then, we convert gyroscope\nreadings into motion fields. Second, we propose a Fundamental Mixtures motion\nmodel for rolling shutter cameras, where an array of rotations within a frame\nare extracted as the ground-truth guidance. Third, we train a convolutional\nneural network with gyroscope motions as input to compensate for the OIS\nmotion. Once finished, the compensation network can be applied for other\nscenes, where the image alignment is purely based on gyroscopes with no need\nfor images contents, delivering strong robustness. Experiments show that our\nresults are comparable with that of non-OIS cameras, and outperform image-based\nalignment results with a relatively large margin.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 03:23:46 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Haipeng", ""], ["Liu", "Shuaicheng", ""], ["Wang", "Jue", ""]]}, {"id": "2101.11189", "submitter": "Yingqian Wang", "authors": "Feng Zhang, Xueying Wang, Shilin Zhou, Yingqian Wang, Yi Hou", "title": "Arbitrary-Oriented Ship Detection through Center-Head Point Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ship detection in remote sensing images plays a crucial role in various\napplications and has drawn increasing attention in recent years. However,\nexisting multi-oriented ship detection methods are generally developed on a set\nof predefined rotated anchor boxes. These predefined boxes not only lead to\ninaccurate angle predictions but also introduce extra hyper-parameters and high\ncomputational cost. Moreover, the prior knowledge of ship size has not been\nfully exploited by existing methods, which hinders the improvement of their\ndetection accuracy. Aiming at solving the above issues, in this paper, we\npropose a \\emph{center-head point extraction based detector} (named CHPDet) to\nachieve arbitrary-oriented ship detection in remote sensing images. Our CHPDet\nformulates arbitrary-oriented ships as rotated boxes with head points which are\nused to determine the direction. The orientation-invariant model (OIM) is used\nto produce orientation-invariant feature maps. Keypoint estimation is performed\nto find the center of ships. Then, the size and head point of the ships are\nregressed. Finally, we use the target size as prior to finetune the results.\nMoreover, we introduce a new dataset for multi-class arbitrary-oriented ship\ndetection in remote sensing images at a fixed ground sample distance (GSD)\nwhich is named FGSD2021. Experimental results on two ship detection datasets\n(i.e., FGSD2021 and HRSC2016) demonstrate that our CHPDet achieves\nstate-of-the-art performance and can well distinguish between bow and stern.\nThe code and dataset will be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 03:58:52 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 13:17:04 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zhang", "Feng", ""], ["Wang", "Xueying", ""], ["Zhou", "Shilin", ""], ["Wang", "Yingqian", ""], ["Hou", "Yi", ""]]}, {"id": "2101.11217", "submitter": "Tejas Khare", "authors": "Tejas Atul Khare and Anuradha C. Phadke", "title": "Automated Crop Field Surveillance using Computer Vision", "comments": "6 Pages, 10 Figures", "journal-ref": "Proceedings reference - 978-1-7281-9885-9/20/$31.00\n  \\c{opyright}2020 IEEE", "doi": "10.1109/DISCOVER50404.2020.9278072", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Artificial Intelligence is everywhere today. But unfortunately, Agriculture\nhas not been able to get that much attention from Artificial Intelligence (AI).\nA lack of automation persists in the agriculture industry. For over many years,\nfarmers and crop field owners have been facing a problem of trespassing of wild\nanimals for which no feasible solution has been provided. Installing a fence or\nbarrier like structure is neither feasible nor efficient due to the large areas\ncovered by the fields. Also, if the landowner can afford to build a wall or\nbarrier, government policies for building walls are often very irksome. The\npaper intends to give a simple intelligible solution to the problem with\nAutomated Crop Field Surveillance using Computer Vision. The solution will\nsignificantly reduce the cost of crops destroyed annually and completely\nautomate the security of the field.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 05:58:28 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Khare", "Tejas Atul", ""], ["Phadke", "Anuradha C.", ""]]}, {"id": "2101.11221", "submitter": "Kwanyoung Park", "authors": "Kwanyoung Park, Junseok Park, Hyunseok Oh, Byoung-Tak Zhang, Youngki\n  Lee", "title": "Learning task-agnostic representation via toddler-inspired learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the inherent limitations of current AI systems, stemming from the\npassive learning mechanisms (e.g., supervised learning), is that they perform\nwell on labeled datasets but cannot deduce knowledge on their own. To tackle\nthis problem, we derive inspiration from a highly intentional learning system\nvia action: the toddler. Inspired by the toddler's learning procedure, we\ndesign an interactive agent that can learn and store task-agnostic visual\nrepresentation while exploring and interacting with objects in the virtual\nenvironment. Experimental results show that such obtained representation was\nexpandable to various vision tasks such as image classification, object\nlocalization, and distance estimation tasks. In specific, the proposed model\nachieved 100%, 75.1% accuracy and 1.62% relative error, respectively, which is\nnoticeably better than autoencoder-based model (99.7%, 66.1%, 1.95%), and also\ncomparable with those of supervised models (100%, 87.3%, 0.71%).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 06:26:56 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Park", "Kwanyoung", ""], ["Park", "Junseok", ""], ["Oh", "Hyunseok", ""], ["Zhang", "Byoung-Tak", ""], ["Lee", "Youngki", ""]]}, {"id": "2101.11222", "submitter": "Jafar Majidpour", "authors": "Jafar Majidpour and Samer Kais Jameel", "title": "Automatic image annotation base on Naive Bayes and Decision Tree\n  classifiers using MPEG-7", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently it has become essential to search for and retrieve high-resolution\nand efficient images easily due to swift development of digital images, many\npresent annotation algorithms facing a big challenge which is the variance for\nrepresent the image where high level represent image semantic and low level\nillustrate the features, this issue is known as semantic gab. This work has\nbeen used MPEG-7 standard to extract the features from the images, where the\ncolor feature was extracted by using Scalable Color Descriptor (SCD) and Color\nLayout Descriptor (CLD), whereas the texture feature was extracted by employing\nEdge Histogram Descriptor (EHD), the CLD produced high dimensionality feature\nvector therefore it is reduced by Principal Component Analysis (PCA). The\nfeatures that have extracted by these three descriptors could be passing to the\nclassifiers (Naive Bayes and Decision Tree) for training. Finally, they\nannotated the query image. In this study TUDarmstadt image bank had been used.\nThe results of tests and comparative performance evaluation indicated better\nprecision and executing time of Naive Bayes classification in comparison with\nDecision Tree classification.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 06:27:16 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Majidpour", "Jafar", ""], ["Jameel", "Samer Kais", ""]]}, {"id": "2101.11223", "submitter": "Rawal Khirodkar", "authors": "Rawal Khirodkar, Visesh Chari, Amit Agrawal, Ambrish Tyagi", "title": "Multi-Instance Pose Networks: Rethinking Top-Down Pose Estimation", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key assumption of top-down human pose estimation approaches is their\nexpectation of having a single person/instance present in the input bounding\nbox. This often leads to failures in crowded scenes with occlusions. We propose\na novel solution to overcome the limitations of this fundamental assumption.\nOur Multi-Instance Pose Network (MIPNet) allows for predicting multiple 2D pose\ninstances within a given bounding box. We introduce a Multi-Instance Modulation\nBlock (MIMB) that can adaptively modulate channel-wise feature responses for\neach instance and is parameter efficient. We demonstrate the efficacy of our\napproach by evaluating on COCO, CrowdPose, and OCHuman datasets. Specifically,\nwe achieve 70.0 AP on CrowdPose and 42.5 AP on OCHuman test sets, a significant\nimprovement of 2.4 AP and 6.5 AP over the prior art, respectively. When using\nground truth bounding boxes for inference, MIPNet achieves an improvement of\n0.7 AP on COCO, 0.9 AP on CrowdPose, and 9.1 AP on OCHuman validation sets\ncompared to HRNet. Interestingly, when fewer, high confidence bounding boxes\nare used, HRNet's performance degrades (by 5 AP) on OCHuman, whereas MIPNet\nmaintains a relatively stable performance (drop of 1 AP) for the same inputs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 06:51:28 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 13:46:02 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Khirodkar", "Rawal", ""], ["Chari", "Visesh", ""], ["Agrawal", "Amit", ""], ["Tyagi", "Ambrish", ""]]}, {"id": "2101.11224", "submitter": "Jianzhe Lin", "authors": "Jianzhe Lin, Ghazal Sahebzamani, Christina Luong, Fatemeh Taheri\n  Dezaki, Mohammad Jafari, Purang Abolmaesumi, Teresa Tsang", "title": "Reciprocal Landmark Detection and Tracking with Extremely Few\n  Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Localization of anatomical landmarks to perform two-dimensional measurements\nin echocardiography is part of routine clinical workflow in cardiac disease\ndiagnosis. Automatic localization of those landmarks is highly desirable to\nimprove workflow and reduce interobserver variability. Training a machine\nlearning framework to perform such localization is hindered given the sparse\nnature of gold standard labels; only few percent of cardiac cine series frames\nare normally manually labeled for clinical use. In this paper, we propose a new\nend-to-end reciprocal detection and tracking model that is specifically\ndesigned to handle the sparse nature of echocardiography labels. The model is\ntrained using few annotated frames across the entire cardiac cine sequence to\ngenerate consistent detection and tracking of landmarks, and an adversarial\ntraining for the model is proposed to take advantage of these annotated frames.\nThe superiority of the proposed reciprocal model is demonstrated using a series\nof experiments.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 06:59:41 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lin", "Jianzhe", ""], ["Sahebzamani", "Ghazal", ""], ["Luong", "Christina", ""], ["Dezaki", "Fatemeh Taheri", ""], ["Jafari", "Mohammad", ""], ["Abolmaesumi", "Purang", ""], ["Tsang", "Teresa", ""]]}, {"id": "2101.11228", "submitter": "Torben Teepe", "authors": "Torben Teepe, Ali Khan, Johannes Gilg, Fabian Herzog, Stefan\n  H\\\"ormann, Gerhard Rigoll", "title": "GaitGraph: Graph Convolutional Network for Skeleton-Based Gait\n  Recognition", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition is a promising video-based biometric for identifying\nindividual walking patterns from a long distance. At present, most gait\nrecognition methods use silhouette images to represent a person in each frame.\nHowever, silhouette images can lose fine-grained spatial information, and most\npapers do not regard how to obtain these silhouettes in complex scenes.\nFurthermore, silhouette images contain not only gait features but also other\nvisual clues that can be recognized. Hence these approaches can not be\nconsidered as strict gait recognition.\n  We leverage recent advances in human pose estimation to estimate robust\nskeleton poses directly from RGB images to bring back model-based gait\nrecognition with a cleaner representation of gait. Thus, we propose GaitGraph\nthat combines skeleton poses with Graph Convolutional Network (GCN) to obtain a\nmodern model-based approach for gait recognition. The main advantages are a\ncleaner, more elegant extraction of the gait features and the ability to\nincorporate powerful spatio-temporal modeling using GCN. Experiments on the\npopular CASIA-B gait dataset show that our method archives state-of-the-art\nperformance in model-based gait recognition.\n  The code and models are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 07:14:06 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:04:32 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Teepe", "Torben", ""], ["Khan", "Ali", ""], ["Gilg", "Johannes", ""], ["Herzog", "Fabian", ""], ["H\u00f6rmann", "Stefan", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "2101.11239", "submitter": "Akila Pemasiri", "authors": "Akila Pemasiri, Kien Nguyen Thanh, Sridha Sridharan, Clinton Fookes", "title": "Im2Mesh GAN: Accurate 3D Hand Mesh Recovery from a Single RGB Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses hand mesh recovery from a single RGB image. In contrast\nto most of the existing approaches where the parametric hand models are\nemployed as the prior, we show that the hand mesh can be learned directly from\nthe input image. We propose a new type of GAN called Im2Mesh GAN to learn the\nmesh through end-to-end adversarial training. By interpreting the mesh as a\ngraph, our model is able to capture the topological relationship among the mesh\nvertices. We also introduce a 3D surface descriptor into the GAN architecture\nto further capture the 3D features associated. We experiment two approaches\nwhere one can reap the benefits of coupled groundtruth data availability of\nimages and the corresponding meshes, while the other combats the more\nchallenging problem of mesh estimations without the corresponding groundtruth.\nThrough extensive evaluations we demonstrate that the proposed method\noutperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 07:38:01 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Pemasiri", "Akila", ""], ["Thanh", "Kien Nguyen", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2101.11245", "submitter": "Kele Xu", "authors": "Kele Xu and Tamas G\\'abor Csap\\'o and Ming Feng", "title": "Convolutional Neural Network-Based Age Estimation Using B-Mode\n  Ultrasound Tongue Image", "comments": "5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ultrasound tongue imaging is widely used for speech production research, and\nit has attracted increasing attention as its potential applications seem to be\nevident in many different fields, such as the visual biofeedback tool for\nsecond language acquisition and silent speech interface. Unlike previous\nstudies, here we explore the feasibility of age estimation using the ultrasound\ntongue image of the speakers. Motivated by the success of deep learning, this\npaper leverages deep learning on this task. We train a deep convolutional\nneural network model on the UltraSuite dataset. The deep model achieves mean\nabsolute error (MAE) of 2.03 for the data from typically developing children,\nwhile MAE is 4.87 for the data from the children with speech sound disorders,\nwhich suggest that age estimation using ultrasound is more challenging for the\nchildren with speech sound disorder. The developed method can be used a tool to\nevaluate the performance of speech therapy sessions. It is also worthwhile to\nnotice that, although we leverage the ultrasound tongue imaging for our study,\nthe proposed methods may also be extended to other imaging modalities (e.g.\nMRI) to assist the studies on speech production.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:00:47 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Xu", "Kele", ""], ["Csap\u00f3", "Tamas G\u00e1bor", ""], ["Feng", "Ming", ""]]}, {"id": "2101.11249", "submitter": "Sai Sukruth Bezugam", "authors": "Sai Sukruth Bezugam, Swatilekha Majumdar, Chetan Ralekar and Tapan\n  Kumar Gandhi", "title": "Efficient Video Summarization Framework using EEG and Eye-tracking\n  Signals", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient video summarization framework that will give\na gist of the entire video in a few key-frames or video skims. Existing video\nsummarization frameworks are based on algorithms that utilize computer vision\nlow-level feature extraction or high-level domain level extraction. However,\nbeing the ultimate user of the summarized video, humans remain the most\nneglected aspect. Therefore, the proposed paper considers human's role in\nsummarization and introduces human visual attention-based summarization\ntechniques. To understand human attention behavior, we have designed and\nperformed experiments with human participants using electroencephalogram (EEG)\nand eye-tracking technology. The EEG and eye-tracking data obtained from the\nexperimentation are processed simultaneously and used to segment frames\ncontaining useful information from a considerable video volume. Thus, the frame\nsegmentation primarily relies on the cognitive judgments of human beings. Using\nour approach, a video is summarized by 96.5% while maintaining higher precision\nand high recall factors. The comparison with the state-of-the-art techniques\ndemonstrates that the proposed approach yields ceiling-level performance with\nreduced computational cost in summarising the videos.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:13:19 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Bezugam", "Sai Sukruth", ""], ["Majumdar", "Swatilekha", ""], ["Ralekar", "Chetan", ""], ["Gandhi", "Tapan Kumar", ""]]}, {"id": "2101.11251", "submitter": "Yuqian Fu", "authors": "Zhihao Liu, Yuqian Fu", "title": "e-ACJ: Accurate Junction Extraction For Event Cameras", "comments": "Accepted by ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Junctions reflect the important geometrical structure information of the\nimage, and are of primary significance to applications such as image matching\nand motion analysis. Previous event-based feature extraction methods are mainly\nfocused on corners, which mainly find their locations, however, ignoring the\ngeometrical structure information like orientations and scales of edges. This\npaper adapts the frame-based a-contrario junction detector(ACJ) to event data,\nproposing the event-based a-contrario junction detector(e-ACJ), which yields\njunctions' locations while giving the scales and orientations of their\nbranches. The proposed method relies on an a-contrario model and can operate on\nasynchronous events directly without generating synthesized event frames. We\nevaluate the performance on public event datasets. The result shows our method\nsuccessfully finds the orientations and scales of branches, while maintaining\nhigh accuracy in junction's location.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:15:11 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 08:55:16 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Liu", "Zhihao", ""], ["Fu", "Yuqian", ""]]}, {"id": "2101.11253", "submitter": "Sanghyun Jo", "authors": "Sanghyun Jo, In-Jae Yu", "title": "Puzzle-CAM: Improved localization via matching partial and full features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Weakly-supervised semantic segmentation (WSSS) is introduced to narrow the\ngap for semantic segmentation performance from pixel-level supervision to\nimage-level supervision. Most advanced approaches are based on class activation\nmaps (CAMs) to generate pseudo-labels to train the segmentation network. The\nmain limitation of WSSS is that the process of generating pseudo-labels from\nCAMs that use an image classifier is mainly focused on the most discriminative\nparts of the objects. To address this issue, we propose Puzzle-CAM, a process\nthat minimizes differences between the features from separate patches and the\nwhole image. Our method consists of a puzzle module and two regularization\nterms to discover the most integrated region in an object. Puzzle-CAM can\nactivate the overall region of an object using image-level supervision without\nrequiring extra parameters. % In experiments, Puzzle-CAM outperformed previous\nstate-of-the-art methods using the same labels for supervision on the PASCAL\nVOC 2012 test dataset. In experiments, Puzzle-CAM outperformed previous\nstate-of-the-art methods using the same labels for supervision on the PASCAL\nVOC 2012 dataset. Code associated with our experiments is available at\nhttps://github.com/OFRIN/PuzzleCAM.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:19:38 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 03:34:21 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 06:24:39 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Jo", "Sanghyun", ""], ["Yu", "In-Jae", ""]]}, {"id": "2101.11254", "submitter": "Haochen Mei", "authors": "Haochen Mei, Wenhui Lei, Ran Gu, Shan Ye, Zhengwentai Sun, Shichuan\n  Zhang, Guotai Wang", "title": "Automatic Segmentation of Gross Target Volume of Nasopharynx Cancer\n  using Ensemble of Multiscale Deep Neural Networks with Spatial Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiotherapy is the main treatment modality for nasopharynx cancer.\nDelineation of Gross Target Volume (GTV) from medical images such as CT and MRI\nimages is a prerequisite for radiotherapy. As manual delineation is\ntime-consuming and laborious, automatic segmentation of GTV has a potential to\nimprove this process. Currently, most of the deep learning-based automatic\ndelineation methods of GTV are mainly performed on medical images like CT\nimages. However, it is challenged by the low contrast between the pathology\nregions and surrounding soft tissues, small target region, and anisotropic\nresolution of clinical CT images. To deal with these problems, we propose a\n2.5D Convolutional Neural Network (CNN) to handle the difference of inplane and\nthrough-plane resolution. Furthermore, we propose a spatial attention module to\nenable the network to focus on small target, and use channel attention to\nfurther improve the segmentation performance. Moreover, we use multi-scale\nsampling method for training so that the networks can learn features at\ndifferent scales, which are combined with a multi-model ensemble method to\nimprove the robustness of segmentation results. We also estimate the\nuncertainty of segmentation results based on our model ensemble, which is of\ngreat importance for indicating the reliability of automatic segmentation\nresults for radiotherapy planning.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:20:49 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Mei", "Haochen", ""], ["Lei", "Wenhui", ""], ["Gu", "Ran", ""], ["Ye", "Shan", ""], ["Sun", "Zhengwentai", ""], ["Zhang", "Shichuan", ""], ["Wang", "Guotai", ""]]}, {"id": "2101.11266", "submitter": "Tomasz Szandala", "authors": "Tomasz Szandala", "title": "TorchPRISM: Principal Image Sections Mapping, a novel method for\n  Convolutional Neural Network features visualization", "comments": "Very early draft, software can be found:\n  https://github.com/szandala/TorchPRISM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce a tool called Principal Image Sections Mapping -\nPRISM, dedicated for PyTorch, but can be easily ported to other deep learning\nframeworks. Presented software relies on Principal Component Analysis to\nvisualize the most significant features recognized by a given Convolutional\nNeural Network. Moreover, it allows to display comparative set features between\nimages processed in the same batch, therefore PRISM can be a method well\nsynerging with technique Explanation by Example.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:54:23 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Szandala", "Tomasz", ""]]}, {"id": "2101.11272", "submitter": "Kyosuke Nishida", "authors": "Ryota Tanaka, Kyosuke Nishida, Sen Yoshida", "title": "VisualMRC: Machine Reading Comprehension on Document Images", "comments": "Accepted as a full paper at AAAI 2021. The first two authors have\n  equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on machine reading comprehension have focused on text-level\nunderstanding but have not yet reached the level of human understanding of the\nvisual layout and content of real-world documents. In this study, we introduce\na new visual machine reading comprehension dataset, named VisualMRC, wherein\ngiven a question and a document image, a machine reads and comprehends texts in\nthe image to answer the question in natural language. Compared with existing\nvisual question answering (VQA) datasets that contain texts in images,\nVisualMRC focuses more on developing natural language understanding and\ngeneration abilities. It contains 30,000+ pairs of a question and an\nabstractive answer for 10,000+ document images sourced from multiple domains of\nwebpages. We also introduce a new model that extends existing\nsequence-to-sequence models, pre-trained with large-scale text corpora, to take\ninto account the visual layout and content of documents. Experiments with\nVisualMRC show that this model outperformed the base sequence-to-sequence\nmodels and a state-of-the-art VQA model. However, its performance is still\nbelow that of humans on most automatic evaluation metrics. The dataset will\nfacilitate research aimed at connecting vision and language understanding.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 09:03:06 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 08:13:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tanaka", "Ryota", ""], ["Nishida", "Kyosuke", ""], ["Yoshida", "Sen", ""]]}, {"id": "2101.11282", "submitter": "Wei Chen", "authors": "Wei Chen, Yu Liu, Weiping Wang, Erwin Bakker, Theodoros Georgiou, Paul\n  Fieguth, Li Liu, and Michael S. Lew", "title": "Deep Image Retrieval: A Survey", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years a vast amount of visual content has been generated and shared\nfrom various fields, such as social media platforms, medical images, and\nrobotics. This abundance of content creation and sharing has introduced new\nchallenges. In particular, searching databases for similar content, i.e.content\nbased image retrieval (CBIR), is a long-established research area, and more\nefficient and accurate methods are needed for real time retrieval. Artificial\nintelligence has made progress in CBIR and has significantly facilitated the\nprocess of intelligent search. In this survey we organize and review recent\nCBIR works that are developed based on deep learning algorithms and techniques,\nincluding insights and techniques from recent papers. We identify and present\nthe commonly-used benchmarks and evaluation methods used in the field. We\ncollect common challenges and propose promising future directions. More\nspecifically, we focus on image retrieval with deep learning and organize the\nstate of the art methods according to the types of deep network structure, deep\nfeatures, feature enhancement methods, and network fine-tuning strategies. Our\nsurvey considers a wide variety of recent methods, aiming to promote a global\nview of the field of instance-based CBIR.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 09:32:58 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 00:33:32 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Chen", "Wei", ""], ["Liu", "Yu", ""], ["Wang", "Weiping", ""], ["Bakker", "Erwin", ""], ["Georgiou", "Theodoros", ""], ["Fieguth", "Paul", ""], ["Liu", "Li", ""], ["Lew", "Michael S.", ""]]}, {"id": "2101.11299", "submitter": "Peixiao Zheng", "authors": "Peixiao Zheng, Xin Guo, Lin Qi", "title": "Edge-Labeling based Directed Gated Graph Network for Few-shot Learning", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing graph-network-based few-shot learning methods obtain similarity\nbetween nodes through a convolution neural network (CNN). However, the CNN is\ndesigned for image data with spatial information rather than vector form node\nfeature. In this paper, we proposed an edge-labeling-based directed gated graph\nnetwork (DGGN) for few-shot learning, which utilizes gated recurrent units to\nimplicitly update the similarity between nodes. DGGN is composed of a gated\nnode aggregation module and an improved gated recurrent unit (GRU) based edge\nupdate module. Specifically, the node update module adopts a gate mechanism\nusing activation of edge feature, making a learnable node aggregation process.\nBesides, improved GRU cells are employed in the edge update procedure to\ncompute the similarity between nodes. Further, this mechanism is beneficial to\ngradient backpropagation through the GRU sequence across layers. Experiment\nresults conducted on two benchmark datasets show that our DGGN achieves a\ncomparable performance to the-state-of-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 10:14:20 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zheng", "Peixiao", ""], ["Guo", "Xin", ""], ["Qi", "Lin", ""]]}, {"id": "2101.11306", "submitter": "Shuohui Li", "authors": "Shuo-Hui Li", "title": "Learning Non-linear Wavelet Transformation via Normalizing Flow", "comments": "Main text: 7 pages, 5 figures. Supplement: 5 pages. Github link:\n  https://github.com/li012589/NeuralWavelet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelet transformation stands as a cornerstone in modern data analysis and\nsignal processing. Its mathematical essence is an invertible transformation\nthat discerns slow patterns from fast patterns in the frequency domain, which\nrepeats at each level. Such an invertible transformation can be learned by a\ndesigned normalizing flow model. With a factor-out scheme resembling the\nwavelet downsampling mechanism, a mutually independent prior, and parameter\nsharing along the depth of the network, one can train normalizing flow models\nto factor-out variables corresponding to fast patterns at different levels,\nthus extending linear wavelet transformations to non-linear learnable models.\nIn this paper, a concrete way of building such flows is given. Then, a\ndemonstration of the model's ability in lossless compression task, progressive\nloading, and super-resolution (upsampling) task. Lastly, an analysis of the\nlearned model in terms of low-pass/high-pass filters is given.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 10:28:51 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Shuo-Hui", ""]]}, {"id": "2101.11342", "submitter": "Yibo Yang", "authors": "Yibo Yang, Shan You, Hongyang Li, Fei Wang, Chen Qian, Zhouchen Lin", "title": "Towards Improving the Consistency, Efficiency, and Flexibility of\n  Differentiable Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most differentiable neural architecture search methods construct a super-net\nfor search and derive a target-net as its sub-graph for evaluation. There\nexists a significant gap between the architectures in search and evaluation. As\na result, current methods suffer from an inconsistent, inefficient, and\ninflexible search process. In this paper, we introduce EnTranNAS that is\ncomposed of Engine-cells and Transit-cells. The Engine-cell is differentiable\nfor architecture search, while the Transit-cell only transits a sub-graph by\narchitecture derivation. Consequently, the gap between the architectures in\nsearch and evaluation is significantly reduced. Our method also spares much\nmemory and computation cost, which speeds up the search process. A feature\nsharing strategy is introduced for more balanced optimization and more\nefficient search. Furthermore, we develop an architecture derivation method to\nreplace the traditional one that is based on a hand-crafted rule. Our method\nenables differentiable sparsification, and keeps the derived architecture\nequivalent to that of Engine-cell, which further improves the consistency\nbetween search and evaluation. Besides, it supports the search for topology\nwhere a node can be connected to prior nodes with any number of connections, so\nthat the searched architectures could be more flexible. For experiments on\nCIFAR-10, our search on the standard space requires only 0.06 GPU-day. We\nfurther have an error rate of 2.22% with 0.07 GPU-day for the search on an\nextended space. We can also directly perform the search on ImageNet with\ntopology learnable and achieve a top-1 error rate of 23.8% in 2.1 GPU-day.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:16:47 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yang", "Yibo", ""], ["You", "Shan", ""], ["Li", "Hongyang", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Lin", "Zhouchen", ""]]}, {"id": "2101.11353", "submitter": "Yufei Cui", "authors": "Yufei Cui, Ziquan Liu, Qiao Li, Yu Mao, Antoni B. Chan, Chun Jason Xue", "title": "Bayesian Nested Neural Networks for Uncertainty Calibration and Adaptive\n  Compression", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested networks or slimmable networks are neural networks whose architectures\ncan be adjusted instantly during testing time, e.g., based on computational\nconstraints. Recent studies have focused on a \"nested dropout\" layer, which is\nable to order the nodes of a layer by importance during training, thus\ngenerating a nested set of sub-networks that are optimal for different\nconfigurations of resources. However, the dropout rate is fixed as a\nhyper-parameter over different layers during the whole training process.\nTherefore, when nodes are removed, the performance decays in a human-specified\ntrajectory rather than in a trajectory learned from data. Another drawback is\nthe generated sub-networks are deterministic networks without well-calibrated\nuncertainty. To address these two problems, we develop a Bayesian approach to\nnested neural networks. We propose a variational ordering unit that draws\nsamples for nested dropout at a low cost, from a proposed Downhill\ndistribution, which provides useful gradients to the parameters of nested\ndropout. Based on this approach, we design a Bayesian nested neural network\nthat learns the order knowledge of the node distributions. In experiments, we\nshow that the proposed approach outperforms the nested network in terms of\naccuracy, calibration, and out-of-domain detection in classification tasks. It\nalso outperforms the related approach on uncertainty-critical tasks in computer\nvision.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:34:58 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Cui", "Yufei", ""], ["Liu", "Ziquan", ""], ["Li", "Qiao", ""], ["Mao", "Yu", ""], ["Chan", "Antoni B.", ""], ["Xue", "Chun Jason", ""]]}, {"id": "2101.11391", "submitter": "Charles Wilmot", "authors": "Charles Wilmot, Bertram E. Shi, Jochen Triesch", "title": "Self-Calibrating Active Binocular Vision via Active Efficient Coding\n  with Deep Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a model of the self-calibration of active binocular vision\ncomprising the simultaneous learning of visual representations, vergence, and\npursuit eye movements. The model follows the principle of Active Efficient\nCoding (AEC), a recent extension of the classic Efficient Coding Hypothesis to\nactive perception. In contrast to previous AEC models, the present model uses\ndeep autoencoders to learn sensory representations. We also propose a new\nformulation of the intrinsic motivation signal that guides the learning of\nbehavior. We demonstrate the performance of the model in simulations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 13:40:16 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Wilmot", "Charles", ""], ["Shi", "Bertram E.", ""], ["Triesch", "Jochen", ""]]}, {"id": "2101.11451", "submitter": "Debarati Chakraborty Dr.", "authors": "Debarati B. Chakraborty, Mukesh Sharma and Bhaskar Vijay", "title": "Controlling by Showing: i-Mimic: A Video-based Method to Control Robotic\n  Arms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A novel concept of vision-based intelligent control of robotic arms is\ndeveloped here in this work. This work enables the controlling of robotic arms\nmotion only with visual inputs, that is, controlling by showing the videos of\ncorrect movements. This work can broadly be sub-divided into two segments. The\nfirst part of this work is to develop an unsupervised vision-based method to\ncontrol robotic arm in 2-D plane, and the second one is with deep CNN in the\nsame task in 3-D plane. The first method is unsupervised, where our aim is to\nperform mimicking of human arm motion in real-time by a manipulator. We\ndeveloped a network, namely the vision-to-motion optical network (DON), where\nthe input should be a video stream containing hand movements of human, the the\noutput would be out the velocity and torque information of the hand movements\nshown in the videos. The output information of the DON is then fed to the\nrobotic arm by enabling it to generate motion according to the real hand\nvideos. The method has been tested with both live-stream video feed as well as\non recorded video obtained from a monocular camera even by intelligently\npredicting the trajectory of human hand hand when it gets occluded. This is why\nthe mimicry of the arm incorporates some intelligence to it and becomes\nintelligent mimic (i-mimic). Alongside the unsupervised method another method\nhas also been developed deploying the deep neural network technique with CNN\n(Convolutional Neural Network) to perform the mimicking, where labelled\ndatasets are used for training. The same dataset, as used in the unsupervised\nDON-based method, is used in the deep CNN method, after manual annotations.\nBoth the proposed methods are validated with off-line as well as with on-line\nvideo datasets in real-time. The entire methodology is validated with real-time\n1-link and simulated n-link manipulators alongwith suitable comparisons.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:35:52 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Chakraborty", "Debarati B.", ""], ["Sharma", "Mukesh", ""], ["Vijay", "Bhaskar", ""]]}, {"id": "2101.11453", "submitter": "Jan Metzen", "authors": "Jan Hendrik Metzen, Nicole Finnie, Robin Hutmacher", "title": "Meta Adversarial Training against Universal Patches", "comments": "Accepted by the ICML 2021 workshop on \"A Blessing in Disguise: The\n  Prospects and Perils of Adversarial Machine Learning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently demonstrated physical-world adversarial attacks have exposed\nvulnerabilities in perception systems that pose severe risks for\nsafety-critical applications such as autonomous driving. These attacks place\nadversarial artifacts in the physical world that indirectly cause the addition\nof a universal patch to inputs of a model that can fool it in a variety of\ncontexts. Adversarial training is the most effective defense against\nimage-dependent adversarial attacks. However, tailoring adversarial training to\nuniversal patches is computationally expensive since the optimal universal\npatch depends on the model weights which change during training. We propose\nmeta adversarial training (MAT), a novel combination of adversarial training\nwith meta-learning, which overcomes this challenge by meta-learning universal\npatches along with model training. MAT requires little extra computation while\ncontinuously adapting a large set of patches to the current model. MAT\nconsiderably increases robustness against universal patch attacks on image\nclassification and traffic-light detection.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:36:23 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 14:07:54 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Finnie", "Nicole", ""], ["Hutmacher", "Robin", ""]]}, {"id": "2101.11461", "submitter": "Fupin Yao", "authors": "Fupin Yao", "title": "Machine learning with limited data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thanks to the availability of powerful computing resources, big data and deep\nlearning algorithms, we have made great progress on computer vision in the last\nfew years. Computer vision systems begin to surpass humans in some tasks, such\nas object recognition, object detection, face recognition and pose estimation.\nLots of computer vision algorithms have been deployed to real world\napplications and started to improve our life quality. However, big data and\nlabels are not always available. Sometimes we only have very limited labeled\ndata, such as medical images which requires experts to label them. In this\npaper, we study few shot image classification, in which we only have very few\nlabeled data. Machine learning with little data is a big challenge. To tackle\nthis challenge, we propose two methods and test their effectiveness thoroughly.\nOne method is to augment image features by mixing the style of these images.\nThe second method is applying spatial attention to explore the relations\nbetween patches of images. We also find that domain shift is a critical issue\nin few shot learning when the training domain and testing domain are different.\nSo we propose a more realistic cross-domain few-shot learning with unlabeled\ndata setting, in which some unlabeled data is available in the target domain.\nWe propose two methods in this setting. Our first method transfers the style\ninformation of the unlabeled target dataset to the samples in the source\ndataset and trains a model with stylized images and original images. Our second\nmethod proposes a unified framework to fully utilize all the data. Both of our\nmethods surpass the baseline method by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 17:10:39 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yao", "Fupin", ""]]}, {"id": "2101.11466", "submitter": "Federico Nesti", "authors": "Federico Nesti, Alessandro Biondi, Giorgio Buttazzo", "title": "Detecting Adversarial Examples by Input Transformations, Defense\n  Perturbations, and Voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over the last few years, convolutional neural networks (CNNs) have proved to\nreach super-human performance in visual recognition tasks. However, CNNs can\neasily be fooled by adversarial examples, i.e., maliciously-crafted images that\nforce the networks to predict an incorrect output while being extremely similar\nto those for which a correct output is predicted. Regular adversarial examples\nare not robust to input image transformations, which can then be used to detect\nwhether an adversarial example is presented to the network. Nevertheless, it is\nstill possible to generate adversarial examples that are robust to such\ntransformations.\n  This paper extensively explores the detection of adversarial examples via\nimage transformations and proposes a novel methodology, called \\textit{defense\nperturbation}, to detect robust adversarial examples with the same input\ntransformations the adversarial examples are robust to. Such a \\textit{defense\nperturbation} is shown to be an effective counter-measure to robust adversarial\nexamples.\n  Furthermore, multi-network adversarial examples are introduced. This kind of\nadversarial examples can be used to simultaneously fool multiple networks,\nwhich is critical in systems that use network redundancy, such as those based\non architectures with majority voting over multiple CNNs. An extensive set of\nexperiments based on state-of-the-art CNNs trained on the Imagenet dataset is\nfinally reported.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:50:41 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Nesti", "Federico", ""], ["Biondi", "Alessandro", ""], ["Buttazzo", "Giorgio", ""]]}, {"id": "2101.11472", "submitter": "Jingwen Zhao", "authors": "Jingwen Zhao, Xuanpeng Li, Qifan Xue, Weigong Zhang", "title": "Spatial-Channel Transformer Network for Trajectory Prediction on the\n  Traffic Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting motion of surrounding agents is critical to real-world\napplications of tactical path planning for autonomous driving. Due to the\ncomplex temporal dependencies and social interactions of agents, on-line\ntrajectory prediction is a challenging task. With the development of attention\nmechanism in recent years, transformer model has been applied in natural\nlanguage sequence processing first and then image processing. In this paper, we\npresent a Spatial-Channel Transformer Network for trajectory prediction with\nattention functions. Instead of RNN models, we employ transformer model to\ncapture the spatial-temporal features of agents. A channel-wise module is\ninserted to measure the social interaction between agents. We find that the\nSpatial-Channel Transformer Network achieves promising results on real-world\ntrajectory prediction datasets on the traffic scenes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 15:03:42 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 03:40:32 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Zhao", "Jingwen", ""], ["Li", "Xuanpeng", ""], ["Xue", "Qifan", ""], ["Zhang", "Weigong", ""]]}, {"id": "2101.11476", "submitter": "Alvaro Gomariz", "authors": "Alvaro Gomariz, Raphael Egli, Tiziano Portenier, C\\'esar\n  Nombela-Arrieta, Orcun Goksel", "title": "Utilizing Uncertainty Estimation in Deep Learning Segmentation of\n  Fluorescence Microscopy Images with Missing Markers", "comments": "Accepted at the IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2021. 4 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluorescence microscopy images contain several channels, each indicating a\nmarker staining the sample. Since many different marker combinations are\nutilized in practice, it has been challenging to apply deep learning based\nsegmentation models, which expect a predefined channel combination for all\ntraining samples as well as at inference for future application. Recent work\ncircumvents this problem using a modality attention approach to be effective\nacross any possible marker combination. However, for combinations that do not\nexist in a labeled training dataset, one cannot have any estimation of\npotential segmentation quality if that combination is encountered during\ninference. Without this, not only one lacks quality assurance but one also does\nnot know where to put any additional imaging and labeling effort. We herein\npropose a method to estimate segmentation quality on unlabeled images by (i)\nestimating both aleatoric and epistemic uncertainties of convolutional neural\nnetworks for image segmentation, and (ii) training a Random Forest model for\nthe interpretation of uncertainty features via regression to their\ncorresponding segmentation metrics. Additionally, we demonstrate that including\nthese uncertainty measures during training can provide an improvement on\nsegmentation performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 15:06:04 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Gomariz", "Alvaro", ""], ["Egli", "Raphael", ""], ["Portenier", "Tiziano", ""], ["Nombela-Arrieta", "C\u00e9sar", ""], ["Goksel", "Orcun", ""]]}, {"id": "2101.11508", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo", "title": "Effects of Image Size on Deep Learning", "comments": "8 pages, 16 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the evaluation of effects of image size on deep learning\nperformance via semantic segmentation of magnetic resonance heart images with\nU-net for fully automated quantification of myocardial infarction. Both\nnon-extra pixel and extra pixel interpolation algorithms are used to change the\nsize of images in datasets of interest. Extra class labels, in interpolated\nground truth segmentation images, are removed using thresholding, median\nfiltering, and subtraction strategies. Common class metrics are used to\nevaluate the quality of semantic segmentation with U-net against the ground\ntruth segmentation while arbitrary threshold, comparison of the sums, and sums\nof differences between medical experts and fully automated results are options\nused to estimate the relationship between medical experts-based quantification\nand fully automated quantification results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:07:48 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 20:25:11 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Rukundo", "Olivier", ""]]}, {"id": "2101.11517", "submitter": "Risheng Liu", "authors": "Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng and Zhouchen Lin", "title": "Investigating Bi-Level Optimization for Learning and Vision from a\n  Unified Perspective: A Survey and Beyond", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bi-Level Optimization (BLO) is originated from the area of economic game\ntheory and then introduced into the optimization community. BLO is able to\nhandle problems with a hierarchical structure, involving two levels of\noptimization tasks, where one task is nested inside the other. In machine\nlearning and computer vision fields, despite the different motivations and\nmechanisms, a lot of complex problems, such as hyper-parameter optimization,\nmulti-task and meta-learning, neural architecture search, adversarial learning\nand deep reinforcement learning, actually all contain a series of closely\nrelated subproblms. In this paper, we first uniformly express these complex\nlearning and vision problems from the perspective of BLO. Then we construct a\nbest-response-based single-level reformulation and establish a unified\nalgorithmic framework to understand and formulate mainstream gradient-based BLO\nmethodologies, covering aspects ranging from fundamental automatic\ndifferentiation schemes to various accelerations, simplifications, extensions\nand their convergence and complexity properties. Last but not least, we discuss\nthe potentials of our unified BLO framework for designing new algorithms and\npoint out some promising directions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:20:23 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 14:16:15 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liu", "Risheng", ""], ["Gao", "Jiaxin", ""], ["Zhang", "Jin", ""], ["Meng", "Deyu", ""], ["Lin", "Zhouchen", ""]]}, {"id": "2101.11529", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Anirudh Thatipelli, Neel Trivedi, Ravi Kiran Sarvadevabhatla", "title": "NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions", "comments": "Code repository at https://github.com/skelemoa/ntu-x", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lack of fine-grained joints such as hand fingers is a fundamental\nperformance bottleneck for state of the art skeleton action recognition models\ntrained on the largest action recognition dataset, NTU-RGBD. To address this\nbottleneck, we introduce a new skeleton based human action dataset - NTU60-X.\nIn addition to the 25 body joints for each skeleton as in NTU-RGBD, NTU60-X\ndataset includes finger and facial joints, enabling a richer skeleton\nrepresentation. We appropriately modify the state of the art approaches to\nenable training using the introduced dataset. Our results demonstrate the\neffectiveness of NTU60-X in overcoming the aforementioned bottleneck and\nimprove state of the art performance, overall and on hitherto worst performing\naction categories.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:33:51 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 10:19:33 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Thatipelli", "Anirudh", ""], ["Trivedi", "Neel", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2101.11530", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Pranay Gupta, Divyanshu Sharma, Ravi Kiran Sarvadevabhatla", "title": "Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action\n  Recognition", "comments": "Accepted at ICIP-2021. Code and pretrained models available at\n  https://github.com/skelemoa/synse-zsl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce SynSE, a novel syntactically guided generative approach for\nZero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined\ngenerative embedding spaces constrained within and across the involved\nmodalities (visual, language). The inter-modal constraints are defined between\naction sequence embedding and embeddings of Parts of Speech (PoS) tagged words\nin the corresponding action description. We deploy SynSE for the task of\nskeleton-based action sequence recognition. Our design choices enable SynSE to\ngeneralize compositionally, i.e., recognize sequences whose action descriptions\ncontain words not encountered during training. We also extend our approach to\nthe more challenging Generalized Zero-Shot Learning (GZSL) problem via a\nconfidence-based gating mechanism. We are the first to present zero-shot\nskeleton action recognition results on the large-scale NTU-60 and NTU-120\nskeleton action datasets with multiple splits. Our results demonstrate SynSE's\nstate of the art performance in both ZSL and GZSL settings compared to strong\nbaselines on the NTU-60 and NTU-120 datasets. The code and pretrained models\nare available at https://github.com/skelemoa/synse-zsl\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:34:27 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 23:59:56 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gupta", "Pranay", ""], ["Sharma", "Divyanshu", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2101.11550", "submitter": "Shin-Nosuke Ishikawa", "authors": "Shin-nosuke Ishikawa, Hideaki Matsumura, Yasunobu Uchiyama and Lindsay\n  Glesener", "title": "Automatic Detection of Occulted Hard X-ray Flares Using Deep-Learning\n  Methods", "comments": "11 pages, 3 figures, accepted for publication in Solar Physics", "journal-ref": null, "doi": "10.1007/s11207-021-01780-x", "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a concept for a machine-learning classification of hard X-ray\n(HXR) emissions from solar flares observed by the Reuven Ramaty High Energy\nSolar Spectroscopic Imager (RHESSI), identifying flares that are either\nocculted by the solar limb or located on the solar disk. Although HXR\nobservations of occulted flares are important for particle-acceleration\nstudies, HXR data analyses for past observations were time consuming and\nrequired specialized expertise. Machine-learning techniques are promising for\nthis situation, and we constructed a sample model to demonstrate the concept\nusing a deep-learning technique. Input data to the model are HXR spectrograms\nthat are easily produced from RHESSI data. The model can detect occulted flares\nwithout the need for image reconstruction nor for visual inspection by experts.\nA technique of convolutional neural networks was used in this model by\nregarding the input data as images. Our model achieved a classification\naccuracy better than 90 %, and the ability for the application of the method to\neither event screening or for an event alert for occulted flares was\nsuccessfully demonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:11:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Ishikawa", "Shin-nosuke", ""], ["Matsumura", "Hideaki", ""], ["Uchiyama", "Yasunobu", ""], ["Glesener", "Lindsay", ""]]}, {"id": "2101.11562", "submitter": "Ting Yao", "authors": "Yehao Li and Yingwei Pan and Ting Yao and Jingwen Chen and Tao Mei", "title": "Scheduled Sampling in Vision-Language Pretraining with Decoupled\n  Encoder-Decoder Network", "comments": "AAAI 2021; Code is publicly available at:\n  https://github.com/YehLi/TDEN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite having impressive vision-language (VL) pretraining with BERT-based\nencoder for VL understanding, the pretraining of a universal encoder-decoder\nfor both VL understanding and generation remains challenging. The difficulty\noriginates from the inherently different peculiarities of the two disciplines,\ne.g., VL understanding tasks capitalize on the unrestricted message passing\nacross modalities, while generation tasks only employ visual-to-textual message\npassing. In this paper, we start with a two-stream decoupled design of\nencoder-decoder structure, in which two decoupled cross-modal encoder and\ndecoder are involved to separately perform each type of proxy tasks, for\nsimultaneous VL understanding and generation pretraining. Moreover, for VL\npretraining, the dominant way is to replace some input visual/word tokens with\nmask tokens and enforce the multi-modal encoder/decoder to reconstruct the\noriginal tokens, but no mask token is involved when fine-tuning on downstream\ntasks. As an alternative, we propose a primary scheduled sampling strategy that\nelegantly mitigates such discrepancy via pretraining encoder-decoder in a\ntwo-pass manner. Extensive experiments demonstrate the compelling\ngeneralizability of our pretrained encoder-decoder by fine-tuning on four VL\nunderstanding and generation downstream tasks. Source code is available at\n\\url{https://github.com/YehLi/TDEN}.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:36:57 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Yehao", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Chen", "Jingwen", ""], ["Mei", "Tao", ""]]}, {"id": "2101.11563", "submitter": "Alan Smeaton", "authors": "Rashmiranjan Das and Gaurav Negi and Alan F. Smeaton", "title": "Detecting Deepfake Videos Using Euler Video Magnification", "comments": "Presented at Electronic Imaging: Media Watermarking, Security, and\n  Forensics, 27 January 2021, 6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in artificial intelligence make it progressively hard to\ndistinguish between genuine and counterfeit media, especially images and\nvideos. One recent development is the rise of deepfake videos, based on\nmanipulating videos using advanced machine learning techniques. This involves\nreplacing the face of an individual from a source video with the face of a\nsecond person, in the destination video. This idea is becoming progressively\nrefined as deepfakes are getting progressively seamless and simpler to compute.\nCombined with the outreach and speed of social media, deepfakes could easily\nfool individuals when depicting someone saying things that never happened and\nthus could persuade people in believing fictional scenarios, creating distress,\nand spreading fake news. In this paper, we examine a technique for possible\nidentification of deepfake videos. We use Euler video magnification which\napplies spatial decomposition and temporal filtering on video data to highlight\nand magnify hidden features like skin pulsation and subtle motions. Our\napproach uses features extracted from the Euler technique to train three models\nto classify counterfeit and unaltered videos and compare the results with\nexisting techniques.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:37:23 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Das", "Rashmiranjan", ""], ["Negi", "Gaurav", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2101.11587", "submitter": "Steven Frank", "authors": "Steven J. Frank", "title": "The Work of Art in an Age of Mechanical Generation", "comments": "This is the author's final version; the article has been accepted for\n  publication in Leonardo Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can we define what it means to be \"creative,\" and if so, can our definition\ndrive artificial intelligence (AI) systems to feats of creativity\nindistinguishable from human efforts? This mixed question is considered from\ntechnological and social perspectives. Beginning with an exploration of the\nvalue we attach to authenticity in works of art, the article considers the\nability of AI to detect forgeries of renowned paintings and, in so doing,\nsomehow reveal the quiddity of a work of art. We conclude by considering\nwhether evolving technical capability can revise traditional relationships\namong art, artist, and the market.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:32:58 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Frank", "Steven J.", ""]]}, {"id": "2101.11599", "submitter": "Einav Yogev-Ofer", "authors": "Einav Yogev-Ofer, Tom Tirer, Raja Giryes", "title": "An Interpretation of Regularization by Denoising and its Application\n  with the Back-Projected Fidelity Term", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of image recovery tasks are ill-posed problems. As such,\nmethods that are based on optimization use cost functions that consist of both\nfidelity and prior (regularization) terms. A recent line of works imposes the\nprior by the Regularization by Denoising (RED) approach, which exploits the\ngood performance of existing image denoising engines. Yet, the relation of RED\nto explicit prior terms is still not well understood, as previous work requires\ntoo strong assumptions on the denoisers. In this paper, we make two\ncontributions. First, we show that the RED gradient can be seen as a\n(sub)gradient of a prior function--but taken at a denoised version of the\npoint. As RED is typically applied with a relatively small noise level, this\ninterpretation indicates a similarity between RED and traditional gradients.\nThis leads to our second contribution: We propose to combine RED with the\nBack-Projection (BP) fidelity term rather than the common Least Squares (LS)\nterm that is used in previous works. We show that the advantages of BP over LS\nfor image deblurring and super-resolution, which have been demonstrated for\ntraditional gradients, carry on to the RED approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:45:35 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yogev-Ofer", "Einav", ""], ["Tirer", "Tom", ""], ["Giryes", "Raja", ""]]}, {"id": "2101.11600", "submitter": "Yoav Alon", "authors": "Yoav Alon and Xiang Yu and Huiyu Zhou", "title": "Synthetic Generation of Three-Dimensional Cancer Cell Models from\n  Histopathological Images", "comments": "For submission in MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic generation of three-dimensional cell models from histopathological\nimages aims to enhance understanding of cell mutation, and progression of\ncancer, necessary for clinical assessment and optimal treatment. Classical\nreconstruction algorithms based on image registration of consecutive slides of\nstained tissues are prone to errors and often not suitable for the training of\nthree-dimensional segmentation algorithms. We propose a novel framework to\ngenerate synthetic three-dimensional histological models based on a\ngenerator-discriminator pattern optimizing constrained features that construct\na 3D model via a Blender interface exploiting smooth shape continuity typical\nfor biological specimens. To capture the spatial context of entire cell\nclusters we deploy a novel deep topology transformer that implements and\nattention mechanism on cell group images to extract features for the frozen\nfeature decoder. The proposed algorithms achieves high quantitative and\nqualitative synthesis evident in comparative evaluation metrics such as a low\nFrechet-Inception scores.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:00:00 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 07:22:31 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Alon", "Yoav", ""], ["Yu", "Xiang", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2101.11604", "submitter": "Md Amirul Islam", "authors": "Md Amirul Islam, Matthew Kowal, Patrick Esser, Sen Jia, Bjorn Ommer,\n  Konstantinos G. Derpanis, Neil Bruce", "title": "Shape or Texture: Understanding Discriminative Features in CNNs", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Contrasting the previous evidence that neurons in the later layers of a\nConvolutional Neural Network (CNN) respond to complex object shapes, recent\nstudies have shown that CNNs actually exhibit a `texture bias': given an image\nwith both texture and shape cues (e.g., a stylized image), a CNN is biased\ntowards predicting the category corresponding to the texture. However, these\nprevious studies conduct experiments on the final classification output of the\nnetwork, and fail to robustly evaluate the bias contained (i) in the latent\nrepresentations, and (ii) on a per-pixel level. In this paper, we design a\nseries of experiments that overcome these issues. We do this with the goal of\nbetter understanding what type of shape information contained in the network is\ndiscriminative, where shape information is encoded, as well as when the network\nlearns about object shape during training. We show that a network learns the\nmajority of overall shape information at the first few epochs of training and\nthat this information is largely encoded in the last few layers of a CNN.\nFinally, we show that the encoding of shape does not imply the encoding of\nlocalized per-pixel semantic information. The experimental results and findings\nprovide a more accurate understanding of the behaviour of current CNNs, thus\nhelping to inform future design choices.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:54:00 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Islam", "Md Amirul", ""], ["Kowal", "Matthew", ""], ["Esser", "Patrick", ""], ["Jia", "Sen", ""], ["Ommer", "Bjorn", ""], ["Derpanis", "Konstantinos G.", ""], ["Bruce", "Neil", ""]]}, {"id": "2101.11605", "submitter": "Aravind Srinivas Lakshminarayanan", "authors": "Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter\n  Abbeel, Ashish Vaswani", "title": "Bottleneck Transformers for Visual Recognition", "comments": "Technical Report, 20 pages, 13 figures, 19 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BoTNet, a conceptually simple yet powerful backbone architecture\nthat incorporates self-attention for multiple computer vision tasks including\nimage classification, object detection and instance segmentation. By just\nreplacing the spatial convolutions with global self-attention in the final\nthree bottleneck blocks of a ResNet and no other changes, our approach improves\nupon the baselines significantly on instance segmentation and object detection\nwhile also reducing the parameters, with minimal overhead in latency. Through\nthe design of BoTNet, we also point out how ResNet bottleneck blocks with\nself-attention can be viewed as Transformer blocks. Without any bells and\nwhistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance\nSegmentation benchmark using the Mask R-CNN framework; surpassing the previous\nbest published single model and single scale results of ResNeSt evaluated on\nthe COCO validation set. Finally, we present a simple adaptation of the BoTNet\ndesign for image classification, resulting in models that achieve a strong\nperformance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to\n2.33x faster in compute time than the popular EfficientNet models on TPU-v3\nhardware. We hope our simple and effective approach will serve as a strong\nbaseline for future research in self-attention models for vision.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:55:27 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Srinivas", "Aravind", ""], ["Lin", "Tsung-Yi", ""], ["Parmar", "Niki", ""], ["Shlens", "Jonathon", ""], ["Abbeel", "Pieter", ""], ["Vaswani", "Ashish", ""]]}, {"id": "2101.11606", "submitter": "Akshita Gupta", "authors": "Akshita Gupta, Sanath Narayan, Salman Khan, Fahad Shahbaz Khan, Ling\n  Shao, Joost van de Weijer", "title": "Generative Multi-Label Zero-Shot Learning", "comments": "10 pages, source code is available at\n  https://github.com/akshitac8/Generative_MLZSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-label zero-shot learning strives to classify images into multiple\nunseen categories for which no data is available during training. The test\nsamples can additionally contain seen categories in the generalized variant.\nExisting approaches rely on learning either shared or label-specific attention\nfrom the seen classes. Nevertheless, computing reliable attention maps for\nunseen classes during inference in a multi-label setting is still a challenge.\nIn contrast, state-of-the-art single-label generative adversarial network (GAN)\nbased approaches learn to directly synthesize the class-specific visual\nfeatures from the corresponding class attribute embeddings. However,\nsynthesizing multi-label features from GANs is still unexplored in the context\nof zero-shot setting. In this work, we introduce different fusion approaches at\nthe attribute-level, feature-level and cross-level (across attribute and\nfeature-levels) for synthesizing multi-label features from their corresponding\nmulti-label class embedding. To the best of our knowledge, our work is the\nfirst to tackle the problem of multi-label feature synthesis in the\n(generalized) zero-shot setting. Comprehensive experiments are performed on\nthree zero-shot image classification benchmarks: NUS-WIDE, Open Images and MS\nCOCO. Our cross-level fusion-based generative approach outperforms the\nstate-of-the-art on all three datasets. Furthermore, we show the generalization\ncapabilities of our fusion approach in the zero-shot detection task on MS COCO,\nachieving favorable performance against existing methods. The source code is\navailable at https://github.com/akshitac8/Generative_MLZSL.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:56:46 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 16:14:42 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Gupta", "Akshita", ""], ["Narayan", "Sanath", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2101.11654", "submitter": "Zahra Mousavi Kouzehkanan", "authors": "Zahra Mousavi Kouzehkanan, Sajad Tavakoli, Arezoo Alipanah", "title": "Easy-GT: Open-Source Software to Facilitate Making the Ground Truth for\n  White Blood Cells Nucleus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The nucleus of white blood cells (WBCs) plays a significant role in their\ndetection and classification. Appropriate feature extraction of the nucleus is\nnecessary to fit a suitable artificial intelligence model to classify WBCs.\nTherefore, designing a method is needed to segment the nucleus accurately.\nThere should be a comparison between the ground truths distinguished by a\nhematologist and the detected nuclei to evaluate the performance of the nucleus\nsegmentation method accurately. It is a time-consuming and tedious task for\nexperts to establish the ground truth manually. This paper presents an\nintelligent open-source software called Easy-GT to create the ground truth of\nWBCs' nucleus faster and easier. This software first detects the nucleus by\nemploying a new Otsu's thresholding-based method with a dice similarity\ncoefficient (DSC) of 95.42 %; the hematologist can then create a more accurate\nground truth, using the designed buttons to modify the threshold value. This\nsoftware can speed up ground truth's forming process more than six times.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 19:24:36 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 23:51:09 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 06:45:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kouzehkanan", "Zahra Mousavi", ""], ["Tavakoli", "Sajad", ""], ["Alipanah", "Arezoo", ""]]}, {"id": "2101.11674", "submitter": "Kaustubh Sadekar", "authors": "Kaustubh Sadekar, Prajwal Singh, Shanmuganathan Raman", "title": "HDIB1M -- Handwritten Document Image Binarization 1 Million Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handwritten document image binarization is a challenging task due to high\ndiversity in the content, page style, and condition of the documents. While the\ntraditional thresholding methods fail to generalize on such challenging\nscenarios, deep learning based methods can generalize well however, require a\nlarge training data. Current datasets for handwritten document image\nbinarization are limited in size and fail to represent several challenging\nreal-world scenarios. To solve this problem, we propose HDIB1M - a handwritten\ndocument image binarization dataset of 1M images. We also present a novel\nmethod used to generate this dataset. To show the effectiveness of our dataset\nwe train a deep learning model UNetED on our dataset and evaluate its\nperformance on other publicly available datasets. The dataset and the code will\nbe made available to the community.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 20:14:14 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 05:44:30 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Sadekar", "Kaustubh", ""], ["Singh", "Prajwal", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2101.11685", "submitter": "Rasul Karimov", "authors": "Rasul Karimov, Yury Malkov, Karim Iskakov, Victor Lempitsky", "title": "CNN with large memory layers", "comments": "Master's dissertation paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work is centred around the recently proposed product key memory\nstructure \\cite{large_memory}, implemented for a number of computer vision\napplications. The memory structure can be regarded as a simple computation\nprimitive suitable to be augmented to nearly all neural network architectures.\nThe memory block allows implementing sparse access to memory with square root\ncomplexity scaling with respect to the memory capacity. The latter scaling is\npossible due to the incorporation of Cartesian product space decomposition of\nthe key space for the nearest neighbour search. We have tested the memory layer\non the classification, image reconstruction and relocalization problems and\nfound that for some of those, the memory layers can provide significant\nspeed/accuracy improvement with the high utilization of the key-value elements,\nwhile others require more careful fine-tuning and suffer from dying keys. To\ntackle the later problem we have introduced a simple technique of memory\nre-initialization which helps us to eliminate unused key-value pairs from the\nmemory and engage them in training again. We have conducted various experiments\nand got improvements in speed and accuracy for classification and PoseNet\nrelocalization models.\n  We showed that the re-initialization has a huge impact on a toy example of\nrandomly labeled data and observed some gains in performance on the image\nclassification task. We have also demonstrated the generalization property\nperseverance of the large memory layers on the relocalization problem, while\nobserving the spatial correlations between the images and the selected memory\ncells.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 20:58:20 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 09:42:58 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Karimov", "Rasul", ""], ["Malkov", "Yury", ""], ["Iskakov", "Karim", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2101.11700", "submitter": "Suiyi Ling", "authors": "Zhenyu Lei, Yejing Xie, Suiyi Ling, Andreas Pastor, Junle Wang,\n  Patrick Le Callet", "title": "Multi-Modal Aesthetic Assessment for MObile Gaming Image", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of various gaming technology, services, game styles,\nand platforms, multi-dimensional aesthetic assessment of the gaming contents is\nbecoming more and more important for the gaming industry. Depending on the\ndiverse needs of diversified game players, game designers, graphical\ndevelopers, etc. in particular conditions, multi-modal aesthetic assessment is\nrequired to consider different aesthetic dimensions/perspectives. Since there\nare different underlying relationships between different aesthetic dimensions,\ne.g., between the `Colorfulness' and `Color Harmony', it could be advantageous\nto leverage effective information attached in multiple relevant dimensions. To\nthis end, we solve this problem via multi-task learning. Our inclination is to\nseek and learn the correlations between different aesthetic relevant dimensions\nto further boost the generalization performance in predicting all the aesthetic\ndimensions. Therefore, the `bottleneck' of obtaining good predictions with\nlimited labeled data for one individual dimension could be unplugged by\nharnessing complementary sources of other dimensions, i.e., augment the\ntraining data indirectly by sharing training information across dimensions.\nAccording to experimental results, the proposed model outperforms\nstate-of-the-art aesthetic metrics significantly in predicting four gaming\naesthetic dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 21:48:31 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Lei", "Zhenyu", ""], ["Xie", "Yejing", ""], ["Ling", "Suiyi", ""], ["Pastor", "Andreas", ""], ["Wang", "Junle", ""], ["Callet", "Patrick Le", ""]]}, {"id": "2101.11709", "submitter": "J. P. J. Chen", "authors": "J. P. J. Chen, K. E. Schmidt, J. C. H. Spence, R. A. Kirian", "title": "A new solution to the curved Ewald sphere problem for 3D image\n  reconstruction in electron microscopy", "comments": null, "journal-ref": null, "doi": "10.1016/j.ultramic.2021.113234", "report-no": null, "categories": "physics.ins-det cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm capable of imaging a three-dimensional object given a\ncollection of two-dimensional images of that object that are significantly\ninfluenced by the curvature of the Ewald sphere. These two-dimensional images\ncannot be approximated as projections of the object. Such an algorithm is\nuseful in cryo-electron microscopy where larger samples, higher resolution, or\nlower energy electron beams are desired, all of which contribute to the\nsignificance of Ewald curvature.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 21:27:14 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 11:35:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chen", "J. P. J.", ""], ["Schmidt", "K. E.", ""], ["Spence", "J. C. H.", ""], ["Kirian", "R. A.", ""]]}, {"id": "2101.11731", "submitter": "Eric Cosatto", "authors": "Eric Cosatto, Kyle Gerard, Hans-Peter Graf, Maki Ogura, Tomoharu\n  Kiyuna, Kanako C. Hatanaka, Yoshihiro Matsuno, Yutaka Hatanaka", "title": "A Multi-Scale Conditional Deep Model for Tumor Cell Ratio Counting", "comments": "To be published in SPIE Medical Imaging 2021 online proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a method to accurately obtain the ratio of tumor cells over an\nentire histological slide. We use deep fully convolutional neural network\nmodels trained to detect and classify cells on images of H&E-stained tissue\nsections. Pathologists' labels consisting of exhaustive nuclei locations and\ntumor regions were used to trained the model in a supervised fashion. We show\nthat combining two models, each working at a different magnification allows the\nsystem to capture both cell-level details and surrounding context to enable\nsuccessful detection and classification of cells as either tumor-cell or\nnormal-cell. Indeed, by conditioning the classification of a single cell on a\nmulti-scale context information, our models mimic the process used by\npathologists who assess cell neoplasticity and tumor extent at different\nmicroscope magnifications. The ratio of tumor cells can then be readily\nobtained by counting the number of cells in each class. To analyze an entire\nslide, we split it into multiple tiles that can be processed in parallel. The\noverall tumor cell ratio can then be aggregated. We perform experiments on a\ndataset of 100 slides with lung tumor specimens from both resection and tissue\nmicro-array (TMA). We train fully-convolutional models using heavy data\naugmentation and batch normalization. On an unseen test set, we obtain an\naverage mean absolute error on predicting the tumor cell ratio of less than 6%,\nwhich is significantly better than the human average of 20% and is key in\nproperly selecting tissue samples for recent genetic panel tests geared at\nprescribing targeted cancer drugs. We perform ablation studies to show the\nimportance of training two models at different magnifications and to justify\nthe choice of some parameters, such as the size of the receptive field.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 22:40:33 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Cosatto", "Eric", ""], ["Gerard", "Kyle", ""], ["Graf", "Hans-Peter", ""], ["Ogura", "Maki", ""], ["Kiyuna", "Tomoharu", ""], ["Hatanaka", "Kanako C.", ""], ["Matsuno", "Yoshihiro", ""], ["Hatanaka", "Yutaka", ""]]}, {"id": "2101.11742", "submitter": "Pall Bjornsson", "authors": "P.A. Bjornsson, B. Helgason, H. Palsson, S. Sigurdsson, V. Gudnason,\n  L.M. Ellingsen", "title": "Automated femur segmentation from computed tomography images using a\n  deep neural network", "comments": null, "journal-ref": null, "doi": "10.1117/12.2581100", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Osteoporosis is a common bone disease that occurs when the creation of new\nbone does not keep up with the loss of old bone, resulting in increased\nfracture risk. Adults over the age of 50 are especially at risk and see their\nquality of life diminished because of limited mobility, which can lead to\nisolation and depression. We are developing a robust screening method capable\nof identifying individuals predisposed to hip fracture to address this clinical\nchallenge. The method uses finite element analysis and relies on segmented\ncomputed tomography (CT) images of the hip. Presently, the segmentation of the\nproximal femur requires manual input, which is a tedious task, prone to human\nerror, and severely limits the practicality of the method in a clinical\ncontext. Here we present a novel approach for segmenting the proximal femur\nthat uses a deep convolutional neural network to produce accurate, automated,\nrobust, and fast segmentations of the femur from CT scans. The network\narchitecture is based on the renowned u-net, which consists of a downsampling\npath to extract increasingly complex features of the input patch and an\nupsampling path to convert the acquired low resolution image into a high\nresolution one. Skipped connections allow us to recover critical spatial\ninformation lost during downsampling. The model was trained on 30 manually\nsegmented CT images and was evaluated on 200 ground truth manual segmentations.\nOur method delivers a mean Dice similarity coefficient (DSC) and 95th\npercentile Hausdorff distance (HD95) of 0.990 and 0.981 mm, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 23:37:56 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Bjornsson", "P. A.", ""], ["Helgason", "B.", ""], ["Palsson", "H.", ""], ["Sigurdsson", "S.", ""], ["Gudnason", "V.", ""], ["Ellingsen", "L. M.", ""]]}, {"id": "2101.11745", "submitter": "Jorge Cipri\\'an S\\'anchez", "authors": "J. F. Cipri\\'an-S\\'anchez and G. Ochoa-Ruiz and M. Gonzalez-Mendoza\n  and L. Rossi", "title": "FIRe-GAN: A novel Deep Learning-based infrared-visible fusion method for\n  wildfire imagery", "comments": "16 pages, 10 figures. Submitted to the Special Issue (SI) in the\n  Neural Computing and Applications Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early wildfire detection is of paramount importance to avoid as much damage\nas possible to the environment, properties, and lives. Deep Learning (DL)\nmodels that can leverage both visible and infrared information have the\npotential to display state-of-the-art performance, with lower false-positive\nrates than existing techniques. However, most DL-based image fusion methods\nhave not been evaluated in the domain of fire imagery. Additionally, to the\nbest of our knowledge, no publicly available dataset contains visible-infrared\nfused fire images. There is a growing interest in DL-based image fusion\ntechniques due to their reduced complexity. Due to the latter, we select three\nstate-of-the-art, DL-based image fusion techniques and evaluate them for the\nspecific task of fire image fusion. We compare the performance of these methods\non selected metrics. Finally, we also present an extension to one of the said\nmethods, that we called FIRe-GAN, that improves the generation of artificial\ninfrared images and fused ones on selected metrics.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 23:53:36 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 17:07:41 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Cipri\u00e1n-S\u00e1nchez", "J. F.", ""], ["Ochoa-Ruiz", "G.", ""], ["Gonzalez-Mendoza", "M.", ""], ["Rossi", "L.", ""]]}, {"id": "2101.11782", "submitter": "Chunhua Shen", "authors": "Qiang Zhou and Chaohui Yu and Chunhua Shen and Zhibin Wang and Hao Li", "title": "Object Detection Made Simpler by Eliminating Heuristic NMS", "comments": "11 pages. Code is available at: https://git.io/PSS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We show a simple NMS-free, end-to-end object detection framework, of which\nthe network is a minimal modification to a one-stage object detector such as\nthe FCOS detection model [Tian et al. 2019]. We attain on par or even improved\ndetection accuracy compared with the original one-stage detector. It performs\ndetection at almost the same inference speed, while being even simpler in that\nnow the post-processing NMS (non-maximum suppression) is eliminated during\ninference. If the network is capable of identifying only one positive sample\nfor prediction for each ground-truth object instance in an image, then NMS\nwould become unnecessary. This is made possible by attaching a compact PSS head\nfor automatic selection of the single positive sample for each instance (see\nFig. 1). As the learning objective involves both one-to-many and one-to-one\nlabel assignments, there is a conflict in the labels of some training examples,\nmaking the learning challenging. We show that by employing a stop-gradient\noperation, we can successfully tackle this issue and train the detector. On the\nCOCO dataset, our simple design achieves superior performance compared to both\nthe FCOS baseline detector with NMS post-processing and the recent end-to-end\nNMS-free detectors. Our extensive ablation studies justify the rationale of the\ndesign choices.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 02:38:29 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 23:25:55 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zhou", "Qiang", ""], ["Yu", "Chaohui", ""], ["Shen", "Chunhua", ""], ["Wang", "Zhibin", ""], ["Li", "Hao", ""]]}, {"id": "2101.11789", "submitter": "Xiaopei Wan", "authors": "Xiaopei Wan, Zhenhua Guo, Chao He, Yujiu Yang, Fangbo Tao", "title": "Augmenting Proposals by the Detector Itself", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lacking enough high quality proposals for RoI box head has impeded two-stage\nand multi-stage object detectors for a long time, and many previous works try\nto solve it via improving RPN's performance or manually generating proposals\nfrom ground truth. However, these methods either need huge training and\ninference costs or bring little improvements. In this paper, we design a novel\ntraining method named APDI, which means augmenting proposals by the detector\nitself and can generate proposals with higher quality. Furthermore, APDI makes\nit possible to integrate IoU head into RoI box head. And it does not add any\nhyperparameter, which is beneficial for future research and downstream tasks.\nExtensive experiments on COCO dataset show that our method brings at least 2.7\nAP improvements on Faster R-CNN with various backbones, and APDI can cooperate\nwith advanced RPNs, such as GA-RPN and Cascade RPN, to obtain extra gains.\nFurthermore, it brings significant improvements on Cascade R-CNN.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 02:48:00 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wan", "Xiaopei", ""], ["Guo", "Zhenhua", ""], ["He", "Chao", ""], ["Yang", "Yujiu", ""], ["Tao", "Fangbo", ""]]}, {"id": "2101.11796", "submitter": "Tsu-Jui Fu", "authors": "Tsu-Jui Fu, William Yang Wang, Daniel McDuff, Yale Song", "title": "DOC2PPT: Automatic Presentation Slides Generation from Scientific\n  Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating presentation materials requires complex multimodal reasoning skills\nto summarize key concepts and arrange them in a logical and visually pleasing\nmanner. Can machines learn to emulate this laborious process? We present a\nnovel task and approach for document-to-slide generation. Solving this involves\ndocument summarization, image and text retrieval, slide structure and layout\nprediction to arrange key elements in a form suitable for presentation. We\npropose a hierarchical sequence-to-sequence approach to tackle our task in an\nend-to-end manner. Our approach exploits the inherent structures within\ndocuments and slides and incorporates paraphrasing and layout prediction\nmodules to generate slides. To help accelerate research in this domain, we\nrelease a dataset about 6K paired documents and slide decks used in our\nexperiments. We show that our approach outperforms strong baselines and\nproduces slides with rich content and aligned imagery.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 03:21:17 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 05:41:08 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Fu", "Tsu-Jui", ""], ["Wang", "William Yang", ""], ["McDuff", "Daniel", ""], ["Song", "Yale", ""]]}, {"id": "2101.11805", "submitter": "Ningtao Liu", "authors": "Ningtao Liu", "title": "Chronological age estimation of lateral cephalometric radiographs with\n  deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional manual age estimation method is crucial labor based on many\nkinds of the X-Ray image. Some current studies have shown that lateral\ncephalometric(LC) images can be used to estimate age. However, these methods\nare based on manually measuring some image features and making age estimates\nbased on experience or scoring. Therefore, these methods are time-consuming and\nlabor-intensive, and the effect will be affected by subjective opinions. In\nthis work, we propose a saliency map-enhanced age estimation method, which can\nautomatically perform age estimation based on LC images. Meanwhile, it can also\nshow the importance of each region in the image for age estimation, which\nundoubtedly increases the method's Interpretability. Our method was tested on\n3014 LC images from 4 to 40 years old. The MEA of the experimental result is\n1.250, which is less than the result of the state-of-the-art benchmark because\nit performs significantly better in the age group with fewer data. Besides, our\nmodel is trained in each area with a high contribution to age estimation in LC\nimages, so the effect of these different areas on the age estimation task was\nverified. Consequently, we conclude that the proposed saliency map enhancements\nchronological age estimation method of lateral cephalometric radiographs can\nwork well in chronological age estimation task, especially when the amount of\ndata is small. Besides, compared with traditional deep learning, our method is\nalso interpretable.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 03:43:24 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Liu", "Ningtao", ""]]}, {"id": "2101.11812", "submitter": "Shaoxiong Wang", "authors": "Chen Wang, Shaoxiong Wang, Branden Romero, Filipe Veiga, Edward\n  Adelson", "title": "SwingBot: Learning Physical Features from In-hand Tactile Exploration\n  for Dynamic Swing-up Manipulation", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several robot manipulation tasks are extremely sensitive to variations of the\nphysical properties of the manipulated objects. One such task is manipulating\nobjects by using gravity or arm accelerations, increasing the importance of\nmass, center of mass, and friction information. We present SwingBot, a robot\nthat is able to learn the physical features of a held object through tactile\nexploration. Two exploration actions (tilting and shaking) provide the tactile\ninformation used to create a physical feature embedding space. With this\nembedding, SwingBot is able to predict the swing angle achieved by a robot\nperforming dynamic swing-up manipulations on a previously unseen object. Using\nthese predictions, it is able to search for the optimal control parameters for\na desired swing-up angle. We show that with the learned physical features our\nend-to-end self-supervised learning pipeline is able to substantially improve\nthe accuracy of swinging up unseen objects. We also show that objects with\nsimilar dynamics are closer to each other on the embedding space and that the\nembedding can be disentangled into values of specific physical properties.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 04:35:15 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wang", "Chen", ""], ["Wang", "Shaoxiong", ""], ["Romero", "Branden", ""], ["Veiga", "Filipe", ""], ["Adelson", "Edward", ""]]}, {"id": "2101.11834", "submitter": "Xuanyang Zhang", "authors": "Xuanyang Zhang, Pengfei Hou, Xiangyu Zhang, Jian Sun", "title": "Neural Architecture Search with Random Labels", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a new variant of neural architecture search\n(NAS) paradigm -- searching with random labels (RLNAS). The task sounds\ncounter-intuitive for most existing NAS algorithms since random label provides\nfew information on the performance of each candidate architecture. Instead, we\npropose a novel NAS framework based on ease-of-convergence hypothesis, which\nrequires only random labels during searching. The algorithm involves two steps:\nfirst, we train a SuperNet using random labels; second, from the SuperNet we\nextract the sub-network whose weights change most significantly during the\ntraining. Extensive experiments are evaluated on multiple datasets (e.g.\nNAS-Bench-201 and ImageNet) and multiple search spaces (e.g. DARTS-like and\nMobileNet-like). Very surprisingly, RLNAS achieves comparable or even better\nresults compared with state-of-the-art NAS methods such as PC-DARTS, Single\nPath One-Shot, even though the counterparts utilize full ground truth labels\nfor searching. We hope our finding could inspire new understandings on the\nessential of NAS.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 06:41:48 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 08:59:00 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Zhang", "Xuanyang", ""], ["Hou", "Pengfei", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "2101.11835", "submitter": "Inbar Helbitz", "authors": "Inbar Helbitz, Shai Avidan", "title": "Reducing ReLU Count for Privacy-Preserving CNN Speedup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-Preserving Machine Learning algorithms must balance classification\naccuracy with data privacy. This can be done using a combination of\ncryptographic and machine learning tools such as Convolutional Neural Networks\n(CNN). CNNs typically consist of two types of operations: a convolutional or\nlinear layer, followed by a non-linear function such as ReLU. Each of these\ntypes can be implemented efficiently using a different cryptographic tool. But\nthese tools require different representations and switching between them is\ntime-consuming and expensive. Recent research suggests that ReLU is responsible\nfor most of the communication bandwidth. ReLU is usually applied at each pixel\n(or activation) location, which is quite expensive. We propose to share ReLU\noperations. Specifically, the ReLU decision of one activation can be used by\nothers, and we explore different ways to group activations and different ways\nto determine the ReLU for such a group of activations. Experiments on several\ndatasets reveal that we can cut the number of ReLU operations by up to three\norders of magnitude and, as a result, cut the communication bandwidth by more\nthan 50%.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 06:49:31 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Helbitz", "Inbar", ""], ["Avidan", "Shai", ""]]}, {"id": "2101.11863", "submitter": "Romann Weber", "authors": "Romann M. Weber", "title": "Exploiting the Hidden Tasks of GANs: Making Implicit Subproblems\n  Explicit", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an alternative perspective on the training of generative\nadversarial networks (GANs), showing that the training step for a GAN generator\ndecomposes into two implicit subproblems. In the first, the discriminator\nprovides new target data to the generator in the form of \"inverse examples\"\nproduced by approximately inverting classifier labels. In the second, these\nexamples are used as targets to update the generator via least-squares\nregression, regardless of the main loss specified to train the network. We\nexperimentally validate our main theoretical result and demonstrate significant\nimprovements over standard GAN training made possible by making these\nsubproblems explicit.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:17:29 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 12:47:36 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 13:35:48 GMT"}, {"version": "v4", "created": "Wed, 12 May 2021 08:16:23 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Weber", "Romann M.", ""]]}, {"id": "2101.11878", "submitter": "Ju He", "authors": "Ju He, Adam Kortylewski, Alan Yuille", "title": "COMPAS: Representation Learning with Compositional Part Sharing for\n  Few-Shot Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot image classification consists of two consecutive learning processes:\n1) In the meta-learning stage, the model acquires a knowledge base from a set\nof training classes. 2) During meta-testing, the acquired knowledge is used to\nrecognize unseen classes from very few examples. Inspired by the compositional\nrepresentation of objects in humans, we train a neural network architecture\nthat explicitly represents objects as a set of parts and their spatial\ncomposition. In particular, during meta-learning, we train a knowledge base\nthat consists of a dictionary of part representations and a dictionary of part\nactivation maps that encode common spatial activation patterns of parts. The\nelements of both dictionaries are shared among the training classes. During\nmeta-testing, the representation of unseen classes is learned using the part\nrepresentations and the part activation maps from the knowledge base. Finally,\nan attention mechanism is used to strengthen those parts that are most\nimportant for each category. We demonstrate the value of our compositional\nlearning framework for a few-shot classification using miniImageNet,\ntieredImageNet, CIFAR-FS, and FC100, where we achieve state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:16:21 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 16:11:23 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["He", "Ju", ""], ["Kortylewski", "Adam", ""], ["Yuille", "Alan", ""]]}, {"id": "2101.11896", "submitter": "Jiahuan Luo", "authors": "Xinle Liang, Yang Liu, Jiahuan Luo, Yuanqin He, Tianjian Chen, Qiang\n  Yang", "title": "Self-supervised Cross-silo Federated Neural Architecture Search", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) provides both model performance and data privacy for\nmachine learning tasks where samples or features are distributed among\ndifferent parties. In the training process of FL, no party has a global view of\ndata distributions or model architectures of other parties. Thus the\nmanually-designed architectures may not be optimal. In the past, Neural\nArchitecture Search (NAS) has been applied to FL to address this critical\nissue. However, existing Federated NAS approaches require prohibitive\ncommunication and computation effort, as well as the availability of\nhigh-quality labels. In this work, we present Self-supervised Vertical\nFederated Neural Architecture Search (SS-VFNAS) for automating FL where\nparticipants hold feature-partitioned data, a common cross-silo scenario called\nVertical Federated Learning (VFL). In the proposed framework, each party first\nconducts NAS using self-supervised approach to find a local optimal\narchitecture with its own data. Then, parties collaboratively improve the local\noptimal architecture in a VFL framework with supervision. We demonstrate\nexperimentally that our approach has superior performance, communication\nefficiency and privacy compared to Federated NAS and is capable of generating\nhigh-performance and highly-transferable heterogeneous architectures even with\ninsufficient overlapping samples, providing automation for those parties\nwithout deep learning expertise.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:57:30 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 02:23:50 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Liang", "Xinle", ""], ["Liu", "Yang", ""], ["Luo", "Jiahuan", ""], ["He", "Yuanqin", ""], ["Chen", "Tianjian", ""], ["Yang", "Qiang", ""]]}, {"id": "2101.11911", "submitter": "Emanuele Bugliarello", "authors": "Emanuele Bugliarello, Desmond Elliott", "title": "The Role of Syntactic Planning in Compositional Image Captioning", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has focused on generalizing to images drawn from the same\ndistribution as the training set, and not to the more challenging problem of\ngeneralizing to different distributions of images. Recently, Nikolaus et al.\n(2019) introduced a dataset to assess compositional generalization in image\ncaptioning, where models are evaluated on their ability to describe images with\nunseen adjective-noun and noun-verb compositions. In this work, we investigate\ndifferent methods to improve compositional generalization by planning the\nsyntactic structure of a caption. Our experiments show that jointly modeling\ntokens and syntactic tags enhances generalization in both RNN- and\nTransformer-based models, while also improving performance on standard metrics.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 10:26:08 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Bugliarello", "Emanuele", ""], ["Elliott", "Desmond", ""]]}, {"id": "2101.11939", "submitter": "Wenguan Wang", "authors": "Wenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, Ender Konukoglu,\n  Luc Van Gool", "title": "Exploring Cross-Image Pixel Contrast for Semantic Segmentation", "comments": "Our code will be available at\n  https://github.com/tfzhou/ContrastiveSeg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current semantic segmentation methods focus only on mining \"local\" context,\ni.e., dependencies between pixels within individual images, by\ncontext-aggregation modules (e.g., dilated convolution, neural attention) or\nstructure-aware optimization criteria (e.g., IoU-like loss). However, they\nignore \"global\" context of the training data, i.e., rich semantic relations\nbetween pixels across different images. Inspired by the recent advance in\nunsupervised contrastive representation learning, we propose a pixel-wise\ncontrastive framework for semantic segmentation in the fully supervised\nsetting. The core idea is to enforce pixel embeddings belonging to a same\nsemantic class to be more similar than embeddings from different classes. It\nraises a pixel-wise metric learning paradigm for semantic segmentation, by\nexplicitly exploring the structures of labeled pixels, which were rarely\nexplored before. Our method can be effortlessly incorporated into existing\nsegmentation frameworks without extra overhead during testing. We\nexperimentally show that, with famous segmentation models (i.e., DeepLabV3,\nHRNet, OCR) and backbones (i.e., ResNet, HR-Net), our method brings consistent\nperformance improvements across diverse datasets (i.e., Cityscapes,\nPASCAL-Context, COCO-Stuff, CamVid). We expect this work will encourage our\ncommunity to rethink the current de facto training paradigm in fully supervised\nsemantic segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 11:35:32 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 23:41:45 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 20:35:21 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 15:16:23 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wang", "Wenguan", ""], ["Zhou", "Tianfei", ""], ["Yu", "Fisher", ""], ["Dai", "Jifeng", ""], ["Konukoglu", "Ender", ""], ["Van Gool", "Luc", ""]]}, {"id": "2101.11943", "submitter": "Simone Palazzo", "authors": "Matteo Pennisi, Isaak Kavasidis, Concetto Spampinato, Vincenzo\n  Schinin\\`a, Simone Palazzo, Francesco Rundo, Massimo Cristofaro, Paolo\n  Campioni, Elisa Pianura, Federica Di Stefano, Ada Petrone, Fabrizio\n  Albarello, Giuseppe Ippolito, Salvatore Cuzzocrea, Sabrina Conoci", "title": "An Explainable AI System for Automated COVID-19 Assessment and Lesion\n  Categorization from CT-scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 infection caused by SARS-CoV-2 pathogen is a catastrophic pandemic\noutbreak all over the world with exponential increasing of confirmed cases and,\nunfortunately, deaths. In this work we propose an AI-powered pipeline, based on\nthe deep-learning paradigm, for automated COVID-19 detection and lesion\ncategorization from CT scans. We first propose a new segmentation module aimed\nat identifying automatically lung parenchyma and lobes. Next, we combined such\nsegmentation network with classification networks for COVID-19 identification\nand lesion categorization. We compare the obtained classification results with\nthose obtained by three expert radiologists on a dataset consisting of 162 CT\nscans. Results showed a sensitivity of 90\\% and a specificity of 93.5% for\nCOVID-19 detection, outperforming those yielded by the expert radiologists, and\nan average lesion categorization accuracy of over 84%. Results also show that a\nsignificant role is played by prior lung and lobe segmentation that allowed us\nto enhance performance by over 20 percent points. The interpretation of the\ntrained AI models, moreover, reveals that the most significant areas for\nsupporting the decision on COVID-19 identification are consistent with the\nlesions clinically associated to the virus, i.e., crazy paving, consolidation\nand ground glass. This means that the artificial models are able to\ndiscriminate a positive patient from a negative one (both controls and patients\nwith interstitial pneumonia tested negative to COVID) by evaluating the\npresence of those lesions into CT scans. Finally, the AI models are integrated\ninto a user-friendly GUI to support AI explainability for radiologists, which\nis publicly available at http://perceivelab.com/covid-ai.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 11:47:35 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Pennisi", "Matteo", ""], ["Kavasidis", "Isaak", ""], ["Spampinato", "Concetto", ""], ["Schinin\u00e0", "Vincenzo", ""], ["Palazzo", "Simone", ""], ["Rundo", "Francesco", ""], ["Cristofaro", "Massimo", ""], ["Campioni", "Paolo", ""], ["Pianura", "Elisa", ""], ["Di Stefano", "Federica", ""], ["Petrone", "Ada", ""], ["Albarello", "Fabrizio", ""], ["Ippolito", "Giuseppe", ""], ["Cuzzocrea", "Salvatore", ""], ["Conoci", "Sabrina", ""]]}, {"id": "2101.11950", "submitter": "Nikolai Stulov", "authors": "Nikolay Stulov and Michael Chertkov", "title": "Neural Particle Image Velocimetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, great progress has been made in the field of optical and\nparticle-based measurement techniques for experimental analysis of fluid flows.\nParticle Image Velocimetry (PIV) technique is widely used to identify flow\nparameters from time-consecutive snapshots of particles injected into the\nfluid. The computation is performed as post-processing of the experimental data\nvia proximity measure between particles in frames of reference. However, the\npost-processing step becomes problematic as the motility and density of the\nparticles increases, since the data emerges in extreme rates and volumes.\nMoreover, existing algorithms for PIV either provide sparse estimations of the\nflow or require large computational time frame preventing from on-line use. The\ngoal of this manuscript is therefore to develop an accurate on-line algorithm\nfor estimation of the fine-grained velocity field from PIV data. As the data\nconstitutes a pair of images, we employ computer vision methods to solve the\nproblem. In this work, we introduce a convolutional neural network adapted to\nthe problem, namely Volumetric Correspondence Network (VCN) which was recently\nproposed for the end-to-end optical flow estimation in computer vision. The\nnetwork is thoroughly trained and tested on a dataset containing both synthetic\nand real flow data. Experimental results are analyzed and compared to that of\nconventional methods as well as other recently introduced methods based on\nneural networks. Our analysis indicates that the proposed approach provides\nimproved efficiency also keeping accuracy on par with other state-of-the-art\nmethods in the field. We also verify through a-posteriori tests that our newly\nconstructed VCN schemes are reproducing well physically relevant statistics of\nvelocity and velocity gradients.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:03:39 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Stulov", "Nikolay", ""], ["Chertkov", "Michael", ""]]}, {"id": "2101.11952", "submitter": "Xue Yang", "authors": "Xue Yang, Junchi Yan, Qi Ming, Wentao Wang, Xiaopeng Zhang, Qi Tian", "title": "Rethinking Rotated Object Detection with Gaussian Wasserstein Distance\n  Loss", "comments": "15 pages, 6 figures, 9 tables, accepted by ICML21, codes are\n  available at https://github.com/yangxue0827/RotationDetection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundary discontinuity and its inconsistency to the final detection metric\nhave been the bottleneck for rotating detection regression loss design. In this\npaper, we propose a novel regression loss based on Gaussian Wasserstein\ndistance as a fundamental approach to solve the problem. Specifically, the\nrotated bounding box is converted to a 2-D Gaussian distribution, which enables\nto approximate the indifferentiable rotational IoU induced loss by the Gaussian\nWasserstein distance (GWD) which can be learned efficiently by gradient\nback-propagation. GWD can still be informative for learning even there is no\noverlapping between two rotating bounding boxes which is often the case for\nsmall object detection. Thanks to its three unique properties, GWD can also\nelegantly solve the boundary discontinuity and square-like problem regardless\nhow the bounding box is defined. Experiments on five datasets using different\ndetectors show the effectiveness of our approach. Codes are available at\nhttps://github.com/yangxue0827/RotationDetection.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:04:35 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 10:30:13 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yang", "Xue", ""], ["Yan", "Junchi", ""], ["Ming", "Qi", ""], ["Wang", "Wentao", ""], ["Zhang", "Xiaopeng", ""], ["Tian", "Qi", ""]]}, {"id": "2101.11986", "submitter": "Li Yuan", "authors": "Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang,\n  Francis EH Tay, Jiashi Feng, Shuicheng Yan", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on\n  ImageNet", "comments": "codes: https://github.com/yitu-opensource/T2T-ViT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers, which are popular for language modeling, have been explored for\nsolving vision tasks recently, \\eg, the Vision Transformer (ViT) for image\nclassification. The ViT model splits each image into a sequence of tokens with\nfixed length and then applies multiple Transformer layers to model their global\nrelation for classification. However, ViT achieves inferior performance to CNNs\nwhen trained from scratch on a midsize dataset like ImageNet. We find it is\nbecause: 1) the simple tokenization of input images fails to model the\nimportant local structure such as edges and lines among neighboring pixels,\nleading to low training sample efficiency; 2) the redundant attention backbone\ndesign of ViT leads to limited feature richness for fixed computation budgets\nand limited training samples. To overcome such limitations, we propose a new\nTokens-To-Token Vision Transformer (T2T-ViT), which incorporates 1) a\nlayer-wise Tokens-to-Token (T2T) transformation to progressively structurize\nthe image to tokens by recursively aggregating neighboring Tokens into one\nToken (Tokens-to-Token), such that local structure represented by surrounding\ntokens can be modeled and tokens length can be reduced; 2) an efficient\nbackbone with a deep-narrow structure for vision transformer motivated by CNN\narchitecture design after empirical study. Notably, T2T-ViT reduces the\nparameter count and MACs of vanilla ViT by half, while achieving more than\n3.0\\% improvement when trained from scratch on ImageNet. It also outperforms\nResNets and achieves comparable performance with MobileNets by directly\ntraining on ImageNet. For example, T2T-ViT with comparable size to ResNet50\n(21.5M parameters) can achieve 83.3\\% top1 accuracy in image resolution\n384$\\times$384 on ImageNet. (Code: https://github.com/yitu-opensource/T2T-ViT)\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 13:25:28 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 11:58:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yuan", "Li", ""], ["Chen", "Yunpeng", ""], ["Wang", "Tao", ""], ["Yu", "Weihao", ""], ["Shi", "Yujun", ""], ["Jiang", "Zihang", ""], ["Tay", "Francis EH", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2101.11987", "submitter": "Shankar Gangisetty", "authors": "Sindhu Hegde and Shankar Gangisetty", "title": "PIG-Net: Inception based Deep Learning Architecture for 3D Point Cloud\n  Segmentation", "comments": "11 pages, 5 Figures, 6 Tables, Accepted in Computers & Graphics\n  Journal 2021", "journal-ref": null, "doi": "10.1016/j.cag.2021.01.004", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Point clouds, being the simple and compact representation of surface geometry\nof 3D objects, have gained increasing popularity with the evolution of deep\nlearning networks for classification and segmentation tasks. Unlike human,\nteaching the machine to analyze the segments of an object is a challenging task\nand quite essential in various machine vision applications. In this paper, we\naddress the problem of segmentation and labelling of the 3D point clouds by\nproposing a inception based deep network architecture called PIG-Net, that\neffectively characterizes the local and global geometric details of the point\nclouds. In PIG-Net, the local features are extracted from the transformed input\npoints using the proposed inception layers and then aligned by feature\ntransform. These local features are aggregated using the global average pooling\nlayer to obtain the global features. Finally, feed the concatenated local and\nglobal features to the convolution layers for segmenting the 3D point clouds.\nWe perform an exhaustive experimental analysis of the PIG-Net architecture on\ntwo state-of-the-art datasets, namely, ShapeNet [1] and PartNet [2]. We\nevaluate the effectiveness of our network by performing ablation study.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 13:27:55 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Hegde", "Sindhu", ""], ["Gangisetty", "Shankar", ""]]}, {"id": "2101.12010", "submitter": "Chengqiao Lin", "authors": "Wei Zeng, Chengqiao Lin, Kang Liu, Juncong Lin, Anthony K. H. Tung", "title": "Modeling Spatial Nonstationarity via Deformable Convolutions for Deep\n  Traffic Flow Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are being increasingly used for short-term traffic flow\nprediction. Existing convolution-based approaches typically partition an\nunderlying territory into grid-like spatial units, and employ standard\nconvolutions to learn spatial dependence among the units. However, standard\nconvolutions with fixed geometric structures cannot fully model the\nnonstationary characteristics of local traffic flows. To overcome the\ndeficiency, we introduce deformable convolution that augments the spatial\nsampling locations with additional offsets, to enhance the modeling capability\nof spatial nonstationarity. On this basis, we design a deep deformable\nconvolutional residual network, namely DeFlow-Net, that can effectively model\nglobal spatial dependence, local spatial nonstationarity, and temporal\nperiodicity of traffic flows. Furthermore, to fit better with convolutions, we\nsuggest to first aggregate traffic flows according to pre-conceived regions of\ninterest, then dispose to sequentially organized raster images for network\ninput. Extensive experiments on real-world traffic flows demonstrate that\nDeFlow-Net outperforms existing solutions using standard convolutions, and\nspatial partition by pre-conceived regions further enhances the performance.\nFinally, we demonstrate the advantage of DeFlow-Net in maintaining spatial\nautocorrelation, and reveal the impacts of partition shapes and scales on deep\ntraffic flow prediction.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 10:16:03 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zeng", "Wei", ""], ["Lin", "Chengqiao", ""], ["Liu", "Kang", ""], ["Lin", "Juncong", ""], ["Tung", "Anthony K. H.", ""]]}, {"id": "2101.12041", "submitter": "Amitojdeep Singh", "authors": "Amitojdeep Singh, Sourya Sengupta, Mohammed Abdul Rasheed,\n  Varadharajan Jayakumar, and Vasudevan Lakshminarayanan", "title": "Uncertainty aware and explainable diagnosis of retinal disease", "comments": "Submitted to SPIE Medical Imaging 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning methods for ophthalmic diagnosis have shown considerable\nsuccess in tasks like segmentation and classification. However, their\nwidespread application is limited due to the models being opaque and vulnerable\nto making a wrong decision in complicated cases. Explainability methods show\nthe features that a system used to make prediction while uncertainty awareness\nis the ability of a system to highlight when it is not sure about the decision.\nThis is one of the first studies using uncertainty and explanations for\ninformed clinical decision making. We perform uncertainty analysis of a deep\nlearning model for diagnosis of four retinal diseases - age-related macular\ndegeneration (AMD), central serous retinopathy (CSR), diabetic retinopathy\n(DR), and macular hole (MH) using images from a publicly available (OCTID)\ndataset. Monte Carlo (MC) dropout is used at the test time to generate a\ndistribution of parameters and the predictions approximate the predictive\nposterior of a Bayesian model. A threshold is computed using the distribution\nand uncertain cases can be referred to the ophthalmologist thus avoiding an\nerroneous diagnosis. The features learned by the model are visualized using a\nproven attribution method from a previous study. The effects of uncertainty on\nmodel performance and the relationship between uncertainty and explainability\nare discussed in terms of clinical significance. The uncertainty information\nalong with the heatmaps make the system more trustworthy for use in clinical\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 23:37:30 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Singh", "Amitojdeep", ""], ["Sengupta", "Sourya", ""], ["Rasheed", "Mohammed Abdul", ""], ["Jayakumar", "Varadharajan", ""], ["Lakshminarayanan", "Vasudevan", ""]]}, {"id": "2101.12050", "submitter": "Yizhou Zhou", "authors": "Yizhou Zhou, Chong Luo, Xiaoyan Sun, Zheng-Jun Zha and Wenjun Zeng", "title": "VAE^2: Preventing Posterior Collapse of Variational Video Predictions in\n  the Wild", "comments": "17 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting future frames of video sequences is challenging due to the complex\nand stochastic nature of the problem. Video prediction methods based on\nvariational auto-encoders (VAEs) have been a great success, but they require\nthe training data to contain multiple possible futures for an observed video\nsequence. This is hard to be fulfilled when videos are captured in the wild\nwhere any given observation only has a determinate future. As a result,\ntraining a vanilla VAE model with these videos inevitably causes posterior\ncollapse. To alleviate this problem, we propose a novel VAE structure, dabbed\nVAE-in-VAE or VAE$^2$. The key idea is to explicitly introduce stochasticity\ninto the VAE. We treat part of the observed video sequence as a random\ntransition state that bridges its past and future, and maximize the likelihood\nof a Markov Chain over the video sequence under all possible transition states.\nA tractable lower bound is proposed for this intractable objective function and\nan end-to-end optimization algorithm is designed accordingly. VAE$^2$ can\nmitigate the posterior collapse problem to a large extent, as it breaks the\ndirect dependence between future and observation and does not directly regress\nthe determinate future provided by the training data. We carry out experiments\non a large-scale dataset called Cityscapes, which contains videos collected\nfrom a number of urban cities. Results show that VAE$^2$ is capable of\npredicting diverse futures and is more resistant to posterior collapse than the\nother state-of-the-art VAE-based approaches. We believe that VAE$^2$ is also\napplicable to other stochastic sequence prediction problems where training data\nare lack of stochasticity.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 15:06:08 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhou", "Yizhou", ""], ["Luo", "Chong", ""], ["Sun", "Xiaoyan", ""], ["Zha", "Zheng-Jun", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2101.12059", "submitter": "Xudong Lin", "authors": "Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh,\n  Lorenzo Torresani", "title": "VX2TEXT: End-to-End Learning of Video-Based Text Generation From\n  Multimodal Inputs", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \\textsc{Vx2Text}, a framework for text generation from multimodal\ninputs consisting of video plus text, speech, or audio. In order to leverage\ntransformer networks, which have been shown to be effective at modeling\nlanguage, each modality is first converted into a set of language embeddings by\na learnable tokenizer. This allows our approach to perform multimodal fusion in\nthe language space, thus eliminating the need for ad-hoc cross-modal fusion\nmodules. To address the non-differentiability of tokenization on continuous\ninputs (e.g., video or audio), we utilize a relaxation scheme that enables\nend-to-end training. Furthermore, unlike prior encoder-only models, our network\nincludes an autoregressive decoder to generate open-ended text from the\nmultimodal embeddings fused by the language encoder. This renders our approach\nfully generative and makes it directly applicable to different \"video+$x$ to\ntext\" problems without the need to design specialized network heads for each\ntask. The proposed framework is not only conceptually simple but also\nremarkably effective: experiments demonstrate that our approach based on a\nsingle architecture outperforms the state-of-the-art on three video-based\ntext-generation tasks -- captioning, question answering and audio-visual\nscene-aware dialog.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 15:22:36 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 23:21:48 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lin", "Xudong", ""], ["Bertasius", "Gedas", ""], ["Wang", "Jue", ""], ["Chang", "Shih-Fu", ""], ["Parikh", "Devi", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "2101.12081", "submitter": "Alessia Bertugli", "authors": "Alessia Bertugli, Stefano Vincenzi, Simone Calderara, Andrea Passerini", "title": "Generalising via Meta-Examples for Continual Learning in the Wild", "comments": "16 pages, 11 figures, 13 tables. arXiv admin note: substantial text\n  overlap with arXiv:2009.08107", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning quickly and continually is still an ambitious task for neural\nnetworks. Indeed, many real-world applications do not reflect the learning\nsetting where neural networks shine, as data are usually few, mostly unlabelled\nand come as a stream. To narrow this gap, we introduce FUSION - Few-shot\nUnSupervIsed cONtinual learning - a novel strategy which aims to deal with\nneural networks that \"learn in the wild\", simulating a real distribution and\nflow of unbalanced tasks. We equip FUSION with MEML - Meta-Example\nMeta-Learning - a new module that simultaneously alleviates catastrophic\nforgetting and favours the generalisation and future learning of new tasks. To\nencourage features reuse during the meta-optimisation, our model exploits a\nsingle inner loop per task, taking advantage of an aggregated representation\nachieved through the use of a self-attention mechanism. To further enhance the\ngeneralisation capability of MEML, we extend it by adopting a technique that\ncreates various augmented tasks and optimises over the hardest. Experimental\nresults on few-shot learning benchmarks show that our model exceeds the other\nbaselines in both FUSION and fully supervised case. We also explore how it\nbehaves in standard continual learning consistently outperforming\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 15:51:54 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Bertugli", "Alessia", ""], ["Vincenzi", "Stefano", ""], ["Calderara", "Simone", ""], ["Passerini", "Andrea", ""]]}, {"id": "2101.12085", "submitter": "Bogdan Savchynskyy", "authors": "Lisa Hutschenreiter, Stefan Haller, Lorenz Feineis, Carsten Rother,\n  Dagmar Kainm\\\"uller, Bogdan Savchynskyy", "title": "Fusion Moves for Graph Matching", "comments": "179 pages (including appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute to approximate algorithms for the quadratic assignment problem\nalso known as graph matching. Inspired by the success of the fusion moves\ntechnique developed for multilabel discrete Markov random fields, we\ninvestigate its applicability to graph matching. In particular, we show how\nfusion moves can be efficiently combined with the dedicated state-of-the-art\ndual methods that have recently shown superior results in computer vision and\nbio-imaging applications. As our empirical evaluation on a wide variety of\ngraph matching datasets suggests, fusion moves significantly improve\nperformance of these methods in terms of speed and quality of the obtained\nsolutions. Our method sets a new state-of-the-art with a notable margin w.r.t.\nits competitors.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:09:46 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 17:38:34 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 16:27:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hutschenreiter", "Lisa", ""], ["Haller", "Stefan", ""], ["Feineis", "Lorenz", ""], ["Rother", "Carsten", ""], ["Kainm\u00fcller", "Dagmar", ""], ["Savchynskyy", "Bogdan", ""]]}, {"id": "2101.12102", "submitter": "Samuel Rivera", "authors": "Deborah Weeks and Samuel Rivera", "title": "Domain Adaptation by Topology Regularization", "comments": null, "journal-ref": "SPIE Defense + Commercial Sensing, 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become the leading approach to assisted target recognition.\nWhile these methods typically require large amounts of labeled training data,\ndomain adaptation (DA) or transfer learning (TL) enables these algorithms to\ntransfer knowledge from a labelled (source) data set to an unlabelled but\nrelated (target) data set of interest. DA enables networks to overcome the\ndistribution mismatch between the source and target that leads to poor\ngeneralization in the target domain. DA techniques align these distributions by\nminimizing a divergence measurement between source and target, making the\ntransfer of knowledge from source to target possible. While these algorithms\nhave advanced significantly in recent years, most do not explicitly leverage\nglobal data manifold structure in aligning the source and target. We propose to\nleverage global data structure by applying a topological data analysis (TDA)\ntechnique called persistent homology to TL.\n  In this paper, we examine the use of persistent homology in a domain\nadversarial (DAd) convolutional neural network (CNN) architecture. The\nexperiments show that aligning persistence alone is insufficient for transfer,\nbut must be considered along with the lifetimes of the topological\nsingularities. In addition, we found that longer lifetimes indicate robust\ndiscriminative features and more favorable structure in data. We found that\nexisting divergence minimization based approaches to DA improve the topological\nstructure, as indicated over a baseline without these regularization\ntechniques. We hope these experiments highlight how topological structure can\nbe leveraged to boost performance in TL tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:45:41 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Weeks", "Deborah", ""], ["Rivera", "Samuel", ""]]}, {"id": "2101.12136", "submitter": "Ghada Sokar", "authors": "Ghada Sokar, Decebal Constantin Mocanu, Mykola Pechenizkiy", "title": "Self-Attention Meta-Learner for Continual Learning", "comments": null, "journal-ref": "20th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to provide intelligent agents capable of learning\nmultiple tasks sequentially with neural networks. One of its main challenging,\ncatastrophic forgetting, is caused by the neural networks non-optimal ability\nto learn in non-stationary distributions. In most settings of the current\napproaches, the agent starts from randomly initialized parameters and is\noptimized to master the current task regardless of the usefulness of the\nlearned representation for future tasks. Moreover, each of the future tasks\nuses all the previously learned knowledge although parts of this knowledge\nmight not be helpful for its learning. These cause interference among tasks,\nespecially when the data of previous tasks is not accessible. In this paper, we\npropose a new method, named Self-Attention Meta-Learner (SAM), which learns a\nprior knowledge for continual learning that permits learning a sequence of\ntasks, while avoiding catastrophic forgetting. SAM incorporates an attention\nmechanism that learns to select the particular relevant representation for each\nfuture task. Each task builds a specific representation branch on top of the\nselected knowledge, avoiding the interference between tasks. We evaluate the\nproposed method on the Split CIFAR-10/100 and Split MNIST benchmarks in the\ntask agnostic inference. We empirically show that we can achieve a better\nperformance than several state-of-the-art methods for continual learning by\nbuilding on the top of selected representation learned by SAM. We also show the\nrole of the meta-attention mechanism in boosting informative features\ncorresponding to the input data and identifying the correct target in the task\nagnostic inference. Finally, we demonstrate that popular existing continual\nlearning methods gain a performance boost when they adopt SAM as a starting\npoint.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 17:35:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Sokar", "Ghada", ""], ["Mocanu", "Decebal Constantin", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2101.12159", "submitter": "Chanho Kim", "authors": "Chanho Kim, Li Fuxin, Mazen Alotaibi, James M. Rehg", "title": "Discriminative Appearance Modeling with Multi-track Pooling for\n  Real-time Multi-object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-object tracking, the tracker maintains in its memory the appearance\nand motion information for each object in the scene. This memory is utilized\nfor finding matches between tracks and detections and is updated based on the\nmatching result. Many approaches model each target in isolation and lack the\nability to use all the targets in the scene to jointly update the memory. This\ncan be problematic when there are similar looking objects in the scene. In this\npaper, we solve the problem of simultaneously considering all tracks during\nmemory updating, with only a small spatial overhead, via a novel multi-track\npooling module. We additionally propose a training strategy adapted to\nmulti-track pooling which generates hard tracking episodes online. We show that\nthe combination of these innovations results in a strong discriminative\nappearance model, enabling the use of greedy data association to achieve online\ntracking performance. Our experiments demonstrate real-time, state-of-the-art\nperformance on public multi-object tracking (MOT) datasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:12:39 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Kim", "Chanho", ""], ["Fuxin", "Li", ""], ["Alotaibi", "Mazen", ""], ["Rehg", "James M.", ""]]}, {"id": "2101.12195", "submitter": "Willi Menapace", "authors": "Willi Menapace, St\\'ephane Lathuili\\`ere, Sergey Tulyakov, Aliaksandr\n  Siarohin, Elisa Ricci", "title": "Playable Video Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the unsupervised learning problem of playable video\ngeneration (PVG). In PVG, we aim at allowing a user to control the generated\nvideo by selecting a discrete action at every time step as when playing a video\ngame. The difficulty of the task lies both in learning semantically consistent\nactions and in generating realistic videos conditioned on the user input. We\npropose a novel framework for PVG that is trained in a self-supervised manner\non a large dataset of unlabelled videos. We employ an encoder-decoder\narchitecture where the predicted action labels act as bottleneck. The network\nis constrained to learn a rich action space using, as main driving loss, a\nreconstruction loss on the generated video. We demonstrate the effectiveness of\nthe proposed approach on several datasets with wide environment variety.\nFurther details, code and examples are available on our project page\nwilli-menapace.github.io/playable-video-generation-website.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:55:58 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Menapace", "Willi", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Tulyakov", "Sergey", ""], ["Siarohin", "Aliaksandr", ""], ["Ricci", "Elisa", ""]]}, {"id": "2101.12242", "submitter": "Philipp Adis", "authors": "Philipp Adis, Nicolas Horst, Mathias Wien", "title": "D3DLO: Deep 3D LiDAR Odometry", "comments": "5 pages, 4 figures, accepted at IEEE ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  LiDAR odometry (LO) describes the task of finding an alignment of subsequent\nLiDAR point clouds. This alignment can be used to estimate the motion of the\nplatform where the LiDAR sensor is mounted on. Currently, on the well-known\nKITTI Vision Benchmark Suite state-of-the-art algorithms are non-learning\napproaches. We propose a network architecture that learns LO by directly\nprocessing 3D point clouds. It is trained on the KITTI dataset in an end-to-end\nmanner without the necessity of pre-defining corresponding pairs of points. An\nevaluation on the KITTI Vision Benchmark Suite shows similar performance to a\npreviously published work, DeepCLR [1], even though our model uses only around\n3.56% of the number of network parameters thereof. Furthermore, a plane point\nextraction is applied which leads to a marginal performance decrease while\nsimultaneously reducing the input size by up to 50%.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 19:23:06 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 13:33:04 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Adis", "Philipp", ""], ["Horst", "Nicolas", ""], ["Wien", "Mathias", ""]]}, {"id": "2101.12254", "submitter": "Aysen Degerli", "authors": "Aysen Degerli, Mete Ahishali, Serkan Kiranyaz, Muhammad E. H.\n  Chowdhury, Moncef Gabbouj", "title": "Reliable COVID-19 Detection Using Chest X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) has emerged the need for computer-aided\ndiagnosis with automatic, accurate, and fast algorithms. Recent studies have\napplied Machine Learning algorithms for COVID-19 diagnosis over chest X-ray\n(CXR) images. However, the data scarcity in these studies prevents a reliable\nevaluation with the potential of overfitting and limits the performance of deep\nnetworks. Moreover, these networks can discriminate COVID-19 pneumonia usually\nfrom healthy subjects only or occasionally, from limited pneumonia types. Thus,\nthere is a need for a robust and accurate COVID-19 detector evaluated over a\nlarge CXR dataset. To address this need, in this study, we propose a reliable\nCOVID-19 detection network: ReCovNet, which can discriminate COVID-19 pneumonia\nfrom 14 different thoracic diseases and healthy subjects. To accomplish this,\nwe have compiled the largest COVID-19 CXR dataset: QaTa-COV19 with 124,616\nimages including 4603 COVID-19 samples. The proposed ReCovNet achieved a\ndetection performance with 98.57% sensitivity and 99.77% specificity.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 19:57:21 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Degerli", "Aysen", ""], ["Ahishali", "Mete", ""], ["Kiranyaz", "Serkan", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2101.12322", "submitter": "Md Amirul Islam", "authors": "Md Amirul Islam, Matthew Kowal, Sen Jia, Konstantinos G. Derpanis, and\n  Neil D. B. Bruce", "title": "Position, Padding and Predictions: A Deeper Look at Position Information\n  in CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In contrast to fully connected networks, Convolutional Neural Networks (CNNs)\nachieve efficiency by learning weights associated with local filters with a\nfinite spatial extent. An implication of this is that a filter may know what it\nis looking at, but not where it is positioned in the image. In this paper, we\nfirst test this hypothesis and reveal that a surprising degree of absolute\nposition information is encoded in commonly used CNNs. We show that zero\npadding drives CNNs to encode position information in their internal\nrepresentations, while a lack of padding precludes position encoding. This\ngives rise to deeper questions about the role of position information in CNNs:\n(i) What boundary heuristics enable optimal position encoding for downstream\ntasks?; (ii) Does position encoding affect the learning of semantic\nrepresentations?; (iii) Does position encoding always improve performance? To\nprovide answers, we perform the largest case study to date on the role that\npadding and border heuristics play in CNNs. We design novel tasks which allow\nus to quantify boundary effects as a function of the distance to the border.\nNumerous semantic objectives reveal the effect of the border on semantic\nrepresentations. Finally, we demonstrate the implications of these findings on\nmultiple real-world tasks to show that position information can both help or\nhurt performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 23:40:32 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Islam", "Md Amirul", ""], ["Kowal", "Matthew", ""], ["Jia", "Sen", ""], ["Derpanis", "Konstantinos G.", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "2101.12346", "submitter": "Jiansheng Fang", "authors": "Jiansheng Fang, Huazhu Fu, Jiang Liu", "title": "Deep Triplet Hashing Network for Case-based Medical Image Retrieval", "comments": "12 pages, 6 figures, MedIA Journal", "journal-ref": null, "doi": "10.1016/j.media.2021.101981", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hashing methods have been shown to be the most efficient approximate\nnearest neighbor search techniques for large-scale image retrieval. However,\nexisting deep hashing methods have a poor small-sample ranking performance for\ncase-based medical image retrieval. The top-ranked images in the returned query\nresults may be as a different class than the query image. This ranking problem\nis caused by classification, regions of interest (ROI), and small-sample\ninformation loss in the hashing space. To address the ranking problem, we\npropose an end-to-end framework, called Attention-based Triplet Hashing (ATH)\nnetwork, to learn low-dimensional hash codes that preserve the classification,\nROI, and small-sample information. We embed a spatial-attention module into the\nnetwork structure of our ATH to focus on ROI information. The spatial-attention\nmodule aggregates the spatial information of feature maps by utilizing\nmax-pooling, element-wise maximum, and element-wise mean operations jointly\nalong the channel axis. The triplet cross-entropy loss can help to map the\nclassification information of images and similarity between images into the\nhash codes. Extensive experiments on two case-based medical datasets\ndemonstrate that our proposed ATH can further improve the retrieval performance\ncompared to the state-of-the-art deep hashing methods and boost the ranking\nperformance for small samples. Compared to the other loss methods, the triplet\ncross-entropy loss can enhance the classification performance and hash\ncode-discriminability\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 01:35:46 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Fang", "Jiansheng", ""], ["Fu", "Huazhu", ""], ["Liu", "Jiang", ""]]}, {"id": "2101.12355", "submitter": "Jerry Wei", "authors": "Jerry Wei and Arief Suriawinata and Bing Ren and Xiaoying Liu and\n  Mikhail Lisovsky and Louis Vaickus and Charles Brown and Michael Baker and\n  Naofumi Tomita and Lorenzo Torresani and Jason Wei and Saeed Hassanpour", "title": "A Petri Dish for Histopathology Image Analysis", "comments": "In proceedings of Artificial Intelligence in Medicine (AIME) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of deep learning, there has been increased interest in using\nneural networks for histopathology image analysis, a field that investigates\nthe properties of biopsy or resected specimens traditionally manually examined\nunder a microscope by pathologists. However, challenges such as limited data,\ncostly annotation, and processing high-resolution and variable-size images make\nit difficult to quickly iterate over model designs. Throughout scientific\nhistory, many significant research directions have leveraged small-scale\nexperimental setups as petri dishes to efficiently evaluate exploratory ideas.\nIn this paper, we introduce a minimalist histopathology image analysis dataset\n(MHIST), an analogous petri dish for histopathology image analysis. MHIST is a\nbinary classification dataset of 3,152 fixed-size images of colorectal polyps,\neach with a gold-standard label determined by the majority vote of seven\nboard-certified gastrointestinal pathologists and annotator agreement level.\nMHIST occupies less than 400 MB of disk space, and a ResNet-18 baseline can be\ntrained to convergence on MHIST in just 6 minutes using 3.5 GB of memory on a\nNVIDIA RTX 3090. As example use cases, we use MHIST to study natural questions\nsuch as how dataset size, network depth, transfer learning, and\nhigh-disagreement examples affect model performance. By introducing MHIST, we\nhope to not only help facilitate the work of current histopathology imaging\nresearchers, but also make the field more-accessible to the general community.\nOur dataset is available at https://bmirds.github.io/MHIST.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 02:01:45 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 02:49:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wei", "Jerry", ""], ["Suriawinata", "Arief", ""], ["Ren", "Bing", ""], ["Liu", "Xiaoying", ""], ["Lisovsky", "Mikhail", ""], ["Vaickus", "Louis", ""], ["Brown", "Charles", ""], ["Baker", "Michael", ""], ["Tomita", "Naofumi", ""], ["Torresani", "Lorenzo", ""], ["Wei", "Jason", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "2101.12378", "submitter": "Angtian Wang", "authors": "Angtian Wang, Adam Kortylewski, Alan Yuille", "title": "NeMo: Neural Mesh Models of Contrastive Features for Robust 3D Pose\n  Estimation", "comments": "Accepted by ICLR 2021. Code is publicly available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D pose estimation is a challenging but important task in computer vision. In\nthis work, we show that standard deep learning approaches to 3D pose estimation\nare not robust when objects are partially occluded or viewed from a previously\nunseen pose. Inspired by the robustness of generative vision models to partial\nocclusion, we propose to integrate deep neural networks with 3D generative\nrepresentations of objects into a unified neural architecture that we term\nNeMo. In particular, NeMo learns a generative model of neural feature\nactivations at each vertex on a dense 3D mesh. Using differentiable rendering\nwe estimate the 3D object pose by minimizing the reconstruction error between\nNeMo and the feature representation of the target image. To avoid local optima\nin the reconstruction loss, we train the feature extractor to maximize the\ndistance between the individual feature representations on the mesh using\ncontrastive learning. Our extensive experiments on PASCAL3D+,\noccluded-PASCAL3D+ and ObjectNet3D show that NeMo is much more robust to\npartial occlusion and unseen pose compared to standard deep networks, while\nretaining competitive performance on regular data. Interestingly, our\nexperiments also show that NeMo performs reasonably well even when the mesh\nrepresentation only crudely approximates the true object geometry with a\ncuboid, hence revealing that the detailed 3D geometry is not needed for\naccurate 3D pose estimation. The code is publicly available at\nhttps://github.com/Angtian/NeMo.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 03:23:12 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 17:59:10 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 05:01:51 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Wang", "Angtian", ""], ["Kortylewski", "Adam", ""], ["Yuille", "Alan", ""]]}, {"id": "2101.12382", "submitter": "Varun Menon", "authors": "Kevin Stephen, Varun Menon", "title": "Re Learning Memory Guided Normality for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The authors have introduced a novel method for unsupervised anomaly detection\nthat utilises a newly introduced Memory Module in their paper. We validate the\nauthors claim that this helps improve performance by helping the network learn\nprototypical patterns, and uses the learnt memory to reduce the representation\ncapacity of Convolutional Neural Networks. Further, we validate the efficacy of\ntwo losses introduced by the authors, Separateness Loss and Compactness Loss\npresented to increase the discriminative power of the memory items and the\ndeeply learned features. We test the efficacy with the help of t-SNE plots of\nthe memory items.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 03:28:57 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Stephen", "Kevin", ""], ["Menon", "Varun", ""]]}, {"id": "2101.12404", "submitter": "Navchetan Awasthi", "authors": "Navchetan Awasthi, Rohit Pardasani and Swati Gupta", "title": "Multi-Threshold Attention U-Net (MTAU) based Model for Multimodal Brain\n  Tumor Segmentation in MRI scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gliomas are one of the most frequent brain tumors and are classified into\nhigh grade and low grade gliomas. The segmentation of various regions such as\ntumor core, enhancing tumor etc. plays an important role in determining\nseverity and prognosis. Here, we have developed a multi-threshold model based\non attention U-Net for identification of various regions of the tumor in\nmagnetic resonance imaging (MRI). We propose a multi-path segmentation and\nbuilt three separate models for the different regions of interest. The proposed\nmodel achieved mean Dice Coefficient of 0.59, 0.72, and 0.61 for enhancing\ntumor, whole tumor and tumor core respectively on the training dataset. The\nsame model gave mean Dice Coefficient of 0.57, 0.73, and 0.61 on the validation\ndataset and 0.59, 0.72, and 0.57 on the test dataset.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 04:53:42 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Awasthi", "Navchetan", ""], ["Pardasani", "Rohit", ""], ["Gupta", "Swati", ""]]}, {"id": "2101.12439", "submitter": "Yu-Jen Ma", "authors": "Yu-Jen Ma, Hong-Han Shuai, and Wen-Huang Cheng", "title": "Spatiotemporal Dilated Convolution with Uncertain Matching for\n  Video-based Crowd Estimation", "comments": "Accepted by IEEE Transactions on Multimedia, 2021\n  (https://ieeexplore.ieee.org/document/9316927)", "journal-ref": null, "doi": "10.1109/TMM.2021.3050059", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel SpatioTemporal convolutional Dense Network\n(STDNet) to address the video-based crowd counting problem, which contains the\ndecomposition of 3D convolution and the 3D spatiotemporal dilated dense\nconvolution to alleviate the rapid growth of the model size caused by the\nConv3D layer. Moreover, since the dilated convolution extracts the multiscale\nfeatures, we combine the dilated convolution with the channel attention block\nto enhance the feature representations. Due to the error that occurs from the\ndifficulty of labeling crowds, especially for videos, imprecise or\nstandard-inconsistent labels may lead to poor convergence for the model. To\naddress this issue, we further propose a new patch-wise regression loss (PRL)\nto improve the original pixel-wise loss. Experimental results on three\nvideo-based benchmarks, i.e., the UCSD, Mall and WorldExpo'10 datasets, show\nthat STDNet outperforms both image- and video-based state-of-the-art methods.\nThe source codes are released at \\url{https://github.com/STDNet/STDNet}.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 07:21:33 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Ma", "Yu-Jen", ""], ["Shuai", "Hong-Han", ""], ["Cheng", "Wen-Huang", ""]]}, {"id": "2101.12447", "submitter": "Alexandros Stergiou MSc", "authors": "Alexandros Stergiou", "title": "The Mind's Eye: Visualizing Class-Agnostic Features of CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual interpretability of Convolutional Neural Networks (CNNs) has gained\nsignificant popularity because of the great challenges that CNN complexity\nimposes to understanding their inner workings. Although many techniques have\nbeen proposed to visualize class features of CNNs, most of them do not provide\na correspondence between inputs and the extracted features in specific layers.\nThis prevents the discovery of stimuli that each layer responds better to. We\npropose an approach to visually interpret CNN features given a set of images by\ncreating corresponding images that depict the most informative features of a\nspecific layer. Exploring features in this class-agnostic manner allows for a\ngreater focus on the feature extractor of CNNs. Our method uses a\ndual-objective activation maximization and distance minimization loss, without\nrequiring a generator network nor modifications to the original model. This\nlimits the number of FLOPs to that of the original network. We demonstrate the\nvisualization quality on widely-used architectures.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 07:46:39 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Stergiou", "Alexandros", ""]]}, {"id": "2101.12463", "submitter": "Hao Li", "authors": "Chenghao Chen and Hao Li", "title": "Robust Representation Learning with Feedback for Single Image Deraining", "comments": null, "journal-ref": "IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR),\n  2021, pp.7742-7751", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deraining network can be interpreted as a conditional generator that aims\nat removing rain streaks from image. Most existing image deraining methods\nignore model errors caused by uncertainty that reduces embedding quality.\nUnlike existing image deraining methods that embed low-quality features into\nthe model directly, we replace low-quality features by latent high-quality\nfeatures. The spirit of closed-loop feedback in the automatic control field is\nborrowed to obtain latent high-quality features. A new method for error\ndetection and feature compensation is proposed to address model errors.\nExtensive experiments on benchmark datasets as well as specific real datasets\ndemonstrate that the proposed method outperforms recent state-of-the-art\nmethods. Code is available at: \\\\ https://github.com/LI-Hao-SJTU/DerainRLNet\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:20:50 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 05:58:20 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 09:42:53 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Chenghao", ""], ["Li", "Hao", ""]]}, {"id": "2101.12478", "submitter": "R\\'emi Petitpierre", "authors": "R\\'emi Petitpierre (Ecole polytechnique f\\'ed\\'erale de Lausanne,\n  EPFL, Switzerland)", "title": "Neural networks for semantic segmentation of historical city maps:\n  Cross-cultural performance and the impact of figurative diversity", "comments": "MSc thesis", "journal-ref": null, "doi": "10.13140/RG.2.2.10973.64484", "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we present a new semantic segmentation model for historical\ncity maps that surpasses the state of the art in terms of flexibility and\nperformance. Research in automatic map processing is largely focused on\nhomogeneous corpora or even individual maps, leading to inflexible algorithms.\nRecently, convolutional neural networks have opened new perspectives for the\ndevelopment of more generic tools. Based on two new maps corpora, the first one\ncentered on Paris and the second one gathering cities from all over the world,\nwe propose a method for operationalizing the figuration based on traditional\ncomputer vision algorithms that allows large-scale quantitative analysis. In a\nsecond step, we propose a semantic segmentation model based on neural networks\nand implement several improvements. Finally, we analyze the impact of map\nfiguration on segmentation performance and evaluate future ways to improve the\nrepresentational flexibility of neural networks. To conclude, we show that\nthese networks are able to semantically segment map data of a very large\nfigurative diversity with efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 09:08:12 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Petitpierre", "R\u00e9mi", "", "Ecole polytechnique f\u00e9d\u00e9rale de Lausanne,\n  EPFL, Switzerland"]]}, {"id": "2101.12482", "submitter": "Xiaoqi Zhao", "authors": "Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, Xiang Ruan", "title": "Self-Supervised Representation Learning for RGB-D Salient Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing CNNs-Based RGB-D Salient Object Detection (SOD) networks are all\nrequired to be pre-trained on the ImageNet to learn the hierarchy features\nwhich can help to provide a good initialization. However, the collection and\nannotation of large-scale datasets are time-consuming and expensive. In this\npaper, we utilize Self-Supervised Representation Learning (SSL) to design two\npretext tasks: the cross-modal auto-encoder and the depth-contour estimation.\nOur pretext tasks require only a few and unlabeled RGB-D datasets to perform\npre-training, which makes the network capture rich semantic contexts and reduce\nthe gap between two modalities, thereby providing an effective initialization\nfor the downstream task. In addition, for the inherent problem of cross-modal\nfusion in RGB-D SOD, we propose a consistency-difference aggregation (CDA)\nmodule that splits a single feature fusion into multi-path fusion to achieve an\nadequate perception of consistent and differential information. The CDA module\nis general and suitable for both cross-modal and cross-level feature fusion.\nExtensive experiments on six benchmark RGB-D SOD datasets, our model\npre-trained on the RGB-D dataset ($6,392$ without any annotations) can perform\nfavorably against most state-of-the-art RGB-D methods pre-trained on ImageNet\n($1,280,000$ with image-level annotations).\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 09:16:06 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 09:13:28 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 10:16:15 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhao", "Xiaoqi", ""], ["Pang", "Youwei", ""], ["Zhang", "Lihe", ""], ["Lu", "Huchuan", ""], ["Ruan", "Xiang", ""]]}, {"id": "2101.12491", "submitter": "Francesco Salvetti", "authors": "Vittorio Mazzia, Francesco Salvetti and Marcello Chiaberge", "title": "Efficient-CapsNet: Capsule Network with Self-Attention Routing", "comments": "Accepted by Scientific Reports", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep convolutional neural networks, assisted by architectural design\nstrategies, make extensive use of data augmentation techniques and layers with\na high number of feature maps to embed object transformations. That is highly\ninefficient and for large datasets implies a massive redundancy of features\ndetectors. Even though capsules networks are still in their infancy, they\nconstitute a promising solution to extend current convolutional networks and\nendow artificial visual perception with a process to encode more efficiently\nall feature affine transformations. Indeed, a properly working capsule network\nshould theoretically achieve higher results with a considerably lower number of\nparameters count due to intrinsic capability to generalize to novel viewpoints.\nNevertheless, little attention has been given to this relevant aspect. In this\npaper, we investigate the efficiency of capsule networks and, pushing their\ncapacity to the limits with an extreme architecture with barely 160K\nparameters, we prove that the proposed architecture is still able to achieve\nstate-of-the-art results on three different datasets with only 2% of the\noriginal CapsNet parameters. Moreover, we replace dynamic routing with a novel\nnon-iterative, highly parallelizable routing algorithm that can easily cope\nwith a reduced number of capsules. Extensive experimentation with other capsule\nimplementations has proved the effectiveness of our methodology and the\ncapability of capsule networks to efficiently embed visual representations more\nprone to generalization.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 09:56:44 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 09:34:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Mazzia", "Vittorio", ""], ["Salvetti", "Francesco", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2101.12505", "submitter": "Chengyang Zhou", "authors": "Chengyang Zhou, Thao Vy Dinh, Heyi Kong, Jonathan Yap, Khung Keong\n  Yeo, Hwee Kuan Lee, Kaicheng Liang", "title": "Automated Deep Learning Analysis of Angiography Video Sequences for\n  Coronary Artery Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of obstructions (stenosis) in coronary arteries is currently\ndone by a physician's visual assessment of coronary angiography video\nsequences. It is laborious, and can be susceptible to interobserver variation.\nPrior studies have attempted to automate this process, but few have\ndemonstrated an integrated suite of algorithms for the end-to-end analysis of\nangiograms. We report an automated analysis pipeline based on deep learning to\nrapidly and objectively assess coronary angiograms, highlight coronary vessels\nof interest, and quantify potential stenosis. We propose a 3-stage automated\nanalysis method consisting of key frame extraction, vessel segmentation, and\nstenosis measurement. We combined powerful deep learning approaches such as\nResNet and U-Net with traditional image processing and geometrical analysis. We\ntrained and tested our algorithms on the Left Anterior Oblique (LAO) view of\nthe right coronary artery (RCA) using anonymized angiograms obtained from a\ntertiary cardiac institution, then tested the generalizability of our technique\nto the Right Anterior Oblique (RAO) view. We demonstrated an overall\nimprovement on previous work, with key frame extraction top-5 precision of\n98.4%, vessel segmentation F1-Score of 0.891 and stenosis measurement 20.7%\nType I Error rate.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:23:49 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Zhou", "Chengyang", ""], ["Dinh", "Thao Vy", ""], ["Kong", "Heyi", ""], ["Yap", "Jonathan", ""], ["Yeo", "Khung Keong", ""], ["Lee", "Hwee Kuan", ""], ["Liang", "Kaicheng", ""]]}, {"id": "2101.12521", "submitter": "Hao Feng", "authors": "Hao Feng, Minghao Chen, Jinming Hu, Dong Shen, Haifeng Liu, Deng Cai", "title": "Complementary Pseudo Labels For Unsupervised Domain Adaptation On Person\n  Re-identification", "comments": "10 pages, 3 figures. Accepted for publication in IEEE Transactions on\n  Image Processing 2021", "journal-ref": null, "doi": "10.1109/TIP.2021.3056212", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, supervised person re-identification (re-ID) models have\nreceived increasing studies. However, these models trained on the source domain\nalways suffer dramatic performance drop when tested on an unseen domain.\nExisting methods are primary to use pseudo labels to alleviate this problem.\nOne of the most successful approaches predicts neighbors of each unlabeled\nimage and then uses them to train the model. Although the predicted neighbors\nare credible, they always miss some hard positive samples, which may hinder the\nmodel from discovering important discriminative information of the unlabeled\ndomain. In this paper, to complement these low recall neighbor pseudo labels,\nwe propose a joint learning framework to learn better feature embeddings via\nhigh precision neighbor pseudo labels and high recall group pseudo labels. The\ngroup pseudo labels are generated by transitively merging neighbors of\ndifferent samples into a group to achieve higher recall. However, the merging\noperation may cause subgroups in the group due to imperfect neighbor\npredictions. To utilize these group pseudo labels properly, we propose using a\nsimilarity-aggregating loss to mitigate the influence of these subgroups by\npulling the input sample towards the most similar embeddings. Extensive\nexperiments on three large-scale datasets demonstrate that our method can\nachieve state-of-the-art performance under the unsupervised domain adaptation\nre-ID setting.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 11:06:36 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 04:38:02 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Feng", "Hao", ""], ["Chen", "Minghao", ""], ["Hu", "Jinming", ""], ["Shen", "Dong", ""], ["Liu", "Haifeng", ""], ["Cai", "Deng", ""]]}, {"id": "2101.12543", "submitter": "Anay Majee", "authors": "Anay Majee and Kshitij Agrawal and Anbumani Subramanian", "title": "Few-Shot Learning for Road Object Detection", "comments": "Accepted to AAAI 2021 Workshop on Meta-Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is a problem of high interest in the evolution of deep\nlearning. In this work, we consider the problem of few-shot object detection\n(FSOD) in a real-world, class-imbalanced scenario. For our experiments, we\nutilize the India Driving Dataset (IDD), as it includes a class of\nless-occurring road objects in the image dataset and hence provides a setup\nsuitable for few-shot learning. We evaluate both metric-learning and\nmeta-learning based FSOD methods, in two experimental settings: (i)\nrepresentative (same-domain) splits from IDD, that evaluates the ability of a\nmodel to learn in the context of road images, and (ii) object classes with\nless-occurring object samples, similar to the open-set setting in real-world.\nFrom our experiments, we demonstrate that the metric-learning method\noutperforms meta-learning on the novel classes by (i) 11.2 mAP points on the\nsame domain, and (ii) 1.0 mAP point on the open-set. We also show that our\nextension of object classes in a real-world open dataset offers a rich ground\nfor few-shot learning studies.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 12:35:11 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 07:23:39 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Majee", "Anay", ""], ["Agrawal", "Kshitij", ""], ["Subramanian", "Anbumani", ""]]}, {"id": "2101.12609", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, Zeynep\n  Akata", "title": "Open World Compositional Zero-Shot Learning", "comments": "Accepted in IEEE CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional Zero-Shot learning (CZSL) requires to recognize state-object\ncompositions unseen during training. In this work, instead of assuming prior\nknowledge about the unseen compositions, we operate in the open world setting,\nwhere the search space includes a large number of unseen compositions some of\nwhich might be unfeasible. In this setting, we start from the cosine similarity\nbetween visual features and compositional embeddings. After estimating the\nfeasibility score of each composition, we use these scores to either directly\nmask the output space or as a margin for the cosine similarity between visual\nfeatures and compositional embeddings during training. Our experiments on two\nstandard CZSL benchmarks show that all the methods suffer severe performance\ndegradation when applied in the open world setting. While our simple CZSL model\nachieves state-of-the-art performances in the closed world scenario, our\nfeasibility scores boost the performance of our approach in the open world\nsetting, clearly outperforming the previous state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 14:45:52 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 10:07:28 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 15:48:21 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Naeem", "Muhammad Ferjad", ""], ["Xian", "Yongqin", ""], ["Akata", "Zeynep", ""]]}, {"id": "2101.12616", "submitter": "Ido Freeman", "authors": "Ido Freeman, Kun Zhao, Anton Kummert", "title": "Polynomial Trajectory Predictions for Improved Learning Performance", "comments": "To appear in IEEE ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising demand for Active Safety systems in automotive applications\nstresses the need for a reliable short to mid-term trajectory prediction.\nAnticipating the unfolding path of road users, one can act to increase the\noverall safety. In this work, we propose to train artificial neural networks\nfor movement understanding by predicting trajectories in their natural form, as\na function of time. Predicting polynomial coefficients allows us to increased\naccuracy and improve generalisation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 14:58:27 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 11:56:26 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Freeman", "Ido", ""], ["Zhao", "Kun", ""], ["Kummert", "Anton", ""]]}, {"id": "2101.12677", "submitter": "Benjamin Kiefer", "authors": "Benjamin Kiefer, Martin Messmer, Andreas Zell", "title": "Leveraging domain labels for object detection from UAVs", "comments": "Under review for ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection from Unmanned Aerial Vehicles (UAVs) is of great importance\nin many aerial vision-based applications. Despite the great success of generic\nobject detection methods, a large performance drop is observed when applied to\nimages captured by UAVs. This is due to large variations in imaging conditions,\nsuch as varying altitudes, dynamically changing viewing angles, and different\ncapture times. We demonstrate that domain knowledge is a valuable source of\ninformation and thus propose domain-aware object detectors by using freely\naccessible sensor data. By splitting the model into cross-domain and\ndomain-specific parts, substantial performance improvements are achieved on\nmultiple datasets across multiple models and metrics. In particular, we achieve\na new state-of-the-art performance on UAVDT for real-time detectors.\nFurthermore, we create a new airborne image dataset by annotating 13 713\nobjects in 2 900 images featuring precise altitude and viewing angle\nannotations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 16:42:52 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Kiefer", "Benjamin", ""], ["Messmer", "Martin", ""], ["Zell", "Andreas", ""]]}, {"id": "2101.12690", "submitter": "Theo Costain", "authors": "Theo W. Costain, Victor Adrian Prisacariu", "title": "Towards Generalising Neural Implicit Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural implicit representations have shown substantial improvements in\nefficiently storing 3D data, when compared to conventional formats. However,\nthe focus of existing work has mainly been on storage and subsequent\nreconstruction. In this work, we show that training neural representations for\nreconstruction tasks alongside conventional tasks can produce more general\nencodings that admit equal quality reconstructions to single task training,\nwhilst improving results on conventional tasks when compared to single task\nencodings. We reformulate the semantic segmentation task, creating a more\nrepresentative task for implicit representation contexts, and through\nmulti-task experiments on reconstruction, classification, and segmentation,\nshow our approach learns feature rich encodings that admit equal performance\nfor each task.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:20:22 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:09:02 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Costain", "Theo W.", ""], ["Prisacariu", "Victor Adrian", ""]]}, {"id": "2101.12694", "submitter": "Martin Me{\\ss}mer", "authors": "Martin Messmer, Benjamin Kiefer, Andreas Zell", "title": "Gaining Scale Invariance in UAV Bird's Eye View Object Detection by\n  Adaptive Resizing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we introduce a new preprocessing step applicable to UAV bird's\neye view imagery, which we call Adaptive Resizing. It is constructed to adjust\nthe vast variances in objects' scales, which are naturally inherent to UAV data\nsets. Furthermore, it improves inference speed by four to five times on\naverage. We test this extensively on UAVDT, VisDrone, and on a new data set, we\ncaptured ourselves. On UAVDT, we achieve more than 100 % relative improvement\nin AP50. Moreover, we show how this method can be applied to a general UAV\nobject detection task. Additionally, we successfully test our method on a\ndomain transfer task where we train on some interval of altitudes and test on a\ndifferent one. Code will be made available at our website.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:26:38 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Messmer", "Martin", ""], ["Kiefer", "Benjamin", ""], ["Zell", "Andreas", ""]]}, {"id": "2101.12699", "submitter": "Weijie J. Su", "authors": "Cong Fang, Hangfeng He, Qi Long, Weijie J. Su", "title": "Layer-Peeled Model: Toward Understanding Well-Trained Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce the Layer-Peeled Model, a nonconvex yet\nanalytically tractable optimization program, in a quest to better understand\ndeep neural networks that are trained for a sufficiently long time. As the name\nsuggests, this new model is derived by isolating the topmost layer from the\nremainder of the neural network, followed by imposing certain constraints\nseparately on the two parts. We demonstrate that the Layer-Peeled Model, albeit\nsimple, inherits many characteristics of well-trained neural networks, thereby\noffering an effective tool for explaining and predicting common empirical\npatterns of deep learning training. First, when working on class-balanced\ndatasets, we prove that any solution to this model forms a simplex equiangular\ntight frame, which in part explains the recently discovered phenomenon of\nneural collapse in deep learning training [PHD20]. Moreover, when moving to the\nimbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto\nunknown phenomenon that we term Minority Collapse, which fundamentally limits\nthe performance of deep learning models on the minority classes. In addition,\nwe use the Layer-Peeled Model to gain insights into how to mitigate Minority\nCollapse. Interestingly, this phenomenon is first predicted by the Layer-Peeled\nModel before its confirmation by our computational experiments.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:37:17 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 20:31:42 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Fang", "Cong", ""], ["He", "Hangfeng", ""], ["Long", "Qi", ""], ["Su", "Weijie J.", ""]]}, {"id": "2101.12727", "submitter": "Samarth Mishra", "authors": "Samarth Mishra, Kate Saenko, Venkatesh Saligrama", "title": "Surprisingly Simple Semi-Supervised Domain Adaptation with Pretraining\n  and Consistency", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual domain adaptation involves learning to classify images from a target\nvisual domain using labels available in a different source domain. A range of\nprior work uses adversarial domain alignment to try and learn a domain\ninvariant feature space, where a good source classifier can perform well on\ntarget data. This however, can lead to errors where class A features in the\ntarget domain get aligned to class B features in source. We show that in the\npresence of a few target labels, simple techniques like self-supervision (via\nrotation prediction) and consistency regularization can be effective without\nany adversarial alignment to learn a good target classifier. Our Pretraining\nand Consistency (PAC) approach, can achieve state of the art accuracy on this\nsemi-supervised domain adaptation task, surpassing multiple adversarial domain\nalignment methods, across multiple datasets. Notably, it outperforms all recent\napproaches by 3-5% on the large and challenging DomainNet benchmark, showing\nthe strength of these simple techniques in fixing errors made by adversarial\nalignment.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:40:17 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Mishra", "Samarth", ""], ["Saenko", "Kate", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "2101.12741", "submitter": "Renshen Wang", "authors": "Renshen Wang, Yasuhisa Fujii and Ashok C. Popat", "title": "Post-OCR Paragraph Recognition by Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Paragraphs are an important class of document entities. We propose a new\napproach for paragraph identification by spatial graph convolutional neural\nnetworks (GCN) applied on OCR text boxes. Two steps, namely line splitting and\nline clustering, are performed to extract paragraphs from the lines in OCR\nresults. Each step uses a beta-skeleton graph constructed from bounding boxes,\nwhere the graph edges provide efficient support for graph convolution\noperations. With only pure layout input features, the GCN model size is 3~4\norders of magnitude smaller compared to R-CNN based models, while achieving\ncomparable or better accuracies on PubLayNet and other datasets. Furthermore,\nthe GCN models show good generalization from synthetic training data to\nreal-world images, and good adaptivity for variable document styles.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:54:53 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 19:17:29 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 22:05:02 GMT"}, {"version": "v4", "created": "Tue, 20 Jul 2021 18:53:39 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Renshen", ""], ["Fujii", "Yasuhisa", ""], ["Popat", "Ashok C.", ""]]}]