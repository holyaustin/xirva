[{"id": "2106.00050", "submitter": "Lukas Hedegaard", "authors": "Lukas Hedegaard and Alexandros Iosifidis", "title": "Continual 3D Convolutional Neural Networks for Real-time Processing of\n  Videos", "comments": "12 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Continual 3D Convolutional Neural Networks (Co3D CNNs),\na new computational formulation of spatio-temporal 3D CNNs, in which videos are\nprocessed frame-by-frame rather than by clip. In online processing tasks\ndemanding frame-wise predictions, Co3D CNNs dispense with the computational\nredundancies of regular 3D CNNs, namely the repeated convolutions over frames,\nwhich appear in multiple clips. While yielding an order of magnitude in\ncomputational savings, Co3D CNNs have memory requirements comparable with that\nof corresponding regular 3D CNNs and are less affected by changes in the size\nof the temporal receptive field. We show that Continual 3D CNNs initialised on\nthe weights from preexisting state-of-the-art video recognition models reduce\nthe floating point operations for frame-wise computations by 10.0-12.4x while\nimproving accuracy on Kinetics-400 by 2.3-3.8. Moreover, we investigate the\ntransient start-up response of Co3D CNNs and perform an extensive benchmark of\nonline processing speed as well as accuracy for publicly available\nstate-of-the-art 3D CNNs on modern hardware.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 18:30:52 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Hedegaard", "Lukas", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2106.00076", "submitter": "Sebastian Cygert", "authors": "Sebastian Cygert, Bart{\\l}omiej Wr\\'oblewski, Karol Wo\\'zniak,\n  Rados{\\l}aw S{\\l}owi\\'nski, Andrzej Czy\\.zewski", "title": "Closer Look at the Uncertainty Estimation in Semantic Segmentation under\n  Distributional Shift", "comments": "International Joint Conference on Neural Networks 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent computer vision algorithms achieve impressive performance on\nmany benchmarks, they lack robustness - presented with an image from a\ndifferent distribution, (e.g. weather or lighting conditions not considered\nduring training), they may produce an erroneous prediction. Therefore, it is\ndesired that such a model will be able to reliably predict its confidence\nmeasure. In this work, uncertainty estimation for the task of semantic\nsegmentation is evaluated under a varying level of domain shift: in a\ncross-dataset setting and when adapting a model trained on data from the\nsimulation. It was shown that simple color transformations already provide a\nstrong baseline, comparable to using more sophisticated style-transfer data\naugmentation. Further, by constructing an ensemble consisting of models using\ndifferent backbones and/or augmentation methods, it was possible to improve\nsignificantly model performance in terms of overall accuracy and uncertainty\nestimation under the domain shift setting. The Expected Calibration Error (ECE)\non challenging GTA to Cityscapes adaptation was reduced from 4.05 to the\ncompetitive value of 1.1. Further, an ensemble of models was utilized in the\nself-training setting to improve the pseudo-labels generation, which resulted\nin a significant gain in the final model accuracy, compared to the standard\nfine-tuning (without ensemble).\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 19:50:43 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Cygert", "Sebastian", ""], ["Wr\u00f3blewski", "Bart\u0142omiej", ""], ["Wo\u017aniak", "Karol", ""], ["S\u0142owi\u0144ski", "Rados\u0142aw", ""], ["Czy\u017cewski", "Andrzej", ""]]}, {"id": "2106.00090", "submitter": "Yuanpeng Liu", "authors": "Zhikun Liu, Yuanpeng Liu, Yuan Hong, Jinwen Meng, Jianguo Wang, Shusen\n  Zheng and Xiao Xu", "title": "Deep learning for prediction of hepatocellular carcinoma recurrence\n  after resection or liver transplantation: a discovery and validation study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study aimed to develop a classifier of prognosis after resection or\nliver transplantation (LT) for HCC by directly analysing the ubiquitously\navailable histological images using deep learning based neural networks.\nNucleus map set was used to train U-net to capture the nuclear architectural\ninformation. Train set included the patients with HCC treated by resection and\nhas a distinct outcome. LT set contained patients with HCC treated by LT. Train\nset and its nuclear architectural information extracted by U-net were used to\ntrain MobileNet V2 based classifier (MobileNetV2_HCC_Class), purpose-built for\nclassifying supersized heterogeneous images. The MobileNetV2_HCC_Class\nmaintained relative higher discriminatory power than the other factors after\nHCC resection or LT in the independent validation set. Pathological review\nshowed that the tumoral areas most predictive of recurrence were characterized\nby presence of stroma, high degree of cytological atypia, nuclear\nhyperchomasia, and a lack of immune infiltration. A clinically useful\nprognostic classifier was developed using deep learning allied to histological\nslides. The classifier has been extensively evaluated in independent patient\npopulations with different treatment, and gives consistent excellent results\nacross the classical clinical, biological and pathological features. The\nclassifier assists in refining the prognostic prediction of HCC patients and\nidentifying patients who would benefit from more intensive management.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 20:27:41 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Liu", "Zhikun", ""], ["Liu", "Yuanpeng", ""], ["Hong", "Yuan", ""], ["Meng", "Jinwen", ""], ["Wang", "Jianguo", ""], ["Zheng", "Shusen", ""], ["Xu", "Xiao", ""]]}, {"id": "2106.00107", "submitter": "Terence Lines", "authors": "Terence Lines (1) and Ana Basiri (1) ((1) School of Geographical and\n  Earth Sciences, University of Glasgow)", "title": "3D map creation using crowdsourced GNSS data", "comments": "25 pages with 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D maps are increasingly useful for many applications such as drone\nnavigation, emergency services, and urban planning. However, creating 3D maps\nand keeping them up-to-date using existing technologies, such as laser\nscanners, is expensive. This paper proposes and implements a novel approach to\ngenerate 2.5D (otherwise known as 3D level-of-detail (LOD) 1) maps for free\nusing Global Navigation Satellite Systems (GNSS) signals, which are globally\navailable and are blocked only by obstacles between the satellites and the\nreceivers. This enables us to find the patterns of GNSS signal availability and\ncreate 3D maps. The paper applies algorithms to GNSS signal strength patterns\nbased on a boot-strapped technique that iteratively trains the signal\nclassifiers while generating the map. Results of the proposed technique\ndemonstrate the ability to create 3D maps using automatically processed GNSS\ndata. The results show that the third dimension, i.e. height of the buildings,\ncan be estimated with below 5 metre accuracy, which is the benchmark\nrecommended by the CityGML standard.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 21:24:35 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lines", "Terence", ""], ["Basiri", "Ana", ""]]}, {"id": "2106.00116", "submitter": "Jenia Jitsev", "authors": "Mehdi Cherti and Jenia Jitsev", "title": "Effect of large-scale pre-training on full and few-shot transfer\n  learning for natural and medical images", "comments": "Preprint. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer learning aims to exploit pre-trained models for more efficient\nfollow-up training on wide range of downstream tasks and datasets, enabling\nsuccessful training also on small data. Recent line of work posits strong\nbenefits for model generalization and transfer when model size, data size, and\ncompute budget are increased for the pre-training. It remains however still\nlargely unclear whether the observed transfer improvement due to increase in\nscale also holds when source and target data distributions are far apart from\neach other. In this work we conduct large-scale pre-training on large source\ndatasets of either natural (ImageNet-21k/1k) or medical chest X-Ray images and\ncompare full and few-shot transfer using different target datasets from both\nnatural and medical imaging domains. Our observations provide evidence that\nwhile pre-training and transfer on closely related datasets do show clear\nbenefit of increasing model and data size during pre-training, such benefits\nare not clearly visible when source and target datasets are further apart.\nThese observations hold across both full and few-shot transfer and indicate\nthat scaling laws pointing to improvement of generalization and transfer with\nincreasing model and data size are incomplete and should be revised by taking\ninto account the type and proximity of the source and target data, to correctly\npredict the effect of model and data scale during pre-training on transfer.\nRemarkably, in full shot transfer to a large X-Ray chest imaging target\n(PadChest), the largest model pre-trained on ImageNet-21k slightly outperforms\nbest models pre-trained on large X-Ray chest imaging data. This indicates\npossibility to obtain high quality models for domain-specific transfer even\nwithout access to large domain-specific data, by pre-training instead on\ncomparably very large, generic source data.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 21:55:56 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:33:51 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cherti", "Mehdi", ""], ["Jitsev", "Jenia", ""]]}, {"id": "2106.00131", "submitter": "Yaling Tao Dr.", "authors": "Yaling Tao, Kentaro Takagi, Kouta Nakata", "title": "Clustering-friendly Representation Learning via Instance Discrimination\n  and Feature Decorrelation", "comments": "15 pages, ICLR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the most fundamental tasks in machine learning.\nRecently, deep clustering has become a major trend in clustering techniques.\nRepresentation learning often plays an important role in the effectiveness of\ndeep clustering, and thus can be a principal cause of performance degradation.\nIn this paper, we propose a clustering-friendly representation learning method\nusing instance discrimination and feature decorrelation. Our\ndeep-learning-based representation learning method is motivated by the\nproperties of classical spectral clustering. Instance discrimination learns\nsimilarities among data and feature decorrelation removes redundant correlation\namong features. We utilize an instance discrimination method in which learning\nindividual instance classes leads to learning similarity among instances.\nThrough detailed experiments and examination, we show that the approach can be\nadapted to learning a latent space for clustering. We design novel\nsoftmax-formulated decorrelation constraints for learning. In evaluations of\nimage clustering using CIFAR-10 and ImageNet-10, our method achieves accuracy\nof 81.5% and 95.4%, respectively. We also show that the softmax-formulated\nconstraints are compatible with various neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 22:59:31 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Tao", "Yaling", ""], ["Takagi", "Kentaro", ""], ["Nakata", "Kouta", ""]]}, {"id": "2106.00134", "submitter": "Tianlong Chen", "authors": "Xuxi Chen, Zhenyu Zhang, Yongduo Sui, Tianlong Chen", "title": "GANs Can Play Lottery Tickets Too", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative adversarial networks (GANs) have gained growing popularity in\nnumerous scenarios, while usually suffer from high parameter complexities for\nresource-constrained real-world applications. However, the compression of GANs\nhas less been explored. A few works show that heuristically applying\ncompression techniques normally leads to unsatisfactory results, due to the\nnotorious training instability of GANs. In parallel, the lottery ticket\nhypothesis shows prevailing success on discriminative models, in locating\nsparse matching subnetworks capable of training in isolation to full model\nperformance. In this work, we for the first time study the existence of such\ntrainable matching subnetworks in deep GANs. For a range of GANs, we certainly\nfind matching subnetworks at 67%-74% sparsity. We observe that with or without\npruning discriminator has a minor effect on the existence and quality of\nmatching subnetworks, while the initialization weights used in the\ndiscriminator play a significant role. We then show the powerful\ntransferability of these subnetworks to unseen tasks. Furthermore, extensive\nexperimental results demonstrate that our found subnetworks substantially\noutperform previous state-of-the-art GAN compression approaches in both image\ngeneration (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN).\nCodes available at https://github.com/VITA-Group/GAN-LTH.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 23:03:00 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Chen", "Xuxi", ""], ["Zhang", "Zhenyu", ""], ["Sui", "Yongduo", ""], ["Chen", "Tianlong", ""]]}, {"id": "2106.00161", "submitter": "Travis Munyer", "authors": "Travis J. E. Munyer, Daniel Brinkman, Chenyu Huang, Xin Zhong", "title": "Integrative Use of Computer Vision and Unmanned Aircraft Technologies in\n  Public Inspection: Foreign Object Debris Image Collection", "comments": "This paper has been accepted for publication by the 22nd Annual\n  International Conference on Digital Government Research. 7 pages, 1 figure, 1\n  table", "journal-ref": null, "doi": "10.1145/3463677.3463743", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Unmanned Aircraft Systems (UAS) have become an important resource for public\nservice providers and smart cities. The purpose of this study is to expand this\nresearch area by integrating computer vision and UAS technology to automate\npublic inspection. As an initial case study for this work, a dataset of common\nforeign object debris (FOD) is developed to assess the potential of\nlight-weight automated detection. This paper presents the rationale and\ncreation of this dataset. Future iterations of our work will include further\ntechnical details analyzing experimental implementation. At a local airport,\nUAS and portable cameras are used to collect the data contained in the initial\nversion of this dataset. After collecting these videos of FOD, they were split\ninto individual frames and stored as several thousand images. These frames are\nthen annotated following standard computer vision format and stored in a\nfolder-structure that reflects our creation method. The dataset annotations are\nvalidated using a custom tool that could be abstracted to fit future\napplications. Initial detection models were successfully created using the\nfamous You Only Look Once algorithm, which indicates the practicality of the\nproposed data. Finally, several potential scenarios that could utilize either\nthis dataset or similar methods for other public service are presented.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 00:45:32 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Munyer", "Travis J. E.", ""], ["Brinkman", "Daniel", ""], ["Huang", "Chenyu", ""], ["Zhong", "Xin", ""]]}, {"id": "2106.00168", "submitter": "Hengduo Li", "authors": "Hengduo Li, Zuxuan Wu, Abhinav Shrivastava, Larry S. Davis", "title": "Rethinking Pseudo Labels for Semi-Supervised Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in semi-supervised object detection (SSOD) are largely driven\nby consistency-based pseudo-labeling methods for image classification tasks,\nproducing pseudo labels as supervisory signals. However, when using pseudo\nlabels, there is a lack of consideration in localization precision and\namplified class imbalance, both of which are critical for detection tasks. In\nthis paper, we introduce certainty-aware pseudo labels tailored for object\ndetection, which can effectively estimate the classification and localization\nquality of derived pseudo labels. This is achieved by converting conventional\nlocalization as a classification task followed by refinement. Conditioned on\nclassification and localization quality scores, we dynamically adjust the\nthresholds used to generate pseudo labels and reweight loss functions for each\ncategory to alleviate the class imbalance problem. Extensive experiments\ndemonstrate that our method improves state-of-the-art SSOD performance by 1-2%\nand 4-6% AP on COCO and PASCAL VOC, respectively. In the limited-annotation\nregime, our approach improves supervised baselines by up to 10% AP using only\n1-10% labeled data from COCO.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 01:32:03 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Li", "Hengduo", ""], ["Wu", "Zuxuan", ""], ["Shrivastava", "Abhinav", ""], ["Davis", "Larry S.", ""]]}, {"id": "2106.00178", "submitter": "Tsu-Jui Fu", "authors": "Tsu-Jui Fu, Xin Eric Wang, William Yang Wang", "title": "Language-Driven Image Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite having promising results, style transfer, which requires preparing\nstyle images in advance, may result in lack of creativity and accessibility.\nFollowing human instruction, on the other hand, is the most natural way to\nperform artistic style transfer that can significantly improve controllability\nfor visual effect applications. We introduce a new task -- language-driven\nimage style transfer (\\texttt{LDIST}) -- to manipulate the style of a content\nimage, guided by a text. We propose contrastive language visual artist (CLVA)\nthat learns to extract visual semantics from style instructions and accomplish\n\\texttt{LDIST} by the patch-wise style discriminator. The discriminator\nconsiders the correlation between language and patches of style images or\ntransferred results to jointly embed style instructions. CLVA further compares\ncontrastive pairs of content image and style instruction to improve the mutual\nrelativeness between transfer results. The transferred results from the same\ncontent image can preserve consistent content structures. Besides, they should\npresent analogous style patterns from style instructions that contain similar\nvisual semantics. The experiments show that our CLVA is effective and achieves\nsuperb transferred results on \\texttt{LDIST}.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 01:58:50 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Fu", "Tsu-Jui", ""], ["Wang", "Xin Eric", ""], ["Wang", "William Yang", ""]]}, {"id": "2106.00180", "submitter": "Tokuhiro Nishikawa", "authors": "Tokuhiro Nishikawa, Daiki Shimada, Jerry Jun Yokono", "title": "Dual Normalization Multitasking for Audio-Visual Sounding Object\n  Localization", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although several research works have been reported on audio-visual sound\nsource localization in unconstrained videos, no datasets and metrics have been\nproposed in the literature to quantitatively evaluate its performance. Defining\nthe ground truth for sound source localization is difficult, because the\nlocation where the sound is produced is not limited to the range of the source\nobject, but the vibrations propagate and spread through the surrounding\nobjects. Therefore we propose a new concept, Sounding Object, to reduce the\nambiguity of the visual location of sound, making it possible to annotate the\nlocation of the wide range of sound sources. With newly proposed metrics for\nquantitative evaluation, we formulate the problem of Audio-Visual Sounding\nObject Localization (AVSOL). We also created the evaluation dataset (AVSOL-E\ndataset) by manually annotating the test set of well-known Audio-Visual Event\n(AVE) dataset. To tackle this new AVSOL problem, we propose a novel multitask\ntraining strategy and architecture called Dual Normalization Multitasking\n(DNM), which aggregates the Audio-Visual Correspondence (AVC) task and the\nclassification task for video events into a single audio-visual similarity map.\nBy efficiently utilize both supervisions by DNM, our proposed architecture\nsignificantly outperforms the baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 02:02:52 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Nishikawa", "Tokuhiro", ""], ["Shimada", "Daiki", ""], ["Yokono", "Jerry Jun", ""]]}, {"id": "2106.00182", "submitter": "Wang Zhou", "authors": "Levente J. Klein, Wang Zhou, Conrad M. Albrecht", "title": "Quantification of Carbon Sequestration in Urban Forests", "comments": null, "journal-ref": "International Conference on Machine Learning (ICML 2021) Workshop", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vegetation, trees in particular, sequester carbon by absorbing carbon dioxide\nfrom the atmosphere. However, the lack of efficient quantification methods of\ncarbon stored in trees renders it difficult to track the process. We present an\napproach to estimate the carbon storage in trees based on fusing multi-spectral\naerial imagery and LiDAR data to identify tree coverage, geometric shape, and\ntree species -- key attributes to carbon storage quantification. We demonstrate\nthat tree species information and their three-dimensional geometric shapes can\nbe estimated from aerial imagery in order to determine the tree's biomass.\nSpecifically, we estimate a total of $52,000$ tons of carbon sequestered in\ntrees for New York City's borough Manhattan.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 02:15:20 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 13:53:34 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Klein", "Levente J.", ""], ["Zhou", "Wang", ""], ["Albrecht", "Conrad M.", ""]]}, {"id": "2106.00184", "submitter": "Jack White", "authors": "Binghao Liu and Yao Ding and Jianbin Jiao and Xiangyang Ji and Qixiang\n  Ye", "title": "Anti-aliasing Semantic Reconstruction for Few-Shot Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encouraging progress in few-shot semantic segmentation has been made by\nleveraging features learned upon base classes with sufficient training data to\nrepresent novel classes with few-shot examples. However, this feature sharing\nmechanism inevitably causes semantic aliasing between novel classes when they\nhave similar compositions of semantic concepts. In this paper, we reformulate\nfew-shot segmentation as a semantic reconstruction problem, and convert base\nclass features into a series of basis vectors which span a class-level semantic\nspace for novel class reconstruction. By introducing contrastive loss, we\nmaximize the orthogonality of basis vectors while minimizing semantic aliasing\nbetween classes. Within the reconstructed representation space, we further\nsuppress interference from other classes by projecting query features to the\nsupport vector for precise semantic activation. Our proposed approach, referred\nto as anti-aliasing semantic reconstruction (ASR), provides a systematic yet\ninterpretable solution for few-shot learning problems. Extensive experiments on\nPASCAL VOC and MS COCO datasets show that ASR achieves strong results compared\nwith the prior works.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 02:17:36 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Liu", "Binghao", ""], ["Ding", "Yao", ""], ["Jiao", "Jianbin", ""], ["Ji", "Xiangyang", ""], ["Ye", "Qixiang", ""]]}, {"id": "2106.00186", "submitter": "ByungSoo Ko", "authors": "Geonmo Gu, Byungsoo Ko, SeoungHyun Go, Sung-Hyun Lee, Jingeun Lee,\n  Minchul Shin", "title": "Towards Real-time and Light-weight Line Segment Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous deep learning-based line segment detection (LSD) suffer from the\nimmense model size and high computational cost for line prediction. This\nconstrains them from real-time inference on computationally restricted\nenvironments. In this paper, we propose a real-time and light-weight line\nsegment detector for resource-constrained environments named Mobile LSD\n(M-LSD). We design an extremely efficient LSD architecture by minimizing the\nbackbone network and removing the typical multi-module process for line\nprediction in previous methods. To maintain competitive performance with such a\nlight-weight network, we present novel training schemes: Segments of Line\nsegment (SoL) augmentation and geometric learning scheme. SoL augmentation\nsplits a line segment into multiple subparts, which are used to provide\nauxiliary line data during the training process. Moreover, the geometric\nlearning scheme allows a model to capture additional geometry cues from\nmatching loss, junction and line segmentation, length and degree regression.\nCompared with TP-LSD-Lite, previously the best real-time LSD method, our model\n(M-LSD-tiny) achieves competitive performance with 2.5% of model size and an\nincrease of 130.5% in inference speed on GPU when evaluated with Wireframe and\nYorkUrban datasets. Furthermore, our model runs at 56.8 FPS and 48.6 FPS on\nAndroid and iPhone mobile devices, respectively. To the best of our knowledge,\nthis is the first real-time deep LSD method available on mobile devices.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 02:28:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gu", "Geonmo", ""], ["Ko", "Byungsoo", ""], ["Go", "SeoungHyun", ""], ["Lee", "Sung-Hyun", ""], ["Lee", "Jingeun", ""], ["Shin", "Minchul", ""]]}, {"id": "2106.00209", "submitter": "Ju He", "authors": "Ju He, Adam Kortylewski, Shaokang Yang, Shuai Liu, Cheng Yang, Changhu\n  Wang, Alan Yuille", "title": "Rethinking Re-Sampling in Imbalanced Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Supervised Learning (SSL) has shown its strong ability in utilizing\nunlabeled data when labeled data is scarce. However, most SSL algorithms work\nunder the assumption that the class distributions are balanced in both training\nand test sets. In this work, we consider the problem of SSL on class-imbalanced\ndata, which better reflects real-world situations but has only received limited\nattention so far. In particular, we decouple the training of the representation\nand the classifier, and systematically investigate the effects of different\ndata re-sampling techniques when training the whole network including a\nclassifier as well as fine-tuning the feature extractor only. We find that data\nre-sampling is of critical importance to learn a good classifier as it\nincreases the accuracy of the pseudo-labels, in particular for the minority\nclasses in the unlabeled data. Interestingly, we find that accurate\npseudo-labels do not help when training the feature extractor, rather\ncontrariwise, data re-sampling harms the training of the feature extractor.\nThis finding is against the general intuition that wrong pseudo-labels always\nharm the model performance in SSL. Based on these findings, we suggest to\nre-think the current paradigm of having a single data re-sampling strategy and\ndevelop a simple yet highly effective Bi-Sampling (BiS) strategy for SSL on\nclass-imbalanced data. BiS implements two different re-sampling strategies for\ntraining the feature extractor and the classifier and integrates this decoupled\ntraining into an end-to-end framework... Code will be released at\nhttps://github.com/TACJu/Bi-Sampling.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 03:58:18 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["He", "Ju", ""], ["Kortylewski", "Adam", ""], ["Yang", "Shaokang", ""], ["Liu", "Shuai", ""], ["Yang", "Cheng", ""], ["Wang", "Changhu", ""], ["Yuille", "Alan", ""]]}, {"id": "2106.00216", "submitter": "Yongjian Deng", "authors": "Yongjian Deng, Hao Chen, Huiying Chen, Youfu Li", "title": "EV-VGCNN: A Voxel Graph CNN for Event-based Object Classification", "comments": "10 pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras report sparse intensity changes and hold noticeable advantages\nof low power consumption, high dynamic range, and high response speed for\nvisual perception and understanding on portable devices. Event-based learning\nmethods have recently achieved massive success on object recognition by\nintegrating events into dense frame-based representations to apply traditional\n2D learning algorithms. However, these approaches introduce much redundant\ninformation during the sparse-to-dense conversion and necessitate models with\nheavy-weight and large capacities, limiting the potential of event cameras on\nreal-life applications. To address the core problem of balancing accuracy and\nmodel complexity for event-based classification models, we (1) construct graph\nrepresentations for event data to utilize their sparsity nature better and\ndesign a lightweight end-to-end graph neural network (EV-VGCNN) for\nclassification; (2) use voxel-wise vertices rather than traditional point-wise\nmethods to incorporate the information from more points; (3) introduce a\nmulti-scale feature relational layer (MFRL) to extract semantic and motion cues\nfrom each vertex adaptively concerning its distances to neighbors.\nComprehensive experiments show that our approach advances state-of-the-art\nclassification accuracy while achieving nearly 20 times parameter reduction\n(merely 0.84M parameters).\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 04:07:03 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Deng", "Yongjian", ""], ["Chen", "Hao", ""], ["Chen", "Huiying", ""], ["Li", "Youfu", ""]]}, {"id": "2106.00227", "submitter": "Fanyi Wang", "authors": "Haotian Hu, Fanyi Wang, Huixiao Le", "title": "VA-GCN: A Vector Attention Graph Convolution Network for learning on\n  Point Clouds", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Owing to the development of research on local aggregation operators, dramatic\nbreakthrough has been made in point cloud analysis models. However, existing\nlocal aggregation operators in the current literature fail to attach decent\nimportance to the local information of the point cloud, which limits the power\nof the models. To fit this gap, we propose an efficient Vector Attention\nConvolution module (VAConv), which utilizes K-Nearest Neighbor (KNN) to extract\nthe neighbor points of each input point, and then uses the elevation and\nazimuth relationship of the vectors between the center point and its neighbors\nto construct an attention weight matrix for edge features. Afterwards, the\nVAConv adopts a dual-channel structure to fuse weighted edge features and\nglobal features. To verify the efficiency of the VAConv, we connect the VAConvs\nwith different receptive fields in parallel to obtain a Multi-scale graph\nconvolutional network, VA-GCN. The proposed VA-GCN achieves state-of-the-art\nperformance on standard benchmarks including ModelNet40, S3DIS and ShapeNet.\nRemarkably, on the ModelNet40 dataset for 3D classification, VA-GCN increased\nby 2.4% compared to the baseline.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 04:49:25 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Hu", "Haotian", ""], ["Wang", "Fanyi", ""], ["Le", "Huixiao", ""]]}, {"id": "2106.00240", "submitter": "Kshitij Gupta", "authors": "Kshitij Gupta, Devansh Gautam, Radhika Mamidi", "title": "Volta at SemEval-2021 Task 6: Towards Detecting Persuasive Texts and\n  Images using Textual and Multimodal Ensemble", "comments": "7 pages, accepted at SemEval-2021 co-located with ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memes are one of the most popular types of content used to spread information\nonline. They can influence a large number of people through rhetorical and\npsychological techniques. The task, Detection of Persuasion Techniques in Texts\nand Images, is to detect these persuasive techniques in memes. It consists of\nthree subtasks: (A) Multi-label classification using textual content, (B)\nMulti-label classification and span identification using textual content, and\n(C) Multi-label classification using visual and textual content. In this paper,\nwe propose a transfer learning approach to fine-tune BERT-based models in\ndifferent modalities. We also explore the effectiveness of ensembles of models\ntrained in different modalities. We achieve an F1-score of 57.0, 48.2, and 52.1\nin the corresponding subtasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 05:41:03 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gupta", "Kshitij", ""], ["Gautam", "Devansh", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2106.00245", "submitter": "Linjie Li", "authors": "Linjie Li, Jie Lei, Zhe Gan, Jingjing Liu", "title": "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With large-scale pre-training, the past two years have witnessed significant\nperformance boost on the Visual Question Answering (VQA) task. Though rapid\nprogresses have been made, it remains unclear whether these state-of-the-art\n(SOTA) VQA models are robust when encountering test examples in the wild. To\nstudy this, we introduce Adversarial VQA, a new large-scale VQA benchmark,\ncollected iteratively via an adversarial human-and-model-in-the-loop procedure.\nThrough this new benchmark, we present several interesting findings. (i)\nSurprisingly, during dataset collection, we find that non-expert annotators can\nsuccessfully attack SOTA VQA models with relative ease. (ii) We test a variety\nof SOTA VQA models on our new dataset to highlight their fragility, and find\nthat both large-scale pre-trained models and adversarial training methods can\nonly achieve far lower performance than what they can achieve on the standard\nVQA v2 dataset. (iii) When considered as data augmentation, our dataset can be\nused to improve the performance on other robust VQA benchmarks. (iv) We present\na detailed analysis of the dataset, providing valuable insights on the\nchallenges it brings to the community. We hope Adversarial VQA can serve as a\nvaluable benchmark that will be used by future work to test the robustness of\nits developed VQA models. Our dataset is publicly available at\nhttps://adversarialvqa. github.io/.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 05:54:41 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Li", "Linjie", ""], ["Lei", "Jie", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""]]}, {"id": "2106.00250", "submitter": "Kshitij Gupta", "authors": "Kshitij Gupta, Devansh Gautam, Radhika Mamidi", "title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags", "comments": "7 pages, accepted at WAT-2021 co-located with ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal Machine Translation (MMT) enriches the source text with visual\ninformation for translation. It has gained popularity in recent years, and\nseveral pipelines have been proposed in the same direction. Yet, the task lacks\nquality datasets to illustrate the contribution of visual modality in the\ntranslation systems. In this paper, we propose our system under the team name\nVolta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We\nalso participate in the textual-only subtask of the same language pair for\nwhich we use mBART, a pretrained multilingual sequence-to-sequence model. For\nmultimodal translation, we propose to enhance the textual input by bringing the\nvisual information to a textual domain by extracting object tags from the\nimage. We also explore the robustness of our system by systematically degrading\nthe source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test\nset and challenge set of the multimodal task.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 06:19:29 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 12:26:04 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 10:44:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gupta", "Kshitij", ""], ["Gautam", "Devansh", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2106.00256", "submitter": "Hao Cheng", "authors": "Hao Cheng, Kim-Hui Yap, and Bihan Wen", "title": "Reconciliation of Statistical and Spatial Sparsity For Robust Image and\n  Image-Set Classification", "comments": "Submitted to IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent image classification algorithms, by learning deep features from\nlarge-scale datasets, have achieved significantly better results comparing to\nthe classic feature-based approaches. However, there are still various\nchallenges of image classifications in practice, such as classifying noisy\nimage or image-set queries and training deep image classification models over\nthe limited-scale dataset. Instead of applying generic deep features, the\nmodel-based approaches can be more effective and data-efficient for robust\nimage and image-set classification tasks, as various image priors are exploited\nfor modeling the inter- and intra-set data variations while preventing\nover-fitting. In this work, we propose a novel Joint Statistical and Spatial\nSparse representation, dubbed \\textit{J3S}, to model the image or image-set\ndata for classification, by reconciling both their local patch structures and\nglobal Gaussian distribution mapped into Riemannian manifold. To the best of\nour knowledge, no work to date utilized both global statistics and local patch\nstructures jointly via joint sparse representation. We propose to solve the\njoint sparse coding problem based on the J3S model, by coupling the local and\nglobal image representations using joint sparsity. The learned J3S models are\nused for robust image and image-set classification. Experiments show that the\nproposed J3S-based image classification scheme outperforms the popular or\nstate-of-the-art competing methods over FMD, UIUC, ETH-80 and YTC databases.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 06:33:24 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Cheng", "Hao", ""], ["Yap", "Kim-Hui", ""], ["Wen", "Bihan", ""]]}, {"id": "2106.00259", "submitter": "Qiufu Li", "authors": "Qiufu Li and Linlin Shen", "title": "3D WaveUNet: 3D Wavelet Integrated Encoder-Decoder Network for Neuron\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D neuron segmentation is a key step for the neuron digital reconstruction,\nwhich is essential for exploring brain circuits and understanding brain\nfunctions. However, the fine line-shaped nerve fibers of neuron could spread in\na large region, which brings great computational cost to the segmentation in 3D\nneuronal images. Meanwhile, the strong noises and disconnected nerve fibers in\nthe image bring great challenges to the task. In this paper, we propose a 3D\nwavelet and deep learning based 3D neuron segmentation method. The neuronal\nimage is first partitioned into neuronal cubes to simplify the segmentation\ntask. Then, we design 3D WaveUNet, the first 3D wavelet integrated\nencoder-decoder network, to segment the nerve fibers in the cubes; the wavelets\ncould assist the deep networks in suppressing data noise and connecting the\nbroken fibers. We also produce a Neuronal Cube Dataset (NeuCuDa) using the\nbiggest available annotated neuronal image dataset, BigNeuron, to train 3D\nWaveUNet. Finally, the nerve fibers segmented in cubes are assembled to\ngenerate the complete neuron, which is digitally reconstructed using an\navailable automatic tracing algorithm. The experimental results show that our\nneuron segmentation method could completely extract the target neuron in noisy\nneuronal images. The integrated 3D wavelets can efficiently improve the\nperformance of 3D neuron segmentation and reconstruction. The code and\npre-trained models for this work will be available at\nhttps://github.com/LiQiufu/3D-WaveUNet.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 06:46:50 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Li", "Qiufu", ""], ["Shen", "Linlin", ""]]}, {"id": "2106.00264", "submitter": "Bo Liu", "authors": "Liu Bo, Qiulei Dong, Zhanyi Hu", "title": "Hardness Sampling for Self-Training Based Transductive Zero-Shot\n  Learning", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Transductive zero-shot learning (T-ZSL) which could alleviate the domain\nshift problem in existing ZSL works, has received much attention recently.\nHowever, an open problem in T-ZSL: how to effectively make use of unseen-class\nsamples for training, still remains. Addressing this problem, we first\nempirically analyze the roles of unseen-class samples with different degrees of\nhardness in the training process based on the uneven prediction phenomenon\nfound in many ZSL methods, resulting in three observations. Then, we propose\ntwo hardness sampling approaches for selecting a subset of diverse and hard\nsamples from a given unseen-class dataset according to these observations. The\nfirst one identifies the samples based on the class-level frequency of the\nmodel predictions while the second enhances the former by normalizing the class\nfrequency via an approximate class prior estimated by an explored prior\nestimation algorithm. Finally, we design a new Self-Training framework with\nHardness Sampling for T-ZSL, called STHS, where an arbitrary inductive ZSL\nmethod could be seamlessly embedded and it is iteratively trained with\nunseen-class samples selected by the hardness sampling approach. We introduce\ntwo typical ZSL methods into the STHS framework and extensive experiments\ndemonstrate that the derived T-ZSL methods outperform many state-of-the-art\nmethods on three public benchmarks. Besides, we note that the unseen-class\ndataset is separately used for training in some existing transductive\ngeneralized ZSL (T-GZSL) methods, which is not strict for a GZSL task. Hence,\nwe suggest a more strict T-GZSL data setting and establish a competitive\nbaseline on this setting by introducing the proposed STHS framework to T-GZSL.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 06:55:19 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bo", "Liu", ""], ["Dong", "Qiulei", ""], ["Hu", "Zhanyi", ""]]}, {"id": "2106.00274", "submitter": "Alex D\\'iaz Santos", "authors": "Alex D\\'iaz and Damian Steele", "title": "Analysis of classifiers robust to noisy labels", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore contemporary robust classification algorithms for overcoming\nclass-dependant labelling noise: Forward, Importance Re-weighting and\nT-revision. The classifiers are trained and evaluated on class-conditional\nrandom label noise data while the final test data is clean. We demonstrate\nmethods for estimating the transition matrix in order to obtain better\nclassifier performance when working with noisy data. We apply deep learning to\nthree data-sets and derive an end-to-end analysis with unknown noise on the\nCIFAR data-set from scratch. The effectiveness and robustness of the\nclassifiers are analysed, and we compare and contrast the results of each\nexperiment are using top-1 accuracy as our criterion.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 07:14:51 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["D\u00edaz", "Alex", ""], ["Steele", "Damian", ""]]}, {"id": "2106.00305", "submitter": "Frank Ruis", "authors": "Frank Ruis, Gertjan Burghouts, Doina Bucur", "title": "Independent Prototype Propagation for Zero-Shot Compositionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans are good at compositional zero-shot reasoning; someone who has never\nseen a zebra before could nevertheless recognize one when we tell them it looks\nlike a horse with black and white stripes. Machine learning systems, on the\nother hand, usually leverage spurious correlations in the training data, and\nwhile such correlations can help recognize objects in context, they hurt\ngeneralization. To be able to deal with underspecified datasets while still\nleveraging contextual clues during classification, we propose ProtoProp, a\nnovel prototype propagation graph method. First we learn prototypical\nrepresentations of objects (e.g., zebra) that are conditionally independent\nw.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate\nthe independent prototypes through a compositional graph, to learn\ncompositional prototypes of novel attribute-object combinations that reflect\nthe dependencies of the target distribution. The method does not rely on any\nexternal data, such as class hierarchy graphs or pretrained word embeddings. We\nevaluate our approach on AO-Clever, a synthetic and strongly visual dataset\nwith clean labels, and UT-Zappos, a noisy real-world dataset of fine-grained\nshoe types. We show that in the generalized compositional zero-shot setting we\noutperform state-of-the-art results, and through ablations we show the\nimportance of each part of the method and their contribution to the final\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:24:09 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 15:26:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ruis", "Frank", ""], ["Burghouts", "Gertjan", ""], ["Bucur", "Doina", ""]]}, {"id": "2106.00318", "submitter": "Julia Guerrero-Viu", "authors": "Julia Guerrero-Viu, Sergio Izquierdo, Philipp Schr\\\"oppel and Thomas\n  Brox", "title": "Semi-Supervised Disparity Estimation with Deep Feature Reconstruction", "comments": "Women in Computer Vision workshop CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of deep learning in disparity estimation, the domain\ngeneralization gap remains an issue. We propose a semi-supervised pipeline that\nsuccessfully adapts DispNet to a real-world domain by joint supervised training\non labeled synthetic data and self-supervised training on unlabeled real data.\nFurthermore, accounting for the limitations of the widely-used photometric\nloss, we analyze the impact of deep feature reconstruction as a promising\nsupervisory signal for disparity estimation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:48:38 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Guerrero-Viu", "Julia", ""], ["Izquierdo", "Sergio", ""], ["Schr\u00f6ppel", "Philipp", ""], ["Brox", "Thomas", ""]]}, {"id": "2106.00329", "submitter": "Zihao Yan", "authors": "Zihao Yan, Zimu Yi, Ruizhen Hu, Niloy J. Mitra, Daniel Cohen-Or, Hui\n  Huang", "title": "Consistent Two-Flow Network for Tele-Registration of Point Clouds", "comments": "Accepted to TVCG 2021, project page at\n  https://vcc.tech/research/2021/CTFNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rigid registration of partial observations is a fundamental problem in\nvarious applied fields. In computer graphics, special attention has been given\nto the registration between two partial point clouds generated by scanning\ndevices. State-of-the-art registration techniques still struggle when the\noverlap region between the two point clouds is small, and completely fail if\nthere is no overlap between the scan pairs. In this paper, we present a\nlearning-based technique that alleviates this problem, and allows registration\nbetween point clouds, presented in arbitrary poses, and having little or even\nno overlap, a setting that has been referred to as tele-registration. Our\ntechnique is based on a novel neural network design that learns a prior of a\nclass of shapes and can complete a partial shape. The key idea is combining the\nregistration and completion tasks in a way that reinforces each other. In\nparticular, we simultaneously train the registration network and completion\nnetwork using two coupled flows, one that register-and-complete, and one that\ncomplete-and-register, and encourage the two flows to produce a consistent\nresult. We show that, compared with each separate flow, this two-flow training\nleads to robust and reliable tele-registration, and hence to a better point\ncloud prediction that completes the registered scans. It is also worth\nmentioning that each of the components in our neural network outperforms\nstate-of-the-art methods in both completion and registration. We further\nanalyze our network with several ablation studies and demonstrate its\nperformance on a large number of partial point clouds, both synthetic and\nreal-world, that have only small or no overlap.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 09:03:21 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 09:41:09 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Yan", "Zihao", ""], ["Yi", "Zimu", ""], ["Hu", "Ruizhen", ""], ["Mitra", "Niloy J.", ""], ["Cohen-Or", "Daniel", ""], ["Huang", "Hui", ""]]}, {"id": "2106.00358", "submitter": "Nicola Messina", "authors": "Nicola Messina, Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro,\n  St\\'ephane Marchand-Maillet", "title": "Towards Efficient Cross-Modal Visual Textual Retrieval using\n  Transformer-Encoder Deep Features", "comments": "Accepted at CBMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval is an important functionality in modern search engines,\nas it increases the user experience by allowing queries and retrieved objects\nto pertain to different modalities. In this paper, we focus on the\nimage-sentence retrieval task, where the objective is to efficiently find\nrelevant images for a given sentence (image-retrieval) or the relevant\nsentences for a given image (sentence-retrieval). Computer vision literature\nreports the best results on the image-sentence matching task using deep neural\nnetworks equipped with attention and self-attention mechanisms. They evaluate\nthe matching performance on the retrieval task by performing sequential scans\nof the whole dataset. This method does not scale well with an increasing amount\nof images or captions. In this work, we explore different preprocessing\ntechniques to produce sparsified deep multi-modal features extracting them from\nstate-of-the-art deep-learning architectures for image-text matching. Our main\nobjective is to lay down the paths for efficient indexing of complex\nmulti-modal descriptions. We use the recently introduced TERN architecture as\nan image-sentence features extractor. It is designed for producing fixed-size\n1024-d vectors describing whole images and sentences, as well as\nvariable-length sets of 1024-d vectors describing the various building\ncomponents of the two modalities (image regions and sentence words\nrespectively). All these vectors are enforced by the TERN design to lie into\nthe same common space. Our experiments show interesting preliminary results on\nthe explored methods and suggest further experimentation in this important\nresearch direction.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:11:46 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Messina", "Nicola", ""], ["Amato", "Giuseppe", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Marchand-Maillet", "St\u00e9phane", ""]]}, {"id": "2106.00359", "submitter": "Adri\\`a Arbu\\'es-Sang\\\"uesa", "authors": "Adri\\`a Arbu\\'es-Sang\\\"uesa, Adri\\'an Mart\\'in, Paulino Granero,\n  Coloma Ballester, Gloria Haro", "title": "Learning Football Body-Orientation as a Matter of Classification", "comments": "Accepted in the AI for Sports Analytics Workshop at ICJAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Orientation is a crucial skill for football players that becomes a\ndifferential factor in a large set of events, especially the ones involving\npasses. However, existing orientation estimation methods, which are based on\ncomputer-vision techniques, still have a lot of room for improvement. To the\nbest of our knowledge, this article presents the first deep learning model for\nestimating orientation directly from video footage. By approaching this\nchallenge as a classification problem where classes correspond to orientation\nbins, and by introducing a cyclic loss function, a well-known convolutional\nnetwork is refined to provide player orientation data. The model is trained by\nusing ground-truth orientation data obtained from wearable EPTS devices, which\nare individually compensated with respect to the perceived orientation in the\ncurrent frame. The obtained results outperform previous methods; in particular,\nthe absolute median error is less than 12 degrees per player. An ablation study\nis included in order to show the potential generalization to any kind of\nfootball video footage.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:12:32 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Arbu\u00e9s-Sang\u00fcesa", "Adri\u00e0", ""], ["Mart\u00edn", "Adri\u00e1n", ""], ["Granero", "Paulino", ""], ["Ballester", "Coloma", ""], ["Haro", "Gloria", ""]]}, {"id": "2106.00368", "submitter": "Michael Rotman", "authors": "Michael Rotman and Lior Wolf", "title": "Natural Statistics of Network Activations and Implications for Knowledge\n  Distillation", "comments": "Accepted to ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a matter that is analog to the study of natural image statistics, we study\nthe natural statistics of the deep neural network activations at various\nlayers. As we show, these statistics, similar to image statistics, follow a\npower law. We also show, both analytically and empirically, that with depth the\nexponent of this power law increases at a linear rate.\n  As a direct implication of our discoveries, we present a method for\nperforming Knowledge Distillation (KD). While classical KD methods consider the\nlogits of the teacher network, more recent methods obtain a leap in performance\nby considering the activation maps. This, however, uses metrics that are\nsuitable for comparing images. We propose to employ two additional loss terms\nthat are based on the spectral properties of the intermediate activation maps.\nThe proposed method obtains state of the art results on multiple image\nrecognition KD benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:18:30 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Rotman", "Michael", ""], ["Wolf", "Lior", ""]]}, {"id": "2106.00371", "submitter": "Oscar Mendez", "authors": "Oscar Mendez, Simon Hadfield, Richard Bowden", "title": "Markov Localisation using Heatmap Regression and Deep Convolutional\n  Odometry", "comments": "IEEE International Conference on Robotics and Automation (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of self-driving vehicles there is strong competition between\napproaches based on visual localisation and LiDAR. While LiDAR provides\nimportant depth information, it is sparse in resolution and expensive. On the\nother hand, cameras are low-cost and recent developments in deep learning mean\nthey can provide high localisation performance. However, several fundamental\nproblems remain, particularly in the domain of uncertainty, where learning\nbased approaches can be notoriously over-confident.\n  Markov, or grid-based, localisation was an early solution to the localisation\nproblem but fell out of favour due to its computational complexity.\nRepresenting the likelihood field as a grid (or volume) means there is a trade\noff between accuracy and memory size. Furthermore, it is necessary to perform\nexpensive convolutions across the entire likelihood volume. Despite the benefit\nof simultaneously maintaining a likelihood for all possible locations, grid\nbased approaches were superseded by more efficient particle filters and Monte\nCarlo Localisation (MCL). However, MCL introduces its own problems e.g.\nparticle deprivation.\n  Recent advances in deep learning hardware allow large likelihood volumes to\nbe stored directly on the GPU, along with the hardware necessary to efficiently\nperform GPU-bound 3D convolutions and this obviates many of the disadvantages\nof grid based methods. In this work, we present a novel CNN-based localisation\napproach that can leverage modern deep learning hardware. By implementing a\ngrid-based Markov localisation approach directly on the GPU, we create a hybrid\nCNN that can perform image-based localisation and odometry-based likelihood\npropagation within a single neural network. The resulting approach is capable\nof outperforming direct pose regression methods as well as state-of-the-art\nlocalisation systems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:28:49 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Mendez", "Oscar", ""], ["Hadfield", "Simon", ""], ["Bowden", "Richard", ""]]}, {"id": "2106.00373", "submitter": "Vincent Vousten", "authors": "Juul P.A. van Boxtel, Vincent R.J. Vousten, Josien Pluim, Nastaran\n  Mohammadian Rad", "title": "Hybrid Deep Neural Network for Brachial Plexus Nerve Segmentation in\n  Ultrasound Images", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound-guided regional anesthesia (UGRA) can replace general anesthesia\n(GA), improving pain control and recovery time. This method can be applied on\nthe brachial plexus (BP) after clavicular surgeries. However, identification of\nthe BP from ultrasound (US) images is difficult, even for trained\nprofessionals. To address this problem, convolutional neural networks (CNNs)\nand more advanced deep neural networks (DNNs) can be used for identification\nand segmentation of the BP nerve region. In this paper, we propose a hybrid\nmodel consisting of a classification model followed by a segmentation model to\nsegment BP nerve regions in ultrasound images. A CNN model is employed as a\nclassifier to precisely select the images with the BP region. Then, a U-net or\nM-net model is used for the segmentation. Our experimental results indicate\nthat the proposed hybrid model significantly improves the segmentation\nperformance over a single segmentation model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:31:47 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["van Boxtel", "Juul P. A.", ""], ["Vousten", "Vincent R. J.", ""], ["Pluim", "Josien", ""], ["Rad", "Nastaran Mohammadian", ""]]}, {"id": "2106.00376", "submitter": "Yanfei Su", "authors": "Yanfei Su, Weiquan Liu, Zhimin Yuan, Ming Cheng, Zhihong Zhang, Xuelun\n  Shen, Cheng Wang", "title": "DLA-Net: Learning Dual Local Attention Features for Semantic\n  Segmentation of Large-Scale Building Facade Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of building facade is significant in various\napplications, such as urban building reconstruction and damage assessment. As\nthere is a lack of 3D point clouds datasets related to the fine-grained\nbuilding facade, we construct the first large-scale building facade point\nclouds benchmark dataset for semantic segmentation. The existing methods of\nsemantic segmentation cannot fully mine the local neighborhood information of\npoint clouds. Addressing this problem, we propose a learnable attention module\nthat learns Dual Local Attention features, called DLA in this paper. The\nproposed DLA module consists of two blocks, including the self-attention block\nand attentive pooling block, which both embed an enhanced position encoding\nblock. The DLA module could be easily embedded into various network\narchitectures for point cloud segmentation, naturally resulting in a new 3D\nsemantic segmentation network with an encoder-decoder architecture, called\nDLA-Net in this work. Extensive experimental results on our constructed\nbuilding facade dataset demonstrate that the proposed DLA-Net achieves better\nperformance than the state-of-the-art methods for semantic segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:39:11 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Su", "Yanfei", ""], ["Liu", "Weiquan", ""], ["Yuan", "Zhimin", ""], ["Cheng", "Ming", ""], ["Zhang", "Zhihong", ""], ["Shen", "Xuelun", ""], ["Wang", "Cheng", ""]]}, {"id": "2106.00389", "submitter": "Nantheera Anantrasirichai", "authors": "Annika Wong and Nantheera Anantrasirichai and Thanarat H.\n  Chalidabhongse and Duangdao Palasuwan and Attakorn Palasuwan and David Bull", "title": "Analysis of Vision-based Abnormal Red Blood Cell Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identification of abnormalities in red blood cells (RBC) is key to diagnosing\na range of medical conditions from anaemia to liver disease. Currently this is\ndone manually, a time-consuming and subjective process. This paper presents an\nautomated process utilising the advantages of machine learning to increase\ncapacity and standardisation of cell abnormality detection, and its performance\nis analysed. Three different machine learning technologies were used: a Support\nVector Machine (SVM), a classical machine learning technology; TabNet, a deep\nlearning architecture for tabular data; U-Net, a semantic segmentation network\ndesigned for medical image segmentation. A critical issue was the highly\nimbalanced nature of the dataset which impacts the efficacy of machine\nlearning. To address this, synthesising minority class samples in feature space\nwas investigated via Synthetic Minority Over-sampling Technique (SMOTE) and\ncost-sensitive learning. A combination of these two methods is investigated to\nimprove the overall performance. These strategies were found to increase\nsensitivity to minority classes. The impact of unknown cells on semantic\nsegmentation is demonstrated, with some evidence of the model applying learning\nof labelled cells to these anonymous cells. These findings indicate both\nclassical models and new deep learning networks as promising methods in\nautomating RBC abnormality detection.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:52:41 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wong", "Annika", ""], ["Anantrasirichai", "Nantheera", ""], ["Chalidabhongse", "Thanarat H.", ""], ["Palasuwan", "Duangdao", ""], ["Palasuwan", "Attakorn", ""], ["Bull", "David", ""]]}, {"id": "2106.00436", "submitter": "Muhammad E. H. Chowdhury", "authors": "Tawsifur Rahman, Alex Akinbi, Muhammad E. H. Chowdhury, Tarik A.\n  Rashid, Abdulkadir \\c{S}eng\\\"ur, Amith Khandakar, Khandaker Reajul Islam,\n  Aras M. Ismael", "title": "COV-ECGNET: COVID-19 detection using ECG trace images with deep\n  convolutional neural network", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliable and rapid identification of the COVID-19 has become crucial to\nprevent the rapid spread of the disease, ease lockdown restrictions and reduce\npressure on public health infrastructures. Recently, several methods and\ntechniques have been proposed to detect the SARS-CoV-2 virus using different\nimages and data. However, this is the first study that will explore the\npossibility of using deep convolutional neural network (CNN) models to detect\nCOVID-19 from electrocardiogram (ECG) trace images. In this work, COVID-19 and\nother cardiovascular diseases (CVDs) were detected using deep-learning\ntechniques. A public dataset of ECG images consists of 1937 images from five\ndistinct categories, such as Normal, COVID-19, myocardial infarction (MI),\nabnormal heartbeat (AHB), and recovered myocardial infarction (RMI) were used\nin this study. Six different deep CNN models (ResNet18, ResNet50, ResNet101,\nInceptionV3, DenseNet201, and MobileNetv2) were used to investigate three\ndifferent classification schemes: two-class classification (Normal vs\nCOVID-19); three-class classification (Normal, COVID-19, and Other CVDs), and\nfinally, five-class classification (Normal, COVID-19, MI, AHB, and RMI). For\ntwo-class and three-class classification, Densenet201 outperforms other\nnetworks with an accuracy of 99.1%, and 97.36%, respectively; while for the\nfive-class classification, InceptionV3 outperforms others with an accuracy of\n97.83%. ScoreCAM visualization confirms that the networks are learning from the\nrelevant area of the trace images. Since the proposed method uses ECG trace\nimages which can be captured by smartphones and are readily available\nfacilities in low-resources countries, this study will help in faster\ncomputer-aided diagnosis of COVID-19 and other cardiac abnormalities.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 12:33:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Rahman", "Tawsifur", ""], ["Akinbi", "Alex", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Rashid", "Tarik A.", ""], ["\u015eeng\u00fcr", "Abdulkadir", ""], ["Khandakar", "Amith", ""], ["Islam", "Khandaker Reajul", ""], ["Ismael", "Aras M.", ""]]}, {"id": "2106.00446", "submitter": "Vasileios Gkitsas", "authors": "V. Gkitsas, V. Sterzentsenko, N. Zioulis, G. Albanis, D. Zarpalas", "title": "PanoDR: Spherical Panorama Diminished Reality for Indoor Scenes", "comments": "Accepted at CVPR, OmniCV Workshop. Code and models are available at\n  https://vcl3d.github.io/PanoDR/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The rising availability of commercial $360^\\circ$ cameras that democratize\nindoor scanning, has increased the interest for novel applications, such as\ninterior space re-design. Diminished Reality (DR) fulfills the requirement of\nsuch applications, to remove existing objects in the scene, essentially\ntranslating this to a counterfactual inpainting task. While recent advances in\ndata-driven inpainting have shown significant progress in generating realistic\nsamples, they are not constrained to produce results with reality mapped\nstructures. To preserve the `reality' in indoor (re-)planning applications, the\nscene's structure preservation is crucial. To ensure structure-aware\ncounterfactual inpainting, we propose a model that initially predicts the\nstructure of an indoor scene and then uses it to guide the reconstruction of an\nempty -- background only -- representation of the same scene. We train and\ncompare against other state-of-the-art methods on a version of the Structured3D\ndataset modified for DR, showing superior results in both quantitative metrics\nand qualitative results, but more interestingly, our approach exhibits a much\nfaster convergence rate. Code and models are available at\nhttps://vcl3d.github.io/PanoDR/ .\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 12:56:53 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gkitsas", "V.", ""], ["Sterzentsenko", "V.", ""], ["Zioulis", "N.", ""], ["Albanis", "G.", ""], ["Zarpalas", "D.", ""]]}, {"id": "2106.00451", "submitter": "Fan Huang", "authors": "Fan Huang", "title": "Highlight Timestamp Detection Model for Comedy Videos via Multimodal\n  Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the videos on the Internet are prevailing. The precise and in-depth\nunderstanding of the videos is a difficult but valuable problem for both\nplatforms and researchers. The existing video understand models do well in\nobject recognition tasks but currently still cannot understand the abstract and\ncontextual features like highlight humor frames in comedy videos. The current\nindustrial works are also mainly focused on the basic category classification\ntask based on the appearances of objects. The feature detection methods for the\nabstract category remains blank. A data structure that includes the information\nof video frames, audio spectrum and texts provide a new direction to explore.\nThe multimodal models are proposed to make this in-depth video understanding\nmission possible. In this paper, we analyze the difficulties in abstract\nunderstanding of videos and propose a multimodal structure to obtain\nstate-of-the-art performance in this field. Then we select several benchmarks\nfor multimodal video understanding and apply the most suitable model to find\nthe best performance. At last, we evaluate the overall spotlights and drawbacks\nof the models and methods in this paper and point out the possible directions\nfor further improvements.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 08:39:19 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Huang", "Fan", ""]]}, {"id": "2106.00472", "submitter": "Dario Fontanel", "authors": "Dario Fontanel, Fabio Cermelli, Massimiliano Mancini, Barbara Caputo", "title": "Detecting Anomalies in Semantic Segmentation with Prototypes", "comments": "SAIAD CVPR21 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional semantic segmentation methods can recognize at test time only the\nclasses that are present in the training set. This is a significant limitation,\nespecially for semantic segmentation algorithms mounted on intelligent\nautonomous systems, deployed in realistic settings. Regardless of how many\nclasses the system has seen at training time, it is inevitable that unexpected,\nunknown objects will appear at test time. The failure in identifying such\nanomalies may lead to incorrect, even dangerous behaviors of the autonomous\nagent equipped with such segmentation model when deployed in the real world.\nCurrent state of the art of anomaly segmentation uses generative models,\nexploiting their incapability to reconstruct patterns unseen during training.\nHowever, training these models is expensive, and their generated artifacts may\ncreate false anomalies. In this paper we take a different route and we propose\nto address anomaly segmentation through prototype learning. Our intuition is\nthat anomalous pixels are those that are dissimilar to all class prototypes\nknown by the model. We extract class prototypes from the training data in a\nlightweight manner using a cosine similarity-based classifier. Experiments on\nStreetHazards show that our approach achieves the new state of the art, with a\nsignificant margin over previous works, despite the reduced computational\noverhead. Code is available at https://github.com/DarioFontanel/PAnS.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:22:33 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Fontanel", "Dario", ""], ["Cermelli", "Fabio", ""], ["Mancini", "Massimiliano", ""], ["Caputo", "Barbara", ""]]}, {"id": "2106.00487", "submitter": "Yingqian Wang", "authors": "Boyang Li, Chao Xiao, Longguang Wang, Yingqian Wang, Zaiping Lin, Miao\n  Li, Wei An, Yulan Guo", "title": "Dense Nested Attention Network for Infrared Small Target Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-frame infrared small target (SIRST) detection aims at separating small\ntargets from clutter backgrounds. With the advances of deep learning, CNN-based\nmethods have yielded promising results in generic object detection due to their\npowerful modeling capability. However, existing CNN-based methods cannot be\ndirectly applied for infrared small targets since pooling layers in their\nnetworks could lead to the loss of targets in deep layers. To handle this\nproblem, we propose a dense nested attention network (DNANet) in this paper.\nSpecifically, we design a dense nested interactive module (DNIM) to achieve\nprogressive interaction among high-level and low-level features. With the\nrepeated interaction in DNIM, infrared small targets in deep layers can be\nmaintained. Based on DNIM, we further propose a cascaded channel and spatial\nattention module (CSAM) to adaptively enhance multi-level features. With our\nDNANet, contextual information of small targets can be well incorporated and\nfully exploited by repeated fusion and enhancement. Moreover, we develop an\ninfrared small target dataset (namely, NUDT-SIRST) and propose a set of\nevaluation metrics to conduct comprehensive performance evaluation. Experiments\non both public and our self-developed datasets demonstrate the effectiveness of\nour method. Compared to other state-of-the-art methods, our method achieves\nbetter performance in terms of probability of detection (Pd), false-alarm rate\n(Fa), and intersection of union (IoU).\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:45:35 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Li", "Boyang", ""], ["Xiao", "Chao", ""], ["Wang", "Longguang", ""], ["Wang", "Yingqian", ""], ["Lin", "Zaiping", ""], ["Li", "Miao", ""], ["An", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "2106.00496", "submitter": "Lili Zhao", "authors": "Lili Zhao, Zezhi Zhu, Xuhu Lin, Xuezhou Guo, Qian Yin, Wenyi Wang,\n  Jianwen Chen", "title": "RAI-Net: Range-Adaptive LiDAR Point Cloud Frame Interpolation Network", "comments": "Accepted by the IEEE International Symposium on Broadband Multimedia\n  Systems and Broadcasting 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR point cloud frame interpolation, which synthesizes the intermediate\nframe between the captured frames, has emerged as an important issue for many\napplications. Especially for reducing the amounts of point cloud transmission,\nit is by predicting the intermediate frame based on the reference frames to\nupsample data to high frame rate ones. However, due to high-dimensional and\nsparse characteristics of point clouds, it is more difficult to predict the\nintermediate frame for LiDAR point clouds than videos. In this paper, we\npropose a novel LiDAR point cloud frame interpolation method, which exploits\nrange images (RIs) as an intermediate representation with CNNs to conduct the\nframe interpolation process. Considering the inherited characteristics of RIs\ndiffer from that of color images, we introduce spatially adaptive convolutions\nto extract range features adaptively, while a high-efficient flow estimation\nmethod is presented to generate optical flows. The proposed model then warps\nthe input frames and range features, based on the optical flows to synthesize\nthe interpolated frame. Extensive experiments on the KITTI dataset have clearly\ndemonstrated that our method consistently achieves superior frame interpolation\nresults with better perceptual quality to that of using state-of-the-art video\nframe interpolation methods. The proposed method could be integrated into any\nLiDAR point cloud compression systems for inter prediction.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:59:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhao", "Lili", ""], ["Zhu", "Zezhi", ""], ["Lin", "Xuhu", ""], ["Guo", "Xuezhou", ""], ["Yin", "Qian", ""], ["Wang", "Wenyi", ""], ["Chen", "Jianwen", ""]]}, {"id": "2106.00506", "submitter": "Gencer Sumbul", "authors": "Gencer Sumbul and Beg\\\"um Demir", "title": "A Novel Graph-Theoretic Deep Representation Learning Method for\n  Multi-Label Remote Sensing Image Retrieval", "comments": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2021. Our code is available at\n  https://git.tu-berlin.de/rsim/GT-DRL-CBIR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel graph-theoretic deep representation learning\nmethod in the framework of multi-label remote sensing (RS) image retrieval\nproblems. The proposed method aims to extract and exploit multi-label\nco-occurrence relationships associated to each RS image in the archive. To this\nend, each training image is initially represented with a graph structure that\nprovides region-based image representation combining both local information and\nthe related spatial organization. Unlike the other graph-based methods, the\nproposed method contains a novel learning strategy to train a deep neural\nnetwork for automatically predicting a graph structure of each RS image in the\narchive. This strategy employs a region representation learning loss function\nto characterize the image content based on its multi-label co-occurrence\nrelationship. Experimental results show the effectiveness of the proposed\nmethod for retrieval problems in RS compared to state-of-the-art deep\nrepresentation learning methods. The code of the proposed method is publicly\navailable at https://git.tu-berlin.de/rsim/GT-DRL-CBIR .\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 14:11:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Sumbul", "Gencer", ""], ["Demir", "Beg\u00fcm", ""]]}, {"id": "2106.00515", "submitter": "Pichao Wang", "authors": "Pichao Wang and Xue Wang and Fan Wang and Ming Lin and Shuning Chang\n  and Wen Xie and Hao Li and Rong Jin", "title": "KVT: k-NN Attention for Boosting Vision Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have dominated computer vision for\nyears, due to its ability in capturing locality and translation invariance.\nRecently, many vision transformer architectures have been proposed and they\nshow promising performance. A key component in vision transformers is the\nfully-connected self-attention which is more powerful than CNNs in modelling\nlong range dependencies. However, since the current dense self-attention uses\nall image patches (tokens) to compute attention matrix, it may neglect locality\nof images patches and involve noisy tokens (e.g., clutter background and\nocclusion), leading to a slow training process and potentially degradation of\nperformance. To address these problems, we propose a sparse attention scheme,\ndubbed k-NN attention, for boosting vision transformers. Specifically, instead\nof involving all the tokens for attention matrix calculation, we only select\nthe top-k similar tokens from the keys for each query to compute the attention\nmap. The proposed k-NN attention naturally inherits the local bias of CNNs\nwithout introducing convolutional operations, as nearby tokens tend to be more\nsimilar than others. In addition, the k-NN attention allows for the exploration\nof long range correlation and at the same time filter out irrelevant tokens by\nchoosing the most similar tokens from the entire image. Despite its simplicity,\nwe verify, both theoretically and empirically, that $k$-NN attention is\npowerful in distilling noise from input tokens and in speeding up training.\nExtensive experiments are conducted by using ten different vision transformer\narchitectures to verify that the proposed k-NN attention can work with any\nexisting transformer architectures to improve its prediction performance.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:49:10 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Pichao", ""], ["Wang", "Xue", ""], ["Wang", "Fan", ""], ["Lin", "Ming", ""], ["Chang", "Shuning", ""], ["Xie", "Wen", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "2106.00537", "submitter": "Longhui Wei", "authors": "Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, Qi Tian", "title": "Exploring the Diversity and Invariance in Yourself for Visual\n  Pre-Training Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, self-supervised learning methods have achieved remarkable success\nin visual pre-training task. By simply pulling the different augmented views of\neach image together or other novel mechanisms, they can learn much unsupervised\nknowledge and significantly improve the transfer performance of pre-training\nmodels. However, these works still cannot avoid the representation collapse\nproblem, i.e., they only focus on limited regions or the extracted features on\ntotally different regions inside each image are nearly the same. Generally,\nthis problem makes the pre-training models cannot sufficiently describe the\nmulti-grained information inside images, which further limits the upper bound\nof their transfer performance. To alleviate this issue, this paper introduces a\nsimple but effective mechanism, called Exploring the Diversity and Invariance\nin Yourself E-DIY. By simply pushing the most different regions inside each\naugmented view away, E-DIY can preserve the diversity of extracted region-level\nfeatures. By pulling the most similar regions from different augmented views of\nthe same image together, E-DIY can ensure the robustness of region-level\nfeatures. Benefited from the above diversity and invariance exploring\nmechanism, E-DIY maximally extracts the multi-grained visual information inside\neach image. Extensive experiments on downstream tasks demonstrate the\nsuperiority of our proposed approach, e.g., there are 2.1% improvements\ncompared with the strong baseline BYOL on COCO while fine-tuning Mask R-CNN\nwith the R50-C4 backbone and 1X learning schedule.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 14:52:36 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wei", "Longhui", ""], ["Xie", "Lingxi", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""], ["Tian", "Qi", ""]]}, {"id": "2106.00557", "submitter": "David Ahmedt-Aristizabal", "authors": "Ruiqi Wang, Mohammad Ali Armin, Simon Denman, Lars Petersson, David\n  Ahmedt-Aristizabal", "title": "Towards Interpretable Attention Networks for Cervical Cancer Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have enabled the development of automated\nframeworks for analysing medical images and signals, including analysis of\ncervical cancer. Many previous works focus on the analysis of isolated cervical\ncells, or do not offer sufficient methods to explain and understand how the\nproposed models reach their classification decisions on multi-cell images.\nHere, we evaluate various state-of-the-art deep learning models and\nattention-based frameworks for the classification of images of multiple\ncervical cells. As we aim to provide interpretable deep learning models to\naddress this task, we also compare their explainability through the\nvisualization of their gradients. We demonstrate the importance of using images\nthat contain multiple cells over using isolated single-cell images. We show the\neffectiveness of the residual channel attention model for extracting important\nfeatures from a group of cells, and demonstrate this model's efficiency for\nthis classification task. This work highlights the benefits of channel\nattention mechanisms in analyzing multiple-cell images for potential relations\nand distributions within a group of cells. It also provides interpretable\nmodels to address the classification of cervical cells.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:28:24 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Ruiqi", ""], ["Armin", "Mohammad Ali", ""], ["Denman", "Simon", ""], ["Petersson", "Lars", ""], ["Ahmedt-Aristizabal", "David", ""]]}, {"id": "2106.00559", "submitter": "Alvaro Quintanar", "authors": "A. Quintanar, D. Fern\\'andez-Llorca, I. Parra, R. Izquierdo, M. A.\n  Sotelo", "title": "Predicting Vehicles Trajectories in Urban Scenarios with Transformer\n  Networks and Augmented Information", "comments": "This work has been accepted for publication at IEEE Intelligent\n  Vehicles Symposium 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behavior of road users is of vital importance for the\ndevelopment of trajectory prediction systems. In this context, the latest\nadvances have focused on recurrent structures, establishing the social\ninteraction between the agents involved in the scene. More recently, simpler\nstructures have also been introduced for predicting pedestrian trajectories,\nbased on Transformer Networks, and using positional information. They allow the\nindividual modelling of each agent's trajectory separately without any complex\ninteraction terms. Our model exploits these simple structures by adding\naugmented data (position and heading), and adapting their use to the problem of\nvehicle trajectory prediction in urban scenarios in prediction horizons up to 5\nseconds. In addition, a cross-performance analysis is performed between\ndifferent types of scenarios, including highways, intersections and\nroundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our\nmodel achieves state-of-the-art results and proves to be flexible and adaptable\nto different types of urban contexts.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:18:55 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 23:38:30 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Quintanar", "A.", ""], ["Fern\u00e1ndez-Llorca", "D.", ""], ["Parra", "I.", ""], ["Izquierdo", "R.", ""], ["Sotelo", "M. A.", ""]]}, {"id": "2106.00566", "submitter": "Jie Ou", "authors": "Jie Ou, Mingjian Chen, Hong Wu", "title": "Full-Resolution Encoder-Decoder Networks with Multi-Scale Feature Fusion\n  for Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To achieve more accurate 2D human pose estimation, we extend the successful\nencoder-decoder network, simple baseline network (SBN), in three ways. To\nreduce the quantization errors caused by the large output stride size, two more\ndecoder modules are appended to the end of the simple baseline network to get\nfull output resolution. Then, the global context blocks (GCBs) are added to the\nencoder and decoder modules to enhance them with global context features.\nFurthermore, we propose a novel spatial-attention-based multi-scale feature\ncollection and distribution module (SA-MFCD) to fuse and distribute multi-scale\nfeatures to boost the pose estimation. Experimental results on the MS COCO\ndataset indicate that our network can remarkably improve the accuracy of human\npose estimation over SBN, our network using ResNet34 as the backbone network\ncan even achieve the same accuracy as SBN with ResNet152, and our networks can\nachieve superior results with big backbone networks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:30:09 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Ou", "Jie", ""], ["Chen", "Mingjian", ""], ["Wu", "Hong", ""]]}, {"id": "2106.00572", "submitter": "Jian-Wei Zhang", "authors": "Jian-Wei Zhang, Lei Lv, Yawei Luo, Hao-Zhe Feng, Yi Yang, Wei Chen", "title": "Prior-Enhanced Few-Shot Segmentation with Meta-Prototypes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot segmentation~(FSS) performance has been extensively promoted by\nintroducing episodic training and class-wise prototypes. However, the FSS\nproblem remains challenging due to three limitations: (1) Models are distracted\nby task-unrelated information; (2) The representation ability of a single\nprototype is limited; (3) Class-related prototypes ignore the prior knowledge\nof base classes. We propose the Prior-Enhanced network with Meta-Prototypes to\ntackle these limitations. The prior-enhanced network leverages the support and\nquery (pseudo-) labels in feature extraction, which guides the model to focus\non the task-related features of the foreground objects, and suppress much noise\ndue to the lack of supervised knowledge. Moreover, we introduce multiple\nmeta-prototypes to encode hierarchical features and learn class-agnostic\nstructural information. The hierarchical features help the model highlight the\ndecision boundary and focus on hard pixels, and the structural information\nlearned from base classes is treated as the prior knowledge for novel classes.\nExperiments show that our method achieves the mean-IoU scores of 60.79% and\n41.16% on PASCAL-$5^i$ and COCO-$20^i$, outperforming the state-of-the-art\nmethod by 3.49% and 5.64% in the 5-shot setting. Moreover, comparing with\n1-shot results, our method promotes 5-shot accuracy by 3.73% and 10.32% on the\nabove two benchmarks. The source code of our method is available at\nhttps://github.com/Jarvis73/PEMP.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:34:30 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhang", "Jian-Wei", ""], ["Lv", "Lei", ""], ["Luo", "Yawei", ""], ["Feng", "Hao-Zhe", ""], ["Yang", "Yi", ""], ["Chen", "Wei", ""]]}, {"id": "2106.00576", "submitter": "Isaac Dunn", "authors": "Isaac Dunn, Hadrien Pouget, Daniel Kroening and Tom Melham", "title": "Exposing Previously Undetectable Faults in Deep Neural Networks", "comments": "Accepted to the ACM SIGSOFT International Symposium on Software\n  Testing and Analysis (ISSTA 2021)", "journal-ref": null, "doi": "10.1145/3460319.3464801", "report-no": null, "categories": "cs.LG cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for testing DNNs solve the oracle problem by constraining\nthe raw features (e.g. image pixel values) to be within a small distance of a\ndataset example for which the desired DNN output is known. But this limits the\nkinds of faults these approaches are able to detect. In this paper, we\nintroduce a novel DNN testing method that is able to find faults in DNNs that\nother methods cannot. The crux is that, by leveraging generative machine\nlearning, we can generate fresh test inputs that vary in their high-level\nfeatures (for images, these include object shape, location, texture, and\ncolour). We demonstrate that our approach is capable of detecting deliberately\ninjected faults as well as new faults in state-of-the-art DNNs, and that in\nboth cases, existing methods are unable to find these faults.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:37:30 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Dunn", "Isaac", ""], ["Pouget", "Hadrien", ""], ["Kroening", "Daniel", ""], ["Melham", "Tom", ""]]}, {"id": "2106.00588", "submitter": "Jianbiao Mei", "authors": "Jianbiao Mei, Mengmeng Wang, Yeneng Lin, Yong Liu", "title": "TransVOS: Video Object Segmentation with Transformers", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Space-Time Memory Network (STM) based methods have achieved\nstate-of-the-art performance in semi-supervised video object segmentation\n(VOS). A critical problem in this task is how to model the dependency both\namong different frames and inside every frame. However, most of these methods\nneglect the spatial relationships (inside each frame) and do not make full use\nof the temporal relationships (among different frames). In this paper, we\npropose a new transformer-based framework, termed TransVOS, introducing a\nvision transformer to fully exploit and model both the temporal and spatial\nrelationships. Moreover, most STM-based approaches employ two disparate\nencoders to extract features of two significant inputs, i.e., reference sets\n(history frames with predicted masks) and query frame, respectively, increasing\nthe models' parameters and complexity. To slim the popular two-encoder pipeline\nwhile keeping the effectiveness, we design a single two-path feature extractor\nto encode the above two inputs in a unified way. Extensive experiments\ndemonstrate the superiority of our TransVOS over state-of-the-art methods on\nboth DAVIS and YouTube-VOS datasets. Codes will be released when it is\npublished.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:56:10 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Mei", "Jianbiao", ""], ["Wang", "Mengmeng", ""], ["Lin", "Yeneng", ""], ["Liu", "Yong", ""]]}, {"id": "2106.00592", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Chen Change Loy, Ziwei Liu", "title": "Semi-Supervised Domain Generalization with Stochastic StyleMatch", "comments": "Tech report. Code available at\n  https://github.com/KaiyangZhou/ssdg-benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing research on domain generalization assumes source data gathered\nfrom multiple domains are fully annotated. However, in real-world applications,\nwe might have only a few labels available from each source domain due to high\nannotation cost, along with abundant unlabeled data that are much easier to\nobtain. In this work, we investigate semi-supervised domain generalization\n(SSDG), a more realistic and practical setting. Our proposed approach,\nStyleMatch, is inspired by FixMatch, a state-of-the-art semi-supervised\nlearning method based on pseudo-labeling, with several new ingredients tailored\nto solve SSDG. Specifically, 1) to mitigate overfitting in the scarce labeled\nsource data while improving robustness against noisy pseudo labels, we\nintroduce stochastic modeling to the classifier's weights, seen as class\nprototypes, with Gaussian distributions. 2) To enhance generalization under\ndomain shift, we upgrade FixMatch's two-view consistency learning paradigm\nbased on weak and strong augmentations to a multi-view version with style\naugmentation as the third complementary view. To provide a comprehensive study\nand evaluation, we establish two SSDG benchmarks, which cover a wide range of\nstrong baseline methods developed in relevant areas including domain\ngeneralization and semi-supervised learning. Extensive experiments demonstrate\nthat StyleMatch achieves the best out-of-distribution generalization\nperformance in the low-data regime. We hope our approach and benchmarks can\npave the way for future research on data-efficient and generalizable learning\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 16:00:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Loy", "Chen Change", ""], ["Liu", "Ziwei", ""]]}, {"id": "2106.00596", "submitter": "Van-Quang Nguyen", "authors": "Van-Quang Nguyen, Masanori Suganuma, Takayuki Okatani", "title": "Look Wide and Interpret Twice: Improving Performance on Interactive\n  Instruction-following Tasks", "comments": "To appear in IJCAI2021. 8-page main paper and Appendix following.\n  Appendix E for details of entry submission to EAI 2021. Github:\n  https://github.com/davidnvq/lwit-alfred", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing interest in the community in making an embodied AI agent\nperform a complicated task while interacting with an environment following\nnatural language directives. Recent studies have tackled the problem using\nALFRED, a well-designed dataset for the task, but achieved only very low\naccuracy. This paper proposes a new method, which outperforms the previous\nmethods by a large margin. It is based on a combination of several new ideas.\nOne is a two-stage interpretation of the provided instructions. The method\nfirst selects and interprets an instruction without using visual information,\nyielding a tentative action sequence prediction. It then integrates the\nprediction with the visual information etc., yielding the final prediction of\nan action and an object. As the object's class to interact is identified in the\nfirst stage, it can accurately select the correct object from the input image.\nMoreover, our method considers multiple egocentric views of the environment and\nextracts essential information by applying hierarchical attention conditioned\non the current instruction. This contributes to the accurate prediction of\nactions for navigation. A preliminary version of the method won the ALFRED\nChallenge 2020. The current version achieves the unseen environment's success\nrate of 4.45% with a single view, which is further improved to 8.37% with\nmultiple views.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 16:06:09 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 14:38:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Nguyen", "Van-Quang", ""], ["Suganuma", "Masanori", ""], ["Okatani", "Takayuki", ""]]}, {"id": "2106.00598", "submitter": "Ezechukwu Nwokedi", "authors": "Ezechukwu I Nwokedi, Rasneer S Bains, Luc Bidaut, Sara Wells, Xujiong\n  Ye, James M Brown", "title": "Unsupervised detection of mouse behavioural anomalies using two-stream\n  convolutional autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the application of unsupervised learning to detecting\nanomalies in mouse video data. The two models presented in this paper are a\ndual-stream, 3D convolutional autoencoder (with residual connections) and a\ndual-stream, 2D convolutional autoencoder. The publicly available dataset used\nhere contains twelve videos of single home-caged mice alongside frame-level\nannotations. Under the pretext that the autoencoder only sees normal events,\nthe video data was handcrafted to treat each behaviour as a pseudo-anomaly\nthereby eliminating them from the others during training. The results are\npresented for one conspicuous behaviour (hang) and one inconspicuous behaviour\n(groom). The performance of these models is compared to a single stream\nautoencoder and a supervised learning model, which are both based on the custom\nCAE. Both models are also tested on the CUHK Avenue dataset were found to\nperform as well as some state-of-the-art architectures.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:30:09 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Nwokedi", "Ezechukwu I", ""], ["Bains", "Rasneer S", ""], ["Bidaut", "Luc", ""], ["Wells", "Sara", ""], ["Ye", "Xujiong", ""], ["Brown", "James M", ""]]}, {"id": "2106.00609", "submitter": "Pan Zhang", "authors": "Pan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Fang Wen", "title": "Robust Mutual Learning for Semi-supervised Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent semi-supervised learning (SSL) methods are commonly based on pseudo\nlabeling. Since the SSL performance is greatly influenced by the quality of\npseudo labels, mutual learning has been proposed to effectively suppress the\nnoises in the pseudo supervision. In this work, we propose robust mutual\nlearning that improves the prior approach in two aspects. First, the vanilla\nmutual learners suffer from the coupling issue that models may converge to\nlearn homogeneous knowledge. We resolve this issue by introducing mean teachers\nto generate mutual supervisions so that there is no direct interaction between\nthe two students. We also show that strong data augmentations, model noises and\nheterogeneous network architectures are essential to alleviate the model\ncoupling. Second, we notice that mutual learning fails to leverage the\nnetwork's own ability for pseudo label refinement. Therefore, we introduce\nself-rectification that leverages the internal knowledge and explicitly\nrectifies the pseudo labels before the mutual teaching. Such self-rectification\nand mutual teaching collaboratively improve the pseudo label accuracy\nthroughout the learning. The proposed robust mutual learning demonstrates\nstate-of-the-art performance on semantic segmentation in low-data regime.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 16:22:01 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhang", "Pan", ""], ["Zhang", "Bo", ""], ["Zhang", "Ting", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""]]}, {"id": "2106.00629", "submitter": "Dario Oliveira", "authors": "Dario Augusto Borges Oliveira", "title": "Decoupling Shape and Density for Liver Lesion Synthesis Using\n  Conditional Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion synthesis received much attention with the rise of efficient\ngenerative models for augmenting training data, drawing lesion evolution\nscenarios, or aiding expert training. The quality and diversity of synthesized\ndata are highly dependent on the annotated data used to train the models, which\nnot rarely struggle to derive very different yet realistic samples from the\ntraining ones. That adds an inherent bias to lesion segmentation algorithms and\nlimits synthesizing lesion evolution scenarios efficiently. This paper presents\na method for decoupling shape and density for liver lesion synthesis, creating\na framework that allows straight-forwardly driving the synthesis. We offer\nqualitative results that show the synthesis control by modifying shape and\ndensity individually, and quantitative results that demonstrate that embedding\nthe density information in the generator model helps to increase lesion\nsegmentation performance compared to using the shape solely.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 16:45:19 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Oliveira", "Dario Augusto Borges", ""]]}, {"id": "2106.00638", "submitter": "Zhiliang Wu", "authors": "Zhiliang Wu, Yinchong Yang, Jindong Gu, Volker Tresp", "title": "Quantifying Predictive Uncertainty in Medical Image Analysis with Deep\n  Kernel Learning", "comments": null, "journal-ref": "2021 IEEE International Conference on Healthcare Informatics\n  (ICHI)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are increasingly being used for the analysis of medical\nimages. However, most works neglect the uncertainty in the model's prediction.\nWe propose an uncertainty-aware deep kernel learning model which permits the\nestimation of the uncertainty in the prediction by a pipeline of a\nConvolutional Neural Network and a sparse Gaussian Process. Furthermore, we\nadapt different pre-training methods to investigate their impacts on the\nproposed model. We apply our approach to Bone Age Prediction and Lesion\nLocalization. In most cases, the proposed model shows better performance\ncompared to common architectures. More importantly, our model expresses\nsystematically higher confidence in more accurate predictions and less\nconfidence in less accurate ones. Our model can also be used to detect\nchallenging and controversial test samples. Compared to related methods such as\nMonte-Carlo Dropout, our approach derives the uncertainty information in a\npurely analytical fashion and is thus computationally more efficient.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:09:47 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wu", "Zhiliang", ""], ["Yang", "Yinchong", ""], ["Gu", "Jindong", ""], ["Tresp", "Volker", ""]]}, {"id": "2106.00645", "submitter": "Giorgio Luigi Morales Luna", "authors": "Giorgio Morales and John Sheppard and Riley Logan and Joseph Shaw", "title": "Hyperspectral Band Selection for Multispectral Image Classification with\n  Convolutional Networks", "comments": "Accepted to appear in the International Joint Conference on Neural\n  Networks 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, Hyperspectral Imaging (HSI) has become a powerful source for\nreliable data in applications such as remote sensing, agriculture, and\nbiomedicine. However, hyperspectral images are highly data-dense and often\nbenefit from methods to reduce the number of spectral bands while retaining the\nmost useful information for a specific application. We propose a novel band\nselection method to select a reduced set of wavelengths, obtained from an HSI\nsystem in the context of image classification. Our approach consists of two\nmain steps: the first utilizes a filter-based approach to find relevant\nspectral bands based on a collinearity analysis between a band and its\nneighbors. This analysis helps to remove redundant bands and dramatically\nreduces the search space. The second step applies a wrapper-based approach to\nselect bands from the reduced set based on their information entropy values,\nand trains a compact Convolutional Neural Network (CNN) to evaluate the\nperformance of the current selection. We present classification results\nobtained from our method and compare them to other feature selection methods on\ntwo hyperspectral image datasets. Additionally, we use the original\nhyperspectral data cube to simulate the process of using actual filters in a\nmultispectral imager. We show that our method produces more suitable results\nfor a multispectral sensor design.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:24:35 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Morales", "Giorgio", ""], ["Sheppard", "John", ""], ["Logan", "Riley", ""], ["Shaw", "Joseph", ""]]}, {"id": "2106.00652", "submitter": "Da Ma", "authors": "Da Ma, Vincent Chow, Karteek Popuri, Mirza Faisal Beg", "title": "Comprehensive Validation of Automated Whole Body Skeletal Muscle,\n  Adipose Tissue, and Bone Segmentation from 3D CT images for Body Composition\n  Analysis: Towards Extended Body Composition", "comments": "This paper is based on concepts presented at the NIH Body Composition\n  and Cancer Outcomes Research Webinar Series on December 17th, 2020 by Mirza\n  Faisal Beg titled \"Automating Body Composition from Routinely Acquired CT\n  images - towards 3D measurements\". The talk is archived\n  [here](https://epi.grants.cancer.gov/events/body-composition/#past)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest advances in computer-assisted precision medicine are making it\nfeasible to move from population-wide models that are useful to discover\naggregate patterns that hold for group-based analysis to patient-specific\nmodels that can drive patient-specific decisions with regard to treatment\nchoices, and predictions of outcomes of treatment. Body Composition is\nrecognized as an important driver and risk factor for a wide variety of\ndiseases, as well as a predictor of individual patient-specific clinical\noutcomes to treatment choices or surgical interventions. 3D CT images are\nroutinely acquired in the oncological worklows and deliver accurate rendering\nof internal anatomy and therefore can be used opportunistically to assess the\namount of skeletal muscle and adipose tissue compartments. Powerful tools of\nartificial intelligence such as deep learning are making it feasible now to\nsegment the entire 3D image and generate accurate measurements of all internal\nanatomy. These will enable the overcoming of the severe bottleneck that existed\npreviously, namely, the need for manual segmentation, which was prohibitive to\nscale to the hundreds of 2D axial slices that made up a 3D volumetric image.\nAutomated tools such as presented here will now enable harvesting whole-body\nmeasurements from 3D CT or MRI images, leading to a new era of discovery of the\ndrivers of various diseases based on individual tissue, organ volume, shape,\nand functional status. These measurements were hitherto unavailable thereby\nlimiting the field to a very small and limited subset. These discoveries and\nthe potential to perform individual image segmentation with high speed and\naccuracy are likely to lead to the incorporation of these 3D measures into\nindividual specific treatment planning models related to nutrition, aging,\nchemotoxicity, surgery and survival after the onset of a major disease such as\ncancer.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:30:45 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 07:14:35 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 01:13:06 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ma", "Da", ""], ["Chow", "Vincent", ""], ["Popuri", "Karteek", ""], ["Beg", "Mirza Faisal", ""]]}, {"id": "2106.00660", "submitter": "Ilia Shumailov", "authors": "David Khachaturov, Ilia Shumailov, Yiren Zhao, Nicolas Papernot, Ross\n  Anderson", "title": "Markpainting: Adversarial Machine Learning meets Inpainting", "comments": "Proceedings of the 38th International Conference on Machine Learning\n  (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inpainting is a learned interpolation technique that is based on generative\nmodeling and used to populate masked or missing pieces in an image; it has wide\napplications in picture editing and retouching. Recently, inpainting started\nbeing used for watermark removal, raising concerns. In this paper we study how\nto manipulate it using our markpainting technique. First, we show how an image\nowner with access to an inpainting model can augment their image in such a way\nthat any attempt to edit it using that model will add arbitrary visible\ninformation. We find that we can target multiple different models\nsimultaneously with our technique. This can be designed to reconstitute a\nwatermark if the editor had been trying to remove it. Second, we show that our\nmarkpainting technique is transferable to models that have different\narchitectures or were trained on different datasets, so watermarks created\nusing it are difficult for adversaries to remove. Markpainting is novel and can\nbe used as a manipulation alarm that becomes visible in the event of\ninpainting.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:45:52 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Khachaturov", "David", ""], ["Shumailov", "Ilia", ""], ["Zhao", "Yiren", ""], ["Papernot", "Nicolas", ""], ["Anderson", "Ross", ""]]}, {"id": "2106.00666", "submitter": "Yuxin Fang", "authors": "Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui\n  Wu, Jianwei Niu, Wenyu Liu", "title": "You Only Look at One Sequence: Rethinking Transformer in Vision through\n  Object Detection", "comments": "18 pages, 7 tables, 5 figures. Add Appendix & some missing references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can Transformer perform $2\\mathrm{D}$ object-level recognition from a pure\nsequence-to-sequence perspective with minimal knowledge about the $2\\mathrm{D}$\nspatial structure? To answer this question, we present You Only Look at One\nSequence (YOLOS), a series of object detection models based on the na\\\"ive\nVision Transformer with the fewest possible modifications as well as inductive\nbiases. We find that YOLOS pre-trained on the mid-sized ImageNet-$1k$ dataset\nonly can already achieve competitive object detection performance on COCO,\n\\textit{e.g.}, YOLOS-Base directly adopted from BERT-Base can achieve $42.0$\nbox AP. We also discuss the impacts as well as limitations of current pre-train\nschemes and model scaling strategies for Transformer in vision through object\ndetection. Code and model weights are available at\n\\url{https://github.com/hustvl/YOLOS}.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:54:09 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 02:28:30 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fang", "Yuxin", ""], ["Liao", "Bencheng", ""], ["Wang", "Xinggang", ""], ["Fang", "Jiemin", ""], ["Qi", "Jiyang", ""], ["Wu", "Rui", ""], ["Niu", "Jianwei", ""], ["Liu", "Wenyu", ""]]}, {"id": "2106.00671", "submitter": "Ashvin Nair", "authors": "Alexander Khazatsky, Ashvin Nair, Daniel Jing, Sergey Levine", "title": "What Can I Do Here? Learning New Skills by Imagining Visual Affordances", "comments": "10 pages, 10 figures. Presented at ICRA 2021. Project website:\n  https://sites.google.com/view/val-rl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalist robot equipped with learned skills must be able to perform many\ntasks in many different environments. However, zero-shot generalization to new\nsettings is not always possible. When the robot encounters a new environment or\nobject, it may need to finetune some of its previously learned skills to\naccommodate this change. But crucially, previously learned behaviors and models\nshould still be suitable to accelerate this relearning. In this paper, we aim\nto study how generative models of possible outcomes can allow a robot to learn\nvisual representations of affordances, so that the robot can sample potentially\npossible outcomes in new situations, and then further train its policy to\nachieve those outcomes. In effect, prior data is used to learn what kinds of\noutcomes may be possible, such that when the robot encounters an unfamiliar\nsetting, it can sample potential outcomes from its model, attempt to reach\nthem, and thereby update both its skills and its outcome model. This approach,\nvisuomotor affordance learning (VAL), can be used to train goal-conditioned\npolicies that operate on raw image inputs, and can rapidly learn to manipulate\nnew objects via our proposed affordance-directed exploration scheme. We show\nthat VAL can utilize prior data to solve real-world tasks such drawer opening,\ngrasping, and placing objects in new scenes with only five minutes of online\nexperience in the new scene.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:58:02 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 01:30:53 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Khazatsky", "Alexander", ""], ["Nair", "Ashvin", ""], ["Jing", "Daniel", ""], ["Levine", "Sergey", ""]]}, {"id": "2106.00673", "submitter": "Xiaoyu Lin", "authors": "Xiaoyu Lin, Deblina Bhattacharjee, Majed El Helou and Sabine\n  S\\\"usstrunk", "title": "Fidelity Estimation Improves Noisy-Image Classification with Pretrained\n  Networks", "comments": "Submitted to IEEE SPL for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification has significantly improved using deep learning. This is\nmainly due to convolutional neural networks (CNNs) that are capable of learning\nrich feature extractors from large datasets. However, most deep learning\nclassification methods are trained on clean images and are not robust when\nhandling noisy ones, even if a restoration preprocessing step is applied. While\nnovel methods address this problem, they rely on modified feature extractors\nand thus necessitate retraining. We instead propose a method that can be\napplied on a pretrained classifier. Our method exploits a fidelity map estimate\nthat is fused into the internal representations of the feature extractor,\nthereby guiding the attention of the network and making it more robust to noisy\ndata. We improve the noisy-image classification (NIC) results by significantly\nlarge margins, especially at high noise levels, and come close to the fully\nretrained approaches. Furthermore, as proof of concept, we show that when using\nour oracle fidelity map we even outperform the fully retrained methods, whether\ntrained on noisy or restored images.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:58:32 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lin", "Xiaoyu", ""], ["Bhattacharjee", "Deblina", ""], ["Helou", "Majed El", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2106.00676", "submitter": "Zejiang Shen", "authors": "Zejiang Shen, Kyle Lo, Lucy Lu Wang, Bailey Kuehl, Daniel S. Weld,\n  Doug Downey", "title": "Incorporating Visual Layout Structures for Scientific Text\n  Classification", "comments": "13 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classifying the core textual components of a scientific paper-title, author,\nbody text, etc.-is a critical first step in automated scientific document\nunderstanding. Previous work has shown how using elementary layout information,\ni.e., each token's 2D position on the page, leads to more accurate\nclassification. We introduce new methods for incorporating VIsual LAyout (VILA)\nstructures, e.g., the grouping of page texts into text lines or text blocks,\ninto language models to further improve performance. We show that the I-VILA\napproach, which simply adds special tokens denoting the boundaries of layout\nstructures into model inputs, can lead to 1.9% Macro F1 improvements for token\nclassification. Moreover, we design a hierarchical model, H-VILA, that encodes\nthe text based on layout structures and record an up-to 47% inference time\nreduction with less than 1.5% Macro F1 loss for the text classification models.\nExperiments are conducted on a newly curated evaluation suite, S2-VLUE, with a\nnovel metric measuring classification uniformity within visual groups and a new\ndataset of gold annotations covering papers from 19 scientific disciplines.\nPre-trained weights, benchmark datasets, and source code will be available at\nhttps://github.com/allenai/VILA.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:59:00 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:35:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shen", "Zejiang", ""], ["Lo", "Kyle", ""], ["Wang", "Lucy Lu", ""], ["Kuehl", "Bailey", ""], ["Weld", "Daniel S.", ""], ["Downey", "Doug", ""]]}, {"id": "2106.00677", "submitter": "Mohamed El Banani", "authors": "Mohamed El Banani, Justin Johnson", "title": "Bootstrap Your Own Correspondences", "comments": "Preprint. 10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric feature extraction is a crucial component of point cloud\nregistration pipelines. Recent work has demonstrated how supervised learning\ncan be leveraged to learn better and more compact 3D features. However, those\napproaches' reliance on ground-truth annotation limits their scalability. We\npropose BYOC: a self-supervised approach that learns visual and geometric\nfeatures from RGB-D video without relying on ground-truth pose or\ncorrespondence. Our key observation is that randomly-initialized CNNs readily\nprovide us with good correspondences; allowing us to bootstrap the learning of\nboth visual and geometric features. Our approach combines classic ideas from\npoint cloud registration with more recent representation learning approaches.\nWe evaluate our approach on indoor scene datasets and find that our method\noutperforms traditional and learned descriptors, while being competitive with\ncurrent state-of-the-art supervised approaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:59:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Banani", "Mohamed El", ""], ["Johnson", "Justin", ""]]}, {"id": "2106.00728", "submitter": "Md Sadman Sakib", "authors": "Md Sadman Sakib, Hailey Baez, David Paulius, and Yu Sun", "title": "Evaluating Recipes Generated from Functional Object-Oriented Network", "comments": "This manuscript has been accepted at Ubiquitous Robots 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The functional object-oriented network (FOON) has been introduced as a\nknowledge representation, which takes the form of a graph, for symbolic task\nplanning. To get a sequential plan for a manipulation task, a robot can obtain\na task tree through a knowledge retrieval process from the FOON. To evaluate\nthe quality of an acquired task tree, we compare it with a conventional form of\ntask knowledge, such as recipes or manuals. We first automatically convert task\ntrees to recipes, and we then compare them with the human-created recipes in\nthe Recipe1M+ dataset via a survey. Our preliminary study finds no significant\ndifference between the recipes in Recipe1M+ and the recipes generated from FOON\ntask trees in terms of correctness, completeness, and clarity.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 19:00:52 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Sakib", "Md Sadman", ""], ["Baez", "Hailey", ""], ["Paulius", "David", ""], ["Sun", "Yu", ""]]}, {"id": "2106.00739", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Ruben Vera-Rodriguez, Carlos Gonzalez-Garcia, Julian\n  Fierrez, Santiago Rengifo, Aythami Morales, Javier Ortega-Garcia, Juan Carlos\n  Ruiz-Garcia, Sergio Romero-Tapiador, Jiajia Jiang, Songxuan Lai, Lianwen Jin,\n  Yecheng Zhu, Javier Galbally, Moises Diaz, Miguel Angel Ferrer, Marta\n  Gomez-Barrero, Ilya Hodashinsky, Konstantin Sarin, Artem Slezkin, Marina\n  Bardamova, Mikhail Svetlakov, Mohammad Saleem, Cintia Lia Sz\\\"ucs, Bence\n  Kovari, Falk Pulsmeyer, Mohamad Wehbi, Dario Zanca, Sumaiya Ahmad, Sarthak\n  Mishra and Suraiya Jabin", "title": "ICDAR 2021 Competition on On-Line Signature Verification", "comments": null, "journal-ref": "Proc. International Conference on Document Analysis and\n  Recognition 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes the experimental framework and results of the ICDAR 2021\nCompetition on On-Line Signature Verification (SVC 2021). The goal of SVC 2021\nis to evaluate the limits of on-line signature verification systems on popular\nscenarios (office/mobile) and writing inputs (stylus/finger) through\nlarge-scale public databases. Three different tasks are considered in the\ncompetition, simulating realistic scenarios as both random and skilled\nforgeries are simultaneously considered on each task. The results obtained in\nSVC 2021 prove the high potential of deep learning methods. In particular, the\nbest on-line signature verification system of SVC 2021 obtained Equal Error\nRate (EER) values of 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3).\n  SVC 2021 will be established as an on-going competition, where researchers\ncan easily benchmark their systems against the state of the art in an open\ncommon platform using large-scale public databases such as DeepSignDB and\nSVC2021_EvalDB, and standard experimental protocols.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 19:33:46 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""], ["Gonzalez-Garcia", "Carlos", ""], ["Fierrez", "Julian", ""], ["Rengifo", "Santiago", ""], ["Morales", "Aythami", ""], ["Ortega-Garcia", "Javier", ""], ["Ruiz-Garcia", "Juan Carlos", ""], ["Romero-Tapiador", "Sergio", ""], ["Jiang", "Jiajia", ""], ["Lai", "Songxuan", ""], ["Jin", "Lianwen", ""], ["Zhu", "Yecheng", ""], ["Galbally", "Javier", ""], ["Diaz", "Moises", ""], ["Ferrer", "Miguel Angel", ""], ["Gomez-Barrero", "Marta", ""], ["Hodashinsky", "Ilya", ""], ["Sarin", "Konstantin", ""], ["Slezkin", "Artem", ""], ["Bardamova", "Marina", ""], ["Svetlakov", "Mikhail", ""], ["Saleem", "Mohammad", ""], ["Sz\u00fccs", "Cintia Lia", ""], ["Kovari", "Bence", ""], ["Pulsmeyer", "Falk", ""], ["Wehbi", "Mohamad", ""], ["Zanca", "Dario", ""], ["Ahmad", "Sumaiya", ""], ["Mishra", "Sarthak", ""], ["Jabin", "Suraiya", ""]]}, {"id": "2106.00783", "submitter": "Dario Fuoli", "authors": "Dario Fuoli, Luc Van Gool, and Radu Timofte", "title": "Fourier Space Losses for Efficient Perceptual Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many super-resolution (SR) models are optimized for high performance only and\ntherefore lack efficiency due to large model complexity. As large models are\noften not practical in real-world applications, we investigate and propose\nnovel loss functions, to enable SR with high perceptual quality from much more\nefficient models. The representative power for a given low-complexity generator\nnetwork can only be fully leveraged by strong guidance towards the optimal set\nof parameters. We show that it is possible to improve the performance of a\nrecently introduced efficient generator architecture solely with the\napplication of our proposed loss functions. In particular, we use a Fourier\nspace supervision loss for improved restoration of missing high-frequency (HF)\ncontent from the ground truth image and design a discriminator architecture\nworking directly in the Fourier domain to better match the target HF\ndistribution. We show that our losses' direct emphasis on the frequencies in\nFourier-space significantly boosts the perceptual image quality, while at the\nsame time retaining high restoration quality in comparison to previously\nproposed loss functions for this task. The performance is further improved by\nutilizing a combination of spatial and frequency domain losses, as both\nrepresentations provide complementary information during training. On top of\nthat, the trained generator achieves comparable results with and is 2.4x and\n48x faster than state-of-the-art perceptual SR methods RankSRGAN and SRFlow\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:34:52 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Fuoli", "Dario", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2106.00799", "submitter": "Laura Elena Cue La Rosa", "authors": "Laura Elena Cu\\'e La Rosa, Camile Sothe, Raul Queiroz Feitosa,\n  Cl\\'audia Maria de Almeida, Marcos Benedito Schimalski, Dario Augusto Borges\n  Oliveira", "title": "Multi-task fully convolutional network for tree species mapping in dense\n  forests using small training hyperspectral data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a multi-task fully convolutional architecture for tree\nspecies mapping in dense forests from sparse and scarce polygon-level\nannotations using hyperspectral UAV-borne data. Our model implements a partial\nloss function that enables dense tree semantic labeling outcomes from non-dense\ntraining samples, and a distance regression complementary task that enforces\ntree crown boundary constraints and substantially improves the model\nperformance. Our multi-task architecture uses a shared backbone network that\nlearns common representations for both tasks and two task-specific decoders,\none for the semantic segmentation output and one for the distance map\nregression. We report that introducing the complementary task boosts the\nsemantic segmentation performance compared to the single-task counterpart in up\nto 10% reaching an overall F1 score of 87.5% and an overall accuracy of 85.9%,\nachieving state-of-art performance for tree species classification in tropical\nforests.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 21:10:10 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["La Rosa", "Laura Elena Cu\u00e9", ""], ["Sothe", "Camile", ""], ["Feitosa", "Raul Queiroz", ""], ["de Almeida", "Cl\u00e1udia Maria", ""], ["Schimalski", "Marcos Benedito", ""], ["Oliveira", "Dario Augusto Borges", ""]]}, {"id": "2106.00815", "submitter": "Sunnie S. Y. Kim", "authors": "Vivien Nguyen and Sunnie S. Y. Kim", "title": "Cleaning and Structuring the Label Space of the iMet Collection 2020", "comments": "A shorter version of this work was accepted to the CVPR 2021 FGVC\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The iMet 2020 dataset is a valuable resource in the space of fine-grained art\nattribution recognition, but we believe it has yet to reach its true potential.\nWe document the unique properties of the dataset and observe that many of the\nattribute labels are noisy, more than is implied by the dataset description.\nOftentimes, there are also semantic relationships between the labels (e.g.,\nidentical, mutual exclusion, subsumption, overlap with uncertainty) which we\nbelieve are underutilized. We propose an approach to cleaning and structuring\nthe iMet 2020 labels, and discuss the implications and value of doing so.\nFurther, we demonstrate the benefits of our proposed approach through several\nexperiments. Our code and cleaned labels are available at\nhttps://github.com/sunniesuhyoung/iMet2020cleaned.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 21:36:26 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Nguyen", "Vivien", ""], ["Kim", "Sunnie S. Y.", ""]]}, {"id": "2106.00817", "submitter": "Michael Baumgartner", "authors": "Michael Baumgartner, Paul F. Jaeger, Fabian Isensee, Klaus H.\n  Maier-Hein", "title": "nnDetection: A Self-configuring Method for Medical Object Detection", "comments": "*Michael Baumgartner and Paul F. J\\\"ager contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous localisation and categorization of objects in medical images,\nalso referred to as medical object detection, is of high clinical relevance\nbecause diagnostic decisions often depend on rating of objects rather than e.g.\npixels. For this task, the cumbersome and iterative process of method\nconfiguration constitutes a major research bottleneck. Recently, nnU-Net has\ntackled this challenge for the task of image segmentation with great success.\nFollowing nnU-Net's agenda, in this work we systematize and automate the\nconfiguration process for medical object detection. The resulting\nself-configuring method, nnDetection, adapts itself without any manual\nintervention to arbitrary medical detection problems while achieving results en\npar with or superior to the state-of-the-art. We demonstrate the effectiveness\nof nnDetection on two public benchmarks, ADAM and LUNA16, and propose 10\nfurther medical object detection tasks on public data sets for comprehensive\nmethod evaluation. Code is at https://github.com/MIC-DKFZ/nnDetection .\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 21:55:03 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Baumgartner", "Michael", ""], ["Jaeger", "Paul F.", ""], ["Isensee", "Fabian", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "2106.00828", "submitter": "Emre Can Kaya", "authors": "Emre Can Kaya, Sebastian Schwarz, Ioan Tabus", "title": "Refining the bounding volumes for lossless compression of voxelized\n  point clouds geometry", "comments": "ICIP \\c{opyright} 2021 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses, in any\n  current or future media, including reprinting/republishing this material for\n  advertising or promotional purposes, creating new collective works, for\n  resale or redistribution to servers or lists, or reuse of any copyrighted\n  component of this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel lossless compression method for point cloud\ngeometry, building on a recent lossy compression method that aimed at\nreconstructing only the bounding volume of a point cloud. The proposed scheme\nstarts by partially reconstructing the geometry from the two depthmaps\nassociated to a single projection direction. The partial reconstruction\nobtained from the depthmaps is completed to a full reconstruction of the point\ncloud by sweeping section by section along one direction and encoding the\npoints which were not contained in the two depthmaps. The main ingredient is a\nlist-based encoding of the inner points (situated inside the feasible regions)\nby a novel arithmetic three dimensional context coding procedure that\nefficiently utilizes rotational invariances present in the input data.\nState-of-the-art bits-per-voxel results are obtained on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 22:16:06 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kaya", "Emre Can", ""], ["Schwarz", "Sebastian", ""], ["Tabus", "Ioan", ""]]}, {"id": "2106.00880", "submitter": "Pourya Shamsolmoali", "authors": "Pourya Shamsolmoali, Masoumeh Zareapoor, Jocelyn Chanussot, Huiyu\n  Zhou, and Jie Yang", "title": "Rotation Equivariant Feature Image Pyramid Network for Object Detection\n  in Optical Remote Sensing Imagery", "comments": "We submitted the old version of the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, there has been substantial progress in object\ndetection on remote sensing images (RSIs) where objects are generally\ndistributed with large-scale variations and have different types of\norientations. Nevertheless, most of the current convolution neural network\napproaches lack the ability to deal with the challenges such as size and\nrotation variations. To address these problems, we propose the rotation\nequivariant feature image pyramid network (REFIPN), an image pyramid network\nbased on rotation equivariance convolution. The proposed pyramid network\nextracts features in a wide range of scales and orientations by using novel\nconvolution filters. These features are used to generate vector fields and\ndetermine the weight and angle of the highest-scoring orientation for all\nspatial locations on an image. Finally, the extracted features go through the\nprediction layers of the detector. The detection performance of the proposed\nmodel is validated on two commonly used aerial benchmarks and the results show\nour propose model can achieve state-of-the-art performance with satisfactory\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 01:33:49 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 01:16:48 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Shamsolmoali", "Pourya", ""], ["Zareapoor", "Masoumeh", ""], ["Chanussot", "Jocelyn", ""], ["Zhou", "Huiyu", ""], ["Yang", "Jie", ""]]}, {"id": "2106.00908", "submitter": "Zhuchen Shao", "authors": "Zhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang\n  Ji, Yongbing Zhang", "title": "TransMIL: Transformer based Correlated Multiple Instance Learning for\n  Whole Slide Image Classication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple instance learning (MIL) is a powerful tool to solve the weakly\nsupervised classification in whole slide image (WSI) based pathology diagnosis.\nHowever, the current MIL methods are usually based on independent and identical\ndistribution hypothesis, thus neglect the correlation among different\ninstances. To address this problem, we proposed a new framework, called\ncorrelated MIL, and provided a proof for convergence. Based on this framework,\nwe devised a Transformer based MIL (TransMIL), which explored both\nmorphological and spatial information. The proposed TransMIL can effectively\ndeal with unbalanced/balanced and binary/multiple classification with great\nvisualization and interpretability. We conducted various experiments for three\ndifferent computational pathology problems and achieved better performance and\nfaster convergence compared with state-of-the-art methods. The test AUC for the\nbinary tumor classification can be up to 93.09% over CAMELYON16 dataset. And\nthe AUC over the cancer subtypes classification can be up to 96.03% and 98.82%\nover TCGA-NSCLC dataset and TCGA-RCC dataset, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 02:57:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Shao", "Zhuchen", ""], ["Bian", "Hao", ""], ["Chen", "Yang", ""], ["Wang", "Yifeng", ""], ["Zhang", "Jian", ""], ["Ji", "Xiangyang", ""], ["Zhang", "Yongbing", ""]]}, {"id": "2106.00912", "submitter": "Wentong Li", "authors": "Hantang Liu, Wentong Li, Jianke Zhu", "title": "Translational Symmetry-Aware Facade Parsing for 3D Building\n  Reconstruction", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effectively parsing the facade is essential to 3D building reconstruction,\nwhich is an important computer vision problem with a large amount of\napplications in high precision map for navigation, computer aided design, and\ncity generation for digital entertainments. To this end, the key is how to\nobtain the shape grammars from 2D images accurately and efficiently. Although\nenjoying the merits of promising results on the semantic parsing, deep learning\nmethods cannot directly make use of the architectural rules, which play an\nimportant role for man-made structures. In this paper, we present a novel\ntranslational symmetry-based approach to improving the deep neural networks.\nOur method employs deep learning models as the base parser, and a module taking\nadvantage of translational symmetry is used to refine the initial parsing\nresults. In contrast to conventional semantic segmentation or bounding box\nprediction, we propose a novel scheme to fuse segmentation with anchor-free\ndetection in a single stage network, which enables the efficient training and\nbetter convergence. After parsing the facades into shape grammars, we employ an\noff-the-shelf rendering engine like Blender to reconstruct the realistic\nhigh-quality 3D models using procedural modeling. We conduct experiments on\nthree public datasets, where our proposed approach outperforms the\nstate-of-the-art methods. In addition, we have illustrated the 3D building\nmodels built from 2D facade images.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 03:10:51 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Hantang", ""], ["Li", "Wentong", ""], ["Zhu", "Jianke", ""]]}, {"id": "2106.00918", "submitter": "Jari Korhonen", "authors": "Jari Korhonen, Yicheng Su, Junyong You", "title": "Consumer Image Quality Prediction using Recurrent Neural Networks for\n  Spatial Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promising results for subjective image quality prediction have been achieved\nduring the past few years by using convolutional neural networks (CNN).\nHowever, the use of CNNs for high resolution image quality assessment remains a\nchallenge, since typical CNN architectures have been designed for small\nresolution input images. In this study, we propose an image quality model that\nattempts to mimic the attention mechanism of human visual system (HVS) by using\na recurrent neural network (RNN) for spatial pooling of the features extracted\nfrom different spatial areas (patches) by a deep CNN-based feature extractor.\nThe experimental study, conducted by using images with different resolutions\nfrom two recently published image quality datasets, indicates that the quality\nprediction accuracy of the proposed method is competitive against benchmark\nmodels representing the state-of-the-art, and the proposed method also performs\nconsistently on different resolution versions of the same dataset.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 03:31:44 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Korhonen", "Jari", ""], ["Su", "Yicheng", ""], ["You", "Junyong", ""]]}, {"id": "2106.00919", "submitter": "Minh-Son To", "authors": "Minh-Son To, Ian G Sarno, Chee Chong, Mark Jenkinson and Gustavo\n  Carneiro", "title": "Self-supervised Lesion Change Detection and Localisation in Longitudinal\n  Multiple Sclerosis Brain Imaging", "comments": "Provisional accepted for MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Longitudinal imaging forms an essential component in the management and\nfollow-up of many medical conditions. The presence of lesion changes on serial\nimaging can have significant impact on clinical decision making, highlighting\nthe important role for automated change detection. Lesion changes can represent\nanomalies in serial imaging, which implies a limited availability of\nannotations and a wide variety of possible changes that need to be considered.\nHence, we introduce a new unsupervised anomaly detection and localisation\nmethod trained exclusively with serial images that do not contain any lesion\nchanges. Our training automatically synthesises lesion changes in serial\nimages, introducing detection and localisation pseudo-labels that are used to\nself-supervise the training of our model. Given the rarity of these lesion\nchanges in the synthesised images, we train the model with the imbalance robust\nfocal Tversky loss. When compared to supervised models trained on different\ndatasets, our method shows competitive performance in the detection and\nlocalisation of new demyelinating lesions on longitudinal magnetic resonance\nimaging in multiple sclerosis patients. Code for the models will be made\navailable on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 03:34:10 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["To", "Minh-Son", ""], ["Sarno", "Ian G", ""], ["Chong", "Chee", ""], ["Jenkinson", "Mark", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2106.00952", "submitter": "Tuan-Anh Nguyen Dang", "authors": "Tuan-Anh Nguyen Dang and Dat-Thanh Nguyen", "title": "End-to-End Information Extraction by Character-Level Embedding and\n  Multi-Stage Attentional U-Net", "comments": "Accepted to BMVC 2019", "journal-ref": "30th British Machine Vision Conference (BMVC) 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Information extraction from document images has received a lot of attention\nrecently, due to the need for digitizing a large volume of unstructured\ndocuments such as invoices, receipts, bank transfers, etc. In this paper, we\npropose a novel deep learning architecture for end-to-end information\nextraction on the 2D character-grid embedding of the document, namely the\n\\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and\nspatial relations between 2D elements, our model leverages a specialized\nmulti-stage encoder-decoders design, in conjunction with efficient uses of the\nself-attention mechanism and the box convolution. Experimental results on\ndifferent datasets show that our model outperforms the baseline U-Net\narchitecture by a large margin while using 40\\% fewer parameters. Moreover, it\nalso significantly improved the baseline in erroneous OCR and limited training\ndata scenario, thus becomes practical for real-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 05:42:51 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Dang", "Tuan-Anh Nguyen", ""], ["Nguyen", "Dat-Thanh", ""]]}, {"id": "2106.00985", "submitter": "Qinyan Dai", "authors": "Qinyan Dai, Juncheng Li, Qiaosi Yi, Faming Fang and Guixu Zhang", "title": "Feedback Network for Mutually Boosted Stereo Image Super-Resolution and\n  Disparity Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under stereo settings, the problem of image super-resolution (SR) and\ndisparity estimation are interrelated that the result of each problem could\nhelp to solve the other. The effective exploitation of correspondence between\ndifferent views facilitates the SR performance, while the high-resolution (HR)\nfeatures with richer details benefit the correspondence estimation. According\nto this motivation, we propose a Stereo Super-Resolution and Disparity\nEstimation Feedback Network (SSRDE-FNet), which simultaneously handles the\nstereo image super-resolution and disparity estimation in a unified framework\nand interact them with each other to further improve their performance.\nSpecifically, the SSRDE-FNet is composed of two dual recursive sub-networks for\nleft and right views. Besides the cross-view information exploitation in the\nlow-resolution (LR) space, HR representations produced by the SR process are\nutilized to perform HR disparity estimation with higher accuracy, through which\nthe HR features can be aggregated to generate a finer SR result. Afterward, the\nproposed HR Disparity Information Feedback (HRDIF) mechanism delivers\ninformation carried by HR disparity back to previous layers to further refine\nthe SR image reconstruction. Extensive experiments demonstrate the\neffectiveness and advancement of SSRDE-FNet.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 07:05:17 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Dai", "Qinyan", ""], ["Li", "Juncheng", ""], ["Yi", "Qiaosi", ""], ["Fang", "Faming", ""], ["Zhang", "Guixu", ""]]}, {"id": "2106.00997", "submitter": "Changhee Han Dr.", "authors": "Changhee Han, Takayuki Okamoto, Koichi Takeuchi, Dimitris Katsios,\n  Andrey Grushnikov, Masaaki Kobayashi, Antoine Choppin, Yutaka Kurashina, Yuki\n  Shimahara", "title": "Tips and Tricks to Improve CNN-based Chest X-ray Diagnosis: A Survey", "comments": "7 pages, 2 figures, to be published in Japanese Journal of Medical\n  Imaging and Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) intrinsically requires large-scale data\nwhereas Chest X-Ray (CXR) images tend to be data/annotation-scarce, leading to\nover-fitting. Therefore, based on our development experience and related work,\nthis paper thoroughly introduces tricks to improve generalization in the CXR\ndiagnosis: how to (i) leverage additional data, (ii) augment/distillate data,\n(iii) regularize training, and (iv) conduct efficient segmentation. As a\ndevelopment example based on such optimization techniques, we also feature\nLPIXEL's CNN-based CXR solution, EIRL Chest Nodule, which improved\nradiologists/non-radiologists' nodule detection sensitivity by 0.100/0.131,\nrespectively, while maintaining specificity.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 07:46:02 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Han", "Changhee", ""], ["Okamoto", "Takayuki", ""], ["Takeuchi", "Koichi", ""], ["Katsios", "Dimitris", ""], ["Grushnikov", "Andrey", ""], ["Kobayashi", "Masaaki", ""], ["Choppin", "Antoine", ""], ["Kurashina", "Yutaka", ""], ["Shimahara", "Yuki", ""]]}, {"id": "2106.01035", "submitter": "Daochang Liu", "authors": "Daochang Liu, Qiyue Li, Tingting Jiang, Yizhou Wang, Rulin Miao, Fei\n  Shan, Ziyu Li", "title": "Towards Unified Surgical Skill Assessment", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical skills have a great influence on surgical safety and patients'\nwell-being. Traditional assessment of surgical skills involves strenuous manual\nefforts, which lacks efficiency and repeatability. Therefore, we attempt to\nautomatically predict how well the surgery is performed using the surgical\nvideo. In this paper, a unified multi-path framework for automatic surgical\nskill assessment is proposed, which takes care of multiple composing aspects of\nsurgical skills, including surgical tool usage, intraoperative event pattern,\nand other skill proxies. The dependency relationships among these different\naspects are specially modeled by a path dependency module in the framework. We\nconduct extensive experiments on the JIGSAWS dataset of simulated surgical\ntasks, and a new clinical dataset of real laparoscopic surgeries. The proposed\nframework achieves promising results on both datasets, with the\nstate-of-the-art on the simulated dataset advanced from 0.71 Spearman's\ncorrelation to 0.80. It is also shown that combining multiple skill aspects\nyields better performance than relying on a single aspect.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:06:43 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Daochang", ""], ["Li", "Qiyue", ""], ["Jiang", "Tingting", ""], ["Wang", "Yizhou", ""], ["Miao", "Rulin", ""], ["Shan", "Fei", ""], ["Li", "Ziyu", ""]]}, {"id": "2106.01055", "submitter": "Ciprian Orhei", "authors": "Ciprian Orhei and Silviu Vert and Radu Vasiu", "title": "A Novel Edge Detection Operator for Identifying Buildings in Augmented\n  Reality Applications", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-59506-7_18", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Augmented Reality is an environment-enhancing technology, widely applied in\nmany domains, such as tourism and culture. One of the major challenges in this\nfield is precise detection and extraction of building information through\nComputer Vision techniques. Edge detection is one of the building blocks\noperations for many feature extraction solutions in Computer Vision. AR systems\nuse edge detection for building extraction or for extraction of facade details\nfrom buildings. In this paper, we propose a novel filter operator for edge\ndetection that aims to extract building contours or facade features better. The\nproposed filter gives more weight for finding vertical and horizontal edges\nthat is an important feature for our aim.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:06:50 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Orhei", "Ciprian", ""], ["Vert", "Silviu", ""], ["Vasiu", "Radu", ""]]}, {"id": "2106.01061", "submitter": "Chen Liang", "authors": "Chen Liang, Yu Wu, Tianfei Zhou, Wenguan Wang, Zongxin Yang, Yunchao\n  Wei and Yi Yang", "title": "Rethinking Cross-modal Interaction from a Top-down Perspective for\n  Referring Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring video object segmentation (RVOS) aims to segment video objects with\nthe guidance of natural language reference. Previous methods typically tackle\nRVOS through directly grounding linguistic reference over the image lattice.\nSuch bottom-up strategy fails to explore object-level cues, easily leading to\ninferior results. In this work, we instead put forward a two-stage, top-down\nRVOS solution. First, an exhaustive set of object tracklets is constructed by\npropagating object masks detected from several sampled frames to the entire\nvideo. Second, a Transformer-based tracklet-language grounding module is\nproposed, which models instance-level visual relations and cross-modal\ninteractions simultaneously and efficiently. Our model ranks first place on\nCVPR2021 Referring Youtube-VOS challenge.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:26:13 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liang", "Chen", ""], ["Wu", "Yu", ""], ["Zhou", "Tianfei", ""], ["Wang", "Wenguan", ""], ["Yang", "Zongxin", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""]]}, {"id": "2106.01085", "submitter": "Divyam Madaan", "authors": "Jaehong Yoon, Divyam Madaan, Eunho Yang, Sung Ju Hwang", "title": "Online Coreset Selection for Rehearsal-based Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A dataset is a shred of crucial evidence to describe a task. However, each\ndata point in the dataset does not have the same potential, as some of the data\npoints can be more representative or informative than others. This unequal\nimportance among the data points may have a large impact in rehearsal-based\ncontinual learning, where we store a subset of the training examples (coreset)\nto be replayed later to alleviate catastrophic forgetting. In continual\nlearning, the quality of the samples stored in the coreset directly affects the\nmodel's effectiveness and efficiency. The coreset selection problem becomes\neven more important under realistic settings, such as imbalanced continual\nlearning or noisy data scenarios. To tackle this problem, we propose Online\nCoreset Selection (OCS), a simple yet effective method that selects the most\nrepresentative and informative coreset at each iteration and trains them in an\nonline manner. Our proposed method maximizes the model's adaptation to a target\ndataset while selecting high-affinity samples to past tasks, which directly\ninhibits catastrophic forgetting. We validate the effectiveness of our coreset\nselection mechanism over various standard, imbalanced, and noisy datasets\nagainst strong continual learning baselines, demonstrating that it improves\ntask adaptation and prevents catastrophic forgetting in a sample-efficient\nmanner.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 11:39:25 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yoon", "Jaehong", ""], ["Madaan", "Divyam", ""], ["Yang", "Eunho", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2106.01088", "submitter": "Haisheng Su", "authors": "Haisheng Su, Jinyuan Feng, Dongliang Wang, Weihao Gan, Wei Wu, Yu Qiao", "title": "TSI: Temporal Saliency Integration for Video Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient spatiotemporal modeling is an important yet challenging problem for\nvideo action recognition. Existing state-of-the-art methods exploit motion\nclues to assist in short-term temporal modeling through temporal difference\nover consecutive frames. However, background noises will be inevitably\nintroduced due to the camera movement. Besides, movements of different actions\ncan vary greatly. In this paper, we propose a Temporal Saliency Integration\n(TSI) block, which mainly contains a Salient Motion Excitation (SME) module and\na Cross-scale Temporal Integration (CTI) module. Specifically, SME aims to\nhighlight the motion-sensitive area through local-global motion modeling, where\nthe background suppression and pyramidal feature difference are conducted\nsuccessively between neighboring frames to capture motion dynamics with less\nbackground noises. CTI is designed to perform multi-scale temporal modeling\nthrough a group of separate 1D convolutions respectively. Meanwhile, temporal\ninteractions across different scales are integrated with attention mechanism.\nThrough these two modules, long short-term temporal relationships can be\nencoded efficiently by introducing limited additional parameters. Extensive\nexperiments are conducted on several popular benchmarks (i.e.,\nSomething-Something v1 & v2, Kinetics-400, UCF-101, and HMDB-51), which\ndemonstrate the effectiveness and superiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 11:43:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Su", "Haisheng", ""], ["Feng", "Jinyuan", ""], ["Wang", "Dongliang", ""], ["Gan", "Weihao", ""], ["Wu", "Wei", ""], ["Qiao", "Yu", ""]]}, {"id": "2106.01100", "submitter": "Michel Pohl", "authors": "Michel Pohl, Mitsuru Uesaka, Hiroyuki Takahashi, Kazuyuki Demachi and\n  Ritu Bhusal Chhatkuli", "title": "Prediction of the Position of External Markers Using a Recurrent Neural\n  Network Trained With Unbiased Online Recurrent Optimization for Safe Lung\n  Cancer Radiotherapy", "comments": "20 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During lung cancer radiotherapy, the position of infrared reflective objects\non the chest can be recorded to estimate the tumor location. However,\nradiotherapy systems usually have a latency inherent to robot control\nlimitations that impedes the radiation delivery precision. Not taking this\nphenomenon into account may cause unwanted damage to healthy tissues and lead\nto side effects such as radiation pneumonitis. In this research, we use nine\nobservation records of the three-dimensional position of three external markers\non the chest and abdomen of healthy individuals breathing during intervals from\n73s to 222s. The sampling frequency is equal to 10Hz and the amplitudes of the\nrecorded trajectories range from 6mm to 40mm in the superior-inferior\ndirection. We forecast the location of each marker simultaneously with a\nhorizon value (the time interval in advance for which the prediction is made)\nbetween 0.1s and 2.0s, using a recurrent neural network (RNN) trained with\nunbiased online recurrent optimization (UORO). We compare its performance with\nan RNN trained with real-time recurrent learning, least mean squares (LMS), and\noffline linear regression. Training and cross-validation are performed during\nthe first minute of each sequence. On average, UORO achieves the lowest\nroot-mean-square (RMS) and maximum error, equal respectively to 1.3mm and\n8.8mm, with a prediction time per time step lower than 2.8ms (Dell Intel core\ni9-9900K 3.60Ghz). Linear regression has the lowest RMS error for the horizon\nvalues 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,\nand UORO for horizon values greater than 0.6s.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:07:31 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Pohl", "Michel", ""], ["Uesaka", "Mitsuru", ""], ["Takahashi", "Hiroyuki", ""], ["Demachi", "Kazuyuki", ""], ["Chhatkuli", "Ritu Bhusal", ""]]}, {"id": "2106.01111", "submitter": "Wei Sun", "authors": "Wei Sun and Tao Wang and Xiongkuo Min and Fuwang Yi and Guangtao Zhai", "title": "Deep Learning based Full-reference and No-reference Quality Assessment\n  Models for Compressed UGC Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep learning based video quality assessment\n(VQA) framework to evaluate the quality of the compressed user's generated\ncontent (UGC) videos. The proposed VQA framework consists of three modules, the\nfeature extraction module, the quality regression module, and the quality\npooling module. For the feature extraction module, we fuse the features from\nintermediate layers of the convolutional neural network (CNN) network into\nfinal quality-aware feature representation, which enables the model to make\nfull use of visual information from low-level to high-level. Specifically, the\nstructure and texture similarities of feature maps extracted from all\nintermediate layers are calculated as the feature representation for the full\nreference (FR) VQA model, and the global mean and standard deviation of the\nfinal feature maps fused by intermediate feature maps are calculated as the\nfeature representation for the no reference (NR) VQA model. For the quality\nregression module, we use the fully connected (FC) layer to regress the\nquality-aware features into frame-level scores. Finally, a\nsubjectively-inspired temporal pooling strategy is adopted to pool frame-level\nscores into the video-level score. The proposed model achieves the best\nperformance among the state-of-the-art FR and NR VQA models on the Compressed\nUGC VQA database and also achieves pretty good performance on the in-the-wild\nUGC VQA databases.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:23:16 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Sun", "Wei", ""], ["Wang", "Tao", ""], ["Min", "Xiongkuo", ""], ["Yi", "Fuwang", ""], ["Zhai", "Guangtao", ""]]}, {"id": "2106.01127", "submitter": "Chun-Hao Chang", "authors": "Chun-Hao Chang, George Alexandru Adam, Anna Goldenberg", "title": "Towards Robust Classification Model by Counterfactual and Invariant Data\n  Generation", "comments": "Accepted in 2021 CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the success of machine learning applications in science, industry,\nand society in general, many approaches are known to be non-robust, often\nrelying on spurious correlations to make predictions. Spuriousness occurs when\nsome features correlate with labels but are not causal; relying on such\nfeatures prevents models from generalizing to unseen environments where such\ncorrelations break. In this work, we focus on image classification and propose\ntwo data generation processes to reduce spuriousness. Given human annotations\nof the subset of the features responsible (causal) for the labels (e.g.\nbounding boxes), we modify this causal set to generate a surrogate image that\nno longer has the same label (i.e. a counterfactual image). We also alter\nnon-causal features to generate images still recognized as the original labels,\nwhich helps to learn a model invariant to these features. In several\nchallenging datasets, our data generations outperform state-of-the-art methods\nin accuracy when spurious correlations break, and increase the saliency focus\non causal features providing better explanations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:48:29 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 06:14:35 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Chang", "Chun-Hao", ""], ["Adam", "George Alexandru", ""], ["Goldenberg", "Anna", ""]]}, {"id": "2106.01132", "submitter": "Benoit Dufumier", "authors": "Benoit Dufumier, Pietro Gori, Ilaria Battaglia, Julie Victor, Antoine\n  Grigis, Edouard Duchesnay", "title": "Benchmarking CNN on 3D Anatomical Brain MRI: Architectures, Data\n  Augmentation and Deep Ensemble Learning", "comments": "17 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Learning (DL) and specifically CNN models have become a de facto method\nfor a wide range of vision tasks, outperforming traditional machine learning\n(ML) methods. Consequently, they drew a lot of attention in the neuroimaging\nfield in particular for phenotype prediction or computer-aided diagnosis.\nHowever, most of the current studies often deal with small single-site cohorts,\nalong with a specific pre-processing pipeline and custom CNN architectures,\nwhich make them difficult to compare to. We propose an extensive benchmark of\nrecent state-of-the-art (SOTA) 3D CNN, evaluating also the benefits of data\naugmentation and deep ensemble learning, on both Voxel-Based Morphometry (VBM)\npre-processing and quasi-raw images. Experiments were conducted on a large\nmulti-site 3D brain anatomical MRI data-set comprising N=10k scans on 3\nchallenging tasks: age prediction, sex classification, and schizophrenia\ndiagnosis. We found that all models provide significantly better predictions\nwith VBM images than quasi-raw data. This finding evolved as the training set\napproaches 10k samples where quasi-raw data almost reach the performance of\nVBM. Moreover, we showed that linear models perform comparably with SOTA CNN on\nVBM data. We also demonstrated that DenseNet and tiny-DenseNet, a lighter\nversion that we proposed, provide a good compromise in terms of performance in\nall data regime. Therefore, we suggest to employ them as the architectures by\ndefault. Critically, we also showed that current CNN are still very biased\ntowards the acquisition site, even when trained with N=10k multi-site images.\nIn this context, VBM pre-processing provides an efficient way to limit this\nsite effect. Surprisingly, we did not find any clear benefit from data\naugmentation techniques. Finally, we proved that deep ensemble learning is well\nsuited to re-calibrate big CNN models without sacrificing performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 13:00:35 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Dufumier", "Benoit", ""], ["Gori", "Pietro", ""], ["Battaglia", "Ilaria", ""], ["Victor", "Julie", ""], ["Grigis", "Antoine", ""], ["Duchesnay", "Edouard", ""]]}, {"id": "2106.01153", "submitter": "Oliver Urbann", "authors": "Oliver Urbann, Oliver Bredtmann, Maximilian Otten, Jan-Philip Richter,\n  Thilo Bauer, David Zibriczky", "title": "Online and Real-Time Tracking in a Surveillance Scenario", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for tracking in a surveillance scenario.\nTypical aspects for this scenario are a 24/7 operation with a static camera\nmounted above the height of a human with many objects or people. The Multiple\nObject Tracking Benchmark 20 (MOT20) reflects this scenario best. We can show\nthat our approach is real-time capable on this benchmark and outperforms all\nother real-time capable approaches in HOTA, MOTA, and IDF1. We achieve this by\ncontributing a fast Siamese network reformulated for linear runtime (instead of\nquadratic) to generate fingerprints from detections. Thus, it is possible to\nassociate the detections to Kalman filters based on multiple tracking specific\nratings: Cosine similarity of fingerprints, Intersection over Union, and pixel\ndistance ratio in the image.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 13:43:25 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Urbann", "Oliver", ""], ["Bredtmann", "Oliver", ""], ["Otten", "Maximilian", ""], ["Richter", "Jan-Philip", ""], ["Bauer", "Thilo", ""], ["Zibriczky", "David", ""]]}, {"id": "2106.01171", "submitter": "P. Christopher Staecker", "authors": "P. Christopher Staecker", "title": "Digital homotopy relations and digital homology theories", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.00473,\n  arXiv:1903.00706", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CV math.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we prove results relating to two homotopy relations and four\nhomology theories developed in the topology of digital images.\n  We introduce a new type of homotopy relation for digitally continuous\nfunctions which we call \"strong homotopy.\" Both digital homotopy and strong\nhomotopy are natural digitizations of classical topological homotopy: the\ndifference between them is analogous to the difference between digital\n4-adjacency and 8-adjacency in the plane.\n  We also consider four different digital homology theories: a simplicial\nhomology theory by Arslan et al which is the homology of the clique complex, a\nsingular simplicial homology theory by D. W. Lee, a cubical homology theory by\nJamil and Ali, and a new kind of cubical homology for digital images with\n$c_1$-adjacency which is easily computed, and generalizes a construction by\nKaraca \\& Ege. We show that the two simplicial homology theories are isomorphic\nto each other, but distinct from the two cubical theories.\n  We also show that homotopic maps have the same induced homomorphisms in the\ncubical homology theory, and strong homotopic maps additionally have the same\ninduced homomorphisms in the simplicial theory.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:10:46 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Staecker", "P. Christopher", ""]]}, {"id": "2106.01178", "submitter": "Danila Rukhovich", "authors": "Danila Rukhovich, Anna Vorontsova, Anton Konushin", "title": "ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View\n  General-Purpose 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the task of multi-view RGB-based 3D object\ndetection as an end-to-end optimization problem. To address this problem, we\npropose ImVoxelNet, a novel fully convolutional method of 3D object detection\nbased on monocular or multi-view RGB images. The number of monocular images in\neach multi-view input can variate during training and inference; actually, this\nnumber might be unique for each multi-view input. ImVoxelNet successfully\nhandles both indoor and outdoor scenes, which makes it general-purpose.\nSpecifically, it achieves state-of-the-art results in car detection on KITTI\n(monocular) and nuScenes (multi-view) benchmarks among all methods that accept\nRGB images. Moreover, it surpasses existing RGB-based 3D object detection\nmethods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark\nfor multi-view 3D object detection. The source code and the trained models are\navailable at \\url{https://github.com/saic-vul/imvoxelnet}.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 14:20:24 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Rukhovich", "Danila", ""], ["Vorontsova", "Anna", ""], ["Konushin", "Anton", ""]]}, {"id": "2106.01217", "submitter": "Bo Peng", "authors": "Bo Peng, Hongxing Fan, Wei Wang, Jing Dong, Yuezun Li, Siwei Lyu, Qi\n  Li, Zhenan Sun, Han Chen, Baoying Chen, Yanjie Hu, Shenghai Luo, Junrui\n  Huang, Yutong Yao, Boyuan Liu, Hefei Ling, Guosheng Zhang, Zhiliang Xu,\n  Changtao Miao, Changlei Lu, Shan He, Xiaoyan Wu, Wanyi Zhuang", "title": "DFGC 2021: A DeepFake Game Competition", "comments": null, "journal-ref": "International Joint Conference on Biometrics (IJCB 2021)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a summary of the DFGC 2021 competition. DeepFake\ntechnology is developing fast, and realistic face-swaps are increasingly\ndeceiving and hard to detect. At the same time, DeepFake detection methods are\nalso improving. There is a two-party game between DeepFake creators and\ndetectors. This competition provides a common platform for benchmarking the\nadversarial game between current state-of-the-art DeepFake creation and\ndetection methods. In this paper, we present the organization, results and top\nsolutions of this competition and also share our insights obtained during this\nevent. We also release the DFGC-21 testing dataset collected from our\nparticipants to further benefit the research community.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:10:13 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Peng", "Bo", ""], ["Fan", "Hongxing", ""], ["Wang", "Wei", ""], ["Dong", "Jing", ""], ["Li", "Yuezun", ""], ["Lyu", "Siwei", ""], ["Li", "Qi", ""], ["Sun", "Zhenan", ""], ["Chen", "Han", ""], ["Chen", "Baoying", ""], ["Hu", "Yanjie", ""], ["Luo", "Shenghai", ""], ["Huang", "Junrui", ""], ["Yao", "Yutong", ""], ["Liu", "Boyuan", ""], ["Ling", "Hefei", ""], ["Zhang", "Guosheng", ""], ["Xu", "Zhiliang", ""], ["Miao", "Changtao", ""], ["Lu", "Changlei", ""], ["He", "Shan", ""], ["Wu", "Xiaoyan", ""], ["Zhuang", "Wanyi", ""]]}, {"id": "2106.01226", "submitter": "Xiaokang Chen", "authors": "Xiaokang Chen, Yuhui Yuan, Gang Zeng, Jingdong Wang", "title": "Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the semi-supervised semantic segmentation problem via\nexploring both labeled data and extra unlabeled data. We propose a novel\nconsistency regularization approach, called cross pseudo supervision (CPS). Our\napproach imposes the consistency on two segmentation networks perturbed with\ndifferent initialization for the same input image. The pseudo one-hot label\nmap, output from one perturbed segmentation network, is used to supervise the\nother segmentation network with the standard cross-entropy loss, and vice\nversa. The CPS consistency has two roles: encourage high similarity between the\npredictions of two perturbed networks for the same input image, and expand\ntraining data by using the unlabeled data with pseudo labels. Experiment\nresults show that our approach achieves the state-of-the-art semi-supervised\nsegmentation performance on Cityscapes and PASCAL VOC 2012. Code is available\nat https://git.io/CPS.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:21:56 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 04:01:04 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chen", "Xiaokang", ""], ["Yuan", "Yuhui", ""], ["Zeng", "Gang", ""], ["Wang", "Jingdong", ""]]}, {"id": "2106.01277", "submitter": "Pierre Gutierrez", "authors": "Pierre Gutierrez, Antoine Cordier, Tha\\\"is Caldeira, Th\\'eophile\n  Sautory", "title": "Data augmentation and pre-trained networks for extremely low data\n  regimes unsupervised visual inspection", "comments": "16 pages, 8 figures, 9 tables, SPIE proceedings of Optical Metrology\n  conference (https://spie.org/conferences-and-exhibitions/optical-metrology)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The use of deep features coming from pre-trained neural networks for\nunsupervised anomaly detection purposes has recently gathered momentum in the\ncomputer vision field. In particular, industrial inspection applications can\ntake advantage of such features, as demonstrated by the multiple successes of\nrelated methods on the MVTec Anomaly Detection (MVTec AD) dataset. These\nmethods make use of neural networks pre-trained on auxiliary classification\ntasks such as ImageNet. However, to our knowledge, no comparative study of\nrobustness to the low data regimes between these approaches has been conducted\nyet. For quality inspection applications, the handling of limited sample sizes\nmay be crucial as large quantities of images are not available for small\nseries. In this work, we aim to compare three approaches based on deep\npre-trained features when varying the quantity of available data in MVTec AD:\nKNN, Mahalanobis, and PaDiM. We show that although these methods are mostly\nrobust to small sample sizes, they still can benefit greatly from using data\naugmentation in the original image space, which allows to deal with very small\nproduction runs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:37:20 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Gutierrez", "Pierre", ""], ["Cordier", "Antoine", ""], ["Caldeira", "Tha\u00efs", ""], ["Sautory", "Th\u00e9ophile", ""]]}, {"id": "2106.01351", "submitter": "Weiyi Xie", "authors": "Weiyi Xie, Colin Jacobs, Bram van Ginneken", "title": "Deep Clustering Activation Maps for Emphysema Subtyping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a deep learning clustering method that exploits dense features\nfrom a segmentation network for emphysema subtyping from computed tomography\n(CT) scans. Using dense features enables high-resolution visualization of image\nregions corresponding to the cluster assignment via dense clustering activation\nmaps (dCAMs). This approach provides model interpretability. We evaluated\nclustering results on 500 subjects from the COPDGenestudy, where radiologists\nmanually annotated emphysema sub-types according to their visual CT assessment.\nWe achieved a 43% unsupervised clustering accuracy, outperforming our baseline\nat 41% and yielding results comparable to supervised classification at 45%. The\nproposed method also offers a better cluster formation than the baseline,\nachieving0.54 in silhouette coefficient and 0.55 in David-Bouldin scores.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 11:24:48 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Xie", "Weiyi", ""], ["Jacobs", "Colin", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2106.01364", "submitter": "Jong-Chyi Su", "authors": "Jong-Chyi Su and Subhransu Maji", "title": "The Semi-Supervised iNaturalist Challenge at the FGVC8 Workshop", "comments": "Tech report for Semi-iNat 2021 challenge, competition page:\n  https://github.com/cvl-umass/semi-inat-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-iNat is a challenging dataset for semi-supervised classification with a\nlong-tailed distribution of classes, fine-grained categories, and domain shifts\nbetween labeled and unlabeled data. This dataset is behind the second iteration\nof the semi-supervised recognition challenge to be held at the FGVC8 workshop\nat CVPR 2021. Different from the previous one, this dataset (i) includes images\nof species from different kingdoms in the natural taxonomy, (ii) is at a larger\nscale -- with 810 in-class and 1629 out-of-class species for a total of 330k\nimages, and (iii) does not provide in/out-of-class labels, but provides coarse\ntaxonomic labels (kingdom and phylum) for the unlabeled images. This document\ndescribes baseline results and the details of the dataset which is available\nhere: \\url{https://github.com/cvl-umass/semi-inat-2021}.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:59:41 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Su", "Jong-Chyi", ""], ["Maji", "Subhransu", ""]]}, {"id": "2106.01401", "submitter": "Peng Gao", "authors": "Peng Gao, Jiasen Lu, Hongsheng Li, Roozbeh Mottaghi, Aniruddha\n  Kembhavi", "title": "Container: Context Aggregation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are ubiquitous in computer vision, with\na myriad of effective and efficient variations. Recently, Transformers --\noriginally introduced in natural language processing -- have been increasingly\nadopted in computer vision. While early adopters continue to employ CNN\nbackbones, the latest networks are end-to-end CNN-free Transformer solutions. A\nrecent surprising finding shows that a simple MLP based solution without any\ntraditional convolutional or Transformer components can produce effective\nvisual representations. While CNNs, Transformers and MLP-Mixers may be\nconsidered as completely disparate architectures, we provide a unified view\nshowing that they are in fact special cases of a more general method to\naggregate spatial context in a neural network stack. We present the \\model\n(CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head\ncontext aggregation that can exploit long-range interactions \\emph{a la}\nTransformers while still exploiting the inductive bias of the local convolution\noperation leading to faster convergence speeds, often seen in CNNs. In contrast\nto Transformer-based methods that do not scale well to downstream tasks that\nrely on larger input image resolutions, our efficient network, named\n\\modellight, can be employed in object detection and instance segmentation\nnetworks such as DETR, RetinaNet and Mask-RCNN to obtain an impressive\ndetection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large\nimprovements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50\nbackbone with a comparable compute and parameter size. Our method also achieves\npromising results on self-supervised learning compared to DeiT on the DINO\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 18:09:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Gao", "Peng", ""], ["Lu", "Jiasen", ""], ["Li", "Hongsheng", ""], ["Mottaghi", "Roozbeh", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "2106.01423", "submitter": "Henry Kvinge", "authors": "Henry Kvinge, Scott Howland, Nico Courts, Lauren A. Phillips, John\n  Buckheit, Zachary New, Elliott Skomski, Jung H. Lee, Sandeep Tiwari, Jessica\n  Hibler, Courtney D. Corley, Nathan O. Hodas", "title": "One Representation to Rule Them All: Identifying Out-of-Support Examples\n  in Few-shot Learning with Generic Representations", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of few-shot learning has made remarkable strides in developing\npowerful models that can operate in the small data regime. Nearly all of these\nmethods assume every unlabeled instance encountered will belong to a handful of\nknown classes for which one has examples. This can be problematic for\nreal-world use cases where one routinely finds 'none-of-the-above' examples. In\nthis paper we describe this challenge of identifying what we term\n'out-of-support' (OOS) examples. We describe how this problem is subtly\ndifferent from out-of-distribution detection and describe a new method of\nidentifying OOS examples within the Prototypical Networks framework using a\nfixed point which we call the generic representation. We show that our method\noutperforms other existing approaches in the literature as well as other\napproaches that we propose in this paper. Finally, we investigate how the use\nof such a generic point affects the geometry of a model's feature space.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 19:07:27 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kvinge", "Henry", ""], ["Howland", "Scott", ""], ["Courts", "Nico", ""], ["Phillips", "Lauren A.", ""], ["Buckheit", "John", ""], ["New", "Zachary", ""], ["Skomski", "Elliott", ""], ["Lee", "Jung H.", ""], ["Tiwari", "Sandeep", ""], ["Hibler", "Jessica", ""], ["Corley", "Courtney D.", ""], ["Hodas", "Nathan O.", ""]]}, {"id": "2106.01424", "submitter": "Marcella Cornia", "authors": "Marco Cagrandi, Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi,\n  Rita Cucchiara", "title": "Learning to Select: A Fully Attentive Approach for Novel Object\n  Captioning", "comments": "ICMR 2021", "journal-ref": null, "doi": "10.1145/3460426.3463587", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning models have lately shown impressive results when applied to\nstandard datasets. Switching to real-life scenarios, however, constitutes a\nchallenge due to the larger variety of visual concepts which are not covered in\nexisting training sets. For this reason, novel object captioning (NOC) has\nrecently emerged as a paradigm to test captioning models on objects which are\nunseen during the training phase. In this paper, we present a novel approach\nfor NOC that learns to select the most relevant objects of an image, regardless\nof their adherence to the training set, and to constrain the generative process\nof a language model accordingly. Our architecture is fully-attentive and\nend-to-end trainable, also when incorporating constraints. We perform\nexperiments on the held-out COCO dataset, where we demonstrate improvements\nover the state of the art, both in terms of adaptability to novel objects and\ncaption quality.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 19:11:21 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Cagrandi", "Marco", ""], ["Cornia", "Marcella", ""], ["Stefanini", "Matteo", ""], ["Baraldi", "Lorenzo", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2106.01428", "submitter": "Zenglin Shi", "authors": "Zenglin Shi, Yunlu Chen, Efstratios Gavves, Pascal Mettes, and Cees\n  G.M. Snoek", "title": "Unsharp Mask Guided Filtering", "comments": "IEEE Transactions on Image Processing, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of this paper is guided image filtering, which emphasizes the\nimportance of structure transfer during filtering by means of an additional\nguidance image. Where classical guided filters transfer structures using\nhand-designed functions, recent guided filters have been considerably advanced\nthrough parametric learning of deep networks. The state-of-the-art leverages\ndeep networks to estimate the two core coefficients of the guided filter. In\nthis work, we posit that simultaneously estimating both coefficients is\nsuboptimal, resulting in halo artifacts and structure inconsistencies. Inspired\nby unsharp masking, a classical technique for edge enhancement that requires\nonly a single coefficient, we propose a new and simplified formulation of the\nguided filter. Our formulation enjoys a filtering prior from a low-pass filter\nand enables explicit structure transfer by estimating a single coefficient.\nBased on our proposed formulation, we introduce a successive guided filtering\nnetwork, which provides multiple filtering results from a single network,\nallowing for a trade-off between accuracy and efficiency. Extensive ablations,\ncomparisons and analysis show the effectiveness and efficiency of our\nformulation and network, resulting in state-of-the-art results across filtering\ntasks like upsampling, denoising, and cross-modality filtering. Code is\navailable at \\url{https://github.com/shizenglin/Unsharp-Mask-Guided-Filtering}.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 19:15:34 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Shi", "Zenglin", ""], ["Chen", "Yunlu", ""], ["Gavves", "Efstratios", ""], ["Mettes", "Pascal", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2106.01439", "submitter": "Eduardo Perez-Pellitero", "authors": "Eduardo P\\'erez-Pellitero and Sibi Catley-Chandar and Ale\\v{s}\n  Leonardis and Radu Timofte", "title": "NTIRE 2021 Challenge on High Dynamic Range Imaging: Dataset, Methods and\n  Results", "comments": "To appear in CVPRW 2021 (NTIRE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the first challenge on high-dynamic range (HDR) imaging\nthat was part of the New Trends in Image Restoration and Enhancement (NTIRE)\nworkshop, held in conjunction with CVPR 2021. This manuscript focuses on the\nnewly introduced dataset, the proposed methods and their results. The challenge\naims at estimating a HDR image from one or multiple respective low-dynamic\nrange (LDR) observations, which might suffer from under- or over-exposed\nregions and different sources of noise. The challenge is composed by two\ntracks: In Track 1 only a single LDR image is provided as input, whereas in\nTrack 2 three differently-exposed LDR images with inter-frame motion are\navailable. In both tracks, the ultimate goal is to achieve the best objective\nHDR reconstruction in terms of PSNR with respect to a ground-truth image,\nevaluated both directly and with a canonical tonemapping operation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 19:45:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["P\u00e9rez-Pellitero", "Eduardo", ""], ["Catley-Chandar", "Sibi", ""], ["Leonardis", "Ale\u0161", ""], ["Timofte", "Radu", ""]]}, {"id": "2106.01444", "submitter": "Joshua Feinglass", "authors": "Joshua Feinglass and Yezhou Yang", "title": "SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption\n  Evaluation via Typicality Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The open-ended nature of visual captioning makes it a challenging area for\nevaluation. The majority of proposed models rely on specialized training to\nimprove human-correlation, resulting in limited adoption, generalizability, and\nexplainabilty. We introduce \"typicality\", a new formulation of evaluation\nrooted in information theory, which is uniquely suited for problems lacking a\ndefinite ground truth. Typicality serves as our framework to develop a novel\nsemantic comparison, SPARCS, as well as referenceless fluency evaluation\nmetrics. Over the course of our analysis, two separate dimensions of fluency\nnaturally emerge: style, captured by metric SPURTS, and grammar, captured in\nthe form of grammatical outlier penalties. Through extensive experiments and\nablation studies on benchmark datasets, we show how these decomposed dimensions\nof semantics and fluency provide greater system-level insight into captioner\ndifferences. Our proposed metrics along with their combination, SMURF, achieve\nstate-of-the-art correlation with human judgment when compared with other\nrule-based evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 19:58:20 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Feinglass", "Joshua", ""], ["Yang", "Yezhou", ""]]}, {"id": "2106.01467", "submitter": "Kamil Akhmetov", "authors": "Kamil Akhmetov", "title": "Domain Adaptation for Facial Expression Classifier via Domain\n  Discrimination and Gradient Reversal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bringing empathy to a computerized system could significantly improve the\nquality of human-computer communications, as soon as machines would be able to\nunderstand customer intentions and better serve their needs. According to\ndifferent studies (Literature Review), visual information is one of the most\nimportant channels of human interaction and contains significant behavioral\nsignals, that may be captured from facial expressions. Therefore, it is\nconsistent and natural that the research in the field of Facial Expression\nRecognition (FER) has acquired increased interest over the past decade due to\nhaving diverse application area including health-care, sociology, psychology,\ndriver-safety, virtual reality, cognitive sciences, security, entertainment,\nmarketing, etc. We propose a new architecture for the task of FER and examine\nthe impact of domain discrimination loss regularization on the learning\nprocess. With regard to observations, including both classical training\nconditions and unsupervised domain adaptation scenarios, important aspects of\nthe considered domain adaptation approach integration are traced. The results\nmay serve as a foundation for further research in the field.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 20:58:24 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Akhmetov", "Kamil", ""]]}, {"id": "2106.01483", "submitter": "Mazin Hnewa", "authors": "Mazin Hnewa and Hayder Radha", "title": "Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection", "comments": "accepted in 2021 IEEE International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of domain adaptation has been instrumental in addressing the domain\nshift problem encountered by many applications. This problem arises due to the\ndifference between the distributions of source data used for training in\ncomparison with target data used during realistic testing scenarios. In this\npaper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)\nframework that employs multiple domain adaptation paths and corresponding\ndomain classifiers at different scales of the recently introduced YOLOv4 object\ndetector to generate domain-invariant features. We train and test our proposed\nmethod using popular datasets. Our experiments show significant improvements in\nobject detection performance when training YOLOv4 using the proposed MS-DAYOLO\nand when tested on target data representing challenging weather conditions for\nautonomous driving applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 21:50:25 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Hnewa", "Mazin", ""], ["Radha", "Hayder", ""]]}, {"id": "2106.01487", "submitter": "Aditya Kusupati", "authors": "Aditya Kusupati, Matthew Wallingford, Vivek Ramanujan, Raghav Somani,\n  Jae Sung Park, Krishna Pillutla, Prateek Jain, Sham Kakade, Ali Farhadi", "title": "LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning binary representations of instances and classes is a classical\nproblem with several high potential applications. In modern settings, the\ncompression of high-dimensional neural representations to low-dimensional\nbinary codes is a challenging task and often require large bit-codes to be\naccurate. In this work, we propose a novel method for Learning Low-dimensional\nbinary Codes (LLC) for instances as well as classes. Our method does not\nrequire any side-information, like annotated attributes or label meta-data, and\nlearns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The\nlearnt codes are super-efficient while still ensuring nearly optimal\nclassification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the\nlearnt codes capture intrinsically important features in the data, by\ndiscovering an intuitive taxonomy over classes. We further quantitatively\nmeasure the quality of our codes by applying it to the efficient image\nretrieval as well as out-of-distribution (OOD) detection problems. For\nImageNet-100 retrieval problem, our learnt binary codes outperform 16 bit\nHashNet using only 10 bits and also are as accurate as 10 dimensional real\nrepresentations. Finally, our learnt binary codes can perform OOD detection,\nout-of-the-box, as accurately as a baseline that needs ~3000 samples to tune\nits threshold, while we require none. Code and pre-trained models are available\nat https://github.com/RAIVNLab/LLC.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 21:57:52 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kusupati", "Aditya", ""], ["Wallingford", "Matthew", ""], ["Ramanujan", "Vivek", ""], ["Somani", "Raghav", ""], ["Park", "Jae Sung", ""], ["Pillutla", "Krishna", ""], ["Jain", "Prateek", ""], ["Kakade", "Sham", ""], ["Farhadi", "Ali", ""]]}, {"id": "2106.01489", "submitter": "Ziyun Li", "authors": "Ziyun Li, Xinshao Wang, Haojin Yang, Di Hu, Neil M. Robertson, David\n  A. Clifton, Christoph Meinel", "title": "Not All Knowledge Is Created Equal", "comments": "Selective mutual knowledge distillation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mutual knowledge distillation (MKD) improves a model by distilling knowledge\nfrom another model. However, not all knowledge is certain and correct,\nespecially under adverse conditions. For example, label noise usually leads to\nless reliable models due to the undesired memorisation [1, 2]. Wrong knowledge\nmisleads the learning rather than helps. This problem can be handled by two\naspects: (i) improving the reliability of a model where the knowledge is from\n(i.e., knowledge source's reliability); (ii) selecting reliable knowledge for\ndistillation. In the literature, making a model more reliable is widely studied\nwhile selective MKD receives little attention. Therefore, we focus on studying\nselective MKD and highlight its importance in this work.\n  Concretely, a generic MKD framework, Confident knowledge selection followed\nby Mutual Distillation (CMD), is designed. The key component of CMD is a\ngeneric knowledge selection formulation, making the selection threshold either\nstatic (CMD-S) or progressive (CMD-P). Additionally, CMD covers two special\ncases: zero knowledge and all knowledge, leading to a unified MKD framework. We\nempirically find CMD-P performs better than CMD-S. The main reason is that a\nmodel's knowledge upgrades and becomes confident as the training progresses.\n  Extensive experiments are present to demonstrate the effectiveness of CMD and\nthoroughly justify the design of CMD. For example, CMD-P obtains new\nstate-of-the-art results in robustness against label noise.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 22:06:55 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Li", "Ziyun", ""], ["Wang", "Xinshao", ""], ["Yang", "Haojin", ""], ["Hu", "Di", ""], ["Robertson", "Neil M.", ""], ["Clifton", "David A.", ""], ["Meinel", "Christoph", ""]]}, {"id": "2106.01499", "submitter": "Mina Khan", "authors": "Mina Khan, P Srivatsa, Advait Rane, Shriram Chenniappa, Asadali\n  Hazariwala, and Pattie Maes", "title": "Personalizing Pre-trained Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised or weakly supervised models trained on large-scale datasets\nhave shown sample-efficient transfer to diverse datasets in few-shot settings.\nWe consider how upstream pretrained models can be leveraged for downstream\nfew-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP\nPERsonalized) uses image representations from CLIP, a large-scale image\nrepresentation learning model trained using weak natural language supervision.\nWe developed a technique, called Multi-label Weight Imprinting (MWI), for\nmulti-label, continual, and few-shot learning, and CLIPPER uses MWI with image\nrepresentations from CLIP. We evaluated CLIPPER on 10 single-label and 5\nmulti-label datasets. Our model shows robust and competitive performance, and\nwe set new benchmarks for few-shot, multi-label, and continual learning. Our\nlightweight technique is also compute-efficient and enables privacy-preserving\napplications as the data is not sent to the upstream model for fine-tuning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 22:58:47 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Khan", "Mina", ""], ["Srivatsa", "P", ""], ["Rane", "Advait", ""], ["Chenniappa", "Shriram", ""], ["Hazariwala", "Asadali", ""], ["Maes", "Pattie", ""]]}, {"id": "2106.01504", "submitter": "Paul McLachlan", "authors": "Ryan Killea, Yun Li, Saeed Bastani, Paul McLachlan", "title": "DeepCompress: Efficient Point Cloud Geometry Compression", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Point clouds are a basic data type that is increasingly of interest as 3D\ncontent becomes more ubiquitous. Applications using point clouds include\nvirtual, augmented, and mixed reality and autonomous driving. We propose a more\nefficient deep learning-based encoder architecture for point clouds compression\nthat incorporates principles from established 3D object detection and image\ncompression architectures. Through an ablation study, we show that\nincorporating the learned activation function from Computational Efficient\nNeural Image Compression (CENIC) and designing more parameter-efficient\nconvolutional blocks yields dramatic gains in efficiency and performance. Our\nproposed architecture incorporates Generalized Divisive Normalization\nactivations and propose a spatially separable InceptionV4-inspired block. We\nthen evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized\nFull Bodies dataset to evaluate our model's performance. Our proposed\nmodifications outperform the baseline approaches by a small margin in terms of\nBjontegard delta rate and PSNR values, yet reduces necessary encoder\nconvolution operations by 8 percent and reduces total encoder parameters by 20\npercent. Our proposed architecture, when considered on its own, has a small\npenalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit\nrate in Point to Plane Distance for the same peak signal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 23:18:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Killea", "Ryan", ""], ["Li", "Yun", ""], ["Bastani", "Saeed", ""], ["McLachlan", "Paul", ""]]}, {"id": "2106.01505", "submitter": "Peihao Zhu", "authors": "Peihao Zhu, Rameen Abdal, John Femiani, Peter Wonka", "title": "Barbershop: GAN-based Image Compositing using Segmentation Masks", "comments": "Project page: https://zpdesu.github.io/Barbershop/ Video:\n  https://youtu.be/ZU-yrAvoJfQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Seamlessly blending features from multiple images is extremely challenging\nbecause of complex relationships in lighting, geometry, and partial occlusion\nwhich cause coupling between different parts of the image. Even though recent\nwork on GANs enables synthesis of realistic hair or faces, it remains difficult\nto combine them into a single, coherent, and plausible image rather than a\ndisjointed set of image patches. We present a novel solution to image blending,\nparticularly for the problem of hairstyle transfer, based on GAN-inversion. We\npropose a novel latent space for image blending which is better at preserving\ndetail and encoding spatial information, and propose a new GAN-embedding\nalgorithm which is able to slightly modify images to conform to a common\nsegmentation mask. Our novel representation enables the transfer of the visual\nproperties from multiple reference images including specific details such as\nmoles and wrinkles, and because we do image blending in a latent-space we are\nable to synthesize images that are coherent. Our approach avoids blending\nartifacts present in other approaches and finds a globally consistent image.\nOur results demonstrate a significant improvement over the current state of the\nart in a user study, with users preferring our blending solution over 95\npercent of the time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 23:20:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhu", "Peihao", ""], ["Abdal", "Rameen", ""], ["Femiani", "John", ""], ["Wonka", "Peter", ""]]}, {"id": "2106.01532", "submitter": "Ang Li", "authors": "Ang Li, Qiuhong Ke, Xingjun Ma, Haiqin Weng, Zhiyuan Zong, Feng Xue,\n  Rui Zhang", "title": "Noise Doesn't Lie: Towards Universal Detection of Deep Inpainting", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image inpainting aims to restore damaged or missing regions in an image\nwith realistic contents. While having a wide range of applications such as\nobject removal and image recovery, deep inpainting techniques also have the\nrisk of being manipulated for image forgery. A promising countermeasure against\nsuch forgeries is deep inpainting detection, which aims to locate the inpainted\nregions in an image. In this paper, we make the first attempt towards universal\ndetection of deep inpainting, where the detection network can generalize well\nwhen detecting different deep inpainting methods. To this end, we first propose\na novel data generation approach to generate a universal training dataset,\nwhich imitates the noise discrepancies exist in real versus inpainted image\ncontents to train universal detectors. We then design a Noise-Image\nCross-fusion Network (NIX-Net) to effectively exploit the discriminative\ninformation contained in both the images and their noise patterns. We\nempirically show, on multiple benchmark datasets, that our approach outperforms\nexisting detection methods by a large margin and generalize well to unseen deep\ninpainting techniques. Our universal training dataset can also significantly\nboost the generalizability of existing detection methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:29:29 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Li", "Ang", ""], ["Ke", "Qiuhong", ""], ["Ma", "Xingjun", ""], ["Weng", "Haiqin", ""], ["Zong", "Zhiyuan", ""], ["Xue", "Feng", ""], ["Zhang", "Rui", ""]]}, {"id": "2106.01534", "submitter": "Xun Yang", "authors": "Xun Yang, Fuli Feng, Wei Ji, Meng Wang, Tat-Seng Chua", "title": "Deconfounded Video Moment Retrieval with Causal Intervention", "comments": "This work has been accepted by SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the task of video moment retrieval (VMR), which aims to localize a\nspecific moment in a video according to a textual query. Existing methods\nprimarily model the matching relationship between query and moment by complex\ncross-modal interactions. Despite their effectiveness, current models mostly\nexploit dataset biases while ignoring the video content, thus leading to poor\ngeneralizability. We argue that the issue is caused by the hidden confounder in\nVMR, {i.e., temporal location of moments}, that spuriously correlates the model\ninput and prediction. How to design robust matching models against the temporal\nlocation biases is crucial but, as far as we know, has not been studied yet for\nVMR.\n  To fill the research gap, we propose a causality-inspired VMR framework that\nbuilds structural causal model to capture the true effect of query and video\ncontent on the prediction. Specifically, we develop a Deconfounded Cross-modal\nMatching (DCM) method to remove the confounding effects of moment location. It\nfirst disentangles moment representation to infer the core feature of visual\ncontent, and then applies causal intervention on the disentangled multimodal\ninput based on backdoor adjustment, which forces the model to fairly\nincorporate each possible location of the target into consideration. Extensive\nexperiments clearly show that our approach can achieve significant improvement\nover the state-of-the-art methods in terms of both accuracy and generalization\n(Codes:\n\\color{blue}{\\url{https://github.com/Xun-Yang/Causal_Video_Moment_Retrieval}}\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:33:26 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Yang", "Xun", ""], ["Feng", "Fuli", ""], ["Ji", "Wei", ""], ["Wang", "Meng", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2106.01538", "submitter": "Alexander Matyasko", "authors": "Alexander Matyasko, Lap-Pui Chau", "title": "PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural networks are sensitive to small input\nperturbations. Since the discovery of this intriguing vulnerability, many\ndefence methods have been proposed that attempt to improve robustness to\nadversarial noise. Fast and accurate attacks are required to compare various\ndefence methods. However, evaluating adversarial robustness has proven to be\nextremely challenging. Existing norm minimisation adversarial attacks require\nthousands of iterations (e.g. Carlini & Wagner attack), are limited to the\nspecific norms (e.g. Fast Adaptive Boundary), or produce sub-optimal results\n(e.g. Brendel & Bethge attack). On the other hand, PGD attack, which is fast,\ngeneral and accurate, ignores the norm minimisation penalty and solves a\nsimpler perturbation-constrained problem. In this work, we introduce a fast,\ngeneral and accurate adversarial attack that optimises the original non-convex\nconstrained minimisation problem. We interpret optimising the Lagrangian of the\nadversarial attack optimisation problem as a two-player game: the first player\nminimises the Lagrangian wrt the adversarial noise; the second player maximises\nthe Lagrangian wrt the regularisation penalty. Our attack algorithm\nsimultaneously optimises primal and dual variables to find the minimal\nadversarial perturbation. In addition, for non-smooth $l_p$-norm minimisation,\nsuch as $l_{\\infty}$-, $l_1$-, and $l_0$-norms, we introduce primal-dual\nproximal gradient descent attack. We show in the experiments that our attack\noutperforms current state-of-the-art $l_{\\infty}$-, $l_2$-, $l_1$-, and\n$l_0$-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against\nunregularised and adversarially trained models.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:45:48 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Matyasko", "Alexander", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "2106.01544", "submitter": "Hong-Yu Zhou", "authors": "Hong-Yu Zhou, Chengdi Wang, Haofeng Li, Gang Wang, Shu Zhang, Weimin\n  Li, Yizhou Yu", "title": "SSMD: Semi-Supervised Medical Image Detection with Adaptive Consistency\n  and Heterogeneous Perturbation", "comments": "Accepted by Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semi-Supervised classification and segmentation methods have been widely\ninvestigated in medical image analysis. Both approaches can improve the\nperformance of fully-supervised methods with additional unlabeled data.\nHowever, as a fundamental task, semi-supervised object detection has not gained\nenough attention in the field of medical image analysis. In this paper, we\npropose a novel Semi-Supervised Medical image Detector (SSMD). The motivation\nbehind SSMD is to provide free yet effective supervision for unlabeled data, by\nregularizing the predictions at each position to be consistent. To achieve the\nabove idea, we develop a novel adaptive consistency cost function to regularize\ndifferent components in the predictions. Moreover, we introduce heterogeneous\nperturbation strategies that work in both feature space and image space, so\nthat the proposed detector is promising to produce powerful image\nrepresentations and robust predictions. Extensive experimental results show\nthat the proposed SSMD achieves the state-of-the-art performance at a wide\nrange of settings. We also demonstrate the strength of each proposed module\nwith comprehensive ablation studies.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 01:59:50 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhou", "Hong-Yu", ""], ["Wang", "Chengdi", ""], ["Li", "Haofeng", ""], ["Wang", "Gang", ""], ["Zhang", "Shu", ""], ["Li", "Weimin", ""], ["Yu", "Yizhou", ""]]}, {"id": "2106.01548", "submitter": "Xiangning Chen", "authors": "Xiangning Chen, Cho-Jui Hsieh, Boqing Gong", "title": "When Vision Transformers Outperform ResNets without Pretraining or\n  Strong Data Augmentations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision Transformers (ViTs) and MLPs signal further efforts on replacing\nhand-wired features or inductive biases with general-purpose neural\narchitectures. Existing works empower the models by massive data, such as\nlarge-scale pretraining and/or repeated strong data augmentations, and still\nreport optimization-related problems (e.g., sensitivity to initialization and\nlearning rate). Hence, this paper investigates ViTs and MLP-Mixers from the\nlens of loss geometry, intending to improve the models' data efficiency at\ntraining and generalization at inference. Visualization and Hessian reveal\nextremely sharp local minima of converged models. By promoting smoothness with\na recently proposed sharpness-aware optimizer, we substantially improve the\naccuracy and robustness of ViTs and MLP-Mixers on various tasks spanning\nsupervised, adversarial, contrastive, and transfer learning (e.g., +5.3\\% and\n+11.0\\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,\nwith the simple Inception-style preprocessing). We show that the improved\nsmoothness attributes to sparser active neurons in the first few layers. The\nresultant ViTs outperform ResNets of similar size and throughput when trained\nfrom scratch on ImageNet without large-scale pretraining or strong data\naugmentations. They also possess more perceptive attention maps.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 02:08:03 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Chen", "Xiangning", ""], ["Hsieh", "Cho-Jui", ""], ["Gong", "Boqing", ""]]}, {"id": "2106.01553", "submitter": "Peng-Shuai Wang", "authors": "Peng-Shuai Wang, Yang Liu, Yu-Qi Yang, Xin Tong", "title": "Spline Positional Encoding for Learning 3D Implicit Signed Distance\n  Fields", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer perceptrons (MLPs) have been successfully used to represent 3D\nshapes implicitly and compactly, by mapping 3D coordinates to the corresponding\nsigned distance values or occupancy values. In this paper, we propose a novel\npositional encoding scheme, called Spline Positional Encoding, to map the input\ncoordinates to a high dimensional space before passing them to MLPs, for\nhelping to recover 3D signed distance fields with fine-scale geometric details\nfrom unorganized 3D point clouds. We verified the superiority of our approach\nover other positional encoding schemes on tasks of 3D shape reconstruction from\ninput point clouds and shape space learning. The efficacy of our approach\nextended to image reconstruction is also demonstrated and evaluated.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 02:37:47 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wang", "Peng-Shuai", ""], ["Liu", "Yang", ""], ["Yang", "Yu-Qi", ""], ["Tong", "Xin", ""]]}, {"id": "2106.01596", "submitter": "Ho Hin Lee", "authors": "Ho Hin Lee, Yucheng Tang, Qi Yang, Xin Yu, Shunxing Bao, Bennett A.\n  Landman, Yuankai Huo", "title": "Attention-Guided Supervised Contrastive Learning for Semantic\n  Segmentation", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has shown superior performance in embedding global and\nspatial invariant features in computer vision (e.g., image classification).\nHowever, its overall success of embedding local and spatial variant features is\nstill limited, especially for semantic segmentation. In a per-pixel prediction\ntask, more than one label can exist in a single image for segmentation (e.g.,\nan image contains both cat, dog, and grass), thereby it is difficult to define\n'positive' or 'negative' pairs in a canonical contrastive learning setting. In\nthis paper, we propose an attention-guided supervised contrastive learning\napproach to highlight a single semantic object every time as the target. With\nour design, the same image can be embedded to different semantic clusters with\nsemantic attention (i.e., coerce semantic masks) as an additional input\nchannel. To achieve such attention, a novel two-stage training strategy is\npresented. We evaluate the proposed method on multi-organ medical image\nsegmentation task, as our major task, with both in-house data and BTCV 2015\ndatasets. Comparing with the supervised and semi-supervised training\nstate-of-the-art in the backbone of ResNet-50, our proposed pipeline yields\nsubstantial improvement of 5.53% and 6.09% in Dice score for both medical image\nsegmentation cohorts respectively. The performance of the proposed method on\nnatural images is assessed via PASCAL VOC 2012 dataset, and achieves 2.75%\nsubstantial improvement.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:01:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Lee", "Ho Hin", ""], ["Tang", "Yucheng", ""], ["Yang", "Qi", ""], ["Yu", "Xin", ""], ["Bao", "Shunxing", ""], ["Landman", "Bennett A.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2106.01603", "submitter": "Xianhang Li", "authors": "Kunchang Li, Xianhang Li, Yali Wang, Jun Wang and Yu Qiao", "title": "CT-Net: Channel Tensorization Network for Video Classification", "comments": "ICLR 2021. Code is available on https://github.com/Andy1621/CT-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D convolution is powerful for video classification but often computationally\nexpensive, recent studies mainly focus on decomposing it on spatial-temporal\nand/or channel dimensions. Unfortunately, most approaches fail to achieve a\npreferable balance between convolutional efficiency and feature-interaction\nsufficiency. For this reason, we propose a concise and novel Channel\nTensorization Network (CT-Net), by treating the channel dimension of input\nfeature as a multiplication of K sub-dimensions. On one hand, it naturally\nfactorizes convolution in a multiple dimension way, leading to a light\ncomputation burden. On the other hand, it can effectively enhance feature\ninteraction from different channels, and progressively enlarge the 3D receptive\nfield of such interaction to boost classification accuracy. Furthermore, we\nequip our CT-Module with a Tensor Excitation (TE) mechanism. It can learn to\nexploit spatial, temporal and channel attention in a high-dimensional manner,\nto improve the cooperative power of all the feature dimensions in our\nCT-Module. Finally, we flexibly adapt ResNet as our CT-Net. Extensive\nexperiments are conducted on several challenging video benchmarks, e.g.,\nKinetics-400, Something-Something V1 and V2. Our CT-Net outperforms a number of\nrecent SOTA approaches, in terms of accuracy and/or efficiency. The codes and\nmodels will be available on https://github.com/Andy1621/CT-Net.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:35:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Li", "Kunchang", ""], ["Li", "Xianhang", ""], ["Wang", "Yali", ""], ["Wang", "Jun", ""], ["Qiao", "Yu", ""]]}, {"id": "2106.01606", "submitter": "Yinpeng Dong", "authors": "Yinpeng Dong, Ke Xu, Xiao Yang, Tianyu Pang, Zhijie Deng, Hang Su, Jun\n  Zhu", "title": "Exploring Memorization in Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that deep learning models have a propensity for fitting the\nentire training set even with random labels, which requires memorization of\nevery training sample. In this paper, we investigate the memorization effect in\nadversarial training (AT) for promoting a deeper understanding of capacity,\nconvergence, generalization, and especially robust overfitting of adversarially\ntrained classifiers. We first demonstrate that deep networks have sufficient\ncapacity to memorize adversarial examples of training data with completely\nrandom labels, but not all AT algorithms can converge under the extreme\ncircumstance. Our study of AT with random labels motivates further analyses on\nthe convergence and generalization of AT. We find that some AT methods suffer\nfrom a gradient instability issue, and the recently suggested complexity\nmeasures cannot explain robust generalization by considering models trained on\nrandom labels. Furthermore, we identify a significant drawback of memorization\nin AT that it could result in robust overfitting. We then propose a new\nmitigation algorithm motivated by detailed memorization analyses. Extensive\nexperiments on various datasets validate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:39:57 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Dong", "Yinpeng", ""], ["Xu", "Ke", ""], ["Yang", "Xiao", ""], ["Pang", "Tianyu", ""], ["Deng", "Zhijie", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2106.01607", "submitter": "Satyapriya Krishna", "authors": "Michiel de Jong, Satyapriya Krishna, Anuva Agarwal", "title": "Grounding Complex Navigational Instructions Using Scene Graphs", "comments": "arXiv admin note: text overlap with arXiv:1706.07230 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training a reinforcement learning agent to carry out natural language\ninstructions is limited by the available supervision, i.e. knowing when the\ninstruction has been carried out. We adapt the CLEVR visual question answering\ndataset to generate complex natural language navigation instructions and\naccompanying scene graphs, yielding an environment-agnostic supervised dataset.\nTo demonstrate the use of this data set, we map the scenes to the VizDoom\nenvironment and use the architecture in \\citet{gatedattention} to train an\nagent to carry out these more complex language instructions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:45:21 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["de Jong", "Michiel", ""], ["Krishna", "Satyapriya", ""], ["Agarwal", "Anuva", ""]]}, {"id": "2106.01615", "submitter": "Quanyu Liao", "authors": "Quanyu Liao, Yuezun Li, Xin Wang, Bin Kong, Bin Zhu, Siwei Lyu,\n  Youbing Yin, Qi Song, Xi Wu", "title": "Imperceptible Adversarial Examples for Fake Image Detection", "comments": "Accepted by ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fooling people with highly realistic fake images generated with Deepfake or\nGANs brings a great social disturbance to our society. Many methods have been\nproposed to detect fake images, but they are vulnerable to adversarial\nperturbations -- intentionally designed noises that can lead to the wrong\nprediction. Existing methods of attacking fake image detectors usually generate\nadversarial perturbations to perturb almost the entire image. This is redundant\nand increases the perceptibility of perturbations. In this paper, we propose a\nnovel method to disrupt the fake image detection by determining key pixels to a\nfake image detector and attacking only the key pixels, which results in the\n$L_0$ and the $L_2$ norms of adversarial perturbations much less than those of\nexisting works. Experiments on two public datasets with three fake image\ndetectors indicate that our proposed method achieves state-of-the-art\nperformance in both white-box and black-box attacks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 06:25:04 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liao", "Quanyu", ""], ["Li", "Yuezun", ""], ["Wang", "Xin", ""], ["Kong", "Bin", ""], ["Zhu", "Bin", ""], ["Lyu", "Siwei", ""], ["Yin", "Youbing", ""], ["Song", "Qi", ""], ["Wu", "Xi", ""]]}, {"id": "2106.01617", "submitter": "Pengfei Xie", "authors": "Pengfei Xie, Linyuan Wang, Ruoxi Qin, Kai Qiao, Shuhao Shi, Guoen Hu,\n  Bin Yan", "title": "Improving the Transferability of Adversarial Examples with New Iteration\n  Framework and Input Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks(DNNs) is vulnerable to be attacked by adversarial\nexamples. Black-box attack is the most threatening attack. At present,\nblack-box attack methods mainly adopt gradient-based iterative attack methods,\nwhich usually limit the relationship between the iteration step size, the\nnumber of iterations, and the maximum perturbation. In this paper, we propose a\nnew gradient iteration framework, which redefines the relationship between the\nabove three. Under this framework, we easily improve the attack success rate of\nDI-TI-MIM. In addition, we propose a gradient iterative attack method based on\ninput dropout, which can be well combined with our framework. We further\npropose a multi dropout rate version of this method. Experimental results show\nthat our best method can achieve attack success rate of 96.2\\% for defense\nmodel on average, which is higher than the state-of-the-art gradient-based\nattacks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 06:36:38 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 19:45:04 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Xie", "Pengfei", ""], ["Wang", "Linyuan", ""], ["Qin", "Ruoxi", ""], ["Qiao", "Kai", ""], ["Shi", "Shuhao", ""], ["Hu", "Guoen", ""], ["Yan", "Bin", ""]]}, {"id": "2106.01618", "submitter": "Quanyu Liao", "authors": "Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Bin Zhu, Youbing Yin, Qi\n  Song, Xi Wu", "title": "Transferable Adversarial Examples for Anchor Free Object Detection", "comments": "Accepted as oral in ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been demonstrated to be vulnerable to adversarial\nattacks: subtle perturbation can completely change prediction result. The\nvulnerability has led to a surge of research in this direction, including\nadversarial attacks on object detection networks. However, previous studies are\ndedicated to attacking anchor-based object detectors. In this paper, we present\nthe first adversarial attack on anchor-free object detectors. It conducts\ncategory-wise, instead of previously instance-wise, attacks on object\ndetectors, and leverages high-level semantic information to efficiently\ngenerate transferable adversarial examples, which can also be transferred to\nattack other object detectors, even anchor-based detectors such as Faster\nR-CNN. Experimental results on two benchmark datasets demonstrate that our\nproposed method achieves state-of-the-art performance and transferability.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 06:38:15 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 01:59:22 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Liao", "Quanyu", ""], ["Wang", "Xin", ""], ["Kong", "Bin", ""], ["Lyu", "Siwei", ""], ["Zhu", "Bin", ""], ["Yin", "Youbing", ""], ["Song", "Qi", ""], ["Wu", "Xi", ""]]}, {"id": "2106.01629", "submitter": "Guillaume Le Moing", "authors": "Guillaume Le Moing and Tuan-Hung Vu and Himalaya Jain and Patrick\n  P\\'erez and Matthieu Cord", "title": "Semantic Palette: Guiding Scene Generation with Class Proportions", "comments": "Accepted to IEEE CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent progress of generative adversarial networks (GANs) at\nsynthesizing photo-realistic images, producing complex urban scenes remains a\nchallenging problem. Previous works break down scene generation into two\nconsecutive phases: unconditional semantic layout synthesis and image synthesis\nconditioned on layouts. In this work, we propose to condition layout generation\nas well for higher semantic control: given a vector of class proportions, we\ngenerate layouts with matching composition. To this end, we introduce a\nconditional framework with novel architecture designs and learning objectives,\nwhich effectively accommodates class proportions to guide the scene generation\nprocess. The proposed architecture also allows partial layout editing with\ninteresting applications. Thanks to the semantic control, we can produce\nlayouts close to the real distribution, helping enhance the whole scene\ngeneration process. On different metrics and urban scene benchmarks, our models\noutperform existing baselines. Moreover, we demonstrate the merit of our\napproach for data augmentation: semantic segmenters trained on real\nlayout-image pairs along with additional ones generated by our approach\noutperform models only trained on real pairs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 07:04:00 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Moing", "Guillaume Le", ""], ["Vu", "Tuan-Hung", ""], ["Jain", "Himalaya", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "2106.01656", "submitter": "Yu Mitsuzumi", "authors": "Yu Mitsuzumi, Go Irie, Daiki Ikami and Takashi Shibata", "title": "Generalized Domain Adaptation", "comments": "Accepted by CVPR 2021. Code is available at\n  https://github.com/nttcslab/Generalized-Domain-Adaptation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many variants of unsupervised domain adaptation (UDA) problems have been\nproposed and solved individually. Its side effect is that a method that works\nfor one variant is often ineffective for or not even applicable to another,\nwhich has prevented practical applications. In this paper, we give a general\nrepresentation of UDA problems, named Generalized Domain Adaptation (GDA). GDA\ncovers the major variants as special cases, which allows us to organize them in\na comprehensive framework. Moreover, this generalization leads to a new\nchallenging setting where existing methods fail, such as when domain labels are\nunknown, and class labels are only partially given to each domain. We propose a\nnovel approach to the new setting. The key to our approach is self-supervised\nclass-destructive learning, which enables the learning of class-invariant\nrepresentations and domain-adversarial classifiers without using any domain\nlabels. Extensive experiments using three benchmark datasets demonstrate that\nour method outperforms the state-of-the-art UDA methods in the new setting and\nthat it is competitive in existing UDA variations as well.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 07:55:18 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Mitsuzumi", "Yu", ""], ["Irie", "Go", ""], ["Ikami", "Daiki", ""], ["Shibata", "Takashi", ""]]}, {"id": "2106.01667", "submitter": "Juan Leon Alcazar", "authors": "Juan Leon Alcazar, Long Mai, Federico Perazzi, Joon-Young Lee, Pablo\n  Arbelaez, Bernard Ghanem, and Fabian Caba Heilbron", "title": "APES: Audiovisual Person Search in Untrimmed Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are arguably one of the most important subjects in video streams, many\nreal-world applications such as video summarization or video editing workflows\noften require the automatic search and retrieval of a person of interest.\nDespite tremendous efforts in the person reidentification and retrieval\ndomains, few works have developed audiovisual search strategies. In this paper,\nwe present the Audiovisual Person Search dataset (APES), a new dataset composed\nof untrimmed videos whose audio (voices) and visual (faces) streams are densely\nannotated. APES contains over 1.9K identities labeled along 36 hours of video,\nmaking it the largest dataset available for untrimmed audiovisual person\nsearch. A key property of APES is that it includes dense temporal annotations\nthat link faces to speech segments of the same identity. To showcase the\npotential of our new dataset, we propose an audiovisual baseline and benchmark\nfor person retrieval. Our study shows that modeling audiovisual cues benefits\nthe recognition of people's identities. To enable reproducibility and promote\nfuture research, the dataset annotations and baseline code are available at:\nhttps://github.com/fuankarion/audiovisual-person-search\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:16:42 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Alcazar", "Juan Leon", ""], ["Mai", "Long", ""], ["Perazzi", "Federico", ""], ["Lee", "Joon-Young", ""], ["Arbelaez", "Pablo", ""], ["Ghanem", "Bernard", ""], ["Heilbron", "Fabian Caba", ""]]}, {"id": "2106.01689", "submitter": "Mirco Planamente", "authors": "Mirco Planamente, Chiara Plizzari, Emanuele Alberti, Barbara Caputo", "title": "Cross-Domain First Person Audio-Visual Action Recognition through\n  Relative Norm Alignment", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First person action recognition is an increasingly researched topic because\nof the growing popularity of wearable cameras. This is bringing to light\ncross-domain issues that are yet to be addressed in this context. Indeed, the\ninformation extracted from learned representations suffers from an intrinsic\nenvironmental bias. This strongly affects the ability to generalize to unseen\nscenarios, limiting the application of current methods in real settings where\ntrimmed labeled data are not available during training. In this work, we\npropose to leverage over the intrinsic complementary nature of audio-visual\nsignals to learn a representation that works well on data seen during training,\nwhile being able to generalize across different domains. To this end, we\nintroduce an audio-visual loss that aligns the contributions from the two\nmodalities by acting on the magnitude of their feature norm representations.\nThis new loss, plugged into a minimal multi-modal action recognition\narchitecture, leads to strong results in cross-domain first person action\nrecognition, as demonstrated by extensive experiments on the popular\nEPIC-Kitchens dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:46:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Planamente", "Mirco", ""], ["Plizzari", "Chiara", ""], ["Alberti", "Emanuele", ""], ["Caputo", "Barbara", ""]]}, {"id": "2106.01700", "submitter": "Neslihan Bayramoglu", "authors": "Neslihan Bayramoglu, Miika T. Nieminen, Simo Saarakkala", "title": "Machine Learning Based Texture Analysis of Patella from X-Rays for\n  Detecting Patellofemoral Osteoarthritis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective is to assess the ability of texture features for detecting\nradiographic patellofemoral osteoarthritis (PFOA) from knee lateral view\nradiographs. We used lateral view knee radiographs from MOST public use\ndatasets (n = 5507 knees). Patellar region-of-interest (ROI) was automatically\ndetected using landmark detection tool (BoneFinder). Hand-crafted features,\nbased on LocalBinary Patterns (LBP), were then extracted to describe the\npatellar texture. First, a machine learning model (Gradient Boosting Machine)\nwas trained to detect radiographic PFOA from the LBP features. Furthermore, we\nused end-to-end trained deep convolutional neural networks (CNNs) directly on\nthe texture patches for detecting the PFOA. The proposed classification models\nwere eventually compared with more conventional reference models that use\nclinical assessments and participant characteristics such as age, sex, body\nmass index(BMI), the total WOMAC score, and tibiofemoral Kellgren-Lawrence (KL)\ngrade. Atlas-guided visual assessment of PFOA status by expert readers provided\nin the MOST public use datasets was used as a classification outcome for the\nmodels. Performance of prediction models was assessed using the area under the\nreceiver operating characteristic curve (ROC AUC), the area under the\nprecision-recall (PR) curve-average precision (AP)-, and Brier score in the\nstratified 5-fold cross validation setting.Of the 5507 knees, 953 (17.3%) had\nPFOA. AUC and AP for the strongest reference model including age, sex, BMI,\nWOMAC score, and tibiofemoral KL grade to predict PFOA were 0.817 and 0.487,\nrespectively. Textural ROI classification using CNN significantly improved the\nprediction performance (ROC AUC= 0.889, AP= 0.714). We present the first study\nthat analyses patellar bone texture for diagnosing PFOA. Our results\ndemonstrates the potential of using texture features of patella to predict\nPFOA.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:03:31 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 07:56:43 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bayramoglu", "Neslihan", ""], ["Nieminen", "Miika T.", ""], ["Saarakkala", "Simo", ""]]}, {"id": "2106.01717", "submitter": "Marc Blanchon", "authors": "Marc Blanchon, D\\'esir\\'e Sidib\\'e, Olivier Morel, Ralph Seulin,\n  Fabrice Meriaudeau", "title": "Towards urban scenes understanding through polarization cues", "comments": "Submitted to Autonomous Robots - Special Issue on Unconventional\n  Sensors in Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous robotics is critically affected by the robustness of its scene\nunderstanding algorithms. We propose a two-axis pipeline based on polarization\nindices to analyze dynamic urban scenes. As robots evolve in unknown\nenvironments, they are prone to encountering specular obstacles. Usually,\nspecular phenomena are rarely taken into account by algorithms which causes\nmisinterpretations and erroneous estimates. By exploiting all the light\nproperties, systems can greatly increase their robustness to events. In\naddition to the conventional photometric characteristics, we propose to include\npolarization sensing.\n  We demonstrate in this paper that the contribution of polarization\nmeasurement increases both the performances of segmentation and the quality of\ndepth estimation. Our polarimetry-based approaches are compared here with other\nstate-of-the-art RGB-centric methods showing interest of using polarization\nimaging.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:40:08 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Blanchon", "Marc", ""], ["Sidib\u00e9", "D\u00e9sir\u00e9", ""], ["Morel", "Olivier", ""], ["Seulin", "Ralph", ""], ["Meriaudeau", "Fabrice", ""]]}, {"id": "2106.01718", "submitter": "Hiroyasu Katsuno", "authors": "Hiroyasu Katsuno, Yuki Kimura, Tomoya Yamazaki and Ichigaku Takigawa", "title": "Fast improvement of TEM image with low-dose electrons by deep learning", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-electron-dose observation is indispensable for observing various samples\nusing a transmission electron microscope; consequently, image processing has\nbeen used to improve transmission electron microscopy (TEM) images. To apply\nsuch image processing to in situ observations, we here apply a convolutional\nneural network to TEM imaging. Using a dataset that includes short-exposure\nimages and long-exposure images, we develop a pipeline for processed\nshort-exposure images, based on end-to-end training. The quality of images\nacquired with a total dose of approximately 5 e- per pixel becomes comparable\nto that of images acquired with a total dose of approximately 1000 e- per\npixel. Because the conversion time is approximately 8 ms, in situ observation\nat 125 fps is possible. This imaging technique enables in situ observation of\nelectron-beam-sensitive specimens.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:42:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Katsuno", "Hiroyasu", ""], ["Kimura", "Yuki", ""], ["Yamazaki", "Tomoya", ""], ["Takigawa", "Ichigaku", ""]]}, {"id": "2106.01722", "submitter": "Weijin Zhu", "authors": "Weijin Zhu, Yao Shen, Linfeng Yu, Lizeth Patricia Aguirre Sanchez", "title": "GMAIR: Unsupervised Object Detection Based on Spatial Attention and\n  Gaussian Mixture", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies on unsupervised object detection based on spatial attention\nhave achieved promising results. Models, such as AIR and SPAIR, output \"what\"\nand \"where\" latent variables that represent the attributes and locations of\nobjects in a scene, respectively. Most of the previous studies concentrate on\nthe \"where\" localization performance; however, we claim that acquiring \"what\"\nobject attributes is also essential for representation learning. This paper\npresents a framework, GMAIR, for unsupervised object detection. It incorporates\nspatial attention and a Gaussian mixture in a unified deep generative model.\nGMAIR can locate objects in a scene and simultaneously cluster them without\nsupervision. Furthermore, we analyze the \"what\" latent variables and clustering\nprocess. Finally, we evaluate our model on MultiMNIST and Fruit2D datasets and\nshow that GMAIR achieves competitive results on localization and clustering\ncompared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:50:13 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhu", "Weijin", ""], ["Shen", "Yao", ""], ["Yu", "Linfeng", ""], ["Sanchez", "Lizeth Patricia Aguirre", ""]]}, {"id": "2106.01739", "submitter": "Aditya Jyoti Paul", "authors": "Aditya Jyoti Paul", "title": "Advances in Classifying the Stages of Diabetic Retinopathy Using\n  Convolutional Neural Networks in Low Memory Edge Devices", "comments": "This paper is currently under review at IEEE MASCON 2021.\n  http://ieeemascon.in", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy (DR) is a severe complication that may lead to retinal\nvascular damage and is one of the leading causes of vision impairment and\nblindness. DR broadly is classified into two stages - non-proliferative (NPDR),\nwhere there are almost no symptoms, except a few microaneurysms, and\nproliferative (PDR) involving a huge number of microaneurysms and hemorrhages,\nsoft and hard exudates, neo-vascularization, macular ischemia or a combination\nof these, making it easier to detect. More specifically, DR is usually\nclassified into five levels, labeled 0-4, from 0 indicating no DR to 4 which is\nmost severe. This paper firstly presents a discussion on the risk factors of\nthe disease, then surveys the recent literature on the topic followed by\nexamining certain techniques which were found to be highly effective in\nimproving the prognosis accuracy. Finally, a convolutional neural network model\nis proposed to detect all the stages of DR on a low-memory edge\nmicrocontroller. The model has a size of just 5.9 MB, accuracy and F1 score\nboth of 94% and an inference speed of about 20 frames per second.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 10:40:54 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Paul", "Aditya Jyoti", ""]]}, {"id": "2106.01744", "submitter": "Ye Chao Bai", "authors": "Yechao Bai, Ziyuan Huang, Lyuyu Shen, Hongliang Guo, Marcelo H. Ang Jr\n  and Daniela Rus", "title": "Multi-Scale Feature Aggregation by Cross-Scale Pixel-to-Region Relation\n  Operation for Semantic Segmentation", "comments": "Accepted to RA-L 2021. in IEEE Robotics and Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2021.3086419", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting multi-scale features has shown great potential in tackling\nsemantic segmentation problems. The aggregation is commonly done with sum or\nconcatenation (concat) followed by convolutional (conv) layers. However, it\nfully passes down the high-level context to the following hierarchy without\nconsidering their interrelation. In this work, we aim to enable the low-level\nfeature to aggregate the complementary context from adjacent high-level feature\nmaps by a cross-scale pixel-to-region relation operation. We leverage\ncross-scale context propagation to make the long-range dependency capturable\neven by the high-resolution low-level features. To this end, we employ an\nefficient feature pyramid network to obtain multi-scale features. We propose a\nRelational Semantics Extractor (RSE) and Relational Semantics Propagator (RSP)\nfor context extraction and propagation respectively. Then we stack several RSP\ninto an RSP head to achieve the progressive top-down distribution of the\ncontext. Experiment results on two challenging datasets Cityscapes and COCO\ndemonstrate that the RSP head performs competitively on both semantic\nsegmentation and panoptic segmentation with high efficiency. It outperforms\nDeeplabV3 [1] by 0.7% with 75% fewer FLOPs (multiply-adds) in the semantic\nsegmentation task.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 10:49:48 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bai", "Yechao", ""], ["Huang", "Ziyuan", ""], ["Shen", "Lyuyu", ""], ["Guo", "Hongliang", ""], ["Ang", "Marcelo H.", "Jr"], ["Rus", "Daniela", ""]]}, {"id": "2106.01764", "submitter": "Kezhou Lin", "authors": "Kezhou Lin and Xiaohan Wang and Zhedong Zheng and Linchao Zhu and Yi\n  Yang", "title": "Less is More: Sparse Sampling for Dense Reaction Predictions", "comments": "Code is available at:\n  https://github.com/HenryLittle/EEV-Challenge-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining viewer responses from videos can be useful for creators and\nstreaming platforms to analyze the video performance and improve the future\nuser experience. In this report, we present our method for 2021 Evoked\nExpression from Videos Challenge. In particular, our model utilizes both audio\nand image modalities as inputs to predict emotion changes of viewers. To model\nlong-range emotion changes, we use a GRU-based model to predict one sparse\nsignal with 1Hz. We observe that the emotion changes are smooth. Therefore, the\nfinal dense prediction is obtained via linear interpolating the signal, which\nis robust to the prediction fluctuation. Albeit simple, the proposed method has\nachieved pearson's correlation score of 0.04430 on the final private test set.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 11:33:59 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Lin", "Kezhou", ""], ["Wang", "Xiaohan", ""], ["Zheng", "Zhedong", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""]]}, {"id": "2106.01804", "submitter": "Haiyang Xu", "authors": "Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming\n  Xiao and Fei Huang", "title": "E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual\n  Learning", "comments": "ACL2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-language pre-training (VLP) on large-scale image-text pairs has\nachieved huge success for the cross-modal downstream tasks. The most existing\npre-training methods mainly adopt a two-step training procedure, which firstly\nemploys a pre-trained object detector to extract region-based visual features,\nthen concatenates the image representation and text embedding as the input of\nTransformer to train. However, these methods face problems of using\ntask-specific visual representation of the specific object detector for generic\ncross-modal understanding, and the computation inefficiency of two-stage\npipeline. In this paper, we propose the first end-to-end vision-language\npre-trained model for both V+L understanding and generation, namely E2E-VLP,\nwhere we build a unified Transformer framework to jointly learn visual\nrepresentation, and semantic alignments between image and text. We incorporate\nthe tasks of object detection and image captioning into pre-training with a\nunified Transformer encoder-decoder architecture for enhancing visual learning.\nAn extensive set of experiments have been conducted on well-established\nvision-language downstream tasks to demonstrate the effectiveness of this novel\nVLP paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 12:50:26 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 06:56:48 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Xu", "Haiyang", ""], ["Yan", "Ming", ""], ["Li", "Chenliang", ""], ["Bi", "Bin", ""], ["Huang", "Songfang", ""], ["Xiao", "Wenming", ""], ["Huang", "Fei", ""]]}, {"id": "2106.01805", "submitter": "Tiange Xiang", "authors": "Tiange Xiang, Chaoyi Zhang, Yang Song, Siqi Liu, Hongliang Yuan,\n  Weidong Cai", "title": "Partial Graph Reasoning for Neural Network Regularization", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularizers helped deep neural networks prevent feature co-adaptations.\nDropout,as a commonly used regularization technique, stochastically disables\nneuron ac-tivations during network optimization. However, such complete feature\ndisposal can affect the feature representation and network understanding.\nToward betterdescriptions of latent representations, we present DropGraph that\nlearns regularization function by constructing a stand-alone graph from the\nbackbone features. DropGraph first samples stochastic spatial feature vectors\nand then incorporates graph reasoning methods to generate feature map\ndistortions. This add-on graph regularizes the network during training and can\nbe completely skipped during inference. We provide intuitions on the linkage\nbetween graph reasoning andDropout with further discussions on how partial\ngraph reasoning method reduces feature correlations. To this end, we\nextensively study the modeling of graphvertex dependencies and the utilization\nof the graph for distorting backbone featuremaps. DropGraph was validated on\nfour tasks with a total of 7 different datasets.The experimental results show\nthat our method outperforms other state-of-the-art regularizers while leaving\nthe base model structure unmodified during inference.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 12:57:01 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Xiang", "Tiange", ""], ["Zhang", "Chaoyi", ""], ["Song", "Yang", ""], ["Liu", "Siqi", ""], ["Yuan", "Hongliang", ""], ["Cai", "Weidong", ""]]}, {"id": "2106.01830", "submitter": "Changhee Han Dr.", "authors": "Akihiro Fukuda, Changhee Han, Kazumi Hakamada", "title": "Effort-free Automated Skeletal Abnormality Detection of Rat Fetuses on\n  Whole-body Micro-CT Scans", "comments": "5 pages, 5 figures, accepted to ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning-based fast and quantitative automated screening plays a key\nrole in analyzing human bones on Computed Tomography (CT) scans. However,\ndespite the requirement in drug safety assessment, such research is rare on\nanimal fetus micro-CT scans due to its laborious data collection and\nannotation. Therefore, we propose various bone feature engineering techniques\nto thoroughly automate the skeletal localization/labeling/abnormality detection\nof rat fetuses on whole-body micro-CT scans with minimum effort. Despite\nlimited training data of 49 fetuses, in skeletal labeling and abnormality\ndetection, we achieve accuracy of 0.900 and 0.810, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 13:31:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Fukuda", "Akihiro", ""], ["Han", "Changhee", ""], ["Hakamada", "Kazumi", ""]]}, {"id": "2106.01835", "submitter": "Pedro Carneiro Neto", "authors": "Pedro C. Neto", "title": "Deep Learning Based Analysis of Prostate Cancer from MP-MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The diagnosis of prostate cancer faces a problem with overdiagnosis that\nleads to damaging side effects due to unnecessary treatment. Research has shown\nthat the use of multi-parametric magnetic resonance images to conduct biopsies\ncan drastically help to mitigate the overdiagnosis, thus reducing the side\neffects on healthy patients. This study aims to investigate the use of deep\nlearning techniques to explore computer-aid diagnosis based on MRI as input.\nSeveral diagnosis problems ranging from classification of lesions as being\nclinically significant or not to the detection and segmentation of lesions are\naddressed with deep learning based approaches.\n  This thesis tackled two main problems regarding the diagnosis of prostate\ncancer. Firstly, XmasNet was used to conduct two large experiments on the\nclassification of lesions. Secondly, detection and segmentation experiments\nwere conducted, first on the prostate and afterward on the prostate cancer\nlesions. The former experiments explored the lesions through a two-dimensional\nspace, while the latter explored models to work with three-dimensional inputs.\nFor this task, the 3D models explored were the 3D U-Net and a pretrained 3D\nResNet-18. A rigorous analysis of all these problems was conducted with a total\nof two networks, two cropping techniques, two resampling techniques, two crop\nsizes, five input sizes and data augmentations experimented for lesion\nclassification. While for segmentation two models, two input sizes and data\naugmentations were experimented. However, while the binary classification of\nthe clinical significance of lesions and the detection and segmentation of the\nprostate already achieve the desired results (0.870 AUC and 0.915 dice score\nrespectively), the classification of the PIRADS score and the segmentation of\nlesions still have a large margin to improve (0.664 accuracy and 0.690 dice\nscore respectively).\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:42:35 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Neto", "Pedro C.", ""]]}, {"id": "2106.01860", "submitter": "Zhe Xu", "authors": "Zhe Xu, Donghuan Lu, Yixin Wang, Jie Luo, Jayender Jagadeesan, Kai Ma,\n  Yefeng Zheng, Xiu Li", "title": "Noisy Labels are Treasure: Mean-Teacher-Assisted Confident Learning for\n  Hepatic Vessel Segmentation", "comments": "11 pages, to appear in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually segmenting the hepatic vessels from Computer Tomography (CT) is far\nmore expertise-demanding and laborious than other structures due to the\nlow-contrast and complex morphology of vessels, resulting in the extreme lack\nof high-quality labeled data. Without sufficient high-quality annotations, the\nusual data-driven learning-based approaches struggle with deficient training.\nOn the other hand, directly introducing additional data with low-quality\nannotations may confuse the network, leading to undesirable performance\ndegradation. To address this issue, we propose a novel mean-teacher-assisted\nconfident learning framework to robustly exploit the noisy labeled data for the\nchallenging hepatic vessel segmentation task. Specifically, with the adapted\nconfident learning assisted by a third party, i.e., the weight-averaged teacher\nmodel, the noisy labels in the additional low-quality dataset can be\ntransformed from \"encumbrance\" to \"treasure\" via progressive pixel-wise\nsoft-correction, thus providing productive guidance. Extensive experiments\nusing two public datasets demonstrate the superiority of the proposed framework\nas well as the effectiveness of each component.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:02:45 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Xu", "Zhe", ""], ["Lu", "Donghuan", ""], ["Wang", "Yixin", ""], ["Luo", "Jie", ""], ["Jagadeesan", "Jayender", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""], ["Li", "Xiu", ""]]}, {"id": "2106.01861", "submitter": "Yuma Kinoshita", "authors": "Yuma Kinoshita and Hitoshi Kiya", "title": "Separated-Spectral-Distribution Estimation Based on Bayesian Inference\n  with Single RGB Camera", "comments": "to appear in IEEE ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel method for separately estimating spectral\ndistributions from images captured by a typical RGB camera. The proposed method\nallows us to separately estimate a spectral distribution of illumination,\nreflectance, or camera sensitivity, while recent hyperspectral cameras are\nlimited to capturing a joint spectral distribution from a scene. In addition,\nthe use of Bayesian inference makes it possible to take into account prior\ninformation of both spectral distributions and image noise as probability\ndistributions. As a result, the proposed method can estimate spectral\ndistributions in a unified way, and it can enhance the robustness of the\nestimation against noise, which conventional spectral-distribution estimation\nmethods cannot. The use of Bayesian inference also enables us to obtain the\nconfidence of estimation results. In an experiment, the proposed method is\nshown not only to outperform conventional estimation methods in terms of RMSE\nbut also to be robust against noise.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:31:47 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kinoshita", "Yuma", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2106.01862", "submitter": "Jesse Hagenaars", "authors": "Federico Paredes-Vall\\'es, Jesse Hagenaars, Guido de Croon", "title": "Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic sensing and computing hold a promise for highly energy-efficient\nand high-bandwidth-sensor processing. A major challenge for neuromorphic\ncomputing is that learning algorithms for traditional artificial neural\nnetworks (ANNs) do not transfer directly to spiking neural networks (SNNs) due\nto the discrete spikes and more complex neuronal dynamics. As a consequence,\nSNNs have not yet been successfully applied to complex, large-scale tasks. In\nthis article, we focus on the self-supervised learning problem of optical flow\nestimation from event-based camera inputs, and investigate the changes that are\nnecessary to the state-of-the-art ANN training pipeline in order to\nsuccessfully tackle it with SNNs. More specifically, we first modify the input\nevent representation to encode a much smaller time slice with minimal explicit\ntemporal information. Consequently, we make the network's neuronal dynamics and\nrecurrent connections responsible for integrating information over time.\nMoreover, we reformulate the self-supervised loss function for event-based\noptical flow to improve its convexity. We perform experiments with various\ntypes of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs,\nwe investigate the effects of elements such as parameter initialization and\noptimization, surrogate gradient shape, and adaptive neuronal mechanisms. We\nfind that initialization and surrogate gradient width play a crucial part in\nenabling learning with sparse inputs, while the inclusion of adaptivity and\nlearnable neuronal parameters can improve performance. We show that the\nperformance of the proposed ANNs and SNNs are on par with that of the current\nstate-of-the-art ANNs trained in a self-supervised manner.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:03:41 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Paredes-Vall\u00e9s", "Federico", ""], ["Hagenaars", "Jesse", ""], ["de Croon", "Guido", ""]]}, {"id": "2106.01863", "submitter": "Yuming Jiang", "authors": "Yuming Jiang, Kelvin C.K. Chan, Xintao Wang, Chen Change Loy, Ziwei\n  Liu", "title": "Robust Reference-based Super-Resolution via C2-Matching", "comments": "To appear in CVPR2021. The source code is available at\n  https://github.com/yumingj/C2-Matching", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising\nparadigm to enhance a low-resolution (LR) input image by introducing an\nadditional high-resolution (HR) reference image. Existing Ref-SR methods mostly\nrely on implicit correspondence matching to borrow HR textures from reference\nimages to compensate for the information loss in input images. However,\nperforming local transfer is difficult because of two gaps between input and\nreference images: the transformation gap (e.g. scale and rotation) and the\nresolution gap (e.g. HR and LR). To tackle these challenges, we propose\nC2-Matching in this work, which produces explicit robust matching crossing\ntransformation and resolution. 1) For the transformation gap, we propose a\ncontrastive correspondence network, which learns transformation-robust\ncorrespondences using augmented views of the input image. 2) For the resolution\ngap, we adopt a teacher-student correlation distillation, which distills\nknowledge from the easier HR-HR matching to guide the more ambiguous LR-HR\nmatching. 3) Finally, we design a dynamic aggregation module to address the\npotential misalignment issue. In addition, to faithfully evaluate the\nperformance of Ref-SR under a realistic setting, we contribute the\nWebly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario.\nExtensive experiments demonstrate that our proposed C2-Matching significantly\noutperforms state of the arts by over 1dB on the standard CUFED5 benchmark.\nNotably, it also shows great generalizability on WR-SR dataset as well as\nrobustness across large scale and rotation transformations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 16:40:36 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Jiang", "Yuming", ""], ["Chan", "Kelvin C. K.", ""], ["Wang", "Xintao", ""], ["Loy", "Chen Change", ""], ["Liu", "Ziwei", ""]]}, {"id": "2106.01866", "submitter": "Hamidreza Kasaei", "authors": "Hamidreza Kasaei, Sha Luo, Remo Sasso, Mohammadreza Kasaei", "title": "Simultaneous Multi-View Object Recognition and Grasping in Open-Ended\n  Domains", "comments": "arXiv admin note: text overlap with arXiv:2103.10997", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot working in human-centric environments needs to know which kind of\nobjects exist in the scene, where they are, and how to grasp and manipulate\nvarious objects in different situations to help humans in everyday tasks.\nTherefore, object recognition and grasping are two key functionalities for such\nrobots. Most state-of-the-art tackles object recognition and grasping as two\nseparate problems while both use visual input. Furthermore, the knowledge of\nthe robot is fixed after the training phase. In such cases, if the robot faces\nnew object categories, it must retrain from scratch to incorporate new\ninformation without catastrophic interference. To address this problem, we\npropose a deep learning architecture with augmented memory capacities to handle\nopen-ended object recognition and grasping simultaneously. In particular, our\napproach takes multi-views of an object as input and jointly estimates\npixel-wise grasp configuration as well as a deep scale- and rotation-invariant\nrepresentation as outputs. The obtained representation is then used for\nopen-ended object recognition through a meta-active learning technique. We\ndemonstrate the ability of our approach to grasp never-seen-before objects and\nto rapidly learn new object categories using very few examples on-site in both\nsimulation and real-world settings.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:12:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kasaei", "Hamidreza", ""], ["Luo", "Sha", ""], ["Sasso", "Remo", ""], ["Kasaei", "Mohammadreza", ""]]}, {"id": "2106.01883", "submitter": "Xue Yang", "authors": "Xue Yang, Xiaojiang Yang, Jirui Yang, Qi Ming, Wentao Wang, Qi Tian,\n  Junchi Yan", "title": "Learning High-Precision Bounding Box for Rotated Object Detection via\n  Kullback-Leibler Divergence", "comments": "15 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing rotated object detectors are mostly inherited from the horizontal\ndetection paradigm, as the latter has evolved into a well-developed area.\nHowever, these detectors are difficult to perform prominently in high-precision\ndetection due to the limitation of current regression loss design, especially\nfor objects with large aspect ratios. Taking the perspective that horizontal\ndetection is a special case for rotated object detection, in this paper, we are\nmotivated to change the design of rotation regression loss from induction\nparadigm to deduction methodology, in terms of the relation between rotation\nand horizontal detection. We show that one essential challenge is how to\nmodulate the coupled parameters in the rotation regression loss, as such the\nestimated parameters can influence to each other during the dynamic joint\noptimization, in an adaptive and synergetic way. Specifically, we first convert\nthe rotated bounding box into a 2-D Gaussian distribution, and then calculate\nthe Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the\nregression loss. By analyzing the gradient of each parameter, we show that KLD\n(and its derivatives) can dynamically adjust the parameter gradients according\nto the characteristics of the object. It will adjust the importance (gradient\nweight) of the angle parameter according to the aspect ratio. This mechanism\ncan be vital for high-precision detection as a slight angle error would cause a\nserious accuracy drop for large aspect ratios objects. More importantly, we\nhave proved that KLD is scale invariant. We further show that the KLD loss can\nbe degenerated into the popular $l_{n}$-norm loss for horizontal detection.\nExperimental results on seven datasets using different detectors show its\nconsistent superiority, and codes are available at\nhttps://github.com/yangxue0827/RotationDetection.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:29:19 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 09:16:58 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yang", "Xue", ""], ["Yang", "Xiaojiang", ""], ["Yang", "Jirui", ""], ["Ming", "Qi", ""], ["Wang", "Wentao", ""], ["Tian", "Qi", ""], ["Yan", "Junchi", ""]]}, {"id": "2106.01896", "submitter": "Battula Balnarsaiah Mr", "authors": "Battula Balnarsaiah, G Rajitha", "title": "Denoising and Optical and SAR Image Classifications Based on Feature\n  Extraction and Sparse Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical image data have been used by the Remote Sensing workforce to study\nland use and cover since such data is easily interpretable. Synthetic Aperture\nRadar (SAR) has the characteristic of obtaining images during all-day,\nall-weather and provides object information that is different from visible and\ninfrared sensors. However, SAR images have more speckle noise and fewer\ndimensions. This paper presents a method for denoising, feature extraction and\ncompares classifications of Optical and SAR images. The image was denoised\nusing K-Singular Value Decomposition (K-SVD) algorithm. A method to map the\nextraordinary goal signatures to be had withinside the SAR or Optical image\nusing support vector machine (SVM) through offering given the enter facts to\nthe supervised classifier. Initially, the Gray Level Histogram (GLH) and Gray\nLevel Co-occurrence Matrix (GLCM) are used for feature extraction. Secondly,\nthe extracted feature vectors from the first step were combined using\ncorrelation analysis to reduce the dimensionality of the feature spaces.\nThirdly, the Classification of SAR images was done in Sparse Representations\nClassification (SRC). The above-mentioned classifications techniques were\ndeveloped and performance parameters are accuracy and Kappa Coefficient\ncalculated using MATLAB 2018a.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:39:30 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Balnarsaiah", "Battula", ""], ["Rajitha", "G", ""]]}, {"id": "2106.01899", "submitter": "Xinjie Fan", "authors": "Xinjie Fan, Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong, Mingyuan\n  Zhou", "title": "Adversarially Adaptive Normalization for Single Domain Generalization", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single domain generalization aims to learn a model that performs well on many\nunseen domains with only one domain data for training. Existing works focus on\nstudying the adversarial domain augmentation (ADA) to improve the model's\ngeneralization capability. The impact on domain generalization of the\nstatistics of normalization layers is still underinvestigated. In this paper,\nwe propose a generic normalization approach, adaptive standardization and\nrescaling normalization (ASR-Norm), to complement the missing part in previous\nworks. ASR-Norm learns both the standardization and rescaling statistics via\nneural networks. This new form of normalization can be viewed as a generic form\nof the traditional normalizations. When trained with ADA, the statistics in\nASR-Norm are learned to be adaptive to the data coming from different domains,\nand hence improves the model generalization performance across domains,\nespecially on the target domain with large discrepancy from the source domain.\nThe experimental results show that ASR-Norm can bring consistent improvement to\nthe state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the\nDigits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the\nimprovement introduced by ASR-Norm is agnostic to the choice of ADA methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 23:58:23 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Fan", "Xinjie", ""], ["Wang", "Qifei", ""], ["Ke", "Junjie", ""], ["Yang", "Feng", ""], ["Gong", "Boqing", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2106.01907", "submitter": "Jinglun Feng", "authors": "Jinglun Feng, Liang Yang, Jiang Biao, Jizhong Xiao", "title": "Robotic Inspection and 3D GPR-based Reconstruction for Underground\n  Utilities", "comments": "arXiv admin note: text overlap with arXiv:2011.02635", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ground Penetrating Radar (GPR) is an effective non-destructive evaluation\n(NDE) device for inspecting and surveying subsurface objects (i.e., rebars,\nutility pipes) in complex environments. However, the current practice for GPR\ndata collection requires a human inspector to move a GPR cart along pre-marked\ngrid lines and record the GPR data in both X and Y directions for\npost-processing by 3D GPR imaging software. It is time-consuming and tedious\nwork to survey a large area. Furthermore, identifying the subsurface targets\ndepends on the knowledge of an experienced engineer, who has to make manual and\nsubjective interpretation that limits the GPR applications, especially in\nlarge-scale scenarios. In addition, the current GPR imaging technology is not\nintuitive, and not for normal users to understand, and not friendly to\nvisualize. To address the above challenges, this paper presents a novel robotic\nsystem to collect GPR data, interpret GPR data, localize the underground\nutilities, reconstruct and visualize the underground objects' dense point cloud\nmodel in a user-friendly manner. This system is composed of three modules: 1) a\nvision-aided Omni-directional robotic data collection platform, which enables\nthe GPR antenna to scan the target area freely with an arbitrary trajectory\nwhile using a visual-inertial-based positioning module tags the GPR\nmeasurements with positioning information; 2) a deep neural network (DNN)\nmigration module to interpret the raw GPR B-scan image into a cross-section of\nobject model; 3) a DNN-based 3D reconstruction method, i.e., GPRNet, to\ngenerate underground utility model represented as fine 3D point cloud.\nComparative studies on synthetic and field GPR raw data with various\nincompleteness and noise are performed.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:58:49 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Feng", "Jinglun", ""], ["Yang", "Liang", ""], ["Biao", "Jiang", ""], ["Xiao", "Jizhong", ""]]}, {"id": "2106.01908", "submitter": "Yuming Shen", "authors": "Yuming Shen and Ziyi Shen and Menghan Wang and Jie Qin and Philip H.S.\n  Torr and Ling Shao", "title": "You Never Cluster Alone", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in self-supervised learning with instance-level contrastive\nobjectives facilitate unsupervised clustering. However, a standalone datum is\nnot perceiving the context of the holistic cluster, and may undergo sub-optimal\nassignment. In this paper, we extend the mainstream contrastive learning\nparadigm to a cluster-level scheme, where all the data subjected to the same\ncluster contribute to a unified representation that encodes the context of each\ndata group. Contrastive learning with this representation then rewards the\nassignment of each datum. To implement this vision, we propose twin-contrast\nclustering (TCC). We define a set of categorical variables as clustering\nassignment confidence, which links the instance-level learning track with the\ncluster-level one. On one hand, with the corresponding assignment variables\nbeing the weight, a weighted aggregation along the data points implements the\nset representation of a cluster. We further propose heuristic cluster\naugmentation equivalents to enable cluster-level contrastive learning. On the\nother hand, we derive the evidence lower-bound of the instance-level\ncontrastive objective with the assignments. By reparametrizing the assignment\nvariables, TCC is trained end-to-end, requiring no alternating steps. Extensive\nexperiments show that TCC outperforms the state-of-the-art on challenging\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:59:59 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Shen", "Yuming", ""], ["Shen", "Ziyi", ""], ["Wang", "Menghan", ""], ["Qin", "Jie", ""], ["Torr", "Philip H. S.", ""], ["Shao", "Ling", ""]]}, {"id": "2106.01915", "submitter": "Changhee Han Dr.", "authors": "Changhee Han", "title": "Pathology-Aware Generative Adversarial Networks for Medical Image\n  Augmentation", "comments": "Ph.D. Thesis (The University of Tokyo) defended in February, 2020.\n  Based on GAN-based synthetic brain MR image generation (ISBI 2018),\n  arXiv:1902.09856, arXiv:1903.12564, arXiv:1905.13456, arXiv:1906.04962,\n  arXiv:2001.03923", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) can play a key role in Medical Image\nAnalysis under large-scale annotated datasets. However, preparing such massive\ndataset is demanding. In this context, Generative Adversarial Networks (GANs)\ncan generate realistic but novel samples, and thus effectively cover the real\nimage distribution. In terms of interpolation, the GAN-based medical image\naugmentation is reliable because medical modalities can display the human\nbody's strong anatomical consistency at fixed position while clearly reflecting\ninter-subject variability; thus, we propose to use noise-to-image GANs (e.g.,\nrandom noise samples to diverse pathological images) for (i) medical Data\nAugmentation (DA) and (ii) physician training. Regarding the DA, the\nGAN-generated images can improve Computer-Aided Diagnosis based on supervised\nlearning. For the physician training, the GANs can display novel desired\npathological images and help train medical trainees despite\ninfrastructural/legal constraints. This thesis contains four GAN projects\naiming to present such novel applications' clinical relevance in collaboration\nwith physicians. Whereas the methods are more generally applicable, this thesis\nonly explores a few oncological applications.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:08:14 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Han", "Changhee", ""]]}, {"id": "2106.01920", "submitter": "Kunal Bhardwaj", "authors": "Kunal Bhardwaj", "title": "Convolutional Neural Network(CNN/ConvNet) in Stock Price Movement\n  Prediction", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With technological advancements and the exponential growth of data, we have\nbeen unfolding different capabilities of neural networks in different sectors.\nIn this paper, I have tried to use a specific type of Neural Network known as\nConvolutional Neural Network(CNN/ConvNet) in the stock market. In other words,\nI have tried to construct and train a convolutional neural network on past\nstock prices data and then tried to predict the movement of stock price i.e.\nwhether the stock price would rise or fall, in the coming time.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:14:46 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Bhardwaj", "Kunal", ""]]}, {"id": "2106.01927", "submitter": "Ao Chen", "authors": "Ao Chen, Chen Li, Haoyuan Chen, Hechen Yang, Peng Zhao, Weiming Hu,\n  Wanli Liu, Shuojia Zou, and Marcin Grzegorzek", "title": "A Comparison for Anti-noise Robustness of Deep Learning Classification\n  Methods on a Tiny Object Image Dataset: from Convolutional Neural Network to\n  Visual Transformer and Performer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification has achieved unprecedented advance with the the rapid\ndevelopment of deep learning. However, the classification of tiny object images\nis still not well investigated. In this paper, we first briefly review the\ndevelopment of Convolutional Neural Network and Visual Transformer in deep\nlearning, and introduce the sources and development of conventional noises and\nadversarial attacks. Then we use various models of Convolutional Neural Network\nand Visual Transformer to conduct a series of experiments on the image dataset\nof tiny objects (sperms and impurities), and compare various evaluation metrics\nin the experimental results to obtain a model with stable performance. Finally,\nwe discuss the problems in the classification of tiny objects and make a\nprospect for the classification of tiny objects in the future.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:28:17 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 06:33:59 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chen", "Ao", ""], ["Li", "Chen", ""], ["Chen", "Haoyuan", ""], ["Yang", "Hechen", ""], ["Zhao", "Peng", ""], ["Hu", "Weiming", ""], ["Liu", "Wanli", ""], ["Zou", "Shuojia", ""], ["Grzegorzek", "Marcin", ""]]}, {"id": "2106.01970", "submitter": "Xiuming Zhang", "authors": "Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec,\n  William T. Freeman, Jonathan T. Barron", "title": "NeRFactor: Neural Factorization of Shape and Reflectance Under an\n  Unknown Illumination", "comments": "Project Page:\n  https://people.csail.mit.edu/xiuming/projects/nerfactor/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of recovering the shape and spatially-varying\nreflectance of an object from posed multi-view images of the object illuminated\nby one unknown lighting condition. This enables the rendering of novel views of\nthe object under arbitrary environment lighting and editing of the object's\nmaterial properties. The key to our approach, which we call Neural Radiance\nFactorization (NeRFactor), is to distill the volumetric geometry of a Neural\nRadiance Field (NeRF) [Mildenhall et al. 2020] representation of the object\ninto a surface representation and then jointly refine the geometry while\nsolving for the spatially-varying reflectance and the environment lighting.\nSpecifically, NeRFactor recovers 3D neural fields of surface normals, light\nvisibility, albedo, and Bidirectional Reflectance Distribution Functions\n(BRDFs) without any supervision, using only a re-rendering loss, simple\nsmoothness priors, and a data-driven BRDF prior learned from real-world BRDF\nmeasurements. By explicitly modeling light visibility, NeRFactor is able to\nseparate shadows from albedo and synthesize realistic soft or hard shadows\nunder arbitrary lighting conditions. NeRFactor is able to recover convincing 3D\nmodels for free-viewpoint relighting in this challenging and underconstrained\ncapture setup for both synthetic and real scenes. Qualitative and quantitative\nexperiments show that NeRFactor outperforms classic and deep learning-based\nstate of the art across various tasks. Our code and data are available at\npeople.csail.mit.edu/xiuming/projects/nerfactor/.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 16:18:01 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Xiuming", ""], ["Srinivasan", "Pratul P.", ""], ["Deng", "Boyang", ""], ["Debevec", "Paul", ""], ["Freeman", "William T.", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "2106.01981", "submitter": "Boris Oreshkin N", "authors": "Boris N. Oreshkin and Florent Bocquelet and F\\'elix G. Harvey and Bay\n  Raitt and Dominic Laflamme", "title": "ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work focuses on the development of a learnable neural representation of\nhuman pose for advanced AI assisted animation tooling. Specifically, we tackle\nthe problem of constructing a full static human pose based on sparse and\nvariable user inputs (e.g. locations and/or orientations of a subset of body\njoints). To solve this problem, we propose a novel neural architecture that\ncombines residual connections with prototype encoding of a partially specified\npose to create a new complete pose from the learned latent space. We show that\nour architecture outperforms a baseline based on Transformer, both in terms of\naccuracy and computational efficiency. Additionally, we develop a user\ninterface to integrate our neural model in Unity, a real-time 3D development\nplatform. Furthermore, we introduce two new datasets representing the static\nhuman pose modeling problem, based on high-quality human motion capture data,\nwhich will be released publicly along with model code.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 16:56:58 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 14:05:01 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Oreshkin", "Boris N.", ""], ["Bocquelet", "Florent", ""], ["Harvey", "F\u00e9lix G.", ""], ["Raitt", "Bay", ""], ["Laflamme", "Dominic", ""]]}, {"id": "2106.02019", "submitter": "Lingjie Liu", "authors": "Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao\n  Gu, Christian Theobalt", "title": "Neural Actor: Neural Free-view Synthesis of Human Actors with Pose\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Actor (NA), a new method for high-quality synthesis of\nhumans from arbitrary viewpoints and under arbitrary controllable poses. Our\nmethod is built upon recent neural scene representation and rendering works\nwhich learn representations of geometry and appearance from only 2D images.\nWhile existing works demonstrated compelling rendering of static scenes and\nplayback of dynamic scenes, photo-realistic reconstruction and rendering of\nhumans with neural implicit methods, in particular under user-controlled novel\nposes, is still difficult. To address this problem, we utilize a coarse body\nmodel as the proxy to unwarp the surrounding 3D space into a canonical pose. A\nneural radiance field learns pose-dependent geometric deformations and pose-\nand view-dependent appearance effects in the canonical space from multi-view\nvideo input. To synthesize novel views of high fidelity dynamic geometry and\nappearance, we leverage 2D texture maps defined on the body model as latent\nvariables for predicting residual deformations and the dynamic appearance.\nExperiments demonstrate that our method achieves better quality than the\nstate-of-the-arts on playback as well as novel pose synthesis, and can even\ngeneralize well to new poses that starkly differ from the training poses.\nFurthermore, our method also supports body shape control of the synthesized\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:40:48 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liu", "Lingjie", ""], ["Habermann", "Marc", ""], ["Rudnev", "Viktor", ""], ["Sarkar", "Kripasindhu", ""], ["Gu", "Jiatao", ""], ["Theobalt", "Christian", ""]]}, {"id": "2106.02022", "submitter": "Micha\\\"el Ramamonjisoa", "authors": "Micha\\\"el Ramamonjisoa and Michael Firman and Jamie Watson and Vincent\n  Lepetit and Daniyar Turmukhambetov", "title": "Single Image Depth Estimation using Wavelet Decomposition", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for predicting accurate depths from monocular\nimages with high efficiency. This optimal efficiency is achieved by exploiting\nwavelet decomposition, which is integrated in a fully differentiable\nencoder-decoder architecture. We demonstrate that we can reconstruct\nhigh-fidelity depth maps by predicting sparse wavelet coefficients. In contrast\nwith previous works, we show that wavelet coefficients can be learned without\ndirect supervision on coefficients. Instead we supervise only the final depth\nimage that is reconstructed through the inverse wavelet transform. We\nadditionally show that wavelet coefficients can be learned in fully\nself-supervised scenarios, without access to ground-truth depth. Finally, we\napply our method to different state-of-the-art monocular depth estimation\nmodels, in each case giving similar or better results compared to the original\nmodel, while requiring less than half the multiply-adds in the decoder network.\nCode at https://github.com/nianticlabs/wavelet-monodepth\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:42:25 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Ramamonjisoa", "Micha\u00ebl", ""], ["Firman", "Michael", ""], ["Watson", "Jamie", ""], ["Lepetit", "Vincent", ""], ["Turmukhambetov", "Daniyar", ""]]}, {"id": "2106.02034", "submitter": "Yongming Rao", "authors": "Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, Cho-Jui\n  Hsieh", "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token\n  Sparsification", "comments": "Project page: https://dynamicvit.ivg-research.xyz/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention is sparse in vision transformers. We observe the final prediction\nin vision transformers is only based on a subset of most informative tokens,\nwhich is sufficient for accurate image recognition. Based on this observation,\nwe propose a dynamic token sparsification framework to prune redundant tokens\nprogressively and dynamically based on the input. Specifically, we devise a\nlightweight prediction module to estimate the importance score of each token\ngiven the current features. The module is added to different layers to prune\nredundant tokens hierarchically. To optimize the prediction module in an\nend-to-end manner, we propose an attention masking strategy to differentiably\nprune a token by blocking its interactions with other tokens. Benefiting from\nthe nature of self-attention, the unstructured sparse tokens are still hardware\nfriendly, which makes our framework easy to achieve actual speed-up. By\nhierarchically pruning 66% of the input tokens, our method greatly reduces\n31%~37% FLOPs and improves the throughput by over 40% while the drop of\naccuracy is within 0.5% for various vision transformers. Equipped with the\ndynamic token sparsification framework, DynamicViT models can achieve very\ncompetitive complexity/accuracy trade-offs compared to state-of-the-art CNNs\nand vision transformers on ImageNet. Code is available at\nhttps://github.com/raoyongming/DynamicViT\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:57:41 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Rao", "Yongming", ""], ["Zhao", "Wenliang", ""], ["Liu", "Benlin", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2106.02036", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar and Kristen Grauman", "title": "Anticipative Video Transformer", "comments": "Ranked #1 on CVPR'21 EPIC-Kitchens Action Anticipation challenge\n  leaderboard. Project page: http://facebookresearch.github.io/AVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Anticipative Video Transformer (AVT), an end-to-end\nattention-based video modeling architecture that attends to the previously\nobserved video in order to anticipate future actions. We train the model\njointly to predict the next action in a video sequence, while also learning\nframe feature encoders that are predictive of successive future frames'\nfeatures. Compared to existing temporal aggregation strategies, AVT has the\nadvantage of both maintaining the sequential progression of observed actions\nwhile still capturing long-range dependencies--both critical for the\nanticipation task. Through extensive experiments, we show that AVT obtains the\nbest reported performance on four popular action anticipation benchmarks:\nEpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads, including\noutperforming all submissions to the EpicKitchens-100 CVPR'21 challenge.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:57:55 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Girdhar", "Rohit", ""], ["Grauman", "Kristen", ""]]}, {"id": "2106.02067", "submitter": "Daniela Mihai", "authors": "Daniela Mihai, Jonathon Hare", "title": "Learning to Draw: Emergent Communication through Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence that visual communication preceded written language and provided a\nbasis for it goes back to prehistory, in forms such as cave and rock paintings\ndepicting traces of our distant ancestors. Emergent communication research has\nsought to explore how agents can learn to communicate in order to\ncollaboratively solve tasks. Existing research has focused on language, with a\nlearned communication channel transmitting sequences of discrete tokens between\nthe agents. In this work, we explore a visual communication channel between\nagents that are allowed to draw with simple strokes. Our agents are\nparameterised by deep neural networks, and the drawing procedure is\ndifferentiable, allowing for end-to-end training. In the framework of a\nreferential communication game, we demonstrate that agents can not only\nsuccessfully learn to communicate by drawing, but with appropriate inductive\nbiases, can do so in a fashion that humans can interpret. We hope to encourage\nfuture research to consider visual communication as a more flexible and\ndirectly interpretable alternative of training collaborative agents.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 18:17:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "2106.02078", "submitter": "Kaustubh Sridhar", "authors": "Kaustubh Sridhar, Oleg Sokolsky, Insup Lee, James Weimer", "title": "Robust Learning via Persistency of Excitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving adversarial robustness of neural networks remains a major\nchallenge. Fundamentally, training a network is a parameter estimation problem.\nIn adaptive control theory, maintaining persistency of excitation (PoE) is\nintegral to ensuring convergence of parameter estimates in dynamical systems to\ntheir robust optima. In this work, we show that network training using gradient\ndescent is equivalent to a dynamical system parameter estimation problem.\nLeveraging this relationship, we prove a sufficient condition for PoE of\ngradient descent is achieved when the learning rate is less than the inverse of\nthe Lipschitz constant of the gradient of loss function. We provide an\nefficient technique for estimating the corresponding Lipschitz constant using\nextreme value theory and demonstrate that by only scaling the learning rate\nschedule we can increase adversarial accuracy by up to 15% points on benchmark\ndatasets. Our approach also universally increases the adversarial accuracy by\n0.1% to 0.3% points in various state-of-the-art adversarially trained models on\nthe AutoAttack benchmark, where every small margin of improvement is\nsignificant.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 18:49:05 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 16:12:55 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 03:32:47 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Sridhar", "Kaustubh", ""], ["Sokolsky", "Oleg", ""], ["Lee", "Insup", ""], ["Weimer", "James", ""]]}, {"id": "2106.02106", "submitter": "Bardia Yousefi", "authors": "Bardia Yousefi, Hossein Memarzadeh Sharifipour, Xavier P.V. Maldague", "title": "Embedded Deep Regularized Block HSIC Thermomics for Early Diagnosis of\n  Breast Cancer", "comments": "Authors version. arXiv admin note: text overlap with arXiv:2010.06784", "journal-ref": "IEEE Transactions on Instrumentation and Measurement 2021", "doi": "10.1109/TIM.2021.3085956", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermography has been used extensively as a complementary diagnostic tool in\nbreast cancer detection. Among thermographic methods matrix factorization (MF)\ntechniques show an unequivocal capability to detect thermal patterns\ncorresponding to vasodilation in cancer cases. One of the biggest challenges in\nsuch techniques is selecting the best representation of the thermal basis. In\nthis study, an embedding method is proposed to address this problem and\nDeep-semi-nonnegative matrix factorization (Deep-SemiNMF) for thermography is\nintroduced, then tested for 208 breast cancer screening cases. First, we apply\nDeep-SemiNMF to infrared images to extract low-rank thermal representations for\neach case. Then, we embed low-rank bases to obtain one basis for each patient.\nAfter that, we extract 300 thermal imaging features, called thermomics, to\ndecode imaging information for the automatic diagnostic model. We reduced the\ndimensionality of thermomics by spanning them onto Hilbert space using RBF\nkernel and select the three most efficient features using the block Hilbert\nSchmidt Independence Criterion Lasso (block HSIC Lasso). The preserved thermal\nheterogeneity successfully classified asymptomatic versus symptomatic patients\napplying a random forest model (cross-validated accuracy of 71.36%\n(69.42%-73.3%)).\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 19:54:31 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yousefi", "Bardia", ""], ["Sharifipour", "Hossein Memarzadeh", ""], ["Maldague", "Xavier P. V.", ""]]}, {"id": "2106.02118", "submitter": "Ju Sun", "authors": "Ju Sun, Le Peng, Taihui Li, Dyah Adila, Zach Zaiman, Genevieve B.\n  Melton, Nicholas Ingraham, Eric Murray, Daniel Boley, Sean Switzer, John L.\n  Burns, Kun Huang, Tadashi Allen, Scott D. Steenburg, Judy Wawira Gichoya,\n  Erich Kummerfeld, Christopher Tignanelli", "title": "A Prospective Observational Study to Investigate Performance of a Chest\n  X-ray Artificial Intelligence Diagnostic Support Tool Across 12 U.S.\n  Hospitals", "comments": "Check out the medRxiv version at\n  https://doi.org/10.1101/2021.06.04.21258316 for updates", "journal-ref": null, "doi": "10.1101/2021.06.04.21258316", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Importance: An artificial intelligence (AI)-based model to predict COVID-19\nlikelihood from chest x-ray (CXR) findings can serve as an important adjunct to\naccelerate immediate clinical decision making and improve clinical decision\nmaking. Despite significant efforts, many limitations and biases exist in\npreviously developed AI diagnostic models for COVID-19. Utilizing a large set\nof local and international CXR images, we developed an AI model with high\nperformance on temporal and external validation.\n  Conclusions and Relevance: AI-based diagnostic tools may serve as an adjunct,\nbut not replacement, for clinical decision support of COVID-19 diagnosis, which\nlargely hinges on exposure history, signs, and symptoms. While AI-based tools\nhave not yet reached full diagnostic potential in COVID-19, they may still\noffer valuable information to clinicians taken into consideration along with\nclinical signs and symptoms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 20:22:32 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 01:56:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sun", "Ju", ""], ["Peng", "Le", ""], ["Li", "Taihui", ""], ["Adila", "Dyah", ""], ["Zaiman", "Zach", ""], ["Melton", "Genevieve B.", ""], ["Ingraham", "Nicholas", ""], ["Murray", "Eric", ""], ["Boley", "Daniel", ""], ["Switzer", "Sean", ""], ["Burns", "John L.", ""], ["Huang", "Kun", ""], ["Allen", "Tadashi", ""], ["Steenburg", "Scott D.", ""], ["Gichoya", "Judy Wawira", ""], ["Kummerfeld", "Erich", ""], ["Tignanelli", "Christopher", ""]]}, {"id": "2106.02141", "submitter": "Matthew Keaton", "authors": "Matthew R. Keaton, Ram J. Zaveri, Meghana Kovur, Cole Henderson,\n  Donald A. Adjeroh, Gianfranco Doretto", "title": "Fine-Grained Visual Classification of Plant Species In The Wild: Object\n  Detection as A Reinforced Means of Attention", "comments": "6 pages, 4 figures. Accepted to the CVPR 2021 FGVC Workshop. Models,\n  testing code, and link to dataset can be found at\n  https://github.com/wvuvl/DARMA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant species identification in the wild is a difficult problem in part due\nto the high variability of the input data, but also because of complications\ninduced by the long-tail effects of the datasets distribution. Inspired by the\nmost recent fine-grained visual classification approaches which are based on\nattention to mitigate the effects of data variability, we explore the idea of\nusing object detection as a form of attention. We introduce a bottom-up\napproach based on detecting plant organs and fusing the predictions of a\nvariable number of organ-based species classifiers. We also curate a new\ndataset with a long-tail distribution for evaluating plant organ detection and\norgan-based species identification, which is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 21:22:18 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Keaton", "Matthew R.", ""], ["Zaveri", "Ram J.", ""], ["Kovur", "Meghana", ""], ["Henderson", "Cole", ""], ["Adjeroh", "Donald A.", ""], ["Doretto", "Gianfranco", ""]]}, {"id": "2106.02154", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "title": "Laplacian-Based Dimensionality Reduction Including Spectral Clustering,\n  Laplacian Eigenmap, Locality Preserving Projection, Graph Embedding, and\n  Diffusion Map: Tutorial and Survey", "comments": "To appear as a part of an upcoming textbook on dimensionality\n  reduction and manifold learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a tutorial and survey paper for nonlinear dimensionality and feature\nextraction methods which are based on the Laplacian of graph of data. We first\nintroduce adjacency matrix, definition of Laplacian matrix, and the\ninterpretation of Laplacian. Then, we cover the cuts of graph and spectral\nclustering which applies clustering in a subspace of data. Different\noptimization variants of Laplacian eigenmap and its out-of-sample extension are\nexplained. Thereafter, we introduce the locality preserving projection and its\nkernel variant as linear special cases of Laplacian eigenmap. Versions of graph\nembedding are then explained which are generalized versions of Laplacian\neigenmap and locality preserving projection. Finally, diffusion map is\nintroduced which is a method based on Laplacian of data and random walks on the\ndata graph.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 22:10:40 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2106.02198", "submitter": "Viktor Vegh", "authors": "Azin Shokraei Fard, David C. Reutens, Viktor Vegh", "title": "CNNs and GANs in MRI-based cross-modality medical image estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modality image estimation involves the generation of images of one\nmedical imaging modality from that of another modality. Convolutional neural\nnetworks (CNNs) have been shown to be useful in identifying, characterising and\nextracting image patterns. Generative adversarial networks (GANs) use CNNs as\ngenerators and estimated images are discriminated as true or false based on an\nadditional network. CNNs and GANs within the image estimation framework may be\nconsidered more generally as deep learning approaches, since imaging data tends\nto be large, leading to a larger number of network weights. Almost all research\nin the CNN/GAN image estimation literature has involved the use of MRI data\nwith the other modality primarily being PET or CT. This review provides an\noverview of the use of CNNs and GANs for MRI-based cross-modality medical image\nestimation. We outline the neural networks implemented, and detail network\nconstructs employed for CNN and GAN image-to-image estimators. Motivations\nbehind cross-modality image estimation are provided as well. GANs appear to\nprovide better utility in cross-modality image estimation in comparison with\nCNNs, a finding drawn based on our analysis involving metrics comparing\nestimated and actual images. Our final remarks highlight key challenges faced\nby the cross-modality medical image estimation field, and suggestions for\nfuture research are outlined.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 01:27:57 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Fard", "Azin Shokraei", ""], ["Reutens", "David C.", ""], ["Vegh", "Viktor", ""]]}, {"id": "2106.02207", "submitter": "Minjee Kim", "authors": "Ryoungwoo Jang, Minjee Kim, Da-in Eun, Kyungjin Cho, Jiyeon Seo,\n  Namkug Kim", "title": "Barcode Method for Generative Model Evaluation driven by Topological\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the performance of generative models in image synthesis is a\nchallenging task. Although the Fr\\'echet Inception Distance is a widely\naccepted evaluation metric, it integrates different aspects (e.g., fidelity and\ndiversity) of synthesized images into a single score and assumes the normality\nof embedded vectors. Recent methods such as precision-and-recall and its\nvariants such as density-and-coverage have been developed to separate fidelity\nand diversity based on k-nearest neighborhood methods. In this study, we\npropose an algorithm named barcode, which is inspired by the topological data\nanalysis and is almost free of assumption and hyperparameter selections. In\nextensive experiments on real-world datasets as well as theoretical approach on\nhigh-dimensional normal samples, it was found that the 'usual' normality\nassumption of embedded vectors has several drawbacks. The experimental results\ndemonstrate that barcode outperforms other methods in evaluating fidelity and\ndiversity of GAN outputs. Official codes can be found in\nhttps://github.com/minjeekim00/Barcode.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 02:07:07 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Jang", "Ryoungwoo", ""], ["Kim", "Minjee", ""], ["Eun", "Da-in", ""], ["Cho", "Kyungjin", ""], ["Seo", "Jiyeon", ""], ["Kim", "Namkug", ""]]}, {"id": "2106.02213", "submitter": "Alex D\\'iaz Santos", "authors": "Alex D\\'iaz, Damian Steele", "title": "Analysis of the robustness of NMF algorithms", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine three non-negative matrix factorization techniques; L2-norm,\nL1-norm, and L2,1-norm. Our aim is to establish the performance of these\ndifferent approaches, and their robustness in real-world applications such as\nfeature selection while managing computational complexity, sensitivity to noise\nand more. We thoroughly examine each approach from a theoretical perspective,\nand examine the performance of each using a series of experiments drawing on\nboth the ORL and YaleB datasets. We examine the Relative Reconstruction Errors\n(RRE), Average Accuracy and Normalized Mutual Information (NMI) as criteria\nunder a range of simulated noise scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 02:35:24 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["D\u00edaz", "Alex", ""], ["Steele", "Damian", ""]]}, {"id": "2106.02221", "submitter": "Lauren Jimenez-Martin", "authors": "Lauren Jimenez-Martin, Daniel A. Vald\\'es P\\'erez, Ana M. Solares\n  Asteasuainzarra, Ludwig Leonard, Marta L. Baguer D\\'iaz-Roma\\~nach", "title": "Specular reflections removal in colposcopic images based on neural\n  networks: Supervised training with no ground truth previous knowledge", "comments": "This new version corrects typos and adds references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cervical cancer is a malignant tumor that seriously threatens women's health,\nand is one of the most common that affects women worldwide. For its early\ndetection, colposcopic images of the cervix are used for searching for possible\ninjuries or abnormalities. An inherent characteristic of these images is the\npresence of specular reflections (brightness) that make it difficult to observe\nsome regions, which might imply misdiagnosis. In this paper, a new strategy\nbased on neural networks is introduced for eliminating specular reflections and\nestimating the unobserved anatomical cervix portion under the bright zones. For\novercoming the fact that the ground truth corresponding to the specular\nreflection regions is always unknown, the new strategy proposes the supervised\ntraining of a neural network to learn how to restore any hidden regions of\ncolposcopic images. Once the specular reflections are identified, they are\nremoved from the image, and the previously trained network is used to fulfill\nthese deleted areas. The quality of the processed images was evaluated\nquantitatively and qualitatively. In 21 of the 22 evaluated images, the\ndetected specular reflections were eliminated, whereas, in the remaining one,\nthese reflections were almost completely eliminated. The distribution of the\ncolors and the content of the restored images are similar to those of the\noriginals. The evaluation carried out by a specialist in Cervix Pathology\nconcluded that, after eliminating the specular reflections, the anatomical and\nphysiological elements of the cervix are observable in the restored images,\nwhich facilitates the medical diagnosis of cervical pathologies. Our method has\nthe potential to improve the early detection of cervical cancer.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 02:54:56 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 15:10:46 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Jimenez-Martin", "Lauren", ""], ["P\u00e9rez", "Daniel A. Vald\u00e9s", ""], ["Asteasuainzarra", "Ana M. Solares", ""], ["Leonard", "Ludwig", ""], ["D\u00edaz-Roma\u00f1ach", "Marta L. Baguer", ""]]}, {"id": "2106.02222", "submitter": "Zhuo Xu", "authors": "Zhuo Xu, and Masayoshi Tomizuka", "title": "History Encoding Representation Design for Human Intention Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this extended abstract, we investigate the design of learning\nrepresentation for human intention inference. In our designed human intention\nprediction task, we propose a history encoding representation that is both\ninterpretable and effective for prediction. Through extensive experiments, we\nshow our prediction framework with a history encoding representation design is\nsuccessful on the human intention prediction problem.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 02:55:46 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Xu", "Zhuo", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2106.02229", "submitter": "Xingyou Song", "authors": "Yingjie Miao, Xingyou Song, Daiyi Peng, Summer Yue, Eugene Brevdo,\n  Aleksandra Faust", "title": "RL-DARTS: Differentiable Architecture Search for Reinforcement Learning", "comments": "19 pages total, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RL-DARTS, one of the first applications of Differentiable\nArchitecture Search (DARTS) in reinforcement learning (RL) to search for\nconvolutional cells, applied to the Procgen benchmark. We outline the initial\ndifficulties of applying neural architecture search techniques in RL, and\ndemonstrate that by simply replacing the image encoder with a DARTS supernet,\nour search method is sample-efficient, requires minimal extra compute\nresources, and is also compatible with off-policy and on-policy RL algorithms,\nneeding only minor changes in preexisting code. Surprisingly, we find that the\nsupernet can be used as an actor for inference to generate replay data in\nstandard RL training loops, and thus train end-to-end. Throughout this training\nprocess, we show that the supernet gradually learns better cells, leading to\nalternative architectures which can be highly competitive against manually\ndesigned policies, but also verify previous design choices for RL policies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 03:08:43 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Miao", "Yingjie", ""], ["Song", "Xingyou", ""], ["Peng", "Daiyi", ""], ["Yue", "Summer", ""], ["Brevdo", "Eugene", ""], ["Faust", "Aleksandra", ""]]}, {"id": "2106.02253", "submitter": "Hang Wang", "authors": "Xuanhong Chen and Hang Wang and Bingbing Ni", "title": "X-volution: On the unification of convolution and self-attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution and self-attention are acting as two fundamental building blocks\nin deep neural networks, where the former extracts local image features in a\nlinear way while the latter non-locally encodes high-order contextual\nrelationships. Though essentially complementary to each other, i.e.,\nfirst-/high-order, stat-of-the-art architectures, i.e., CNNs or transformers\nlack a principled way to simultaneously apply both operations in a single\ncomputational module, due to their heterogeneous computing pattern and\nexcessive burden of global dot-product for visual tasks. In this work, we\ntheoretically derive a global self-attention approximation scheme, which\napproximates a self-attention via the convolution operation on transformed\nfeatures. Based on the approximated scheme, we establish a multi-branch\nelementary module composed of both convolution and self-attention operation,\ncapable of unifying both local and non-local feature interaction. Importantly,\nonce trained, this multi-branch module could be conditionally converted into a\nsingle standard convolution operation via structural re-parameterization,\nrendering a pure convolution styled operator named X-volution, ready to be\nplugged into any modern networks as an atomic operation. Extensive experiments\ndemonstrate that the proposed X-volution, achieves highly competitive visual\nunderstanding improvements (+1.2% top-1 accuracy on ImageNet classification,\n+1.7 box AP and +1.5 mask AP on COCO detection and segmentation).\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 04:32:02 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 09:03:46 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chen", "Xuanhong", ""], ["Wang", "Hang", ""], ["Ni", "Bingbing", ""]]}, {"id": "2106.02257", "submitter": "Jiayi Wei", "authors": "Jiayi Wei, Xilian Li, Yi Zhang, Xin Wang", "title": "Visual Question Rewriting for Increasing Response Rate", "comments": null, "journal-ref": null, "doi": "10.1145/3404835.3463114", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When a human asks questions online, or when a conversational virtual agent\nasks human questions, questions triggering emotions or with details might more\nlikely to get responses or answers. we explore how to automatically rewrite\nnatural language questions to improve the response rate from people. In\nparticular, a new task of Visual Question Rewriting(VQR) task is introduced to\nexplore how visual information can be used to improve the new questions. A data\nset containing around 4K bland questions, attractive questions and images\ntriples is collected. We developed some baseline sequence to sequence models\nand more advanced transformer based models, which take a bland question and a\nrelated image as input and output a rewritten question that is expected to be\nmore attractive. Offline experiments and mechanical Turk based evaluations show\nthat it is possible to rewrite bland questions in a more detailed and\nattractive way to increase the response rate, and images can be helpful.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 04:46:47 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wei", "Jiayi", ""], ["Li", "Xilian", ""], ["Zhang", "Yi", ""], ["Wang", "Xin", ""]]}, {"id": "2106.02258", "submitter": "Yanan Chang", "authors": "Shangfei Wang, Yanan Chang, Guozhu Peng, Bowen Pan", "title": "Exploring Adversarial Learning for Deep Semi-Supervised Facial Action\n  Unit Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current works formulate facial action unit (AU) recognition as a supervised\nlearning problem, requiring fully AU-labeled facial images during training. It\nis challenging if not impossible to provide AU annotations for large numbers of\nfacial images. Fortunately, AUs appear on all facial images, whether manually\nlabeled or not, satisfy the underlying anatomic mechanisms and human behavioral\nhabits. In this paper, we propose a deep semi-supervised framework for facial\naction unit recognition from partially AU-labeled facial images. Specifically,\nthe proposed deep semi-supervised AU recognition approach consists of a deep\nrecognition network and a discriminator D. The deep recognition network R\nlearns facial representations from large-scale facial images and AU classifiers\nfrom limited ground truth AU labels. The discriminator D is introduced to\nenforce statistical similarity between the AU distribution inherent in ground\ntruth AU labels and the distribution of the predicted AU labels from labeled\nand unlabeled facial images. The deep recognition network aims to minimize\nrecognition loss from the labeled facial images, to faithfully represent\ninherent AU distribution for both labeled and unlabeled facial images, and to\nconfuse the discriminator. During training, the deep recognition network R and\nthe discriminator D are optimized alternately. Thus, the inherent AU\ndistributions caused by underlying anatomic mechanisms are leveraged to\nconstruct better feature representations and AU classifiers from partially\nAU-labeled data during training. Experiments on two benchmark databases\ndemonstrate that the proposed approach successfully captures AU distributions\nthrough adversarial learning and outperforms state-of-the-art AU recognition\nwork.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 04:50:00 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wang", "Shangfei", ""], ["Chang", "Yanan", ""], ["Peng", "Guozhu", ""], ["Pan", "Bowen", ""]]}, {"id": "2106.02267", "submitter": "Yingtao Tian", "authors": "Yingtao Tian, Tarin Clanuwat, Chikahiko Suzuki, Asanobu Kitamoto", "title": "Ukiyo-e Analysis and Creativity with Attribute and Geometry Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of Ukiyo-e, an important genre of pre-modern Japanese art, focuses\non the object and style like other artwork researches. Such study has benefited\nfrom the renewed interest by the machine learning community in culturally\nimportant topics, leading to interdisciplinary works including collections of\nimages, quantitative approaches, and machine learning-based creativities. They,\nhowever, have several drawbacks, and it remains challenging to integrate these\nworks into a comprehensive view. To bridge this gap, we propose a holistic\napproach We first present a large-scale Ukiyo-e dataset with coherent semantic\nlabels and geometric annotations, then show its value in a quantitative study\nof Ukiyo-e paintings' object using these labels and annotations. We further\ndemonstrate the machine learning methods could help style study through soft\ncolor decomposition of Ukiyo-e, and finally provides joint insights into object\nand style by composing sketches and colors using colorization. Dataset\navailable at https://github.com/rois-codh/arc-ukiyoe-faces\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 05:24:20 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Tian", "Yingtao", ""], ["Clanuwat", "Tarin", ""], ["Suzuki", "Chikahiko", ""], ["Kitamoto", "Asanobu", ""]]}, {"id": "2106.02277", "submitter": "Qihang Yu", "authors": "Qihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan Yuille, Wei Shen", "title": "Glance-and-Gaze Vision Transformer", "comments": "codes and models will be made available at\n  https://github.com/yucornetto/GG-Transformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there emerges a series of vision Transformers, which show superior\nperformance with a more compact model size than conventional convolutional\nneural networks, thanks to the strong ability of Transformers to model\nlong-range dependencies. However, the advantages of vision Transformers also\ncome with a price: Self-attention, the core part of Transformer, has a\nquadratic complexity to the input sequence length. This leads to a dramatic\nincrease of computation and memory cost with the increase of sequence length,\nthus introducing difficulties when applying Transformers to the vision tasks\nthat require dense predictions based on high-resolution feature maps. In this\npaper, we propose a new vision Transformer, named Glance-and-Gaze Transformer\n(GG-Transformer), to address the aforementioned issues. It is motivated by the\nGlance and Gaze behavior of human beings when recognizing objects in natural\nscenes, with the ability to efficiently model both long-range dependencies and\nlocal context. In GG-Transformer, the Glance and Gaze behavior is realized by\ntwo parallel branches: The Glance branch is achieved by performing\nself-attention on the adaptively-dilated partitions of the input, which leads\nto a linear complexity while still enjoying a global receptive field; The Gaze\nbranch is implemented by a simple depth-wise convolutional layer, which\ncompensates local image context to the features obtained by the Glance\nmechanism. We empirically demonstrate our method achieves consistently superior\nperformance over previous state-of-the-art Transformers on various vision tasks\nand benchmarks. The codes and models will be made available at\nhttps://github.com/yucornetto/GG-Transformer.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:13:47 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yu", "Qihang", ""], ["Xia", "Yingda", ""], ["Bai", "Yutong", ""], ["Lu", "Yongyi", ""], ["Yuille", "Alan", ""], ["Shen", "Wei", ""]]}, {"id": "2106.02280", "submitter": "Amanpreet Singh", "authors": "Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Alberto Lopez\n  Magana, Wojciech Galuba, Devi Parikh, Douwe Kiela", "title": "Human-Adversarial Visual Question Answering", "comments": "22 pages, 13 figures. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performance on the most commonly used Visual Question Answering dataset (VQA\nv2) is starting to approach human accuracy. However, in interacting with\nstate-of-the-art VQA models, it is clear that the problem is far from being\nsolved. In order to stress test VQA models, we benchmark them against\nhuman-adversarial examples. Human subjects interact with a state-of-the-art VQA\nmodel, and for each image in the dataset, attempt to find a question where the\nmodel's predicted answer is incorrect. We find that a wide range of\nstate-of-the-art models perform poorly when evaluated on these examples. We\nconduct an extensive analysis of the collected adversarial examples and provide\nguidance on future research directions. We hope that this Adversarial VQA\n(AdVQA) benchmark can help drive progress in the field and advance the state of\nthe art.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:25:32 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Sheng", "Sasha", ""], ["Singh", "Amanpreet", ""], ["Goswami", "Vedanuj", ""], ["Magana", "Jose Alberto Lopez", ""], ["Galuba", "Wojciech", ""], ["Parikh", "Devi", ""], ["Kiela", "Douwe", ""]]}, {"id": "2106.02285", "submitter": "Zheng-Ning Liu", "authors": "Shi-Min Hu, Zheng-Ning Liu, Meng-Hao Guo, Jun-Xiong Cai, Jiahui Huang,\n  Tai-Jiang Mu, Ralph R. Martin", "title": "Subdivision-Based Mesh Convolution Networks", "comments": "Codes are available in https://github.com/lzhengning/SubdivNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have made great breakthroughs in 2D\ncomputer vision. However, the irregular structure of meshes makes it hard to\nexploit the power of CNNs directly. A subdivision surface provides a\nhierarchical multi-resolution structure, and each face in a closed 2-manifold\ntriangle mesh is exactly adjacent to three faces. Motivated by these two\nproperties, this paper introduces a novel and flexible CNN framework, named\nSubdivNet, for 3D triangle meshes with Loop subdivision sequence connectivity.\nMaking an analogy between mesh faces and pixels in a 2D image allows us to\npresent a mesh convolution operator to aggregate local features from adjacent\nfaces. By exploiting face neighborhoods, this convolution can support standard\n2D convolutional network concepts, e.g. variable kernel size, stride, and\ndilation. Based on the multi-resolution hierarchy, we propose a spatial uniform\npooling layer which merges four faces into one and an upsampling method which\nsplits one face into four. As a result, many popular 2D CNN architectures can\nbe readily adapted to processing 3D meshes. Meshes with arbitrary connectivity\ncan be remeshed to hold Loop subdivision sequence connectivity via\nself-parameterization, making SubdivNet a general approach. Experiments on mesh\nclassification, segmentation, correspondence, and retrieval from the real-world\ndemonstrate the effectiveness and efficiency of SubdivNet.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:50:34 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Hu", "Shi-Min", ""], ["Liu", "Zheng-Ning", ""], ["Guo", "Meng-Hao", ""], ["Cai", "Jun-Xiong", ""], ["Huang", "Jiahui", ""], ["Mu", "Tai-Jiang", ""], ["Martin", "Ralph R.", ""]]}, {"id": "2106.02288", "submitter": "Leon Amadeus Varga", "authors": "Leon Amadeus Varga, Andreas Zell", "title": "Tackling the Background Bias in Sparse Object Detection via Cropped\n  Windows", "comments": "Submitted (WACV2022)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging\ntask. The recordings are mostly sparse and contain only small objects. In this\nwork, we propose a simple tiling method that improves the detection capability\nin the remote sensing case without modifying the model itself. By reducing the\nbackground bias and enabling the usage of higher image resolutions during\ntraining, our method can improve the performance of models substantially. The\nprocedure was validated on three different data sets and outperformed similar\napproaches in performance and speed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:59:56 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Varga", "Leon Amadeus", ""], ["Zell", "Andreas", ""]]}, {"id": "2106.02299", "submitter": "Liying Lu", "authors": "Liying Lu, Wenbo Li, Xin Tao, Jiangbo Lu, Jiaya Jia", "title": "MASA-SR: Matching Acceleration and Spatial Adaptation for\n  Reference-Based Image Super-Resolution", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reference-based image super-resolution (RefSR) has shown promising success in\nrecovering high-frequency details by utilizing an external reference image\n(Ref). In this task, texture details are transferred from the Ref image to the\nlow-resolution (LR) image according to their point- or patch-wise\ncorrespondence. Therefore, high-quality correspondence matching is critical. It\nis also desired to be computationally efficient. Besides, existing RefSR\nmethods tend to ignore the potential large disparity in distributions between\nthe LR and Ref images, which hurts the effectiveness of the information\nutilization. In this paper, we propose the MASA network for RefSR, where two\nnovel modules are designed to address these problems. The proposed Match &\nExtraction Module significantly reduces the computational cost by a\ncoarse-to-fine correspondence matching scheme. The Spatial Adaptation Module\nlearns the difference of distribution between the LR and Ref images, and remaps\nthe distribution of Ref features to that of LR features in a spatially adaptive\nway. This scheme makes the network robust to handle different reference images.\nExtensive quantitative and qualitative experiments validate the effectiveness\nof our proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:15:32 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Lu", "Liying", ""], ["Li", "Wenbo", ""], ["Tao", "Xin", ""], ["Lu", "Jiangbo", ""], ["Jia", "Jiaya", ""]]}, {"id": "2106.02320", "submitter": "Gengwei Zhang", "authors": "Gengwei Zhang, Guoliang Kang, Yunchao Wei, Yi Yang", "title": "Few-Shot Segmentation via Cycle-Consistent Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot segmentation aims to train a segmentation model that can fast adapt\nto novel classes with few exemplars. The conventional training paradigm is to\nlearn to make predictions on query images conditioned on the features from\nsupport images. Previous methods only utilized the semantic-level prototypes of\nsupport images as the conditional information. These methods cannot utilize all\npixel-wise support information for the query predictions, which is however\ncritical for the segmentation task. In this paper, we focus on utilizing\npixel-wise relationships between support and target images to facilitate the\nfew-shot semantic segmentation task. We design a novel Cycle-Consistent\nTransformer (CyCTR) module to aggregate pixel-wise support features into query\nones. CyCTR performs cross-attention between features from different images,\ni.e. support and query images. We observe that there may exist unexpected\nirrelevant pixel-level support features. Directly performing cross-attention\nmay aggregate these features from support to query and bias the query features.\nThus, we propose using a novel cycle-consistent attention mechanism to filter\nout possible harmful support features and encourage query features to attend to\nthe most informative pixels from support images. Experiments on all few-shot\nsegmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable\nimprovement compared to previous state-of-the-art methods. Specifically, on\nPascal-$5^i$ and COCO-$20^i$ datasets, we achieve 66.6% and 45.6% mIoU for\n5-shot segmentation, outperforming previous state-of-the-art by 4.6% and 7.1%\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:57:48 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhang", "Gengwei", ""], ["Kang", "Guoliang", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""]]}, {"id": "2106.02324", "submitter": "Fusen Wang", "authors": "Fusen Wang and Jun Sang and Zhongyuan Wu and Qi Liu and Nong Sang", "title": "Hybrid attention network based on progressive embedding scale-context\n  for crowd counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing crowd counting methods usually adopted attention mechanism to\ntackle background noise, or applied multi-level features or multi-scales\ncontext fusion to tackle scale variation. However, these approaches deal with\nthese two problems separately. In this paper, we propose a Hybrid Attention\nNetwork (HAN) by employing Progressive Embedding Scale-context (PES)\ninformation, which enables the network to simultaneously suppress noise and\nadapt head scale variation. We build the hybrid attention mechanism through\nparalleling spatial attention and channel attention module, which makes the\nnetwork to focus more on the human head area and reduce the interference of\nbackground objects. Besides, we embed certain scale-context to the hybrid\nattention along the spatial and channel dimensions for alleviating these\ncounting errors caused by the variation of perspective and head scale. Finally,\nwe propose a progressive learning strategy through cascading multiple hybrid\nattention modules with embedding different scale-context, which can gradually\nintegrate different scale-context information into the current feature map from\nglobal to local. Ablation experiments provides that the network architecture\ncan gradually learn multi-scale features and suppress background noise.\nExtensive experiments demonstrate that HANet obtain state-of-the-art counting\nperformance on four mainstream datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:10:21 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wang", "Fusen", ""], ["Sang", "Jun", ""], ["Wu", "Zhongyuan", ""], ["Liu", "Qi", ""], ["Sang", "Nong", ""]]}, {"id": "2106.02328", "submitter": "Torsten Sch\\\"on", "authors": "Thangapavithraa Balaji, Patrick Blies, Georg G\\\"ori, Raphael Mitsch,\n  Marcel Wasserer, Torsten Sch\\\"on", "title": "Temporally coherent video anonymization through GAN inpainting", "comments": "Preprint of our FG2021 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work tackles the problem of temporally coherent face anonymization in\nnatural video streams.We propose JaGAN, a two-stage system starting with\ndetecting and masking out faces with black image patches in all individual\nframes of the video. The second stage leverages a privacy-preserving Video\nGenerative Adversarial Network designed to inpaint the missing image patches\nwith artificially generated faces. Our initial experiments reveal that image\nbased generative models are not capable of inpainting patches showing temporal\ncoherent appearance across neighboring video frames. To address this issue we\nintroduce a newly curated video collection, which is made publicly available\nfor the research community along with this paper. We also introduce the\nIdentity Invariance Score IdI as a means to quantify temporal coherency between\nneighboring frames.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:19:44 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Balaji", "Thangapavithraa", ""], ["Blies", "Patrick", ""], ["G\u00f6ri", "Georg", ""], ["Mitsch", "Raphael", ""], ["Wasserer", "Marcel", ""], ["Sch\u00f6n", "Torsten", ""]]}, {"id": "2106.02335", "submitter": "Mikkel Abrahamsen", "authors": "Mikkel Abrahamsen", "title": "Covering Polygons is Even Harder", "comments": "41 pages, 32 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the MINIMUM CONVEX COVER (MCC) problem, we are given a simple polygon\n$\\mathcal P$ and an integer $k$, and the question is if there exist $k$ convex\npolygons whose union is $\\mathcal P$. It is known that MCC is\n$\\mathsf{NP}$-hard [Culberson & Reckhow: Covering polygons is hard, FOCS\n1988/Journal of Algorithms 1994] and in $\\exists\\mathbb{R}$ [O'Rourke: The\ncomplexity of computing minimum convex covers for polygons, Allerton 1982]. We\nprove that MCC is $\\exists\\mathbb{R}$-hard, and the problem is thus\n$\\exists\\mathbb{R}$-complete. In other words, the problem is equivalent to\ndeciding whether a system of polynomial equations and inequalities with integer\ncoefficients has a real solution.\n  If a cover for our constructed polygon exists, then so does a cover\nconsisting entirely of triangles. As a byproduct, we therefore also establish\nthat it is $\\exists\\mathbb{R}$-complete to decide whether $k$ triangles cover a\ngiven polygon.\n  The issue that it was not known if finding a minimum cover is in\n$\\mathsf{NP}$ has repeatedly been raised in the literature, and it was\nmentioned as a \"long-standing open question\" already in 2001 [Eidenbenz &\nWidmayer: An approximation algorithm for minimum convex cover with logarithmic\nperformance guarantee, ESA 2001/SIAM Journal on Computing 2003]. We prove that\nassuming the widespread belief that $\\mathsf{NP}\\neq\\exists\\mathbb{R}$, the\nproblem is not in $\\mathsf{NP}$.\n  An implication of the result is that many natural approaches to finding small\ncovers are bound to give suboptimal solutions in some cases, since irrational\ncoordinates of arbitrarily high algebraic degree can be needed for the corners\nof the pieces in an optimal solution.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:29:48 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Abrahamsen", "Mikkel", ""]]}, {"id": "2106.02342", "submitter": "Wenhao Wu", "authors": "Deng Huang, Wenhao Wu, Weiwen Hu, Xu Liu, Dongliang He, Zhihua Wu,\n  Xiangmiao Wu, Mingkui Tan, Errui Ding", "title": "ASCNet: Self-supervised Video Representation Learning with\n  Appearance-Speed Consistency", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We study self-supervised video representation learning, which is a\nchallenging task due to 1) a lack of labels for explicit supervision and 2)\nunstructured and noisy visual information. Existing methods mainly use\ncontrastive loss with video clips as the instances and learn visual\nrepresentation by discriminating instances from each other, but they require\ncareful treatment of negative pairs by relying on large batch sizes, memory\nbanks, extra modalities, or customized mining strategies, inevitably including\nnoisy data. In this paper, we observe that the consistency between positive\nsamples is the key to learn robust video representations. Specifically, we\npropose two tasks to learn the appearance and speed consistency, separately.\nThe appearance consistency task aims to maximize the similarity between two\nclips of the same video with different playback speeds. The speed consistency\ntask aims to maximize the similarity between two clips with the same playback\nspeed but different appearance information. We show that joint optimization of\nthe two tasks consistently improves the performance on downstream tasks, e.g.,\naction recognition and video retrieval. Remarkably, for action recognition on\nthe UCF-101 dataset, we achieve 90.8% accuracy without using any additional\nmodalities or negative pairs for unsupervised pretraining, outperforming the\nImageNet supervised pre-trained model. Codes and models will be available.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:44:50 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Huang", "Deng", ""], ["Wu", "Wenhao", ""], ["Hu", "Weiwen", ""], ["Liu", "Xu", ""], ["He", "Dongliang", ""], ["Wu", "Zhihua", ""], ["Wu", "Xiangmiao", ""], ["Tan", "Mingkui", ""], ["Ding", "Errui", ""]]}, {"id": "2106.02343", "submitter": "Shin'ya Yamaguchi", "authors": "Shin'ya Yamaguchi and Sekitoshi Kanai", "title": "F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks built from deep convolutional neural networks\n(GANs) lack the ability to exactly replicate the high-frequency components of\nnatural images. To alleviate this issue, we introduce two novel training\ntechniques called frequency dropping (F-Drop) and frequency matching (F-Match).\nThe key idea of F-Drop is to filter out unnecessary high-frequency components\nfrom the input images of the discriminators. This simple modification prevents\nthe discriminators from being confused by perturbations of the high-frequency\ncomponents. In addition, F-Drop makes the GANs focus on fitting in the\nlow-frequency domain, in which there are the dominant components of natural\nimages. F-Match minimizes the difference between real and fake images in the\nfrequency domain for generating more realistic images. F-Match is implemented\nas a regularization term in the objective functions of the generators; it\npenalizes the batch mean error in the frequency domain. F-Match helps the\ngenerators to fit in the high-frequency domain filtered out by F-Drop to the\nreal image. We experimentally demonstrate that the combination of F-Drop and\nF-Match improves the generative performance of GANs in both the frequency and\nspatial domain on multiple image benchmarks (CIFAR, TinyImageNet, STL-10,\nCelebA, and ImageNet).\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:51:58 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Yamaguchi", "Shin'ya", ""], ["Kanai", "Sekitoshi", ""]]}, {"id": "2106.02351", "submitter": "Tiancai Wang", "authors": "Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, Yichen Wei", "title": "SOLQ: Segmenting Objects by Learning Queries", "comments": "Tech Report.Code is available at\n  https://github.com/megvii-research/SOLQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end framework for instance segmentation.\nBased on the recently introduced DETR [1], our method, termed SOLQ, segments\nobjects by learning unified queries. In SOLQ, each query represents one object\nand has multiple representations: class, location and mask. The object queries\nlearned perform classification, box regression and mask encoding simultaneously\nin an unified vector form. During training phase, the mask vectors encoded are\nsupervised by the compression coding of raw spatial masks. In inference time,\nmask vectors produced can be directly transformed to spatial masks by the\ninverse process of compression coding. Experimental results show that SOLQ can\nachieve state-of-the-art performance, surpassing most of existing approaches.\nMoreover, the joint learning of unified query representation can greatly\nimprove the detection performance of original DETR. We hope our SOLQ can serve\nas a strong baseline for the Transformer-based instance segmentation. Code is\navailable at https://github.com/megvii-research/SOLQ.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:03:31 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 12:41:36 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Dong", "Bin", ""], ["Zeng", "Fangao", ""], ["Wang", "Tiancai", ""], ["Zhang", "Xiangyu", ""], ["Wei", "Yichen", ""]]}, {"id": "2106.02377", "submitter": "Larissa Triess", "authors": "Larissa T. Triess and Mariella Dreissig and Christoph B. Rist and J.\n  Marius Z\\\"ollner", "title": "A Survey on Deep Domain Adaptation for LiDAR Perception", "comments": "Accepted at IEEE Intelligent Vehicles Symposium (IV) 2021 Workshop on\n  Autonomy at Scale. 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable systems for automated driving have to reliably cope with an\nopen-world setting. This means, the perception systems are exposed to drastic\ndomain shifts, like changes in weather conditions, time-dependent aspects, or\ngeographic regions. Covering all domains with annotated data is impossible\nbecause of the endless variations of domains and the time-consuming and\nexpensive annotation process. Furthermore, fast development cycles of the\nsystem additionally introduce hardware changes, such as sensor types and\nvehicle setups, and the required knowledge transfer from simulation. To enable\nscalable automated driving, it is therefore crucial to address these domain\nshifts in a robust and efficient manner. Over the last years, a vast amount of\ndifferent domain adaptation techniques evolved. There already exists a number\nof survey papers for domain adaptation on camera images, however, a survey for\nLiDAR perception is absent. Nevertheless, LiDAR is a vital sensor for automated\ndriving that provides detailed 3D scans of the vehicle's surroundings. To\nstimulate future research, this paper presents a comprehensive review of recent\nprogress in domain adaptation methods and formulates interesting research\nquestions specifically targeted towards LiDAR perception.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:42:51 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 06:42:57 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Triess", "Larissa T.", ""], ["Dreissig", "Mariella", ""], ["Rist", "Christoph B.", ""], ["Z\u00f6llner", "J. Marius", ""]]}, {"id": "2106.02385", "submitter": "Zhe Min", "authors": "Zhe Min, Fernando J. Bianco, Qianye Yang, Rachael Rodell, Wen Yan,\n  Dean Barratt, Yipeng Hu", "title": "Controlling False Positive/Negative Rates for Deep-Learning-Based\n  Prostate Cancer Detection on Multiparametric MR images", "comments": "Accepted by 25th UK Conference on Medical Image Understanding and\n  Analysis(MIUA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prostate cancer (PCa) is one of the leading causes of death for men\nworldwide. Multi-parametric magnetic resonance (mpMR) imaging has emerged as a\nnon-invasive diagnostic tool for detecting and localising prostate tumours by\nspecialised radiologists. These radiological examinations, for example, for\ndifferentiating malignant lesions from benign prostatic hyperplasia in\ntransition zones and for defining the boundaries of clinically significant\ncancer, remain challenging and highly skill-and-experience-dependent. We first\ninvestigate experimental results in developing object detection neural networks\nthat are trained to predict the radiological assessment, using these\nhigh-variance labels. We further argue that such a computer-assisted diagnosis\n(CAD) system needs to have the ability to control the false-positive rate (FPR)\nor false-negative rate (FNR), in order to be usefully deployed in a clinical\nworkflow, informing clinical decisions without further human intervention. This\nwork proposes a novel PCa detection network that incorporates a lesion-level\ncost-sensitive loss and an additional slice-level loss based on a\nlesion-to-slice mapping function, to manage the lesion- and slice-level costs,\nrespectively. Our experiments based on 290 clinical patients concludes that 1)\nThe lesion-level FNR was effectively reduced from 0.19 to 0.10 and the\nlesion-level FPR was reduced from 1.03 to 0.66 by changing the lesion-level\ncost; 2) The slice-level FNR was reduced from 0.19 to 0.00 by taking into\naccount the slice-level cost; (3) Both lesion-level and slice-level FNRs were\nreduced with lower FP/FPR by changing the lesion-level or slice-level costs,\ncompared with post-training threshold adjustment using networks without the\nproposed cost-aware training.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:51:27 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Min", "Zhe", ""], ["Bianco", "Fernando J.", ""], ["Yang", "Qianye", ""], ["Rodell", "Rachael", ""], ["Yan", "Wen", ""], ["Barratt", "Dean", ""], ["Hu", "Yipeng", ""]]}, {"id": "2106.02395", "submitter": "Federica Granese", "authors": "Federica Granese, Marco Romanelli, Daniele Gorla, Catuscia\n  Palamidessi, Pablo Piantanida", "title": "DOCTOR: A Simple Method for Detecting Misclassification Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have shown to perform very well on large scale\nobject recognition problems and lead to widespread use for real-world\napplications, including situations where DNN are implemented as \"black boxes\".\nA promising approach to secure their use is to accept decisions that are likely\nto be correct while discarding the others. In this work, we propose DOCTOR, a\nsimple method that aims to identify whether the prediction of a DNN classifier\nshould (or should not) be trusted so that, consequently, it would be possible\nto accept it or to reject it. Two scenarios are investigated: Totally Black Box\n(TBB) where only the soft-predictions are available and Partially Black Box\n(PBB) where gradient-propagation to perform input pre-processing is allowed.\nEmpirically, we show that DOCTOR outperforms all state-of-the-art methods on\nvarious well-known images and sentiment analysis datasets. In particular, we\nobserve a reduction of up to $4\\%$ of the false rejection rate (FRR) in the PBB\nscenario. DOCTOR can be applied to any pre-trained model, it does not require\nprior information about the underlying dataset and is as simple as the simplest\navailable methods in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 10:20:10 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Granese", "Federica", ""], ["Romanelli", "Marco", ""], ["Gorla", "Daniele", ""], ["Palamidessi", "Catuscia", ""], ["Piantanida", "Pablo", ""]]}, {"id": "2106.02400", "submitter": "Manh-Duy Nguyen", "authors": "Manh-Duy Nguyen, Binh T. Nguyen, Cathal Gurrin", "title": "A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conventional approaches to image-text retrieval mainly focus on indexing\nvisual objects appearing in pictures but ignore the interactions between these\nobjects. Such objects occurrences and interactions are equivalently useful and\nimportant in this field as they are usually mentioned in the text. Scene graph\npresentation is a suitable method for the image-text matching challenge and\nobtained good results due to its ability to capture the inter-relationship\ninformation. Both images and text are represented in scene graph levels and\nformulate the retrieval challenge as a scene graph matching challenge. In this\npaper, we introduce the Local and Global Scene Graph Matching (LGSGM) model\nthat enhances the state-of-the-art method by integrating an extra graph\nconvolution network to capture the general information of a graph.\nSpecifically, for a pair of scene graphs of an image and its caption, two\nseparate models are used to learn the features of each graph's nodes and edges.\nThen a Siamese-structure graph convolution model is employed to embed graphs\ninto vector forms. We finally combine the graph-level and the vector-level to\ncalculate the similarity of this image-text pair. The empirical experiments\nshow that our enhancement with the combination of levels can improve the\nperformance of the baseline method by increasing the recall by more than 10% on\nthe Flickr30k dataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 10:33:14 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Nguyen", "Manh-Duy", ""], ["Nguyen", "Binh T.", ""], ["Gurrin", "Cathal", ""]]}, {"id": "2106.02426", "submitter": "Zekun Luo", "authors": "Zekun Luo, Zheng Fang, Sixiao Zheng, Yabiao Wang, Yanwei Fu", "title": "NMS-Loss: Learning with Non-Maximum Suppression for Crowded Pedestrian\n  Detection", "comments": "ICMR2021 camera ready version", "journal-ref": null, "doi": "10.1145/3460426.3463588", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Maximum Suppression (NMS) is essential for object detection and affects\nthe evaluation results by incorporating False Positives (FP) and False\nNegatives (FN), especially in crowd occlusion scenes. In this paper, we raise\nthe problem of weak connection between the training targets and the evaluation\nmetrics caused by NMS and propose a novel NMS-Loss making the NMS procedure can\nbe trained end-to-end without any additional network parameters. Our NMS-Loss\npunishes two cases when FP is not suppressed and FN is wrongly eliminated by\nNMS. Specifically, we propose a pull loss to pull predictions with the same\ntarget close to each other, and a push loss to push predictions with different\ntargets away from each other. Experimental results show that with the help of\nNMS-Loss, our detector, namely NMS-Ped, achieves impressive results with Miss\nRate of 5.92% on Caltech dataset and 10.08% on CityPersons dataset, which are\nboth better than state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 12:06:46 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Luo", "Zekun", ""], ["Fang", "Zheng", ""], ["Zheng", "Sixiao", ""], ["Wang", "Yabiao", ""], ["Fu", "Yanwei", ""]]}, {"id": "2106.02473", "submitter": "Weiming Hu", "authors": "Weiming Hu, Chen Li, Xiaoyan Li, Md Mamunur Rahaman, Jiquan Ma,\n  Haoyuan Chen, Wanli Liu, Changhao Sun, Yudong Yao, Marcin Grzegorzek", "title": "A New Gastric Histopathology Subsize Image Database (GasHisSDB) for\n  Classification Algorithm Test: from Linear Regression to Visual Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total\nof 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,\n120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to\nrealize the function of valuating image classification. In order to prove that\nthe methods of different periods in the field of image classification have\ndiscrepancies on GasHisSDB, we select a variety of classifiers for evaluation.\nSeven classical machine learning classifiers, three CNN classifiers and a novel\ntransformer-based classifier are selected for testing on image classification\ntasks. GasHisSDB is available at the\nURL:https://github.com/NEUhwm/GasHisSDB.git.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:19:14 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 04:51:05 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 02:06:03 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 08:54:49 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Hu", "Weiming", ""], ["Li", "Chen", ""], ["Li", "Xiaoyan", ""], ["Rahaman", "Md Mamunur", ""], ["Ma", "Jiquan", ""], ["Chen", "Haoyuan", ""], ["Liu", "Wanli", ""], ["Sun", "Changhao", ""], ["Yao", "Yudong", ""], ["Grzegorzek", "Marcin", ""]]}, {"id": "2106.02495", "submitter": "Bowen Li", "authors": "Bowen Li, Changhong Fu, Fangqiang Ding, Junjie Ye, and Fuling Lin", "title": "ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV\n  Tracking", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior correlation filter (CF)-based tracking methods for unmanned aerial\nvehicles (UAVs) have virtually focused on tracking in the daytime. However,\nwhen the night falls, the trackers will encounter more harsh scenes, which can\neasily lead to tracking failure. In this regard, this work proposes a novel\ntracker with anti-dark function (ADTrack). The proposed method integrates an\nefficient and effective low-light image enhancer into a CF-based tracker.\nBesides, a target-aware mask is simultaneously generated by virtue of image\nillumination variation. The target-aware mask can be applied to jointly train a\ntarget-focused filter that assists the context filter for robust tracking.\nSpecifically, ADTrack adopts dual regression, where the context filter and the\ntarget-focused filter restrict each other for dual filter learning. Exhaustive\nexperiments are conducted on typical dark sceneries benchmark, consisting of 37\ntypical night sequences from authoritative benchmarks, i.e., UAVDark, and our\nnewly constructed benchmark UAVDark70. The results have shown that ADTrack\nfavorably outperforms other state-of-the-art trackers and achieves a real-time\nspeed of 34 frames/s on a single CPU, greatly extending robust UAV tracking to\nnight scenes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:05:24 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Li", "Bowen", ""], ["Fu", "Changhong", ""], ["Ding", "Fangqiang", ""], ["Ye", "Junjie", ""], ["Lin", "Fuling", ""]]}, {"id": "2106.02514", "submitter": "Chenjie Cao", "authors": "Chenjie Cao, Yuxin Hong, Xiang Li, Chengrong Wang, Chengming Xu,\n  XiangYang Xue, Yanwei Fu", "title": "The Image Local Autoregressive Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, AutoRegressive (AR) models for the whole image generation empowered\nby transformers have achieved comparable or even better performance to\nGenerative Adversarial Networks (GANs). Unfortunately, directly applying such\nAR models to edit/change local image regions, may suffer from the problems of\nmissing global information, slow inference speed, and information leakage of\nlocal guidance. To address these limitations, we propose a novel model -- image\nLocal Autoregressive Transformer (iLAT), to better facilitate the locally\nguided image synthesis. Our iLAT learns the novel local discrete\nrepresentations, by the newly proposed local autoregressive (LA) transformer of\nthe attention mask and convolution mechanism. Thus iLAT can efficiently\nsynthesize the local image regions by key guidance information. Our iLAT is\nevaluated on various locally guided image syntheses, such as pose-guided person\nimage synthesis and face editing. Both the quantitative and qualitative results\nshow the efficacy of our model.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:33:25 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Cao", "Chenjie", ""], ["Hong", "Yuxin", ""], ["Li", "Xiang", ""], ["Wang", "Chengrong", ""], ["Xu", "Chengming", ""], ["Xue", "XiangYang", ""], ["Fu", "Yanwei", ""]]}, {"id": "2106.02520", "submitter": "Sunghwan Hong", "authors": "Seokju Cho, Sunghwan Hong, Sangryul Jeon, Yunsung Lee, Kwanghoon Sohn\n  and Seungryong Kim", "title": "Semantic Correspondence with Transformers", "comments": "Code and trained models will be made available at\n  https://github.com/SunghwanHong/CATs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel cost aggregation network, called Cost Aggregation with\nTransformers (CATs), to find dense correspondences between semantically similar\nimages with additional challenges posed by large intra-class appearance and\ngeometric variations. Compared to previous hand-crafted or CNN-based methods\naddressing the cost aggregation stage, which either lack robustness to severe\ndeformations or inherit the limitation of CNNs that fail to discriminate\nincorrect matches due to limited receptive fields, CATs explore global\nconsensus among initial correlation map with the help of some architectural\ndesigns that allow us to exploit full potential of self-attention mechanism.\nSpecifically, we include appearance affinity modelling to disambiguate the\ninitial correlation maps and multi-level aggregation to benefit from\nhierarchical feature representations within Transformer-based aggregator, and\ncombine with swapping self-attention and residual connections not only to\nenforce consistent matching, but also to ease the learning process. We conduct\nexperiments to demonstrate the effectiveness of the proposed model over the\nlatest methods and provide extensive ablation studies. Code and trained models\nwill be made available at https://github.com/SunghwanHong/CATs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:39:03 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Cho", "Seokju", ""], ["Hong", "Sunghwan", ""], ["Jeon", "Sangryul", ""], ["Lee", "Yunsung", ""], ["Sohn", "Kwanghoon", ""], ["Kim", "Seungryong", ""]]}, {"id": "2106.02523", "submitter": "Osman Semih Kayhan", "authors": "Osman Semih Kayhan, Bart Vredebregt and Jan C. van Gemert", "title": "Hallucination In Object Detection -- A Study In Visual Part Verification", "comments": "ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that object detectors can hallucinate and detect missing objects;\npotentially even accurately localized at their expected, but non-existing,\nposition. This is particularly problematic for applications that rely on visual\npart verification: detecting if an object part is present or absent. We show\nhow popular object detectors hallucinate objects in a visual part verification\ntask and introduce the first visual part verification dataset: DelftBikes,\nwhich has 10,000 bike photographs, with 22 densely annotated parts per image,\nwhere some parts may be missing. We explicitly annotated an extra object state\nlabel for each part to reflect if a part is missing or intact. We propose to\nevaluate visual part verification by relying on recall and compare popular\nobject detectors on DelftBikes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:47:11 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Kayhan", "Osman Semih", ""], ["Vredebregt", "Bart", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2106.02527", "submitter": "Tong Qin", "authors": "Tong Qin, Yuxin Zheng, Tongqing Chen, Yilun Chen, and Qing Su", "title": "RoadMap: A Light-Weight Semantic Map for Visual Localization towards\n  Autonomous Driving", "comments": "IEEE International Conference on Robotics and Automation, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate localization is of crucial importance for autonomous driving tasks.\nNowadays, we have seen a lot of sensor-rich vehicles (e.g. Robo-taxi) driving\non the street autonomously, which rely on high-accurate sensors (e.g. Lidar and\nRTK GPS) and high-resolution map. However, low-cost production cars cannot\nafford such high expenses on sensors and maps. How to reduce costs? How do\nsensor-rich vehicles benefit low-cost cars? In this paper, we proposed a\nlight-weight localization solution, which relies on low-cost cameras and\ncompact visual semantic maps. The map is easily produced and updated by\nsensor-rich vehicles in a crowd-sourced way. Specifically, the map consists of\nseveral semantic elements, such as lane line, crosswalk, ground sign, and stop\nline on the road surface. We introduce the whole framework of on-vehicle\nmapping, on-cloud maintenance, and user-end localization. The map data is\ncollected and preprocessed on vehicles. Then, the crowd-sourced data is\nuploaded to a cloud server. The mass data from multiple vehicles are merged on\nthe cloud so that the semantic map is updated in time. Finally, the semantic\nmap is compressed and distributed to production cars, which use this map for\nlocalization. We validate the performance of the proposed map in real-world\nexperiments and compare it against other algorithms. The average size of the\nsemantic map is $36$ kb/km. We highlight that this framework is a reliable and\npractical localization solution for autonomous driving.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:55:10 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Qin", "Tong", ""], ["Zheng", "Yuxin", ""], ["Chen", "Tongqing", ""], ["Chen", "Yilun", ""], ["Su", "Qing", ""]]}, {"id": "2106.02531", "submitter": "Georgios Batzolis", "authors": "Georgios Batzolis, Marcello Carioni, Christian Etmann, Soroosh\n  Afyouni, Zoe Kourtzi, Carola Bibiane Sch\\\"onlieb", "title": "CAFLOW: Conditional Autoregressive Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce CAFLOW, a new diverse image-to-image translation model that\nsimultaneously leverages the power of auto-regressive modeling and the modeling\nefficiency of conditional normalizing flows. We transform the conditioning\nimage into a sequence of latent encodings using a multi-scale normalizing flow\nand repeat the process for the conditioned image. We model the conditional\ndistribution of the latent encodings by modeling the auto-regressive\ndistributions with an efficient multi-scale normalizing flow, where each\nconditioning factor affects image synthesis at its respective resolution scale.\nOur proposed framework performs well on a range of image-to-image translation\ntasks. It outperforms former designs of conditional flows because of its\nexpressive auto-regressive structure.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:57:41 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Batzolis", "Georgios", ""], ["Carioni", "Marcello", ""], ["Etmann", "Christian", ""], ["Afyouni", "Soroosh", ""], ["Kourtzi", "Zoe", ""], ["Sch\u00f6nlieb", "Carola Bibiane", ""]]}, {"id": "2106.02566", "submitter": "Tristan Gomez", "authors": "Tristan Gomez, Suiyi Ling, Thomas Fr\\'eour, Harold Mouch\\`ere", "title": "Improve the Interpretability of Attention: A Fast, Accurate, and\n  Interpretable High-Resolution Attention Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prevalence of employing attention mechanisms has brought along concerns\non the interpretability of attention distributions. Although it provides\ninsights about how a model is operating, utilizing attention as the explanation\nof model predictions is still highly dubious. The community is still seeking\nmore interpretable strategies for better identifying local active regions that\ncontribute the most to the final decision. To improve the interpretability of\nexisting attention models, we propose a novel Bilinear Representative\nNon-Parametric Attention (BR-NPA) strategy that captures the task-relevant\nhuman-interpretable information. The target model is first distilled to have\nhigher-resolution intermediate feature maps. From which, representative\nfeatures are then grouped based on local pairwise feature similarity, to\nproduce finer-grained, more precise attention maps highlighting task-relevant\nparts of the input. The obtained attention maps are ranked according to the\n`active level' of the compound feature, which provides information regarding\nthe important level of the highlighted regions. The proposed model can be\neasily adapted in a wide variety of modern deep models, where classification is\ninvolved. It is also more accurate, faster, and with a smaller memory footprint\nthan usual neural attention modules. Extensive experiments showcase more\ncomprehensive visual explanations compared to the state-of-the-art\nvisualization model across multiple tasks including few-shot classification,\nperson re-identification, fine-grained image classification. The proposed\nvisualization model sheds imperative light on how neural networks `pay their\nattention' differently in different tasks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 15:57:37 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 10:25:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gomez", "Tristan", ""], ["Ling", "Suiyi", ""], ["Fr\u00e9our", "Thomas", ""], ["Mouch\u00e8re", "Harold", ""]]}, {"id": "2106.02567", "submitter": "Elahe Arani", "authors": "Ratnajit Mukherjee, Haris Iqbal, Shabbir Marzban, Ahmed Badar, Terence\n  Brouns, Shruthi Gowda, Elahe Arani and Bahram Zonooz", "title": "AI Driven Road Maintenance Inspection", "comments": "accepted at 27th ITS World Congress, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Road infrastructure maintenance inspection is typically a labour-intensive\nand critical task to ensure the safety of all the road users. In this work, we\npropose a detailed methodology to use state-of-the-art techniques in artificial\nintelligence and computer vision to automate a sizeable portion of the\nmaintenance inspection subtasks and reduce the labour costs. The proposed\nmethodology uses state-of-the-art computer vision techniques such as object\ndetection and semantic segmentation to automate inspections on primary road\nstructures such as the road surface, markings, barriers (guardrails) and\ntraffic signs. The models are mostly trained on commercially viable datasets\nand augmented with proprietary data. We demonstrate that our AI models can not\nonly automate and scale maintenance inspections on primary road structures but\nalso result in higher recall compared to traditional manual inspections.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 15:59:46 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Mukherjee", "Ratnajit", ""], ["Iqbal", "Haris", ""], ["Marzban", "Shabbir", ""], ["Badar", "Ahmed", ""], ["Brouns", "Terence", ""], ["Gowda", "Shruthi", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2106.02581", "submitter": "Narinder Singh Punn", "authors": "Himanshu Batra, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali\n  Agarwal", "title": "BERT-Based Sentiment Analysis: A Software Engineering Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis can provide a suitable lead for the tools used in software\nengineering along with the API recommendation systems and relevant libraries to\nbe used. In this context, the existing tools like SentiCR, SentiStrength-SE,\netc. exhibited low f1-scores that completely defeats the purpose of deployment\nof such strategies, thereby there is enough scope for performance improvement.\nRecent advancements show that transformer based pre-trained models (e.g., BERT,\nRoBERTa, ALBERT, etc.) have displayed better results in the text classification\ntask. Following this context, the present research explores different\nBERT-based models to analyze the sentences in GitHub comments, Jira comments,\nand Stack Overflow posts. The paper presents three different strategies to\nanalyse BERT based model for sentiment analysis, where in the first strategy\nthe BERT based pre-trained models are fine-tuned; in the second strategy an\nensemble model is developed from BERT variants, and in the third strategy a\ncompressed model (Distil BERT) is used. The experimental results show that the\nBERT based ensemble approach and the compressed BERT model attain improvements\nby 6-12% over prevailing tools for the F1 measure on all three datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:28:26 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 05:48:32 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 13:16:19 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Batra", "Himanshu", ""], ["Punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2106.02585", "submitter": "Timm Hess", "authors": "Timm Hess, Martin Mundt, Iuliia Pliushch, Visvanathan Ramesh", "title": "A Procedural World Generation Framework for Systematic Evaluation of\n  Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several families of continual learning techniques have been proposed to\nalleviate catastrophic interference in deep neural network training on\nnon-stationary data. However, a comprehensive comparison and analysis of\nlimitations remains largely open due to the inaccessibility to suitable\ndatasets. Empirical examination not only varies immensely between individual\nworks, it further currently relies on contrived composition of benchmarks\nthrough subdivision and concatenation of various prevalent static vision\ndatasets. In this work, our goal is to bridge this gap by introducing a\ncomputer graphics simulation framework that repeatedly renders only upcoming\nurban scene fragments in an endless real-time procedural world generation\nprocess. At its core lies a modular parametric generative model with adaptable\ngenerative factors. The latter can be used to flexibly compose data streams,\nwhich significantly facilitates a detailed analysis and allows for effortless\ninvestigation of various continual learning schemes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:31:43 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Hess", "Timm", ""], ["Mundt", "Martin", ""], ["Pliushch", "Iuliia", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "2106.02594", "submitter": "Hiroyasu Akada", "authors": "Hiroyasu Akada, Shariq Farooq Bhat, Ibraheem Alhashim, Peter Wonka", "title": "Self-Supervised Learning of Domain Invariant Features for Depth\n  Estimation", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of unsupervised synthetic-to-realistic domain\nadaptation for single image depth estimation. An essential building block of\nsingle image depth estimation is an encoder-decoder task network that takes RGB\nimages as input and produces depth maps as output. In this paper, we propose a\nnovel training strategy to force the task network to learn domain invariant\nrepresentations in a self-supervised manner. Specifically, we extend\nself-supervised learning from traditional representation learning, which works\non images from a single domain, to domain invariant representation learning,\nwhich works on images from two different domains by utilizing an image-to-image\ntranslation network. Firstly, we use our bidirectional image-to-image\ntranslation network to transfer domain-specific styles between synthetic and\nreal domains. This style transfer operation allows us to obtain similar images\nfrom the different domains. Secondly, we jointly train our task network and\nSiamese network with the same images from the different domains to obtain\ndomain invariance for the task network. Finally, we fine-tune the task network\nusing labeled synthetic and unlabeled real-world data. Our training strategy\nyields improved generalization capability in the real-world domain. We carry\nout an extensive evaluation on two popular datasets for depth estimation, KITTI\nand Make3D. The results demonstrate that our proposed method outperforms the\nstate-of-the-art both qualitatively and quantitatively. The source code and\nmodel weights will be made available.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:45:48 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 17:00:57 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 09:02:07 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Akada", "Hiroyasu", ""], ["Bhat", "Shariq Farooq", ""], ["Alhashim", "Ibraheem", ""], ["Wonka", "Peter", ""]]}, {"id": "2106.02598", "submitter": "Viktor Kress", "authors": "Viktor Kress, Fabian Jeske, Stefan Zernetsch, Konrad Doll, Bernhard\n  Sick", "title": "Pose and Semantic Map Based Probabilistic Forecast of Vulnerable Road\n  Users' Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, an approach for probabilistic trajectory forecasting of\nvulnerable road users (VRUs) is presented, which considers past movements and\nthe surrounding scene. Past movements are represented by 3D poses reflecting\nthe posture and movements of individual body parts. The surrounding scene is\nmodeled in the form of semantic maps showing, e.g., the course of streets,\nsidewalks, and the occurrence of obstacles. The forecasts are generated in\ngrids discretizing the space and in the form of arbitrary discrete probability\ndistributions. The distributions are evaluated in terms of their reliability,\nsharpness, and positional accuracy. We compare our method with an approach that\nprovides forecasts in the form of Gaussian distributions and discuss the\nrespective advantages and disadvantages. Thereby, we investigate the impact of\nusing poses and semantic maps. With a technique called spatial label smoothing,\nour approach achieves reliable forecasts. Overall, the poses have a positive\nimpact on the forecasts. The semantic maps offer the opportunity to adapt the\nprobability distributions to the individual situation, although at the\nconsidered forecasted time horizon of 2.52 s they play a minor role compared to\nthe past movements of the VRU. Our method is evaluated on a dataset recorded in\ninner-city traffic using a research vehicle. The dataset is made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:56:13 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Kress", "Viktor", ""], ["Jeske", "Fabian", ""], ["Zernetsch", "Stefan", ""], ["Doll", "Konrad", ""], ["Sick", "Bernhard", ""]]}, {"id": "2106.02599", "submitter": "Kuan Zhang", "authors": "Kuan Zhang, Haoji Hu, Kenneth Philbrick, Gian Marco Conte, Joseph D.\n  Sobek, Pouria Rouzrokh, Bradley J. Erickson", "title": "SOUP-GAN: Super-Resolution MRI Using Generative Adversarial Networks", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing demand for high-resolution (HR) medical images in both the\nclinical and research applications. Image quality is inevitably traded off with\nthe acquisition time for better patient comfort, lower examination costs, dose,\nand fewer motion-induced artifacts. For many image-based tasks, increasing the\napparent resolution in the perpendicular plane to produce multi-planar\nreformats or 3D images is commonly used. Single image super-resolution (SR) is\na promising technique to provide HR images based on unsupervised learning to\nincrease resolution of a 2D image, but there are few reports on 3D SR. Further,\nperceptual loss is proposed in the literature to better capture the textual\ndetails and edges than using pixel-wise loss functions, by comparing the\nsemantic distances in the high-dimensional feature space of a pre-trained 2D\nnetwork (e.g., VGG). However, it is not clear how one should generalize it to\n3D medical images, and the attendant implications are still unclear. In this\npaper, we propose a framework called SOUP-GAN: Super-resolution Optimized Using\nPerceptual-tuned Generative Adversarial Network (GAN), in order to produce\nthinner slice (e.g., high resolution in the 'Z' plane) medical images with\nanti-aliasing and deblurring. The proposed method outperforms other\nconventional resolution-enhancement methods and previous SR work on medical\nimages upon both qualitative and quantitative comparisons. Specifically, we\nexamine the model in terms of its generalization for various SR ratios and\nimaging modalities. By addressing those limitations, our model shows promise as\na novel 3D SR interpolation technique, providing potential applications in both\nclinical and research settings.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:59:23 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhang", "Kuan", ""], ["Hu", "Haoji", ""], ["Philbrick", "Kenneth", ""], ["Conte", "Gian Marco", ""], ["Sobek", "Joseph D.", ""], ["Rouzrokh", "Pouria", ""], ["Erickson", "Bradley J.", ""]]}, {"id": "2106.02634", "submitter": "Vincent Sitzmann", "authors": "Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B.\n  Tenenbaum, Fredo Durand", "title": "Light Field Networks: Neural Scene Representations with\n  Single-Evaluation Rendering", "comments": "First two authors contributed equally. Project website:\n  https://vsitzmann.github.io/lfns/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring representations of 3D scenes from 2D observations is a fundamental\nproblem of computer graphics, computer vision, and artificial intelligence.\nEmerging 3D-structured neural scene representations are a promising approach to\n3D scene understanding. In this work, we propose a novel neural scene\nrepresentation, Light Field Networks or LFNs, which represent both geometry and\nappearance of the underlying 3D scene in a 360-degree, four-dimensional light\nfield parameterized via a neural implicit representation. Rendering a ray from\nan LFN requires only a *single* network evaluation, as opposed to hundreds of\nevaluations per ray for ray-marching or volumetric based renderers in\n3D-structured neural scene representations. In the setting of simple scenes, we\nleverage meta-learning to learn a prior over LFNs that enables multi-view\nconsistent light field reconstruction from as little as a single image\nobservation. This results in dramatic reductions in time and memory complexity,\nand enables real-time rendering. The cost of storing a 360-degree light field\nvia an LFN is two orders of magnitude lower than conventional methods such as\nthe Lumigraph. Utilizing the analytical differentiability of neural implicit\nrepresentations and a novel parameterization of light space, we further\ndemonstrate the extraction of sparse depth maps from LFNs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:54:49 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Sitzmann", "Vincent", ""], ["Rezchikov", "Semon", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Durand", "Fredo", ""]]}, {"id": "2106.02636", "submitter": "Rowan Zellers", "authors": "Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park,\n  Jize Cao, Ali Farhadi, Yejin Choi", "title": "MERLOT: Multimodal Neural Script Knowledge Models", "comments": "project page at https://rowanzellers.com/merlot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As humans, we understand events in the visual world contextually, performing\nmultimodal reasoning across time to make inferences about the past, present,\nand future. We introduce MERLOT, a model that learns multimodal script\nknowledge by watching millions of YouTube videos with transcribed speech -- in\nan entirely label-free, self-supervised manner. By pretraining with a mix of\nboth frame-level (spatial) and video-level (temporal) objectives, our model not\nonly learns to match images to temporally corresponding words, but also to\ncontextualize what is happening globally over time. As a result, MERLOT\nexhibits strong out-of-the-box representations of temporal commonsense, and\nachieves state-of-the-art performance on 12 different video QA datasets when\nfinetuned. It also transfers well to the world of static images, allowing\nmodels to reason about the dynamic context behind visual scenes. On Visual\nCommonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,\noutperforming state-of-the-art models of similar size by over 3%, even those\nthat make heavy use of auxiliary supervised data (like object bounding boxes).\n  Ablation analyses demonstrate the complementary importance of: 1) training on\nvideos versus static images; 2) scaling the magnitude and diversity of the\npretraining video corpus; and 3) using diverse objectives that encourage\nfull-stack multimodal reasoning, from the recognition to cognition level.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:57:39 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 01:28:33 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zellers", "Rowan", ""], ["Lu", "Ximing", ""], ["Hessel", "Jack", ""], ["Yu", "Youngjae", ""], ["Park", "Jae Sung", ""], ["Cao", "Jize", ""], ["Farhadi", "Ali", ""], ["Choi", "Yejin", ""]]}, {"id": "2106.02637", "submitter": "Fangyun Wei", "authors": "Fangyun Wei, Yue Gao, Zhirong Wu, Han Hu, Stephen Lin", "title": "Aligning Pretraining for Detection via Object-Level Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-level contrastive representation learning has proven to be highly\neffective as a generic model for transfer learning. Such generality for\ntransfer learning, however, sacrifices specificity if we are interested in a\ncertain downstream task. We argue that this could be sub-optimal and thus\nadvocate a design principle which encourages alignment between the\nself-supervised pretext task and the downstream task. In this paper, we follow\nthis principle with a pretraining method specifically designed for the task of\nobject detection. We attain alignment in the following three aspects: 1)\nobject-level representations are introduced via selective search bounding boxes\nas object proposals; 2) the pretraining network architecture incorporates the\nsame dedicated modules used in the detection pipeline (e.g. FPN); 3) the\npretraining is equipped with object detection properties such as object-level\ntranslation invariance and scale invariance. Our method, called Selective\nObject COntrastive learning (SoCo), achieves state-of-the-art results for\ntransfer performance on COCO detection using a Mask R-CNN framework. Code and\nmodels will be made available.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:59:52 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wei", "Fangyun", ""], ["Gao", "Yue", ""], ["Wu", "Zhirong", ""], ["Hu", "Han", ""], ["Lin", "Stephen", ""]]}, {"id": "2106.02638", "submitter": "Zongxin Yang", "authors": "Zongxin Yang, Yunchao Wei, Yi Yang", "title": "Associating Objects with Transformers for Video Object Segmentation", "comments": "18 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how to realize better and more efficient embedding\nlearning to tackle the semi-supervised video object segmentation under\nchallenging multi-object scenarios. The state-of-the-art methods learn to\ndecode features with a single positive object and thus have to match and\nsegment each target separately under multi-object scenarios, consuming multiple\ntimes computing resources. To solve the problem, we propose an Associating\nObjects with Transformers (AOT) approach to match and decode multiple objects\nuniformly. In detail, AOT employs an identification mechanism to associate\nmultiple targets into the same high-dimensional embedding space. Thus, we can\nsimultaneously process the matching and segmentation decoding of multiple\nobjects as efficiently as processing a single object. For sufficiently modeling\nmulti-object association, a Long Short-Term Transformer is designed for\nconstructing hierarchical matching and propagation. We conduct extensive\nexperiments on both multi-object and single-object benchmarks to examine AOT\nvariant networks with different complexities. Particularly, our AOT-L\noutperforms all the state-of-the-art competitors on three popular benchmarks,\ni.e., YouTube-VOS (83.7% J&F), DAVIS 2017 (83.0%), and DAVIS 2016 (91.0%),\nwhile keeping more than 3X faster multi-object run-time. Meanwhile, our AOT-T\ncan maintain real-time multi-object speed on the above benchmarks. We ranked\n1st in the 3rd Large-scale Video Object Segmentation Challenge. The code will\nbe publicly available at https://github.com/z-x-yang/AOT.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:59:57 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 14:48:51 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Yang", "Zongxin", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""]]}, {"id": "2106.02669", "submitter": "Jafar Pourbemany", "authors": "Jafar Pourbemany, Almabrok Essa, and Ye Zhu", "title": "Real Time Video based Heart and Respiration Rate Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, research about monitoring vital signs by smartphones grows\nsignificantly. There are some special sensors like Electrocardiogram (ECG) and\nPhotoplethysmographic (PPG) to detect heart rate (HR) and respiration rate\n(RR). Smartphone cameras also can measure HR by detecting and processing\nimaging Photoplethysmographic (iPPG) signals from the video of a user's face.\nIndeed, the variation in the intensity of the green channel can be measured by\nthe iPPG signals of the video. This study aimed to provide a method to extract\nheart rate and respiration rate using the video of individuals' faces. The\nproposed method is based on measuring fluctuations in the Hue, and can\ntherefore extract both HR and RR from the video of a user's face. The proposed\nmethod is evaluated by performing on 25 healthy individuals. For each subject,\n20 seconds video of his/her face is recorded. Results show that the proposed\napproach of measuring iPPG using Hue gives more accurate rates than the Green\nchannel.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 19:03:21 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Pourbemany", "Jafar", ""], ["Essa", "Almabrok", ""], ["Zhu", "Ye", ""]]}, {"id": "2106.02689", "submitter": "Chun-Fu (Richard) Chen", "authors": "Chun-Fu Chen, Rameswar Panda, Quanfu Fan", "title": "RegionViT: Regional-to-Local Attention for Vision Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision transformer (ViT) has recently showed its strong capability in\nachieving comparable results to convolutional neural networks (CNNs) on image\nclassification. However, vanilla ViT simply inherits the same architecture from\nthe natural language processing directly, which is often not optimized for\nvision applications. Motivated by this, in this paper, we propose a new\narchitecture that adopts the pyramid structure and employ a novel\nregional-to-local attention rather than global self-attention in vision\ntransformers. More specifically, our model first generates regional tokens and\nlocal tokens from an image with different patch sizes, where each regional\ntoken is associated with a set of local tokens based on the spatial location.\nThe regional-to-local attention includes two steps: first, the regional\nself-attention extract global information among all regional tokens and then\nthe local self-attention exchanges the information among one regional token and\nthe associated local tokens via self-attention. Therefore, even though local\nself-attention confines the scope in a local region but it can still receive\nglobal information. Extensive experiments on three vision tasks, including\nimage classification, object detection and action recognition, show that our\napproach outperforms or is on par with state-of-the-art ViT variants including\nmany concurrent works. Our source codes and models will be publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 19:57:11 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chen", "Chun-Fu", ""], ["Panda", "Rameswar", ""], ["Fan", "Quanfu", ""]]}, {"id": "2106.02694", "submitter": "Fanjie Kong", "authors": "Fanjie Kong, Ricardo Henao", "title": "Efficient Classification of Very Large Images with Tiny Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of applications in the computer vision domain,\nspecially, in medical imaging and remote sensing, are challenging when the goal\nis to classify very large images with tiny objects. More specifically, these\ntype of classification tasks face two key challenges: $i$) the size of the\ninput image in the target dataset is usually in the order of megapixels,\nhowever, existing deep architectures do not easily operate on such big images\ndue to memory constraints, consequently, we seek a memory-efficient method to\nprocess these images; and $ii$) only a small fraction of the input images are\ninformative of the label of interest, resulting in low region of interest (ROI)\nto image ratio. However, most of the current convolutional neural networks\n(CNNs) are designed for image classification datasets that have relatively\nlarge ROIs and small image size (sub-megapixel). Existing approaches have\naddressed these two challenges in isolation. We present an end-to-end CNN model\ntermed Zoom-In network that leverages hierarchical attention sampling for\nclassification of large images with tiny objects using a single GPU. We\nevaluate our method on two large-image datasets and one gigapixel dataset.\nExperimental results show that our model achieves higher accuracy than existing\nmethods while requiring less computing resources.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:13:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kong", "Fanjie", ""], ["Henao", "Ricardo", ""]]}, {"id": "2106.02701", "submitter": "Thomas Athey", "authors": "Thomas L. Athey, Daniel Tward, Ulrich Mueller, Michael I. Miller", "title": "Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction", "comments": "First draft presented at 2021 Brain Initiative Cell Census Network PI\n  meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in brain clearing and imaging have made it possible to image\nentire mammalian brains at sub-micron resolution. These images offer the\npotential to assemble brain-wide atlases of projection neuron morphology, but\nmanual neuron reconstruction remains a bottleneck. Here we present a method\ninspired by hidden Markov modeling and appearance modeling of fluorescent\nneuron images that can automatically trace neuronal processes. Our method\nleverages dynamic programming to scale to terabyte sized image data and can be\napplied to images with one or more neurons. We applied our algorithm to the\noutput of image segmentation models where false negatives severed neuronal\nprocesses, and showed that it can follow axons in the presence of noise or\nnearby neurons. Our method has the potential to be integrated into a semi or\nfully automated reconstruction pipeline. Additionally, it creates a framework\nthrough which users can intervene with hard constraints to, for example, rule\nout certain reconstructions, or assign axons to particular cell bodies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:24:56 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Athey", "Thomas L.", ""], ["Tward", "Daniel", ""], ["Mueller", "Ulrich", ""], ["Miller", "Michael I.", ""]]}, {"id": "2106.02711", "submitter": "Wamiq Reyaz Para", "authors": "Wamiq Reyaz Para, Shariq Farooq Bhat, Paul Guerrero, Tom Kelly, Niloy\n  Mitra, Leonidas Guibas, Peter Wonka", "title": "SketchGen: Generating Constrained CAD Sketches", "comments": "21 pages, 12 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer-aided design (CAD) is the most widely used modeling approach for\ntechnical design. The typical starting point in these designs is 2D sketches\nwhich can later be extruded and combined to obtain complex three-dimensional\nassemblies. Such sketches are typically composed of parametric primitives, such\nas points, lines, and circular arcs, augmented with geometric constraints\nlinking the primitives, such as coincidence, parallelism, or orthogonality.\nSketches can be represented as graphs, with the primitives as nodes and the\nconstraints as edges. Training a model to automatically generate CAD sketches\ncan enable several novel workflows, but is challenging due to the complexity of\nthe graphs and the heterogeneity of the primitives and constraints. In\nparticular, each type of primitive and constraint may require a record of\ndifferent size and parameter types. We propose SketchGen as a generative model\nbased on a transformer architecture to address the heterogeneity problem by\ncarefully designing a sequential language for the primitives and constraints\nthat allows distinguishing between different primitive or constraint types and\ntheir parameters, while encouraging our model to re-use information across\nrelated parameters, encoding shared structure. A particular highlight of our\nwork is the ability to produce primitives linked via constraints that enables\nthe final output to be further regularized via a constraint solver. We evaluate\nour model by demonstrating constraint prediction for given sets of primitives\nand full sketch generation from scratch, showing that our approach\nsignificantly out performs the state-of-the-art in CAD sketch generation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:45:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Para", "Wamiq Reyaz", ""], ["Bhat", "Shariq Farooq", ""], ["Guerrero", "Paul", ""], ["Kelly", "Tom", ""], ["Mitra", "Niloy", ""], ["Guibas", "Leonidas", ""], ["Wonka", "Peter", ""]]}, {"id": "2106.02719", "submitter": "Lluis Castrejon", "authors": "Lluis Castrejon, Nicolas Ballas, Aaron Courville", "title": "Hierarchical Video Generation for Complex Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos can often be created by first outlining a global description of the\nscene and then adding local details. Inspired by this we propose a hierarchical\nmodel for video generation which follows a coarse to fine approach. First our\nmodel generates a low resolution video, establishing the global scene\nstructure, that is then refined by subsequent levels in the hierarchy. We train\neach level in our hierarchy sequentially on partial views of the videos. This\nreduces the computational complexity of our generative model, which scales to\nhigh-resolution videos beyond a few frames. We validate our approach on\nKinetics-600 and BDD100K, for which we train a three level model capable of\ngenerating 256x256 videos with 48 frames.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:03:52 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Castrejon", "Lluis", ""], ["Ballas", "Nicolas", ""], ["Courville", "Aaron", ""]]}, {"id": "2106.02733", "submitter": "Ivan Sosnovik", "authors": "Ivan Sosnovik, Artem Moskalev, Arnold Smeulders", "title": "DISCO: accurate Discrete Scale Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale is often seen as a given, disturbing factor in many vision tasks. When\ndoing so it is one of the factors why we need more data during learning. In\nrecent work scale equivariance was added to convolutional neural networks. It\nwas shown to be effective for a range of tasks. We aim for accurate\nscale-equivariant convolutional neural networks (SE-CNNs) applicable for\nproblems where high granularity of scale and small filter sizes are required.\nCurrent SE-CNNs rely on weight sharing and filter rescaling, the latter of\nwhich is accurate for integer scales only. To reach accurate scale\nequivariance, we derive general constraints under which scale-convolution\nremains equivariant to discrete rescaling. We find the exact solution for all\ncases where it exists, and compute the approximation for the rest. The discrete\nscale-convolution pays off, as demonstrated in a new state-of-the-art\nclassification on MNIST-scale and improving the results on STL-10. With the\nsame SE scheme, we also improve the computational effort of a scale-equivariant\nSiamese tracker on OTB-13.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:48:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sosnovik", "Ivan", ""], ["Moskalev", "Artem", ""], ["Smeulders", "Arnold", ""]]}, {"id": "2106.02740", "submitter": "Dina Bashkirova", "authors": "Dina Bashkirova, Ziliang Zhu, James Akl, Fadi Alladkani, Ping Hu,\n  Vitaly Ablavsky, Berk Calli, Sarah Adel Bargal, Kate Saenko", "title": "ZeroWaste Dataset: Towards Automated Waste Recycling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Less than 35% of recyclable waste is being actually recycled in the US, which\nleads to increased soil and sea pollution and is one of the major concerns of\nenvironmental researchers as well as the common public. At the heart of the\nproblem is the inefficiencies of the waste sorting process (separating paper,\nplastic, metal, glass, etc.) due to the extremely complex and cluttered nature\nof the waste stream. Automated waste detection strategies have a great\npotential to enable more efficient, reliable and safer waste sorting practices,\nbut the literature lacks comprehensive datasets and methodology for the\nindustrial waste sorting solutions. In this paper, we take a step towards\ncomputer-aided waste detection and present the first in-the-wild\nindustrial-grade waste detection and segmentation dataset, ZeroWaste. This\ndataset contains over1800fully segmented video frames collected from a real\nwaste sorting plant along with waste material labels for training and\nevaluation of the segmentation methods, as well as over6000unlabeled frames\nthat can be further used for semi-supervised and self-supervised learning\ntechniques. ZeroWaste also provides frames of the conveyor belt before and\nafter the sorting process, comprising a novel setup that can be used for\nweakly-supervised segmentation. We present baselines for fully-, semi- and\nweakly-supervised segmentation methods. Our experimental results demonstrate\nthat state-of-the-art segmentation methods struggle to correctly detect and\nclassify target objects which suggests the challenging nature of our proposed\nin-the-wild dataset. We believe that ZeroWastewill catalyze research in object\ndetection and semantic segmentation in extreme clutter as well as applications\nin the recycling domain. Our project page can be found\nathttp://ai.bu.edu/zerowaste/.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 22:17:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bashkirova", "Dina", ""], ["Zhu", "Ziliang", ""], ["Akl", "James", ""], ["Alladkani", "Fadi", ""], ["Hu", "Ping", ""], ["Ablavsky", "Vitaly", ""], ["Calli", "Berk", ""], ["Bargal", "Sarah Adel", ""], ["Saenko", "Kate", ""]]}, {"id": "2106.02749", "submitter": "Bhavin Choksi", "authors": "Bhavin Choksi, Milad Mozafari, Callum Biggs O'May, Benjamin Ador,\n  Andrea Alamia, Rufin VanRullen", "title": "Predify: Augmenting deep neural networks with brain-inspired predictive\n  coding dynamics", "comments": "Preprint under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks excel at image classification, but their performance is\nfar less robust to input perturbations than human perception. In this work we\nexplore whether this shortcoming may be partly addressed by incorporating\nbrain-inspired recurrent dynamics in deep convolutional networks. We take\ninspiration from a popular framework in neuroscience: 'predictive coding'. At\neach layer of the hierarchical model, generative feedback 'predicts' (i.e.,\nreconstructs) the pattern of activity in the previous layer. The reconstruction\nerrors are used to iteratively update the network's representations across\ntimesteps, and to optimize the network's feedback weights over the natural\nimage dataset-a form of unsupervised training. We show that implementing this\nstrategy into two popular networks, VGG16 and EfficientNetB0, improves their\nrobustness against various corruptions. We hypothesize that other feedforward\nnetworks could similarly benefit from the proposed framework. To promote\nresearch in this direction, we provide an open-sourced PyTorch-based package\ncalled Predify, which can be used to implement and investigate the impacts of\nthe predictive coding dynamics in any convolutional neural network.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 22:48:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Choksi", "Bhavin", ""], ["Mozafari", "Milad", ""], ["O'May", "Callum Biggs", ""], ["Ador", "Benjamin", ""], ["Alamia", "Andrea", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2106.02773", "submitter": "Jiaming Wang", "authors": "Zhenfeng Shao, Jiaming Wang, Lianbing Deng, Xiao Huang, Tao Lu,\n  Ruiqian Zhang, Xianwei Lv, Qing Ding, and Zhiqiang Wang", "title": "GLSD: The Global Large-Scale Ship Database and Baseline Evaluations", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a challenging global large-scale ship database\n(called GLSD), designed specifically for ship detection tasks. The designed\nGLSD database includes a total of 140,616 annotated instances from 100,729\nimages. Based on the collected images, we propose 13 categories that widely\nexists in international routes. These categories include sailing boat, fishing\nboat, passenger ship, war ship, general cargo ship, container ship, bulk cargo\ncarrier, barge, ore carrier, speed boat, canoe, oil carrier, and tug. The\nmotivations of developing GLSD include the following: 1) providing a refined\nship detection database; 2) providing the worldwide researchers of ship\ndetection and exhaustive label information (bounding box and ship class label)\nin one uniform global database; and 3) providing a large-scale ship database\nwith geographic information (port and country information) that benefits\nmulti-modal analysis. In addition, we discuss the evaluation protocols given\nimage characteristics in GLSD and analyze the performance of selected\nstate-of-the-art object detection algorithms on GSLD, providing baselines for\nfuture studies. More information regarding the designed GLSD can be found at\nhttps://github.com/jiaming-wang/GLSD.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 01:49:41 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Shao", "Zhenfeng", ""], ["Wang", "Jiaming", ""], ["Deng", "Lianbing", ""], ["Huang", "Xiao", ""], ["Lu", "Tao", ""], ["Zhang", "Ruiqian", ""], ["Lv", "Xianwei", ""], ["Ding", "Qing", ""], ["Wang", "Zhiqiang", ""]]}, {"id": "2106.02775", "submitter": "Justin Yang", "authors": "Justin Yang and Judith E. Fan", "title": "Visual communication of object concepts at different levels of\n  abstraction", "comments": "To appear in Proceedings of the 43rd Annual Meeting of the Cognitive\n  Science Society. 7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  People can produce drawings of specific entities (e.g., Garfield), as well as\ngeneral categories (e.g., \"cat\"). What explains this ability to produce such\nvaried drawings of even highly familiar object concepts? We hypothesized that\ndrawing objects at different levels of abstraction depends on both sensory\ninformation and representational goals, such that drawings intended to portray\na recently seen object preserve more detail than those intended to represent a\ncategory. Participants drew objects cued either with a photo or a category\nlabel. For each cue type, half the participants aimed to draw a specific\nexemplar; the other half aimed to draw the category. We found that label-cued\ncategory drawings were the most recognizable at the basic level, whereas\nphoto-cued exemplar drawings were the least recognizable. Together, these\nfindings highlight the importance of task context for explaining how people use\ndrawings to communicate visual concepts in different ways.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 02:13:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yang", "Justin", ""], ["Fan", "Judith E.", ""]]}, {"id": "2106.02778", "submitter": "Yunfei Long", "authors": "Yunfei Long, Daniel Morris, Xiaoming Liu, Marcos Castro, Punarjay\n  Chakravarty and Praveen Narayanan", "title": "Radar-Camera Pixel Depth Association for Depth Completion", "comments": "IEEE Conference on Computer Vision and Pattern Recognition, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While radar and video data can be readily fused at the detection level,\nfusing them at the pixel level is potentially more beneficial. This is also\nmore challenging in part due to the sparsity of radar, but also because\nautomotive radar beams are much wider than a typical pixel combined with a\nlarge baseline between camera and radar, which results in poor association\nbetween radar pixels and color pixel. A consequence is that depth completion\nmethods designed for LiDAR and video fare poorly for radar and video. Here we\npropose a radar-to-pixel association stage which learns a mapping from radar\nreturns to pixels. This mapping also serves to densify radar returns. Using\nthis as a first stage, followed by a more traditional depth completion method,\nwe are able to achieve image-guided depth completion with radar and video. We\ndemonstrate performance superior to camera and radar alone on the nuScenes\ndataset. Our source code is available at https://github.com/longyunf/rc-pda.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 02:21:52 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Long", "Yunfei", ""], ["Morris", "Daniel", ""], ["Liu", "Xiaoming", ""], ["Castro", "Marcos", ""], ["Chakravarty", "Punarjay", ""], ["Narayanan", "Praveen", ""]]}, {"id": "2106.02781", "submitter": "Huanan Wang", "authors": "Huanan Wang, Xinyu Zhang, Jun Li, Zhiwei Li, Lei Yang, Shuyue Pan,\n  Yongqiang Deng", "title": "IPS300+: a Challenging Multimodal Dataset for Intersection Perception\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the high complexity and occlusion, insufficient perception in the\ncrowded urban intersection can be a serious safety risk for both human drivers\nand autonomous algorithms, whereas CVIS (Cooperative Vehicle Infrastructure\nSystem) is a proposed solution for full-participants perception in this\nscenario. However, the research on roadside multimodal perception is still in\nits infancy, and there is no open-source dataset for such scenario.\nAccordingly, this paper fills the gap. Through an IPS (Intersection Perception\nSystem) installed at the diagonal of the intersection, this paper proposes a\nhigh-quality multimodal dataset for the intersection perception task. The\ncenter of the experimental intersection covers an area of 3000m2, and the\nextended distance reaches 300m, which is typical for CVIS. The first batch of\nopen-source data includes 14198 frames, and each frame has an average of 319.84\nlabels, which is 9.6 times larger than the most crowded dataset (H3D dataset in\n2019) by now. In order to facilitate further study, this dataset tries to keep\nthe label documents consistent with the KITTI dataset, and a standardized\nbenchmark is created for algorithm evaluation. Our dataset is available at:\nhttp://www.openmpd.com/column/other_datasets.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 02:48:27 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Huanan", ""], ["Zhang", "Xinyu", ""], ["Li", "Jun", ""], ["Li", "Zhiwei", ""], ["Yang", "Lei", ""], ["Pan", "Shuyue", ""], ["Deng", "Yongqiang", ""]]}, {"id": "2106.02795", "submitter": "Yang Li", "authors": "Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, Samy Bengio", "title": "Learnable Fourier Features for Multi-Dimensional Spatial Positional\n  Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attentional mechanisms are order-invariant. Positional encoding is a crucial\ncomponent to allow attention-based deep model architectures such as Transformer\nto address sequences or images where the position of information matters. In\nthis paper, we propose a novel positional encoding method based on learnable\nFourier features. Instead of hard-coding each position as a token or a vector,\nwe represent each position, which can be multi-dimensional, as a trainable\nencoding based on learnable Fourier feature mapping, modulated with a\nmulti-layer perceptron. The representation is particularly advantageous for a\nspatial multi-dimensional position, e.g., pixel positions on an image, where\n$L_2$ distances or more complex positional relationships need to be captured.\nOur experiments based on several public benchmark tasks show that our learnable\nFourier feature representation for multi-dimensional positional encoding\noutperforms existing methods by both improving the accuracy and allowing faster\nconvergence.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 04:40:18 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 19:32:53 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Li", "Yang", ""], ["Si", "Si", ""], ["Li", "Gang", ""], ["Hsieh", "Cho-Jui", ""], ["Bengio", "Samy", ""]]}, {"id": "2106.02796", "submitter": "Sourbh Bhadane", "authors": "Sourbh Bhadane, Aaron B. Wagner, Jayadev Acharya", "title": "Principal Bit Analysis: Autoencoding with Schur-Concave Loss", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a linear autoencoder in which the latent variables are quantized,\nor corrupted by noise, and the constraint is Schur-concave in the set of latent\nvariances. Although finding the optimal encoder/decoder pair for this setup is\na nonconvex optimization problem, we show that decomposing the source into its\nprincipal components is optimal. If the constraint is strictly Schur-concave\nand the empirical covariance matrix has only simple eigenvalues, then any\noptimal encoder/decoder must decompose the source in this way. As one\napplication, we consider a strictly Schur-concave constraint that estimates the\nnumber of bits needed to represent the latent variables under fixed-rate\nencoding, a setup that we call \\emph{Principal Bit Analysis (PBA)}. This yields\na practical, general-purpose, fixed-rate compressor that outperforms existing\nalgorithms. As a second application, we show that a prototypical\nautoencoder-based variable-rate compressor is guaranteed to decompose the\nsource into its principal components.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 04:45:30 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 16:16:13 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Bhadane", "Sourbh", ""], ["Wagner", "Aaron B.", ""], ["Acharya", "Jayadev", ""]]}, {"id": "2106.02800", "submitter": "Mengjia Xu", "authors": "Qian Zhang, Konstantina Sampani, Mengjia Xu, Shengze Cai, Yixiang\n  Deng, He Li, Jennifer K. Sun, George Em Karniadakis", "title": "AOSLO-net: A deep learning-based method for automatic segmentation of\n  retinal microaneurysms from adaptive optics scanning laser ophthalmoscope\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Microaneurysms (MAs) are one of the earliest signs of diabetic retinopathy\n(DR), a frequent complication of diabetes that can lead to visual impairment\nand blindness. Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides\nreal-time retinal images with resolution down to 2 $\\mu m$ and thus allows\ndetection of the morphologies of individual MAs, a potential marker that might\ndictate MA pathology and affect the progression of DR. In contrast to the\nnumerous automatic models developed for assessing the number of MAs on fundus\nphotographs, currently there is no high throughput image protocol available for\nautomatic analysis of AOSLO photographs. To address this urgency, we introduce\nAOSLO-net, a deep neural network framework with customized training policies to\nautomatically segment MAs from AOSLO images. We evaluate the performance of\nAOSLO-net using 87 DR AOSLO images and our results demonstrate that the\nproposed model outperforms the state-of-the-art segmentation model both in\naccuracy and cost and enables correct MA morphological classification.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 05:06:36 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 21:41:47 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhang", "Qian", ""], ["Sampani", "Konstantina", ""], ["Xu", "Mengjia", ""], ["Cai", "Shengze", ""], ["Deng", "Yixiang", ""], ["Li", "He", ""], ["Sun", "Jennifer K.", ""], ["Karniadakis", "George Em", ""]]}, {"id": "2106.02804", "submitter": "Kuai Yu", "authors": "Kuai Yu, Hakeem Frank, Daniel Wilson", "title": "Points2Polygons: Context-Based Segmentation from Weak Labels Using\n  Adversarial Networks", "comments": "Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In applied image segmentation tasks, the ability to provide numerous and\nprecise labels for training is paramount to the accuracy of the model at\ninference time. However, this overhead is often neglected, and recently\nproposed segmentation architectures rely heavily on the availability and\nfidelity of ground truth labels to achieve state-of-the-art accuracies. Failure\nto acknowledge the difficulty in creating adequate ground truths can lead to an\nover-reliance on pre-trained models or a lack of adoption in real-world\napplications. We introduce Points2Polygons (P2P), a model which makes use of\ncontextual metric learning techniques that directly addresses this problem.\nPoints2Polygons performs well against existing fully-supervised segmentation\nbaselines with limited training data, despite using lightweight segmentation\nmodels (U-Net with a ResNet18 backbone) and having access to only weak labels\nin the form of object centroids and no pre-training. We demonstrate this on\nseveral different small but non-trivial datasets. We show that metric learning\nusing contextual data provides key insights for self-supervised tasks in\ngeneral, and allow segmentation models to easily generalize across\ntraditionally label-intensive domains in computer vision.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 05:17:45 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yu", "Kuai", ""], ["Frank", "Hakeem", ""], ["Wilson", "Daniel", ""]]}, {"id": "2106.02809", "submitter": "Kaihao Zhang", "authors": "Lirong Zheng, Yanshan Li, Kaihao Zhang, Wenhan Luo", "title": "T-Net: Deep Stacked Scale-Iteration Network for Image Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hazy images reduce the visibility of the image content, and haze will lead to\nfailure in handling subsequent computer vision tasks. In this paper, we address\nthe problem of image dehazing by proposing a dehazing network named T-Net,\nwhich consists of a backbone network based on the U-Net architecture and a dual\nattention module. And it can achieve multi-scale feature fusion by using skip\nconnections with a new fusion strategy. Furthermore, by repeatedly unfolding\nthe plain T-Net, Stack T-Net is proposed to take advantage of the dependence of\ndeep features across stages via a recursive strategy. In order to reduce\nnetwork parameters, the intra-stage recursive computation of ResNet is adopted\nin our Stack T-Net. And we take both the stage-wise result and the original\nhazy image as input to each T-Net and finally output the prediction of clean\nimage. Experimental results on both synthetic and real-world images demonstrate\nthat our plain T-Net and the advanced Stack T-Net perform favorably against the\nstate-of-the-art dehazing algorithms, and show that our Stack T-Net could\nfurther improve the dehazing effect, demonstrating the effectiveness of the\nrecursive strategy.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 06:01:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zheng", "Lirong", ""], ["Li", "Yanshan", ""], ["Zhang", "Kaihao", ""], ["Luo", "Wenhan", ""]]}, {"id": "2106.02813", "submitter": "Narinder Singh Punn", "authors": "Harish Rajora, Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali\n  Agarwal", "title": "Machine learning equipped web based disease prediction and recommender\n  system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Worldwide, several cases go undiagnosed due to poor healthcare support in\nremote areas. In this context, a centralized system is needed for effective\nmonitoring and analysis of the medical records. A web-based patient diagnostic\nsystem is a central platform to store the medical history and predict the\npossible disease based on the current symptoms experienced by a patient to\nensure faster and accurate diagnosis. Early disease prediction can help the\nusers determine the severity of the disease and take quick action. The proposed\nweb-based disease prediction system utilizes machine learning based\nclassification techniques on a data set acquired from the National Centre of\nDisease Control (NCDC). $K$-nearest neighbor (K-NN), random forest and naive\nbayes classification approaches are utilized and an ensemble voting algorithm\nis also proposed where each classifier is assigned weights dynamically based on\nthe prediction confidence. The proposed system is also equipped with a\nrecommendation scheme to recommend the type of tests based on the existing\nsymptoms of the patient, so that necessary precautions can be taken. A\ncentralized database ensures that the medical data is preserved and there is\ntransparency in the system. The tampering into the system is prevented by\ngiving the no \"updation\" rights once the diagnosis is created.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 06:47:54 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 14:05:03 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rajora", "Harish", ""], ["Punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2106.02824", "submitter": "Wei Shen", "authors": "Yilin Wang, Shaozuo Yu, Xiaokang Yang, Wei Shen", "title": "Making CNNs Interpretable by Building Dynamic Sequential Decision\n  Forests with Top-down Hierarchy Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generic model transfer scheme to make\nConvlutional Neural Networks (CNNs) interpretable, while maintaining their high\nclassification accuracy. We achieve this by building a differentiable decision\nforest on top of CNNs, which enjoys two characteristics: 1) During training,\nthe tree hierarchies of the forest are learned in a top-down manner under the\nguidance from the category semantics embedded in the pre-trained CNN weights;\n2) During inference, a single decision tree is dynamically selected from the\nforest for each input sample, enabling the transferred model to make sequential\ndecisions corresponding to the attributes shared by semantically-similar\ncategories, rather than directly performing flat classification. We name the\ntransferred model deep Dynamic Sequential Decision Forest (dDSDF). Experimental\nresults show that dDSDF not only achieves higher classification accuracy than\nits conuterpart, i.e., the original CNN, but has much better interpretability,\nas qualitatively it has plausible hierarchies and quantitatively it leads to\nmore precise saliency maps.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 07:41:18 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Yilin", ""], ["Yu", "Shaozuo", ""], ["Yang", "Xiaokang", ""], ["Shen", "Wei", ""]]}, {"id": "2106.02842", "submitter": "Luca Ciampi", "authors": "Luca Ciampi, Claudio Gennaro, Fabio Carrara, Fabrizio Falchi, Claudio\n  Vairo, Giuseppe Amato", "title": "Multi-Camera Vehicle Counting Using Edge-AI", "comments": "Submitted to Expert Systems With Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel solution to automatically count vehicles in a\nparking lot using images captured by smart cameras. Unlike most of the\nliterature on this task, which focuses on the analysis of single images, this\npaper proposes the use of multiple visual sources to monitor a wider parking\narea from different perspectives. The proposed multi-camera system is capable\nof automatically estimate the number of cars present in the entire parking lot\ndirectly on board the edge devices. It comprises an on-device deep\nlearning-based detector that locates and counts the vehicles from the captured\nimages and a decentralized geometric-based approach that can analyze the\ninter-camera shared areas and merge the data acquired by all the devices. We\nconduct the experimental evaluation on an extended version of the CNRPark-EXT\ndataset, a collection of images taken from the parking lot on the campus of the\nNational Research Council (CNR) in Pisa, Italy. We show that our system is\nrobust and takes advantage of the redundant information deriving from the\ndifferent cameras, improving the overall performance without requiring any\nextra geometrical information of the monitored scene.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 08:52:20 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ciampi", "Luca", ""], ["Gennaro", "Claudio", ""], ["Carrara", "Fabio", ""], ["Falchi", "Fabrizio", ""], ["Vairo", "Claudio", ""], ["Amato", "Giuseppe", ""]]}, {"id": "2106.02845", "submitter": "Jiaxing Huang", "authors": "Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu", "title": "Semi-Supervised Domain Adaptation via Adaptive and Progressive Feature\n  Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary domain adaptive semantic segmentation aims to address data\nannotation challenges by assuming that target domains are completely\nunannotated. However, annotating a few target samples is usually very\nmanageable and worthwhile especially if it improves the adaptation performance\nsubstantially. This paper presents SSDAS, a Semi-Supervised Domain Adaptive\nimage Segmentation network that employs a few labeled target samples as anchors\nfor adaptive and progressive feature alignment between labeled source samples\nand unlabeled target samples. We position the few labeled target samples as\nreferences that gauge the similarity between source and target features and\nguide adaptive inter-domain alignment for learning more similar source\nfeatures. In addition, we replace the dissimilar source features by\nhigh-confidence target features continuously during the iterative training\nprocess, which achieves progressive intra-domain alignment between confident\nand unconfident target features. Extensive experiments show the proposed SSDAS\ngreatly outperforms a number of baselines, i.e., UDA-based semantic\nsegmentation and SSDA-based image classification. In addition, SSDAS is\ncomplementary and can be easily incorporated into UDA-based methods with\nconsistent improvements in domain adaptive semantic segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 09:12:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Huang", "Jiaxing", ""], ["Guan", "Dayan", ""], ["Xiao", "Aoran", ""], ["Lu", "Shijian", ""]]}, {"id": "2106.02852", "submitter": "Yehui Tang", "authors": "Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu,\n  Dacheng Tao", "title": "Patch Slimming for Efficient Vision Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the efficiency problem for visual transformers by\nexcavating redundant calculation in given networks. The recent transformer\narchitecture has demonstrated its effectiveness for achieving excellent\nperformance on a series of computer vision tasks. However, similar to that of\nconvolutional neural networks, the huge computational cost of vision\ntransformers is still a severe issue. Considering that the attention mechanism\naggregates different patches layer-by-layer, we present a novel patch slimming\napproach that discards useless patches in a top-down paradigm. We first\nidentify the effective patches in the last layer and then use them to guide the\npatch selection process of previous layers. For each layer, the impact of a\npatch on the final output feature is approximated and patches with less impact\nwill be removed. Experimental results on benchmark datasets demonstrate that\nthe proposed method can significantly reduce the computational costs of vision\ntransformers without affecting their performances. For example, over 45% FLOPs\nof the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the\nImageNet dataset.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 09:46:00 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tang", "Yehui", ""], ["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Xu", "Chang", ""], ["Guo", "Jianyuan", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "2106.02853", "submitter": "Jun Ling", "authors": "Jun Ling, Han Xue, Li Song, Rong Xie and Xiao Gu", "title": "Region-aware Adaptive Instance Normalization for Image Harmonization", "comments": "Accepted to IEEE CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image composition plays a common but important role in photo editing. To\nacquire photo-realistic composite images, one must adjust the appearance and\nvisual style of the foreground to be compatible with the background. Existing\ndeep learning methods for harmonizing composite images directly learn an image\nmapping network from the composite to the real one, without explicit\nexploration on visual style consistency between the background and the\nforeground images. To ensure the visual style consistency between the\nforeground and the background, in this paper, we treat image harmonization as a\nstyle transfer problem. In particular, we propose a simple yet effective\nRegion-aware Adaptive Instance Normalization (RAIN) module, which explicitly\nformulates the visual style from the background and adaptively applies them to\nthe foreground. With our settings, our RAIN module can be used as a drop-in\nmodule for existing image harmonization networks and is able to bring\nsignificant improvements. Extensive experiments on the existing image\nharmonization benchmark datasets show the superior capability of the proposed\nmethod. Code is available at {https://github.com/junleen/RainNet}.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 09:57:17 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ling", "Jun", ""], ["Xue", "Han", ""], ["Song", "Li", ""], ["Xie", "Rong", ""], ["Gu", "Xiao", ""]]}, {"id": "2106.02859", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Xiaolin Hu", "title": "Convolutional Neural Networks with Gated Recurrent Connections", "comments": "Accepted by TPAMI. An extension of our previous NeurIPS 2017 paper\n  \"Gated recurrent convolution neural network for OCR\". We demonstrate the good\n  performance of GRCNN on image classification and object detection. Codes are\n  available at: https://github.com/Jianf-Wang/GRCNN", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3054614", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The convolutional neural network (CNN) has become a basic model for solving\nmany computer vision problems. In recent years, a new class of CNNs, recurrent\nconvolution neural network (RCNN), inspired by abundant recurrent connections\nin the visual systems of animals, was proposed. The critical element of RCNN is\nthe recurrent convolutional layer (RCL), which incorporates recurrent\nconnections between neurons in the standard convolutional layer. With\nincreasing number of recurrent computations, the receptive fields (RFs) of\nneurons in RCL expand unboundedly, which is inconsistent with biological facts.\nWe propose to modulate the RFs of neurons by introducing gates to the recurrent\nconnections. The gates control the amount of context information inputting to\nthe neurons and the neurons' RFs therefore become adaptive. The resulting layer\nis called gated recurrent convolution layer (GRCL). Multiple GRCLs constitute a\ndeep model called gated RCNN (GRCNN). The GRCNN was evaluated on several\ncomputer vision tasks including object recognition, scene text recognition and\nobject detection, and obtained much better results than the RCNN. In addition,\nwhen combined with other adaptive RF techniques, the GRCNN demonstrated\ncompetitive performance to the state-of-the-art models on benchmark datasets\nfor these tasks. The codes are released at\n\\href{https://github.com/Jianf-Wang/GRCNN}{https://github.com/Jianf-Wang/GRCNN}.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 10:14:59 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Jianfeng", ""], ["Hu", "Xiaolin", ""]]}, {"id": "2106.02864", "submitter": "Suvidha Tripathi Dr", "authors": "Suvidha Tripathi, Satish Kumar Singh, Hwee Kuan Lee", "title": "An End-to-End Breast Tumour Classification Model Using Context-Based\n  Patch Modelling- A BiLSTM Approach for Image Classification", "comments": "36 pages, 5 figures, 9 tables. Published in Computerized Medical\n  Imaging and Graphics", "journal-ref": "Computerized Medical Imaging and Graphics, 87, 101838 (2021)", "doi": "10.1016/j.compmedimag.2020.101838", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers working on computational analysis of Whole Slide Images (WSIs) in\nhistopathology have primarily resorted to patch-based modelling due to large\nresolution of each WSI. The large resolution makes WSIs infeasible to be fed\ndirectly into the machine learning models due to computational constraints.\nHowever, due to patch-based analysis, most of the current methods fail to\nexploit the underlying spatial relationship among the patches. In our work, we\nhave tried to integrate this relationship along with feature-based correlation\namong the extracted patches from the particular tumorous region. For the given\ntask of classification, we have used BiLSTMs to model both forward and backward\ncontextual relationship. RNN based models eliminate the limitation of sequence\nsize by allowing the modelling of variable size images within a deep learning\nmodel. We have also incorporated the effect of spatial continuity by exploring\ndifferent scanning techniques used to sample patches. To establish the\nefficiency of our approach, we trained and tested our model on two datasets,\nmicroscopy images and WSI tumour regions. After comparing with contemporary\nliterature we achieved the better performance with accuracy of 90% for\nmicroscopy image dataset. For WSI tumour region dataset, we compared the\nclassification results with deep learning networks such as ResNet, DenseNet,\nand InceptionV3 using maximum voting technique. We achieved the highest\nperformance accuracy of 84%. We found out that BiLSTMs with CNN features have\nperformed much better in modelling patches into an end-to-end Image\nclassification network. Additionally, the variable dimensions of WSI tumour\nregions were used for classification without the need for resizing. This\nsuggests that our method is independent of tumour image size and can process\nlarge dimensional images without losing the resolution details.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 10:43:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tripathi", "Suvidha", ""], ["Singh", "Satish Kumar", ""], ["Lee", "Hwee Kuan", ""]]}, {"id": "2106.02874", "submitter": "Jiaxing Huang", "authors": "Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu", "title": "RDA: Robust Domain Adaptation via Fourier Adversarial Attacking", "comments": "Accepted to ICCV2021 (International Conference on Computer Vision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled\nsource domain and an unsupervised loss in an unlabeled target domain, which\noften faces more severe overfitting (than classical supervised learning) as the\nsupervised source loss has clear domain gap and the unsupervised target loss is\noften noisy due to the lack of annotations. This paper presents RDA, a robust\ndomain adaptation technique that introduces adversarial attacking to mitigate\noverfitting in UDA. We achieve robust domain adaptation by a novel Fourier\nadversarial attacking (FAA) method that allows large magnitude of perturbation\nnoises but has minimal modification of image semantics, the former is critical\nto the effectiveness of its generated adversarial samples due to the existence\nof 'domain gaps'. Specifically, FAA decomposes images into multiple frequency\ncomponents (FCs) and generates adversarial samples by just perturbating certain\nFCs that capture little semantic information. With FAA-generated samples, the\ntraining can continue the 'random walk' and drift into an area with a flat loss\nlandscape, leading to more robust domain adaptation. Extensive experiments over\nmultiple domain adaptation tasks show that RDA can work with different computer\nvision tasks with superior performance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 11:38:41 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 09:56:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Huang", "Jiaxing", ""], ["Guan", "Dayan", ""], ["Xiao", "Aoran", ""], ["Lu", "Shijian", ""]]}, {"id": "2106.02884", "submitter": "Qian Zhao", "authors": "Hui Wang, Zongsheng Yue, Qian Zhao, Deyu Meng", "title": "A Deep Variational Bayesian Framework for Blind Image Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blind image deblurring is an important yet very challenging problem in\nlow-level vision. Traditional optimization based methods generally formulate\nthis task as a maximum-a-posteriori estimation or variational inference\nproblem, whose performance highly relies on the handcraft priors for both the\nlatent image and the blur kernel. In contrast, recent deep learning methods\ngenerally learn, from a large collection of training images, deep neural\nnetworks (DNNs) directly mapping the blurry image to the clean one or to the\nblur kernel, paying less attention to the physical degradation process of the\nblurry image. In this paper, we present a deep variational Bayesian framework\nfor blind image deblurring. Under this framework, the posterior of the latent\nclean image and blur kernel can be jointly estimated in an amortized inference\nfashion with DNNs, and the involved inference DNNs can be trained by fully\nconsidering the physical blur model, together with the supervision of data\ndriven priors for the clean image and blur kernel, which is naturally led to by\nthe evidence lower bound objective. Comprehensive experiments are conducted to\nsubstantiate the effectiveness of the proposed framework. The results show that\nit can not only achieve a promising performance with relatively simple\nnetworks, but also enhance the performance of existing DNNs for deblurring.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 12:47:36 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Hui", ""], ["Yue", "Zongsheng", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""]]}, {"id": "2106.02885", "submitter": "Jiaxing Huang", "authors": "Jiaxing Huang, Dayan Guan, Aoran Xiao, Shijian Lu, Ling Shao", "title": "Category Contrast for Unsupervised Domain Adaptation in Visual Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance contrast for unsupervised representation learning has achieved great\nsuccess in recent years. In this work, we explore the idea of instance\ncontrastive learning in unsupervised domain adaptation (UDA) and propose a\nnovel Category Contrast technique (CaCo) that introduces semantic priors on top\nof instance discrimination for visual UDA tasks. By considering instance\ncontrastive learning as a dictionary look-up operation, we construct a\nsemantics-aware dictionary with samples from both source and target domains\nwhere each target sample is assigned a (pseudo) category label based on the\ncategory priors of source samples. This allows category contrastive learning\n(between target queries and the category-level dictionary) for\ncategory-discriminative yet domain-invariant feature representations: samples\nof the same category (from either source or target domain) are pulled closer\nwhile those of different categories are pushed apart simultaneously. Extensive\nUDA experiments in multiple visual tasks ($e.g.$, segmentation, classification\nand detection) show that the simple implementation of CaCo achieves superior\nperformance as compared with the highly-optimized state-of-the-art methods.\nAnalytically and empirically, the experiments also demonstrate that CaCo is\ncomplementary to existing UDA methods and generalizable to other learning\nsetups such as semi-supervised learning, unsupervised model adaptation, etc.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 12:51:35 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 03:08:14 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Huang", "Jiaxing", ""], ["Guan", "Dayan", ""], ["Xiao", "Aoran", ""], ["Lu", "Shijian", ""], ["Shao", "Ling", ""]]}, {"id": "2106.02898", "submitter": "Kai Han", "authors": "Mingjian Zhu, Kai Han, Enhua Wu, Qiulin Zhang, Ying Nie, Zhenzhong\n  Lan, Yunhe Wang", "title": "Dynamic Resolution Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are often of sophisticated design\nwith numerous convolutional layers and learnable parameters for the accuracy\nreason. To alleviate the expensive costs of deploying them on mobile devices,\nrecent works have made huge efforts for excavating redundancy in pre-defined\narchitectures. Nevertheless, the redundancy on the input resolution of modern\nCNNs has not been fully investigated, i.e., the resolution of input image is\nfixed. In this paper, we observe that the smallest resolution for accurately\npredicting the given image is different using the same neural network. To this\nend, we propose a novel dynamic-resolution network (DRNet) in which the\nresolution is determined dynamically based on each input sample. Thus, a\nresolution predictor with negligible computational costs is explored and\noptimized jointly with the desired network. In practice, the predictor learns\nthe smallest resolution that can retain and even exceed the original\nrecognition accuracy for each image. During the inference, each input image\nwill be resized to its predicted resolution for minimizing the overall\ncomputation burden. We then conduct extensive experiments on several benchmark\nnetworks and datasets. The results show that our DRNet can be embedded in any\noff-the-shelf network architecture to obtain a considerable reduction in\ncomputational complexity. For instance, DRNet achieves similar performance with\nan about 34% computation reduction, while gains 1.4% accuracy increase with 10%\ncomputation reduction compared to the original ResNet-50 on ImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 13:48:33 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhu", "Mingjian", ""], ["Han", "Kai", ""], ["Wu", "Enhua", ""], ["Zhang", "Qiulin", ""], ["Nie", "Ying", ""], ["Lan", "Zhenzhong", ""], ["Wang", "Yunhe", ""]]}, {"id": "2106.02914", "submitter": "Yue Wu", "authors": "Yue Wu, Yuan Lan, Luchan Zhang, Yang Xiang", "title": "Feature Flow Regularization: Improving Structured Sparsity in Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning is a model compression method that removes redundant parameters in\ndeep neural networks (DNNs) while maintaining accuracy. Most available filter\npruning methods require complex treatments such as iterative pruning, features\nstatistics/ranking, or additional optimization designs in the training process.\nIn this paper, we propose a simple and effective regularization strategy from a\nnew perspective of evolution of features, which we call feature flow\nregularization (FFR), for improving structured sparsity and filter pruning in\nDNNs. Specifically, FFR imposes controls on the gradient and curvature of\nfeature flow along the neural network, which implicitly increases the sparsity\nof the parameters. The principle behind FFR is that coherent and smooth\nevolution of features will lead to an efficient network that avoids redundant\nparameters. The high structured sparsity obtained from FFR enables us to prune\nfilters effectively. Experiments with VGGNets, ResNets on CIFAR-10/100, and\nTiny ImageNet datasets demonstrate that FFR can significantly improve both\nunstructured and structured sparsity. Our pruning results in terms of reduction\nof parameters and FLOPs are comparable to or even better than those of\nstate-of-the-art pruning methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 15:00:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wu", "Yue", ""], ["Lan", "Yuan", ""], ["Zhang", "Luchan", ""], ["Xiang", "Yang", ""]]}, {"id": "2106.02923", "submitter": "Travers Rhodes", "authors": "Travers Rhodes, Daniel D. Lee", "title": "Local Disentanglement in Variational Auto-Encoders Using Jacobian $L_1$\n  Regularization", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There have been many recent advances in representation learning; however,\nunsupervised representation learning can still struggle with model\nidentification issues. Variational Auto-Encoders (VAEs) and their extensions\nsuch as $\\beta$-VAEs have been shown to locally align latent variables with PCA\ndirections, which can help to improve model disentanglement under some\nconditions. Borrowing inspiration from Independent Component Analysis (ICA) and\nsparse coding, we propose applying an $L_1$ loss to the VAE's generative\nJacobian during training to encourage local latent variable alignment with\nindependent factors of variation in the data. We demonstrate our results on a\nvariety of datasets, giving qualitative and quantitative results using\ninformation theoretic and modularity measures that show our added $L_1$ cost\nencourages local axis alignment of the latent representation with individual\nfactors of variation.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 15:40:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rhodes", "Travers", ""], ["Lee", "Daniel D.", ""]]}, {"id": "2106.02930", "submitter": "Jiachen Li", "authors": "Defu Cao and Jiachen Li and Hengbo Ma and Masayoshi Tomizuka", "title": "Spectral Temporal Graph Neural Network for Trajectory Prediction", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective understanding of the contextual environment and accurate motion\nforecasting of surrounding agents is crucial for the development of autonomous\nvehicles and social mobile robots. This task is challenging since the behavior\nof an autonomous agent is not only affected by its own intention, but also by\nthe static environment and surrounding dynamically interacting agents. Previous\nworks focused on utilizing the spatial and temporal information in time domain\nwhile not sufficiently taking advantage of the cues in frequency domain. To\nthis end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which\ncan capture inter-agent correlations and temporal dependency simultaneously in\nfrequency domain in addition to time domain. SpecTGNN operates on both an agent\ngraph with dynamic state information and an environment graph with the features\nextracted from context images in two streams. The model integrates graph\nFourier transform, spectral graph convolution and temporal gated convolution to\nencode history information and forecast future trajectories. Moreover, we\nincorporate a multi-head spatio-temporal attention mechanism to mitigate the\neffect of error propagation in a long time horizon. We demonstrate the\nperformance of SpecTGNN on two public trajectory prediction benchmark datasets,\nwhich achieves state-of-the-art performance in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 16:51:54 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cao", "Defu", ""], ["Li", "Jiachen", ""], ["Ma", "Hengbo", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2106.02953", "submitter": "Shashi Kant Gupta", "authors": "Shashi Kant Gupta, Mengmi Zhang, Chia-Chien Wu, Jeremy M. Wolfe,\n  Gabriel Kreiman", "title": "Visual Search Asymmetry: Deep Nets and Humans Share Similar Inherent\n  Biases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual search is a ubiquitous and often challenging daily task, exemplified\nby looking for the car keys at home or a friend in a crowd. An intriguing\nproperty of some classical search tasks is an asymmetry such that finding a\ntarget A among distractors B can be easier than finding B among A. To elucidate\nthe mechanisms responsible for asymmetry in visual search, we propose a\ncomputational model that takes a target and a search image as inputs and\nproduces a sequence of eye movements until the target is found. The model\nintegrates eccentricity-dependent visual recognition with target-dependent\ntop-down cues. We compared the model against human behavior in six paradigmatic\nsearch tasks that show asymmetry in humans. Without prior exposure to the\nstimuli or task-specific training, the model provides a plausible mechanism for\nsearch asymmetry. We hypothesized that the polarity of search asymmetry arises\nfrom experience with the natural environment. We tested this hypothesis by\ntraining the model on an augmented version of ImageNet where the biases of\nnatural images were either removed or reversed. The polarity of search\nasymmetry disappeared or was altered depending on the training protocol. This\nstudy highlights how classical perceptual properties can emerge in neural\nnetwork models, without the need for task-specific training, but rather as a\nconsequence of the statistical properties of the developmental diet fed to the\nmodel. All source code and stimuli are publicly available\nhttps://github.com/kreimanlab/VisualSearchAsymmetry\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 19:46:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gupta", "Shashi Kant", ""], ["Zhang", "Mengmi", ""], ["Wu", "Chia-Chien", ""], ["Wolfe", "Jeremy M.", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "2106.02990", "submitter": "Ziyu Jiang", "authors": "Ziyu Jiang, Tianlong Chen, Bobak Mortazavi, Zhangyang Wang", "title": "Self-Damaging Contrastive Learning", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent breakthrough achieved by contrastive learning accelerates the pace\nfor deploying unsupervised training on real-world data applications. However,\nunlabeled data in reality is commonly imbalanced and shows a long-tail\ndistribution, and it is unclear how robustly the latest contrastive learning\nmethods could perform in the practical scenario. This paper proposes to\nexplicitly tackle this challenge, via a principled framework called\nSelf-Damaging Contrastive Learning (SDCLR), to automatically balance the\nrepresentation learning without knowing the classes. Our main inspiration is\ndrawn from the recent finding that deep models have difficult-to-memorize\nsamples, and those may be exposed through network pruning. It is further\nnatural to hypothesize that long-tail samples are also tougher for the model to\nlearn well due to insufficient examples. Hence, the key innovation in SDCLR is\nto create a dynamic self-competitor model to contrast with the target model,\nwhich is a pruned version of the latter. During training, contrasting the two\nmodels will lead to adaptive online mining of the most easily forgotten samples\nfor the current target model, and implicitly emphasize them more in the\ncontrastive loss. Extensive experiments across multiple datasets and imbalance\nsettings show that SDCLR significantly improves not only overall accuracies but\nalso balancedness, in terms of linear evaluation on the full-shot and few-shot\nsettings. Our code is available at: https://github.com/VITA-Group/SDCLR.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 00:04:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Jiang", "Ziyu", ""], ["Chen", "Tianlong", ""], ["Mortazavi", "Bobak", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2106.02994", "submitter": "Alex Wong", "authors": "Alex Wong, Safa Cicek, and Stefano Soatto", "title": "Learning Topology from Synthetic Data for Unsupervised Depth Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a method for inferring dense depth maps from images and sparse\ndepth measurements by leveraging synthetic data to learn the association of\nsparse point clouds with dense natural shapes, and using the image as evidence\nto validate the predicted depth map. Our learned prior for natural shapes uses\nonly sparse depth as input, not images, so the method is not affected by the\ncovariate shift when attempting to transfer learned models from synthetic data\nto real ones. This allows us to use abundant synthetic data with ground truth\nto learn the most difficult component of the reconstruction process, which is\ntopology estimation, and use the image to refine the prediction based on\nphotometric evidence. Our approach uses fewer parameters than previous methods,\nyet, achieves the state of the art on both indoor and outdoor benchmark\ndatasets. Code available at:\nhttps://github.com/alexklwong/learning-topology-synthetic-data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 00:21:12 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 01:46:56 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wong", "Alex", ""], ["Cicek", "Safa", ""], ["Soatto", "Stefano", ""]]}, {"id": "2106.03010", "submitter": "Alex Wong", "authors": "Alex Wong, Xiaohan Fei, Byung-Woo Hong, and Stefano Soatto", "title": "An Adaptive Framework for Learning Unsupervised Depth Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a method to infer a dense depth map from a color image and\nassociated sparse depth measurements. Our main contribution lies in the design\nof an annealing process for determining co-visibility (occlusions,\ndisocclusions) and the degree of regularization to impose on the model. We show\nthat regularization and co-visibility are related via the fitness (residual) of\nmodel to data and both can be unified into a single framework to improve the\nlearning process. Our method is an adaptive weighting scheme that guides\noptimization by measuring the residual at each pixel location over each\ntraining step for (i) estimating a soft visibility mask and (ii) determining\nthe amount of regularization. We demonstrate the effectiveness our method by\napplying it to several recent unsupervised depth completion methods and\nimproving their performance on public benchmark datasets, without incurring\nadditional trainable parameters or increase in inference time. Code available\nat: https://github.com/alexklwong/adaframe-depth-completion.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 02:27:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wong", "Alex", ""], ["Fei", "Xiaohan", ""], ["Hong", "Byung-Woo", ""], ["Soatto", "Stefano", ""]]}, {"id": "2106.03021", "submitter": "Limin Wang", "authors": "Zeyu Ruan, Changqing Zou, Longhai Wu, Gangshan Wu, Limin Wang", "title": "SADRNet: Self-Aligned Dual Face Regression Networks for Robust 3D Dense\n  Face Alignment and Reconstruction", "comments": "To appear in IEEE Transactions on Image Processing. Code and model is\n  available at https://github.com/MCG-NJU/SADRNet", "journal-ref": null, "doi": "10.1109/TIP.2021.3087397", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Three-dimensional face dense alignment and reconstruction in the wild is a\nchallenging problem as partial facial information is commonly missing in\noccluded and large pose face images. Large head pose variations also increase\nthe solution space and make the modeling more difficult. Our key idea is to\nmodel occlusion and pose to decompose this challenging task into several\nrelatively more manageable subtasks. To this end, we propose an end-to-end\nframework, termed as Self-aligned Dual face Regression Network (SADRNet), which\npredicts a pose-dependent face, a pose-independent face. They are combined by\nan occlusion-aware self-alignment to generate the final 3D face. Extensive\nexperiments on two popular benchmarks, AFLW2000-3D and Florence, demonstrate\nthat the proposed method achieves significant superior performance over\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 03:31:24 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ruan", "Zeyu", ""], ["Zou", "Changqing", ""], ["Wu", "Longhai", ""], ["Wu", "Gangshan", ""], ["Wang", "Limin", ""]]}, {"id": "2106.03041", "submitter": "John Cai", "authors": "John Cai, Bill Cai, Shengmei Shen", "title": "DAMSL: Domain Agnostic Meta Score-based Learning", "comments": "Accepted to CVPR 2021 L2ID Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Domain Agnostic Meta Score-based Learning (DAMSL),\na novel, versatile and highly effective solution that delivers significant\nout-performance over state-of-the-art methods for cross-domain few-shot\nlearning. We identify key problems in previous meta-learning methods\nover-fitting to the source domain, and previous transfer-learning methods\nunder-utilizing the structure of the support set. The core idea behind our\nmethod is that instead of directly using the scores from a fine-tuned feature\nencoder, we use these scores to create input coordinates for a domain agnostic\nmetric space. A graph neural network is applied to learn an embedding and\nrelation function over these coordinates to process all information contained\nin the score distribution of the support set. We test our model on both\nestablished CD-FSL benchmarks and new domains and show that our method\novercomes the limitations of previous meta-learning and transfer-learning\nmethods to deliver substantial improvements in accuracy across both smaller and\nlarger domain shifts.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 06:08:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cai", "John", ""], ["Cai", "Bill", ""], ["Shen", "Shengmei", ""]]}, {"id": "2106.03043", "submitter": "Jing Jin", "authors": "Jing Jin and Junhui Hou", "title": "Occlusion-aware Unsupervised Learning of Depth from 4-D Light Fields", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth estimation is a fundamental issue in 4-D light field processing and\nanalysis. Although recent supervised learning-based light field depth\nestimation methods have significantly improved the accuracy and efficiency of\ntraditional optimization-based ones, these methods rely on the training over\nlight field data with ground-truth depth maps which are challenging to obtain\nor even unavailable for real-world light field data. Besides, due to the\ninevitable gap (or domain difference) between real-world and synthetic data,\nthey may suffer from serious performance degradation when generalizing the\nmodels trained with synthetic data to real-world data. By contrast, we propose\nan unsupervised learning-based method, which does not require ground-truth\ndepth as supervision during training. Specifically, based on the basic\nknowledge of the unique geometry structure of light field data, we present an\nocclusion-aware strategy to improve the accuracy on occlusion areas, in which\nwe explore the angular coherence among subsets of the light field views to\nestimate initial depth maps, and utilize a constrained unsupervised loss to\nlearn their corresponding reliability for final depth prediction. Additionally,\nwe adopt a multi-scale network with a weighted smoothness loss to handle the\ntextureless areas. Experimental results on synthetic data show that our method\ncan significantly shrink the performance gap between the previous unsupervised\nmethod and supervised ones, and produce depth maps with comparable accuracy to\ntraditional methods with obviously reduced computational cost. Moreover,\nexperiments on real-world datasets show that our method can avoid the domain\nshift problem presented in supervised methods, demonstrating the great\npotential of our method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 06:19:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Jin", "Jing", ""], ["Hou", "Junhui", ""]]}, {"id": "2106.03052", "submitter": "Jian Cheng", "authors": "Jian Cheng, Ziyang Liu, Hao Guan, Zhenzhou Wu, Haogang Zhu, Jiyang\n  Jiang, Wei Wen, Dacheng Tao, Tao Liu", "title": "Brain Age Estimation From MRI Using Cascade Networks with Ranking Loss", "comments": "Accepted by IEEE transactions on Medical Imaging, 13 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TMI.2021.3085948", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronological age of healthy people is able to be predicted accurately using\ndeep neural networks from neuroimaging data, and the predicted brain age could\nserve as a biomarker for detecting aging-related diseases. In this paper, a\nnovel 3D convolutional network, called two-stage-age-network (TSAN), is\nproposed to estimate brain age from T1-weighted MRI data. Compared with\nexisting methods, TSAN has the following improvements. First, TSAN uses a\ntwo-stage cascade network architecture, where the first-stage network estimates\na rough brain age, then the second-stage network estimates the brain age more\naccurately from the discretized brain age by the first-stage network. Second,\nto our knowledge, TSAN is the first work to apply novel ranking losses in brain\nage estimation, together with the traditional mean square error (MSE) loss.\nThird, densely connected paths are used to combine feature maps with different\nscales. The experiments with $6586$ MRIs showed that TSAN could provide\naccurate brain age estimation, yielding mean absolute error (MAE) of $2.428$\nand Pearson's correlation coefficient (PCC) of $0.985$, between the estimated\nand chronological ages. Furthermore, using the brain age gap between brain age\nand chronological age as a biomarker, Alzheimer's disease (AD) and Mild\nCognitive Impairment (MCI) can be distinguished from healthy control (HC)\nsubjects by support vector machine (SVM). Classification AUC in AD/HC and\nMCI/HC was $0.904$ and $0.823$, respectively. It showed that brain age gap is\nan effective biomarker associated with risk of dementia, and has potential for\nearly-stage dementia risk screening. The codes and trained models have been\nreleased on GitHub: https://github.com/Milan-BUAA/TSAN-brain-age-estimation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 07:11:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cheng", "Jian", ""], ["Liu", "Ziyang", ""], ["Guan", "Hao", ""], ["Wu", "Zhenzhou", ""], ["Zhu", "Haogang", ""], ["Jiang", "Jiyang", ""], ["Wen", "Wei", ""], ["Tao", "Dacheng", ""], ["Liu", "Tao", ""]]}, {"id": "2106.03064", "submitter": "Soumyabrata Dev", "authors": "Mayank Jain, Conor Meegan, and Soumyabrata Dev", "title": "Using GANs to Augment Data for Cloud Image Segmentation Task", "comments": "Published in IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While cloud/sky image segmentation has extensive real-world applications, a\nlarge amount of labelled data is needed to train a highly accurate models to\nperform the task. Scarcity of such volumes of cloud/sky images with\ncorresponding ground-truth binary maps makes it highly difficult to train such\ncomplex image segmentation models. In this paper, we demonstrate the\neffectiveness of using Generative Adversarial Networks (GANs) to generate data\nto augment the training set in order to increase the prediction accuracy of\nimage segmentation model. We further present a way to estimate ground-truth\nbinary maps for the GAN-generated images to facilitate their effective use as\naugmented images. Finally, we validate our work with different statistical\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 09:01:43 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Jain", "Mayank", ""], ["Meegan", "Conor", ""], ["Dev", "Soumyabrata", ""]]}, {"id": "2106.03069", "submitter": "Haocong Rao", "authors": "Haocong Rao, Shihao Xu, Xiping Hu, Jun Cheng, Bin Hu", "title": "Multi-Level Graph Encoding with Structural-Collaborative Relation\n  Learning for Skeleton-Based Person Re-Identification", "comments": "Accepted at IJCAI 2021 Main Track. Sole copyright holder is IJCAI.\n  Codes are available at https://github.com/Kali-Hac/MG-SCR", "journal-ref": "In IJCAI, 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based person re-identification (Re-ID) is an emerging open topic\nproviding great value for safety-critical applications. Existing methods\ntypically extract hand-crafted features or model skeleton dynamics from the\ntrajectory of body joints, while they rarely explore valuable relation\ninformation contained in body structure or motion. To fully explore body\nrelations, we construct graphs to model human skeletons from different levels,\nand for the first time propose a Multi-level Graph encoding approach with\nStructural-Collaborative Relation learning (MG-SCR) to encode discriminative\ngraph features for person Re-ID. Specifically, considering that\nstructurally-connected body components are highly correlated in a skeleton, we\nfirst propose a multi-head structural relation layer to learn different\nrelations of neighbor body-component nodes in graphs, which helps aggregate key\ncorrelative features for effective node representations. Second, inspired by\nthe fact that body-component collaboration in walking usually carries\nrecognizable patterns, we propose a cross-level collaborative relation layer to\ninfer collaboration between different level components, so as to capture more\ndiscriminative skeleton graph features. Finally, to enhance graph dynamics\nencoding, we propose a novel self-supervised sparse sequential prediction task\nfor model pre-training, which facilitates encoding high-level graph semantics\nfor person Re-ID. MG-SCR outperforms state-of-the-art skeleton-based methods,\nand it achieves superior performance to many multi-modal methods that utilize\nextra RGB or depth features. Our codes are available at\nhttps://github.com/Kali-Hac/MG-SCR.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 09:09:57 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rao", "Haocong", ""], ["Xu", "Shihao", ""], ["Hu", "Xiping", ""], ["Cheng", "Jun", ""], ["Hu", "Bin", ""]]}, {"id": "2106.03087", "submitter": "Yixin Zhuang", "authors": "Yixin Zhuang and Yunzhe Liu and Baoquan Chen", "title": "Neural Implicit 3D Shapes from Single Images with Spatial Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D shape reconstruction from a single image has been a long-standing problem\nin computer vision. The problem is ill-posed and highly challenging due to the\ninformation loss and occlusion that occurred during the imagery capture. In\ncontrast to previous methods that learn holistic shape priors, we propose a\nmethod to learn spatial pattern priors for inferring the invisible regions of\nthe underlying shape, wherein each 3D sample in the implicit shape\nrepresentation is associated with a set of points generated by hand-crafted 3D\nmappings, along with their local image features. The proposed spatial pattern\nis significantly more informative and has distinctive descriptions on both\nvisible and occluded locations. Most importantly, the key to our work is the\nubiquitousness of the spatial patterns across shapes, which enables reasoning\ninvisible parts of the underlying objects and thus greatly mitigates the\nocclusion issue. We devise a neural network that integrates spatial pattern\nrepresentations and demonstrate the superiority of the proposed method on\nwidely used metrics.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 10:35:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhuang", "Yixin", ""], ["Liu", "Yunzhe", ""], ["Chen", "Baoquan", ""]]}, {"id": "2106.03088", "submitter": "Bingchen Zhao", "authors": "Siwei Yang, Shaozuo Yu, Bingchen Zhao, Yin Wang", "title": "Reducing the feature divergence of RGB and near-infrared images using\n  Switchable Normalization", "comments": "CVPR2020 AgriVision workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual pattern recognition over agricultural areas is an important\napplication of aerial image processing. In this paper, we consider the\nmulti-modality nature of agricultural aerial images and show that naively\ncombining different modalities together without taking the feature divergence\ninto account can lead to sub-optimal results. Thus, we apply a Switchable\nNormalization block to our DeepLabV3 segmentation model to alleviate the\nfeature divergence. Using the popular symmetric Kullback Leibler divergence\nmeasure, we show that our model can greatly reduce the divergence between RGB\nand near-infrared channels. Together with a hybrid loss function, our model\nachieves nearly 10\\% improvements in mean IoU over previously published\nbaseline.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 10:48:59 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yang", "Siwei", ""], ["Yu", "Shaozuo", ""], ["Zhao", "Bingchen", ""], ["Wang", "Yin", ""]]}, {"id": "2106.03089", "submitter": "Muchen Li", "authors": "Muchen Li, Leonid Sigal", "title": "Referring Transformer: A One-step Approach to Multi-task Visual\n  Grounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important step towards visual reasoning, visual grounding (e.g., phrase\nlocalization, referring expression comprehension/segmentation) has been widely\nexplored Previous approaches to referring expression comprehension (REC) or\nsegmentation (RES) either suffer from limited performance, due to a two-stage\nsetup, or require the designing of complex task-specific one-stage\narchitectures. In this paper, we propose a simple one-stage multi-task\nframework for visual grounding tasks. Specifically, we leverage a transformer\narchitecture, where two modalities are fused in a visual-lingual encoder. In\nthe decoder, the model learns to generate contextualized lingual queries which\nare then decoded and used to directly regress the bounding box and produce a\nsegmentation mask for the corresponding referred regions. With this simple but\nhighly contextualized model, we outperform state-of-the-arts methods by a large\nmargin on both REC and RES tasks. We also show that a simple pre-training\nschedule (on an external dataset) further improves the performance. Extensive\nexperiments and ablations illustrate that our model benefits greatly from\ncontextualized information and multi-task training.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 10:53:39 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 12:22:08 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Li", "Muchen", ""], ["Sigal", "Leonid", ""]]}, {"id": "2106.03090", "submitter": "Sunghwan Hong", "authors": "Sunghwan Hong, Seungryong Kim", "title": "Deep Matching Prior: Test-Time Optimization for Dense Correspondence", "comments": "19 pages, 19 figures. The code will be made available at\n  https://github.com/SunghwanHong/Deep-Matching-Prior", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional techniques to establish dense correspondences across visually or\nsemantically similar images focused on designing a task-specific matching\nprior, which is difficult to model. To overcome this, recent learning-based\nmethods have attempted to learn a good matching prior within a model itself on\nlarge training data. The performance improvement was apparent, but the need for\nsufficient training data and intensive learning hinders their applicability.\nMoreover, using the fixed model at test time does not account for the fact that\na pair of images may require their own prior, thus providing limited\nperformance and poor generalization to unseen images. In this paper, we show\nthat an image pair-specific prior can be captured by solely optimizing the\nuntrained matching networks on an input pair of images. Tailored for such\ntest-time optimization for dense correspondence, we present a residual matching\nnetwork and a confidence-aware contrastive loss to guarantee a meaningful\nconvergence. Experiments demonstrate that our framework, dubbed Deep Matching\nPrior (DMP), is competitive, or even outperforms, against the latest\nlearning-based methods on several benchmarks for geometric matching and\nsemantic matching, even though it requires neither large training data nor\nintensive learning. With the networks pre-trained, DMP attains state-of-the-art\nperformance on all benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 10:56:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Hong", "Sunghwan", ""], ["Kim", "Seungryong", ""]]}, {"id": "2106.03097", "submitter": "Gihun Lee", "authors": "Gihun Lee, Yongjin Shin, Minchan Jeong, Se-Young Yun", "title": "Preservation of the Global Knowledge by Not-True Self Knowledge\n  Distillation in Federated Learning", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Federated Learning (FL), a strong global model is collaboratively learned\nby aggregating the clients' locally trained models. Although this allows no\nneed to access clients' data directly, the global model's convergence often\nsuffers from data heterogeneity. This paper suggests that forgetting could be\nthe bottleneck of global convergence. We observe that fitting on biased local\ndistribution shifts the feature on global distribution and results in\nforgetting of global knowledge. We consider this phenomenon as an analogy to\nContinual Learning, which also faces catastrophic forgetting when fitted on the\nnew task distribution. Based on our findings, we hypothesize that tackling down\nthe forgetting in local training relives the data heterogeneity problem. To\nthis end, we propose a simple yet effective framework Federated Local\nSelf-Distillation (FedLSD), which utilizes the global knowledge on locally\navailable data. By following the global perspective on local data, FedLSD\nencourages the learned features to preserve global knowledge and have\nconsistent views across local models, thus improving convergence without\ncompromising data privacy. Under our framework, we further extend FedLSD to\nFedLS-NTD, which only considers the not-true class signals to compensate noisy\nprediction of the global model. We validate that both FedLSD and FedLS-NTD\nsignificantly improve the performance in standard FL benchmarks in various\nsetups, especially in the extreme data heterogeneity cases.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 11:51:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lee", "Gihun", ""], ["Shin", "Yongjin", ""], ["Jeong", "Minchan", ""], ["Yun", "Se-Young", ""]]}, {"id": "2106.03106", "submitter": "Zhendong Wang", "authors": "Zhendong Wang, Xiaodong Cun, Jianmin Bao, Jianzhuang Liu", "title": "Uformer: A General U-Shaped Transformer for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Uformer, an effective and efficient\nTransformer-based architecture, in which we build a hierarchical\nencoder-decoder network using the Transformer block for image restoration.\nUformer has two core designs to make it suitable for this task. The first key\nelement is a local-enhanced window Transformer block, where we use\nnon-overlapping window-based self-attention to reduce the computational\nrequirement and employ the depth-wise convolution in the feed-forward network\nto further improve its potential for capturing local context. The second key\nelement is that we explore three skip-connection schemes to effectively deliver\ninformation from the encoder to the decoder. Powered by these two designs,\nUformer enjoys a high capability for capturing useful dependencies for image\nrestoration. Extensive experiments on several image restoration tasks\ndemonstrate the superiority of Uformer, including image denoising, deraining,\ndeblurring and demoireing. We expect that our work will encourage further\nresearch to explore Transformer-based architectures for low-level vision tasks.\nThe code and models will be available at\nhttps://github.com/ZhendongWang6/Uformer.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 12:33:22 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Zhendong", ""], ["Cun", "Xiaodong", ""], ["Bao", "Jianmin", ""], ["Liu", "Jianzhuang", ""]]}, {"id": "2106.03110", "submitter": "Xiong Zhou", "authors": "Xiong Zhou, Xianming Liu, Junjun Jiang, Xin Gao, Xiangyang Ji", "title": "Asymmetric Loss Functions for Learning with Noisy Labels", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robust loss functions are essential for training deep neural networks with\nbetter generalization power in the presence of noisy labels. Symmetric loss\nfunctions are confirmed to be robust to label noise. However, the symmetric\ncondition is overly restrictive. In this work, we propose a new class of loss\nfunctions, namely \\textit{asymmetric loss functions}, which are robust to\nlearning with noisy labels for various types of noise. We investigate general\ntheoretical properties of asymmetric loss functions, including classification\ncalibration, excess risk bound, and noise tolerance. Meanwhile, we introduce\nthe asymmetry ratio to measure the asymmetry of a loss function. The empirical\nresults show that a higher ratio would provide better noise tolerance.\nMoreover, we modify several commonly-used loss functions and establish the\nnecessary and sufficient conditions for them to be asymmetric. Experimental\nresults on benchmark datasets demonstrate that asymmetric loss functions can\noutperform state-of-the-art methods. The code is available at\n\\href{https://github.com/hitcszx/ALFs}{https://github.com/hitcszx/ALFs}\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 12:52:48 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhou", "Xiong", ""], ["Liu", "Xianming", ""], ["Jiang", "Junjun", ""], ["Gao", "Xin", ""], ["Ji", "Xiangyang", ""]]}, {"id": "2106.03112", "submitter": "Li Yang", "authors": "Yang Li, Hong Zhang, Yu Zhang", "title": "Rethinking Training from Scratch for Object Detection", "comments": "tech reports", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ImageNet pre-training initialization is the de-facto standard for object\ndetection. He et al. found it is possible to train detector from scratch(random\ninitialization) while needing a longer training schedule with proper\nnormalization technique. In this paper, we explore to directly pre-training on\ntarget dataset for object detection. Under this situation, we discover that the\nwidely adopted large resizing strategy e.g. resize image to (1333, 800) is\nimportant for fine-tuning but it's not necessary for pre-training.\nSpecifically, we propose a new training pipeline for object detection that\nfollows `pre-training and fine-tuning', utilizing low resolution images within\ntarget dataset to pre-training detector then load it to fine-tuning with high\nresolution images. With this strategy, we can use batch normalization(BN) with\nlarge bath size during pre-training, it's also memory efficient that we can\napply it on machine with very limited GPU memory(11G). We call it direct\ndetection pre-training, and also use direct pre-training for short. Experiment\nresults show that direct pre-training accelerates the pre-training phase by\nmore than 11x on COCO dataset while with even +1.8mAP compared to ImageNet\npre-training. Besides, we found direct pre-training is also applicable to\ntransformer based backbones e.g. Swin Transformer. Code will be available.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 13:05:57 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Yang", ""], ["Zhang", "Hong", ""], ["Zhang", "Yu", ""]]}, {"id": "2106.03121", "submitter": "Ananay Agarwal", "authors": "Ananye Agarwal, Pradeep Shenoy, Mausam", "title": "End-to-End Neuro-Symbolic Architecture for Image-to-Image Reasoning\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural models and symbolic algorithms have recently been combined for tasks\nrequiring both perception and reasoning. Neural models ground perceptual input\ninto a conceptual vocabulary, on which a classical reasoning algorithm is\napplied to generate output. A key limitation is that such neural-to-symbolic\nmodels can only be trained end-to-end for tasks where the output space is\nsymbolic. In this paper, we study neural-symbolic-neural models for reasoning\ntasks that require a conversion from an image input (e.g., a partially filled\nsudoku) to an image output (e.g., the image of the completed sudoku). While\ndesigning such a three-step hybrid architecture may be straightforward, the key\ntechnical challenge is end-to-end training -- how to backpropagate without\nintermediate supervision through the symbolic component. We propose NSNnet, an\narchitecture that combines an image reconstruction loss with a novel output\nencoder to generate a supervisory signal, develops update algorithms that\nleverage policy gradient methods for supervision, and optimizes loss using a\nnovel subsampling heuristic. We experiment on problem settings where symbolic\nalgorithms are easily specified: a visual maze solving task and a visual Sudoku\nsolver where the supervision is in image form. Experiments show high accuracy\nwith significantly less data compared to purely neural approaches.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 13:27:33 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Agarwal", "Ananye", ""], ["Shenoy", "Pradeep", ""], ["Mausam", "", ""]]}, {"id": "2106.03128", "submitter": "Tao Ma", "authors": "Tao Ma, Yikang Li", "title": "MOC-GAN: Mixing Objects and Captions to Generate Realistic Images", "comments": "9 pages, 3 figures, submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating images with conditional descriptions gains increasing interests in\nrecent years. However, existing conditional inputs are suffering from either\nunstructured forms (captions) or limited information and expensive labeling\n(scene graphs). For a targeted scene, the core items, objects, are usually\ndefinite while their interactions are flexible and hard to clearly define.\nThus, we introduce a more rational setting, generating a realistic image from\nthe objects and captions. Under this setting, objects explicitly define the\ncritical roles in the targeted images and captions implicitly describe their\nrich attributes and connections. Correspondingly, a MOC-GAN is proposed to mix\nthe inputs of two modalities to generate realistic images. It firstly infers\nthe implicit relations between object pairs from the captions to build a\nhidden-state scene graph. So a multi-layer representation containing objects,\nrelations and captions is constructed, where the scene graph provides the\nstructures of the scene and the caption provides the image-level guidance. Then\na cascaded attentive generative network is designed to coarse-to-fine generate\nphrase patch by paying attention to the most relevant words in the caption. In\naddition, a phrase-wise DAMSM is proposed to better supervise the fine-grained\nphrase-patch consistency. On COCO dataset, our method outperforms the\nstate-of-the-art methods on both Inception Score and FID while maintaining high\nvisual quality. Extensive experiments demonstrate the unique features of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 14:04:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ma", "Tao", ""], ["Li", "Yikang", ""]]}, {"id": "2106.03135", "submitter": "Janis Postels", "authors": "Janis Postels, Mengya Liu, Riccardo Spezialetti, Luc Van Gool,\n  Federico Tombari", "title": "Go with the Flows: Mixtures of Normalizing Flows for Point Cloud\n  Generation and Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently normalizing flows (NFs) have demonstrated state-of-the-art\nperformance on modeling 3D point clouds while allowing sampling with arbitrary\nresolution at inference time. However, these flow-based models still require\nlong training times and large models for representing complicated geometries.\nThis work enhances their representational power by applying mixtures of NFs to\npoint clouds. We show that in this more general framework each component learns\nto specialize in a particular subregion of an object in a completely\nunsupervised fashion. By instantiating each mixture component with a\ncomparatively small NF we generate point clouds with improved details compared\nto single-flow-based models while using fewer parameters and considerably\nreducing the inference runtime. We further demonstrate that by adding data\naugmentation, individual mixture components can learn to specialize in a\nsemantically meaningful manner. We evaluate mixtures of NFs on generation,\nautoencoding and single-view reconstruction based on the ShapeNet dataset.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 14:25:45 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 12:04:02 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Postels", "Janis", ""], ["Liu", "Mengya", ""], ["Spezialetti", "Riccardo", ""], ["Van Gool", "Luc", ""], ["Tombari", "Federico", ""]]}, {"id": "2106.03136", "submitter": "Ravi Shekhar Tiwari Mr.", "authors": "Ravi Shekhar Tiwari, Supraja P, Rijo Jackson Tom", "title": "3D Convolution Neural Network based Person Identification using Gait\n  cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Human identification plays a prominent role in terms of security. In modern\ntimes security is becoming the key term for an individual or a country,\nespecially for countries which are facing internal or external threats. Gait\nanalysis is interpreted as the systematic study of the locomotive in humans. It\ncan be used to extract the exact walking features of individuals. Walking\nfeatures depends on biological as well as the physical feature of the object;\nhence, it is unique to every individual. In this work, gait features are used\nto identify an individual. The steps involve object detection, background\nsubtraction, silhouettes extraction, skeletonization, and training 3D\nConvolution Neural Network on these gait features. The model is trained and\nevaluated on the dataset acquired by CASIA B Gait, which consists of 15000\nvideos of 124 subjects walking pattern captured from 11 different angles\ncarrying objects such as bag and coat. The proposed method focuses more on the\nlower body part to extract features such as the angle between knee and thighs,\nhip angle, angle of contact, and many other features. The experimental results\nare compared with amongst accuracies of silhouettes as datasets for training\nand skeletonized image as training data. The results show that extracting the\ninformation from skeletonized data yields improved accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 14:27:06 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tiwari", "Ravi Shekhar", ""], ["P", "Supraja", ""], ["Tom", "Rijo Jackson", ""]]}, {"id": "2106.03143", "submitter": "Tatiana Likhomanenko", "authors": "Tatiana Likhomanenko, Qiantong Xu, Ronan Collobert, Gabriel Synnaeve,\n  Alex Rogozhnikov", "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without positional information, attention-based transformer neural networks\nare permutation-invariant. Absolute or relative positional embeddings are the\nmost popular ways to feed transformer models positional information. Absolute\npositional embeddings are simple to implement, but suffer from generalization\nissues when evaluating on sequences of different length than those seen at\ntraining time. Relative positions are more robust to length change, but are\nmore complex to implement and yield inferior model throughput. In this paper,\nwe propose an augmentation-based approach (CAPE) for absolute positional\nembeddings, which keeps the advantages of both absolute (simplicity and speed)\nand relative position embeddings (better generalization). In addition, our\nempirical evaluation on state-of-the-art models in machine translation, image\nand speech recognition demonstrates that CAPE leads to better generalization\nperformance as well as increased stability with respect to training\nhyper-parameters.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 14:54:55 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 02:42:35 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Likhomanenko", "Tatiana", ""], ["Xu", "Qiantong", ""], ["Collobert", "Ronan", ""], ["Synnaeve", "Gabriel", ""], ["Rogozhnikov", "Alex", ""]]}, {"id": "2106.03146", "submitter": "Mingyuan Mao", "authors": "Teli Ma, Mingyuan Mao, Honghui Zheng, Peng Gao, Xiaodi Wang, Shumin\n  Han, Errui Ding, Baochang Zhang, David Doermann", "title": "Oriented Object Detection with Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection with Transformers (DETR) has achieved a competitive\nperformance over traditional detectors, such as Faster R-CNN. However, the\npotential of DETR remains largely unexplored for the more challenging task of\narbitrary-oriented object detection problem. We provide the first attempt and\nimplement Oriented Object DEtection with TRansformer ($\\bf O^2DETR$) based on\nan end-to-end network. The contributions of $\\rm O^2DETR$ include: 1) we\nprovide a new insight into oriented object detection, by applying Transformer\nto directly and efficiently localize objects without a tedious process of\nrotated anchors as in conventional detectors; 2) we design a simple but highly\nefficient encoder for Transformer by replacing the attention mechanism with\ndepthwise separable convolution, which can significantly reduce the memory and\ncomputational cost of using multi-scale features in the original Transformer;\n3) our $\\rm O^2DETR$ can be another new benchmark in the field of oriented\nobject detection, which achieves up to 3.85 mAP improvement over Faster R-CNN\nand RetinaNet. We simply fine-tune the head mounted on $\\rm O^2DETR$ in a\ncascaded architecture and achieve a competitive performance over SOTA in the\nDOTA dataset.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 14:57:17 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ma", "Teli", ""], ["Mao", "Mingyuan", ""], ["Zheng", "Honghui", ""], ["Gao", "Peng", ""], ["Wang", "Xiaodi", ""], ["Han", "Shumin", ""], ["Ding", "Errui", ""], ["Zhang", "Baochang", ""], ["Doermann", "David", ""]]}, {"id": "2106.03149", "submitter": "Shang-Hua Gao", "authors": "Shang-Hua Gao and Zhong-Yu Li and Ming-Hsuan Yang and Ming-Ming Cheng\n  and Junwei Han and Philip Torr", "title": "Large-scale Unsupervised Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powered by the ImageNet dataset, unsupervised learning on large-scale data\nhas made significant advances for classification tasks. There are two major\nchallenges to allow such an attractive learning modality for segmentation\ntasks: i) a large-scale benchmark for assessing algorithms is missing; ii)\nunsupervised shape representation learning is difficult. We propose a new\nproblem of large-scale unsupervised semantic segmentation (LUSS) with a newly\ncreated benchmark dataset to track the research progress. Based on the ImageNet\ndataset, we propose the ImageNet-S dataset with 1.2 million training images and\n40k high-quality semantic segmentation annotations for evaluation. Our\nbenchmark has a high data diversity and a clear task objective. We also present\na simple yet effective baseline method that works surprisingly well for LUSS.\nIn addition, we benchmark related un/weakly supervised methods accordingly,\nidentifying the challenges and possible directions of LUSS.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 15:02:11 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gao", "Shang-Hua", ""], ["Li", "Zhong-Yu", ""], ["Yang", "Ming-Hsuan", ""], ["Cheng", "Ming-Ming", ""], ["Han", "Junwei", ""], ["Torr", "Philip", ""]]}, {"id": "2106.03152", "submitter": "Fadime Sener", "authors": "Fadime Sener, Dibyadip Chatterjee, Angela Yao", "title": "Technical Report: Temporal Aggregate Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report extends our work presented in [9] with more\nexperiments. In [9], we tackle long-term video understanding, which requires\nreasoning from current and past or future observations and raises several\nfundamental questions. How should temporal or sequential relationships be\nmodelled? What temporal extent of information and context needs to be\nprocessed? At what temporal scale should they be derived? [9] addresses these\nquestions with a flexible multi-granular temporal aggregation framework. In\nthis report, we conduct further experiments with this framework on different\ntasks and a new dataset, EPIC-KITCHENS-100.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 15:27:47 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 07:11:24 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sener", "Fadime", ""], ["Chatterjee", "Dibyadip", ""], ["Yao", "Angela", ""]]}, {"id": "2106.03158", "submitter": "Fadime Sener", "authors": "Fadime Sener, Rishabh Saraf, Angela Yao", "title": "Learning Video Models from Text: Zero-Shot Anticipation for Procedural\n  Actions", "comments": "arXiv admin note: text overlap with arXiv:1812.02501", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we teach a robot to recognize and make predictions for activities that it\nhas never seen before? We tackle this problem by learning models for video from\ntext. This paper presents a hierarchical model that generalizes instructional\nknowledge from large-scale text-corpora and transfers the knowledge to video.\nGiven a portion of an instructional video, our model recognizes and predicts\ncoherent and plausible actions multiple steps into the future, all in rich\nnatural language. To demonstrate the capabilities of our model, we introduce\nthe \\emph{Tasty Videos Dataset V2}, a collection of 4022 recipes for zero-shot\nlearning, recognition and anticipation. Extensive experiments with various\nevaluation metrics demonstrate the potential of our method for generalization,\ngiven limited video data for training models.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 15:43:39 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sener", "Fadime", ""], ["Saraf", "Rishabh", ""], ["Yao", "Angela", ""]]}, {"id": "2106.03162", "submitter": "Fadime Sener", "authors": "Abhinav Rai, Fadime Sener, Angela Yao", "title": "Transformed ROIs for Capturing Visual Transformations in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the visual changes that an action brings to a scene is critical for\nvideo understanding. Currently, CNNs process one local neighbourhood at a time,\nso contextual relationships over longer ranges, while still learnable, are\nindirect. We present TROI, a plug-and-play module for CNNs to reason between\nmid-level feature representations that are otherwise separated in space and\ntime. The module relates localized visual entities such as hands and\ninteracting objects and transforms their corresponding regions of interest\ndirectly in the feature maps of convolutional layers. With TROI, we achieve\nstate-of-the-art action recognition results on the large-scale datasets\nSomething-Something-V2 and Epic-Kitchens-100.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 15:59:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rai", "Abhinav", ""], ["Sener", "Fadime", ""], ["Yao", "Angela", ""]]}, {"id": "2106.03171", "submitter": "Lei Qi", "authors": "Yue Wang, Lei Qi, Yinghuan Shi, Yang Gao", "title": "Feature-based Style Randomization for Domain Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a recent noticeable topic, domain generalization (DG) aims to first learn\na generic model on multiple source domains and then directly generalize to an\narbitrary unseen target domain without any additional adaption. In previous DG\nmodels, by generating virtual data to supplement observed source domains, the\ndata augmentation based methods have shown its effectiveness. To simulate the\npossible unseen domains, most of them enrich the diversity of original data via\nimage-level style transformation. However, we argue that the potential styles\nare hard to be exhaustively illustrated and fully augmented due to the limited\nreferred styles, leading the diversity could not be always guaranteed. Unlike\nimage-level augmentation, we in this paper develop a simple yet effective\nfeature-based style randomization module to achieve feature-level augmentation,\nwhich can produce random styles via integrating random noise into the original\nstyle. Compared with existing image-level augmentation, our feature-level\naugmentation favors a more goal-oriented and sample-diverse way. Furthermore,\nto sufficiently explore the efficacy of the proposed module, we design a novel\nprogressive training strategy to enable all parameters of the network to be\nfully trained. Extensive experiments on three standard benchmark datasets,\ni.e., PACS, VLCS and Office-Home, highlight the superiority of our method\ncompared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 16:34:44 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Yue", ""], ["Qi", "Lei", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "2106.03180", "submitter": "Yun Liu", "authors": "Yun Liu, Guolei Sun, Yu Qiu, Le Zhang, Ajad Chhatkuli, Luc Van Gool", "title": "Transformer in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the low-efficiency flaw of vision transformer caused by the high\ncomputational/space complexity in Multi-Head Self-Attention (MHSA). To this\nend, we propose the Hierarchical MHSA (H-MHSA), whose representation is\ncomputed in a hierarchical manner. Specifically, our H-MHSA first learns\nfeature relationships within small grids by viewing image patches as tokens.\nThen, small grids are merged into larger ones, within which feature\nrelationship is learned by viewing each small grid at the preceding step as a\ntoken. This process is iterated to gradually reduce the number of tokens. The\nH-MHSA module is readily pluggable into any CNN architectures and amenable to\ntraining via backpropagation. We call this new backbone TransCNN, and it\nessentially inherits the advantages of both transformer and CNN. Experiments\ndemonstrate that TransCNN achieves state-of-the-art accuracy for image\nrecognition. Code and pretrained models are available at\nhttps://github.com/yun-liu/TransCNN. This technical report will keep updating\nby adding more experiments.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 17:01:13 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 07:23:14 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Liu", "Yun", ""], ["Sun", "Guolei", ""], ["Qiu", "Yu", ""], ["Zhang", "Le", ""], ["Chhatkuli", "Ajad", ""], ["Van Gool", "Luc", ""]]}, {"id": "2106.03188", "submitter": "Ahmed Abbas", "authors": "Ahmed Abbas, Paul Swoboda", "title": "Combinatorial Optimization for Panoptic Segmentation: An End-to-End\n  Trainable Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end trainable architecture for simultaneous semantic and\ninstance segmentation (a.k.a. panoptic segmentation) consisting of a\nconvolutional neural network and an asymmetric multiway cut problem solver. The\nlatter solves a combinatorial optimization problem that elegantly incorporates\nsemantic and boundary predictions to produce a panoptic labeling. Our\nformulation allows to directly maximize a smooth surrogate of the panoptic\nquality metric by backpropagating the gradient through the optimization\nproblem. Experimental evaluation shows improvement of end-to-end learning\nw.r.t. comparable approaches on Cityscapes and COCO datasets. Overall, our\napproach shows the utility of using combinatorial optimization in tandem with\ndeep learning in a challenging large scale real-world problem and showcases\nbenefits and insights into training such an architecture end-to-end.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 17:39:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Abbas", "Ahmed", ""], ["Swoboda", "Paul", ""]]}, {"id": "2106.03210", "submitter": "Dogucan Yaman", "authors": "Dogucan Yaman and Haz{\\i}m Kemal Ekenel and Alexander Waibel", "title": "Alpha Matte Generation from Single Input for Portrait Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portrait matting is an important research problem with a wide range of\napplications, such as video conference app, image/video editing, and\npost-production. The goal is to predict an alpha matte that identifies the\neffect of each pixel on the foreground subject. Traditional approaches and most\nof the existing works utilized an additional input, e.g., trimap, background\nimage, to predict alpha matte. However, providing additional input is not\nalways practical. Besides, models are too sensitive to these additional inputs.\nIn this paper, we introduce an additional input-free approach to perform\nportrait matting using Generative Adversarial Nets (GANs). We divide the main\ntask into two subtasks. For this, we propose a segmentation network for the\nperson segmentation and the alpha generation network for alpha matte\nprediction. While the segmentation network takes an input image and produces a\ncoarse segmentation map, the alpha generation network utilizes the same input\nimage as well as a coarse segmentation map that is produced by the segmentation\nnetwork to predict the alpha matte. Besides, we present a segmentation encoding\nblock to downsample the coarse segmentation map and provide feature\nrepresentation to the residual block. Furthermore, we propose border loss to\npenalize only the borders of the subject separately which is more likely to be\nchallenging and we also adapt perceptual loss for portrait matting. To train\nthe proposed system, we combine two different popular training datasets to\nimprove the amount of data as well as diversity to address domain shift\nproblems in the inference time. We tested our model on three different\nbenchmark datasets, namely Adobe Image Matting dataset, Portrait Matting\ndataset, and Distinctions dataset. The proposed method outperformed the MODNet\nmethod that also takes a single input.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 18:53:42 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 09:36:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yaman", "Dogucan", ""], ["Ekenel", "Haz\u0131m Kemal", ""], ["Waibel", "Alexander", ""]]}, {"id": "2106.03223", "submitter": "Debesh Jha", "authors": "Rabindra Khadga, Debesh Jha, Sharib Ali, Steven Hicks, Vajira\n  Thambawita, Michael A. Riegler, and P{\\aa}l Halvorsen", "title": "Few-shot segmentation of medical images based on meta-learning with\n  implicit gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical supervised methods commonly used often suffer from the requirement\nof an abudant number of training samples and are unable to generalize on unseen\ndatasets. As a result, the broader application of any trained model is very\nlimited in clinical settings. However, few-shot approaches can minimize the\nneed for enormous reliable ground truth labels that are both labor intensive\nand expensive. To this end, we propose to exploit an optimization-based\nimplicit model agnostic meta-learning {iMAML} algorithm in a few-shot setting\nfor medical image segmentation. Our approach can leverage the learned weights\nfrom a diverse set of training samples and can be deployed on a new unseen\ndataset. We show that unlike classical few-shot learning approaches, our method\nhas improved generalization capability. To our knowledge, this is the first\nwork that exploits iMAML for medical image segmentation. Our quantitative\nresults on publicly available skin and polyp datasets show that the proposed\nmethod outperforms the naive supervised baseline model and two recent few-shot\nsegmentation approaches by large margins.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 19:52:06 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Khadga", "Rabindra", ""], ["Jha", "Debesh", ""], ["Ali", "Sharib", ""], ["Hicks", "Steven", ""], ["Thambawita", "Vajira", ""], ["Riegler", "Michael A.", ""], ["Halvorsen", "P\u00e5l", ""]]}, {"id": "2106.03225", "submitter": "Tianlong Chen", "authors": "Zhenyu Zhang, Xuxi Chen, Tianlong Chen, Zhangyang Wang", "title": "Efficient Lottery Ticket Finding: Less Data is More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lottery ticket hypothesis (LTH) reveals the existence of winning tickets\n(sparse but critical subnetworks) for dense networks, that can be trained in\nisolation from random initialization to match the latter's accuracies. However,\nfinding winning tickets requires burdensome computations in the\ntrain-prune-retrain process, especially on large-scale datasets (e.g.,\nImageNet), restricting their practical benefits. This paper explores a new\nperspective on finding lottery tickets more efficiently, by doing so only with\na specially selected subset of data, called Pruning-Aware Critical set (PrAC\nset), rather than using the full training set. The concept of PrAC set was\ninspired by the recent observation, that deep networks have samples that are\neither hard to memorize during training, or easy to forget during pruning. A\nPrAC set is thus hypothesized to capture those most challenging and informative\nexamples for the dense model. We observe that a high-quality winning ticket can\nbe found with training and pruning the dense network on the very compact PrAC\nset, which can substantially save training iterations for the ticket finding\nprocess. Extensive experiments validate our proposal across diverse datasets\nand network architectures. Specifically, on CIFAR-10, CIFAR-100, and Tiny\nImageNet, we locate effective PrAC sets at 35.32%~78.19% of their training set\nsizes. On top of them, we can obtain the same competitive winning tickets for\nthe corresponding dense networks, yet saving up to 82.85%~92.77%,\n63.54%~74.92%, and 76.14%~86.56% training iterations, respectively. Crucially,\nwe show that a PrAC set found is reusable across different network\narchitectures, which can amortize the extra cost of finding PrAC sets, yielding\na practical regime for efficient lottery ticket finding.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 19:58:17 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Zhenyu", ""], ["Chen", "Xuxi", ""], ["Chen", "Tianlong", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2106.03242", "submitter": "Elahe Arani", "authors": "Ahmed Badar, Arnav Varma, Adrian Staniec, Mahmoud Gamal, Omar Magdy,\n  Haris Iqbal, Elahe Arani and Bahram Zonooz", "title": "Highlighting the Importance of Reducing Research Bias and Carbon\n  Emissions in CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have become commonplace in addressing\nmajor challenges in computer vision. Researchers are not only coming up with\nnew CNN architectures but are also researching different techniques to improve\nthe performance of existing architectures. However, there is a tendency to\nover-emphasize performance improvement while neglecting certain important\nvariables such as simplicity, versatility, the fairness of comparisons, and\nenergy efficiency. Overlooking these variables in architectural design and\nevaluation has led to research bias and a significantly negative environmental\nimpact. Furthermore, this can undermine the positive impact of research in\nusing deep learning models to tackle climate change. Here, we perform an\nextensive and fair empirical study of a number of proposed techniques to gauge\nthe utility of each technique for segmentation and classification. Our findings\nrestate the importance of favoring simplicity over complexity in model design\n(Occam's Razor). Furthermore, our results indicate that simple standardized\npractices can lead to a significant reduction in environmental impact with\nlittle drop in performance. We highlight that there is a need to rethink the\ndesign and evaluation of CNNs to alleviate the issue of research bias and\ncarbon emissions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 20:42:00 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Badar", "Ahmed", ""], ["Varma", "Arnav", ""], ["Staniec", "Adrian", ""], ["Gamal", "Mahmoud", ""], ["Magdy", "Omar", ""], ["Iqbal", "Haris", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2106.03259", "submitter": "Ran Liu", "authors": "Ran Liu", "title": "Understand and Improve Contrastive Learning Methods for Visual\n  Representation: A Review", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional supervised learning methods are hitting a bottleneck because of\ntheir dependency on expensive manually labeled data and their weaknesses such\nas limited generalization ability and vulnerability to adversarial attacks. A\npromising alternative, self-supervised learning, as a type of unsupervised\nlearning, has gained popularity because of its potential to learn effective\ndata representations without manual labeling. Among self-supervised learning\nalgorithms, contrastive learning has achieved state-of-the-art performance in\nseveral fields of research. This literature review aims to provide an\nup-to-date analysis of the efforts of researchers to understand the key\ncomponents and the limitations of self-supervised learning.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 21:59:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Ran", ""]]}, {"id": "2106.03283", "submitter": "Zhanning Gao", "authors": "Zhanning Gao, Le Wang, Nebojsa Jojic, Zhenxing Niu, Nanning Zheng,\n  Gang Hua", "title": "Video Imprint", "comments": null, "journal-ref": "IEEE transactions on pattern analysis and machine intelligence,\n  41(12), 3086-3099 (2018)", "doi": "10.1109/TPAMI.2018.2866114", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new unified video analytics framework (ER3) is proposed for complex event\nretrieval, recognition and recounting, based on the proposed video imprint\nrepresentation, which exploits temporal correlations among image features\nacross video frames. With the video imprint representation, it is convenient to\nreverse map back to both temporal and spatial locations in video frames,\nallowing for both key frame identification and key areas localization within\neach frame. In the proposed framework, a dedicated feature alignment module is\nincorporated for redundancy removal across frames to produce the tensor\nrepresentation, i.e., the video imprint. Subsequently, the video imprint is\nindividually fed into both a reasoning network and a feature aggregation\nmodule, for event recognition/recounting and event retrieval tasks,\nrespectively. Thanks to its attention mechanism inspired by the memory networks\nused in language modeling, the proposed reasoning network is capable of\nsimultaneous event category recognition and localization of the key pieces of\nevidence for event recounting. In addition, the latent structure in our\nreasoning network highlights the areas of the video imprint, which can be\ndirectly used for event recounting. With the event retrieval task, the compact\nvideo representation aggregated from the video imprint contributes to better\nretrieval results than existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 00:32:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gao", "Zhanning", ""], ["Wang", "Le", ""], ["Jojic", "Nebojsa", ""], ["Niu", "Zhenxing", ""], ["Zheng", "Nanning", ""], ["Hua", "Gang", ""]]}, {"id": "2106.03299", "submitter": "Sukjun Hwang", "authors": "Sukjun Hwang, Miran Heo, Seoung Wug Oh, Seon Joo Kim", "title": "Video Instance Segmentation using Inter-Frame Communication Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel end-to-end solution for video instance segmentation (VIS)\nbased on transformers. Recently, the per-clip pipeline shows superior\nperformance over per-frame methods leveraging richer information from multiple\nframes. However, previous per-clip models require heavy computation and memory\nusage to achieve frame-to-frame communications, limiting practicality. In this\nwork, we propose Inter-frame Communication Transformers (IFC), which\nsignificantly reduces the overhead for information-passing between frames by\nefficiently encoding the context within the input clip. Specifically, we\npropose to utilize concise memory tokens as a mean of conveying information as\nwell as summarizing each frame scene. The features of each frame are enriched\nand correlated with other frames through exchange of information between the\nprecisely encoded memory tokens. We validate our method on the latest benchmark\nsets and achieved the state-of-the-art performance (AP 44.6 on YouTube-VIS 2019\nval set using the offline inference) while having a considerably fast runtime\n(89.4 FPS). Our method can also be applied to near-online inference for\nprocessing a video in real-time with only a small delay. The code will be made\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 02:08:39 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Hwang", "Sukjun", ""], ["Heo", "Miran", ""], ["Oh", "Seoung Wug", ""], ["Kim", "Seon Joo", ""]]}, {"id": "2106.03310", "submitter": "Zi Wang", "authors": "Zi Wang", "title": "Zero-Shot Knowledge Distillation from a Decision-Based Black-Box Model", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation (KD) is a successful approach for deep neural network\nacceleration, with which a compact network (student) is trained by mimicking\nthe softmax output of a pre-trained high-capacity network (teacher). In\ntradition, KD usually relies on access to the training samples and the\nparameters of the white-box teacher to acquire the transferred knowledge.\nHowever, these prerequisites are not always realistic due to storage costs or\nprivacy issues in real-world applications. Here we propose the concept of\ndecision-based black-box (DB3) knowledge distillation, with which the student\nis trained by distilling the knowledge from a black-box teacher (parameters are\nnot accessible) that only returns classes rather than softmax outputs. We start\nwith the scenario when the training set is accessible. We represent a sample's\nrobustness against other classes by computing its distances to the teacher's\ndecision boundaries and use it to construct the soft label for each training\nsample. After that, the student can be trained via standard KD. We then extend\nthis approach to a more challenging scenario in which even accessing the\ntraining data is not feasible. We propose to generate pseudo samples\ndistinguished by the teacher's decision boundaries to the largest extent and\nconstruct soft labels for them, which are used as the transfer set. We evaluate\nour approaches on various benchmark networks and datasets and experiment\nresults demonstrate their effectiveness. Codes are available at:\nhttps://github.com/zwang84/zsdb3kd.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 02:46:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Zi", ""]]}, {"id": "2106.03316", "submitter": "Ying Dai", "authors": "Ying Dai", "title": "Exploring to establish an appropriate model for image aesthetic\n  assessment via CNN-based RSRL: An empirical study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To establish an appropriate model for photo aesthetic assessment, in this\npaper, a D-measure which reflects the disentanglement degree of the final layer\nFC nodes of CNN is introduced. By combining F-measure with D-measure to obtain\na FD measure, an algorithm of determining the optimal model from the multiple\nphoto score prediction models generated by CNN-based repetitively self-revised\nlearning(RSRL) is proposed. Furthermore, the first fixation perspective(FFP)\nand the assessment interest region(AIR) of the models are defined and\ncalculated. The experimental results show that the FD measure is effective for\nestablishing the appropriate model from the multiple score prediction models\nwith different CNN structures. Moreover, the FD-determined optimal models with\nthe comparatively high FD always have the FFP an AIR which are close to the\nhuman's aesthetic perception when enjoying photos.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 03:20:00 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 01:37:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Dai", "Ying", ""]]}, {"id": "2106.03323", "submitter": "Jie Gui", "authors": "Jie Gui, Xiaofeng Cong, Yuan Cao, Wenqi Ren, Jun Zhang, Jing Zhang,\n  Dacheng Tao", "title": "A Comprehensive Survey on Image Dehazing Based on Deep Learning", "comments": "Paper accepted at IJCAI 2021 (Survey Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of haze significantly reduces the quality of images. Researchers\nhave designed a variety of algorithms for image dehazing (ID) to restore the\nquality of hazy images. However, there are few studies that summarize the deep\nlearning (DL) based dehazing technologies. In this paper, we conduct a\ncomprehensive survey on the recent proposed dehazing methods. Firstly, we\nsummarize the commonly used datasets, loss functions and evaluation metrics.\nSecondly, we group the existing researches of ID into two major categories:\nsupervised ID and unsupervised ID. The core ideas of various influential\ndehazing models are introduced. Finally, the open issues for future research on\nID are pointed out.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 03:51:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gui", "Jie", ""], ["Cong", "Xiaofeng", ""], ["Cao", "Yuan", ""], ["Ren", "Wenqi", ""], ["Zhang", "Jun", ""], ["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2106.03330", "submitter": "Trung-Nghia Le", "authors": "Trung-Nghia Le and Tam V. Nguyen and Minh-Triet Tran", "title": "Contextual Guided Segmentation Framework for Semi-supervised Video\n  Instance Segmentation", "comments": "Project page: https://sites.google.com/view/ltnghia/research/vos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose Contextual Guided Segmentation (CGS) framework for\nvideo instance segmentation in three passes. In the first pass, i.e., preview\nsegmentation, we propose Instance Re-Identification Flow to estimate main\nproperties of each instance (i.e., human/non-human, rigid/deformable,\nknown/unknown category) by propagating its preview mask to other frames. In the\nsecond pass, i.e., contextual segmentation, we introduce multiple contextual\nsegmentation schemes. For human instance, we develop skeleton-guided\nsegmentation in a frame along with object flow to correct and refine the result\nacross frames. For non-human instance, if the instance has a wide variation in\nappearance and belongs to known categories (which can be inferred from the\ninitial mask), we adopt instance segmentation. If the non-human instance is\nnearly rigid, we train FCNs on synthesized images from the first frame of a\nvideo sequence. In the final pass, i.e., guided segmentation, we develop a\nnovel fined-grained segmentation method on non-rectangular regions of interest\n(ROIs). The natural-shaped ROI is generated by applying guided attention from\nthe neighbor frames of the current one to reduce the ambiguity in the\nsegmentation of different overlapping instances. Forward mask propagation is\nfollowed by backward mask propagation to further restore missing instance\nfragments due to re-appeared instances, fast motion, occlusion, or heavy\ndeformation. Finally, instances in each frame are merged based on their depth\nvalues, together with human and non-human object interaction and rare instance\npriority. Experiments conducted on the DAVIS Test-Challenge dataset demonstrate\nthe effectiveness of our proposed framework. We achieved the 3rd consistently\nin the DAVIS Challenges 2017-2019 with 75.4%, 72.4%, and 78.4% in terms of\nglobal score, region similarity, and contour accuracy, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 04:16:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Le", "Trung-Nghia", ""], ["Nguyen", "Tam V.", ""], ["Tran", "Minh-Triet", ""]]}, {"id": "2106.03331", "submitter": "Peizhao Li", "authors": "Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad I. Morariu, Handong Zhao,\n  Rajiv Jain, Varun Manjunatha, Hongfu Liu", "title": "SelfDoc: Self-Supervised Document Representation Learning", "comments": "To appear in CVPR'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose SelfDoc, a task-agnostic pre-training framework for document image\nunderstanding. Because documents are multimodal and are intended for sequential\nreading, our framework exploits the positional, textual, and visual information\nof every semantically meaningful component in a document, and it models the\ncontextualization between each block of content. Unlike existing document\npre-training models, our model is coarse-grained instead of treating individual\nwords as input, therefore avoiding an overly fine-grained with excessive\ncontextualization. Beyond that, we introduce cross-modal learning in the model\npre-training phase to fully leverage multimodal information from unlabeled\ndocuments. For downstream usage, we propose a novel modality-adaptive attention\nmechanism for multimodal feature fusion by adaptively emphasizing language and\nvision signals. Our framework benefits from self-supervised pre-training on\ndocuments without requiring annotations by a feature masking training strategy.\nIt achieves superior performance on multiple downstream tasks with\nsignificantly fewer document images used in the pre-training stage compared to\nprevious works.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 04:19:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Peizhao", ""], ["Gu", "Jiuxiang", ""], ["Kuen", "Jason", ""], ["Morariu", "Vlad I.", ""], ["Zhao", "Handong", ""], ["Jain", "Rajiv", ""], ["Manjunatha", "Varun", ""], ["Liu", "Hongfu", ""]]}, {"id": "2106.03336", "submitter": "Kefan Chen", "authors": "Kefan Chen, Noah Snavely, Ameesh Makadia", "title": "Wide-Baseline Relative Camera Pose Estimation with Directional Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern deep learning techniques that regress the relative camera pose between\ntwo images have difficulty dealing with challenging scenarios, such as large\ncamera motions resulting in occlusions and significant changes in perspective\nthat leave little overlap between images. These models continue to struggle\neven with the benefit of large supervised training datasets. To address the\nlimitations of these models, we take inspiration from techniques that show\nregressing keypoint locations in 2D and 3D can be improved by estimating a\ndiscrete distribution over keypoint locations. Analogously, in this paper we\nexplore improving camera pose regression by instead predicting a discrete\ndistribution over camera poses. To realize this idea, we introduce\nDirectionNet, which estimates discrete distributions over the 5D relative pose\nspace using a novel parameterization to make the estimation problem tractable.\nSpecifically, DirectionNet factorizes relative camera pose, specified by a 3D\nrotation and a translation direction, into a set of 3D direction vectors. Since\n3D directions can be identified with points on the sphere, DirectionNet\nestimates discrete distributions on the sphere as its output. We evaluate our\nmodel on challenging synthetic and real pose estimation datasets constructed\nfrom Matterport3D and InteriorNet. Promising results show a near 50% reduction\nin error over direct regression methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 04:46:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chen", "Kefan", ""], ["Snavely", "Noah", ""], ["Makadia", "Ameesh", ""]]}, {"id": "2106.03348", "submitter": "Qiming Zhang", "authors": "Yufei Xu, Qiming Zhang, Jing Zhang, Dacheng Tao", "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias", "comments": "23 pages, adding downstream task results including detection,\n  semantic segmentation, human pose, and video object segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have shown great potential in various computer vision tasks\nowing to their strong capability in modeling long-range dependency using the\nself-attention mechanism. Nevertheless, vision transformers treat an image as\n1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in\nmodeling local visual structures and dealing with scale variance.\nAlternatively, they require large-scale training data and longer training\nschedules to learn the IB implicitly. In this paper, we propose a novel Vision\nTransformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE.\nTechnically, ViTAE has several spatial pyramid reduction modules to downsample\nand embed the input image into tokens with rich multi-scale context by using\nmultiple convolutions with different dilation rates. In this way, it acquires\nan intrinsic scale invariance IB and is able to learn robust feature\nrepresentation for objects at various scales. Moreover, in each transformer\nlayer, ViTAE has a convolution block in parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. Experiments on ImageNet\nas well as downstream tasks prove the superiority of ViTAE over the baseline\ntransformer and concurrent works. Source code and pretrained models will be\navailable at GitHub.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 05:31:06 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 15:07:18 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Xu", "Yufei", ""], ["Zhang", "Qiming", ""], ["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2106.03351", "submitter": "Matthias Perkonigg", "authors": "Matthias Perkonigg, Johannes Hofmanninger, Georg Langs", "title": "Continual Active Learning for Efficient Adaptation of Machine Learning\n  Models to Changing Image Acquisition", "comments": "Accepted for publication at the 27th international conference on\n  Information Processing in Medical Imaging (IPMI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging in clinical routine is subject to changing scanner protocols,\nhardware, or policies in a typically heterogeneous set of acquisition hardware.\nAccuracy and reliability of deep learning models suffer from those changes as\ndata and targets become inconsistent with their initial static training set.\nContinual learning can adapt to a continuous data stream of a changing imaging\nenvironment. Here, we propose a method for continual active learning on a data\nstream of medical images. It recognizes shifts or additions of new imaging\nsources - domains -, adapts training accordingly, and selects optimal examples\nfor labelling. Model training has to cope with a limited labelling budget,\nresembling typical real world scenarios. We demonstrate our method on\nT1-weighted magnetic resonance images from three different scanners with the\ntask of brain age estimation. Results demonstrate that the proposed method\noutperforms naive active learning while requiring less manual labelling.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 05:39:06 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Perkonigg", "Matthias", ""], ["Hofmanninger", "Johannes", ""], ["Langs", "Georg", ""]]}, {"id": "2106.03375", "submitter": "Xinqi Zhu", "authors": "Xinqi Zhu, Chang Xu, Dacheng Tao", "title": "Commutative Lie Group VAE for Disentanglement Learning", "comments": "Accepted in ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We view disentanglement learning as discovering an underlying structure that\nequivariantly reflects the factorized variations shown in data. Traditionally,\nsuch a structure is fixed to be a vector space with data variations represented\nby translations along individual latent dimensions. We argue this simple\nstructure is suboptimal since it requires the model to learn to discard the\nproperties (e.g. different scales of changes, different levels of abstractness)\nof data variations, which is an extra work than equivariance learning. Instead,\nwe propose to encode the data variations with groups, a structure not only can\nequivariantly represent variations, but can also be adaptively optimized to\npreserve the properties of data variations. Considering it is hard to conduct\ntraining on group structures, we focus on Lie groups and adopt a\nparameterization using Lie algebra. Based on the parameterization, some\ndisentanglement learning constraints are naturally derived. A simple model\nnamed Commutative Lie Group VAE is introduced to realize the group-based\ndisentanglement learning. Experiments show that our model can effectively learn\ndisentangled representations without supervision, and can achieve\nstate-of-the-art performance without extra constraints.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 07:03:14 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhu", "Xinqi", ""], ["Xu", "Chang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2106.03382", "submitter": "Tutian Tang", "authors": "Tutian Tang, Wenqiang Xu, Ruolin Ye, Yan-Feng Wang, Cewu Lu", "title": "ContourRender: Detecting Arbitrary Contour Shape For Instance\n  Segmentation In One Pass", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct contour regression for instance segmentation is a challenging task.\nPrevious works usually achieve it by learning to progressively refine the\ncontour prediction or adopting a shape representation with limited\nexpressiveness. In this work, we argue that the difficulty in regressing the\ncontour points in one pass is mainly due to the ambiguity when discretizing a\nsmooth contour into a polygon. To address the ambiguity, we propose a novel\ndifferentiable rendering-based approach named \\textbf{ContourRender}. During\ntraining, it first predicts a contour generated by an invertible shape\nsignature, and then optimizes the contour with the more stable silhouette by\nconverting it to a contour mesh and rendering the mesh to a 2D map.\n  This method significantly improves the quality of contour without iterations\nor cascaded refinements. Moreover, as optimization is not needed during\ninference, the inference speed will not be influenced.\n  Experiments show the proposed ContourRender outperforms all the contour-based\ninstance segmentation approaches on COCO, while stays competitive with the\niteration-based state-of-the-art on Cityscapes. In addition, we specifically\nselect a subset from COCO val2017 named COCO ContourHard-val to further\ndemonstrate the contour quality improvements. Codes, models, and dataset split\nwill be released.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 07:23:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tang", "Tutian", ""], ["Xu", "Wenqiang", ""], ["Ye", "Ruolin", ""], ["Wang", "Yan-Feng", ""], ["Lu", "Cewu", ""]]}, {"id": "2106.03388", "submitter": "Jian-Wei Zhang", "authors": "Jian-Wei Zhang, Wei Chen, K. Ina Ly, Xubin Zhang, Fan Yan, Justin\n  Jordan, Gordon Harris, Scott Plotkin, Pengyi Hao, and Wenli Cai", "title": "DINs: Deep Interactive Networks for Neurofibroma Segmentation in\n  Neurofibromatosis Type 1 on Whole-Body MRI", "comments": "Accepted by IEEE Journal of Biomedical and Health Informatics (JBHI)", "journal-ref": "IEEE Journal of Biomedical and Health Informatics, 2021", "doi": "10.1109/JBHI.2021.3087735", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurofibromatosis type 1 (NF1) is an autosomal dominant tumor predisposition\nsyndrome that involves the central and peripheral nervous systems. Accurate\ndetection and segmentation of neurofibromas are essential for assessing tumor\nburden and longitudinal tumor size changes. Automatic convolutional neural\nnetworks (CNNs) are sensitive and vulnerable as tumors' variable anatomical\nlocation and heterogeneous appearance on MRI. In this study, we propose deep\ninteractive networks (DINs) to address the above limitations. User interactions\nguide the model to recognize complicated tumors and quickly adapt to\nheterogeneous tumors. We introduce a simple but effective Exponential Distance\nTransform (ExpDT) that converts user interactions into guide maps regarded as\nthe spatial and appearance prior. Comparing with popular Euclidean and geodesic\ndistances, ExpDT is more robust to various image sizes, which reserves the\ndistribution of interactive inputs. Furthermore, to enhance the tumor-related\nfeatures, we design a deep interactive module to propagate the guides into\ndeeper layers. We train and evaluate DINs on three MRI data sets from NF1\npatients. The experiment results yield significant improvements of 44% and 14%\nin DSC comparing with automated and other interactive methods, respectively. We\nalso experimentally demonstrate the efficiency of DINs in reducing user burden\nwhen comparing with conventional interactive methods. The source code of our\nmethod is available at \\url{https://github.com/Jarvis73/DINs}.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 07:29:29 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhang", "Jian-Wei", ""], ["Chen", "Wei", ""], ["Ly", "K. Ina", ""], ["Zhang", "Xubin", ""], ["Yan", "Fan", ""], ["Jordan", "Justin", ""], ["Harris", "Gordon", ""], ["Plotkin", "Scott", ""], ["Hao", "Pengyi", ""], ["Cai", "Wenli", ""]]}, {"id": "2106.03412", "submitter": "Silvia-Laura Pintea", "authors": "Silvia L.Pintea and Nergis Tomen and Stanley F. Goes and Marco Loog\n  and Jan C. van Gemert", "title": "Resolution learning in deep convolutional networks using scale-space\n  theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Resolution in deep convolutional neural networks (CNNs) is typically bounded\nby the receptive field size through filter sizes, and subsampling layers or\nstrided convolutions on feature maps. The optimal resolution may vary\nsignificantly depending on the dataset. Modern CNNs hard-code their resolution\nhyper-parameters in the network architecture which makes tuning such\nhyper-parameters cumbersome. We propose to do away with hard-coded resolution\nhyper-parameters and aim to learn the appropriate resolution from data. We use\nscale-space theory to obtain a self-similar parametrization of filters and make\nuse of the N-Jet: a truncated Taylor series to approximate a filter by a\nlearned combination of Gaussian derivative filters. The parameter sigma of the\nGaussian basis controls both the amount of detail the filter encodes and the\nspatial extent of the filter. Since sigma is a continuous parameter, we can\noptimize it with respect to the loss. The proposed N-Jet layer achieves\ncomparable performance when used in state-of-the art architectures, while\nlearning the correct resolution in each layer automatically. We evaluate our\nN-Jet layer on both classification and segmentation, and we show that learning\nsigma is especially beneficial for inputs at multiple sizes.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:23:02 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:08:16 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Pintea", "Silvia L.", ""], ["Tomen", "Nergis", ""], ["Goes", "Stanley F.", ""], ["Loog", "Marco", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2106.03418", "submitter": "Takashi Isobe", "authors": "Takashi Isobe, Xu Jia, Shuaijun Chen, Jianzhong He, Yongjie Shi,\n  Jianzhuang Liu, Huchuan Lu, Shengjin Wang", "title": "Multi-Target Domain Adaptation with Collaborative Consistency Learning", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently unsupervised domain adaptation for the semantic segmentation task\nhas become more and more popular due to high-cost of pixel-level annotation on\nreal-world images. However, most domain adaptation methods are only restricted\nto single-source-single-target pair, and can not be directly extended to\nmultiple target domains. In this work, we propose a collaborative learning\nframework to achieve unsupervised multi-target domain adaptation. An\nunsupervised domain adaptation expert model is first trained for each\nsource-target pair and is further encouraged to collaborate with each other\nthrough a bridge built between different target domains. These expert models\nare further improved by adding the regularization of making the consistent\npixel-wise prediction for each sample with the same structured context. To\nobtain a single model that works across multiple target domains, we propose to\nsimultaneously learn a student model which is trained to not only imitate the\noutput of each expert on the corresponding target domain, but also to pull\ndifferent expert close to each other with regularization on their weights.\nExtensive experiments demonstrate that the proposed method can effectively\nexploit rich structured information contained in both labeled source domain and\nmultiple unlabeled target domains. Not only does it perform well across\nmultiple target domains but also performs favorably against state-of-the-art\nunsupervised domain adaptation methods specially trained on a single\nsource-target pair\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:36:20 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Isobe", "Takashi", ""], ["Jia", "Xu", ""], ["Chen", "Shuaijun", ""], ["He", "Jianzhong", ""], ["Shi", "Yongjie", ""], ["Liu", "Jianzhuang", ""], ["Lu", "Huchuan", ""], ["Wang", "Shengjin", ""]]}, {"id": "2106.03422", "submitter": "Yuyang Zhao", "authors": "Yuyang Zhao, Zhun Zhong, Zhiming Luo, Gim Hee Lee, Nicu Sebe", "title": "Source-Free Open Compound Domain Adaptation in Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a new concept, named source-free open compound\ndomain adaptation (SF-OCDA), and study it in semantic segmentation. SF-OCDA is\nmore challenging than the traditional domain adaptation but it is more\npractical. It jointly considers (1) the issues of data privacy and data storage\nand (2) the scenario of multiple target domains and unseen open domains. In\nSF-OCDA, only the source pre-trained model and the target data are available to\nlearn the target model. The model is evaluated on the samples from the target\nand unseen open domains. To solve this problem, we present an effective\nframework by separating the training process into two stages: (1) pre-training\na generalized source model and (2) adapting a target model with self-supervised\nlearning. In our framework, we propose the Cross-Patch Style Swap (CPSS) to\ndiversify samples with various patch styles in the feature-level, which can\nbenefit the training of both stages. First, CPSS can significantly improve the\ngeneralization ability of the source model, providing more accurate\npseudo-labels for the latter stage. Second, CPSS can reduce the influence of\nnoisy pseudo-labels and also avoid the model overfitting to the target domain\nduring self-supervised learning, consistently boosting the performance on the\ntarget and open domains. Experiments demonstrate that our method produces\nstate-of-the-art results on the C-Driving dataset. Furthermore, our model also\nachieves the leading performance on CityScapes for domain generalization.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:38:41 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhao", "Yuyang", ""], ["Zhong", "Zhun", ""], ["Luo", "Zhiming", ""], ["Lee", "Gim Hee", ""], ["Sebe", "Nicu", ""]]}, {"id": "2106.03432", "submitter": "YiFeng Ding", "authors": "Yifeng Ding, Shuwei Dong, Yujun Tong, Zhanyu Ma, Bo Xiao, and Haibin\n  Ling", "title": "Channel DropBlock: An Improved Regularization Method for Fine-Grained\n  Visual Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying the sub-categories of an object from the same super-category\n(e.g., bird) in a fine-grained visual classification (FGVC) task highly relies\non mining multiple discriminative features. Existing approaches mainly tackle\nthis problem by introducing attention mechanisms to locate the discriminative\nparts or feature encoding approaches to extract the highly parameterized\nfeatures in a weakly-supervised fashion. In this work, we propose a lightweight\nyet effective regularization method named Channel DropBlock (CDB), in\ncombination with two alternative correlation metrics, to address this problem.\nThe key idea is to randomly mask out a group of correlated channels during\ntraining to destruct features from co-adaptations and thus enhance feature\nrepresentations. Extensive experiments on three benchmark FGVC datasets show\nthat CDB effectively improves the performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:03:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ding", "Yifeng", ""], ["Dong", "Shuwei", ""], ["Tong", "Yujun", ""], ["Ma", "Zhanyu", ""], ["Xiao", "Bo", ""], ["Ling", "Haibin", ""]]}, {"id": "2106.03437", "submitter": "Kaizhi Yang", "authors": "Kaizhi Yang and Xuejin Chen", "title": "Unsupervised Learning for Cuboid Shape Abstraction via Joint\n  Segmentation from Point Clouds", "comments": "11 pages", "journal-ref": null, "doi": "10.1145/3450626.3459873", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing complex 3D objects as simple geometric primitives, known as\nshape abstraction, is important for geometric modeling, structural analysis,\nand shape synthesis. In this paper, we propose an unsupervised shape\nabstraction method to map a point cloud into a compact cuboid representation.\nWe jointly predict cuboid allocation as part segmentation and cuboid shapes and\nenforce the consistency between the segmentation and shape abstraction for\nself-learning. For the cuboid abstraction task, we transform the input point\ncloud into a set of parametric cuboids using a variational auto-encoder\nnetwork. The segmentation network allocates each point into a cuboid\nconsidering the point-cuboid affinity. Without manual annotations of parts in\npoint clouds, we design four novel losses to jointly supervise the two branches\nin terms of geometric similarity and cuboid compactness. We evaluate our method\non multiple shape collections and demonstrate its superiority over existing\nshape abstraction methods. Moreover, based on our network architecture and\nlearned representations, our approach supports various applications including\nstructured shape generation, shape interpolation, and structural shape\nclustering.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:15:16 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yang", "Kaizhi", ""], ["Chen", "Xuejin", ""]]}, {"id": "2106.03450", "submitter": "Kuikun Liu", "authors": "Kuikun Liu, Jie Yang, Cai Sun, Haoyuan Chi", "title": "supervised adptive threshold network for instance segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, instance segmentation is attracting more and more attention in\nmachine learning region. However, there exists some defects on the information\npropagation in previous Mask R-CNN and other network models. In this paper, we\npropose supervised adaptive threshold network for instance segmentation.\nSpecifically, we adopt the Mask R-CNN method based on adaptive threshold, and\nby establishing a layered adaptive network structure, it performs adaptive\nbinarization on the probability graph generated by Mask RCNN to obtain better\nsegmentation effect and reduce the error rate. At the same time, an adaptive\nfeature pool is designed to make the transmission between different layers of\nthe network more accurate and effective, reduce the loss in the process of\nfeature transmission, and further improve the mask method. Experiments on\nbenchmark data sets indicate that the effectiveness of the proposed model\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:25:44 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Kuikun", ""], ["Yang", "Jie", ""], ["Sun", "Cai", ""], ["Chi", "Haoyuan", ""]]}, {"id": "2106.03452", "submitter": "Songyou Peng", "authors": "Songyou Peng, Chiyu \"Max\" Jiang, Yiyi Liao, Michael Niemeyer, Marc\n  Pollefeys, Andreas Geiger", "title": "Shape As Points: A Differentiable Poisson Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural implicit representations gained popularity in 3D\nreconstruction due to their expressiveness and flexibility. However, the\nimplicit nature of neural implicit representations results in slow inference\ntime and requires careful initialization. In this paper, we revisit the classic\nyet ubiquitous point cloud representation and introduce a differentiable\npoint-to-mesh layer using a differentiable formulation of Poisson Surface\nReconstruction (PSR) that allows for a GPU-accelerated fast solution of the\nindicator function given an oriented point cloud. The differentiable PSR layer\nallows us to efficiently and differentiably bridge the explicit 3D point\nrepresentation with the 3D mesh via the implicit indicator field, enabling\nend-to-end optimization of surface reconstruction metrics such as Chamfer\ndistance. This duality between points and meshes hence allows us to represent\nshapes as oriented point clouds, which are explicit, lightweight and\nexpressive. Compared to neural implicit representations, our Shape-As-Points\n(SAP) model is more interpretable, lightweight, and accelerates inference time\nby one order of magnitude. Compared to other explicit representations such as\npoints, patches, and meshes, SAP produces topology-agnostic, watertight\nmanifold surfaces. We demonstrate the effectiveness of SAP on the task of\nsurface reconstruction from unoriented point clouds and learning-based\nreconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:28:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Peng", "Songyou", ""], ["Jiang", "Chiyu \"Max\"", ""], ["Liao", "Yiyi", ""], ["Niemeyer", "Michael", ""], ["Pollefeys", "Marc", ""], ["Geiger", "Andreas", ""]]}, {"id": "2106.03455", "submitter": "Henghui Ding", "authors": "Xiaohong Wang, Xudong Jiang, Henghui Ding, Yuqian Zhao, Jun Liu", "title": "Knowledge-aware Deep Framework for Collaborative Skin Lesion\n  Segmentation and Melanoma Recognition", "comments": "Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2021.108075", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have shown their superior performance in\ndermatologist clinical inspection. Nevertheless, melanoma diagnosis is still a\nchallenging task due to the difficulty of incorporating the useful\ndermatologist clinical knowledge into the learning process. In this paper, we\npropose a novel knowledge-aware deep framework that incorporates some clinical\nknowledge into collaborative learning of two important melanoma diagnosis\ntasks, i.e., skin lesion segmentation and melanoma recognition. Specifically,\nto exploit the knowledge of morphological expressions of the lesion region and\nalso the periphery region for melanoma identification, a lesion-based pooling\nand shape extraction (LPSE) scheme is designed, which transfers the structure\ninformation obtained from skin lesion segmentation into melanoma recognition.\nMeanwhile, to pass the skin lesion diagnosis knowledge from melanoma\nrecognition to skin lesion segmentation, an effective diagnosis guided feature\nfusion (DGFF) strategy is designed. Moreover, we propose a recursive mutual\nlearning mechanism that further promotes the inter-task cooperation, and thus\niteratively improves the joint learning capability of the model for both skin\nlesion segmentation and melanoma recognition. Experimental results on two\npublicly available skin lesion datasets show the effectiveness of the proposed\nmethod for melanoma analysis.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:33:45 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Xiaohong", ""], ["Jiang", "Xudong", ""], ["Ding", "Henghui", ""], ["Zhao", "Yuqian", ""], ["Liu", "Jun", ""]]}, {"id": "2106.03479", "submitter": "Hao Xu", "authors": "Hao Xu, Nianjin Ye, Shuaicheng Liu, Guanghui Liu, Bing Zeng", "title": "FINet: Dual Branches Feature Interaction for Partial-to-Partial Point\n  Cloud Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data association is important in the point cloud registration. In this work,\nwe propose to solve the partial-to-partial registration from a new perspective,\nby introducing feature interactions between the source and the reference clouds\nat the feature extraction stage, such that the registration can be realized\nwithout the explicit mask estimation or attentions for the overlapping\ndetection as adopted previously. Specifically, we present FINet, a feature\ninteraction-based structure with the capability to enable and strengthen the\ninformation associating between the inputs at multiple stages. To achieve this,\nwe first split the features into two components, one for the rotation and one\nfor the translation, based on the fact that they belong to different solution\nspaces, yielding a dual branches structure. Second, we insert several\ninteraction modules at the feature extractor for the data association. Third,\nwe propose a transformation sensitivity loss to obtain rotation-attentive and\ntranslation-attentive features. Experiments demonstrate that our method\nperforms higher precision and robustness compared to the state-of-the-art\ntraditional and learning-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 10:15:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xu", "Hao", ""], ["Ye", "Nianjin", ""], ["Liu", "Shuaicheng", ""], ["Liu", "Guanghui", ""], ["Zeng", "Bing", ""]]}, {"id": "2106.03485", "submitter": "Diego Doimo", "authors": "Diego Doimo, Aldo Glielmo, Sebastian Goldt, Alessandro Laio", "title": "Representation mitosis in wide neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) defy the classical bias-variance trade-off:\nadding parameters to a DNN that exactly interpolates its training data will\ntypically improve its generalisation performance. Explaining the mechanism\nbehind the benefit of such over-parameterisation is an outstanding challenge\nfor deep learning theory. Here, we study the last layer representation of\nvarious deep architectures such as Wide-ResNets for image classification and\nfind evidence for an underlying mechanism that we call *representation\nmitosis*: if the last hidden representation is wide enough, its neurons tend to\nsplit into groups which carry identical information, and differ from each other\nonly by a statistically independent noise. Like in a mitosis process, the\nnumber of such groups, or ``clones'', increases linearly with the width of the\nlayer, but only if the width is above a critical value. We show that a key\ningredient to activate mitosis is continuing the training process until the\ntraining error is zero. Finally, we show that in one of the learning tasks we\nconsidered, a wide model with several automatically developed clones performs\nsignificantly better than a deep ensemble based on architectures in which the\nlast layer has the same size as the clones.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 10:18:54 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Doimo", "Diego", ""], ["Glielmo", "Aldo", ""], ["Goldt", "Sebastian", ""], ["Laio", "Alessandro", ""]]}, {"id": "2106.03487", "submitter": "Panagiotis Antoniadis", "authors": "Panagiotis Antoniadis, Panagiotis P. Filntisis, Petros Maragos", "title": "Exploiting Emotional Dependencies with Graph Convolutional Networks for\n  Facial Expression Recognition", "comments": "9 pages, 8 figures, 3 tables, submitted to the 16th IEEE\n  International Conference on Automatic Face and Gesture Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past few years, deep learning methods have shown remarkable results\nin many face-related tasks including automatic facial expression recognition\n(FER) in-the-wild. Meanwhile, numerous models describing the human emotional\nstates have been proposed by the psychology community. However, we have no\nclear evidence as to which representation is more appropriate and the majority\nof FER systems use either the categorical or the dimensional model of affect.\nInspired by recent work in multi-label classification, this paper proposes a\nnovel multi-task learning (MTL) framework that exploits the dependencies\nbetween these two models using a Graph Convolutional Network (GCN) to recognize\nfacial expressions in-the-wild. Specifically, a shared feature representation\nis learned for both discrete and continuous recognition in a MTL setting.\nMoreover, the facial expression classifiers and the valence-arousal regressors\nare learned through a GCN that explicitly captures the dependencies between\nthem. To evaluate the performance of our method under real-world conditions we\ntrain our models on AffectNet dataset. The results of our experiments show that\nour method outperforms the current state-of-the-art methods on discrete FER.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 10:20:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Antoniadis", "Panagiotis", ""], ["Filntisis", "Panagiotis P.", ""], ["Maragos", "Petros", ""]]}, {"id": "2106.03496", "submitter": "Francesco Cappio Borlino", "authors": "F. Cappio Borlino, S. Polizzotto, A. D'Innocente, S. Bucci, B. Caputo,\n  T. Tommasi", "title": "Self-Supervision & Meta-Learning for One-Shot Unsupervised Cross-Domain\n  Detection", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.11610", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep detection models have largely demonstrated to be extremely powerful in\ncontrolled settings, but appear brittle and fail when applied off-the-shelf on\nunseen domains. All the adaptive approaches developed to amend this issue\naccess a sizable amount of target samples at training time, a strategy not\nsuitable when the target is unknown and its data are not available in advance.\nConsider for instance the task of monitoring image feeds from social media: as\nevery image is uploaded by a different user it belongs to a different target\ndomain that is impossible to foresee during training. Our work addresses this\nsetting, presenting an object detection algorithm able to perform unsupervised\nadaptation across domains by using only one target sample, seen at test time.\nWe introduce a multi-task architecture that one-shot adapts to any incoming\nsample by iteratively solving a self-supervised task on it. We further exploit\nmeta-learning to simulate single-sample cross domain learning episodes and\nbetter align to the test condition. Moreover, a cross-task pseudo-labeling\nprocedure allows to focus on the image foreground and enhances the adaptation\nprocess. A thorough benchmark analysis against the most recent cross-domain\ndetection methods and a detailed ablation study show the advantage of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 10:33:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Borlino", "F. Cappio", ""], ["Polizzotto", "S.", ""], ["D'Innocente", "A.", ""], ["Bucci", "S.", ""], ["Caputo", "B.", ""], ["Tommasi", "T.", ""]]}, {"id": "2106.03502", "submitter": "Naoya Fushishita", "authors": "Naoya Fushishita, Antonio Tejero-de-Pablos, Yusuke Mukuta, Tatsuya\n  Harada", "title": "Efficient training for future video generation based on hierarchical\n  disentangled representation of latent variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating videos predicting the future of a given sequence has been an area\nof active research in recent years. However, an essential problem remains\nunsolved: most of the methods require large computational cost and memory usage\nfor training. In this paper, we propose a novel method for generating future\nprediction videos with less memory usage than the conventional methods. This is\na critical stepping stone in the path towards generating videos with high image\nquality, similar to that of generated images in the latest works in the field\nof image generation. We achieve high-efficiency by training our method in two\nstages: (1) image reconstruction to encode video frames into latent variables,\nand (2) latent variable prediction to generate the future sequence. Our method\nachieves an efficient compression of video into low-dimensional latent\nvariables by decomposing each frame according to its hierarchical structure.\nThat is, we consider that video can be separated into background and foreground\nobjects, and that each object holds time-varying and time-independent\ninformation independently. Our experiments show that the proposed method can\nefficiently generate future prediction videos, even for complex datasets that\ncannot be handled by previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 10:43:23 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 15:22:18 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Fushishita", "Naoya", ""], ["Tejero-de-Pablos", "Antonio", ""], ["Mukuta", "Yusuke", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2106.03503", "submitter": "Tilo Strutz", "authors": "Tilo Strutz", "title": "The Distance Transform and its Computation", "comments": "24 pages, 22 figures, 1 table, 9 listings", "journal-ref": null, "doi": null, "report-no": "TECHP/2021/06", "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance transformation is an image processing technique used for many\ndifferent applications. Related to a binary image, the general idea is to\ndetermine the distance of all background points to the nearest object point (or\nvice versa). In this tutorial, different approaches are explained in detail and\ncompared using examples. Corresponding source code is provided to facilitate\nown investigations. A particular objective of this tutorial is to clarify the\ndifference between arbitrary distance transforms and exact Euclidean distance\ntransformations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 10:46:26 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Strutz", "Tilo", ""]]}, {"id": "2106.03505", "submitter": "Wei Yao", "authors": "Shaocheng Jia, Xin Pei, Wei Yao and S.C. Wong", "title": "Self-supervised Depth Estimation Leveraging Global Perception and\n  Geometric Smoothness Using On-board Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised depth estimation has drawn much attention in recent years as\nit does not require labeled data but image sequences. Moreover, it can be\nconveniently used in various applications, such as autonomous driving,\nrobotics, realistic navigation, and smart cities. However, extracting global\ncontextual information from images and predicting a geometrically natural depth\nmap remain challenging. In this paper, we present DLNet for pixel-wise depth\nestimation, which simultaneously extracts global and local features with the\naid of our depth Linformer block. This block consists of the Linformer and\ninnovative soft split multi-layer perceptron blocks. Moreover, a\nthree-dimensional geometry smoothness loss is proposed to predict a\ngeometrically natural depth map by imposing the second-order smoothness\nconstraint on the predicted three-dimensional point clouds, thereby realizing\nimproved performance as a byproduct. Finally, we explore the multi-scale\nprediction strategy and propose the maximum margin dual-scale prediction\nstrategy for further performance improvement. In experiments on the KITTI and\nMake3D benchmarks, the proposed DLNet achieves performance competitive to those\nof the state-of-the-art methods, reducing time and space complexities by more\nthan $62\\%$ and $56\\%$, respectively. Extensive testing on various real-world\nsituations further demonstrates the strong practicality and generalization\ncapability of the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 10:53:27 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Jia", "Shaocheng", ""], ["Pei", "Xin", ""], ["Yao", "Wei", ""], ["Wong", "S. C.", ""]]}, {"id": "2106.03527", "submitter": "Alexandros Kouris", "authors": "Alexandros Kouris, Stylianos I. Venieris, Stefanos Laskaridis,\n  Nicholas D. Lane", "title": "Multi-Exit Semantic Segmentation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation arises as the backbone of many vision systems, spanning\nfrom self-driving cars and robot navigation to augmented reality and\nteleconferencing. Frequently operating under stringent latency constraints\nwithin a limited resource envelope, optimising for efficient execution becomes\nimportant. To this end, we propose a framework for converting state-of-the-art\nsegmentation models to MESS networks; specially trained CNNs that employ\nparametrised early exits along their depth to save computation during inference\non easier samples. Designing and training such networks naively can hurt\nperformance. Thus, we propose a two-staged training process that pushes\nsemantically important features early in the network. We co-optimise the\nnumber, placement and architecture of the attached segmentation heads, along\nwith the exit policy, to adapt to the device capabilities and\napplication-specific requirements. Optimising for speed, MESS networks can\nachieve latency gains of up to 2.83x over state-of-the-art methods with no\naccuracy degradation. Accordingly, optimising for accuracy, we achieve an\nimprovement of up to 5.33 pp, under the same computational budget.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 11:37:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kouris", "Alexandros", ""], ["Venieris", "Stylianos I.", ""], ["Laskaridis", "Stefanos", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2106.03538", "submitter": "Marcello Carioni", "authors": "Subhadip Mukherjee, Marcello Carioni, Ozan \\\"Oktem, Carola-Bibiane\n  Sch\\\"onlieb", "title": "End-to-end reconstruction meets data-driven regularization for inverse\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised approach for learning end-to-end reconstruction\noperators for ill-posed inverse problems. The proposed method combines the\nclassical variational framework with iterative unrolling, which essentially\nseeks to minimize a weighted combination of the expected distortion in the\nmeasurement space and the Wasserstein-1 distance between the distributions of\nthe reconstruction and ground-truth. More specifically, the regularizer in the\nvariational setting is parametrized by a deep neural network and learned\nsimultaneously with the unrolled reconstruction operator. The variational\nproblem is then initialized with the reconstruction of the unrolled operator\nand solved iteratively till convergence. Notably, it takes significantly fewer\niterations to converge, thanks to the excellent initialization obtained via the\nunrolled operator. The resulting approach combines the computational efficiency\nof end-to-end unrolled reconstruction with the well-posedness and\nnoise-stability guarantees of the variational setting. Moreover, we demonstrate\nwith the example of X-ray computed tomography (CT) that our approach\noutperforms state-of-the-art unsupervised methods, and that it outperforms or\nis on par with state-of-the-art supervised learned reconstruction approaches.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 12:05:06 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mukherjee", "Subhadip", ""], ["Carioni", "Marcello", ""], ["\u00d6ktem", "Ozan", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2106.03614", "submitter": "Mo Zhou", "authors": "Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Nanning Zheng, Gang Hua", "title": "Adversarial Attack and Defense in Deep Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network classifiers are vulnerable to adversarial attack, where\nan imperceptible perturbation could result in misclassification. However, the\nvulnerability of DNN-based image ranking systems remains under-explored. In\nthis paper, we propose two attacks against deep ranking systems, i.e.,\nCandidate Attack and Query Attack, that can raise or lower the rank of chosen\ncandidates by adversarial perturbations. Specifically, the expected ranking\norder is first represented as a set of inequalities, and then a triplet-like\nobjective function is designed to obtain the optimal perturbation. Conversely,\nan anti-collapse triplet defense is proposed to improve the ranking model\nrobustness against all proposed attacks, where the model learns to prevent the\npositive and negative samples being pulled close to each other by adversarial\nattack. To comprehensively measure the empirical adversarial robustness of a\nranking model with our defense, we propose an empirical robustness score, which\ninvolves a set of representative attacks against ranking models. Our\nadversarial ranking attacks and defenses are evaluated on MNIST, Fashion-MNIST,\nCUB200-2011, CARS196 and Stanford Online Products datasets. Experimental\nresults demonstrate that a typical deep ranking system can be effectively\ncompromised by our attacks. Nevertheless, our defense can significantly improve\nthe ranking system robustness, and simultaneously mitigate a wide range of\nattacks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:41:45 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhou", "Mo", ""], ["Wang", "Le", ""], ["Niu", "Zhenxing", ""], ["Zhang", "Qilin", ""], ["Zheng", "Nanning", ""], ["Hua", "Gang", ""]]}, {"id": "2106.03630", "submitter": "Patrick Emami", "authors": "Patrick Emami, Pan He, Sanjay Ranka, Anand Rangarajan", "title": "Efficient Iterative Amortized Inference for Learning Symmetric and\n  Disentangled Multi-Object Representations", "comments": "Published in ICML'21. Code and data:\n  https://github.com/pemami4911/EfficientMORL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Unsupervised multi-object representation learning depends on inductive biases\nto guide the discovery of object-centric representations that generalize.\nHowever, we observe that methods for learning these representations are either\nimpractical due to long training times and large memory consumption or forego\nkey inductive biases. In this work, we introduce EfficientMORL, an efficient\nframework for the unsupervised learning of object-centric representations. We\nshow that optimization challenges caused by requiring both symmetry and\ndisentanglement can in fact be addressed by high-cost iterative amortized\ninference by designing the framework to minimize its dependence on it. We take\na two-stage approach to inference: first, a hierarchical variational\nautoencoder extracts symmetric and disentangled representations through\nbottom-up inference, and second, a lightweight network refines the\nrepresentations with top-down feedback. The number of refinement steps taken\nduring training is reduced following a curriculum, so that at test time with\nzero steps the model achieves 99.1% of the refined decomposition performance.\nWe demonstrate strong object decomposition and disentanglement on the standard\nmulti-object benchmark while achieving nearly an order of magnitude faster\ntraining and test time inference over the previous state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:02:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Emami", "Patrick", ""], ["He", "Pan", ""], ["Ranka", "Sanjay", ""], ["Rangarajan", "Anand", ""]]}, {"id": "2106.03640", "submitter": "Dominic Masters", "authors": "Dominic Masters, Antoine Labatie, Zach Eaton-Rosen and Carlo Luschi", "title": "Making EfficientNet More Efficient: Exploring Batch-Independent\n  Normalization, Group Convolutions and Reduced Resolution Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Much recent research has been dedicated to improving the efficiency of\ntraining and inference for image classification. This effort has commonly\nfocused on explicitly improving theoretical efficiency, often measured as\nImageNet validation accuracy per FLOP. These theoretical savings have, however,\nproven challenging to achieve in practice, particularly on high-performance\ntraining accelerators.\n  In this work, we focus on improving the practical efficiency of the\nstate-of-the-art EfficientNet models on a new class of accelerator, the\nGraphcore IPU. We do this by extending this family of models in the following\nways: (i) generalising depthwise convolutions to group convolutions; (ii)\nadding proxy-normalized activations to match batch normalization performance\nwith batch-independent statistics; (iii) reducing compute by lowering the\ntraining resolution and inexpensively fine-tuning at higher resolution. We find\nthat these three methods improve the practical efficiency for both training and\ninference. Our code will be made available online.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:10:52 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:10:55 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 08:50:21 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Masters", "Dominic", ""], ["Labatie", "Antoine", ""], ["Eaton-Rosen", "Zach", ""], ["Luschi", "Carlo", ""]]}, {"id": "2106.03650", "submitter": "Zilong Huang", "authors": "Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, Bin Fu", "title": "Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Very recently, Window-based Transformers, which computed self-attention\nwithin non-overlapping local windows, demonstrated promising results on image\nclassification, semantic segmentation, and object detection. However, less\nstudy has been devoted to the cross-window connection which is the key element\nto improve the representation ability. In this work, we revisit the spatial\nshuffle as an efficient way to build connections among windows. As a result, we\npropose a new vision transformer, named Shuffle Transformer, which is highly\nefficient and easy to implement by modifying two lines of code. Furthermore,\nthe depth-wise convolution is introduced to complement the spatial shuffle for\nenhancing neighbor-window connections. The proposed architectures achieve\nexcellent performance on a wide range of visual tasks including image-level\nclassification, object detection, and semantic segmentation. Code will be\nreleased for reproduction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:22:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Huang", "Zilong", ""], ["Ben", "Youcheng", ""], ["Luo", "Guozhong", ""], ["Cheng", "Pei", ""], ["Yu", "Gang", ""], ["Fu", "Bin", ""]]}, {"id": "2106.03668", "submitter": "Jiaming Liu", "authors": "Jiaming Liu, M. Salman Asif, Brendt Wohlberg, and Ulugbek S. Kamilov", "title": "Recovery Analysis for Plug-and-Play Priors using the Restricted\n  Eigenvalue Condition", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The plug-and-play priors (PnP) and regularization by denoising (RED) methods\nhave become widely used for solving inverse problems by leveraging pre-trained\ndeep denoisers as image priors. While the empirical imaging performance and the\ntheoretical convergence properties of these algorithms have been widely\ninvestigated, their recovery properties have not previously been theoretically\nanalyzed. We address this gap by showing how to establish theoretical recovery\nguarantees for PnP/RED by assuming that the solution of these methods lies near\nthe fixed-points of a deep neural network. We also present numerical results\ncomparing the recovery performance of PnP/RED in compressive sensing against\nthat of recent compressive sensing algorithms based on generative models. Our\nnumerical results suggest that PnP with a pre-trained artifact removal network\nprovides significantly better results compared to the existing state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:45:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Jiaming", ""], ["Asif", "M. Salman", ""], ["Wohlberg", "Brendt", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "2106.03669", "submitter": "Joshua Pearce", "authors": "Kanlayanee Kaweesinsakul, Siranee Nuchitprasitchai and Joshua M.\n  Pearce", "title": "Open source disease analysis system of cactus by artificial intelligence\n  and image processing", "comments": "Preprint for IAIT2021", "journal-ref": null, "doi": "10.1145/3468784.3469075", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There is a growing interest in cactus cultivation because of numerous cacti\nuses from houseplants to food and medicinal applications. Various diseases\nimpact the growth of cacti. To develop an automated model for the analysis of\ncactus disease and to be able to quickly treat and prevent damage to the\ncactus. The Faster R-CNN and YOLO algorithm technique were used to analyze\ncactus diseases automatically distributed into six groups: 1) anthracnose, 2)\ncanker, 3) lack of care, 4) aphid, 5) rusts and 6) normal group. Based on the\nexperimental results the YOLOv5 algorithm was found to be more effective at\ndetecting and identifying cactus disease than the Faster R-CNN algorithm. Data\ntraining and testing with YOLOv5S model resulted in a precision of 89.7% and an\naccuracy (recall) of 98.5%, which is effective enough for further use in a\nnumber of applications in cactus cultivation. Overall the YOLOv5 algorithm had\na test time per image of only 26 milliseconds. Therefore, the YOLOv5 algorithm\nwas found to suitable for mobile applications and this model could be further\ndeveloped into a program for analyzing cactus disease.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:46:23 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kaweesinsakul", "Kanlayanee", ""], ["Nuchitprasitchai", "Siranee", ""], ["Pearce", "Joshua M.", ""]]}, {"id": "2106.03683", "submitter": "Maria Kyrarini", "authors": "Krishna Chaitanya Kodur, Kaustubh Rajpathak, Akilesh\n  Rajavenkatanarayanan, Maria Kyrarini, Fillia Makedon", "title": "Towards a Multi-purpose Robotic Nursing Assistant", "comments": "accepted at ICRA 2021 Workshop on No-Touch Care for Worker Safety\n  During Pandemic Response", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robotic nursing aid is one of the heavily researched areas in robotics\nnowadays. Several robotic assistants exist that only focus on a specific\nfunction related to nurses assistance or functions related to patient aid.\nThere is a need for a unified system that not only performs tasks that would\nassist nurses and reduce their burden but also perform tasks that help a\npatient. In recent times, due to the COVID-19 pandemic, there is also an\nincrease in the need for robotic assistants that have teleoperation\ncapabilities to provide better protection against the virus spread. To address\nthese requirements, we propose a novel Multi-purpose Intelligent Nurse Aid\n(MINA) robotic system that is capable of providing walking assistance to the\npatients and perform teleoperation tasks with an easy-to-use and intuitive\nGraphical User Interface (GUI). This paper also presents preliminary results\nfrom the walking assistant task that improves upon the current state-of-the-art\nmethods and shows the developed GUI for teleoperation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:00:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kodur", "Krishna Chaitanya", ""], ["Rajpathak", "Kaustubh", ""], ["Rajavenkatanarayanan", "Akilesh", ""], ["Kyrarini", "Maria", ""], ["Makedon", "Fillia", ""]]}, {"id": "2106.03686", "submitter": "Udaya Sampath Karunathilaka Perera Miriya Thanthrige", "authors": "Udaya S.K.P. Miriya Thanthrige, Peter Jung, and Aydin Sezgin", "title": "Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the detection of material defects, which are inside a layered\nmaterial structure using compressive sensing based multiple-input and\nmultiple-output (MIMO) wireless radar. Here, the strong clutter due to the\nreflection of the layered structure's surface often makes the detection of the\ndefects challenging. Thus, sophisticated signal separation methods are required\nfor improved defect detection. In many scenarios, the number of defects that we\nare interested in is limited and the signaling response of the layered\nstructure can be modeled as a low-rank structure. Therefore, we propose joint\nrank and sparsity minimization for defect detection. In particular, we propose\na non-convex approach based on the iteratively reweighted nuclear and\n$\\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy\ncompared to the conventional nuclear norm and $\\ell_1-$norm minimization. To\nthis end, an iterative algorithm is designed to estimate the low-rank and\nsparse contributions. Further, we propose deep learning to learn the parameters\nof the algorithm (i.e., algorithm unfolding) to improve the accuracy and the\nspeed of convergence of the algorithm. Our numerical results show that the\nproposed approach outperforms the conventional approaches in terms of mean\nsquare errors of the recovered low-rank and sparse components and the speed of\nconvergence.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:00:33 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 19:42:53 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Thanthrige", "Udaya S. K. P. Miriya", ""], ["Jung", "Peter", ""], ["Sezgin", "Aydin", ""]]}, {"id": "2106.03694", "submitter": "Srikanta Sannigrahi", "authors": "Srikanta Sannigrahi, Bidroha Basu, Arunima Sarkar Basu, Francesco\n  Pilla", "title": "Detection of marine floating plastic using Sentinel-2 imagery and\n  machine learning models", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing level of marine plastic pollution poses severe threats to the\nmarine ecosystem and biodiversity. The present study attempted to explore the\nfull functionality of open Sentinel satellite data and ML models for detecting\nand classifying floating plastic debris in Mytilene (Greece), Limassol\n(Cyprus), Calabria (Italy), and Beirut (Lebanon). Two ML models, i.e. Support\nVector Machine (SVM) and Random Forest (RF) were utilized to carry out the\nclassification analysis. In-situ plastic location data was collected from the\ncontrol experiment conducted in Mytilene, Greece and Limassol, Cyprus, and the\nsame was considered for training the models. Both remote sensing bands and\nspectral indices were used for developing the ML models. A spectral signature\nprofile for plastic was created for discriminating the floating plastic from\nother marine debris. A newly developed index, kernel Normalized Difference\nVegetation Index (kNDVI), was incorporated into the modelling to examine its\ncontribution to model performances. Both SVM and RF were performed well in five\nmodels and test case combinations. Among the two ML models, the highest\nperformance was measured for the RF. The inclusion of kNDVI was found effective\nand increased the model performances, reflected by high balanced accuracy\nmeasured for model 2 (~80% to ~98 % for SVM and ~87% to ~97 % for RF). Using\nthe best-performed model, an automated floating plastic detection system was\ndeveloped and tested in Calabria and Beirut. For both sites, the trained model\nhad detected the floating plastic with ~99% accuracy. Among the six predictors,\nthe FDI was found the most important variable for detecting marine floating\nplastic. These findings collectively suggest that high-resolution remote\nsensing imagery and the automated ML models can be an effective alternative for\nthe cost-effective detection of marine floating plastic.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 20:24:30 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 08:59:05 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sannigrahi", "Srikanta", ""], ["Basu", "Bidroha", ""], ["Basu", "Arunima Sarkar", ""], ["Pilla", "Francesco", ""]]}, {"id": "2106.03705", "submitter": "Saad Nadeem", "authors": "Navdeep Dahiya, Gourav Jhanwar, Anthony Yezzi, Masoud Zarepisheh, and\n  Saad Nadeem", "title": "Deep Learning 3D Dose Prediction for Conventional Lung IMRT Using\n  Consistent/Unbiased Automated Plans", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) 3D dose prediction has recently gained a lot of attention.\nHowever, the variability of plan quality in the training dataset, generated\nmanually by planners with wide range of expertise, can dramatically effect the\nquality of the final predictions. Moreover, any changes in the clinical\ncriteria requires a new set of manually generated plans by planners to build a\nnew prediction model. In this work, we instead use consistent plans generated\nby our in-house automated planning system (named ``ECHO'') to train the DL\nmodel. ECHO (expedited constrained hierarchical optimization) generates\nconsistent/unbiased plans by solving large-scale constrained optimization\nproblems sequentially. If the clinical criteria changes, a new training data\nset can be easily generated offline using ECHO, with no or limited human\nintervention, making the DL-based prediction model easily adaptable to the\nchanges in the clinical practice. We used 120 conventional lung patients (100\nfor training, 20 for testing) with different beam configurations and trained\nour DL-model using manually-generated as well as automated ECHO plans. We\nevaluated different inputs: (1) CT+(PTV/OAR)contours, and (2) CT+contours+beam\nconfigurations, and different loss functions: (1) MAE (mean absolute error),\nand (2) MAE+DVH (dose volume histograms). The quality of the predictions was\ncompared using different DVH metrics as well as dose-score and DVH-score,\nrecently introduced by the AAPM knowledge-based planning grand challenge. The\nbest results were obtained using automated ECHO plans and CT+contours+beam as\ntraining inputs and MAE+DVH as loss function.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:15:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Dahiya", "Navdeep", ""], ["Jhanwar", "Gourav", ""], ["Yezzi", "Anthony", ""], ["Zarepisheh", "Masoud", ""], ["Nadeem", "Saad", ""]]}, {"id": "2106.03714", "submitter": "Zhou Daquan", "authors": "Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li,\n  Xiaojie Jin, Qibin Hou, Jiashi Feng", "title": "Refiner: Refining Self-attention for Vision Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision Transformers (ViTs) have shown competitive accuracy in image\nclassification tasks compared with CNNs. Yet, they generally require much more\ndata for model pre-training. Most of recent works thus are dedicated to\ndesigning more complex architectures or training methods to address the\ndata-efficiency issue of ViTs. However, few of them explore improving the\nself-attention mechanism, a key factor distinguishing ViTs from CNNs. Different\nfrom existing works, we introduce a conceptually simple scheme, called refiner,\nto directly refine the self-attention maps of ViTs. Specifically, refiner\nexplores attention expansion that projects the multi-head attention maps to a\nhigher-dimensional space to promote their diversity. Further, refiner applies\nconvolutions to augment local patterns of the attention maps, which we show is\nequivalent to a distributed local attention features are aggregated locally\nwith learnable kernels and then globally aggregated with self-attention.\nExtensive experiments demonstrate that refiner works surprisingly well.\nSignificantly, it enables ViTs to achieve 86% top-1 classification accuracy on\nImageNet with only 81M parameters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:24:54 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhou", "Daquan", ""], ["Shi", "Yujun", ""], ["Kang", "Bingyi", ""], ["Yu", "Weihao", ""], ["Jiang", "Zihang", ""], ["Li", "Yuan", ""], ["Jin", "Xiaojie", ""], ["Hou", "Qibin", ""], ["Feng", "Jiashi", ""]]}, {"id": "2106.03719", "submitter": "Tsai-Shien Chen", "authors": "Tsai-Shien Chen, Wei-Chih Hung, Hung-Yu Tseng, Shao-Yi Chien,\n  Ming-Hsuan Yang", "title": "Incremental False Negative Detection for Contrastive Learning", "comments": "Submitted to NeurIPS 2021. Code:\n  https://github.com/tsaishien-chen/IFND", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has recently shown great potential in vision tasks\nvia contrastive learning, which aims to discriminate each image, or instance,\nin the dataset. However, such instance-level learning ignores the semantic\nrelationship between instances and repels the anchor equally from the\nsemantically similar samples, termed as false negatives. In this work, we first\nempirically highlight that the unfavorable effect from false negatives is more\nsignificant for the datasets containing images with more semantic concepts. To\naddress the issue, we introduce a novel incremental false negative detection\nfor self-supervised contrastive learning. Following the training process, when\nthe encoder is gradually better-trained and the embedding space becomes more\nsemantically structural, our method incrementally detects more reliable false\nnegatives. Subsequently, during contrastive learning, we discuss two strategies\nto explicitly remove the detected false negatives. Extensive experiments show\nthat our proposed method outperforms other self-supervised contrastive learning\nframeworks on multiple benchmarks within a limited compute.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:29:14 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chen", "Tsai-Shien", ""], ["Hung", "Wei-Chih", ""], ["Tseng", "Hung-Yu", ""], ["Chien", "Shao-Yi", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2106.03720", "submitter": "Siddhant Kapil", "authors": "Charu Sharma, Siddhant R. Kapil, David Chapman", "title": "Person Re-Identification with a Locally Aware Transformer", "comments": "10 pages, 2 figure, submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Person Re-Identification is an important problem in computer vision-based\nsurveillance applications, in which the same person is attempted to be\nidentified from surveillance photographs in a variety of nearby zones. At\npresent, the majority of Person re-ID techniques are based on Convolutional\nNeural Networks (CNNs), but Vision Transformers are beginning to displace pure\nCNNs for a variety of object recognition tasks. The primary output of a vision\ntransformer is a global classification token, but vision transformers also\nyield local tokens which contain additional information about local regions of\nthe image. Techniques to make use of these local tokens to improve\nclassification accuracy are an active area of research. We propose a novel\nLocally Aware Transformer (LA-Transformer) that employs a Parts-based\nConvolution Baseline (PCB)-inspired strategy for aggregating globally enhanced\nlocal classification tokens into an ensemble of $\\sqrt{N}$ classifiers, where\n$N$ is the number of patches. An additional novelty is that we incorporate\nblockwise fine-tuning which further improves re-ID accuracy. LA-Transformer\nwith blockwise fine-tuning achieves rank-1 accuracy of $98.27 \\%$ with standard\ndeviation of $0.13$ on the Market-1501 and $98.7\\%$ with standard deviation of\n$0.2$ on the CUHK03 dataset respectively, outperforming all other\nstate-of-the-art published methods at the time of writing.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:31:19 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 17:59:48 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sharma", "Charu", ""], ["Kapil", "Siddhant R.", ""], ["Chapman", "David", ""]]}, {"id": "2106.03727", "submitter": "Royson Lee", "authors": "Royson Lee, Stylianos I. Venieris, Nicholas D. Lane", "title": "Deep Neural Network-based Enhancement for Image and Video Streaming\n  Systems: A Survey and Future Directions", "comments": "Accepted for publication at the ACM Computing Surveys (CSUR) journal,\n  2021. arXiv admin note: text overlap with arXiv:2010.05838", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-enabled smartphones and ultra-wide displays are transforming a\nvariety of visual apps spanning from on-demand movies and 360{\\deg} videos to\nvideo-conferencing and live streaming. However, robustly delivering visual\ncontent under fluctuating networking conditions on devices of diverse\ncapabilities remains an open problem. In recent years, advances in the field of\ndeep learning on tasks such as super-resolution and image enhancement have led\nto unprecedented performance in generating high-quality images from low-quality\nones, a process we refer to as neural enhancement. In this paper, we survey\nstate-of-the-art content delivery systems that employ neural enhancement as a\nkey component in achieving both fast response time and high visual quality. We\nfirst present the components and architecture of existing content delivery\nsystems, highlighting their challenges and motivating the use of neural\nenhancement models as a countermeasure. We then cover the deployment challenges\nof these models and analyze existing systems and their design decisions in\nefficiently overcoming these technical challenges. Additionally, we underline\nthe key trends and common approaches across systems that target diverse\nuse-cases. Finally, we present promising future directions based on the latest\ninsights from deep learning research to further boost the quality of experience\nof content delivery systems.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:42:36 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lee", "Royson", ""], ["Venieris", "Stylianos I.", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2106.03734", "submitter": "Ahmed Aldahdooh", "authors": "Ahmed Aldahdooh, Wassim Hamidouche, Olivier Deforges", "title": "Reveal of Vision Transformers Robustness against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention-based networks have achieved state-of-the-art performance in many\ncomputer vision tasks, such as image classification. Unlike Convolutional\nNeural Network (CNN), the major part of the vanilla Vision Transformer (ViT) is\nthe attention block that brings the power of mimicking the global context of\nthe input image. This power is data hunger and hence, the larger the training\ndata the better the performance. To overcome this limitation, many ViT-based\nnetworks, or hybrid-ViT, have been proposed to include local context during the\ntraining. The robustness of ViTs and its variants against adversarial attacks\nhas not been widely invested in the literature. Some robustness attributes were\nrevealed in few previous works and hence, more insight robustness attributes\nare yet unrevealed. This work studies the robustness of ViT variants 1) against\ndifferent $L_p$-based adversarial attacks in comparison with CNNs and 2) under\nAdversarial Examples (AEs) after applying preprocessing defense methods. To\nthat end, we run a set of experiments on 1000 images from ImageNet-1k and then\nprovide an analysis that reveals that vanilla ViT or hybrid-ViT are more robust\nthan CNNs. For instance, we found that 1) Vanilla ViTs or hybrid-ViTs are more\nrobust than CNNs under $L_0$, $L_1$, $L_2$, $L_\\infty$-based, and Color Channel\nPerturbations (CCP) attacks. 2) Vanilla ViTs are not responding to\npreprocessing defenses that mainly reduce the high frequency components while,\nhybrid-ViTs are more responsive to such defense. 3) CCP can be used as a\npreprocessing defense and larger ViT variants are found to be more responsive\nthan other models. Furthermore, feature maps, attention maps, and Grad-CAM\nvisualization jointly with image quality measures, and perturbations' energy\nspectrum are provided for an insight understanding of attention-based models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:59:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Aldahdooh", "Ahmed", ""], ["Hamidouche", "Wassim", ""], ["Deforges", "Olivier", ""]]}, {"id": "2106.03738", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni and Anelia Angelova and Michael S. Ryoo and Irfan Essa", "title": "Unsupervised Action Segmentation for Instructional Videos", "comments": "4 page abstract for LUV workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of automatically discovering atomic\nactions in unsupervised manner from instructional videos, which are rarely\nannotated with atomic actions. We present an unsupervised approach to learn\natomic actions of structured human tasks from a variety of instructional videos\nbased on a sequential stochastic autoregressive model for temporal segmentation\nof videos. This learns to represent and discover the sequential relationship\nbetween different atomic actions of the task, and which provides automatic and\nunsupervised self-labeling.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:02:06 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Angelova", "Anelia", ""], ["Ryoo", "Michael S.", ""], ["Essa", "Irfan", ""]]}, {"id": "2106.03743", "submitter": "Antoine Labatie", "authors": "Antoine Labatie, Dominic Masters, Zach Eaton-Rosen, Carlo Luschi", "title": "Proxy-Normalizing Activations to Match Batch Normalization while\n  Removing Batch Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We investigate the reasons for the performance degradation incurred with\nbatch-independent normalization. We find that the prototypical techniques of\nlayer normalization and instance normalization both induce the appearance of\nfailure modes in the neural network's pre-activations: (i) layer normalization\ninduces a collapse towards channel-wise constant functions; (ii) instance\nnormalization induces a lack of variability in instance statistics, symptomatic\nof an alteration of the expressivity. To alleviate failure mode (i) without\naggravating failure mode (ii), we introduce the technique \"Proxy Normalization\"\nthat normalizes post-activations using a proxy distribution. When combined with\nlayer normalization or group normalization, this batch-independent\nnormalization emulates batch normalization's behavior and consistently matches\nor exceeds its performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:08:48 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 13:50:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Labatie", "Antoine", ""], ["Masters", "Dominic", ""], ["Eaton-Rosen", "Zach", ""], ["Luschi", "Carlo", ""]]}, {"id": "2106.03746", "submitter": "Marco De Nadai", "authors": "Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri and Marco\n  De Nadai", "title": "Efficient Training of Visual Transformers with Small-Size Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Transformers (VTs) are emerging as an architectural paradigm\nalternative to Convolutional networks (CNNs). Differently from CNNs, VTs can\ncapture global relations between image elements and they potentially have a\nlarger representation capacity. However, the lack of the typical convolutional\ninductive bias makes these models more data-hungry than common CNNs. In fact,\nsome local properties of the visual domain which are embedded in the CNN\narchitectural design, in VTs should be learned from samples. In this paper, we\nempirically analyse different VTs, comparing their robustness in a small\ntraining-set regime, and we show that, despite having a comparable accuracy\nwhen trained on ImageNet, their performance on smaller datasets can be largely\ndifferent. Moreover, we propose a self-supervised task which can extract\nadditional information from images with only a negligible computational\noverhead. This task encourages the VTs to learn spatial relations within an\nimage and makes the VT training much more robust when training data are scarce.\nOur task is used jointly with the standard (supervised) training and it does\nnot depend on specific architectural choices, thus it can be easily plugged in\nthe existing VTs. Using an extensive evaluation with different VTs and\ndatasets, we show that our method can improve (sometimes dramatically) the\nfinal accuracy of the VTs. The code will be available upon acceptance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:14:06 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Yahui", ""], ["Sangineto", "Enver", ""], ["Bi", "Wei", ""], ["Sebe", "Nicu", ""], ["Lepri", "Bruno", ""], ["De Nadai", "Marco", ""]]}, {"id": "2106.03755", "submitter": "Hankui Peng", "authors": "Hankui Peng, Angelica I. Aviles-Rivero, Carola-Bibiane Schonlieb", "title": "HERS Superpixels: Deep Affinity Learning for Hierarchical Entropy Rate\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixels serve as a powerful preprocessing tool in many computer vision\ntasks. By using superpixel representation, the number of image primitives can\nbe largely reduced by orders of magnitudes. The majority of superpixel methods\nuse handcrafted features, which usually do not translate well into strong\nadherence to object boundaries. A few recent superpixel methods have introduced\ndeep learning into the superpixel segmentation process. However, none of these\nmethods is able to produce superpixels in near real-time, which is crucial to\nthe applicability of a superpixel method in practice. In this work, we propose\na two-stage graph-based framework for superpixel segmentation. In the first\nstage, we introduce an efficient Deep Affinity Learning (DAL) network that\nlearns pairwise pixel affinities by aggregating multi-scale information. In the\nsecond stage, we propose a highly efficient superpixel method called\nHierarchical Entropy Rate Segmentation (HERS). Using the learned affinities\nfrom the first stage, HERS builds a hierarchical tree structure that can\nproduce any number of highly adaptive superpixels instantaneously. We\ndemonstrate, through visual and numerical experiments, the effectiveness and\nefficiency of our method compared to various state-of-the-art superpixel\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:20:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Peng", "Hankui", ""], ["Aviles-Rivero", "Angelica I.", ""], ["Schonlieb", "Carola-Bibiane", ""]]}, {"id": "2106.03761", "submitter": "Tiago Salvador", "authors": "Tiago Salvador, Stephanie Cairns, Vikram Voleti, Noah Marshall, Adam\n  Oberman", "title": "Bias Mitigation of Face Recognition Models Through Calibration", "comments": "22 pages, 20 tables, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition models suffer from bias: for example, the probability of a\nfalse positive (incorrect face match) strongly depends on sensitive attributes\nlike ethnicity. As a result, these models may disproportionately and negatively\nimpact minority groups when used in law enforcement. In this work, we introduce\nthe Bias Mitigation Calibration (BMC) method, which (i) increases model\naccuracy (improving the state-of-the-art), (ii) produces fairly-calibrated\nprobabilities, (iii) significantly reduces the gap in the false positive rates,\nand (iv) does not require knowledge of the sensitive attribute.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:26:26 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Salvador", "Tiago", ""], ["Cairns", "Stephanie", ""], ["Voleti", "Vikram", ""], ["Marshall", "Noah", ""], ["Oberman", "Adam", ""]]}, {"id": "2106.03770", "submitter": "Samuel Chassot", "authors": "Luca Barras, Samuel Chassot, Daniel Filipe Nunes Silva", "title": "Few-Shot Unsupervised Image-to-Image Translation on complex scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised image-to-image translation methods have received a lot of\nattention in the last few years. Multiple techniques emerged tackling the\ninitial challenge from different perspectives. Some focus on learning as much\nas possible from several target style images for translations while other make\nuse of object detection in order to produce more realistic results on\ncontent-rich scenes. In this work, we assess how a method that has initially\nbeen developed for single object translation performs on more diverse and\ncontent-rich images. Our work is based on the FUNIT[1] framework and we train\nit with a more diverse dataset. This helps understanding how such method\nbehaves beyond their initial frame of application. We present a way to extend a\ndataset based on object detection. Moreover, we propose a way to adapt the\nFUNIT framework in order to leverage the power of object detection that one can\nsee in other methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:33:19 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Barras", "Luca", ""], ["Chassot", "Samuel", ""], ["Silva", "Daniel Filipe Nunes", ""]]}, {"id": "2106.03772", "submitter": "Yiding Yang", "authors": "Yiding Yang, Zhou Ren, Haoxiang Li, Chunluan Zhou, Xinchao Wang, Gang\n  Hua", "title": "Learning Dynamics via Graph Neural Networks for Human Pose Estimation\n  and Tracking", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation and tracking serve as crucial steps for video\nunderstanding. Most state-of-the-art approaches rely on first estimating poses\nin each frame and only then implementing data association and refinement.\nDespite the promising results achieved, such a strategy is inevitably prone to\nmissed detections especially in heavily-cluttered scenes, since this\ntracking-by-detection paradigm is, by nature, largely dependent on visual\nevidences that are absent in the case of occlusion. In this paper, we propose a\nnovel online approach to learning the pose dynamics, which are independent of\npose detections in current fame, and hence may serve as a robust estimation\neven in challenging scenarios including occlusion. Specifically, we derive this\nprediction of dynamics through a graph neural network~(GNN) that explicitly\naccounts for both spatial-temporal and visual information. It takes as input\nthe historical pose tracklets and directly predicts the corresponding poses in\nthe following frame for each tracklet. The predicted poses will then be\naggregated with the detected poses, if any, at the same frame so as to produce\nthe final pose, potentially recovering the occluded joints missed by the\nestimator. Experiments on PoseTrack 2017 and PoseTrack 2018 datasets\ndemonstrate that the proposed method achieves results superior to the state of\nthe art on both human pose estimation and tracking tasks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:36:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yang", "Yiding", ""], ["Ren", "Zhou", ""], ["Li", "Haoxiang", ""], ["Zhou", "Chunluan", ""], ["Wang", "Xinchao", ""], ["Hua", "Gang", ""]]}, {"id": "2106.03774", "submitter": "Riccardo de Lutio", "authors": "Riccardo de Lutio, Yihang She, Stefano D'Aronco, Stefania Russo,\n  Philipp Brun, Jan D. Wegner, Konrad Schindler", "title": "Digital Taxonomist: Identifying Plant Species in Citizen Scientists'\n  Photographs", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic identification of plant specimens from amateur photographs could\nimprove species range maps, thus supporting ecosystems research as well as\nconservation efforts. However, classifying plant specimens based on image data\nalone is challenging: some species exhibit large variations in visual\nappearance, while at the same time different species are often visually\nsimilar; additionally, species observations follow a highly imbalanced,\nlong-tailed distribution due to differences in abundance as well as observer\nbiases. On the other hand, most species observations are accompanied by side\ninformation about the spatial, temporal and ecological context. Moreover,\nbiological species are not an unordered list of classes but embedded in a\nhierarchical taxonomic structure. We propose a machine learning model that\ntakes into account these additional cues in a unified framework. Our Digital\nTaxonomist is able to identify plant species in photographs more correctly.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:38:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["de Lutio", "Riccardo", ""], ["She", "Yihang", ""], ["D'Aronco", "Stefano", ""], ["Russo", "Stefania", ""], ["Brun", "Philipp", ""], ["Wegner", "Jan D.", ""], ["Schindler", "Konrad", ""]]}, {"id": "2106.03776", "submitter": "Tien-Cuong Nguyen", "authors": "Synh Viet-Uyen Ha, Cuong Tien Nguyen, Hung Ngoc Phan, Nhat Minh Chung,\n  Phuong Hoai Ha", "title": "CDN-MEDAL: Two-stage Density and Difference Approximation Framework for\n  Motion Analysis", "comments": "14 pages, 5 figures, to be submitted to IEEE TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background modeling is a promising research area in video analysis with a\nvariety of video surveillance applications. Recent years have witnessed the\nproliferation of deep neural networks via effective learning-based approaches\nin motion analysis. However, these techniques only provide a limited\ndescription of the observed scenes' insufficient properties where a\nsingle-valued mapping is learned to approximate the temporal conditional\naverages of the target background. On the other hand, statistical learning in\nimagery domains has become one of the most prevalent approaches with high\nadaptation to dynamic context transformation, notably Gaussian Mixture Models,\ncombined with a foreground extraction step. In this work, we propose a novel,\ntwo-stage method of change detection with two convolutional neural networks.\nThe first architecture is grounded on the unsupervised Gaussian mixtures\nstatistical learning to describe the scenes' salient features. The second one\nimplements a light-weight pipeline of foreground detection. Our two-stage\nframework contains approximately 3.5K parameters in total but still maintains\nrapid convergence to intricate motion patterns. Our experiments on publicly\navailable datasets show that our proposed networks are not only capable of\ngeneralizing regions of moving objects in unseen cases with promising results\nbut also are competitive in performance efficiency and effectiveness regarding\nforeground segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:39:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ha", "Synh Viet-Uyen", ""], ["Nguyen", "Cuong Tien", ""], ["Phan", "Hung Ngoc", ""], ["Chung", "Nhat Minh", ""], ["Ha", "Phuong Hoai", ""]]}, {"id": "2106.03793", "submitter": "Ruben Hemelings", "authors": "Ruben Hemelings, Bart Elen, Jo\\~ao Barbosa Breda, Erwin Bellon,\n  Matthew B Blaschko, Patrick De Boever, Ingeborg Stalmans", "title": "Pointwise visual field estimation from optical coherence tomography in\n  glaucoma: a structure-function analysis using deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background/Aims: Standard Automated Perimetry (SAP) is the gold standard to\nmonitor visual field (VF) loss in glaucoma management, but is prone to\nintra-subject variability. We developed and validated a deep learning (DL)\nregression model that estimates pointwise and overall VF loss from unsegmented\noptical coherence tomography (OCT) scans. Methods: Eight DL regression models\nwere trained with various retinal imaging modalities: circumpapillary OCT at\n3.5mm, 4.1mm, 4.7mm diameter, and scanning laser ophthalmoscopy (SLO) en face\nimages to estimate mean deviation (MD) and 52 threshold values. This\nretrospective study used data from patients who underwent a complete glaucoma\nexamination, including a reliable Humphrey Field Analyzer (HFA) 24-2 SITA\nStandard VF exam and a SPECTRALIS OCT scan using the Glaucoma Module Premium\nEdition. Results: A total of 1378 matched OCT-VF pairs of 496 patients (863\neyes) were included for training and evaluation of the DL models. Average\nsample MD was -7.53dB (from -33.8dB to +2.0dB). For 52 VF threshold values\nestimation, the circumpapillary OCT scan with the largest radius (4.7mm)\nachieved the best performance among all individual models (Pearson r=0.77, 95%\nCI=[0.72-0.82]). For MD, prediction averaging of OCT-trained models (3.5mm,\n4.1mm, 4.7mm) resulted in a Pearson r of 0.78 [0.73-0.83] on the validation set\nand comparable performance on the test set (Pearson r=0.79 [0.75-0.82]).\nConclusion: DL on unsegmented OCT scans accurately predicts pointwise and mean\ndeviation of 24-2 VF in glaucoma patients. Automated VF from unsegmented OCT\ncould be a solution for patients unable to produce reliable perimetry results.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:58:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Hemelings", "Ruben", ""], ["Elen", "Bart", ""], ["Breda", "Jo\u00e3o Barbosa", ""], ["Bellon", "Erwin", ""], ["Blaschko", "Matthew B", ""], ["De Boever", "Patrick", ""], ["Stalmans", "Ingeborg", ""]]}, {"id": "2106.03797", "submitter": "Hoang D. Nguyen", "authors": "Alex To, Maican Liu, Muhammad Hazeeq Bin Muhammad Hairul, Joseph G.\n  Davis, Jeannie S.A. Lee, Henrik Hesse and Hoang D. Nguyen", "title": "Drone-based AI and 3D Reconstruction for Digital Twin Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Twin is an emerging technology at the forefront of Industry 4.0, with\nthe ultimate goal of combining the physical space and the virtual space. To\ndate, the Digital Twin concept has been applied in many engineering fields,\nproviding useful insights in the areas of engineering design, manufacturing,\nautomation, and construction industry. While the nexus of various technologies\nopens up new opportunities with Digital Twin, the technology requires a\nframework to integrate the different technologies, such as the Building\nInformation Model used in the Building and Construction industry. In this work,\nan Information Fusion framework is proposed to seamlessly fuse heterogeneous\ncomponents in a Digital Twin framework from the variety of technologies\ninvolved. This study aims to augment Digital Twin in buildings with the use of\nAI and 3D reconstruction empowered by unmanned aviation vehicles. We proposed a\ndrone-based Digital Twin augmentation framework with reusable and customisable\ncomponents. A proof of concept is also developed, and extensive evaluation is\nconducted for 3D reconstruction and applications of AI for defect detection.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:31:15 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["To", "Alex", ""], ["Liu", "Maican", ""], ["Hairul", "Muhammad Hazeeq Bin Muhammad", ""], ["Davis", "Joseph G.", ""], ["Lee", "Jeannie S. A.", ""], ["Hesse", "Henrik", ""], ["Nguyen", "Hoang D.", ""]]}, {"id": "2106.03798", "submitter": "Ruizhi Shao", "authors": "Ruizhi Shao, Hongwen Zhang, He Zhang, Yanpei Cao, Tao Yu, Yebin Liu", "title": "DoubleField: Bridging the Neural Surface and Radiance Fields for\n  High-fidelity Human Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DoubleField, a novel representation combining the merits of both\nsurface field and radiance field for high-fidelity human rendering. Within\nDoubleField, the surface field and radiance field are associated together by a\nshared feature embedding and a surface-guided sampling strategy. In this way,\nDoubleField has a continuous but disentangled learning space for geometry and\nappearance modeling, which supports fast training, inference, and finetuning.\nTo achieve high-fidelity free-viewpoint rendering, DoubleField is further\naugmented to leverage ultra-high-resolution inputs, where a view-to-view\ntransformer and a transfer learning scheme are introduced for more efficient\nlearning and finetuning from sparse-view inputs at original resolutions. The\nefficacy of DoubleField is validated by the quantitative evaluations on several\ndatasets and the qualitative results in a real-world sparse multi-view system,\nshowing its superior capability for photo-realistic free-viewpoint human\nrendering. For code and demo video, please refer to our project page:\nhttp://www.liuyebin.com/dbfield/dbfield.html.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:08:17 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 13:22:11 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shao", "Ruizhi", ""], ["Zhang", "Hongwen", ""], ["Zhang", "He", ""], ["Cao", "Yanpei", ""], ["Yu", "Tao", ""], ["Liu", "Yebin", ""]]}, {"id": "2106.03801", "submitter": "Razvan Caramalau", "authors": "Razvan Caramalau, Binod Bhattarai, Tae-Kyun Kim", "title": "Visual Transformer for Task-aware Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pool-based sampling in active learning (AL) represents a key framework for\nan-notating informative data when dealing with deep learning models. In this\npaper, we present a novel pipeline for pool-based Active Learning. Unlike most\nprevious works, our method exploits accessible unlabelled examples during\ntraining to estimate their co-relation with the labelled examples. Another\ncontribution of this paper is to adapt Visual Transformer as a sampler in the\nAL pipeline. Visual Transformer models non-local visual concept dependency\nbetween labelled and unlabelled examples, which is crucial to identifying the\ninfluencing unlabelled examples. Also, compared to existing methods where the\nlearner and the sampler are trained in a multi-stage manner, we propose to\ntrain them in a task-aware jointly manner which enables transforming the latent\nspace into two separate tasks: one that classifies the labelled examples; the\nother that distinguishes the labelling direction. We evaluated our work on four\ndifferent challenging benchmarks of classification and detection tasks viz.\nCIFAR10, CIFAR100,FashionMNIST, RaFD, and Pascal VOC 2007. Our extensive\nempirical and qualitative evaluations demonstrate the superiority of our method\ncompared to the existing methods. Code available:\nhttps://github.com/razvancaramalau/Visual-Transformer-for-Task-aware-Active-Learning\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:13:59 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Caramalau", "Razvan", ""], ["Bhattarai", "Binod", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2106.03804", "submitter": "Daniel Rebain", "authors": "Daniel Rebain, Ke Li, Vincent Sitzmann, Soroosh Yazdani, Kwang Moo Yi,\n  Andrea Tagliasacchi", "title": "Deep Medial Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit representations of geometry, such as occupancy fields or signed\ndistance fields (SDF), have recently re-gained popularity in encoding 3D solid\nshape in a functional form. In this work, we introduce medial fields: a field\nfunction derived from the medial axis transform (MAT) that makes available\ninformation about the underlying 3D geometry that is immediately useful for a\nnumber of downstream tasks. In particular, the medial field encodes the local\nthickness of a 3D shape, and enables O(1) projection of a query point onto the\nmedial axis. To construct the medial field we require nothing but the SDF of\nthe shape itself, thus allowing its straightforward incorporation in any\napplication that relies on signed distance fields. Working in unison with the\nO(1) surface projection supported by the SDF, the medial field opens the door\nfor an entirely new set of efficient, shape-aware operations on implicit\nrepresentations. We present three such applications, including a modification\nto sphere tracing that renders implicit representations with better convergence\nproperties, a fast construction method for memory-efficient rigid-body\ncollision proxies, and an efficient approximation of ambient occlusion that\nremains stable with respect to viewpoint variations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:15:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rebain", "Daniel", ""], ["Li", "Ke", ""], ["Sitzmann", "Vincent", ""], ["Yazdani", "Soroosh", ""], ["Yi", "Kwang Moo", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "2106.03805", "submitter": "Andrew Ilyas", "authors": "Guillaume Leclerc, Hadi Salman, Andrew Ilyas, Sai Vemprala, Logan\n  Engstrom, Vibhav Vineet, Kai Xiao, Pengchuan Zhang, Shibani Santurkar, Greg\n  Yang, Ashish Kapoor, Aleksander Madry", "title": "3DB: A Framework for Debugging Computer Vision Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce 3DB: an extendable, unified framework for testing and debugging\nvision models using photorealistic simulation. We demonstrate, through a wide\nrange of use cases, that 3DB allows users to discover vulnerabilities in\ncomputer vision systems and gain insights into how models make decisions. 3DB\ncaptures and generalizes many robustness analyses from prior work, and enables\none to study their interplay. Finally, we find that the insights generated by\nthe system transfer to the physical world.\n  We are releasing 3DB as a library (https://github.com/3db/3db) alongside a\nset of example analyses, guides, and documentation: https://3db.github.io/3db/ .\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:16:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Leclerc", "Guillaume", ""], ["Salman", "Hadi", ""], ["Ilyas", "Andrew", ""], ["Vemprala", "Sai", ""], ["Engstrom", "Logan", ""], ["Vineet", "Vibhav", ""], ["Xiao", "Kai", ""], ["Zhang", "Pengchuan", ""], ["Santurkar", "Shibani", ""], ["Yang", "Greg", ""], ["Kapoor", "Ashish", ""], ["Madry", "Aleksander", ""]]}, {"id": "2106.03814", "submitter": "Ankan Dash", "authors": "Ankan Dash, Junyi Ye, Guiling Wang", "title": "High Resolution Solar Image Generation using Generative Adversarial\n  Networks", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We applied Deep Learning algorithm known as Generative Adversarial Networks\n(GANs) to perform solar image-to-image translation. That is, from Solar\nDynamics Observatory (SDO)/Helioseismic and Magnetic Imager(HMI) line of sight\nmagnetogram images to SDO/Atmospheric Imaging Assembly(AIA) 0304-{\\AA} images.\nThe Ultraviolet(UV)/Extreme Ultraviolet(EUV) observations like the\nSDO/AIA0304-{\\AA} images were only made available to scientists in the late\n1990s even though the magenetic field observations like the SDO/HMI have been\navailable since the 1970s. Therefore by leveraging Deep Learning algorithms\nlike GANs we can give scientists access to complete datasets for analysis. For\ngenerating high resolution solar images we use the Pix2PixHD and Pix2Pix\nalgorithms. The Pix2PixHD algorithm was specifically designed for high\nresolution image generation tasks, and the Pix2Pix algorithm is by far the most\nwidely used image to image translation algorithm. For training and testing we\nused the data for the year 2012, 2013 and 2014. The results show that our deep\nlearning models are capable of generating high resolution(1024 x 1024 pixels)\nAIA0304 images from HMI magnetograms. Specifically, the pixel-to-pixel Pearson\nCorrelation Coefficient of the images generated by Pix2PixHD and original\nimages is as high as 0.99. The number is 0.962 if Pix2Pix is used to generate\nimages. The results we get for our Pix2PixHD model is better than the results\nobtained by previous works done by others to generate AIA0304 images. Thus, we\ncan use these models to generate AIA0304 images when the AIA0304 data is not\navailable which can be used for understanding space weather and giving\nresearchers the capability to predict solar events such as Solar Flares and\nCoronal Mass Ejections. As far as we know, our work is the first attempt to\nleverage Pix2PixHD algorithm for SDO/HMI to SDO/AIA0304 image-to-image\ntranslation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:24:33 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Dash", "Ankan", ""], ["Ye", "Junyi", ""], ["Wang", "Guiling", ""]]}, {"id": "2106.03821", "submitter": "Baptiste Pouthier", "authors": "Baptiste Pouthier, Laurent Pilati, Leela K. Gudupudi, Charles\n  Bouveyron and Frederic Precioso", "title": "Active Speaker Detection as a Multi-Objective Optimization with\n  Uncertainty-based Multimodal Fusion", "comments": "In INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now well established from a variety of studies that there is a\nsignificant benefit from combining video and audio data in detecting active\nspeakers. However, either of the modalities can potentially mislead audiovisual\nfusion by inducing unreliable or deceptive information. This paper outlines\nactive speaker detection as a multi-objective learning problem to leverage best\nof each modalities using a novel self-attention, uncertainty-based multimodal\nfusion scheme. Results obtained show that the proposed multi-objective learning\narchitecture outperforms traditional approaches in improving both mAP and AUC\nscores. We further demonstrate that our fusion strategy surpasses, in active\nspeaker detection, other modality fusion methods reported in various\ndisciplines. We finally show that the proposed method significantly improves\nthe state-of-the-art on the AVA-ActiveSpeaker dataset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:38:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Pouthier", "Baptiste", ""], ["Pilati", "Laurent", ""], ["Gudupudi", "Leela K.", ""], ["Bouveyron", "Charles", ""], ["Precioso", "Frederic", ""]]}, {"id": "2106.03839", "submitter": "Goutam Bhat", "authors": "Goutam Bhat and Martin Danelljan and Radu Timofte and Kazutoshi Akita\n  and Wooyeong Cho and Haoqiang Fan and Lanpeng Jia and Daeshik Kim and Bruno\n  Lecouat and Youwei Li and Shuaicheng Liu and Ziluan Liu and Ziwei Luo and\n  Takahiro Maeda and Julien Mairal and Christian Micheloni and Xuan Mo and\n  Takeru Oba and Pavel Ostyakov and Jean Ponce and Sanghyeok Son and Jian Sun\n  and Norimichi Ukita and Rao Muhammad Umer and Youliang Yan and Lei Yu and\n  Magauiya Zhussip and Xueyi Zou", "title": "NTIRE 2021 Challenge on Burst Super-Resolution: Methods and Results", "comments": "NTIRE 2021 Burst Super-Resolution challenge report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the NTIRE2021 challenge on burst super-resolution. Given a\nRAW noisy burst as input, the task in the challenge was to generate a clean RGB\nimage with 4 times higher resolution. The challenge contained two tracks; Track\n1 evaluating on synthetically generated data, and Track 2 using real-world\nbursts from mobile camera. In the final testing phase, 6 teams submitted\nresults using a diverse set of solutions. The top-performing methods set a new\nstate-of-the-art for the burst super-resolution task.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:55:28 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bhat", "Goutam", ""], ["Danelljan", "Martin", ""], ["Timofte", "Radu", ""], ["Akita", "Kazutoshi", ""], ["Cho", "Wooyeong", ""], ["Fan", "Haoqiang", ""], ["Jia", "Lanpeng", ""], ["Kim", "Daeshik", ""], ["Lecouat", "Bruno", ""], ["Li", "Youwei", ""], ["Liu", "Shuaicheng", ""], ["Liu", "Ziluan", ""], ["Luo", "Ziwei", ""], ["Maeda", "Takahiro", ""], ["Mairal", "Julien", ""], ["Micheloni", "Christian", ""], ["Mo", "Xuan", ""], ["Oba", "Takeru", ""], ["Ostyakov", "Pavel", ""], ["Ponce", "Jean", ""], ["Son", "Sanghyeok", ""], ["Sun", "Jian", ""], ["Ukita", "Norimichi", ""], ["Umer", "Rao Muhammad", ""], ["Yan", "Youliang", ""], ["Yu", "Lei", ""], ["Zhussip", "Magauiya", ""], ["Zou", "Xueyi", ""]]}, {"id": "2106.03844", "submitter": "Tal Reiss", "authors": "Tal Reiss, Yedid Hoshen", "title": "Mean-Shifted Contrastive Loss for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep anomaly detection methods learn representations that separate between\nnormal and anomalous samples. Very effective representations are obtained when\npowerful externally trained feature extractors (e.g. ResNets pre-trained on\nImageNet) are fine-tuned on the training data which consists of normal samples\nand no anomalies. However, this is a difficult task that can suffer from\ncatastrophic collapse, i.e. it is prone to learning trivial and non-specific\nfeatures. In this paper, we propose a new loss function which can overcome\nfailure modes of both center-loss and contrastive-loss methods. Furthermore, we\ncombine it with a confidence-invariant angular center loss, which replaces the\nEuclidean distance used in previous work, that was sensitive to prediction\nconfidence. Our improvements yield a new anomaly detection approach, based on\n$\\textit{Mean-Shifted Contrastive Loss}$, which is both more accurate and less\nsensitive to catastrophic collapse than previous methods. Our method achieves\nstate-of-the-art anomaly detection performance on multiple benchmarks including\n$97.5\\%$ ROC-AUC on the CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:58:03 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Reiss", "Tal", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2106.03847", "submitter": "Omri Avrahami", "authors": "Omri Avrahami, Dani Lischinski, Ohad Fried", "title": "GAN Cocktail: mixing GANs without dataset access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Today's generative models are capable of synthesizing high-fidelity images,\nbut each model specializes on a specific target domain. This raises the need\nfor model merging: combining two or more pretrained generative models into a\nsingle unified one. In this work we tackle the problem of model merging, given\ntwo constraints that often come up in the real world: (1) no access to the\noriginal training data, and (2) without increasing the size of the neural\nnetwork. To the best of our knowledge, model merging under these constraints\nhas not been studied thus far. We propose a novel, two-stage solution. In the\nfirst stage, we transform the weights of all the models to the same parameter\nspace by a technique we term model rooting. In the second stage, we merge the\nrooted models by averaging their weights and fine-tuning them for each specific\ndomain, using only data generated by the original trained models. We\ndemonstrate that our approach is superior to baseline methods and to existing\ntransfer learning techniques, and investigate several applications.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:59:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Avrahami", "Omri", ""], ["Lischinski", "Dani", ""], ["Fried", "Ohad", ""]]}, {"id": "2106.03849", "submitter": "Rishabh Kabra", "authors": "Rishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia\n  Creswell, Matthew Botvinick, Alexander Lerchner, Christopher P. Burgess", "title": "SIMONe: View-Invariant, Temporally-Abstracted Object Representations via\n  Unsupervised Video Decomposition", "comments": "Animated figures are available at\n  https://sites.google.com/view/simone-scene-understanding/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help agents reason about scenes in terms of their building blocks, we wish\nto extract the compositional structure of any given scene (in particular, the\nconfiguration and characteristics of objects comprising the scene). This\nproblem is especially difficult when scene structure needs to be inferred while\nalso estimating the agent's location/viewpoint, as the two variables jointly\ngive rise to the agent's observations. We present an unsupervised variational\napproach to this problem. Leveraging the shared structure that exists across\ndifferent scenes, our model learns to infer two sets of latent representations\nfrom RGB video input alone: a set of \"object\" latents, corresponding to the\ntime-invariant, object-level contents of the scene, as well as a set of \"frame\"\nlatents, corresponding to global time-varying elements such as viewpoint. This\nfactorization of latents allows our model, SIMONe, to represent object\nattributes in an allocentric manner which does not depend on viewpoint.\nMoreover, it allows us to disentangle object dynamics and summarize their\ntrajectories as time-abstracted, view-invariant, per-object properties. We\ndemonstrate these capabilities, as well as the model's performance in terms of\nview synthesis and instance segmentation, across three procedurally generated\nvideo datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:59:23 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kabra", "Rishabh", ""], ["Zoran", "Daniel", ""], ["Erdogan", "Goker", ""], ["Matthey", "Loic", ""], ["Creswell", "Antonia", ""], ["Botvinick", "Matthew", ""], ["Lerchner", "Alexander", ""], ["Burgess", "Christopher P.", ""]]}, {"id": "2106.03899", "submitter": "Sina Mohseni", "authors": "Sina Mohseni and Arash Vahdat and Jay Yadawa", "title": "Multi-task Transformation Learning for Robust Out-of-Distribution\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting out-of-distribution (OOD) samples plays a key role in open-world\nand safety-critical applications such as autonomous systems and healthcare.\nSelf-supervised representation learning techniques (e.g., contrastive learning\nand pretext learning) are well suited for learning representation that can\nidentify OOD samples. In this paper, we propose a simple framework that\nleverages multi-task transformation learning for training effective\nrepresentation for OOD detection which outperforms state-of-the-art OOD\ndetection performance and robustness on several image datasets. We empirically\nobserve that the OOD performance depends on the choice of data transformations\nwhich itself depends on the in-domain training set. To address this problem, we\npropose a simple mechanism for selecting the transformations automatically and\nmodulate their effect on representation learning without requiring any OOD\ntraining samples. We characterize the criteria for a desirable OOD detector for\nreal-world applications and demonstrate the efficacy of our proposed technique\nagainst a diverse range of the state-of-the-art OOD detection techniques.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 18:18:26 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Mohseni", "Sina", ""], ["Vahdat", "Arash", ""], ["Yadawa", "Jay", ""]]}, {"id": "2106.03905", "submitter": "Abdullah Aleem", "authors": "Abdullah Aleem, Manoj Prabhakar Nallabothula, Pete Setabutr, Joelle A.\n  Hallak and Darvin Yi", "title": "AutoPtosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blepharoptosis, or ptosis as it is more commonly referred to, is a condition\nof the eyelid where the upper eyelid droops. The current diagnosis for ptosis\ninvolves cumbersome manual measurements that are time-consuming and prone to\nhuman error. In this paper, we present AutoPtosis, an artificial intelligence\nbased system with interpretable results for rapid diagnosis of ptosis. We\nutilize a diverse dataset collected from the Illinois Ophthalmic Database Atlas\n(I-ODA) to develop a robust deep learning model for prediction and also develop\na clinically inspired model that calculates the marginal reflex distance and\niris ratio. AutoPtosis achieved 95.5% accuracy on physician verified data that\nhad an equal class balance. The proposed algorithm can help in the rapid and\ntimely diagnosis of ptosis, significantly reduce the burden on the healthcare\nsystem, and save the patients and clinics valuable resources.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 18:32:31 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:41:00 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Aleem", "Abdullah", ""], ["Nallabothula", "Manoj Prabhakar", ""], ["Setabutr", "Pete", ""], ["Hallak", "Joelle A.", ""], ["Yi", "Darvin", ""]]}, {"id": "2106.03911", "submitter": "Kevin Zakka", "authors": "Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette\n  Bohg, Debidatta Dwibedi", "title": "XIRL: Cross-embodiment Inverse Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the visual cross-embodiment imitation setting, in which agents\nlearn policies from videos of other agents (such as humans) demonstrating the\nsame task, but with stark differences in their embodiments -- shape, actions,\nend-effector dynamics, etc. In this work, we demonstrate that it is possible to\nautomatically discover and learn vision-based reward functions from\ncross-embodiment demonstration videos that are robust to these differences.\nSpecifically, we present a self-supervised method for Cross-embodiment Inverse\nReinforcement Learning (XIRL) that leverages temporal cycle-consistency\nconstraints to learn deep visual embeddings that capture task progression from\noffline videos of demonstrations across multiple expert agents, each performing\nthe same task differently due to embodiment differences. Prior to our work,\nproducing rewards from self-supervised embeddings has typically required\nalignment with a reference trajectory, which may be difficult to acquire. We\nshow empirically that if the embeddings are aware of task-progress, simply\ntaking the negative distance between the current state and goal state in the\nlearned embedding space is useful as a reward for training policies with\nreinforcement learning. We find our learned reward function not only works for\nembodiments seen during training, but also generalizes to entirely new\nembodiments. We also find that XIRL policies are more sample efficient than\nbaselines, and in some cases exceed the sample efficiency of the same agent\ntrained with ground truth sparse rewards.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 18:45:07 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zakka", "Kevin", ""], ["Zeng", "Andy", ""], ["Florence", "Pete", ""], ["Tompson", "Jonathan", ""], ["Bohg", "Jeannette", ""], ["Dwibedi", "Debidatta", ""]]}, {"id": "2106.03932", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Maja Taseska, Gerhard Rigoll", "title": "How to Design a Three-Stage Architecture for Audio-Visual Active Speaker\n  Detection in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Successful active speaker detection requires a three-stage pipeline: (i)\naudio-visual encoding for all speakers in the clip, (ii) inter-speaker relation\nmodeling between a reference speaker and the background speakers within each\nframe, and (iii) temporal modeling for the reference speaker. Each stage of\nthis pipeline plays an important role for the final performance of the created\narchitecture. Based on a series of controlled experiments, this work presents\nseveral practical guidelines for audio-visual active speaker detection.\nCorrespondingly, we present a new architecture called ASDNet, which achieves a\nnew state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%\noutperforming the second best with a large margin of 4.7%. Our code and\npretrained models are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 19:44:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Taseska", "Maja", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "2106.03941", "submitter": "Guangyu Ren", "authors": "Guangyu Ren, Yanchu Xie, Tianhong Dai, Tania Stathaki", "title": "Progressive Multi-scale Fusion Network for RGB-D Salient Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection(SOD) aims at locating the most significant object\nwithin a given image. In recent years, great progress has been made in applying\nSOD on many vision tasks. The depth map could provide additional spatial prior\nand boundary cues to boost the performance. Combining the depth information\nwith image data obtained from standard visual cameras has been widely used in\nrecent SOD works, however, introducing depth information in a suboptimal fusion\nstrategy may have negative influence in the performance of SOD. In this paper,\nwe discuss about the advantages of the so-called progressive multi-scale fusion\nmethod and propose a mask-guided feature aggregation module(MGFA). The proposed\nframework can effectively combine the two features of different modalities and,\nfurthermore, alleviate the impact of erroneous depth features, which are\ninevitably caused by the variation of depth quality. We further introduce a\nmask-guided refinement module(MGRM) to complement the high-level semantic\nfeatures and reduce the irrelevant features from multi-scale fusion, leading to\nan overall refinement of detection. Experiments on five challenging benchmarks\ndemonstrate that the proposed method outperforms 11 state-of-the-art methods\nunder different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:02:39 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ren", "Guangyu", ""], ["Xie", "Yanchu", ""], ["Dai", "Tianhong", ""], ["Stathaki", "Tania", ""]]}, {"id": "2106.03956", "submitter": "Krishna Regmi", "authors": "Sarah Shiraz, Krishna Regmi, Shruti Vyas, Yogesh S. Rawat, Mubarak\n  Shah", "title": "Novel View Video Prediction Using a Dual Representation", "comments": "Accepted in ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of novel view video prediction; given a set of input\nvideo clips from a single/multiple views, our network is able to predict the\nvideo from a novel view. The proposed approach does not require any priors and\nis able to predict the video from wider angular distances, upto 45 degree, as\ncompared to the recent studies predicting small variations in viewpoint.\nMoreover, our method relies only onRGB frames to learn a dual representation\nwhich is used to generate the video from a novel viewpoint. The dual\nrepresentation encompasses a view-dependent and a global representation which\nincorporates complementary details to enable novel view video prediction. We\ndemonstrate the effectiveness of our framework on two real world datasets:\nNTU-RGB+D and CMU Panoptic. A comparison with the State-of-the-art novel view\nvideo prediction methods shows an improvement of 26.1% in SSIM, 13.6% in PSNR,\nand 60% inFVD scores without using explicit priors from target views.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:41:33 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shiraz", "Sarah", ""], ["Regmi", "Krishna", ""], ["Vyas", "Shruti", ""], ["Rawat", "Yogesh S.", ""], ["Shah", "Mubarak", ""]]}, {"id": "2106.03959", "submitter": "Zhiwu Huang", "authors": "Rhea Sanjay Sukthanker, Zhiwu Huang, Suryansh Kumar, Radu Timofte, Luc\n  Van Gool", "title": "Generative Flows with Invertible Attentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-based generative models have shown excellent ability to explicitly learn\nthe probability density function of data via a sequence of invertible\ntransformations. Yet, modeling long-range dependencies over normalizing flows\nremains understudied. To fill the gap, in this paper, we introduce two types of\ninvertible attention mechanisms for generative flow models. To be precise, we\npropose map-based and scaled dot-product attention for unconditional and\nconditional generative flow models. The key idea is to exploit split-based\nattention mechanisms to learn the attention weights and input representations\non every two splits of flow feature maps. Our method provides invertible\nattention modules with tractable Jacobian determinants, enabling seamless\nintegration of it at any positions of the flow-based models. The proposed\nattention mechanism can model the global data dependencies, leading to more\ncomprehensive flow models. Evaluation on multiple generation tasks demonstrates\nthat the introduced attention flow idea results in efficient flow models and\ncompares favorably against the state-of-the-art unconditional and conditional\ngenerative flow methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:43:04 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 16:11:56 GMT"}, {"version": "v3", "created": "Sat, 26 Jun 2021 07:24:08 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sukthanker", "Rhea Sanjay", ""], ["Huang", "Zhiwu", ""], ["Kumar", "Suryansh", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "2106.03987", "submitter": "Udaranga Wickramasinghe", "authors": "Udaranga Wickramasinghe and Pascal Fua", "title": "Weakly Supervised Volumetric Image Segmentation with Deformed Templates", "comments": "13 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many approaches that use weak-supervision to train networks to\nsegment 2D images. By contrast, existing 3D approaches rely on full-supervision\nof a subset of 2D slices of the 3D image volume. In this paper, we propose an\napproach that is truly weakly-supervised in the sense that we only need to\nprovide a sparse set of 3D point on the surface of target objects, an easy task\nthat can be quickly done. We use the 3D points to deform a 3D template so that\nit roughly matches the target object outlines and we introduce an architecture\nthat exploits the supervision provided by coarse template to train a network to\nfind accurate boundaries.\n  We evaluate the performance of our approach on Computed Tomography (CT),\nMagnetic Resonance Imagery (MRI) and Electron Microscopy (EM) image datasets.\nWe will show that it outperforms a more traditional approach to\nweak-supervision in 3D at a reduced supervision cost.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 22:09:34 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wickramasinghe", "Udaranga", ""], ["Fua", "Pascal", ""]]}, {"id": "2106.04004", "submitter": "Jiaman Li", "authors": "Jiaman Li, Ruben Villegas, Duygu Ceylan, Jimei Yang, Zhengfei Kuang,\n  Hao Li, Yajie Zhao", "title": "Task-Generic Hierarchical Human Motion Prior using VAEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep generative model that describes human motions can benefit a wide range\nof fundamental computer vision and graphics tasks, such as providing robustness\nto video-based human pose estimation, predicting complete body movements for\nmotion capture systems during occlusions, and assisting key frame animation\nwith plausible movements. In this paper, we present a method for learning\ncomplex human motions independent of specific tasks using a combined global and\nlocal latent space to facilitate coarse and fine-grained modeling.\nSpecifically, we propose a hierarchical motion variational autoencoder (HM-VAE)\nthat consists of a 2-level hierarchical latent space. While the global latent\nspace captures the overall global body motion, the local latent space enables\nto capture the refined poses of the different body parts. We demonstrate the\neffectiveness of our hierarchical motion variational autoencoder in a variety\nof tasks including video-based human pose estimation, motion completion from\npartial observations, and motion synthesis from sparse key-frames. Even though,\nour model has not been trained for any of these tasks specifically, it provides\nsuperior performance than task-specific alternatives. Our general-purpose human\nmotion prior model can fix corrupted human body animations and generate\ncomplete movements from incomplete observations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 23:11:42 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Jiaman", ""], ["Villegas", "Ruben", ""], ["Ceylan", "Duygu", ""], ["Yang", "Jimei", ""], ["Kuang", "Zhengfei", ""], ["Li", "Hao", ""], ["Zhao", "Yajie", ""]]}, {"id": "2106.04007", "submitter": "Brandon Wagstaff", "authors": "Brandon Wagstaff and Valentin Peretroukhin and Jonathan Kelly", "title": "Self-Supervised Structure-from-Motion through Tightly-Coupled Depth and\n  Egomotion Networks", "comments": "Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much recent literature has formulated structure-from-motion (SfM) as a\nself-supervised learning problem where the goal is to jointly learn neural\nnetwork models of depth and egomotion through view synthesis. Herein, we\naddress the open problem of how to optimally couple the depth and egomotion\nnetwork components. Toward this end, we introduce several notions of coupling,\ncategorize existing approaches, and present a novel tightly-coupled approach\nthat leverages the interdependence of depth and egomotion at training and at\ninference time. Our approach uses iterative view synthesis to recursively\nupdate the egomotion network input, permitting contextual information to be\npassed between the components without explicit weight sharing. Through\nsubstantial experiments, we demonstrate that our approach promotes consistency\nbetween the depth and egomotion predictions at test time, improves\ngeneralization on new data, and leads to state-of-the-art accuracy on indoor\nand outdoor depth and egomotion evaluation benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 23:30:45 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wagstaff", "Brandon", ""], ["Peretroukhin", "Valentin", ""], ["Kelly", "Jonathan", ""]]}, {"id": "2106.04010", "submitter": "Debadeepta Dey", "authors": "Debadeepta Dey, Shital Shah, Sebastien Bubeck", "title": "FEAR: A Simple Lightweight Method to Rank Architectures", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fundamental problem in Neural Architecture Search (NAS) is to efficiently\nfind high-performing architectures from a given search space. We propose a\nsimple but powerful method which we call FEAR, for ranking architectures in any\nsearch space. FEAR leverages the viewpoint that neural networks are powerful\nnon-linear feature extractors. First, we train different architectures in the\nsearch space to the same training or validation error. Then, we compare the\nusefulness of the features extracted by each architecture. We do so with a\nquick training keeping most of the architecture frozen. This gives fast\nestimates of the relative performance. We validate FEAR on Natsbench topology\nsearch space on three different datasets against competing baselines and show\nstrong ranking correlation especially compared to recently proposed zero-cost\nmethods. FEAR particularly excels at ranking high-performance architectures in\nthe search space. When used in the inner loop of discrete search algorithms\nlike random search, FEAR can cut down the search time by approximately 2.4X\nwithout losing accuracy. We additionally empirically study very recently\nproposed zero-cost measures for ranking and find that they breakdown in ranking\nperformance as training proceeds and also that data-agnostic ranking scores\nwhich ignore the dataset do not generalize across dissimilar datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 23:38:21 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Dey", "Debadeepta", ""], ["Shah", "Shital", ""], ["Bubeck", "Sebastien", ""]]}, {"id": "2106.04024", "submitter": "Serguei Barannikov", "authors": "Serguei Barannikov, Ilya Trofimov, Grigorii Sotnikov, Ekaterina\n  Trimbach, Alexander Korotin, Alexander Filippov, Evgeny Burnaev", "title": "Manifold Topology Divergence: a Framework for Comparing Data Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.AT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We develop a framework for comparing data manifolds, aimed, in particular,\ntowards the evaluation of deep generative models. We describe a novel tool,\nCross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional\nspace, tracks multiscale topology spacial discrepancies between manifolds on\nwhich the distributions are concentrated. Based on the Cross-Barcode, we\nintroduce the Manifold Topology Divergence score (MTop-Divergence) and apply it\nto assess the performance of deep generative models in various domains: images,\n3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN,\nCIFAR10, FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate\nthat the MTop-Divergence accurately detects various degrees of mode-dropping,\nintra-mode collapse, mode invention, and image disturbance. Our algorithm\nscales well (essentially linearly) with the increase of the dimension of the\nambient high-dimensional space. It is one of the first TDA-based practical\nmethodologies that can be applied universally to datasets of different sizes\nand dimensions, including the ones on which the most recent GANs in the visual\ndomain are trained. The proposed method is domain agnostic and does not rely on\npre-trained networks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 00:30:43 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Barannikov", "Serguei", ""], ["Trofimov", "Ilya", ""], ["Sotnikov", "Grigorii", ""], ["Trimbach", "Ekaterina", ""], ["Korotin", "Alexander", ""], ["Filippov", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2106.04025", "submitter": "Taehun Kim", "authors": "Taehun Kim, Jinseong Kim, Daijin Kim", "title": "SpaceMeshLab: Spatial Context Memoization and Meshgrid Atrous\n  Convolution Consensus for Semantic Segmentation", "comments": "5 pages, 3 figures, 4 tables. To appear in the proceedings of the\n  28th IEEE International Conference on Image Processing (IEEE - ICIP),\n  September 19-22, 2021, Anchorage, Alaska, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation networks adopt transfer learning from image\nclassification networks which occurs a shortage of spatial context information.\nFor this reason, we propose Spatial Context Memoization (SpaM), a bypassing\nbranch for spatial context by retaining the input dimension and constantly\ncommunicating its spatial context and rich semantic information mutually with\nthe backbone network. Multi-scale context information for semantic segmentation\nis crucial for dealing with diverse sizes and shapes of target objects in the\ngiven scene. Conventional multi-scale context scheme adopts multiple effective\nreceptive fields by multiple dilation rates or pooling operations, but often\nsuffer from misalignment problem with respect to the target pixel. To this end,\nwe propose Meshgrid Atrous Convolution Consensus (MetroCon^2) which brings\nmulti-scale scheme into fine-grained multi-scale object context using\nconvolutions with meshgrid-like scattered dilation rates. SpaceMeshLab\n(ResNet-101 + SpaM + MetroCon^2) achieves 82.0% mIoU in Cityscapes test and\n53.5% mIoU on Pascal-Context validation set.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 00:38:02 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kim", "Taehun", ""], ["Kim", "Jinseong", ""], ["Kim", "Daijin", ""]]}, {"id": "2106.04026", "submitter": "Dae-Hyeok Lee", "authors": "Dae-Hyeok Lee, Dong-Kyun Han, Sung-Jin Kim, Ji-Hoon Jeong, and\n  Seong-Whan Lee", "title": "Subject-Independent Brain-Computer Interface for Decoding High-Level\n  Visual Imagery Tasks", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interface (BCI) is used for communication between humans and\ndevices by recognizing status and intention of humans. Communication between\nhumans and a drone using electroencephalogram (EEG) signals is one of the most\nchallenging issues in the BCI domain. In particular, the control of drone\nswarms (the direction and formation) has more advantages compared to the\ncontrol of a drone. The visual imagery (VI) paradigm is that subjects visually\nimagine specific objects or scenes. Reduction of the variability among EEG\nsignals of subjects is essential for practical BCI-based systems. In this\nstudy, we proposed the subepoch-wise feature encoder (SEFE) to improve the\nperformances in the subject-independent tasks by using the VI dataset. This\nstudy is the first attempt to demonstrate the possibility of generalization\namong subjects in the VI-based BCI. We used the leave-one-subject-out\ncross-validation for evaluating the performances. We obtained higher\nperformances when including our proposed module than excluding our proposed\nmodule. The DeepConvNet with SEFE showed the highest performance of 0.72 among\nsix different decoding models. Hence, we demonstrated the feasibility of\ndecoding the VI dataset in the subject-independent task with robust\nperformances by using our proposed module.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 00:39:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Lee", "Dae-Hyeok", ""], ["Han", "Dong-Kyun", ""], ["Kim", "Sung-Jin", ""], ["Jeong", "Ji-Hoon", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2106.04051", "submitter": "Haoxuan You", "authors": "Yang Hu, Haoxuan You, Zhecan Wang, Zhicheng Wang, Erjin Zhou, Yue Gao", "title": "Graph-MLP: Node Classification without Message Passing in Graph", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing\nwith non-Euclidean structural data. Both spatial-based and spectral-based GNNs\nare relying on adjacency matrix to guide message passing among neighbors during\nfeature aggregation. Recent works have mainly focused on powerful message\npassing modules, however, in this paper, we show that none of the message\npassing modules is necessary. Instead, we propose a pure\nmultilayer-perceptron-based framework, Graph-MLP with the supervision signal\nleveraging graph structure, which is sufficient for learning discriminative\nnode representation. In model-level, Graph-MLP only includes multi-layer\nperceptrons, activation function, and layer normalization. In the loss level,\nwe design a neighboring contrastive (NContrast) loss to bridge the gap between\nGNNs and MLPs by utilizing the adjacency information implicitly. This design\nallows our model to be lighter and more robust when facing large-scale graph\ndata and corrupted adjacency information. Extensive experiments prove that even\nwithout adjacency information in testing phase, our framework can still reach\ncomparable and even superior performance against the state-of-the-art models in\nthe graph node classification task.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 02:07:21 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Hu", "Yang", ""], ["You", "Haoxuan", ""], ["Wang", "Zhecan", ""], ["Wang", "Zhicheng", ""], ["Zhou", "Erjin", ""], ["Gao", "Yue", ""]]}, {"id": "2106.04053", "submitter": "Mingjie Sun", "authors": "Mingjie Sun, Jimin Xiao, Eng Gee Lim, Si Liu, John Y. Goulermas", "title": "Discriminative Triad Matching and Reconstruction for Weakly Referring\n  Expression Grounding", "comments": "TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are tackling the weakly-supervised referring expression\ngrounding task, for the localization of a referent object in an image according\nto a query sentence, where the mapping between image regions and queries are\nnot available during the training stage. In traditional methods, an object\nregion that best matches the referring expression is picked out, and then the\nquery sentence is reconstructed from the selected region, where the\nreconstruction difference serves as the loss for back-propagation. The existing\nmethods, however, conduct both the matching and the reconstruction\napproximately as they ignore the fact that the matching correctness is unknown.\nTo overcome this limitation, a discriminative triad is designed here as the\nbasis to the solution, through which a query can be converted into one or\nmultiple discriminative triads in a very scalable way. Based on the\ndiscriminative triad, we further propose the triad-level matching and\nreconstruction modules which are lightweight yet effective for the\nweakly-supervised training, making it three times lighter and faster than the\nprevious state-of-the-art methods. One important merit of our work is its\nsuperior performance despite the simple and neat design. Specifically, the\nproposed method achieves a new state-of-the-art accuracy when evaluated on\nRefCOCO (39.21%), RefCOCO+ (39.18%) and RefCOCOg (43.24%) datasets, that is\n4.17%, 4.08% and 7.8% higher than the previous one, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 02:15:11 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sun", "Mingjie", ""], ["Xiao", "Jimin", ""], ["Lim", "Eng Gee", ""], ["Liu", "Si", ""], ["Goulermas", "John Y.", ""]]}, {"id": "2106.04054", "submitter": "Bingfeng Zhang", "authors": "Bingfeng Zhang, Jimin Xiao, Jianbo Jiao, Yunchao Wei, Yao Zhao", "title": "Affinity Attention Graph Neural Network for Weakly Supervised Semantic\n  Segmentation", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TAPMI 2021)", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3083269", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised semantic segmentation is receiving great attention due to\nits low human annotation cost. In this paper, we aim to tackle bounding box\nsupervised semantic segmentation, i.e., training accurate semantic segmentation\nmodels using bounding box annotations as supervision. To this end, we propose\nAffinity Attention Graph Neural Network ($A^2$GNN). Following previous\npractices, we first generate pseudo semantic-aware seeds, which are then formed\ninto semantic graphs based on our newly proposed affinity Convolutional Neural\nNetwork (CNN). Then the built graphs are input to our $A^2$GNN, in which an\naffinity attention layer is designed to acquire the short- and long- distance\ninformation from soft graph edges to accurately propagate semantic labels from\nthe confident seeds to the unlabeled pixels. However, to guarantee the\nprecision of the seeds, we only adopt a limited number of confident pixel seed\nlabels for $A^2$GNN, which may lead to insufficient supervision for training.\nTo alleviate this issue, we further introduce a new loss function and a\nconsistency-checking mechanism to leverage the bounding box constraint, so that\nmore reliable guidance can be included for the model optimization. Experiments\nshow that our approach achieves new state-of-the-art performances on Pascal VOC\n2012 datasets (val: 76.5\\%, test: 75.2\\%). More importantly, our approach can\nbe readily applied to bounding box supervised instance segmentation task or\nother weakly supervised semantic segmentation tasks, with state-of-the-art or\ncomparable performance among almot all weakly supervised tasks on PASCAL VOC or\nCOCO dataset. Our source code will be available at\nhttps://github.com/zbf1991/A2GNN.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 02:19:21 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhang", "Bingfeng", ""], ["Xiao", "Jimin", ""], ["Jiao", "Jianbo", ""], ["Wei", "Yunchao", ""], ["Zhao", "Yao", ""]]}, {"id": "2106.04066", "submitter": "Wenhao Ding", "authors": "Wenhao Ding, Bo Li, Kim Ji Eun, Ding Zhao", "title": "Semantically Controllable Scene Generation with Guidance of Explicit\n  Knowledge", "comments": "14 pages, 6 figures, Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Generative Models (DGMs) are known for their superior capability in\ngenerating realistic data. Extending purely data-driven approaches, recent\nspecialized DGMs may satisfy additional controllable requirements such as\nembedding a traffic sign in a driving scene, by manipulating patterns\n\\textit{implicitly} in the neuron or feature level. In this paper, we introduce\na novel method to incorporate domain knowledge \\textit{explicitly} in the\ngeneration process to achieve semantically controllable scene generation. We\ncategorize our knowledge into two types to be consistent with the composition\nof natural scenes, where the first type represents the property of objects and\nthe second type represents the relationship among objects. We then propose a\ntree-structured generative model to learn complex scene representation, whose\nnodes and edges are naturally corresponding to the two types of knowledge\nrespectively. Knowledge can be explicitly integrated to enable semantically\ncontrollable scene generation by imposing semantic rules on properties of nodes\nand edges in the tree structure. We construct a synthetic example to illustrate\nthe controllability and explainability of our method in a clean setting. We\nfurther extend the synthetic example to realistic autonomous vehicle driving\nenvironments and conduct extensive experiments to show that our method\nefficiently identifies adversarial traffic scenes against different\nstate-of-the-art 3D point cloud segmentation models satisfying the traffic\nrules specified as the explicit knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 02:51:33 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 20:30:13 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 19:08:51 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ding", "Wenhao", ""], ["Li", "Bo", ""], ["Eun", "Kim Ji", ""], ["Zhao", "Ding", ""]]}, {"id": "2106.04067", "submitter": "Ruizhi Shao", "authors": "Ruizhi Shao, Gaochang Wu, Yuemei Zhou, Ying Fu, Yebin Liu", "title": "LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution\n  Homography Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-resolution image alignment is a key problem in multiscale gigapixel\nphotography, which requires to estimate homography matrix using images with\nlarge resolution gap. Existing deep homography methods concatenate the input\nimages or features, neglecting the explicit formulation of correspondences\nbetween them, which leads to degraded accuracy in cross-resolution challenges.\nIn this paper, we consider the cross-resolution homography estimation as a\nmultimodal problem, and propose a local transformer network embedded within a\nmultiscale structure to explicitly learn correspondences between the multimodal\ninputs, namely, input images with different resolutions. The proposed local\ntransformer adopts a local attention map specifically for each position in the\nfeature. By combining the local transformer with the multiscale structure, the\nnetwork is able to capture long-short range correspondences efficiently and\naccurately. Experiments on both the MS-COCO dataset and the real-captured\ncross-resolution dataset show that the proposed network outperforms existing\nstate-of-the-art feature-based and deep-learning-based homography estimation\nmethods, and is able to accurately align images under $10\\times$ resolution\ngap.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 02:51:45 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 04:55:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Shao", "Ruizhi", ""], ["Wu", "Gaochang", ""], ["Zhou", "Yuemei", ""], ["Fu", "Ying", ""], ["Liu", "Yebin", ""]]}, {"id": "2106.04073", "submitter": "Lin Sui", "authors": "Lin Sui, Chen-Lin Zhang, Jianxin Wu", "title": "Salvage of Supervision in Weakly Supervised Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection (WSOD) has recently attracted much\nattention. However, the method, performance and speed gaps between WSOD and\nfully supervised detection prevent WSOD from being applied in real-world tasks.\nTo bridge the gaps, this paper proposes a new framework, Salvage of Supervision\n(SoS), with the key idea being to harness every potentially useful supervisory\nsignal in WSOD: the weak image-level labels, the pseudo-labels, and the power\nof semi-supervised object detection. This paper shows that each type of\nsupervisory signal brings in notable improvements, outperforms existing WSOD\nmethods (which mainly use only the weak labels) by large margins. The proposed\nSoS-WSOD method achieves 64.4 $m\\text{AP}_{50}$ on VOC2007, 61.9\n$m\\text{AP}_{50}$ on VOC2012 and 16.4 $m\\text{AP}_{50:95}$ on MS-COCO, and also\nhas fast inference speed. Ablations and visualization further verify the\neffectiveness of SoS.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 03:13:24 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Sui", "Lin", ""], ["Zhang", "Chen-Lin", ""], ["Wu", "Jianxin", ""]]}, {"id": "2106.04090", "submitter": "Zhi-Song Liu", "authors": "Zhi-Song Liu and Wan-Chi Siu and Li-Wen Wang", "title": "Variational AutoEncoder for Reference based Image Super-Resolution", "comments": "10 pages, 6 figures", "journal-ref": "2021 IEEE Conference on Computer Vision and Pattern Recognition\n  Workshop", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel reference based image super-resolution\napproach via Variational AutoEncoder (RefVAE). Existing state-of-the-art\nmethods mainly focus on single image super-resolution which cannot perform well\non large upsampling factors, e.g., 8$\\times$. We propose a reference based\nimage super-resolution, for which any arbitrary image can act as a reference\nfor super-resolution. Even using random map or low-resolution image itself, the\nproposed RefVAE can transfer the knowledge from the reference to the\nsuper-resolved images. Depending upon different references, the proposed method\ncan generate different versions of super-resolved images from a hidden\nsuper-resolution space. Besides using different datasets for some standard\nevaluations with PSNR and SSIM, we also took part in the NTIRE2021 SR Space\nchallenge and have provided results of the randomness evaluation of our\napproach. Compared to other state-of-the-art methods, our approach achieves\nhigher diverse scores.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 04:12:38 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Zhi-Song", ""], ["Siu", "Wan-Chi", ""], ["Wang", "Li-Wen", ""]]}, {"id": "2106.04095", "submitter": "Yulin Li", "authors": "Yulin Li, Jianfeng He, Tianzhu Zhang, Xiang Liu, Yongdong Zhang, Feng\n  Wu", "title": "Diverse Part Discovery: Occluded Person Re-identification with\n  Part-Aware Transformer", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occluded person re-identification (Re-ID) is a challenging task as persons\nare frequently occluded by various obstacles or other persons, especially in\nthe crowd scenario. To address these issues, we propose a novel end-to-end\nPart-Aware Transformer (PAT) for occluded person Re-ID through diverse part\ndiscovery via a transformer encoderdecoder architecture, including a pixel\ncontext based transformer encoder and a part prototype based transformer\ndecoder. The proposed PAT model enjoys several merits. First, to the best of\nour knowledge, this is the first work to exploit the transformer\nencoder-decoder architecture for occluded person Re-ID in a unified deep model.\nSecond, to learn part prototypes well with only identity labels, we design two\neffective mechanisms including part diversity and part discriminability.\nConsequently, we can achieve diverse part discovery for occluded person Re-ID\nin a weakly supervised manner. Extensive experimental results on six\nchallenging benchmarks for three tasks (occluded, partial and holistic Re-ID)\ndemonstrate that our proposed PAT performs favorably against stat-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 04:29:07 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Yulin", ""], ["He", "Jianfeng", ""], ["Zhang", "Tianzhu", ""], ["Liu", "Xiang", ""], ["Zhang", "Yongdong", ""], ["Wu", "Feng", ""]]}, {"id": "2106.04104", "submitter": "Peter Karpov", "authors": "Peter Karpov", "title": "Design of Low-Artifact Interpolation Kernels by Means of Computer\n  Algebra", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a number of new piecewise-polynomial kernels for image\ninterpolation. The kernels are constructed by optimizing a measure of\ninterpolation quality based on the magnitude of anisotropic artifacts. The\nkernel design process is performed symbolically using Mathematica computer\nalgebra system. Experimental evaluation involving 14 image quality assessment\nmethods demonstrates that our results compare favorably with the existing\nlinear interpolators.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 05:06:51 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Karpov", "Peter", ""]]}, {"id": "2106.04108", "submitter": "Sitong Wu", "authors": "Sitong Wu, Tianyi Wu, Fangjian Lin, Shengwei Tian, Guodong Guo", "title": "Fully Transformer Networks for Semantic Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have shown impressive performance in various natural language\nprocessing and computer vision tasks, due to the capability of modeling\nlong-range dependencies. Recent progress has demonstrated to combine such\ntransformers with CNN-based semantic image segmentation models is very\npromising. However, it is not well studied yet on how well a pure transformer\nbased approach can achieve for image segmentation. In this work, we explore a\nnovel framework for semantic image segmentation, which is encoder-decoder based\nFully Transformer Networks (FTN). Specifically, we first propose a Pyramid\nGroup Transformer (PGT) as the encoder for progressively learning hierarchical\nfeatures, while reducing the computation complexity of the standard visual\ntransformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse\nsemantic-level and spatial-level information from multiple levels of the PGT\nencoder for semantic image segmentation. Surprisingly, this simple baseline can\nachieve new state-of-the-art results on multiple challenging semantic\nsegmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. The\nsource code will be released upon the publication of this work.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 05:15:28 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wu", "Sitong", ""], ["Wu", "Tianyi", ""], ["Lin", "Fangjian", ""], ["Tian", "Shengwei", ""], ["Guo", "Guodong", ""]]}, {"id": "2106.04112", "submitter": "Siqi Deng", "authors": "Siqi Deng, Yuanjun Xiong, Meng Wang, Wei Xia, Stefano Soatto", "title": "Harnessing Unrecognizable Faces for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common implementation of face recognition systems as a cascade of a\ndetection stage and a recognition or verification stage can cause problems\nbeyond failures of the detector. When the detector succeeds, it can detect\nfaces that cannot be recognized, no matter how capable the recognition system.\nRecognizability, a latent variable, should therefore be factored into the\ndesign and implementation of face recognition systems. We propose a measure of\nrecognizability of a face image that leverages a key empirical observation: an\nembedding of face images, implemented by a deep neural network trained using\nmostly recognizable identities, induces a partition of the hypersphere whereby\nunrecognizable identities cluster together. This occurs regardless of the\nphenomenon that causes a face to be unrecognizable, it be optical or motion\nblur, partial occlusion, spatial quantization, poor illumination. Therefore, we\nuse the distance from such an \"unrecognizable identity\" as a measure of\nrecognizability, and incorporate it in the design of the over-all system. We\nshow that accounting for recognizability reduces error rate of single-image\nface recognition by 58% at FAR=1e-5 on the IJB-C Covariate Verification\nbenchmark, and reduces verification error rate by 24% at FAR=1e-5 in set-based\nrecognition on the IJB-C benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 05:25:03 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Deng", "Siqi", ""], ["Xiong", "Yuanjun", ""], ["Wang", "Meng", ""], ["Xia", "Wei", ""], ["Soatto", "Stefano", ""]]}, {"id": "2106.04121", "submitter": "Bowen Shi", "authors": "Bowen Shi, Xiaopeng Zhang, Haohang Xu, Wenrui Dai, Junni Zou, Hongkai\n  Xiong, Qi Tian", "title": "Multi-dataset Pretraining: A Unified Model for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting annotated data for semantic segmentation is time-consuming and\nhard to scale up. In this paper, we for the first time propose a unified\nframework, termed as Multi-Dataset Pretraining, to take full advantage of the\nfragmented annotations of different datasets. The highlight is that the\nannotations from different domains can be efficiently reused and consistently\nboost performance for each specific domain. This is achieved by first\npretraining the network via the proposed pixel-to-prototype contrastive loss\nover multiple datasets regardless of their taxonomy labels, and followed by\nfine-tuning the pretrained model over specific dataset as usual. In order to\nbetter model the relationship among images and classes from different datasets,\nwe extend the pixel level embeddings via cross dataset mixing and propose a\npixel-to-class sparse coding strategy that explicitly models the pixel-class\nsimilarity over the manifold embedding space. In this way, we are able to\nincrease intra-class compactness and inter-class separability, as well as\nconsidering inter-class similarity across different datasets for better\ntransferability. Experiments conducted on several benchmarks demonstrate its\nsuperior performance. Notably, MDP consistently outperforms the pretrained\nmodels over ImageNet by a considerable margin, while only using less than 10%\nsamples for pretraining.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:13:11 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shi", "Bowen", ""], ["Zhang", "Xiaopeng", ""], ["Xu", "Haohang", ""], ["Dai", "Wenrui", ""], ["Zou", "Junni", ""], ["Xiong", "Hongkai", ""], ["Tian", "Qi", ""]]}, {"id": "2106.04127", "submitter": "Sixing Yin", "authors": "Sixing Yin, Yameng Han, Shufang Li", "title": "Left Ventricle Contouring in Cardiac Images Based on Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image segmentation is one of the important tasks of computer-aided\ndiagnosis in medical image analysis. Since most medical images have the\ncharacteristics of blurred boundaries and uneven intensity distribution,\nthrough existing segmentation methods, the discontinuity within the target area\nand the discontinuity of the target boundary are likely to lead to rough or\neven erroneous boundary delineation. In this paper, we propose a new iterative\nrefined interactive segmentation method for medical images based on agent\nreinforcement learning, which focuses on the problem of target segmentation\nboundaries. We model the dynamic process of drawing the target contour in a\ncertain order as a Markov Decision Process (MDP) based on a deep reinforcement\nlearning method. In the dynamic process of continuous interaction between the\nagent and the image, the agent tracks the boundary point by point in order\nwithin a limited length range until the contour of the target is completely\ndrawn. In this process, the agent can quickly improve the segmentation\nperformance by exploring an interactive policy in the image. The method we\nproposed is simple and effective. At the same time, we evaluate our method on\nthe cardiac MRI scan data set. Experimental results show that our method has a\nbetter segmentation effect on the left ventricle in a small number of medical\nimage data sets, especially in terms of segmentation boundaries, this method is\nbetter than existing methods. Based on our proposed method, the dynamic\ngeneration process of the predicted contour trajectory of the left ventricle\nwill be displayed online at https://github.com/H1997ym/LV-contour-trajectory.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:30:32 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yin", "Sixing", ""], ["Han", "Yameng", ""], ["Li", "Shufang", ""]]}, {"id": "2106.04128", "submitter": "Yifei Yuan", "authors": "Yifei Yuan and Wai Lam", "title": "Conversational Fashion Image Retrieval via Multiturn Natural Language\n  Feedback", "comments": "Accepted by SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of conversational fashion image retrieval via multiturn\nnatural language feedback. Most previous studies are based on single-turn\nsettings. Existing models on multiturn conversational fashion image retrieval\nhave limitations, such as employing traditional models, and leading to\nineffective performance. We propose a novel framework that can effectively\nhandle conversational fashion image retrieval with multiturn natural language\nfeedback texts. One characteristic of the framework is that it searches for\ncandidate images based on exploitation of the encoded reference image and\nfeedback text information together with the conversation history. Furthermore,\nthe image fashion attribute information is leveraged via a mutual attention\nstrategy. Since there is no existing fashion dataset suitable for the multiturn\nsetting of our task, we derive a large-scale multiturn fashion dataset via\nadditional manual annotation efforts on an existing single-turn dataset. The\nexperiments show that our proposed model significantly outperforms existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:34:25 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yuan", "Yifei", ""], ["Lam", "Wai", ""]]}, {"id": "2106.04130", "submitter": "Yuting He", "authors": "Yuting He, Rongjun Ge, Xiaoming Qi, Guanyu Yang, Yang Chen, Youyong\n  Kong, Huazhong Shu, Jean-Louis Coatrieux, Shuo Li", "title": "EnMcGAN: Adversarial Ensemble Learning for 3D Complete Renal Structures\n  Segmentation", "comments": null, "journal-ref": "Information Processing in Medical Imaging (IPMI) 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D complete renal structures(CRS) segmentation targets on segmenting the\nkidneys, tumors, renal arteries and veins in one inference. Once successful, it\nwill provide preoperative plans and intraoperative guidance for laparoscopic\npartial nephrectomy(LPN), playing a key role in the renal cancer treatment.\nHowever, no success has been reported in 3D CRS segmentation due to the complex\nshapes of renal structures, low contrast and large anatomical variation. In\nthis study, we utilize the adversarial ensemble learning and propose Ensemble\nMulti-condition GAN(EnMcGAN) for 3D CRS segmentation for the first time. Its\ncontribution is three-fold. 1)Inspired by windowing, we propose the\nmulti-windowing committee which divides CTA image into multiple narrow windows\nwith different window centers and widths enhancing the contrast for salient\nboundaries and soft tissues. And then, it builds an ensemble segmentation model\non these narrow windows to fuse the segmentation superiorities and improve\nwhole segmentation quality. 2)We propose the multi-condition GAN which equips\nthe segmentation model with multiple discriminators to encourage the segmented\nstructures meeting their real shape conditions, thus improving the shape\nfeature extraction ability. 3)We propose the adversarial weighted ensemble\nmodule which uses the trained discriminators to evaluate the quality of\nsegmented structures, and normalizes these evaluation scores for the ensemble\nweights directed at the input image, thus enhancing the ensemble results. 122\npatients are enrolled in this study and the mean Dice coefficient of the renal\nstructures achieves 84.6%. Extensive experiments with promising results on\nrenal structures reveal powerful segmentation accuracy and great clinical\nsignificance in renal cancer treatment.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:40:42 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["He", "Yuting", ""], ["Ge", "Rongjun", ""], ["Qi", "Xiaoming", ""], ["Yang", "Guanyu", ""], ["Chen", "Yang", ""], ["Kong", "Youyong", ""], ["Shu", "Huazhong", ""], ["Coatrieux", "Jean-Louis", ""], ["Li", "Shuo", ""]]}, {"id": "2106.04139", "submitter": "Chao Zhang", "authors": "Takumi Nakane, Xuequan Lu, Haoran Xie, Chao Zhang", "title": "Image Deformation Estimation via Multi-Objective Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The free-form deformation model can represent a wide range of non-rigid\ndeformations by manipulating a control point lattice over the image. However,\ndue to a large number of parameters, it is challenging to fit the free-form\ndeformation model directly to the deformed image for deformation estimation\nbecause of the complexity of the fitness landscape. In this paper, we cast the\nregistration task as a multi-objective optimization problem (MOP) according to\nthe fact that regions affected by each control point overlap with each other.\nSpecifically, by partitioning the template image into several regions and\nmeasuring the similarity of each region independently, multiple objectives are\nbuilt and deformation estimation can thus be realized by solving the MOP with\noff-the-shelf multi-objective evolutionary algorithms (MOEAs). In addition, a\ncoarse-to-fine strategy is realized by image pyramid combined with control\npoint mesh subdivision. Specifically, the optimized candidate solutions of the\ncurrent image level are inherited by the next level, which increases the\nability to deal with large deformation. Also, a post-processing procedure is\nproposed to generate a single output utilizing the Pareto optimal solutions.\nComparative experiments on both synthetic and real-world images show the\neffectiveness and usefulness of our deformation estimation method.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:52:12 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Nakane", "Takumi", ""], ["Lu", "Xuequan", ""], ["Xie", "Haoran", ""], ["Zhang", "Chao", ""]]}, {"id": "2106.04144", "submitter": "Gabriel Tjio", "authors": "Gabriel Tjio, Ping Liu, Joey Tianyi Zhou, Rick Siow Mong Goh", "title": "Adversarial Semantic Hallucination for Domain Generalized Semantic\n  Segmentation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks may perform poorly when the test and train data\nare from different domains. While this problem can be mitigated by using the\ntarget domain data to align the source and target domain feature\nrepresentations, the target domain data may be unavailable due to privacy\nconcerns. Consequently, there is a need for methods that generalize well\nwithout access to target domain data during training. In this work, we propose\nan adversarial hallucination approach, which combines a class-wise\nhallucination module and a semantic segmentation module. Since the segmentation\nperformance varies across different classes, we design a semantic-conditioned\nstyle hallucination layer to adaptively stylize each class. The classwise\nstylization parameters are generated from the semantic knowledge in the\nsegmentation probability maps of the source domain image. Both modules compete\nadversarially, with the hallucination module generating increasingly\n'difficult' style images to challenge the segmentation module. In response, the\nsegmentation module improves its performance as it is trained with generated\nsamples at an appropriate class-wise difficulty level. Experiments on state of\nthe art domain adaptation work demonstrate the efficacy of our proposed method\nwhen no target domain data are available for training.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 07:07:45 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 04:05:26 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Tjio", "Gabriel", ""], ["Liu", "Ping", ""], ["Zhou", "Joey Tianyi", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "2106.04150", "submitter": "Tingting Xie", "authors": "Ting-Ting Xie, Christos Tzelepis, Fan Fu, Ioannis Patras", "title": "Few-Shot Action Localization without Knowing Boundaries", "comments": "ICMR21 Camera ready; link to code:\n  https://github.com/June01/WFSAL-icmr21", "journal-ref": null, "doi": "10.1145/3460426.3463643", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to localize actions in long, cluttered, and untrimmed videos is a\nhard task, that in the literature has typically been addressed assuming the\navailability of large amounts of annotated training samples for each class --\neither in a fully-supervised setting, where action boundaries are known, or in\na weakly-supervised setting, where only class labels are known for each video.\nIn this paper, we go a step further and show that it is possible to learn to\nlocalize actions in untrimmed videos when a) only one/few trimmed examples of\nthe target action are available at test time, and b) when a large collection of\nvideos with only class label annotation (some trimmed and some weakly annotated\nuntrimmed ones) are available for training; with no overlap between the classes\nused during training and testing. To do so, we propose a network that learns to\nestimate Temporal Similarity Matrices (TSMs) that model a fine-grained\nsimilarity pattern between pairs of videos (trimmed or untrimmed), and uses\nthem to generate Temporal Class Activation Maps (TCAMs) for seen or unseen\nclasses. The TCAMs serve as temporal attention mechanisms to extract\nvideo-level representations of untrimmed videos, and to temporally localize\nactions at test time. To the best of our knowledge, we are the first to propose\na weakly-supervised, one/few-shot action localization network that can be\ntrained in an end-to-end fashion. Experimental results on THUMOS14 and\nActivityNet1.2 datasets, show that our method achieves performance comparable\nor better to state-of-the-art fully-supervised, few-shot learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 07:32:43 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Xie", "Ting-Ting", ""], ["Tzelepis", "Christos", ""], ["Fu", "Fan", ""], ["Patras", "Ioannis", ""]]}, {"id": "2106.04151", "submitter": "Jingjing Li", "authors": "Zhekai Du, Jingjing Li, Hongzu Su, Lei Zhu, Ke Lu", "title": "Cross-Domain Gradient Discrepancy Minimization for Unsupervised Domain\n  Adaptation", "comments": "Accepted to CVPR 2021, Codes are avaliable at\n  https://github.com/lijin118/CGDM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised Domain Adaptation (UDA) aims to generalize the knowledge learned\nfrom a well-labeled source domain to an unlabeled target domain. Recently,\nadversarial domain adaptation with two distinct classifiers (bi-classifier) has\nbeen introduced into UDA which is effective to align distributions between\ndifferent domains. Previous bi-classifier adversarial learning methods only\nfocus on the similarity between the outputs of two distinct classifiers.\nHowever, the similarity of the outputs cannot guarantee the accuracy of target\nsamples, i.e., target samples may match to wrong categories even if the\ndiscrepancy between two classifiers is small. To challenge this issue, in this\npaper, we propose a cross-domain gradient discrepancy minimization (CGDM)\nmethod which explicitly minimizes the discrepancy of gradients generated by\nsource samples and target samples. Specifically, the gradient gives a cue for\nthe semantic information of target samples so it can be used as a good\nsupervision to improve the accuracy of target samples. In order to compute the\ngradient signal of target samples, we further obtain target pseudo labels\nthrough a clustering-based self-supervised learning. Extensive experiments on\nthree widely used UDA datasets show that our method surpasses many previous\nstate-of-the-arts. Codes are available at https://github.com/lijin118/CGDM.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 07:35:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Du", "Zhekai", ""], ["Li", "Jingjing", ""], ["Su", "Hongzu", ""], ["Zhu", "Lei", ""], ["Lu", "Ke", ""]]}, {"id": "2106.04169", "submitter": "Salman Khan Dr.", "authors": "Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan,\n  Fatih Porikli", "title": "On Improving Adversarial Transferability of Vision Transformers", "comments": "Code: https://git.io/JZmG3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision transformers (ViTs) process input images as sequences of patches via\nself-attention; a radically different architecture than convolutional neural\nnetworks (CNNs). This makes it interesting to study the adversarial feature\nspace of ViT models and their transferability. In particular, we observe that\nadversarial patterns found via conventional adversarial attacks show very low\nblack-box transferability even for large ViT models. However, we show that this\nphenomenon is only due to the sub-optimal attack procedures that do not\nleverage the true representation potential of ViTs. A deep ViT is composed of\nmultiple blocks, with a consistent architecture comprising of self-attention\nand feed-forward layers, where each block is capable of independently producing\na class token. Formulating an attack using only the last class token\n(conventional approach) does not directly leverage the discriminative\ninformation stored in the earlier tokens, leading to poor adversarial\ntransferability of ViTs. Using the compositional nature of ViT models, we\nenhance the transferability of existing attacks by introducing two novel\nstrategies specific to the architecture of ViT models. (i) Self-Ensemble: We\npropose a method to find multiple discriminative pathways by dissecting a\nsingle ViT model into an ensemble of networks. This allows explicitly utilizing\nclass-specific information at each ViT block. (ii) Token Refinement: We then\npropose to refine the tokens to further enhance the discriminative capacity at\neach block of ViT. Our token refinement systematically combines the class\ntokens with structural information preserved within the patch tokens. An\nadversarial attack, when applied to such refined tokens within the ensemble of\nclassifiers found in a single vision transformer, has significantly higher\ntransferability.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 08:20:38 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Naseer", "Muzammal", ""], ["Ranasinghe", "Kanchana", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""], ["Porikli", "Fatih", ""]]}, {"id": "2106.04175", "submitter": "Laurent Kloeker", "authors": "Laurent Kloeker, Fabian Thomsen, Lutz Eckstein, Philip Trettner, Tim\n  Elsner, Julius Nehring-Wirxel, Kersten Schuster, Leif Kobbelt, Michael Hoesch", "title": "Highly accurate digital traffic recording as a basis for future mobility\n  research: Methods and concepts of the research project HDV-Mess", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research project HDV-Mess aims at a currently missing, but very crucial\ncomponent for addressing important challenges in the field of connected and\nautomated driving on public roads. The goal is to record traffic events at\nvarious relevant locations with high accuracy and to collect real traffic data\nas a basis for the development and validation of current and future sensor\ntechnologies as well as automated driving functions. For this purpose, it is\nnecessary to develop a concept for a mobile modular system of measuring\nstations for highly accurate traffic data acquisition, which enables a\ntemporary installation of a sensor and communication infrastructure at\ndifferent locations. Within this paper, we first discuss the project goals\nbefore we present our traffic detection concept using mobile modular\nintelligent transport systems stations (ITS-Ss). We then explain the approaches\nfor data processing of sensor raw data to refined trajectories, data\ncommunication, and data validation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 08:28:46 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kloeker", "Laurent", ""], ["Thomsen", "Fabian", ""], ["Eckstein", "Lutz", ""], ["Trettner", "Philip", ""], ["Elsner", "Tim", ""], ["Nehring-Wirxel", "Julius", ""], ["Schuster", "Kersten", ""], ["Kobbelt", "Leif", ""], ["Hoesch", "Michael", ""]]}, {"id": "2106.04178", "submitter": "Xuan Cheng", "authors": "Xuan Cheng, Tianshu Xie, Xiaomin Wang, Jiali Deng, Minghui Liu, Ming\n  Liu", "title": "White Paper Assistance: A Step Forward Beyond the Shortcut Learning", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The promising performances of CNNs often overshadow the need to examine\nwhether they are doing in the way we are actually interested. We show through\nexperiments that even over-parameterized models would still solve a dataset by\nrecklessly leveraging spurious correlations, or so-called 'shortcuts'. To\ncombat with this unintended propensity, we borrow the idea of printer test page\nand propose a novel approach called White Paper Assistance. Our proposed method\ninvolves the white paper to detect the extent to which the model has preference\nfor certain characterized patterns and alleviates it by forcing the model to\nmake a random guess on the white paper. We show the consistent accuracy\nimprovements that are manifest in various architectures, datasets and\ncombinations with other techniques. Experiments have also demonstrated the\nversatility of our approach on fine-grained recognition, imbalanced\nclassification and robustness to corruptions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 08:35:44 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cheng", "Xuan", ""], ["Xie", "Tianshu", ""], ["Wang", "Xiaomin", ""], ["Deng", "Jiali", ""], ["Liu", "Minghui", ""], ["Liu", "Ming", ""]]}, {"id": "2106.04180", "submitter": "Chenfeng Xu", "authors": "Chenfeng Xu, Shijia Yang, Bohan Zhai, Bichen Wu, Xiangyu Yue, Wei\n  Zhan, Peter Vajda, Kurt Keutzer, Masayoshi Tomizuka", "title": "Image2Point: 3D Point-Cloud Understanding with Pretrained 2D ConvNets", "comments": "The code is avaliable at:\n  \\url{https://github.com/chenfengxu714/image2point}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point-clouds and 2D images are different visual representations of the\nphysical world. While human vision can understand both representations,\ncomputer vision models designed for 2D image and 3D point-cloud understanding\nare quite different. Our paper investigates the potential for transferability\nbetween these two representations by empirically investigating whether this\napproach works, what factors affect the transfer performance, and how to make\nit work even better. We discovered that we can indeed use the same neural net\nmodel architectures to understand both images and point-clouds. Moreover, we\ncan transfer pretrained weights from image models to point-cloud models with\nminimal effort. Specifically, based on a 2D ConvNet pretrained on an image\ndataset, we can transfer the image model to a point-cloud model by\n\\textit{inflating} 2D convolutional filters to 3D then finetuning its input,\noutput, and optionally normalization layers. The transferred model can achieve\ncompetitive performance on 3D point-cloud classification, indoor and driving\nscene segmentation, even beating a wide range of point-cloud models that adopt\ntask-specific architectures and use a variety of tricks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 08:42:55 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Xu", "Chenfeng", ""], ["Yang", "Shijia", ""], ["Zhai", "Bohan", ""], ["Wu", "Bichen", ""], ["Yue", "Xiangyu", ""], ["Zhan", "Wei", ""], ["Vajda", "Peter", ""], ["Keutzer", "Kurt", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2106.04185", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Vivek Kwatra, Christian Frueh, John Lewis, Chris\n  Bregler", "title": "LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from\n  Video using Pose and Lighting Normalization", "comments": "Accepted to IEEE CVPR 2021. Brief demo video available at:\n  https://www.youtube.com/watch?v=L1StbX9OznY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a video-based learning framework for animating\npersonalized 3D talking faces from audio. We introduce two training-time data\nnormalizations that significantly improve data sample efficiency. First, we\nisolate and represent faces in a normalized space that decouples 3D geometry,\nhead pose, and texture. This decomposes the prediction problem into regressions\nover the 3D face shape and the corresponding 2D texture atlas. Second, we\nleverage facial symmetry and approximate albedo constancy of skin to isolate\nand remove spatio-temporal lighting variations. Together, these normalizations\nallow simple networks to generate high fidelity lip-sync videos under novel\nambient illumination while training with just a single speaker-specific video.\nFurther, to stabilize temporal dynamics, we introduce an auto-regressive\napproach that conditions the model on its previous visual state. Human ratings\nand objective metrics demonstrate that our method outperforms contemporary\nstate-of-the-art audio-driven video reenactment benchmarks in terms of realism,\nlip-sync and visual quality scores. We illustrate several applications enabled\nby our framework.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 08:56:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Lahiri", "Avisek", ""], ["Kwatra", "Vivek", ""], ["Frueh", "Christian", ""], ["Lewis", "John", ""], ["Bregler", "Chris", ""]]}, {"id": "2106.04195", "submitter": "Pengpeng Liu", "authors": "Pengpeng Liu and Michael R. Lyu and Irwin King and Jia Xu", "title": "Learning by Distillation: A Self-Supervised Learning Framework for\n  Optical Flow Estimation", "comments": "TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DistillFlow, a knowledge distillation approach to learning optical\nflow. DistillFlow trains multiple teacher models and a student model, where\nchallenging transformations are applied to the input of the student model to\ngenerate hallucinated occlusions as well as less confident predictions. Then, a\nself-supervised learning framework is constructed: confident predictions from\nteacher models are served as annotations to guide the student model to learn\noptical flow for those less confident predictions. The self-supervised learning\nframework enables us to effectively learn optical flow from unlabeled data, not\nonly for non-occluded pixels, but also for occluded pixels. DistillFlow\nachieves state-of-the-art unsupervised learning performance on both KITTI and\nSintel datasets. Our self-supervised pre-trained model also provides an\nexcellent initialization for supervised fine-tuning, suggesting an alternate\ntraining paradigm in contrast to current supervised learning methods that\nhighly rely on pre-training on synthetic data. At the time of writing, our\nfine-tuned models ranked 1st among all monocular methods on the KITTI 2015\nbenchmark, and outperform all published methods on the Sintel Final benchmark.\nMore importantly, we demonstrate the generalization capability of DistillFlow\nin three aspects: framework generalization, correspondence generalization and\ncross-dataset generalization.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 09:13:34 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Pengpeng", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""], ["Xu", "Jia", ""]]}, {"id": "2106.04208", "submitter": "Miguel Ivo Ferreira Fernandes", "authors": "Miguel Fernandes, Antonello Scaldaferri, Giuseppe Fiameni, Tao Teng,\n  Matteo Gatti, Stefano Poni, Claudio Semini, Darwin Caldwell, Fei Chen", "title": "Grapevine Winter Pruning Automation: On Potential Pruning Points\n  Detection through 2D Plant Modeling using Grapevine Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Grapevine winter pruning is a complex task, that requires skilled workers to\nexecute it correctly. The complexity of this task is also the reason why it is\ntime consuming. Considering that this operation takes about 80-120 hours/ha to\nbe completed, and therefore is even more crucial in large-size vineyards, an\nautomated system can help to speed up the process. To this end, this paper\npresents a novel multidisciplinary approach that tackles this challenging task\nby performing object segmentation on grapevine images, used to create a\nrepresentative model of the grapevine plants. Second, a set of potential\npruning points is generated from this plant representation. We will describe\n(a) a methodology for data acquisition and annotation, (b) a neural network\nfine-tuning for grapevine segmentation, (c) an image processing based method\nfor creating the representative model of grapevines, starting from the inferred\nsegmentation and (d) potential pruning points detection and localization, based\non the plant model which is a simplification of the grapevine structure. With\nthis approach, we are able to identify a significant set of potential pruning\npoints on the canes, that can be used, with further selection, to derive the\nfinal set of the real pruning points.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 09:36:54 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Fernandes", "Miguel", ""], ["Scaldaferri", "Antonello", ""], ["Fiameni", "Giuseppe", ""], ["Teng", "Tao", ""], ["Gatti", "Matteo", ""], ["Poni", "Stefano", ""], ["Semini", "Claudio", ""], ["Caldwell", "Darwin", ""], ["Chen", "Fei", ""]]}, {"id": "2106.04215", "submitter": "Laurent Colbois", "authors": "Laurent Colbois, Tiago de Freitas Pereira and S\\'ebastien Marcel", "title": "On the use of automatically generated synthetic image datasets for\n  benchmarking face recognition", "comments": "11 pages, Accepted for publication in the 2021 International Joint\n  Conference on Biometrics (IJCB 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of large-scale face datasets has been key in the progress of\nface recognition. However, due to licensing issues or copyright infringement,\nsome datasets are not available anymore (e.g. MS-Celeb-1M). Recent advances in\nGenerative Adversarial Networks (GANs), to synthesize realistic face images,\nprovide a pathway to replace real datasets by synthetic datasets, both to train\nand benchmark face recognition (FR) systems. The work presented in this paper\nprovides a study on benchmarking FR systems using a synthetic dataset. First,\nwe introduce the proposed methodology to generate a synthetic dataset, without\nthe need for human intervention, by exploiting the latent structure of a\nStyleGAN2 model with multiple controlled factors of variation. Then, we confirm\nthat (i) the generated synthetic identities are not data subjects from the\nGAN's training dataset, which is verified on a synthetic dataset with 10K+\nidentities; (ii) benchmarking results on the synthetic dataset are a good\nsubstitution, often providing error rates and system ranking similar to the\nbenchmarking on the real dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 09:54:02 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Colbois", "Laurent", ""], ["Pereira", "Tiago de Freitas", ""], ["Marcel", "S\u00e9bastien", ""]]}, {"id": "2106.04225", "submitter": "Andrea Alamia", "authors": "Andrea Alamia, Milad Mozafari, Bhavin Choksi and Rufin VanRullen", "title": "On the role of feedback in visual processing: a predictive coding\n  perspective", "comments": "'Andrea Alamia' and 'Milad Mozafari' contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain-inspired machine learning is gaining increasing consideration,\nparticularly in computer vision. Several studies investigated the inclusion of\ntop-down feedback connections in convolutional networks; however, it remains\nunclear how and when these connections are functionally helpful. Here we\naddress this question in the context of object recognition under noisy\nconditions. We consider deep convolutional networks (CNNs) as models of\nfeed-forward visual processing and implement Predictive Coding (PC) dynamics\nthrough feedback connections (predictive feedback) trained for reconstruction\nor classification of clean images. To directly assess the computational role of\npredictive feedback in various experimental situations, we optimize and\ninterpret the hyper-parameters controlling the network's recurrent dynamics.\nThat is, we let the optimization process determine whether top-down connections\nand predictive coding dynamics are functionally beneficial. Across different\nmodel depths and architectures (3-layer CNN, ResNet18, and EfficientNetB0) and\nagainst various types of noise (CIFAR100-C), we find that the network\nincreasingly relies on top-down predictions as the noise level increases; in\ndeeper networks, this effect is most prominent at lower layers. In addition,\nthe accuracy of the network implementing PC dynamics significantly increases\nover time-steps, compared to its equivalent forward network. All in all, our\nresults provide novel insights relevant to Neuroscience by confirming the\ncomputational role of feedback connections in sensory systems, and to Machine\nLearning by revealing how these can improve the robustness of current vision\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 10:07:23 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Alamia", "Andrea", ""], ["Mozafari", "Milad", ""], ["Choksi", "Bhavin", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2106.04260", "submitter": "Alexander Meinke", "authors": "Alexander Meinke, Julian Bitterwolf, Matthias Hein", "title": "Provably Robust Detection of Out-of-distribution Data (almost) for free", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applying machine learning in safety-critical systems, a reliable\nassessment of the uncertainy of a classifier is required. However, deep neural\nnetworks are known to produce highly overconfident predictions on\nout-of-distribution (OOD) data and even if trained to be non-confident on OOD\ndata one can still adversarially manipulate OOD data so that the classifer\nagain assigns high confidence to the manipulated samples. In this paper we\npropose a novel method where from first principles we combine a certifiable OOD\ndetector with a standard classifier into an OOD aware classifier. In this way\nwe achieve the best of two worlds: certifiably adversarially robust OOD\ndetection, even for OOD samples close to the in-distribution, without loss in\nprediction accuracy and close to state-of-the-art OOD detection performance for\nnon-manipulated OOD data. Moreover, due to the particular construction our\nclassifier provably avoids the asymptotic overconfidence problem of standard\nneural networks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:40:49 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Meinke", "Alexander", ""], ["Bitterwolf", "Julian", ""], ["Hein", "Matthias", ""]]}, {"id": "2106.04263", "submitter": "Qi Han", "authors": "Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Jiaying Liu,\n  Jingdong Wang", "title": "Demystifying Local Vision Transformer: Sparse Connectivity, Weight\n  Sharing, and Dynamic Weight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vision Transformer (ViT) attains state-of-the-art performance in visual\nrecognition, and the variant, Local Vision Transformer, makes further\nimprovements. The major component in Local Vision Transformer, local attention,\nperforms the attention separately over small local windows. We rephrase local\nattention as a channel-wise locally-connected layer and analyze it from two\nnetwork regularization manners, sparse connectivity and weight sharing, as well\nas weight computation. Sparse connectivity: there is no connection across\nchannels, and each position is connected to the positions within a small local\nwindow. Weight sharing: the connection weights for one position are shared\nacross channels or within each group of channels. Dynamic weight: the\nconnection weights are dynamically predicted according to each image instance.\nWe point out that local attention resembles depth-wise convolution and its\ndynamic version in sparse connectivity. The main difference lies in weight\nsharing - depth-wise convolution shares connection weights (kernel weights)\nacross spatial positions. We empirically observe that the models based on\ndepth-wise convolution and the dynamic variant with lower computation\ncomplexity perform on-par with or sometimes slightly better than Swin\nTransformer, an instance of Local Vision Transformer, for ImageNet\nclassification, COCO object detection and ADE semantic segmentation. These\nobservations suggest that Local Vision Transformer takes advantage of two\nregularization forms and dynamic weight to increase the network capacity.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:47:44 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Han", "Qi", ""], ["Fan", "Zejia", ""], ["Dai", "Qi", ""], ["Sun", "Lei", ""], ["Cheng", "Ming-Ming", ""], ["Liu", "Jiaying", ""], ["Wang", "Jingdong", ""]]}, {"id": "2106.04269", "submitter": "Nermin Samet", "authors": "Nermin Samet and Emre Akbas", "title": "HPRNet: Hierarchical Point Regression for Whole-Body Human Pose\n  Estimation", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new bottom-up one-stage method for whole-body\npose estimation, which we name \"hierarchical point regression,\" or HPRNet for\nshort, referring to the network that implements this method. To handle the\nscale variance among different body parts, we build a hierarchical point\nrepresentation of body parts and jointly regress them. Unlike the existing\ntwo-stage methods, our method predicts whole-body pose in a constant time\nindependent of the number of people in an image. On the COCO WholeBody dataset,\nHPRNet significantly outperforms all previous bottom-up methods on the keypoint\ndetection of all whole-body parts (i.e. body, foot, face and hand); it also\nachieves state-of-the-art results in the face (75.4 AP) and hand (50.4 AP)\nkeypoint detection. Code and models are available at\nhttps://github.com/nerminsamet/HPRNet.git.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:56:38 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Samet", "Nermin", ""], ["Akbas", "Emre", ""]]}, {"id": "2106.04274", "submitter": "Yicheng Deng", "authors": "Yicheng Deng, Cheng Sun, Yongqi Sun and Jiahui Zhu", "title": "A Synchronized Reprojection-based Model for 3D Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  3D human pose estimation is still a challenging problem despite the large\namount of work that has been done in this field. Generally, most methods\ndirectly use neural networks and ignore certain constraints (e.g., reprojection\nconstraints and joint angle and bone length constraints). This paper proposes a\nweakly supervised GAN-based model for 3D human pose estimation that considers\n3D information along with 2D information simultaneously, in which a\nreprojection network is employed to learn the mapping of the distribution from\n3D poses to 2D poses. In particular, we train the reprojection network and the\ngenerative adversarial network synchronously. Furthermore, inspired by the\ntypical kinematic chain space (KCS) matrix, we propose a weighted KCS matrix,\nwhich is added into the discriminator's input to impose joint angle and bone\nlength constraints. The experimental results on Human3.6M show that our method\noutperforms state-of-the-art methods by approximately 5.1\\%.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:11:56 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 05:31:44 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Deng", "Yicheng", ""], ["Sun", "Cheng", ""], ["Sun", "Yongqi", ""], ["Zhu", "Jiahui", ""]]}, {"id": "2106.04281", "submitter": "Luka Posilovi\\'c", "authors": "Luka Posilovi\\'c, Duje Medak, Marko Subasic, Marko Budimir, Sven\n  Loncaric", "title": "Generative adversarial network with object detector discriminator for\n  enhanced defect detection on ultrasonic B-scans", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2021.06.094", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-destructive testing is a set of techniques for defect detection in\nmaterials. While the set of imaging techniques are manifold, ultrasonic imaging\nis the one used the most. The analysis is mainly performed by human inspectors\nmanually analyzing recorded images. The low number of defects in real\nultrasonic inspections and legal issues considering data from such inspections\nmake it difficult to obtain proper results from automatic ultrasonic image\n(B-scan) analysis. In this paper, we present a novel deep learning Generative\nAdversarial Network model for generating ultrasonic B-scans with defects in\ndistinct locations. Furthermore, we show that generated B-scans can be used for\nsynthetic data augmentation, and can improve the performance of deep\nconvolutional neural object detection networks. Our novel method is\ndemonstrated on a dataset of almost 4000 B-scans with more than 6000 annotated\ndefects. Defect detection performance when training on real data yielded\naverage precision of 71%. By training only on generated data the results\nincreased to 72.1%, and by mixing generated and real data we achieve 75.7%\naverage precision. We believe that synthetic data generation can generalize to\nother challenges with limited datasets and could be used for training human\npersonnel.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:21:21 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Posilovi\u0107", "Luka", ""], ["Medak", "Duje", ""], ["Subasic", "Marko", ""], ["Budimir", "Marko", ""], ["Loncaric", "Sven", ""]]}, {"id": "2106.04283", "submitter": "Marc Tyndel", "authors": "Rayhane Mama, Marc S. Tyndel, Hashiam Kadhim, Cole Clifford, Ragavan\n  Thurairatnam", "title": "NWT: Towards natural audio-to-video generation with representation\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce NWT, an expressive speech-to-video model. Unlike\napproaches that use domain-specific intermediate representations such as pose\nkeypoints, NWT learns its own latent representations, with minimal assumptions\nabout the audio and video content. To this end, we propose a novel discrete\nvariational autoencoder with adversarial loss, dVAE-Adv, which learns a new\ndiscrete latent representation we call Memcodes. Memcodes are straightforward\nto implement, require no additional loss terms, are stable to train compared\nwith other approaches, and show evidence of interpretability. To predict on the\nMemcode space, we use an autoregressive encoder-decoder model conditioned on\naudio. Additionally, our model can control latent attributes in the generated\nvideo that are not annotated in the data. We train NWT on clips from HBO's Last\nWeek Tonight with John Oliver. NWT consistently scores above other approaches\nin Mean Opinion Score (MOS) on tests of overall video naturalness, facial\nnaturalness and expressiveness, and lipsync quality. This work sets a strong\nbaseline for generalized audio-to-video synthesis. Samples are available at\nhttps://next-week-tonight.github.io/NWT/.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:22:29 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Mama", "Rayhane", ""], ["Tyndel", "Marc S.", ""], ["Kadhim", "Hashiam", ""], ["Clifford", "Cole", ""], ["Thurairatnam", "Ragavan", ""]]}, {"id": "2106.04324", "submitter": "Christian Zimmermann", "authors": "Christian Zimmermann, Max Argus and Thomas Brox", "title": "Contrastive Representation Learning for Hand Shape Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work presents improvements in monocular hand shape estimation by\nbuilding on top of recent advances in unsupervised learning. We extend momentum\ncontrastive learning and contribute a structured collection of hand images,\nwell suited for visual representation learning, which we call HanCo. We find\nthat the representation learned by established contrastive learning methods can\nbe improved significantly by exploiting advanced background removal techniques\nand multi-view information. These allow us to generate more diverse instance\npairs than those obtained by augmentations commonly used in exemplar based\napproaches. Our method leads to a more suitable representation for the hand\nshape estimation task and shows a 4.7% reduction in mesh error and a 3.6%\nimprovement in F-score compared to an ImageNet pretrained baseline. We make our\nbenchmark dataset publicly available, to encourage further research into this\ndirection.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 13:31:58 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 07:06:44 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Zimmermann", "Christian", ""], ["Argus", "Max", ""], ["Brox", "Thomas", ""]]}, {"id": "2106.04332", "submitter": "Negar Heidari", "authors": "Negar Heidari and Alexandros Iosifidis", "title": "Progressive Spatio-Temporal Bilinear Network with Monte Carlo Dropout\n  for Landmark-based Facial Expression Recognition with Uncertainty Estimation", "comments": "6 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been widely used for feature learning in facial\nexpression recognition systems. However, small datasets and large intra-class\nvariability can lead to overfitting. In this paper, we propose a method which\nlearns an optimized compact network topology for real-time facial expression\nrecognition utilizing localized facial landmark features. Our method employs a\nspatio-temporal bilinear layer as backbone to capture the motion of facial\nlandmarks during the execution of a facial expression effectively. Besides, it\ntakes advantage of Monte Carlo Dropout to capture the model's uncertainty which\nis of great importance to analyze and treat uncertain cases. The performance of\nour method is evaluated on three widely used datasets and it is comparable to\nthat of video-based state-of-the-art methods while it has much less complexity.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 13:40:30 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Heidari", "Negar", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2106.04345", "submitter": "Nouna Khandan", "authors": "Nouna Khandan", "title": "An Intelligent Hybrid Model for Identity Document Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitization, i.e., the process of converting information into a digital\nformat, may provide various opportunities (e.g., increase in productivity,\ndisaster recovery, and environmentally friendly solutions) and challenges for\nbusinesses. In this context, one of the main challenges would be to accurately\nclassify numerous scanned documents uploaded every day by customers as usual\nbusiness processes. For example, processes in banking (e.g., applying for\nloans) or the Government Registry of BDM (Births, Deaths, and Marriages)\napplications may involve uploading several documents such as a driver's license\nand passport. There are not many studies available to address the challenge as\nan application of image classification. Although some studies are available\nwhich used various methods, a more accurate model is still required. The\ncurrent study has proposed a robust fusion model to define the type of identity\ndocuments accurately. The proposed approach is based on two different methods\nin which images are classified based on their visual features and text\nfeatures. A novel model based on statistics and regression has been proposed to\ncalculate the confidence level for the feature-based classifier. A fuzzy-mean\nfusion model has been proposed to combine the classifier results based on their\nconfidence score. The proposed approach has been implemented using Python and\nexperimentally validated on synthetic and real-world datasets. The performance\nof the proposed model is evaluated using the Receiver Operating Characteristic\n(ROC) curve analysis.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:08:00 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Khandan", "Nouna", ""]]}, {"id": "2106.04372", "submitter": "Hocine Cherifi", "authors": "Mahammed Messadi, Hocine Cherifi (Le2i), Abdelhafid Bessaid", "title": "Segmentation and ABCD rule extraction for skin tumors classification", "comments": null, "journal-ref": "Journal of Convergence for Information Technology, 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last years, computer vision-based diagnosis systems have been\nwidely used in several hospitals and dermatology clinics, aiming at the early\ndetection of malignant melanoma tumor, which is among the most frequent types\nof skin cancer. In this work, we present an automated diagnosis system based on\nthe ABCD rule used in clinical diagnosis in order to discriminate benign from\nmalignant skin lesions. First, to reduce the influence of small structures, a\npreprocessing step based on morphological and fast marching schemes is used. In\nthe second step, an unsupervised approach for lesion segmentation is proposed.\nIterative thresholding is applied to initialize level set automatically. As the\ndetection of an automated border is an important step for the correctness of\nsubsequent phases in the computerized melanoma recognition systems, we compare\nits accuracy with growcut and mean shift algorithms, and discuss how these\nresults may influence in the following steps: the feature extraction and the\nfinal lesion classification. Relying on visual diagnosis four features:\nAsymmetry (A), Border (B), Color (C) and Diversity (D) are computed and used to\nconstruct a classification module based on artificial neural network for the\nrecognition of malignant melanoma. This framework has been tested on a\ndermoscopic database [16] of 320 images. The classification results show an\nincreasing true detection rate and a decreasing false positive rate.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:07:59 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Messadi", "Mahammed", "", "Le2i"], ["Cherifi", "Hocine", "", "Le2i"], ["Bessaid", "Abdelhafid", ""]]}, {"id": "2106.04381", "submitter": "Leonardo Rundo", "authors": "Leonardo Rundo", "title": "Computer-Assisted Analysis of Biomedical Images", "comments": "PhD Thesis in Computer Science, University of Milano-Bicocca, Milan,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, the amount of heterogeneous biomedical data is increasing more and\nmore thanks to novel sensing techniques and high-throughput technologies. In\nreference to biomedical image analysis, the advances in image acquisition\nmodalities and high-throughput imaging experiments are creating new challenges.\nThis huge information ensemble could overwhelm the analytic capabilities needed\nby physicians in their daily decision-making tasks as well as by biologists\ninvestigating complex biochemical systems. In particular, quantitative imaging\nmethods convey scientifically and clinically relevant information in\nprediction, prognosis or treatment response assessment, by also considering\nradiomics approaches. Therefore, the computational analysis of medical and\nbiological images plays a key role in radiology and laboratory applications. In\nthis regard, frameworks based on advanced Machine Learning and Computational\nIntelligence can significantly improve traditional Image Processing and Pattern\nRecognition approaches. However, conventional Artificial Intelligence\ntechniques must be tailored to address the unique challenges concerning\nbiomedical imaging data. This thesis aims at proposing novel and advanced\ncomputer-assisted methods for biomedical image analysis, also as an instrument\nin the development of Clinical Decision Support Systems, by always keeping in\nmind the clinical feasibility of the developed solutions. In conclusion, the\nultimate goal of these research studies is to gain clinically and biologically\nuseful insights that can guide differential diagnosis and therapies, leading\ntowards biomedical data integration for personalized medicine. As a matter of\nfact, the proposed computer-assisted bioimage analysis methods can be\nbeneficial for the definition of imaging biomarkers, as well as for\nquantitative medicine and biology.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:59:48 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Rundo", "Leonardo", ""]]}, {"id": "2106.04387", "submitter": "Mathieu Marsot", "authors": "Marsot Mathieu, Wuhrer Stefanie, Franco Jean-Sebastien, Durocher\n  Stephane", "title": "Multi-frame sequence generator of 4D human body motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of generating temporally and spatially dense 4D human\nbody motion. On the one hand generative modeling has been extensively studied\nas a per time-frame static fitting problem for dense 3D models such as mesh\nrepresentations, where the temporal aspect is left out of the generative model.\nOn the other hand, temporal generative models exist for sparse human models\nsuch as marker-based capture representations, but have not to our knowledge\nbeen extended to dense 3D shapes. We propose to bridge this gap with a\ngenerative auto-encoder-based framework, which encodes morphology, global\nlocomotion including translation and rotation, and multi-frame temporal motion\nas a single latent space vector. To assess its generalization and factorization\nabilities, we train our model on a cyclic locomotion subset of AMASS,\nleveraging the dense surface models it provides for an extensive set of motion\ncaptures. Our results validate the ability of the model to reconstruct 4D\nsequences of human locomotions within a low error bound, and the meaningfulness\nof latent space interpolation between latent vectors representing different\nmulti-frame sequences and locomotion types. We also illustrate the benefits of\nthe approach for 4D human motion prediction of future frames from initial human\nlocomotion frames, showing promising abilities of our model to learn realistic\nspatio-temporal features of human motion. We show that our model allows for\ndata completion of both spatially and temporally sparse data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:56:46 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 07:36:55 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Mathieu", "Marsot", ""], ["Stefanie", "Wuhrer", ""], ["Jean-Sebastien", "Franco", ""], ["Stephane", "Durocher", ""]]}, {"id": "2106.04400", "submitter": "Jingjing Xiong", "authors": "Jingjing Xiong, Lai-Man Po, Wing-Yin Yu, Chang Zhou, Pengfei Xian and\n  Weifeng Ou", "title": "CSRNet: Cascaded Selective Resolution Network for Real-time Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time semantic segmentation has received considerable attention due to\ngrowing demands in many practical applications, such as autonomous vehicles,\nrobotics, etc. Existing real-time segmentation approaches often utilize feature\nfusion to improve segmentation accuracy. However, they fail to fully consider\nthe feature information at different resolutions and the receptive fields of\nthe networks are relatively limited, thereby compromising the performance. To\ntackle this problem, we propose a light Cascaded Selective Resolution Network\n(CSRNet) to improve the performance of real-time segmentation through multiple\ncontext information embedding and enhanced feature aggregation. The proposed\nnetwork builds a three-stage segmentation system, which integrates feature\ninformation from low resolution to high resolution and achieves feature\nrefinement progressively. CSRNet contains two critical modules: the Shorted\nPyramid Fusion Module (SPFM) and the Selective Resolution Module (SRM). The\nSPFM is a computationally efficient module to incorporate the global context\ninformation and significantly enlarge the receptive field at each stage. The\nSRM is designed to fuse multi-resolution feature maps with various receptive\nfields, which assigns soft channel attentions across the feature maps and helps\nto remedy the problem caused by multi-scale objects. Comprehensive experiments\non two well-known datasets demonstrate that the proposed CSRNet effectively\nimproves the performance for real-time segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:22:09 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Xiong", "Jingjing", ""], ["Po", "Lai-Man", ""], ["Yu", "Wing-Yin", ""], ["Zhou", "Chang", ""], ["Xian", "Pengfei", ""], ["Ou", "Weifeng", ""]]}, {"id": "2106.04403", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Ioannis Kazakos, Carles Ventura, Miriam Bellver, Carina Silberer and\n  Xavier Giro-i-Nieto", "title": "SynthRef: Generation of Synthetic Referring Expressions for Object\n  Segmentation", "comments": "Accepted as poster at the NAACL 2021 Visually Grounded Interaction\n  and Language (ViGIL) Workshop. 4 pages. Project website:\n  https://imatge-upc.github.io/synthref/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in deep learning have brought significant progress in visual\ngrounding tasks such as language-guided video object segmentation. However,\ncollecting large datasets for these tasks is expensive in terms of annotation\ntime, which represents a bottleneck. To this end, we propose a novel method,\nnamely SynthRef, for generating synthetic referring expressions for target\nobjects in an image (or video frame), and we also present and disseminate the\nfirst large-scale dataset with synthetic referring expressions for video object\nsegmentation. Our experiments demonstrate that by training with our synthetic\nreferring expressions one can improve the ability of a model to generalize\nacross different datasets, without any additional annotation cost. Moreover,\nour formulation allows its application to any object detection or segmentation\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:28:13 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 05:39:51 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Kazakos", "Ioannis", ""], ["Ventura", "Carles", ""], ["Bellver", "Miriam", ""], ["Silberer", "Carina", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2106.04411", "submitter": "Sangwon Jung", "authors": "Sangwon Jung, Donggyu Lee, Taeeon Park and Taesup Moon", "title": "Fair Feature Distillation for Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness is becoming an increasingly crucial issue for computer vision,\nespecially in the human-related decision systems. However, achieving\nalgorithmic fairness, which makes a model produce indiscriminative outcomes\nagainst protected groups, is still an unresolved problem. In this paper, we\ndevise a systematic approach which reduces algorithmic biases via feature\ndistillation for visual recognition tasks, dubbed as MMD-based Fair\nDistillation (MFD). While the distillation technique has been widely used in\ngeneral to improve the prediction accuracy, to the best of our knowledge, there\nhas been no explicit work that also tries to improve fairness via distillation.\nFurthermore, We give a theoretical justification of our MFD on the effect of\nknowledge distillation and fairness. Throughout the extensive experiments, we\nshow our MFD significantly mitigates the bias against specific minorities\nwithout any loss of the accuracy on both synthetic and real-world face\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 18:00:07 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 13:55:34 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Jung", "Sangwon", ""], ["Lee", "Donggyu", ""], ["Park", "Taeeon", ""], ["Moon", "Taesup", ""]]}, {"id": "2106.04413", "submitter": "Shengdong Zhang", "authors": "Shengdong Zhang, Ehsan Nezhadarya, Homa Fashandi, Jiayi Liu, Darin\n  Graham, Mohak Shah", "title": "Stochastic Whitening Batch Normalization", "comments": "Accepted to the Main Conference of CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Batch Normalization (BN) is a popular technique for training Deep Neural\nNetworks (DNNs). BN uses scaling and shifting to normalize activations of\nmini-batches to accelerate convergence and improve generalization. The recently\nproposed Iterative Normalization (IterNorm) method improves these properties by\nwhitening the activations iteratively using Newton's method. However, since\nNewton's method initializes the whitening matrix independently at each training\nstep, no information is shared between consecutive steps. In this work, instead\nof exact computation of whitening matrix at each time step, we estimate it\ngradually during training in an online fashion, using our proposed Stochastic\nWhitening Batch Normalization (SWBN) algorithm. We show that while SWBN\nimproves the convergence rate and generalization of DNNs, its computational\noverhead is less than that of IterNorm. Due to the high efficiency of the\nproposed method, it can be easily employed in most DNN architectures with a\nlarge number of layers. We provide comprehensive experiments and comparisons\nbetween BN, IterNorm, and SWBN layers to demonstrate the effectiveness of the\nproposed technique in conventional (many-shot) image classification and\nfew-shot classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 20:45:42 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhang", "Shengdong", ""], ["Nezhadarya", "Ehsan", ""], ["Fashandi", "Homa", ""], ["Liu", "Jiayi", ""], ["Graham", "Darin", ""], ["Shah", "Mohak", ""]]}, {"id": "2106.04419", "submitter": "Joseph Gesnouin Mr.", "authors": "Rapha\\\"el Rozenberg, Joseph Gesnouin and Fabien Moutarde", "title": "Asymmetrical Bi-RNN for pedestrian trajectory encoding", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pedestrian motion behavior involves a combination of individual goals and\nsocial interactions with other agents. In this article, we present an\nasymmetrical bidirectional recurrent neural network architecture called U-RNN\nto encode pedestrian trajectories and evaluate its relevance to replace LSTMs\nfor various forecasting models. Experimental results on the Trajnet++ benchmark\nshow that the U-LSTM variant yields better results regarding every available\nmetrics (ADE, FDE, Collision rate) than common trajectory encoders for a\nvariety of approaches and interaction modules, suggesting that the proposed\napproach is a viable alternative to the de facto sequence encoding RNNs.\n  Our implementation of the asymmetrical Bi-RNNs for the Trajnet++ benchmark is\navailable at:\ngithub.com/JosephGesnouin/Asymmetrical-Bi-RNNs-to-encode-pedestrian-trajectories\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 12:05:15 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 12:33:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rozenberg", "Rapha\u00ebl", ""], ["Gesnouin", "Joseph", ""], ["Moutarde", "Fabien", ""]]}, {"id": "2106.04427", "submitter": "Alexander Hepburn", "authors": "Alexander Hepburn and Valero Laparra and Raul Santos-Rodriguez and\n  Johannes Ball\\'e and Jes\\'us Malo", "title": "On the relation between statistical learning and perceptual distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been demonstrated many times that the behavior of the human visual\nsystem is connected to the statistics of natural images. Since machine learning\nrelies on the statistics of training data as well, the above connection has\ninteresting implications when using perceptual distances (which mimic the\nbehavior of the human visual system) as a loss function. In this paper, we aim\nto unravel the non-trivial relationship between the probability distribution of\nthe data, perceptual distances, and unsupervised machine learning. To this end,\nwe show that perceptual sensitivity is correlated with the probability of an\nimage in its close neighborhood. We also explore the relation between distances\ninduced by autoencoders and the probability distribution of the data used for\ntraining them, as well as how these induced distances are correlated with human\nperception. Finally, we discuss why perceptual distances might not lead to\nnoticeable gains in performance over standard Euclidean distances in common\nimage processing tasks except when data is scarce and the perceptual distance\nprovides regularization.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:56:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Hepburn", "Alexander", ""], ["Laparra", "Valero", ""], ["Santos-Rodriguez", "Raul", ""], ["Ball\u00e9", "Johannes", ""], ["Malo", "Jes\u00fas", ""]]}, {"id": "2106.04428", "submitter": "Younggeun Kim", "authors": "Younggeun Kim, Donghee Son", "title": "Noise Conditional Flow Model for Learning the Super-Resolution Space", "comments": "Final CVPR2021 workshop version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fundamentally, super-resolution is ill-posed problem because a low-resolution\nimage can be obtained from many high-resolution images. Recent studies for\nsuper-resolution cannot create diverse super-resolution images. Although SRFlow\ntried to account for ill-posed nature of the super-resolution by predicting\nmultiple high-resolution images given a low-resolution image, there is room to\nimprove the diversity and visual quality. In this paper, we propose Noise\nConditional flow model for Super-Resolution, NCSR, which increases the visual\nquality and diversity of images through noise conditional layer. To learn more\ndiverse data distribution, we add noise to training data. However, low-quality\nimages are resulted from adding noise. We propose the noise conditional layer\nto overcome this phenomenon. The noise conditional layer makes our model\ngenerate more diverse images with higher visual quality than other works.\nFurthermore, we show that this layer can overcome data distribution mismatch, a\nproblem that arises in normalizing flow models. With these benefits, NCSR\noutperforms baseline in diversity and visual quality and achieves better visual\nquality than traditional GAN-based models. We also get outperformed scores at\nNTIRE 2021 challenge.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 07:43:52 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kim", "Younggeun", ""], ["Son", "Donghee", ""]]}, {"id": "2106.04434", "submitter": "Yuxin Deng", "authors": "Jiayi Ma and Yuxin Deng", "title": "SDGMNet: Statistic-based Dynamic Gradient Modulation for Local\n  Descriptor Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modifications on triplet loss that rescale the back-propagated gradients of\nspecial pairs have made significant progress on local descriptor learning.\nHowever, current gradient modulation strategies are mainly static so that they\nwould suffer from changes of training phases or datasets. In this paper, we\npropose a dynamic gradient modulation, named SDGMNet, to improve triplet loss\nfor local descriptor learning. The core of our method is formulating modulation\nfunctions with statistical characteristics which are estimated dynamically.\nFirstly, we perform deep analysis on back propagation of general triplet-based\nloss and introduce included angle for distance measure. On this basis,\nauto-focus modulation is employed to moderate the impact of statistically\nuncommon individual pairs in stochastic gradient descent optimization;\nprobabilistic margin cuts off the gradients of proportional Siamese pairs that\nare believed to reach the optimum; power adjustment balances the total weights\nof negative pairs and positive pairs. Extensive experiments demonstrate that\nour novel descriptor surpasses previous state-of-the-arts on standard\nbenchmarks including patch verification, matching and retrieval tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:10:31 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 12:45:28 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ma", "Jiayi", ""], ["Deng", "Yuxin", ""]]}, {"id": "2106.04463", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali, Debesh Jha, Noha Ghatwary, Stefano Realdon, Renato\n  Cannizzaro, Osama E. Salem, Dominique Lamarque, Christian Daul, Kim V.\n  Anonsen, Michael A. Riegler, P{\\aa}l Halvorsen, Jens Rittscher, Thomas de\n  Lange, and James E. East", "title": "PolypGen: A multi-center polyp detection and segmentation dataset for\n  generalisability assessment", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Polyps in the colon are widely known as cancer precursors identified by\ncolonoscopy either related to diagnostic work-up for symptoms, colorectal\ncancer screening or systematic surveillance of certain diseases. Whilst most\npolyps are benign, the number, size and the surface structure of the polyp are\ntightly linked to the risk of colon cancer. There exists a high missed\ndetection rate and incomplete removal of colon polyps due to the variable\nnature, difficulties to delineate the abnormality, high recurrence rates and\nthe anatomical topography of the colon. In the past, several methods have been\nbuilt to automate polyp detection and segmentation. However, the key issue of\nmost methods is that they have not been tested rigorously on a large\nmulti-center purpose-built dataset. Thus, these methods may not generalise to\ndifferent population datasets as they overfit to a specific population and\nendoscopic surveillance. To this extent, we have curated a dataset from 6\ndifferent centers incorporating more than 300 patients. The dataset includes\nboth single frame and sequence data with 3446 annotated polyp labels with\nprecise delineation of polyp boundaries verified by six senior\ngastroenterologists. To our knowledge, this is the most comprehensive detection\nand pixel-level segmentation dataset curated by a team of computational\nscientists and expert gastroenterologists. This dataset has been originated as\nthe part of the Endocv2021 challenge aimed at addressing generalisability in\npolyp detection and segmentation. In this paper, we provide comprehensive\ninsight into data construction and annotation strategies, annotation quality\nassurance and technical validation for our extended EndoCV2021 dataset which we\nrefer to as PolypGen.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:48:17 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ali", "Sharib", ""], ["Jha", "Debesh", ""], ["Ghatwary", "Noha", ""], ["Realdon", "Stefano", ""], ["Cannizzaro", "Renato", ""], ["Salem", "Osama E.", ""], ["Lamarque", "Dominique", ""], ["Daul", "Christian", ""], ["Anonsen", "Kim V.", ""], ["Riegler", "Michael A.", ""], ["Halvorsen", "P\u00e5l", ""], ["Rittscher", "Jens", ""], ["de Lange", "Thomas", ""], ["East", "James E.", ""]]}, {"id": "2106.04471", "submitter": "Hubert P. H. Shum", "authors": "Manli Zhu, Qianhui Men, Edmond S. L. Ho, Howard Leung, Hubert P. H.\n  Shum", "title": "Interpreting Deep Learning based Cerebral Palsy Prediction with Channel\n  Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early prediction of cerebral palsy is essential as it leads to early\ntreatment and monitoring. Deep learning has shown promising results in\nbiomedical engineering thanks to its capacity of modelling complicated data\nwith its non-linear architecture. However, due to their complex structure, deep\nlearning models are generally not interpretable by humans, making it difficult\nfor clinicians to rely on the findings. In this paper, we propose a channel\nattention module for deep learning models to predict cerebral palsy from\ninfants' body movements, which highlights the key features (i.e. body joints)\nthe model identifies as important, thereby indicating why certain diagnostic\nresults are found. To highlight the capacity of the deep network in modelling\ninput features, we utilize raw joint positions instead of hand-crafted\nfeatures. We validate our system with a real-world infant movement dataset. Our\nproposed channel attention module enables the visualization of the vital joints\nto this disease that the network considers. Our system achieves 91.67%\naccuracy, suppressing other state-of-the-art deep learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:57:17 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhu", "Manli", ""], ["Men", "Qianhui", ""], ["Ho", "Edmond S. L.", ""], ["Leung", "Howard", ""], ["Shum", "Hubert P. H.", ""]]}, {"id": "2106.04477", "submitter": "Xuelin Chen", "authors": "Xuelin Chen, Weiyu Li, Daniel Cohen-Or, Niloy J. Mitra, Baoquan Chen", "title": "MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary\n  Monocular Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing novel views of dynamic humans from stationary monocular cameras\nis a popular scenario. This is particularly attractive as it does not require\nstatic scenes, controlled environments, or specialized hardware. In contrast to\ntechniques that exploit multi-view observations to constrain the modeling,\ngiven a single fixed viewpoint only, the problem of modeling the dynamic scene\nis significantly more under-constrained and ill-posed. In this paper, we\nintroduce Neural Motion Consensus Flow (MoCo-Flow), a representation that\nmodels the dynamic scene using a 4D continuous time-variant function. The\nproposed representation is learned by an optimization which models a dynamic\nscene that minimizes the error of rendering all observation images. At the\nheart of our work lies a novel optimization formulation, which is constrained\nby a motion consensus regularization on the motion flow. We extensively\nevaluate MoCo-Flow on several datasets that contain human motions of varying\ncomplexity, and compare, both qualitatively and quantitatively, to several\nbaseline methods and variants of our methods. Pretrained model, code, and data\nwill be released for research purposes upon paper acceptance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:03:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chen", "Xuelin", ""], ["Li", "Weiyu", ""], ["Cohen-Or", "Daniel", ""], ["Mitra", "Niloy J.", ""], ["Chen", "Baoquan", ""]]}, {"id": "2106.04484", "submitter": "Daniel Rosenberg", "authors": "Daniel Rosenberg, Itai Gat, Amir Feder, Roi Reichart", "title": "Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused\n  Interventions", "comments": "ACL 2021. Our code and data are available at\n  https://danrosenberg.github.io/rad-measure/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have shown promising results in visual question\nanswering (VQA) tasks, but a more careful look reveals that they often do not\nunderstand the rich signal they are being fed with. To understand and better\nmeasure the generalization capabilities of VQA systems, we look at their\nrobustness to counterfactually augmented data. Our proposed augmentations are\ndesigned to make a focused intervention on a specific property of the question\nsuch that the answer changes. Using these augmentations, we propose a new\nrobustness measure, Robustness to Augmented Data (RAD), which measures the\nconsistency of model predictions between original and augmented examples.\nThrough extensive experimentation, we show that RAD, unlike classical accuracy\nmeasures, can quantify when state-of-the-art systems are not robust to\ncounterfactuals. We find substantial failure cases which reveal that current\nVQA systems are still brittle. Finally, we connect between robustness and\ngeneralization, demonstrating the predictive power of RAD for performance on\nunseen augmentations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:09:47 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Rosenberg", "Daniel", ""], ["Gat", "Itai", ""], ["Feder", "Amir", ""], ["Reichart", "Roi", ""]]}, {"id": "2106.04488", "submitter": "Jiapeng Zhu", "authors": "Jiapeng Zhu, Ruili Feng, Yujun Shen, Deli Zhao, Zhengjun Zha, Jingren\n  Zhou, Qifeng Chen", "title": "Low-Rank Subspaces in GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The latent space of a Generative Adversarial Network (GAN) has been shown to\nencode rich semantics within some subspaces. To identify these subspaces,\nresearchers typically analyze the statistical information from a collection of\nsynthesized data, and the identified subspaces tend to control image attributes\nglobally (i.e., manipulating an attribute causes the change of an entire\nimage). By contrast, this work introduces low-rank subspaces that enable more\nprecise control of GAN generation. Concretely, given an arbitrary image and a\nregion of interest (e.g., eyes of face images), we manage to relate the latent\nspace to the image region with the Jacobian matrix and then use low-rank\nfactorization to discover steerable latent subspaces. There are three\ndistinguishable strengths of our approach that can be aptly called LowRankGAN.\nFirst, compared to analytic algorithms in prior work, our low-rank\nfactorization of Jacobians is able to find the low-dimensional representation\nof attribute manifold, making image editing more precise and controllable.\nSecond, low-rank factorization naturally yields a null space of attributes such\nthat moving the latent code within it only affects the outer region of\ninterest. Therefore, local image editing can be simply achieved by projecting\nan attribute vector into the null space without relying on a spatial mask as\nexisting methods do. Third, our method can robustly work with a local region\nfrom one image for analysis yet well generalize to other images, making it much\neasy to use in practice. Extensive experiments on state-of-the-art GAN models\n(including StyleGAN2 and BigGAN) trained on various datasets demonstrate the\neffectiveness of our LowRankGAN.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:16:32 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhu", "Jiapeng", ""], ["Feng", "Ruili", ""], ["Shen", "Yujun", ""], ["Zhao", "Deli", ""], ["Zha", "Zhengjun", ""], ["Zhou", "Jingren", ""], ["Chen", "Qifeng", ""]]}, {"id": "2106.04520", "submitter": "Hanting Li", "authors": "Hanting Li, Mingzhe Sui, Feng Zhao, Zhengjun Zha, and Feng Wu", "title": "MVT: Mask Vision Transformer for Facial Expression Recognition in the\n  wild", "comments": "11 pages, 6 figures, 5 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Expression Recognition (FER) in the wild is an extremely challenging\ntask in computer vision due to variant backgrounds, low-quality facial images,\nand the subjectiveness of annotators. These uncertainties make it difficult for\nneural networks to learn robust features on limited-scale datasets. Moreover,\nthe networks can be easily distributed by the above factors and perform\nincorrect decisions. Recently, vision transformer (ViT) and data-efficient\nimage transformers (DeiT) present their significant performance in traditional\nclassification tasks. The self-attention mechanism makes transformers obtain a\nglobal receptive field in the first layer which dramatically enhances the\nfeature extraction capability. In this work, we first propose a novel pure\ntransformer-based mask vision transformer (MVT) for FER in the wild, which\nconsists of two modules: a transformer-based mask generation network (MGN) to\ngenerate a mask that can filter out complex backgrounds and occlusion of face\nimages, and a dynamic relabeling module to rectify incorrect labels in FER\ndatasets in the wild. Extensive experimental results demonstrate that our MVT\noutperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with\n89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable\nresult on AffectNet-8 with 61.40%.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:58:10 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 13:03:06 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Hanting", ""], ["Sui", "Mingzhe", ""], ["Zhao", "Feng", ""], ["Zha", "Zhengjun", ""], ["Wu", "Feng", ""]]}, {"id": "2106.04531", "submitter": "Prithvijit Chattopadhyay Chattopadhyay", "authors": "Prithvijit Chattopadhyay, Judy Hoffman, Roozbeh Mottaghi, Aniruddha\n  Kembhavi", "title": "RobustNav: Towards Benchmarking Robustness in Embodied Navigation", "comments": "18 pages, 8 figures, Code: https://github.com/allenai/robustnav", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an attempt towards assessing the robustness of embodied navigation agents,\nwe propose RobustNav, a framework to quantify the performance of embodied\nnavigation agents when exposed to a wide variety of visual - affecting RGB\ninputs - and dynamics - affecting transition dynamics - corruptions. Most\nrecent efforts in visual navigation have typically focused on generalizing to\nnovel target environments with similar appearance and dynamics characteristics.\nWith RobustNav, we find that some standard embodied navigation agents\nsignificantly underperform (or fail) in the presence of visual or dynamics\ncorruptions. We systematically analyze the kind of idiosyncrasies that emerge\nin the behavior of such agents when operating under corruptions. Finally, for\nvisual corruptions in RobustNav, we show that while standard techniques to\nimprove robustness such as data-augmentation and self-supervised adaptation\noffer some zero-shot resistance and improvements in navigation performance,\nthere is still a long way to go in terms of recovering lost performance\nrelative to clean \"non-corrupt\" settings, warranting more research in this\ndirection. Our code is available at https://github.com/allenai/robustnav\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:14:33 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chattopadhyay", "Prithvijit", ""], ["Hoffman", "Judy", ""], ["Mottaghi", "Roozbeh", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "2106.04533", "submitter": "Tianlong Chen", "authors": "Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, Zhangyang Wang", "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision transformers (ViTs) have recently received explosive popularity, but\ntheir enormous model sizes and training costs remain daunting. Conventional\npost-training pruning often incurs higher training budgets. In contrast, this\npaper aims to trim down both the training memory overhead and the inference\ncomplexity, without sacrificing the achievable accuracy. We launch and report\nthe first-of-its-kind comprehensive exploration, on taking a unified approach\nof integrating sparsity in ViTs \"from end to end\". Specifically, instead of\ntraining full ViTs, we dynamically extract and train sparse subnetworks, while\nsticking to a fixed small parameter budget. Our approach jointly optimizes\nmodel parameters and explores connectivity throughout training, ending up with\none sparse network as the final output. The approach is seamlessly extended\nfrom unstructured to structured sparsity, the latter by considering to guide\nthe prune-and-grow of self-attention heads inside ViTs. For additional\nefficiency gains, we further co-explore data and architecture sparsity, by\nplugging in a novel learnable token selector to adaptively determine the\ncurrently most vital patches. Extensive results on ImageNet with diverse ViT\nbackbones validate the effectiveness of our proposals which obtain\nsignificantly reduced computational cost and almost unimpaired generalization.\nPerhaps most surprisingly, we find that the proposed sparse (co-)training can\neven improve the ViT accuracy rather than compromising it, making sparsity a\ntantalizing \"free lunch\". For example, our sparsified DeiT-Small at (5%, 50%)\nsparsity for (data, architecture), improves 0.28% top-1 accuracy, and meanwhile\nenjoys 49.32% FLOPs and 4.40% running time savings. Our codes are available at\nhttps://github.com/VITA-Group/SViTE.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:18:00 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 22:28:31 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chen", "Tianlong", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Yuan", "Lu", ""], ["Zhang", "Lei", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2106.04540", "submitter": "Jordan Lei", "authors": "Jordan Lei, Ari S. Benjamin, Konrad P. Kording", "title": "Object Based Attention Through Internal Gating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object-based attention is a key component of the visual system, relevant for\nperception, learning, and memory. Neurons tuned to features of attended objects\ntend to be more active than those associated with non-attended objects. There\nis a rich set of models of this phenomenon in computational neuroscience.\nHowever, there is currently a divide between models that successfully match\nphysiological data but can only deal with extremely simple problems and models\nof attention used in computer vision. For example, attention in the brain is\nknown to depend on top-down processing, whereas self-attention in deep learning\ndoes not. Here, we propose an artificial neural network model of object-based\nattention that captures the way in which attention is both top-down and\nrecurrent. Our attention model works well both on simple test stimuli, such as\nthose using images of handwritten digits, and on more complex stimuli, such as\nnatural images drawn from the COCO dataset. We find that our model replicates a\nrange of findings from neuroscience, including attention-invariant tuning,\ninhibition of return, and attention-mediated scaling of activity. Understanding\nobject based attention is both computationally interesting and a key problem\nfor computational neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:20:50 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Lei", "Jordan", ""], ["Benjamin", "Ari S.", ""], ["Kording", "Konrad P.", ""]]}, {"id": "2106.04550", "submitter": "Amir Bar", "authors": "Amir Bar, Xin Wang, Vadim Kantorov, Colorado J Reed, Roei Herzig, Gal\n  Chechik, Anna Rohrbach, Trevor Darrell, Amir Globerson", "title": "DETReg: Unsupervised Pretraining with Region Priors for Object Detection", "comments": "preprint, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised pretraining has recently proven beneficial for computer vision\ntasks, including object detection. However, previous self-supervised approaches\nare not designed to handle a key aspect of detection: localizing objects. Here,\nwe present DETReg, an unsupervised pretraining approach for object DEtection\nwith TRansformers using Region priors. Motivated by the two tasks underlying\nobject detection: localization and categorization, we combine two complementary\nsignals for self-supervision. For an object localization signal, we use pseudo\nground truth object bounding boxes from an off-the-shelf unsupervised region\nproposal method, Selective Search, which does not require training data and can\ndetect objects at a high recall rate and very low precision. The categorization\nsignal comes from an object embedding loss that encourages invariant object\nrepresentations, from which the object category can be inferred. We show how to\ncombine these two signals to train the Deformable DETR detection architecture\nfrom large amounts of unlabeled data. DETReg improves the performance over\ncompetitive baselines and previous self-supervised methods on standard\nbenchmarks like MS COCO and PASCAL VOC. DETReg also outperforms previous\nsupervised and unsupervised baseline approaches on low-data regime when trained\nwith only 1%, 2%, 5%, and 10% of the labeled data on MS COCO. For code and\npretrained models, visit the project page at https://amirbar.net/detreg\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:39:14 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Bar", "Amir", ""], ["Wang", "Xin", ""], ["Kantorov", "Vadim", ""], ["Reed", "Colorado J", ""], ["Herzig", "Roei", ""], ["Chechik", "Gal", ""], ["Rohrbach", "Anna", ""], ["Darrell", "Trevor", ""], ["Globerson", "Amir", ""]]}, {"id": "2106.04555", "submitter": "Tommi Kerola", "authors": "Tommi Kerola, Jie Li, Atsushi Kanehira, Yasunori Kudo, Alexis Vallet,\n  Adrien Gaidon", "title": "Hierarchical Lov\\'asz Embeddings for Proposal-free Panoptic Segmentation", "comments": "13 pages, 9 figures, including supplementary material. To be\n  published in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation brings together two separate tasks: instance and\nsemantic segmentation. Although they are related, unifying them faces an\napparent paradox: how to learn simultaneously instance-specific and\ncategory-specific (i.e. instance-agnostic) representations jointly. Hence,\nstate-of-the-art panoptic segmentation methods use complex models with a\ndistinct stream for each task. In contrast, we propose Hierarchical Lov\\'asz\nEmbeddings, per pixel feature vectors that simultaneously encode instance- and\ncategory-level discriminative information. We use a hierarchical Lov\\'asz hinge\nloss to learn a low-dimensional embedding space structured into a unified\nsemantic and instance hierarchy without requiring separate network branches or\nobject proposals. Besides modeling instances precisely in a proposal-free\nmanner, our Hierarchical Lov\\'asz Embeddings generalize to categories by using\na simple Nearest-Class-Mean classifier, including for non-instance \"stuff\"\nclasses where instance segmentation methods are not applicable. Our simple\nmodel achieves state-of-the-art results compared to existing proposal-free\npanoptic segmentation methods on Cityscapes, COCO, and Mapillary Vistas.\nFurthermore, our model demonstrates temporal stability between video frames.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:43:54 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kerola", "Tommi", ""], ["Li", "Jie", ""], ["Kanehira", "Atsushi", ""], ["Kudo", "Yasunori", ""], ["Vallet", "Alexis", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2106.04560", "submitter": "Xiaohua Zhai", "authors": "Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer", "title": "Scaling Vision Transformers", "comments": "Xiaohua, Alex, and Lucas contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based neural networks such as the Vision Transformer (ViT) have\nrecently attained state-of-the-art results on many computer vision benchmarks.\nScale is a primary ingredient in attaining excellent results, therefore,\nunderstanding a model's scaling properties is a key to designing future\ngenerations effectively. While the laws for scaling Transformer language models\nhave been studied, it is unknown how Vision Transformers scale. To address\nthis, we scale ViT models and data, both up and down, and characterize the\nrelationships between error rate, data, and compute. Along the way, we refine\nthe architecture and training of ViT, reducing memory consumption and\nincreasing accuracy the resulting models. As a result, we successfully train a\nViT model with two billion parameters, which attains a new state-of-the-art on\nImageNet of 90.45% top-1 accuracy. The model also performs well on few-shot\nlearning, for example, attaining 84.86% top-1 accuracy on ImageNet with only 10\nexamples per class.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:47:39 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhai", "Xiaohua", ""], ["Kolesnikov", "Alexander", ""], ["Houlsby", "Neil", ""], ["Beyer", "Lucas", ""]]}, {"id": "2106.04566", "submitter": "Ceyuan Yang", "authors": "Ceyuan Yang, Yujun Shen, Yinghao Xu, Bolei Zhou", "title": "Data-Efficient Instance Generation from Instance Discrimination", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have significantly advanced image\nsynthesis, however, the synthesis quality drops significantly given a limited\namount of training data. To improve the data efficiency of GAN training, prior\nwork typically employs data augmentation to mitigate the overfitting of the\ndiscriminator yet still learn the discriminator with a bi-classification (i.e.,\nreal vs. fake) task. In this work, we propose a data-efficient Instance\nGeneration (InsGen) method based on instance discrimination. Concretely,\nbesides differentiating the real domain from the fake domain, the discriminator\nis required to distinguish every individual image, no matter it comes from the\ntraining set or from the generator. In this way, the discriminator can benefit\nfrom the infinite synthesized samples for training, alleviating the overfitting\nproblem caused by insufficient training data. A noise perturbation strategy is\nfurther introduced to improve its discriminative power. Meanwhile, the learned\ninstance discrimination capability from the discriminator is in turn exploited\nto encourage the generator for diverse generation. Extensive experiments\ndemonstrate the effectiveness of our method on a variety of datasets and\ntraining settings. Noticeably, on the setting of 2K training images from the\nFFHQ dataset, we outperform the state-of-the-art approach with 23.5% FID\nimprovement.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:52:59 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yang", "Ceyuan", ""], ["Shen", "Yujun", ""], ["Xu", "Yinghao", ""], ["Zhou", "Bolei", ""]]}, {"id": "2106.04569", "submitter": "Nataniel Ruiz", "authors": "Nataniel Ruiz, Adam Kortylewski, Weichao Qiu, Cihang Xie, Sarah Adel\n  Bargal, Alan Yuille, Stan Sclaroff", "title": "Simulated Adversarial Testing of Face Recognition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most machine learning models are validated and tested on fixed datasets. This\ncan give an incomplete picture of the capabilities and weaknesses of the model.\nSuch weaknesses can be revealed at test time in the real world. The risks\ninvolved in such failures can be loss of profits, loss of time or even loss of\nlife in certain critical applications. In order to alleviate this issue,\nsimulators can be controlled in a fine-grained manner using interpretable\nparameters to explore the semantic image manifold. In this work, we propose a\nframework for learning how to test machine learning algorithms using simulators\nin an adversarial manner in order to find weaknesses in the model before\ndeploying it in critical scenarios. We apply this model in a face recognition\nscenario. We are the first to show that weaknesses of models trained on real\ndata can be discovered using simulated samples. Using our proposed method, we\ncan find adversarial synthetic faces that fool contemporary face recognition\nmodels. This demonstrates the fact that these models have weaknesses that are\nnot measured by commonly used validation datasets. We hypothesize that this\ntype of adversarial examples are not isolated, but usually lie in connected\ncomponents in the latent space of the simulator. We present a method to find\nthese adversarial regions as opposed to the typical adversarial points found in\nthe adversarial example literature.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:58:10 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ruiz", "Nataniel", ""], ["Kortylewski", "Adam", ""], ["Qiu", "Weichao", ""], ["Xie", "Cihang", ""], ["Bargal", "Sarah Adel", ""], ["Yuille", "Alan", ""], ["Sclaroff", "Stan", ""]]}, {"id": "2106.04570", "submitter": "Canwen Xu", "authors": "Wangchunshu Zhou and Canwen Xu and Julian McAuley", "title": "Meta Learning for Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Meta Learning for Knowledge Distillation (MetaDistil), a simple\nyet effective alternative to traditional knowledge distillation (KD) methods\nwhere the teacher model is fixed during training. We show the teacher network\ncan learn to better transfer knowledge to the student network (i.e., learning\nto teach) with the feedback from the performance of the distilled student\nnetwork in a meta learning framework. Moreover, we introduce a pilot update\nmechanism to improve the alignment between the inner-learner and meta-learner\nin meta learning algorithms that focus on an improved inner-learner.\nExperiments on various benchmarks show that MetaDistil can yield significant\nimprovements compared with traditional KD algorithms and is less sensitive to\nthe choice of different student capacity and hyperparameters, facilitating the\nuse of KD on different tasks and models. The code is available at\nhttps://github.com/JetRunner/MetaDistil\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:59:03 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhou", "Wangchunshu", ""], ["Xu", "Canwen", ""], ["McAuley", "Julian", ""]]}, {"id": "2106.04605", "submitter": "Qingyi Si", "authors": "Qingyi Si, Zheng Lin, Mingyu Zheng, Peng Fu, Weiping Wang", "title": "Check It Again: Progressive Visual Question Answering via Visual\n  Entailment", "comments": "ACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While sophisticated Visual Question Answering models have achieved remarkable\nsuccess, they tend to answer questions only according to superficial\ncorrelations between question and answer. Several recent approaches have been\ndeveloped to address this language priors problem. However, most of them\npredict the correct answer according to one best output without checking the\nauthenticity of answers. Besides, they only explore the interaction between\nimage and question, ignoring the semantics of candidate answers. In this paper,\nwe propose a select-and-rerank (SAR) progressive framework based on Visual\nEntailment. Specifically, we first select the candidate answers relevant to the\nquestion or the image, then we rerank the candidate answers by a visual\nentailment task, which verifies whether the image semantically entails the\nsynthetic statement of the question and each candidate answer. Experimental\nresults show the effectiveness of our proposed framework, which establishes a\nnew state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:00:38 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Si", "Qingyi", ""], ["Lin", "Zheng", ""], ["Zheng", "Mingyu", ""], ["Fu", "Peng", ""], ["Wang", "Weiping", ""]]}, {"id": "2106.04619", "submitter": "Julius von K\\\"ugelgen", "authors": "Julius von K\\\"ugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel,\n  Bernhard Sch\\\"olkopf, Michel Besserve, Francesco Locatello", "title": "Self-Supervised Learning with Data Augmentations Provably Isolates\n  Content from Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised representation learning has shown remarkable success in a\nnumber of domains. A common practice is to perform data augmentation via\nhand-crafted transformations intended to leave the semantics of the data\ninvariant. We seek to understand the empirical success of this approach from a\ntheoretical perspective. We formulate the augmentation process as a latent\nvariable model by postulating a partition of the latent representation into a\ncontent component, which is assumed invariant to augmentation, and a style\ncomponent, which is allowed to change. Unlike prior work on disentanglement and\nindependent component analysis, we allow for both nontrivial statistical and\ncausal dependencies in the latent space. We study the identifiability of the\nlatent representation based on pairs of views of the observations and prove\nsufficient conditions that allow us to identify the invariant content partition\nup to an invertible mapping in both generative and discriminative settings. We\nfind numerical simulations with dependent latent variables are consistent with\nour theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional,\nvisually complex images with rich causal dependencies, which we use to study\nthe effect of data augmentations performed in practice.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:18:09 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["von K\u00fcgelgen", "Julius", ""], ["Sharma", "Yash", ""], ["Gresele", "Luigi", ""], ["Brendel", "Wieland", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Besserve", "Michel", ""], ["Locatello", "Francesco", ""]]}, {"id": "2106.04627", "submitter": "Matej Grcic", "authors": "Matej Grci\\'c, Ivan Grubi\\v{s}i\\'c, Sini\\v{s}a \\v{S}egvi\\'c", "title": "Densely connected normalizing flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Normalizing flows are bijective mappings between inputs and latent\nrepresentations with a fully factorized distribution. They are very attractive\ndue to exact likelihood evaluation and efficient sampling. However, their\neffective capacity is often insufficient since the bijectivity constraint\nlimits the model width. We address this issue by incrementally padding\nintermediate representations with noise. We precondition the noise in\naccordance with previous invertible units, which we describe as cross-unit\ncoupling. Our invertible glow-like modules express intra-unit affine coupling\nas a fusion of a densely connected block and Nystr\\\"om self-attention. We refer\nto our architecture as DenseFlow since both cross-unit and intra-unit couplings\nrely on dense connectivity. Experiments show significant improvements due to\nthe proposed contributions, and reveal state-of-the-art density estimation\namong all generative models under moderate computing budgets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:24:41 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Grci\u0107", "Matej", ""], ["Grubi\u0161i\u0107", "Ivan", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "2106.04630", "submitter": "Rongmei Lin", "authors": "Rongmei Lin, Xiang He, Jie Feng, Nasser Zalmout, Yan Liang, Li Xiong,\n  Xin Luna Dong", "title": "PAM: Understanding Product Images in Cross Product Category Attribute\n  Extraction", "comments": "KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467164", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding product attributes plays an important role in improving online\nshopping experience for customers and serves as an integral part for\nconstructing a product knowledge graph. Most existing methods focus on\nattribute extraction from text description or utilize visual information from\nproduct images such as shape and color. Compared to the inputs considered in\nprior works, a product image in fact contains more information, represented by\na rich mixture of words and visual clues with a layout carefully designed to\nimpress customers. This work proposes a more inclusive framework that fully\nutilizes these different modalities for attribute extraction. Inspired by\nrecent works in visual question answering, we use a transformer based sequence\nto sequence model to fuse representations of product text, Optical Character\nRecognition (OCR) tokens and visual objects detected in the product image. The\nframework is further extended with the capability to extract attribute value\nacross multiple product categories with a single model, by training the decoder\nto predict both product category and attribute value and conditioning its\noutput on product category. The model provides a unified attribute extraction\nsolution desirable at an e-commerce platform that offers numerous product\ncategories with a diverse body of product attributes. We evaluated the model on\ntwo product attributes, one with many possible values and one with a small set\nof possible values, over 14 product categories and found the model could\nachieve 15% gain on the Recall and 10% gain on the F1 score compared to\nexisting methods using text-only features.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:30:17 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lin", "Rongmei", ""], ["He", "Xiang", ""], ["Feng", "Jie", ""], ["Zalmout", "Nasser", ""], ["Liang", "Yan", ""], ["Xiong", "Li", ""], ["Dong", "Xin Luna", ""]]}, {"id": "2106.04632", "submitter": "Linjie Li", "authors": "Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai,\n  Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, Tamara Lee Berg,\n  Mohit Bansal, Jingjing Liu, Lijuan Wang, Zicheng Liu", "title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding\n  Evaluation", "comments": "VALUE is available at https://value-leaderboard.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most existing video-and-language (VidL) research focuses on a single dataset,\nor multiple datasets of a single task. In reality, a truly useful VidL system\nis expected to be easily generalizable to diverse tasks, domains, and datasets.\nTo facilitate the evaluation of such systems, we introduce Video-And-Language\nUnderstanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets\nover 3 popular tasks: (i) text-to-video retrieval; (ii) video question\nanswering; and (iii) video captioning. VALUE benchmark aims to cover a broad\nrange of video genres, video lengths, data volumes, and task difficulty levels.\nRather than focusing on single-channel videos with visual information only,\nVALUE promotes models that leverage information from both video frames and\ntheir associated subtitles, as well as models that share knowledge across\nmultiple tasks. We evaluate various baseline methods with and without\nlarge-scale VidL pre-training, and systematically investigate the impact of\nvideo input channels, fusion methods, and different video representations. We\nalso study the transferability between tasks, and conduct multi-task learning\nunder different settings. The significant gap between our best model and human\nperformance calls for future study for advanced VidL models. VALUE is available\nat https://value-leaderboard.github.io/.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:34:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Li", "Linjie", ""], ["Lei", "Jie", ""], ["Gan", "Zhe", ""], ["Yu", "Licheng", ""], ["Chen", "Yen-Chun", ""], ["Pillai", "Rohit", ""], ["Cheng", "Yu", ""], ["Zhou", "Luowei", ""], ["Wang", "Xin Eric", ""], ["Wang", "William Yang", ""], ["Berg", "Tamara Lee", ""], ["Bansal", "Mohit", ""], ["Liu", "Jingjing", ""], ["Wang", "Lijuan", ""], ["Liu", "Zicheng", ""]]}, {"id": "2106.04650", "submitter": "Hengyong Yu", "authors": "Dayang Wang, Zhan Wu, Hengyong Yu", "title": "TED-net: Convolution-free T2T Vision Transformer-based Encoder-decoder\n  Dilation network for Low-dose CT Denoising", "comments": "10 pages, manuscript for 12th International Workshop on Machine\n  Learning in Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low dose computed tomography is a mainstream for clinical applications.\nHow-ever, compared to normal dose CT, in the low dose CT (LDCT) images, there\nare stronger noise and more artifacts which are obstacles for practical\napplications. In the last few years, convolution-based end-to-end deep learning\nmethods have been widely used for LDCT image denoising. Recently, transformer\nhas shown superior performance over convolution with more feature interactions.\nYet its ap-plications in LDCT denoising have not been fully cultivated. Here,\nwe propose a convolution-free T2T vision transformer-based Encoder-decoder\nDilation net-work (TED-net) to enrich the family of LDCT denoising algorithms.\nThe model is free of convolution blocks and consists of a symmetric\nencoder-decoder block with sole transformer. Our model is evaluated on the\nAAPM-Mayo clinic LDCT Grand Challenge dataset, and results show outperformance\nover the state-of-the-art denoising methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 19:26:55 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wang", "Dayang", ""], ["Wu", "Zhan", ""], ["Yu", "Hengyong", ""]]}, {"id": "2106.04723", "submitter": "Stylianos Venieris", "authors": "Stylianos I. Venieris and Ioannis Panopoulos and Iakovos S. Venieris", "title": "OODIn: An Optimised On-Device Inference Framework for Heterogeneous\n  Mobile Devices", "comments": "Accepted at the 7th IEEE International Conference on Smart Computing\n  (SMARTCOMP), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radical progress in the field of deep learning (DL) has led to unprecedented\naccuracy in diverse inference tasks. As such, deploying DL models across mobile\nplatforms is vital to enable the development and broad availability of the\nnext-generation intelligent apps. Nevertheless, the wide and optimised\ndeployment of DL models is currently hindered by the vast system heterogeneity\nof mobile devices, the varying computational cost of different DL models and\nthe variability of performance needs across DL applications. This paper\nproposes OODIn, a framework for the optimised deployment of DL apps across\nheterogeneous mobile devices. OODIn comprises a novel DL-specific software\narchitecture together with an analytical framework for modelling DL\napplications that: (1) counteract the variability in device resources and DL\nmodels by means of a highly parametrised multi-layer design; and (2) perform a\nprincipled optimisation of both model- and system-level parameters through a\nmulti-objective formulation, designed for DL inference apps, in order to adapt\nthe deployment to the user-specified performance requirements and device\ncapabilities. Quantitative evaluation shows that the proposed framework\nconsistently outperforms status-quo designs across heterogeneous devices and\ndelivers up to 4.3x and 3.5x performance gain over highly optimised platform-\nand model-aware designs respectively, while effectively adapting execution to\ndynamic changes in resource availability.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 22:38:18 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Venieris", "Stylianos I.", ""], ["Panopoulos", "Ioannis", ""], ["Venieris", "Iakovos S.", ""]]}, {"id": "2106.04726", "submitter": "Ashkan Kazemi", "authors": "Ashkan Kazemi, Kiran Garimella, Gautam Kishore Shahi, Devin Gaffney,\n  Scott A. Hale", "title": "Tiplines to Combat Misinformation on Encrypted Platforms: A Case Study\n  of the 2019 Indian Election on WhatsApp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is currently no easy way to fact-check content on WhatsApp and other\nend-to-end encrypted platforms at scale. In this paper, we analyze the\nusefulness of a crowd-sourced \"tipline\" through which users can submit content\n(\"tips\") that they want fact-checked. We compare the tips sent to a WhatsApp\ntipline run during the 2019 Indian national elections with the messages\ncirculating in large, public groups on WhatsApp and other social media\nplatforms during the same period. We find that tiplines are a very useful lens\ninto WhatsApp conversations: a significant fraction of messages and images sent\nto the tipline match with the content being shared on public WhatsApp groups\nand other social media. Our analysis also shows that tiplines cover the most\npopular content well, and a majority of such content is often shared to the\ntipline before appearing in large, public WhatsApp groups. Overall, our\nfindings suggest tiplines can be an effective source for discovering content to\nfact-check.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 23:08:47 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 17:49:31 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Kazemi", "Ashkan", ""], ["Garimella", "Kiran", ""], ["Shahi", "Gautam Kishore", ""], ["Gaffney", "Devin", ""], ["Hale", "Scott A.", ""]]}, {"id": "2106.04732", "submitter": "Rebecca Roelofs", "authors": "David Berthelot, Rebecca Roelofs, Kihyuk Sohn, Nicholas Carlini, Alex\n  Kurakin", "title": "AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend semi-supervised learning to the problem of domain adaptation to\nlearn significantly higher-accuracy models that train on one data distribution\nand test on a different one. With the goal of generality, we introduce\nAdaMatch, a method that unifies the tasks of unsupervised domain adaptation\n(UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation\n(SSDA). In an extensive experimental study, we compare its behavior with\nrespective state-of-the-art techniques from SSL, SSDA, and UDA on vision\nclassification tasks. We find AdaMatch either matches or significantly exceeds\nthe state-of-the-art in each case using the same hyper-parameters regardless of\nthe dataset or task. For example, AdaMatch nearly doubles the accuracy compared\nto that of the prior state-of-the-art on the UDA task for DomainNet and even\nexceeds the accuracy of the prior state-of-the-art obtained with pre-training\nby 6.4% when AdaMatch is trained completely from scratch. Furthermore, by\nproviding AdaMatch with just one labeled example per class from the target\ndomain (i.e., the SSDA setting), we increase the target accuracy by an\nadditional 6.1%, and with 5 labeled examples, by 13.6%.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 23:39:12 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Berthelot", "David", ""], ["Roelofs", "Rebecca", ""], ["Sohn", "Kihyuk", ""], ["Carlini", "Nicholas", ""], ["Kurakin", "Alex", ""]]}, {"id": "2106.04767", "submitter": "Zhilu Zhang", "authors": "Zhilu Zhang, Vianne R. Gao, Mert R. Sabuncu", "title": "Ex uno plures: Splitting One Model into an Ensemble of Subnetworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) dropout is a simple and efficient ensembling method that can\nimprove the accuracy and confidence calibration of high-capacity deep neural\nnetwork models. However, MC dropout is not as effective as more\ncompute-intensive methods such as deep ensembles. This performance gap can be\nattributed to the relatively poor quality of individual models in the MC\ndropout ensemble and their lack of diversity. These issues can in turn be\ntraced back to the coupled training and substantial parameter sharing of the\ndropout models. Motivated by this perspective, we propose a strategy to compute\nan ensemble of subnetworks, each corresponding to a non-overlapping dropout\nmask computed via a pruning strategy and trained independently. We show that\nthe proposed subnetwork ensembling method can perform as well as standard deep\nensembles in both accuracy and uncertainty estimates, yet with a computational\nefficiency similar to MC dropout. Lastly, using several computer vision\ndatasets like CIFAR10/100, CUB200, and Tiny-Imagenet, we experimentally\ndemonstrate that subnetwork ensembling also consistently outperforms recently\nproposed approaches that efficiently ensemble neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 01:49:49 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhang", "Zhilu", ""], ["Gao", "Vianne R.", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2106.04776", "submitter": "Hao Sun", "authors": "Lele Luan, Yang Liu, Hao Sun", "title": "Uncovering Closed-form Governing Equations of Nonlinear Dynamics from\n  Videos", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distilling analytical models from data has the potential to advance our\nunderstanding and prediction of nonlinear dynamics. Although discovery of\ngoverning equations based on observed system states (e.g., trajectory time\nseries) has revealed success in a wide range of nonlinear dynamics, uncovering\nthe closed-form equations directly from raw videos still remains an open\nchallenge. To this end, we introduce a novel end-to-end unsupervised deep\nlearning framework to uncover the mathematical structure of equations that\ngoverns the dynamics of moving objects in videos. Such an architecture consists\nof (1) an encoder-decoder network that learns low-dimensional spatial/pixel\ncoordinates of the moving object, (2) a learnable Spatial-Physical\nTransformation component that creates mapping between the extracted\nspatial/pixel coordinates and the latent physical states of dynamics, and (3) a\nnumerical integrator-based sparse regression module that uncovers the\nparsimonious closed-form governing equations of learned physical states and,\nmeanwhile, serves as a constraint to the autoencoder. The efficacy of the\nproposed method is demonstrated by uncovering the governing equations of a\nvariety of nonlinear dynamical systems depicted by moving objects in videos.\nThe resulting computational framework enables discovery of parsimonious\ninterpretable model in a flexible and accessible sensing environment where only\nvideos are available.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 02:50:11 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Luan", "Lele", ""], ["Liu", "Yang", ""], ["Sun", "Hao", ""]]}, {"id": "2106.04778", "submitter": "Astitva Srivastava", "authors": "Sai Sagar Jinka, Rohan Chacko, Astitva Srivastava, Avinash Sharma,\n  P.J. Narayanan", "title": "SHARP: Shape-Aware Reconstruction of People In Loose Clothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D human body reconstruction from monocular images is an interesting and\nill-posed problem in computer vision with wider applications in multiple\ndomains. In this paper, we propose SHARP, a novel end-to-end trainable network\nthat accurately recovers the detailed geometry and appearance of 3D people in\nloose clothing from a monocular image. We propose a sparse and efficient fusion\nof a parametric body prior with a non-parametric peeled depth map\nrepresentation of clothed models. The parametric body prior constraints our\nmodel in two ways: first, the network retains geometrically consistent body\nparts that are not occluded by clothing, and second, it provides a body shape\ncontext that improves prediction of the peeled depth maps. This enables SHARP\nto recover fine-grained 3D geometrical details with just L1 losses on the 2D\nmaps, given an input image. We evaluate SHARP on publicly available Cloth3D and\nTHuman datasets and report superior performance to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 02:54:53 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 06:33:30 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Jinka", "Sai Sagar", ""], ["Chacko", "Rohan", ""], ["Srivastava", "Astitva", ""], ["Sharma", "Avinash", ""], ["Narayanan", "P. J.", ""]]}, {"id": "2106.04779", "submitter": "Ruihui Li", "authors": "Ruihui Li, Xianzhi Li, Pheng-Ann Heng, and Chi-Wing Fu", "title": "Point Cloud Upsampling via Disentangled Refinement", "comments": "CVPR 2021, website https://liruihui.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds produced by 3D scanning are often sparse, non-uniform, and\nnoisy. Recent upsampling approaches aim to generate a dense point set, while\nachieving both distribution uniformity and proximity-to-surface, and possibly\namending small holes, all in a single network. After revisiting the task, we\npropose to disentangle the task based on its multi-objective nature and\nformulate two cascaded sub-networks, a dense generator and a spatial refiner.\nThe dense generator infers a coarse but dense output that roughly describes the\nunderlying surface, while the spatial refiner further fine-tunes the coarse\noutput by adjusting the location of each point. Specifically, we design a pair\nof local and global refinement units in the spatial refiner to evolve a coarse\nfeature map. Also, in the spatial refiner, we regress a per-point offset vector\nto further adjust the coarse outputs in fine-scale. Extensive qualitative and\nquantitative results on both synthetic and real-scanned datasets demonstrate\nthe superiority of our method over the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 02:58:42 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Li", "Ruihui", ""], ["Li", "Xianzhi", ""], ["Heng", "Pheng-Ann", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "2106.04784", "submitter": "Byunggook Na", "authors": "Byunggook Na, Jisoo Mok, Hyeokjun Choe, Sungroh Yoon", "title": "Accelerating Neural Architecture Search via Proxy Data", "comments": "Accepted to IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite the increasing interest in neural architecture search (NAS), the\nsignificant computational cost of NAS is a hindrance to researchers. Hence, we\npropose to reduce the cost of NAS using proxy data, i.e., a representative\nsubset of the target data, without sacrificing search performance. Even though\ndata selection has been used across various fields, our evaluation of existing\nselection methods for NAS algorithms offered by NAS-Bench-1shot1 reveals that\nthey are not always appropriate for NAS and a new selection method is\nnecessary. By analyzing proxy data constructed using various selection methods\nthrough data entropy, we propose a novel proxy data selection method tailored\nfor NAS. To empirically demonstrate the effectiveness, we conduct thorough\nexperiments across diverse datasets, search spaces, and NAS algorithms.\nConsequently, NAS algorithms with the proposed selection discover architectures\nthat are competitive with those obtained using the entire dataset. It\nsignificantly reduces the search cost: executing DARTS with the proposed\nselection requires only 40 minutes on CIFAR-10 and 7.5 hours on ImageNet with a\nsingle GPU. Additionally, when the architecture searched on ImageNet using the\nproposed selection is inversely transferred to CIFAR-10, a state-of-the-art\ntest error of 2.4\\% is yielded. Our code is available at\nhttps://github.com/nabk89/NAS-with-Proxy-data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 03:08:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Na", "Byunggook", ""], ["Mok", "Jisoo", ""], ["Choe", "Hyeokjun", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2106.04803", "submitter": "Zihang Dai", "authors": "Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers have attracted increasing interests in computer vision, but they\nstill fall behind state-of-the-art convolutional networks. In this work, we\nshow that while Transformers tend to have larger model capacity, their\ngeneralization can be worse than convolutional networks due to the lack of the\nright inductive bias. To effectively combine the strengths from both\narchitectures, we present CoAtNets(pronounced \"coat\" nets), a family of hybrid\nmodels built from two key insights:(1) depthwise Convolution and self-Attention\ncan be naturally unified via simple relative attention; (2) vertically stacking\nconvolution layers and attention layers in a principled way is surprisingly\neffective in improving generalization, capacity and efficiency. Experiments\nshow that our CoAtNets achieve state-of-the-art performance under different\nresource constraints across various datasets. For example, CoAtNet achieves\n86.0% ImageNet top-1 accuracy without extra data, and 89.77% with extra JFT\ndata, outperforming prior arts of both convolutional networks and Transformers.\nNotably, when pre-trained with 13M images fromImageNet-21K, our CoAtNet\nachieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images\nfrom JFT while using 23x less data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 04:35:31 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Dai", "Zihang", ""], ["Liu", "Hanxiao", ""], ["Le", "Quoc V.", ""], ["Tan", "Mingxing", ""]]}, {"id": "2106.04822", "submitter": "Fatemeh Alishahi", "authors": "Fatemeh Alishahi and Amirhossein Mohajerin-Ariaei", "title": "Fast Computational Ghost Imaging using Unpaired Deep Learning and a\n  Constrained Generative Adversarial Network", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unpaired training can be the only option available for fast deep\nlearning-based ghost imaging, where obtaining a high signal-to-noise ratio\n(SNR) image copy of each low SNR ghost image could be practically\ntime-consuming and challenging. This paper explores the capabilities of deep\nlearning to leverage computational ghost imaging when there is a lack of paired\ntraining images. The deep learning approach proposed here enables fast ghost\nimaging through reconstruction of high SNR images from faint and hastily shot\nghost images using a constrained Wasserstein generative adversarial network. In\nthe proposed approach, the objective function is regularized to enforce the\ngeneration of faithful and relevant high SNR images to the ghost copies. This\nregularization measures the distance between reconstructed images and the faint\nghost images in a low-noise manifold generated by a shadow network. The\nperformance of the constrained network is shown to be particularly important\nfor ghost images with low SNR. The proposed pipeline is able to reconstruct\nhigh-quality images from the ghost images with SNR values not necessarily equal\nto the SNR of the training set.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 05:50:54 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Alishahi", "Fatemeh", ""], ["Mohajerin-Ariaei", "Amirhossein", ""]]}, {"id": "2106.04840", "submitter": "Xiao Wang", "authors": "Xiao Wang, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian, Feng Wu", "title": "Tracking by Joint Local and Global Search: A Target-aware Attention\n  based Approach", "comments": "Accepted by IEEE TNNLS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking-by-detection is a very popular framework for single object tracking\nwhich attempts to search the target object within a local search window for\neach frame. Although such local search mechanism works well on simple videos,\nhowever, it makes the trackers sensitive to extremely challenging scenarios,\nsuch as heavy occlusion and fast motion. In this paper, we propose a novel and\ngeneral target-aware attention mechanism (termed TANet) and integrate it with\ntracking-by-detection framework to conduct joint local and global search for\nrobust tracking. Specifically, we extract the features of target object patch\nand continuous video frames, then we concatenate and feed them into a decoder\nnetwork to generate target-aware global attention maps. More importantly, we\nresort to adversarial training for better attention prediction. The appearance\nand motion discriminator networks are designed to ensure its consistency in\nspatial and temporal views. In the tracking procedure, we integrate the\ntarget-aware attention with multiple trackers by exploring candidate search\nregions for robust tracking. Extensive experiments on both short-term and\nlong-term tracking benchmark datasets all validated the effectiveness of our\nalgorithm. The project page of this paper can be found at\n\\url{https://sites.google.com/view/globalattentiontracking/home/extend}.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 06:54:15 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wang", "Xiao", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""], ["Wang", "Yaowei", ""], ["Tian", "Yonghong", ""], ["Wu", "Feng", ""]]}, {"id": "2106.04852", "submitter": "Baoyun Peng", "authors": "Baoyun Peng, Min Liu, Heng Yang, Zhaoning Zhang, Dongsheng Li", "title": "Deep Tiny Network for Recognition-Oriented Face Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has made significant progress in recent years due to deep\nconvolutional neural networks (CNN). In many face recognition (FR) scenarios,\nface images are acquired from a sequence with huge intra-variations. These\nintra-variations, which are mainly affected by the low-quality face images,\ncause instability of recognition performance. Previous works have focused on\nad-hoc methods to select frames from a video or use face image quality\nassessment (FIQA) methods, which consider only a particular or combination of\nseveral distortions.\n  In this work, we present an efficient non-reference image quality assessment\nfor FR that directly links image quality assessment (IQA) and FR. More\nspecifically, we propose a new measurement to evaluate image quality without\nany reference. Based on the proposed quality measurement, we propose a deep\nTiny Face Quality network (tinyFQnet) to learn a quality prediction function\nfrom data.\n  We evaluate the proposed method for different powerful FR models on two\nclassical video-based (or template-based) benchmark: IJB-B and YTF. Extensive\nexperiments show that, although the tinyFQnet is much smaller than the others,\nthe proposed method outperforms state-of-the-art quality assessment methods in\nterms of effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 07:20:54 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Peng", "Baoyun", ""], ["Liu", "Min", ""], ["Yang", "Heng", ""], ["Zhang", "Zhaoning", ""], ["Li", "Dongsheng", ""]]}, {"id": "2106.04898", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Wei Yi", "title": "Continuous-discrete multiple target tracking with out-of-sequence\n  measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives the optimal Bayesian processing of an out-of-sequence\n(OOS) set of measurements in continuous-time for multiple target tracking. We\nconsider a multi-target system modelled in continuous time that is discretised\nat the time steps when we receive the measurements, which are distributed\naccording to the standard point target model. All information about this system\nat the sampled time steps is provided by the posterior density on the set of\nall trajectories. This density can be computed via the continuous-discrete\ntrajectory Poisson multi-Bernoulli mixture (TPMBM) filter. When we receive an\nOOS measurement, the optimal Bayesian processing performs a retrodiction step\nthat adds trajectory information at the OOS measurement time stamp followed by\nan update step. After the OOS measurement update, the posterior remains in\nTPMBM form. We also provide a computationally lighter alternative based on a\ntrajectory Poisson multi-Bernoulli filter. The effectiveness of the two\napproaches to handle OOS measurements is evaluated via simulations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:37:01 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Yi", "Wei", ""]]}, {"id": "2106.04914", "submitter": "Attila Lengyel", "authors": "Attila Lengyel, Jan C. van Gemert", "title": "Exploiting Learned Symmetries in Group Equivariant Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group Equivariant Convolutions (GConvs) enable convolutional neural networks\nto be equivariant to various transformation groups, but at an additional\nparameter and compute cost. We investigate the filter parameters learned by\nGConvs and find certain conditions under which they become highly redundant. We\nshow that GConvs can be efficiently decomposed into depthwise separable\nconvolutions while preserving equivariance properties and demonstrate improved\nperformance and data efficiency on two datasets. All code is publicly available\nat github.com/Attila94/SepGrouPy.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:50:22 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lengyel", "Attila", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2106.04919", "submitter": "Nibaran Das", "authors": "Hritam Basak, Rohit Kundu, Sukanta Chakraborty, Nibaran Das", "title": "Cervical Cytology Classification Using PCA & GWO Enhanced Deep Features\n  Selection", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cervical cancer is one of the most deadly and common diseases among women\nworldwide. It is completely curable if diagnosed in an early stage, but the\ntedious and costly detection procedure makes it unviable to conduct\npopulation-wise screening. Thus, to augment the effort of the clinicians, in\nthis paper, we propose a fully automated framework that utilizes Deep Learning\nand feature selection using evolutionary optimization for cytology image\nclassification. The proposed framework extracts Deep feature from several\nConvolution Neural Network models and uses a two-step feature reduction\napproach to ensure reduction in computation cost and faster convergence. The\nfeatures extracted from the CNN models form a large feature space whose\ndimensionality is reduced using Principal Component Analysis while preserving\n99% of the variance. A non-redundant, optimal feature subset is selected from\nthis feature space using an evolutionary optimization algorithm, the Grey Wolf\nOptimizer, thus improving the classification performance. Finally, the selected\nfeature subset is used to train an SVM classifier for generating the final\npredictions. The proposed framework is evaluated on three publicly available\nbenchmark datasets: Mendeley Liquid Based Cytology (4-class) dataset, Herlev\nPap Smear (7-class) dataset, and the SIPaKMeD Pap Smear (5-class) dataset\nachieving classification accuracies of 99.47%, 98.32% and 97.87% respectively,\nthus justifying the reliability of the approach. The relevant codes for the\nproposed approach can be found in:\nhttps://github.com/DVLP-CMATERJU/Two-Step-Feature-Enhancement\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:57:22 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Basak", "Hritam", ""], ["Kundu", "Rohit", ""], ["Chakraborty", "Sukanta", ""], ["Das", "Nibaran", ""]]}, {"id": "2106.04921", "submitter": "Yuhang Yang", "authors": "Yuhang Yang, Zilin Ding, Xuan Cheng, Xiaomin Wang, Ming Liu", "title": "Self-supervised Feature Enhancement: Applying Internal Pretext Task to\n  Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional self-supervised learning requires CNNs using external pretext\ntasks (i.e., image- or video-based tasks) to encode high-level semantic visual\nrepresentations. In this paper, we show that feature transformations within\nCNNs can also be regarded as supervisory signals to construct the\nself-supervised task, called \\emph{internal pretext task}. And such a task can\nbe applied for the enhancement of supervised learning. Specifically, we first\ntransform the internal feature maps by discarding different channels, and then\ndefine an additional internal pretext task to identify the discarded channels.\nCNNs are trained to predict the joint labels generated by the combination of\nself-supervised labels and original labels. By doing so, we let CNNs know which\nchannels are missing while classifying in the hope to mine richer feature\ninformation. Extensive experiments show that our approach is effective on\nvarious models and datasets. And it's worth noting that we only incur\nnegligible computational overhead. Furthermore, our approach can also be\ncompatible with other methods to get better results.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:59:35 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Yang", "Yuhang", ""], ["Ding", "Zilin", ""], ["Cheng", "Xuan", ""], ["Wang", "Xiaomin", ""], ["Liu", "Ming", ""]]}, {"id": "2106.04922", "submitter": "Zilin Ding", "authors": "Zilin Ding, Yuhang Yang, Xuan Cheng, Xiaomin Wang, Ming Liu", "title": "Self-supervision of Feature Transformation for Further Improving\n  Supervised Learning", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning, which benefits from automatically constructing\nlabels through pre-designed pretext task, has recently been applied for\nstrengthen supervised learning. Since previous self-supervised pretext tasks\nare based on input, they may incur huge additional training overhead. In this\npaper we find that features in CNNs can be also used for self-supervision. Thus\nwe creatively design the \\emph{feature-based pretext task} which requires only\na small amount of additional training overhead. In our task we discard\ndifferent particular regions of features, and then train the model to\ndistinguish these different features. In order to fully apply our feature-based\npretext task in supervised learning, we also propose a novel learning framework\ncontaining multi-classifiers for further improvement. Original labels will be\nexpanded to joint labels via self-supervision of feature transformations. With\nmore semantic information provided by our self-supervised tasks, this approach\ncan train CNNs more effectively. Extensive experiments on various supervised\nlearning tasks demonstrate the accuracy improvement and wide applicability of\nour method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 09:06:33 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ding", "Zilin", ""], ["Yang", "Yuhang", ""], ["Cheng", "Xuan", ""], ["Wang", "Xiaomin", ""], ["Liu", "Ming", ""]]}, {"id": "2106.04957", "submitter": "Ester Gonzalez-Sosa", "authors": "E. Gonzalez-Sosa, G. Robledo, D. Gonzalez-Morin, P. Perez-Garcia and\n  A. Villegas", "title": "Real Time Egocentric Object Segmentation: THU-READ Labeling and\n  Benchmarking Results", "comments": "Accepted for presentation at EPIC@CVPR2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric segmentation has attracted recent interest in the computer vision\ncommunity due to their potential in Mixed Reality (MR) applications. While most\nprevious works have been focused on segmenting egocentric human body parts\n(mainly hands), little attention has been given to egocentric objects. Due to\nthe lack of datasets of pixel-wise annotations of egocentric objects, in this\npaper we contribute with a semantic-wise labeling of a subset of 2124 images\nfrom the RGB-D THU-READ Dataset. We also report benchmarking results using\nThundernet, a real-time semantic segmentation network, that could allow future\nintegration with end-to-end MR applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 10:10:02 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Gonzalez-Sosa", "E.", ""], ["Robledo", "G.", ""], ["Gonzalez-Morin", "D.", ""], ["Perez-Garcia", "P.", ""], ["Villegas", "A.", ""]]}, {"id": "2106.04961", "submitter": "Lei Bi", "authors": "Kai-Chieh Liang, Lei Bi, Ashnil Kumar, Michael Fulham, Jinman Kim", "title": "Spatio-Temporal Dual-Stream Neural Network for Sequential Whole-Body PET\n  Segmentation", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential whole-body 18F-Fluorodeoxyglucose (FDG) positron emission\ntomography (PET) scans are regarded as the imaging modality of choice for the\nassessment of treatment response in the lymphomas because they detect treatment\nresponse when there may not be changes on anatomical imaging. Any computerized\nanalysis of lymphomas in whole-body PET requires automatic segmentation of the\nstudies so that sites of disease can be quantitatively monitored over time.\nState-of-the-art PET image segmentation methods are based on convolutional\nneural networks (CNNs) given their ability to leverage annotated datasets to\nderive high-level features about the disease process. Such methods, however,\nfocus on PET images from a single time-point and discard information from other\nscans or are targeted towards specific organs and cannot cater for the multiple\nstructures in whole-body PET images. In this study, we propose a\nspatio-temporal 'dual-stream' neural network (ST-DSNN) to segment sequential\nwhole-body PET scans. Our ST-DSNN learns and accumulates image features from\nthe PET images done over time. The accumulated image features are used to\nenhance the organs / structures that are consistent over time to allow easier\nidentification of sites of active lymphoma. Our results show that our method\noutperforms the state-of-the-art PET image segmentation methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 10:15:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Liang", "Kai-Chieh", ""], ["Bi", "Lei", ""], ["Kumar", "Ashnil", ""], ["Fulham", "Michael", ""], ["Kim", "Jinman", ""]]}, {"id": "2106.04966", "submitter": "Edmond S. L. Ho", "authors": "Kevin D. McCay, Edmond S. L. Ho, Dimitrios Sakkos, Wai Lok Woo, Claire\n  Marcroft, Patricia Dulson, Nicholas D. Embleton", "title": "Towards Explainable Abnormal Infant Movements Identification: A\n  Body-part Based Prediction and Visualisation Framework", "comments": "Proceedings of the 2021 IEEE EMBS International Conference on\n  Biomedical & Health Informatics (BHI), accepted, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing early diagnosis of cerebral palsy (CP) is key to enhancing the\ndevelopmental outcomes for those affected. Diagnostic tools such as the General\nMovements Assessment (GMA), have produced promising results in early diagnosis,\nhowever these manual methods can be laborious.\n  In this paper, we propose a new framework for the automated classification of\ninfant body movements, based upon the GMA, which unlike previous methods, also\nincorporates a visualization framework to aid with interpretability. Our\nproposed framework segments extracted features to detect the presence of\nFidgety Movements (FMs) associated with the GMA spatiotemporally. These\nfeatures are then used to identify the body-parts with the greatest\ncontribution towards a classification decision and highlight the related\nbody-part segment providing visual feedback to the user.\n  We quantitatively compare the proposed framework's classification performance\nwith several other methods from the literature and qualitatively evaluate the\nvisualization's veracity. Our experimental results show that the proposed\nmethod performs more robustly than comparable techniques in this setting whilst\nsimultaneously providing relevant visual interpretability.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 10:25:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["McCay", "Kevin D.", ""], ["Ho", "Edmond S. L.", ""], ["Sakkos", "Dimitrios", ""], ["Woo", "Wai Lok", ""], ["Marcroft", "Claire", ""], ["Dulson", "Patricia", ""], ["Embleton", "Nicholas D.", ""]]}, {"id": "2106.04989", "submitter": "Yi-Chen Lo", "authors": "Yi-Chen Lo, Chia-Che Chang, Hsuan-Chao Chiu, Yu-Hao Huang, Chia-Ping\n  Chen, Yu-Lin Chang, Kevin Jou", "title": "CLCC: Contrastive Learning for Color Constancy", "comments": "Accepted at CVPR 2021. Our code is available at\n  https://github.com/howardyclo/clcc-cvpr21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present CLCC, a novel contrastive learning framework for\ncolor constancy. Contrastive learning has been applied for learning\nhigh-quality visual representations for image classification. One key aspect to\nyield useful representations for image classification is to design illuminant\ninvariant augmentations. However, the illuminant invariant assumption conflicts\nwith the nature of the color constancy task, which aims to estimate the\nilluminant given a raw image. Therefore, we construct effective contrastive\npairs for learning better illuminant-dependent features via a novel raw-domain\ncolor augmentation. On the NUS-8 dataset, our method provides $17.5\\%$ relative\nimprovements over a strong baseline, reaching state-of-the-art performance\nwithout increasing model complexity. Furthermore, our method achieves\ncompetitive performance on the Gehler dataset with $3\\times$ fewer parameters\ncompared to top-ranking deep learning methods. More importantly, we show that\nour model is more robust to different scenes under close proximity of\nilluminants, significantly reducing $28.7\\%$ worst-case error in data-sparse\nregions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 11:16:31 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lo", "Yi-Chen", ""], ["Chang", "Chia-Che", ""], ["Chiu", "Hsuan-Chao", ""], ["Huang", "Yu-Hao", ""], ["Chen", "Chia-Ping", ""], ["Chang", "Yu-Lin", ""], ["Jou", "Kevin", ""]]}, {"id": "2106.04990", "submitter": "Shashanka Venkataramanan", "authors": "Shashanka Venkataramanan, Bill Psomas, Yannis Avrithis, Ewa Kijak,\n  Laurent Amsaleg, Konstantinos Karantzalos", "title": "It Takes Two to Tango: Mixup for Deep Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Metric learning involves learning a discriminative representation such that\nembeddings of similar classes are encouraged to be close, while embeddings of\ndissimilar classes are pushed far apart. State-of-the-art methods focus mostly\non sophisticated loss functions or mining strategies. On the one hand, metric\nlearning losses consider two or more examples at a time. On the other hand,\nmodern data augmentation methods for classification consider two or more\nexamples at a time. The combination of the two ideas is under-studied.\n  In this work, we aim to bridge this gap and improve representations using\nmixup, which is a powerful data augmentation approach interpolating two or more\nexamples and corresponding target labels at a time. This task is challenging\nbecause, unlike classification, the loss functions used in metric learning are\nnot additive over examples, so the idea of interpolating target labels is not\nstraightforward. To the best of our knowledge, we are the first to investigate\nmixing examples and target labels for deep metric learning. We develop a\ngeneralized formulation that encompasses existing metric learning loss\nfunctions and modify it to accommodate for mixup, introducing Metric Mix, or\nMetrix. We show that mixing inputs, intermediate representations or embeddings\nalong with target labels significantly improves representations and outperforms\nstate-of-the-art metric learning methods on four benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 11:20:03 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Venkataramanan", "Shashanka", ""], ["Psomas", "Bill", ""], ["Avrithis", "Yannis", ""], ["Kijak", "Ewa", ""], ["Amsaleg", "Laurent", ""], ["Karantzalos", "Konstantinos", ""]]}, {"id": "2106.04996", "submitter": "Kaiyu Li", "authors": "Sheng Fang, Kaiyu Li, Zhe Li", "title": "Salient Positions based Attention Network for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-attention mechanism has attracted wide publicity for its most\nimportant advantage of modeling long dependency, and its variations in computer\nvision tasks, the non-local block tries to model the global dependency of the\ninput feature maps. Gathering global contextual information will inevitably\nneed a tremendous amount of memory and computing resources, which has been\nextensively studied in the past several years. However, there is a further\nproblem with the self-attention scheme: is all information gathered from the\nglobal scope helpful for the contextual modelling? To our knowledge, few\nstudies have focused on the problem. Aimed at both questions this paper\nproposes the salient positions-based attention scheme SPANet, which is inspired\nby some interesting observations on the attention maps and affinity matrices\ngenerated in self-attention scheme. We believe these observations are\nbeneficial for better understanding of the self-attention. SPANet uses the\nsalient positions selection algorithm to select only a limited amount of\nsalient points to attend in the attention map computing. This approach will not\nonly spare a lot of memory and computing resources, but also try to distill the\npositive information from the transformation of the input feature maps. In the\nimplementation, considering the feature maps with channel high dimensions,\nwhich are completely different from the general visual image, we take the\nsquared power of the feature maps along the channel dimension as the saliency\nmetric of the positions. In general, different from the non-local block method,\nSPANet models the contextual information using only the selected positions\ninstead of all, along the channel dimension instead of space dimension. Our\nsource code is available at https://github.com/likyoo/SPANet.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 11:32:29 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Fang", "Sheng", ""], ["Li", "Kaiyu", ""], ["Li", "Zhe", ""]]}, {"id": "2106.05001", "submitter": "Mi Luo", "authors": "Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, Jiashi Feng", "title": "No Fear of Heterogeneity: Classifier Calibration for Federated Learning\n  with Non-IID Data", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge in training classification models in the real-world\nfederated system is learning with non-IID data. To cope with this, most of the\nexisting works involve enforcing regularization in local optimization or\nimproving the model aggregation scheme at the server. Other works also share\npublic datasets or synthesized samples to supplement the training of\nunder-represented classes or introduce a certain level of personalization.\nThough effective, they lack a deep understanding of how the data heterogeneity\naffects each layer of a deep classification model. In this paper, we bridge\nthis gap by performing an experimental analysis of the representations learned\nby different layers. Our observations are surprising: (1) there exists a\ngreater bias in the classifier than other layers, and (2) the classification\nperformance can be significantly improved by post-calibrating the classifier\nafter federated training. Motivated by the above findings, we propose a novel\nand simple algorithm called Classifier Calibration with Virtual Representations\n(CCVR), which adjusts the classifier using virtual representations sampled from\nan approximated gaussian mixture model. Experimental results demonstrate that\nCCVR achieves state-of-the-art performance on popular federated learning\nbenchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple\nyet effective method can shed some light on the future research of federated\nlearning with non-IID data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:02:29 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Luo", "Mi", ""], ["Chen", "Fei", ""], ["Hu", "Dapeng", ""], ["Zhang", "Yifan", ""], ["Liang", "Jian", ""], ["Feng", "Jiashi", ""]]}, {"id": "2106.05003", "submitter": "Guanchen Ding", "authors": "Jingyuan Chen, Guanchen Ding, Yuchen Yang, Wenwei Han, Kangmin Xu,\n  Tianyi Gao, Zhe Zhang, Wanping Ouyang, Hao Cai, Zhenzhong Chen", "title": "Dual-Modality Vehicle Anomaly Detection via Bilateral Trajectory Tracing", "comments": "9 pages, 5 figures, accepted to CVPRW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic anomaly detection has played a crucial role in Intelligent\nTransportation System (ITS). The main challenges of this task lie in the highly\ndiversified anomaly scenes and variational lighting conditions. Although much\nwork has managed to identify the anomaly in homogenous weather and scene, few\nresolved to cope with complex ones. In this paper, we proposed a dual-modality\nmodularized methodology for the robust detection of abnormal vehicles. We\nintroduced an integrated anomaly detection framework comprising the following\nmodules: background modeling, vehicle tracking with detection, mask\nconstruction, Region of Interest (ROI) backtracking, and dual-modality tracing.\nConcretely, we employed background modeling to filter the motion information\nand left the static information for later vehicle detection. For the vehicle\ndetection and tracking module, we adopted YOLOv5 and multi-scale tracking to\nlocalize the anomalies. Besides, we utilized the frame difference and tracking\nresults to identify the road and obtain the mask. In addition, we introduced\nmultiple similarity estimation metrics to refine the anomaly period via\nbacktracking. Finally, we proposed a dual-modality bilateral tracing module to\nrefine the time further. The experiments conducted on the Track 4 testset of\nthe NVIDIA 2021 AI City Challenge yielded a result of 0.9302 F1-Score and\n3.4039 root mean square error (RMSE), indicating the effectiveness of our\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:04:25 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chen", "Jingyuan", ""], ["Ding", "Guanchen", ""], ["Yang", "Yuchen", ""], ["Han", "Wenwei", ""], ["Xu", "Kangmin", ""], ["Gao", "Tianyi", ""], ["Zhang", "Zhe", ""], ["Ouyang", "Wanping", ""], ["Cai", "Hao", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2106.05036", "submitter": "Dawei Zhou", "authors": "Dawei Zhou, Tongliang Liu, Bo Han, Nannan Wang, Chunlei Peng, Xinbo\n  Gao", "title": "Towards Defending against Adversarial Examples via Attack-Invariant\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial noise. Their\nadversarial robustness can be improved by exploiting adversarial examples.\nHowever, given the continuously evolving attacks, models trained on seen types\nof adversarial examples generally cannot generalize well to unseen types of\nadversarial examples. To solve this problem, in this paper, we propose to\nremove adversarial noise by learning generalizable invariant features across\nattacks which maintain semantic classification information. Specifically, we\nintroduce an adversarial feature learning mechanism to disentangle invariant\nfeatures from adversarial noise. A normalization term has been proposed in the\nencoded space of the attack-invariant features to address the bias issue\nbetween the seen and unseen types of attacks. Empirical evaluations demonstrate\nthat our method could provide better protection in comparison to previous\nstate-of-the-art approaches, especially against unseen types of attacks and\nadaptive attacks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:49:54 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhou", "Dawei", ""], ["Liu", "Tongliang", ""], ["Han", "Bo", ""], ["Wang", "Nannan", ""], ["Peng", "Chunlei", ""], ["Gao", "Xinbo", ""]]}, {"id": "2106.05047", "submitter": "Hao Fang", "authors": "Hao Fang, Daoxin Zhang, Yi Zhang, Minghao Chen, Jiawei Li, Yao Hu,\n  Deng Cai and Xiaofei He", "title": "Salient Object Ranking with Position-Preserved Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation can detect where the objects are in an image, but hard\nto understand the relationship between them. We pay attention to a typical\nrelationship, relative saliency. A closely related task, salient object\ndetection, predicts a binary map highlighting a visually salient region while\nhard to distinguish multiple objects. Directly combining two tasks by\npost-processing also leads to poor performance. There is a lack of research on\nrelative saliency at present, limiting the practical applications such as\ncontent-aware image cropping, video summary, and image labeling.\n  In this paper, we study the Salient Object Ranking (SOR) task, which manages\nto assign a ranking order of each detected object according to its visual\nsaliency. We propose the first end-to-end framework of the SOR task and solve\nit in a multi-task learning fashion. The framework handles instance\nsegmentation and salient object ranking simultaneously. In this framework, the\nSOR branch is independent and flexible to cooperate with different detection\nmethods, so that easy to use as a plugin. We also introduce a\nPosition-Preserved Attention (PPA) module tailored for the SOR branch. It\nconsists of the position embedding stage and feature interaction stage.\nConsidering the importance of position in saliency comparison, we preserve\nabsolute coordinates of objects in ROI pooling operation and then fuse\npositional information with semantic features in the first stage. In the\nfeature interaction stage, we apply the attention mechanism to obtain\nproposals' contextualized representations to predict their relative ranking\norders. Extensive experiments have been conducted on the ASR dataset. Without\nbells and whistles, our proposed method outperforms the former state-of-the-art\nmethod significantly. The code will be released publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 13:00:05 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 02:23:59 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Fang", "Hao", ""], ["Zhang", "Daoxin", ""], ["Zhang", "Yi", ""], ["Chen", "Minghao", ""], ["Li", "Jiawei", ""], ["Hu", "Yao", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""]]}, {"id": "2106.05058", "submitter": "Ziyuan Huang", "authors": "Ziyuan Huang, Zhiwu Qing, Xiang Wang, Yutong Feng, Shiwei Zhang,\n  Jianwen Jiang, Zhurong Xia, Mingqian Tang, Nong Sang, Marcelo H. Ang Jr", "title": "Towards Training Stronger Video Vision Transformers for\n  EPIC-KITCHENS-100 Action Recognition", "comments": "CVPRW 2021, EPIC-KITCHENS-100 Competition Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent surge in the research of vision transformers, they have\ndemonstrated remarkable potential for various challenging computer vision\napplications, such as image recognition, point cloud classification as well as\nvideo understanding. In this paper, we present empirical results for training a\nstronger video vision transformer on the EPIC-KITCHENS-100 Action Recognition\ndataset. Specifically, we explore training techniques for video vision\ntransformers, such as augmentations, resolutions as well as initialization,\netc. With our training recipe, a single ViViT model achieves the performance of\n47.4\\% on the validation set of EPIC-KITCHENS-100 dataset, outperforming what\nis reported in the original paper by 3.4%. We found that video transformers are\nespecially good at predicting the noun in the verb-noun action prediction task.\nThis makes the overall action prediction accuracy of video transformers notably\nhigher than convolutional ones. Surprisingly, even the best video transformers\nunderperform the convolutional networks on the verb prediction. Therefore, we\ncombine the video vision transformers and some of the convolutional video\nnetworks and present our solution to the EPIC-KITCHENS-100 Action Recognition\ncompetition.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 13:26:02 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Huang", "Ziyuan", ""], ["Qing", "Zhiwu", ""], ["Wang", "Xiang", ""], ["Feng", "Yutong", ""], ["Zhang", "Shiwei", ""], ["Jiang", "Jianwen", ""], ["Xia", "Zhurong", ""], ["Tang", "Mingqian", ""], ["Sang", "Nong", ""], ["Ang", "Marcelo H.", "Jr"]]}, {"id": "2106.05082", "submitter": "Liheng Bian", "authors": "Lintao Peng, Liheng Bian, Tiexin Liu and Jun Zhang", "title": "Agile wide-field imaging with selective high resolution", "comments": "12pages,6figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide-field and high-resolution (HR) imaging is essential for various\napplications such as aviation reconnaissance, topographic mapping and safety\nmonitoring. The existing techniques require a large-scale detector array to\ncapture HR images of the whole field, resulting in high complexity and heavy\ncost. In this work, we report an agile wide-field imaging framework with\nselective high resolution that requires only two detectors. It builds on the\nstatistical sparsity prior of natural scenes that the important targets locate\nonly at small regions of interests (ROI), instead of the whole field. Under\nthis assumption, we use a short-focal camera to image wide field with a certain\nlow resolution, and use a long-focal camera to acquire the HR images of ROI. To\nautomatically locate ROI in the wide field in real time, we propose an\nefficient deep-learning based multiscale registration method that is robust and\nblind to the large setting differences (focal, white balance, etc) between the\ntwo cameras. Using the registered location, the long-focal camera mounted on a\ngimbal enables real-time tracking of the ROI for continuous HR imaging. We\ndemonstrated the novel imaging framework by building a proof-of-concept setup\nwith only 1181 gram weight, and assembled it on an unmanned aerial vehicle for\nair-to-ground monitoring. Experiments show that the setup maintains\n120$^{\\circ}$ wide field-of-view (FOV) with selective 0.45$mrad$ instantaneous\nFOV.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:01:34 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 04:14:53 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Peng", "Lintao", ""], ["Bian", "Liheng", ""], ["Liu", "Tiexin", ""], ["Zhang", "Jun", ""]]}, {"id": "2106.05094", "submitter": "Yancong Lin", "authors": "Yancong Lin, Silvia-Laura Pintea, Jan van Gemert", "title": "Semi-supervised lane detection with Deep Hough Transform", "comments": "ICIP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current work on lane detection relies on large manually annotated datasets.\nWe reduce the dependency on annotations by leveraging massive cheaply available\nunlabelled data. We propose a novel loss function exploiting geometric\nknowledge of lanes in Hough space, where a lane can be identified as a local\nmaximum. By splitting lanes into separate channels, we can localize each lane\nvia simple global max-pooling. The location of the maximum encodes the layout\nof a lane, while the intensity indicates the the probability of a lane being\npresent. Maximizing the log-probability of the maximal bins helps neural\nnetworks find lanes without labels. On the CULane and TuSimple datasets, we\nshow that the proposed Hough Transform loss improves performance significantly\nby learning from large amounts of unlabelled images.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:17:29 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Lin", "Yancong", ""], ["Pintea", "Silvia-Laura", ""], ["van Gemert", "Jan", ""]]}, {"id": "2106.05095", "submitter": "Lihe Yang", "authors": "Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, Yang Gao", "title": "ST++: Make Self-training Work Better for Semi-supervised Semantic\n  Segmentation", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate if we could make the self-training -- a simple\nbut popular framework -- work better for semi-supervised segmentation. Since\nthe core issue in semi-supervised setting lies in effective and efficient\nutilization of unlabeled data, we notice that increasing the diversity and\nhardness of unlabeled data is crucial to performance improvement. Being aware\nof this fact, we propose to adopt the most plain self-training scheme coupled\nwith appropriate strong data augmentations on unlabeled data (namely ST) for\nthis task, which surprisingly outperforms previous methods under various\nsettings without any bells and whistles. Moreover, to alleviate the negative\nimpact of the wrongly pseudo labeled images, we further propose an advanced\nself-training framework (namely ST++), that performs selective re-training via\nselecting and prioritizing the more reliable unlabeled images. As a result, the\nproposed ST++ boosts the performance of semi-supervised model significantly and\nsurpasses existing methods by a large margin on the Pascal VOC 2012 and\nCityscapes benchmark. Overall, we hope this straightforward and simple\nframework will serve as a strong baseline or competitor for future works. Code\nis available at https://github.com/LiheYoung/ST-PlusPlus.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:18:32 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Yang", "Lihe", ""], ["Zhuo", "Wei", ""], ["Qi", "Lei", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "2106.05106", "submitter": "Atul Sahay", "authors": "Atul Sahay and Imon Mukherjee and Kavi Arya", "title": "An Efficient Point of Gaze Estimator for Low-Resolution Imaging Systems\n  Using Extracted Ocular Features Based Neural Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A user's eyes provide means for Human Computer Interaction (HCI) research as\nan important modal. The time to time scientific explorations of the eye has\nalready seen an upsurge of the benefits in HCI applications from gaze\nestimation to the measure of attentiveness of a user looking at a screen for a\ngiven time period. The eye tracking system as an assisting, interactive tool\ncan be incorporated by physically disabled individuals, fitted best for those\nwho have eyes as only a limited set of communication. The threefold objective\nof this paper is - 1. To introduce a neural network based architecture to\npredict users' gaze at 9 positions displayed in the 11.31{\\deg} visual range on\nthe screen, through a low resolution based system such as a webcam in real time\nby learning various aspects of eyes as an ocular feature set. 2.A collection of\ncoarsely supervised feature set obtained in real time which is also validated\nthrough the user case study presented in the paper for 21 individuals ( 17 men\nand 4 women ) from whom a 35k set of instances was derived with an accuracy\nscore of 82.36% and f1_score of 82.2% and 3.A detailed study over applicability\nand underlying challenges of such systems. The experimental results verify the\nfeasibility and validity of the proposed eye gaze tracking model.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:35:55 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sahay", "Atul", ""], ["Mukherjee", "Imon", ""], ["Arya", "Kavi", ""]]}, {"id": "2106.05109", "submitter": "Daniel Frisch", "authors": "Daniel Frisch and Uwe D. Hanebeck", "title": "Gaussian Mixture Estimation from Weighted Samples", "comments": "7 pages, 2 (10) figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating the parameters of a Gaussian mixture density with a\ngiven number of components best representing a given set of weighted samples.\nWe adopt a density interpretation of the samples by viewing them as a discrete\nDirac mixture density over a continuous domain with weighted components. Hence,\nGaussian mixture fitting is viewed as density re-approximation. In order to\nspeed up computation, an expectation-maximization method is proposed that\nproperly considers not only the sample locations, but also the corresponding\nweights. It is shown that methods from literature do not treat the weights\ncorrectly, resulting in wrong estimates. This is demonstrated with simple\ncounterexamples. The proposed method works in any number of dimensions with the\nsame computational load as standard Gaussian mixture estimators for unweighted\nsamples.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:38:46 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Frisch", "Daniel", ""], ["Hanebeck", "Uwe D.", ""]]}, {"id": "2106.05113", "submitter": "Guy Gaziv", "authors": "Guy Gaziv, Michal Irani", "title": "More than meets the eye: Self-supervised depth reconstruction from brain\n  activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past few years, significant advancements were made in reconstruction\nof observed natural images from fMRI brain recordings using deep-learning\ntools. Here, for the first time, we show that dense 3D depth maps of observed\n2D natural images can also be recovered directly from fMRI brain recordings. We\nuse an off-the-shelf method to estimate the unknown depth maps of natural\nimages. This is applied to both: (i) the small number of images presented to\nsubjects in an fMRI scanner (images for which we have fMRI recordings -\nreferred to as \"paired\" data), and (ii) a very large number of natural images\nwith no fMRI recordings (\"unpaired data\"). The estimated depth maps are then\nused as an auxiliary reconstruction criterion to train for depth reconstruction\ndirectly from fMRI. We propose two main approaches: Depth-only recovery and\njoint image-depth RGBD recovery. Because the number of available \"paired\"\ntraining data (images with fMRI) is small, we enrich the training data via\nself-supervised cycle-consistent training on many \"unpaired\" data (natural\nimages & depth maps without fMRI). This is achieved using our newly defined and\ntrained Depth-based Perceptual Similarity metric as a reconstruction criterion.\nWe show that predicting the depth map directly from fMRI outperforms its\nindirect sequential recovery from the reconstructed images. We further show\nthat activations from early cortical visual areas dominate our depth\nreconstruction results, and propose means to characterize fMRI voxels by their\ndegree of depth-information tuning. This work adds an important layer of\ndecoded information, extending the current envelope of visual brain decoding\ncapabilities.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:46:09 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Gaziv", "Guy", ""], ["Irani", "Michal", ""]]}, {"id": "2106.05121", "submitter": "Diane Bouchacourt", "authors": "Diane Bouchacourt, Mark Ibrahim, Ari S. Morcos", "title": "Grounding inductive biases in natural images:invariance stems from\n  variations in data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform well on unseen and potentially out-of-distribution samples, it is\ndesirable for machine learning models to have a predictable response with\nrespect to transformations affecting the factors of variation of the input.\nInvariance is commonly achieved through hand-engineered data augmentation, but\ndo standard data augmentations address transformations that explain variations\nin real data? While prior work has focused on synthetic data, we attempt here\nto characterize the factors of variation in a real dataset, ImageNet, and study\nthe invariance of both standard residual networks and the recently proposed\nvision transformer with respect to changes in these factors. We show standard\naugmentation relies on a precise combination of translation and scale, with\ntranslation recapturing most of the performance improvement -- despite the\n(approximate) translation invariance built in to convolutional architectures,\nsuch as residual networks. In fact, we found that scale and translation\ninvariance was similar across residual networks and vision transformer models\ndespite their markedly different inductive biases. We show the training data\nitself is the main source of invariance, and that data augmentation only\nfurther increases the learned invariances. Interestingly, the invariances\nbrought from the training process align with the ImageNet factors of variation\nwe found. Finally, we find that the main factors of variation in ImageNet\nmostly relate to appearance and are specific to each class.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:58:57 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Bouchacourt", "Diane", ""], ["Ibrahim", "Mark", ""], ["Morcos", "Ari S.", ""]]}, {"id": "2106.05124", "submitter": "Siyuan Cao", "authors": "Si-Yuan Cao, Hui-Liang Shen, Lun Luo, Shu-Jie Chen, and Chunguang Li", "title": "PCNet: A Structure Similarity Enhancement Method for Multispectral and\n  Multimodal Image Registration", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral and multimodal image processing is important in the community\nof computer vision and computational photography. As the acquired multispectral\nand multimodal data are generally misaligned due to the alternation or movement\nof the image device, the image registration procedure is necessary. The\nregistration of multispectral or multimodal image is challenging due to the\nnon-linear intensity and gradient variation. To cope with this challenge, we\npropose the phase congruency network (PCNet), which is able to enhance the\nstructure similarity and alleviate the non-linear intensity and gradient\nvariation. The images can then be aligned using the similarity enhanced\nfeatures produced by the network. PCNet is constructed under the guidance of\nthe phase congruency prior. The network contains three trainable layers\naccompany with the modified learnable Gabor kernels according to the phase\ncongruency theory. Thanks to the prior knowledge, PCNet is extremely\nlight-weight and can be trained on quite a small amount of multispectral data.\nPCNet can be viewed to be fully convolutional and hence can take input of\narbitrary sizes. Once trained, PCNet is applicable on a variety of\nmultispectral and multimodal data such as RGB/NIR and flash/no-flash images\nwithout additional further tuning. Experimental results validate that PCNet\noutperforms current state-of-the-art registration algorithms, including the\ndeep-learning based ones that have the number of parameters hundreds times\ncompared to PCNet. Thanks to the similarity enhancement training, PCNet\noutperforms the original phase congruency algorithm with two-thirds less\nfeature channels.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:00:51 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cao", "Si-Yuan", ""], ["Shen", "Hui-Liang", ""], ["Luo", "Lun", ""], ["Chen", "Shu-Jie", ""], ["Li", "Chunguang", ""]]}, {"id": "2106.05132", "submitter": "Giorgio Ciano", "authors": "Giorgio Ciano, Paolo Andreini, Tommaso Mazzierli, Monica Bianchini and\n  Franco Scarselli", "title": "A multi-stage GAN for multi-organ chest X-ray image generation and\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-organ segmentation of X-ray images is of fundamental importance for\ncomputer aided diagnosis systems. However, the most advanced semantic\nsegmentation methods rely on deep learning and require a huge amount of labeled\nimages, which are rarely available due to both the high cost of human resources\nand the time required for labeling. In this paper, we present a novel\nmulti-stage generation algorithm based on Generative Adversarial Networks\n(GANs) that can produce synthetic images along with their semantic labels and\ncan be used for data augmentation. The main feature of the method is that,\nunlike other approaches, generation occurs in several stages, which simplifies\nthe procedure and allows it to be used on very small datasets. The method has\nbeen evaluated on the segmentation of chest radiographic images, showing\npromising results. The multistage approach achieves state-of-the-art and, when\nvery few images are used to train the GANs, outperforms the corresponding\nsingle-stage approach.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:15:19 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ciano", "Giorgio", ""], ["Andreini", "Paolo", ""], ["Mazzierli", "Tommaso", ""], ["Bianchini", "Monica", ""], ["Scarselli", "Franco", ""]]}, {"id": "2106.05144", "submitter": "Pau Riba", "authors": "Pau Riba, Adri\\`a Molina, Lluis Gomez, Oriol Ramos-Terrades and Josep\n  Llad\\'os", "title": "Learning to Rank Words: Optimizing Ranking Metrics for Word Spotting", "comments": "Accepted at ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we explore and evaluate the use of ranking-based objective\nfunctions for learning simultaneously a word string and a word image encoder.\nWe consider retrieval frameworks in which the user expects a retrieval list\nranked according to a defined relevance score. In the context of a word\nspotting problem, the relevance score has been set according to the string edit\ndistance from the query string. We experimentally demonstrate the competitive\nperformance of the proposed model on query-by-string word spotting for both,\nhandwritten and real scene word images. We also provide the results for\nquery-by-example word spotting, although it is not the main focus of this work.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:39:05 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Riba", "Pau", ""], ["Molina", "Adri\u00e0", ""], ["Gomez", "Lluis", ""], ["Ramos-Terrades", "Oriol", ""], ["Llad\u00f3s", "Josep", ""]]}, {"id": "2106.05152", "submitter": "Ju Sun", "authors": "Le Peng, Hengyue Liang, Taihui Li, Ju Sun", "title": "Rethink Transfer Learning in Medical Image Classification", "comments": "Under review for MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer learning (TL) with deep convolutional neural networks (DCNNs) has\nproved successful in medical image classification (MIC). However, the current\npractice is puzzling, as MIC typically relies only on low- and/or mid-level\nfeatures that are learned in the bottom layers of DCNNs. Following this\nintuition, we question the current strategies of TL in MIC. In this paper, we\nperform careful experimental comparisons between shallow and deep networks for\nclassification on two chest x-ray datasets, using different TL strategies. We\nfind that deep models are not always favorable, and finetuning truncated deep\nmodels almost always yields the best performance, especially in data-poor\nregimes.\n  Project webpage:\nhttps://sun-umn.github.io/Transfer-Learning-in-Medical-Imaging/\n  Keywords: Transfer learning, Medical image classification, Feature hierarchy,\nMedical imaging, Evaluation metrics, Imbalanced data\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:51:03 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 16:40:18 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Peng", "Le", ""], ["Liang", "Hengyue", ""], ["Li", "Taihui", ""], ["Sun", "Ju", ""]]}, {"id": "2106.05187", "submitter": "Wang Yifan", "authors": "Wang Yifan, Lukas Rahmann, Olga Sorkine-Hornung", "title": "Geometry-Consistent Neural Shape Representation with Implicit\n  Displacement Fields", "comments": "includes supplementary; ver2 corrected typos in eq(1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present implicit displacement fields, a novel representation for detailed\n3D geometry. Inspired by a classic surface deformation technique, displacement\nmapping, our method represents a complex surface as a smooth base surface plus\na displacement along the base's normal directions, resulting in a\nfrequency-based shape decomposition, where the high frequency signal is\nconstrained geometrically by the low frequency signal. Importantly, this\ndisentanglement is unsupervised thanks to a tailored architectural design that\nhas an innate frequency hierarchy by construction. We explore implicit\ndisplacement field surface reconstruction and detail transfer and demonstrate\nsuperior representational power, training stability and generalizability.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:26:18 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 09:25:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yifan", "Wang", ""], ["Rahmann", "Lukas", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "2106.05209", "submitter": "Shuxuan Guo", "authors": "Shuxuan Guo and Jose M. Alvarez and Mathieu Salzmann", "title": "Distilling Image Classifiers in Object Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation constitutes a simple yet effective way to improve the\nperformance of a compact student network by exploiting the knowledge of a more\npowerful teacher. Nevertheless, the knowledge distillation literature remains\nlimited to the scenario where the student and the teacher tackle the same task.\nHere, we investigate the problem of transferring knowledge not only across\narchitectures but also across tasks. To this end, we study the case of object\ndetection and, instead of following the standard detector-to-detector\ndistillation approach, introduce a classifier-to-detector knowledge transfer\nframework. In particular, we propose strategies to exploit the classification\nteacher to improve both the detector's recognition accuracy and localization\nperformance. Our experiments on several detectors with different backbones\ndemonstrate the effectiveness of our approach, allowing us to outperform the\nstate-of-the-art detector-to-detector distillation methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:50:10 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Guo", "Shuxuan", ""], ["Alvarez", "Jose M.", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2106.05210", "submitter": "Ho Kei Cheng", "authors": "Ho Kei Cheng, Yu-Wing Tai, Chi-Keung Tang", "title": "Rethinking Space-Time Networks with Improved Memory Coverage for\n  Efficient Video Object Segmentation", "comments": "Project page: https://hkchengrex.github.io/STCN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a simple yet effective approach to modeling space-time\ncorrespondences in the context of video object segmentation. Unlike most\nexisting approaches, we establish correspondences directly between frames\nwithout re-encoding the mask features for every object, leading to a highly\nefficient and robust framework. With the correspondences, every node in the\ncurrent query frame is inferred by aggregating features from the past in an\nassociative fashion. We cast the aggregation process as a voting problem and\nfind that the existing inner-product affinity leads to poor use of memory with\na small (fixed) subset of memory nodes dominating the votes, regardless of the\nquery. In light of this phenomenon, we propose using the negative squared\nEuclidean distance instead to compute the affinities. We validated that every\nmemory node now has a chance to contribute, and experimentally showed that such\ndiversified voting is beneficial to both memory efficiency and inference\naccuracy. The synergy of correspondence networks and diversified voting works\nexceedingly well, achieves new state-of-the-art results on both DAVIS and\nYouTubeVOS datasets while running significantly faster at 20+ FPS for multiple\nobjects without bells and whistles.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:50:57 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cheng", "Ho Kei", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "2106.05214", "submitter": "Sergio Naval Marimont", "authors": "Sergio Naval Marimont and Giacomo Tarroni", "title": "Implicit field learning for unsupervised anomaly detection in medical\n  images", "comments": "10 pages, 3 figures. Accepted for publication in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel unsupervised out-of-distribution detection method for\nmedical images based on implicit fields image representations. In our approach,\nan auto-decoder feed-forward neural network learns the distribution of healthy\nimages in the form of a mapping between spatial coordinates and probabilities\nover a proxy for tissue types. At inference time, the learnt distribution is\nused to retrieve, from a given test image, a restoration, i.e. an image\nmaximally consistent with the input one but belonging to the healthy\ndistribution. Anomalies are localized using the voxel-wise probability\npredicted by our model for the restored image. We tested our approach in the\ntask of unsupervised localization of gliomas on brain MR images and compared it\nto several other VAE-based anomaly detection methods. Results show that the\nproposed technique substantially outperforms them (average DICE 0.640 vs 0.518\nfor the best performing VAE-based alternative) while also requiring\nconsiderably less computing time.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:57:22 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Marimont", "Sergio Naval", ""], ["Tarroni", "Giacomo", ""]]}, {"id": "2106.05215", "submitter": "Sumit Mukherjee", "authors": "Sumit Mukherjee, Tina Sederholm, Anthony C. Roman, Ria Sankar, Sherrie\n  Caltagirone, Juan Lavista Ferres", "title": "A machine learning pipeline for aiding school identification from child\n  trafficking images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Child trafficking in a serious problem around the world. Every year there are\nmore than 4 million victims of child trafficking around the world, many of them\nfor the purposes of child sexual exploitation. In collaboration with UK Police\nand a non-profit focused on child abuse prevention, Global Emancipation\nNetwork, we developed a proof-of-concept machine learning pipeline to aid the\nidentification of children from intercepted images. In this work, we focus on\nimages that contain children wearing school uniforms to identify the school of\norigin. In the absence of a machine learning pipeline, this hugely time\nconsuming and labor intensive task is manually conducted by law enforcement\npersonnel. Thus, by automating aspects of the school identification process, we\nhope to significantly impact the speed of this portion of child identification.\nOur proposed pipeline consists of two machine learning models: i) to identify\nwhether an image of a child contains a school uniform in it, and ii)\nidentification of attributes of different school uniform items (such as\ncolor/texture of shirts, sweaters, blazers etc.). We describe the data\ncollection, labeling, model development and validation process, along with\nstrategies for efficient searching of schools using the model predictions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:57:58 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Mukherjee", "Sumit", ""], ["Sederholm", "Tina", ""], ["Roman", "Anthony C.", ""], ["Sankar", "Ria", ""], ["Caltagirone", "Sherrie", ""], ["Ferres", "Juan Lavista", ""]]}, {"id": "2106.05230", "submitter": "Javier Barbero-G\\'omez", "authors": "Javier Barbero-G\\'omez, Pedro-Antonio Guti\\'errez, V\\'ictor-Manuel\n  Vargas, Juan-Antonio Vallejo-Casas, C\\'esar Herv\\'as-Mart\\'inez", "title": "An ordinal CNN approach for the assessment of neurological damage in\n  Parkinson's disease patients", "comments": "To be published in Expert Systems with Applications, 33 pages, 6\n  figures", "journal-ref": null, "doi": "10.1016/j.eswa.2021.115271", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D image scans are an assessment tool for neurological damage in Parkinson's\ndisease (PD) patients. This diagnosis process can be automatized to help\nmedical staff through Decision Support Systems (DSSs), and Convolutional Neural\nNetworks (CNNs) are good candidates, because they are effective when applied to\nspatial data. This paper proposes a 3D CNN ordinal model for assessing the\nlevel or neurological damage in PD patients. Given that CNNs need large\ndatasets to achieve acceptable performance, a data augmentation method is\nadapted to work with spatial data. We consider the Ordinal Graph-based\nOversampling via Shortest Paths (OGO-SP) method, which applies a gamma\nprobability distribution for inter-class data generation. A modification of\nOGO-SP is proposed, the OGO-SP-$\\beta$ algorithm, which applies the beta\ndistribution for generating synthetic samples in the inter-class region, a\nbetter suited distribution when compared to gamma. The evaluation of the\ndifferent methods is based on a novel 3D image dataset provided by the Hospital\nUniversitario 'Reina Sof\\'ia' (C\\'ordoba, Spain). We show how the ordinal\nmethodology improves the performance with respect to the nominal one, and how\nOGO-SP-$\\beta$ yields better performance than OGO-SP.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:38:59 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Barbero-G\u00f3mez", "Javier", ""], ["Guti\u00e9rrez", "Pedro-Antonio", ""], ["Vargas", "V\u00edctor-Manuel", ""], ["Vallejo-Casas", "Juan-Antonio", ""], ["Herv\u00e1s-Mart\u00ednez", "C\u00e9sar", ""]]}, {"id": "2106.05233", "submitter": "Benjamin Walter", "authors": "Benjamin Walter", "title": "Analysis of convolutional neural network image classifiers in a\n  hierarchical max-pooling model with additional local pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is considered, and a hierarchical max-pooling model with\nadditional local pooling is introduced. Here the additional local pooling\nenables the hierachical model to combine parts of the image which have a\nvariable relative distance towards each other. Various convolutional neural\nnetwork image classifiers are introduced and compared in view of their rate of\nconvergence. The finite sample size performance of the estimates is analyzed by\napplying them to simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:08:00 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Walter", "Benjamin", ""]]}, {"id": "2106.05237", "submitter": "Xiaohua Zhai", "authors": "Lucas Beyer, Xiaohua Zhai, Am\\'elie Royer, Larisa Markeeva, Rohan\n  Anil, Alexander Kolesnikov", "title": "Knowledge distillation: A good teacher is patient and consistent", "comments": "Lucas, Xiaohua, Am\\'elie, Larisa, and Alex contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing discrepancy in computer vision between large-scale models\nthat achieve state-of-the-art performance and models that are affordable in\npractical applications. In this paper we address this issue and significantly\nbridge the gap between these two types of models. Throughout our empirical\ninvestigation we do not aim to necessarily propose a new method, but strive to\nidentify a robust and effective recipe for making state-of-the-art large scale\nmodels affordable in practice. We demonstrate that, when performed correctly,\nknowledge distillation can be a powerful tool for reducing the size of large\nmodels without compromising their performance. In particular, we uncover that\nthere are certain implicit design choices, which may drastically affect the\neffectiveness of distillation. Our key contribution is the explicit\nidentification of these design choices, which were not previously articulated\nin the literature. We back up our findings by a comprehensive empirical study,\ndemonstrate compelling results on a wide range of vision datasets and, in\nparticular, obtain a state-of-the-art ResNet-50 model for ImageNet, which\nachieves 82.8\\% top-1 accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:20:40 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Beyer", "Lucas", ""], ["Zhai", "Xiaohua", ""], ["Royer", "Am\u00e9lie", ""], ["Markeeva", "Larisa", ""], ["Anil", "Rohan", ""], ["Kolesnikov", "Alexander", ""]]}, {"id": "2106.05238", "submitter": "Matthew Willetts", "authors": "Matthew Willetts, Brooks Paige", "title": "I Don't Need $\\mathbf{u}$: Identifiable Non-Linear ICA Without Side\n  Information", "comments": "11 pages plus appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a new approach for identifiable non-linear ICA\nmodels. Recently there has been a renaissance in identifiability results in\ndeep generative models, not least for non-linear ICA. These prior works,\nhowever, have assumed access to a sufficiently-informative auxiliary set of\nobservations, denoted $\\mathbf{u}$. We show here how identifiability can be\nobtained in the absence of this side-information, rendering possible\nfully-unsupervised identifiable non-linear ICA. While previous theoretical\nresults have established the impossibility of identifiable non-linear ICA in\nthe presence of infinitely-flexible universal function approximators, here we\nrely on the intrinsically-finite modelling capacity of any particular chosen\nparameterisation of a deep generative model. In particular, we focus on\ngenerative models which perform clustering in their latent space -- a model\nstructure which matches previous identifiable models, but with the learnt\nclustering providing a synthetic form of auxiliary information. We evaluate our\nproposals using VAEs, on synthetic and image datasets, and find that the\nlearned clusterings function effectively: deep generative models with latent\nclusterings are empirically identifiable, to the same degree as models which\nrely on side information.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:22:08 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Willetts", "Matthew", ""], ["Paige", "Brooks", ""]]}, {"id": "2106.05241", "submitter": "Fabian Falck", "authors": "Fabian Falck, Haoting Zhang, Matthew Willetts, George Nicholson,\n  Christopher Yau, Christopher C Holmes", "title": "Multi-Facet Clustering Variational Autoencoders", "comments": "main text: 15 pages, appendices: 33 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work in deep clustering focuses on finding a single partition of data.\nHowever, high-dimensional data, such as images, typically feature multiple\ninteresting characteristics one could cluster over. For example, images of\nobjects against a background could be clustered over the shape of the object\nand separately by the colour of the background. In this paper, we introduce\nMulti-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of\nvariational autoencoders with a hierarchy of latent variables, each with a\nMixture-of-Gaussians prior, that learns multiple clusterings simultaneously,\nand is trained fully unsupervised and end-to-end. MFCVAE uses a\nprogressively-trained ladder architecture which leads to highly stable\nperformance. We provide novel theoretical results for optimising the ELBO\nanalytically with respect to the categorical variational posterior\ndistribution, and corrects earlier influential theoretical work. On image\nbenchmarks, we demonstrate that our approach separates out and clusters over\ndifferent aspects of the data in a disentangled manner. We also show other\nadvantages of our model: the compositionality of its latent space and that it\nprovides controlled generation of samples.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:36:38 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Falck", "Fabian", ""], ["Zhang", "Haoting", ""], ["Willetts", "Matthew", ""], ["Nicholson", "George", ""], ["Yau", "Christopher", ""], ["Holmes", "Christopher C", ""]]}, {"id": "2106.05258", "submitter": "Ali Jahanian", "authors": "Ali Jahanian, Xavier Puig, Yonglong Tian, Phillip Isola", "title": "Generative Models as a Data Source for Multiview Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative models are now capable of producing highly realistic images that\nlook nearly indistinguishable from the data on which they are trained. This\nraises the question: if we have good enough generative models, do we still need\ndatasets? We investigate this question in the setting of learning\ngeneral-purpose visual representations from a black-box generative model rather\nthan directly from data. Given an off-the-shelf image generator without any\naccess to its training data, we train representations from the samples output\nby this generator. We compare several representation learning methods that can\nbe applied to this setting, using the latent space of the generator to generate\nmultiple \"views\" of the same semantic content. We show that for contrastive\nmethods, this multiview data can naturally be used to identify positive pairs\n(nearby in latent space) and negative pairs (far apart in latent space). We\nfind that the resulting representations rival those learned directly from real\ndata, but that good performance requires care in the sampling strategy applied\nand the training method. Generative models can be viewed as a compressed and\norganized copy of a dataset, and we envision a future where more and more\n\"model zoos\" proliferate while datasets become increasingly unwieldy, missing,\nor private. This paper suggests several techniques for dealing with visual\nrepresentation learning in such a future. Code is released on our project page:\nhttps://ali-design.github.io/GenRep/\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:54:55 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Jahanian", "Ali", ""], ["Puig", "Xavier", ""], ["Tian", "Yonglong", ""], ["Isola", "Phillip", ""]]}, {"id": "2106.05261", "submitter": "Jiachun Li", "authors": "Bin Liang and Jiachun Li and Jianjun Huang", "title": "We Can Always Catch You: Detecting Adversarial Patched Objects WITH or\n  WITHOUT Signature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the object detection based on deep learning has proven to be\nvulnerable to adversarial patch attacks. The attackers holding a specially\ncrafted patch can hide themselves from the state-of-the-art person detectors,\ne.g., YOLO, even in the physical world. This kind of attack can bring serious\nsecurity threats, such as escaping from surveillance cameras. In this paper, we\ndeeply explore the detection problems about the adversarial patch attacks to\nthe object detection. First, we identify a leverageable signature of existing\nadversarial patches from the point of the visualization explanation. A fast\nsignature-based defense method is proposed and demonstrated to be effective.\nSecond, we design an improved patch generation algorithm to reveal the risk\nthat the signature-based way may be bypassed by the techniques emerging in the\nfuture. The newly generated adversarial patches can successfully evade the\nproposed signature-based defense. Finally, we present a novel\nsignature-independent detection method based on the internal content semantics\nconsistency rather than any attack-specific prior knowledge. The fundamental\nintuition is that the adversarial object can appear locally but disappear\nglobally in an input image. The experiments demonstrate that the\nsignature-independent method can effectively detect the existing and improved\nattacks. It has also proven to be a general method by detecting unforeseen and\neven other types of attacks without any attack-specific prior knowledge. The\ntwo proposed detection methods can be adopted in different scenarios, and we\nbelieve that combining them can offer a comprehensive protection.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:58:08 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 07:38:23 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liang", "Bin", ""], ["Li", "Jiachun", ""], ["Huang", "Jianjun", ""]]}, {"id": "2106.05264", "submitter": "Relja Arandjelovi\\'c", "authors": "Relja Arandjelovi\\'c, Andrew Zisserman", "title": "NeRF in detail: Learning to sample for view synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural radiance fields (NeRF) methods have demonstrated impressive novel view\nsynthesis performance. The core approach is to render individual rays by\nquerying a neural network at points sampled along the ray to obtain the density\nand colour of the sampled points, and integrating this information using the\nrendering equation. Since dense sampling is computationally prohibitive, a\ncommon solution is to perform coarse-to-fine sampling.\n  In this work we address a clear limitation of the vanilla coarse-to-fine\napproach -- that it is based on a heuristic and not trained end-to-end for the\ntask at hand. We introduce a differentiable module that learns to propose\nsamples and their importance for the fine network, and consider and compare\nmultiple alternatives for its neural architecture. Training the proposal module\nfrom scratch can be unstable due to lack of supervision, so an effective\npre-training strategy is also put forward. The approach, named `NeRF in detail'\n(NeRF-ID), achieves superior view synthesis quality over NeRF and the\nstate-of-the-art on the synthetic Blender benchmark and on par or better\nperformance on the real LLFF-NeRF scenes. Furthermore, by leveraging the\npredicted sample importance, a 25% saving in computation can be achieved\nwithout significantly sacrificing the rendering quality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:59:10 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Arandjelovi\u0107", "Relja", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2106.05266", "submitter": "Shaowei Liu", "authors": "Shaowei Liu, Hanwen Jiang, Jiarui Xu, Sifei Liu, Xiaolong Wang", "title": "Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in\n  Time", "comments": "CVPR 2021, Project page: https://stevenlsw.github.io/Semi-Hand-Object", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D hand and object pose from a single image is an extremely\nchallenging problem: hands and objects are often self-occluded during\ninteractions, and the 3D annotations are scarce as even humans cannot directly\nlabel the ground-truths from a single image perfectly. To tackle these\nchallenges, we propose a unified framework for estimating the 3D hand and\nobject poses with semi-supervised learning. We build a joint learning framework\nwhere we perform explicit contextual reasoning between hand and object\nrepresentations by a Transformer. Going beyond limited 3D annotations in a\nsingle image, we leverage the spatial-temporal consistency in large-scale\nhand-object videos as a constraint for generating pseudo labels in\nsemi-supervised learning. Our method not only improves hand pose estimation in\nchallenging real-world dataset, but also substantially improve the object pose\nwhich has fewer ground-truths per instance. By training with large-scale\ndiverse videos, our model also generalizes better across multiple out-of-domain\ndatasets. Project page and code: https://stevenlsw.github.io/Semi-Hand-Object\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:59:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Liu", "Shaowei", ""], ["Jiang", "Hanwen", ""], ["Xu", "Jiarui", ""], ["Liu", "Sifei", ""], ["Wang", "Xiaolong", ""]]}, {"id": "2106.05304", "submitter": "Ankit Goyal", "authors": "Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, Jia Deng", "title": "Revisiting Point Cloud Shape Classification with a Simple and Effective\n  Baseline", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing point cloud data is an important component of many real-world\nsystems. As such, a wide variety of point-based approaches have been proposed,\nreporting steady benchmark improvements over time. We study the key ingredients\nof this progress and uncover two critical results. First, we find that\nauxiliary factors like different evaluation schemes, data augmentation\nstrategies, and loss functions, which are independent of the model\narchitecture, make a large difference in performance. The differences are large\nenough that they obscure the effect of architecture. When these factors are\ncontrolled for, PointNet++, a relatively older network, performs competitively\nwith recent methods. Second, a very simple projection-based method, which we\nrefer to as SimpleView, performs surprisingly well. It achieves on par or\nbetter results than sophisticated state-of-the-art methods on ModelNet40 while\nbeing half the size of PointNet++. It also outperforms state-of-the-art methods\non ScanObjectNN, a real-world point cloud benchmark, and demonstrates better\ncross-dataset generalization. Code is available at\nhttps://github.com/princeton-vl/SimpleView.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 18:01:11 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Goyal", "Ankit", ""], ["Law", "Hei", ""], ["Liu", "Bowei", ""], ["Newell", "Alejandro", ""], ["Deng", "Jia", ""]]}, {"id": "2106.05308", "submitter": "Eduardo Arnold", "authors": "Eduardo Arnold, Sajjad Mozaffari, Mehrdad Dianati, Paul Jennings", "title": "Visual Sensor Pose Optimisation Using Rendering-based Visibility Models\n  for Robust Cooperative Perception", "comments": "15 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual Sensor Networks can be used in a variety of perception applications\nsuch as infrastructure support for autonomous driving in complex road segments.\nThe pose of the sensors in such networks directly determines the coverage of\nthe environment and objects therein, which impacts the performance of\napplications such as object detection and tracking. Existing sensor pose\noptimisation methods in the literature either maximise the coverage of ground\nsurfaces, or consider the visibility of the target objects as binary variables,\nwhich cannot represent various degrees of visibility. Such formulations cannot\nguarantee the visibility of the target objects as they fail to consider\nocclusions. This paper proposes two novel sensor pose optimisation methods,\nbased on gradient-ascent and Integer Programming techniques, which maximise the\nvisibility of multiple target objects in cluttered environments. Both methods\nconsider a realistic visibility model based on a rendering engine that provides\npixel-level visibility information about the target objects. The proposed\nmethods are evaluated in a complex environment and compared to existing methods\nin the literature. The evaluation results indicate that explicitly modelling\nthe visibility of target objects is critical to avoid occlusions in cluttered\nenvironments. Furthermore, both methods significantly outperform existing\nmethods in terms of object visibility.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 18:02:32 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Arnold", "Eduardo", ""], ["Mozaffari", "Sajjad", ""], ["Dianati", "Mehrdad", ""], ["Jennings", "Paul", ""]]}, {"id": "2106.05316", "submitter": "M. Hamed Mozaffari", "authors": "M. Hamed Mozaffari and Li-Lin Tay", "title": "Raman spectral analysis of mixtures with one-dimensional convolutional\n  neural network", "comments": "9 pages, 5 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the combination of robust one-dimensional convolutional neural\nnetworks (1-D CNNs) and Raman spectroscopy has shown great promise in rapid\nidentification of unknown substances with good accuracy. Using this technique,\nresearchers can recognize a pure compound and distinguish it from unknown\nsubstances in a mixture. The novelty of this approach is that the trained\nneural network operates automatically without any pre- or post-processing of\ndata. Some studies have attempted to extend this technique to the\nclassification of pure compounds in an unknown mixture. However, the\napplication of 1-D CNNs has typically been restricted to binary classifications\nof pure compounds. Here we will highlight a new approach in spectral\nrecognition and quantification of chemical components in a multicomponent\nmixture. Two 1-D CNN models, RaMixNet I and II, have been developed for this\npurpose. The former is for rapid classification of components in a mixture\nwhile the latter is for quantitative determination of those constituents. In\nthe proposed method, there is no limit to the number of compounds in a mixture.\nA data augmentation method is also introduced by adding random baselines to the\nRaman spectra. The experimental results revealed that the classification\naccuracy of RaMixNet I and II is 100% for analysis of unknown test mixtures; at\nthe same time, the RaMixNet II model may achieve a regression accuracy of 88%\nfor the quantification of each component.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 16:23:30 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Mozaffari", "M. Hamed", ""], ["Tay", "Li-Lin", ""]]}, {"id": "2106.05321", "submitter": "Michalis Lazarou Mr", "authors": "Michalis Lazarou, Yannis Avrithis, Tania Stathaki", "title": "Tensor feature hallucination for few-shot learning", "comments": "This is an extended work based on arXiv:2104.09467 including new\n  experiments, new experimental settings and further analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification addresses the challenge of classifying examples given\nnot just limited supervision but limited data as well. An attractive solution\nis synthetic data generation. However, most such methods are overly\nsophisticated, focusing on high-quality, realistic data in the input space. It\nis unclear whether adapting them to the few-shot regime and using them for the\ndownstream task of classification is the right approach. Previous works on\nsynthetic data generation for few-shot classification focus on exploiting\ncomplex models, e.g. a Wasserstein GAN with multiple regularizers or a network\nthat transfers latent diversities from known to novel classes.\n  We follow a different approach and investigate how a simple and\nstraightforward synthetic data generation method can be used effectively. We\nmake two contributions, namely we show that: (1) using a simple loss function\nis more than enough for training a feature generator in the few-shot setting;\nand (2) learning to generate tensor features instead of vector features is\nsuperior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show\nthat our method sets a new state of the art, outperforming more sophisticated\nfew-shot data augmentation methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 18:25:08 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Lazarou", "Michalis", ""], ["Avrithis", "Yannis", ""], ["Stathaki", "Tania", ""]]}, {"id": "2106.05350", "submitter": "Kevin Thandiackal", "authors": "Kevin Thandiackal (1 and 2), Tiziano Portenier (2), Andrea Giovannini\n  (1), Maria Gabrani (1), Orcun Goksel (2 and 3) ((1) IBM Research Europe, (2)\n  ETH Zurich, (3) Uppsala University)", "title": "Match What Matters: Generative Implicit Feature Replay for Continual\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are prone to catastrophic forgetting when trained\nincrementally on different tasks. In order to prevent forgetting, most existing\nmethods retain a small subset of previously seen samples, which in turn can be\nused for joint training with new tasks. While this is indeed effective, it may\nnot always be possible to store such samples, e.g., due to data protection\nregulations. In these cases, one can instead employ generative models to create\nartificial samples or features representing memories from previous tasks.\nFollowing a similar direction, we propose GenIFeR (Generative Implicit Feature\nReplay) for class-incremental learning. The main idea is to train a generative\nadversarial network (GAN) to generate images that contain realistic features.\nWhile the generator creates images at full resolution, the discriminator only\nsees the corresponding features extracted by the continually trained\nclassifier. Since the classifier compresses raw images into features that are\nactually relevant for classification, the GAN can match this target\ndistribution more accurately. On the other hand, allowing the generator to\ncreate full resolution images has several benefits: In contrast to previous\napproaches, the feature extractor of the classifier does not have to be frozen.\nIn addition, we can employ augmentations on generated images, which not only\nboosts classification performance, but also mitigates discriminator overfitting\nduring GAN training. We empirically show that GenIFeR is superior to both\nconventional generative image and feature replay. In particular, we\nsignificantly outperform the state-of-the-art in generative replay for various\nsettings on the CIFAR-100 and CUB-200 datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 19:29:41 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Thandiackal", "Kevin", "", "1 and 2"], ["Portenier", "Tiziano", "", "2 and 3"], ["Giovannini", "Andrea", "", "2 and 3"], ["Gabrani", "Maria", "", "2 and 3"], ["Goksel", "Orcun", "", "2 and 3"]]}, {"id": "2106.05375", "submitter": "Madhawa Vidanapathirana", "authors": "Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa, Angel X. Chang\n  and Manolis Savva", "title": "Plan2Scene: Converting Floorplans to 3D Scenes", "comments": "This paper is accepted to CVPR 2021. For code, data and pretrained\n  models, see https://3dlg-hcvc.github.io/plan2scene/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of converting a floorplan and a set of associated photos\nof a residence into a textured 3D mesh model, a task which we call Plan2Scene.\nOur system 1) lifts a floorplan image to a 3D mesh model; 2) synthesizes\nsurface textures based on the input photos; and 3) infers textures for\nunobserved surfaces using a graph neural network architecture. To train and\nevaluate our system we create indoor surface texture datasets, and augment a\ndataset of floorplans and photos from prior work with rectified surface crops\nand additional annotations. Our approach handles the challenge of producing\ntileable textures for dominant surfaces such as floors, walls, and ceilings\nfrom a sparse set of unaligned photos that only partially cover the residence.\nQualitative and quantitative evaluations show that our system produces\nrealistic 3D interior models, outperforming baseline approaches on a suite of\ntexture quality metrics and as measured by a holistic user study.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 20:32:20 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Vidanapathirana", "Madhawa", ""], ["Wu", "Qirui", ""], ["Furukawa", "Yasutaka", ""], ["Chang", "Angel X.", ""], ["Savva", "Manolis", ""]]}, {"id": "2106.05390", "submitter": "Julio Hurtado", "authors": "Julio Hurtado, Alain Raymond-Saez and Alvaro Soto", "title": "Optimizing Reusable Knowledge for Continual Learning via Metalearning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When learning tasks over time, artificial neural networks suffer from a\nproblem known as Catastrophic Forgetting (CF). This happens when the weights of\na network are overwritten during the training of a new task causing forgetting\nof old information. To address this issue, we propose MetA Reusable Knowledge\nor MARK, a new method that fosters weight reusability instead of overwriting\nwhen learning a new task. Specifically, MARK keeps a set of shared weights\namong tasks. We envision these shared weights as a common Knowledge Base (KB)\nthat is not only used to learn new tasks, but also enriched with new knowledge\nas the model learns new tasks. Key components behind MARK are two-fold. On the\none hand, a metalearning approach provides the key mechanism to incrementally\nenrich the KB with new knowledge and to foster weight reusability among tasks.\nOn the other hand, a set of trainable masks provides the key mechanism to\nselectively choose from the KB relevant weights to solve each task. By using\nMARK, we achieve state of the art results in several popular benchmarks,\nsurpassing the best performing methods in terms of average accuracy by over 10%\non the 20-Split-MiniImageNet dataset, while achieving almost zero forgetfulness\nusing 55% of the number of parameters. Furthermore, an ablation study provides\nevidence that, indeed, MARK is learning reusable knowledge that is selectively\nused by each task.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 21:09:21 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Hurtado", "Julio", ""], ["Raymond-Saez", "Alain", ""], ["Soto", "Alvaro", ""]]}, {"id": "2106.05392", "submitter": "Mandela Patrick", "authors": "Mandela Patrick, Dylan Campbell, Yuki M. Asano, Ishan Misra Florian\n  Metze, Christoph Feichtenhofer, Andrea Vedaldi, Jo\\\\~ao F. Henriques", "title": "Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers", "comments": "Project page: https://facebookresearch.github.io/Motionformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video transformers, the time dimension is often treated in the same way as\nthe two spatial dimensions. However, in a scene where objects or the camera may\nmove, a physical point imaged at one location in frame $t$ may be entirely\nunrelated to what is found at that location in frame $t+k$. These temporal\ncorrespondences should be modeled to facilitate learning about dynamic scenes.\nTo this end, we propose a new drop-in block for video transformers --\ntrajectory attention -- that aggregates information along implicitly determined\nmotion paths. We additionally propose a new method to address the quadratic\ndependence of computation and memory on the input size, which is particularly\nimportant for high resolution or long videos. While these ideas are useful in a\nrange of settings, we apply them to the specific task of video action\nrecognition with a transformer model and obtain state-of-the-art results on the\nKinetics, Something--Something V2, and Epic-Kitchens datasets. Code and models\nare available at: https://github.com/facebookresearch/Motionformer\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 21:16:05 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Patrick", "Mandela", ""], ["Campbell", "Dylan", ""], ["Asano", "Yuki M.", ""], ["Metze", "Ishan Misra Florian", ""], ["Feichtenhofer", "Christoph", ""], ["Vedaldi", "Andrea", ""], ["Henriques", "Jo\\\u00e3o F.", ""]]}, {"id": "2106.05430", "submitter": "Xin Ma", "authors": "Xin Ma and Won Hwa Kim", "title": "Very Compact Clusters with Structural Regularization via Similarity and\n  Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering algorithms have significantly improved along with Deep Neural\nNetworks which provide effective representation of data. Existing methods are\nbuilt upon deep autoencoder and self-training process that leverages the\ndistribution of cluster assignments of samples. However, as the fundamental\nobjective of the autoencoder is focused on efficient data reconstruction, the\nlearnt space may be sub-optimal for clustering. Moreover, it requires highly\neffective codes (i.e., representation) of data, otherwise the initial cluster\ncenters often cause stability issues during self-training. Many\nstate-of-the-art clustering algorithms use convolution operation to extract\nefficient codes but their applications are limited to image data. In this\nregard, we propose an end-to-end deep clustering algorithm, i.e., Very Compact\nClusters (VCC), for the general datasets, which takes advantage of\ndistributions of local relationships of samples near the boundary of clusters,\nso that they can be properly separated and pulled to cluster centers to form\ncompact clusters. Experimental results on various datasets illustrate that our\nproposed approach achieves better clustering performance over most of the\nstate-of-the-art clustering methods, and the data embeddings learned by VCC\nwithout convolution for image data are even comparable with specialized\nconvolutional methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 23:22:03 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:07:53 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ma", "Xin", ""], ["Kim", "Won Hwa", ""]]}, {"id": "2106.05436", "submitter": "Mengyuan Fang", "authors": "Mengyuan Fang, Luliang Tang, Zihan Kan, Xue Yang, Tao Pei, Qingquan\n  Li, Chaokui Li", "title": "An adaptive Origin-Destination flows cluster-detecting method to\n  identify urban mobility trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Origin-Destination (OD) flow, as an abstract representation of the object`s\nmovement or interaction, has been used to reveal the urban mobility and\nhuman-land interaction pattern. As an important spatial analysis approach, the\nclustering methods of point events have been extended to OD flows to identify\nthe dominant trends and spatial structures of urban mobility. However, the\nexisting methods for OD flow cluster-detecting are limited both in specific\nspatial scale and the uncertain result due to different parameters setting,\nwhich is difficult for complicated OD flows clustering under spatial\nheterogeneity. To address these limitations, in this paper, we proposed a novel\nOD flows cluster-detecting method based on the OPTICS algorithm which can\nidentify OD flow clusters with various aggregation scales. The method can\nadaptively determine parameter value from the dataset without prior knowledge\nand artificial intervention. Experiments indicated that our method outperformed\nthree state-of-the-art methods with more accurate and complete of clusters and\nless noise. As a case study, our method is applied to identify the potential\nroutes for public transport service settings by detecting OD flow clusters\nwithin urban travel data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 00:14:54 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Fang", "Mengyuan", ""], ["Tang", "Luliang", ""], ["Kan", "Zihan", ""], ["Yang", "Xue", ""], ["Pei", "Tao", ""], ["Li", "Qingquan", ""], ["Li", "Chaokui", ""]]}, {"id": "2106.05437", "submitter": "Shashank Bujimalla", "authors": "Shashank Bujimalla, Mahesh Subedar, Omesh Tickoo", "title": "Data augmentation to improve robustness of image captioning solutions", "comments": "CVPR VizWiz 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the impact of motion blur, a common quality flaw in\nreal world images, on a state-of-the-art two-stage image captioning solution,\nand notice a degradation in solution performance as blur intensity increases.\nWe investigate techniques to improve the robustness of the solution to motion\nblur using training data augmentation at each or both stages of the solution,\ni.e., object detection and captioning, and observe improved results. In\nparticular, augmenting both the stages reduces the CIDEr-D degradation for high\nmotion blur intensity from 68.7 to 11.7 on MS COCO dataset, and from 22.4 to\n6.8 on Vizwiz dataset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 00:17:50 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Bujimalla", "Shashank", ""], ["Subedar", "Mahesh", ""], ["Tickoo", "Omesh", ""]]}, {"id": "2106.05438", "submitter": "Alexander H. Liu", "authors": "Alexander H. Liu, SouYoung Jin, Cheng-I Jeff Lai, Andrew Rouditchenko,\n  Aude Oliva, James Glass", "title": "Cross-Modal Discrete Representation Learning", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in representation learning have demonstrated an ability to\nrepresent information from different modalities such as video, text, and audio\nin a single high-level embedding vector. In this work we present a\nself-supervised learning framework that is able to learn a representation that\ncaptures finer levels of granularity across different modalities such as\nconcepts or events represented by visual objects or spoken words. Our framework\nrelies on a discretized embedding space created via vector quantization that is\nshared across different modalities. Beyond the shared embedding space, we\npropose a Cross-Modal Code Matching objective that forces the representations\nfrom different views (modalities) to have a similar distribution over the\ndiscrete embedding space such that cross-modal objects/actions localization can\nbe performed without direct supervision. In our experiments we show that the\nproposed discretized multi-modal fine-grained representation (e.g.,\npixel/word/frame) can complement high-level summary representations (e.g.,\nvideo/sentence/waveform) for improved performance on cross-modal retrieval\ntasks. We also observe that the discretized representation uses individual\nclusters to represent the same semantic concept across modalities.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 00:23:33 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liu", "Alexander H.", ""], ["Jin", "SouYoung", ""], ["Lai", "Cheng-I Jeff", ""], ["Rouditchenko", "Andrew", ""], ["Oliva", "Aude", ""], ["Glass", "James", ""]]}, {"id": "2106.05441", "submitter": "Xin Xu", "authors": "Pengyu Xie, Xin Xu, Zheng Wang, and Toshihiko Yamasaki", "title": "Unsupervised Video Person Re-identification via Noise and Hard frame\n  Aware Clustering", "comments": "Appearing at ICME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised video-based person re-identification (re-ID) methods extract\nricher features from video tracklets than image-based ones. The\nstate-of-the-art methods utilize clustering to obtain pseudo-labels and train\nthe models iteratively. However, they underestimate the influence of two kinds\nof frames in the tracklet: 1) noise frames caused by detection errors or heavy\nocclusions exist in the tracklet, which may be allocated with unreliable labels\nduring clustering; 2) the tracklet also contains hard frames caused by pose\nchanges or partial occlusions, which are difficult to distinguish but\ninformative. This paper proposes a Noise and Hard frame Aware Clustering (NHAC)\nmethod. NHAC consists of a graph trimming module and a node re-sampling module.\nThe graph trimming module obtains stable graphs by removing noise frame nodes\nto improve the clustering accuracy. The node re-sampling module enhances the\ntraining of hard frame nodes to learn rich tracklet information. Experiments\nconducted on two video-based datasets demonstrate the effectiveness of the\nproposed NHAC under the unsupervised re-ID setting.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 00:52:06 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Xie", "Pengyu", ""], ["Xu", "Xin", ""], ["Wang", "Zheng", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2106.05453", "submitter": "Dawei Zhou", "authors": "Dawei Zhou, Nannan Wang, Xinbo Gao, Bo Han, Jun Yu, Xiaoyu Wang,\n  Tongliang Liu", "title": "Improving White-box Robustness of Pre-processing Defenses via Joint\n  Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial noise. A range of\nadversarial defense techniques have been proposed to mitigate the interference\nof adversarial noise, among which the input pre-processing methods are scalable\nand show great potential to safeguard DNNs. However, pre-processing methods may\nsuffer from the robustness degradation effect, in which the defense reduces\nrather than improving the adversarial robustness of a target model in a\nwhite-box setting. A potential cause of this negative effect is that\nadversarial training examples are static and independent to the pre-processing\nmodel. To solve this problem, we investigate the influence of full adversarial\nexamples which are crafted against the full model, and find they indeed have a\npositive impact on the robustness of defenses. Furthermore, we find that simply\nchanging the adversarial training examples in pre-processing methods does not\ncompletely alleviate the robustness degradation effect. This is due to the\nadversarial risk of the pre-processed model being neglected, which is another\ncause of the robustness degradation effect. Motivated by above analyses, we\npropose a method called Joint Adversarial Training based Pre-processing (JATP)\ndefense. Specifically, we formulate a feature similarity based adversarial risk\nfor the pre-processing model by using full adversarial examples found in a\nfeature space. Unlike standard adversarial training, we only update the\npre-processing model, which prompts us to introduce a pixel-wise loss to\nimprove its cross-model transferability. We then conduct a joint adversarial\ntraining on the pre-processing model to minimize this overall risk. Empirical\nresults show that our method could effectively mitigate the robustness\ndegradation effect across different target models in comparison to previous\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 01:45:32 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhou", "Dawei", ""], ["Wang", "Nannan", ""], ["Gao", "Xinbo", ""], ["Han", "Bo", ""], ["Yu", "Jun", ""], ["Wang", "Xiaoyu", ""], ["Liu", "Tongliang", ""]]}, {"id": "2106.05458", "submitter": "Yuhao Huang", "authors": "Xindi Hu, Limin Wang, Xin Yang, Xu Zhou, Wufeng Xue, Yan Cao,\n  Shengfeng Liu, Yuhao Huang, Shuangping Guo, Ning Shang, Dong Ni, and Ning Gu", "title": "Joint Landmark and Structure Learning for Automatic Evaluation of\n  Developmental Dysplasia of the Hip", "comments": "Accepted by IEEE Journal of Biomedical and Health Informatics. 14\n  pages, 10 figures and 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultrasound (US) screening of the infant hip is vital for the early\ndiagnosis of developmental dysplasia of the hip (DDH). The US diagnosis of DDH\nrefers to measuring alpha and beta angles that quantify hip joint development.\nThese two angles are calculated from key anatomical landmarks and structures of\nthe hip. However, this measurement process is not trivial for sonographers and\nusually requires a thorough understanding of complex anatomical structures. In\nthis study, we propose a multi-task framework to learn the relationships among\nlandmarks and structures jointly and automatically evaluate DDH. Our multi-task\nnetworks are equipped with three novel modules. Firstly, we adopt Mask R-CNN as\nthe basic framework to detect and segment key anatomical structures and add one\nlandmark detection branch to form a new multi-task framework. Secondly, we\npropose a novel shape similarity loss to refine the incomplete anatomical\nstructure prediction robustly and accurately. Thirdly, we further incorporate\nthe landmark-structure consistent prior to ensure the consistency of the bony\nrim estimated from the segmented structure and the detected landmark. In our\nexperiments, 1,231 US images of the infant hip from 632 patients are collected,\nof which 247 images from 126 patients are tested. The average errors in alpha\nand beta angles are 2.221 degrees and 2.899 degrees. About 93% and 85%\nestimates of alpha and beta angles have errors less than 5 degrees,\nrespectively. Experimental results demonstrate that the proposed method can\naccurately and robustly realize the automatic evaluation of DDH, showing great\npotential for clinical application.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 02:12:08 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Hu", "Xindi", ""], ["Wang", "Limin", ""], ["Yang", "Xin", ""], ["Zhou", "Xu", ""], ["Xue", "Wufeng", ""], ["Cao", "Yan", ""], ["Liu", "Shengfeng", ""], ["Huang", "Yuhao", ""], ["Guo", "Shuangping", ""], ["Shang", "Ning", ""], ["Ni", "Dong", ""], ["Gu", "Ning", ""]]}, {"id": "2106.05487", "submitter": "Khoa Nguyen Tuan", "authors": "Khoa Tuan Nguyen, Ganghee Jang and Won-ki Jeong", "title": "RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation", "comments": "Submitted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The segmentation of nanoscale electron microscopy (EM) images is crucial but\nchallenging in connectomics. Recent advances in deep learning have demonstrated\nthe significant potential of automatic segmentation for tera-scale EM images.\nHowever, none of the existing segmentation methods are error-free, and they\nrequire proofreading, which is typically implemented as an interactive,\nsemi-automatic process via manual intervention. Herein, we propose a fully\nautomatic proofreading method based on reinforcement learning. The main idea is\nto model the human decision process in proofreading using a reinforcement agent\nto achieve fully automatic proofreading. We systematically design the proposed\nsystem by combining multiple reinforcement learning agents in a hierarchical\nmanner, where each agent focuses only on a specific task while preserving\ndependency between agents. Furthermore, we also demonstrate that the episodic\ntask setting of reinforcement learning can efficiently manage a combination of\nmerge and split errors concurrently presented in the input. We demonstrate the\nefficacy of the proposed system by comparing it with state-of-the-art\nproofreading methods using various testing examples.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 04:02:41 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Nguyen", "Khoa Tuan", ""], ["Jang", "Ganghee", ""], ["Jeong", "Won-ki", ""]]}, {"id": "2106.05499", "submitter": "Hongsong Wang", "authors": "Hongsong Wang, Shengcai Liao, and Ling Shao", "title": "AFAN: Augmented Feature Alignment Network for Cross-Domain Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation for object detection is a challenging problem\nwith many real-world applications. Unfortunately, it has received much less\nattention than supervised object detection. Models that try to address this\ntask tend to suffer from a shortage of annotated training samples. Moreover,\nexisting methods of feature alignments are not sufficient to learn\ndomain-invariant representations. To address these limitations, we propose a\nnovel augmented feature alignment network (AFAN) which integrates intermediate\ndomain image generation and domain-adversarial training into a unified\nframework. An intermediate domain image generator is proposed to enhance\nfeature alignments by domain-adversarial training with automatically generated\nsoft domain labels. The synthetic intermediate domain images progressively\nbridge the domain divergence and augment the annotated source domain training\ndata. A feature pyramid alignment is designed and the corresponding feature\ndiscriminator is used to align multi-scale convolutional features of different\nsemantic levels. Last but not least, we introduce a region feature alignment\nand an instance discriminator to learn domain-invariant features for object\nproposals. Our approach significantly outperforms the state-of-the-art methods\non standard benchmarks for both similar and dissimilar domain adaptations.\nFurther extensive experiments verify the effectiveness of each component and\ndemonstrate that the proposed network can learn domain-invariant\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 05:01:20 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wang", "Hongsong", ""], ["Liao", "Shengcai", ""], ["Shao", "Ling", ""]]}, {"id": "2106.05517", "submitter": "Yang Liu", "authors": "Yang Liu, Weifeng Zhang, Chao Xiang, Tu Zheng, Deng Cai", "title": "Learning to Affiliate: Mutual Centralized Learning for Few-shot\n  Classification", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning (FSL) aims to learn a classifier that can be easily adapted\nto accommodate new tasks not seen during training, given only a few examples.\nTo handle the limited-data problem in few-shot regimes, recent methods tend to\ncollectively use a set of local features to densely represent an image instead\nof using a mixed global feature. They generally explore a unidirectional\nquery-to-support paradigm in FSL, e.g., find the nearest/optimal support\nfeature for each query feature and aggregate these local matches for a joint\nclassification. In this paper, we propose a new method Mutual Centralized\nLearning (MCL) to fully affiliate the two disjoint sets of dense features in a\nbidirectional paradigm. We associate each local feature with a particle that\ncan bidirectionally random walk in a discrete feature space by the\naffiliations. To estimate the class probability, we propose the features'\naccessibility that measures the expected number of visits to the support\nfeatures of that class in a Markov process. We relate our method to learning a\ncentrality on an affiliation network and demonstrate its capability to be\nplugged in existing methods by highlighting centralized local features.\nExperiments show that our method achieves the state-of-the-art on both\nminiImageNet and tieredImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 06:16:00 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liu", "Yang", ""], ["Zhang", "Weifeng", ""], ["Xiang", "Chao", ""], ["Zheng", "Tu", ""], ["Cai", "Deng", ""]]}, {"id": "2106.05519", "submitter": "Yuge Huang", "authors": "Xingkun Xu, Yuge Huang, Pengcheng Shen, Shaoxin Li, Jilin Li, Feiyue\n  Huang, Yong Li, Zhen Cui", "title": "Consistent Instance False Positive Improves Fairness in Face Recognition", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demographic bias is a significant challenge in practical face recognition\nsystems. Existing methods heavily rely on accurate demographic annotations.\nHowever, such annotations are usually unavailable in real scenarios. Moreover,\nthese methods are typically designed for a specific demographic group and are\nnot general enough. In this paper, we propose a false positive rate penalty\nloss, which mitigates face recognition bias by increasing the consistency of\ninstance False Positive Rate (FPR). Specifically, we first define the instance\nFPR as the ratio between the number of the non-target similarities above a\nunified threshold and the total number of the non-target similarities. The\nunified threshold is estimated for a given total FPR. Then, an additional\npenalty term, which is in proportion to the ratio of instance FPR overall FPR,\nis introduced into the denominator of the softmax-based loss. The larger the\ninstance FPR, the larger the penalty. By such unequal penalties, the instance\nFPRs are supposed to be consistent. Compared with the previous debiasing\nmethods, our method requires no demographic annotations. Thus, it can mitigate\nthe bias among demographic groups divided by various attributes, and these\nattributes are not needed to be previously predefined during training.\nExtensive experimental results on popular benchmarks demonstrate the\nsuperiority of our method over state-of-the-art competitors. Code and trained\nmodels are available at https://github.com/Tencent/TFace.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 06:20:37 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Xu", "Xingkun", ""], ["Huang", "Yuge", ""], ["Shen", "Pengcheng", ""], ["Li", "Shaoxin", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Li", "Yong", ""], ["Cui", "Zhen", ""]]}, {"id": "2106.05525", "submitter": "Shahnewaz Ali", "authors": "Yaqub Jonmohamadi, Shahnewaz Ali, Fengbei Liu, Jonathan Roberts, Ross\n  Crawford, Gustavo Carneiro, Ajay K. Pandey", "title": "3D Semantic Mapping from Arthroscopy using Out-of-distribution Pose and\n  Depth and In-distribution Segmentation Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimally invasive surgery (MIS) has many documented advantages, but the\nsurgeon's limited visual contact with the scene can be problematic. Hence,\nsystems that can help surgeons navigate, such as a method that can produce a 3D\nsemantic map, can compensate for the limitation above. In theory, we can borrow\n3D semantic mapping techniques developed for robotics, but this requires\nfinding solutions to the following challenges in MIS: 1) semantic segmentation,\n2) depth estimation, and 3) pose estimation. In this paper, we propose the\nfirst 3D semantic mapping system from knee arthroscopy that solves the three\nchallenges above. Using out-of-distribution non-human datasets, where pose\ncould be labeled, we jointly train depth+pose estimators using selfsupervised\nand supervised losses. Using an in-distribution human knee dataset, we train a\nfully-supervised semantic segmentation system to label arthroscopic image\npixels into femur, ACL, and meniscus. Taking testing images from human knees,\nwe combine the results from these two systems to automatically create 3D\nsemantic maps of the human knee. The result of this work opens the pathway to\nthe generation of intraoperative 3D semantic mapping, registration with\npre-operative data, and robotic-assisted arthroscopy\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 06:28:44 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Jonmohamadi", "Yaqub", ""], ["Ali", "Shahnewaz", ""], ["Liu", "Fengbei", ""], ["Roberts", "Jonathan", ""], ["Crawford", "Ross", ""], ["Carneiro", "Gustavo", ""], ["Pandey", "Ajay K.", ""]]}, {"id": "2106.05528", "submitter": "Rui Wang", "authors": "Rui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, Yu-Gang\n  Jiang", "title": "Cross-domain Contrastive Learning for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na fully-labeled source domain to a different unlabeled target domain. Most\nexisting UDA methods learn domain-invariant feature representations by\nminimizing feature distances across domains. In this work, we build upon\ncontrastive self-supervised learning to align features so as to reduce the\ndomain discrepancy between training and testing sets. Exploring the same set of\ncategories shared by both domains, we introduce a simple yet effective\nframework CDCL, for domain alignment. In particular, given an anchor image from\none domain, we minimize its distances to cross-domain samples from the same\nclass relative to those from different categories. Since target labels are\nunavailable, we use a clustering-based approach with carefully initialized\ncenters to produce pseudo labels. In addition, we demonstrate that CDCL is a\ngeneral framework and can be adapted to the data-free setting, where the source\ndata are unavailable during training, with minimal modification. We conduct\nexperiments on two widely used domain adaptation benchmarks, i.e., Office-31\nand VisDA-2017, and demonstrate that CDCL achieves state-of-the-art performance\non both datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 06:32:30 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wang", "Rui", ""], ["Wu", "Zuxuan", ""], ["Weng", "Zejia", ""], ["Chen", "Jingjing", ""], ["Qi", "Guo-Jun", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2106.05531", "submitter": "Ivan Bajic", "authors": "Ashiv Dhondea, Robert A. Cohen, Ivan V. Baji\\'c", "title": "CALTeC: Content-Adaptive Linear Tensor Completion for Collaborative\n  Intelligence", "comments": "5 pages, 4 figures, accepted for presentation at IEEE ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In collaborative intelligence, an artificial intelligence (AI) model is\ntypically split between an edge device and the cloud. Feature tensors produced\nby the edge sub-model are sent to the cloud via an imperfect communication\nchannel. At the cloud side, parts of the feature tensor may be missing due to\npacket loss. In this paper we propose a method called Content-Adaptive Linear\nTensor Completion (CALTeC) to recover the missing feature data. The proposed\nmethod is fast, data-adaptive, does not require pre-training, and produces\nbetter results than existing methods for tensor data recovery in collaborative\nintelligence.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 06:45:52 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Dhondea", "Ashiv", ""], ["Cohen", "Robert A.", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "2106.05542", "submitter": "Eun-Soo Jung Ph.D.", "authors": "Eun-Soo Jung, HyeongGwan Son, Kyusam Oh, Yongkeun Yun, Soonhwan Kwon,\n  Min Soo Kim", "title": "DUET: Detection Utilizing Enhancement for Text in Scanned or Captured\n  Documents", "comments": null, "journal-ref": "2020 25th International Conference on Pattern Recognition (ICPR)", "doi": "10.1109/ICPR48806.2021.9412928", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep neural model for text detection in document images.\nFor robust text detection in noisy scanned documents, the advantages of\nmulti-task learning are adopted by adding an auxiliary task of text\nenhancement. Namely, our proposed model is designed to perform noise reduction\nand text region enhancement as well as text detection. Moreover, we enrich the\ntraining data for the model with synthesized document images that are fully\nlabeled for text detection and enhancement, thus overcome the insufficiency of\nlabeled document image data. For the effective exploitation of the synthetic\nand real data, the training process is separated in two phases. The first phase\nis training only synthetic data in a fully-supervised manner. Then real data\nwith only detection labels are added in the second phase. The enhancement task\nfor the real data is weakly-supervised with information from their detection\nlabels. Our methods are demonstrated in a real document dataset with\nperformances exceeding those of other text detection methods. Moreover,\nablations are conducted and the results confirm the effectiveness of the\nsynthetic data, auxiliary task, and weak-supervision. Whereas the existing text\ndetection studies mostly focus on the text in scenes, our proposed method is\noptimized to the applications for the text in scanned documents.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 07:08:31 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Jung", "Eun-Soo", ""], ["Son", "HyeongGwan", ""], ["Oh", "Kyusam", ""], ["Yun", "Yongkeun", ""], ["Kwon", "Soonhwan", ""], ["Kim", "Min Soo", ""]]}, {"id": "2106.05545", "submitter": "Yibo Guo", "authors": "Yibo Guo, Haidi Wang, Yiming Fan, Shunyao Li, Mingliang Xu", "title": "Super-Resolution Image Reconstruction Based on Self-Calibrated\n  Convolutional GAN", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the effective application of deep learning in computer vision,\nbreakthroughs have been made in the research of super-resolution images\nreconstruction. However, many researches have pointed out that the\ninsufficiency of the neural network extraction on image features may bring the\ndeteriorating of newly reconstructed image. On the other hand, the generated\npictures are sometimes too artificial because of over-smoothing. In order to\nsolve the above problems, we propose a novel self-calibrated convolutional\ngenerative adversarial networks. The generator consists of feature extraction\nand image reconstruction. Feature extraction uses self-calibrated convolutions,\nwhich contains four portions, and each portion has specific functions. It can\nnot only expand the range of receptive fields, but also obtain long-range\nspatial and inter-channel dependencies. Then image reconstruction is performed,\nand finally a super-resolution image is reconstructed. We have conducted\nthorough experiments on different datasets including set5, set14 and BSD100\nunder the SSIM evaluation method. The experimental results prove the\neffectiveness of the proposed network.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 07:12:27 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Guo", "Yibo", ""], ["Wang", "Haidi", ""], ["Fan", "Yiming", ""], ["Li", "Shunyao", ""], ["Xu", "Mingliang", ""]]}, {"id": "2106.05549", "submitter": "Julia Rosenzweig", "authors": "Julia Rosenzweig, Eduardo Brito, Hans-Ulrich Kobialka, Maram Akila,\n  Nico M. Schmidt, Peter Schlicht, Jan David Schneider, Fabian H\\\"uger,\n  Matthias Rottmann, Sebastian Houben, Tim Wirtz", "title": "Validation of Simulation-Based Testing: Bypassing Domain Shift with\n  Label-to-Image Synthesis", "comments": "The first two authors contributed equally. Accepted at the 4th\n  Workshop on \"Ensuring and Validating Safety for Automated Vehicles\" (WS13),\n  IV2021. Under IEEE Copyright", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications can benefit from simulated data for\nsystematic validation - in particular if real-life data is difficult to obtain\nor annotate. However, since simulations are prone to domain shift w.r.t.\nreal-life data, it is crucial to verify the transferability of the obtained\nresults. We propose a novel framework consisting of a generative label-to-image\nsynthesis model together with different transferability measures to inspect to\nwhat extent we can transfer testing results of semantic segmentation models\nfrom synthetic data to equivalent real-life data. With slight modifications,\nour approach is extendable to, e.g., general multi-class classification tasks.\nGrounded on the transferability analysis, our approach additionally allows for\nextensive testing by incorporating controlled simulations. We validate our\napproach empirically on a semantic segmentation task on driving scenes.\nTransferability is tested using correlation analysis of IoU and a learned\ndiscriminator. Although the latter can distinguish between real-life and\nsynthetic tests, in the former we observe surprisingly strong correlations of\n0.7 for both cars and pedestrians.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 07:23:58 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Rosenzweig", "Julia", ""], ["Brito", "Eduardo", ""], ["Kobialka", "Hans-Ulrich", ""], ["Akila", "Maram", ""], ["Schmidt", "Nico M.", ""], ["Schlicht", "Peter", ""], ["Schneider", "Jan David", ""], ["H\u00fcger", "Fabian", ""], ["Rottmann", "Matthias", ""], ["Houben", "Sebastian", ""], ["Wirtz", "Tim", ""]]}, {"id": "2106.05554", "submitter": "Zefan Li", "authors": "Zefan Li, Chenxi Liu, Alan Yuille, Bingbing Ni, Wenjun Zhang and Wen\n  Gao", "title": "Progressive Stage-wise Learning for Unsupervised Feature Representation\n  Enhancement", "comments": "Accepted by the IEEE conference on computer vision and pattern\n  recognition. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning methods have recently shown their competitiveness\nagainst supervised training. Typically, these methods use a single objective to\ntrain the entire network. But one distinct advantage of unsupervised over\nsupervised learning is that the former possesses more variety and freedom in\ndesigning the objective. In this work, we explore new dimensions of\nunsupervised learning by proposing the Progressive Stage-wise Learning (PSL)\nframework. For a given unsupervised task, we design multilevel tasks and define\ndifferent learning stages for the deep network. Early learning stages are\nforced to focus on lowlevel tasks while late stages are guided to extract\ndeeper information through harder tasks. We discover that by progressive\nstage-wise learning, unsupervised feature representation can be effectively\nenhanced. Our extensive experiments show that PSL consistently improves results\nfor the leading unsupervised learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 07:33:19 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 13:50:38 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Li", "Zefan", ""], ["Liu", "Chenxi", ""], ["Yuille", "Alan", ""], ["Ni", "Bingbing", ""], ["Zhang", "Wenjun", ""], ["Gao", "Wen", ""]]}, {"id": "2106.05596", "submitter": "Sanka Rasnayaka", "authors": "Sachith Seneviratne, Nuran Kasthuriaarachchi, Sanka Rasnayaka", "title": "Multi-Dataset Benchmarks for Masked Identification using Contrastive\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The COVID-19 pandemic has drastically changed accepted norms globally. Within\nthe past year, masks have been used as a public health response to limit the\nspread of the virus. This sudden change has rendered many face recognition\nbased access control, authentication and surveillance systems ineffective.\nOfficial documents such as passports, driving license and national identity\ncards are enrolled with fully uncovered face images. However, in the current\nglobal situation, face matching systems should be able to match these reference\nimages with masked face images. As an example, in an airport or security\ncheckpoint it is safer to match the unmasked image of the identifying document\nto the masked person rather than asking them to remove the mask. We find that\ncurrent facial recognition techniques are not robust to this form of occlusion.\n  To address this unique requirement presented due to the current circumstance,\nwe propose a set of re-purposed datasets and a benchmark for researchers to\nuse. We also propose a contrastive visual representation learning based\npre-training workflow which is specialized to masked vs unmasked face matching.\nWe ensure that our method learns robust features to differentiate people across\nvarying data collection scenarios. We achieve this by training over many\ndifferent datasets and validating our result by testing on various holdout\ndatasets. The specialized weights trained by our method outperform standard\nface recognition features for masked to unmasked face matching. We believe the\nprovided synthetic mask generating code, our novel training approach and the\ntrained weights from the masked face models will help in adopting existing face\nrecognition systems to operate in the current global environment. We\nopen-source all contributions for broader use by the research community.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 08:58:10 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Seneviratne", "Sachith", ""], ["Kasthuriaarachchi", "Nuran", ""], ["Rasnayaka", "Sanka", ""]]}, {"id": "2106.05597", "submitter": "Corentin Kervadec", "authors": "Corentin Kervadec, Christian Wolf, Grigory Antipov, Moez Baccouche and\n  Madiha Nadri", "title": "Supervising the Transfer of Reasoning Patterns in VQA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for Visual Question Anwering (VQA) are notorious for leveraging\ndataset biases rather than performing reasoning, hindering generalization. It\nhas been recently shown that better reasoning patterns emerge in attention\nlayers of a state-of-the-art VQA model when they are trained on perfect\n(oracle) visual inputs. This provides evidence that deep neural networks can\nlearn to reason when training conditions are favorable enough. However,\ntransferring this learned knowledge to deployable models is a challenge, as\nmuch of it is lost during the transfer. We propose a method for knowledge\ntransfer based on a regularization term in our loss function, supervising the\nsequence of required reasoning operations. We provide a theoretical analysis\nbased on PAC-learning, showing that such program prediction can lead to\ndecreased sample complexity under mild hypotheses. We also demonstrate the\neffectiveness of this approach experimentally on the GQA dataset and show its\ncomplementarity to BERT-like self-supervised pre-training.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 08:58:43 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Kervadec", "Corentin", ""], ["Wolf", "Christian", ""], ["Antipov", "Grigory", ""], ["Baccouche", "Moez", ""], ["Nadri", "Madiha", ""]]}, {"id": "2106.05601", "submitter": "Philipp Terh\\\"orst", "authors": "Philipp Terh\\\"orst, Andr\\'e Boller, Naser Damer, Florian Kirchbuchner,\n  Arjan Kuijper", "title": "MiDeCon: Unsupervised and Accurate Fingerprint and Minutia Quality\n  Assessment based on Minutia Detection Confidence", "comments": "Accepted at IJCB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An essential factor to achieve high accuracies in fingerprint recognition\nsystems is the quality of its samples. Previous works mainly proposed\nsupervised solutions based on image properties that neglects the minutiae\nextraction process, despite that most fingerprint recognition techniques are\nbased on detected minutiae. Consequently, a fingerprint image might be assigned\na high quality even if the utilized minutia extractor produces unreliable\ninformation. In this work, we propose a novel concept of assessing minutia and\nfingerprint quality based on minutia detection confidence (MiDeCon). MiDeCon\ncan be applied to an arbitrary deep learning based minutia extractor and does\nnot require quality labels for learning. We propose using the detection\nreliability of the extracted minutia as its quality indicator. By combining the\nhighest minutia qualities, MiDeCon also accurately determines the quality of a\nfull fingerprint. Experiments are conducted on the publicly available databases\nof the FVC 2006 and compared against several baselines, such as NIST's\nwidely-used fingerprint image quality software NFIQ1 and NFIQ2. The results\ndemonstrate a significantly stronger quality assessment performance of the\nproposed MiDeCon-qualities as related works on both, minutia- and\nfingerprint-level. The implementation is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 09:06:01 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Terh\u00f6rst", "Philipp", ""], ["Boller", "Andr\u00e9", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2106.05607", "submitter": "Tianyu Wang", "authors": "Tianyu Wang, Miaomiao Liu, Kee Siong Ng", "title": "Spatially Invariant Unsupervised 3D Object Segmentation with Graph\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we tackle the problem of unsupervised 3D object segmentation\nfrom a point cloud without RGB information. In particular, we propose a\nframework, SPAIR3D, to model a point cloud as a spatial mixture model and\njointly learn the multiple-object representation and segmentation in 3D via\nVariational Autoencoders (VAE). Inspired by SPAIR, we adopt an\nobject-specification scheme that describes each object's location relative to\nits local voxel grid cell rather than the point cloud as a whole. To model the\nspatial mixture model on point clouds, we derive the Chamfer Likelihood, which\nfits naturally into the variational training pipeline. We further design a new\nspatially invariant graph neural network to generate a varying number of 3D\npoints as a decoder within our VAE. Experimental results demonstrate that\nSPAIR3D is capable of detecting and segmenting variable number of objects\nwithout appearance information across diverse scenes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 09:20:16 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 12:07:16 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wang", "Tianyu", ""], ["Liu", "Miaomiao", ""], ["Ng", "Kee Siong", ""]]}, {"id": "2106.05610", "submitter": "Laxman Dhulipala", "authors": "Laxman Dhulipala, David Eisenstat, Jakub {\\L}\\k{a}cki, Vahab Mirrokni,\n  Jessica Shi", "title": "Hierarchical Agglomerative Graph Clustering in Nearly-Linear Time", "comments": "This is the full version of the paper appearing in ICML'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the widely used hierarchical agglomerative clustering (HAC)\nalgorithm on edge-weighted graphs. We define an algorithmic framework for\nhierarchical agglomerative graph clustering that provides the first efficient\n$\\tilde{O}(m)$ time exact algorithms for classic linkage measures, such as\ncomplete- and WPGMA-linkage, as well as other measures. Furthermore, for\naverage-linkage, arguably the most popular variant of HAC, we provide an\nalgorithm that runs in $\\tilde{O}(n\\sqrt{m})$ time. For this variant, this is\nthe first exact algorithm that runs in subquadratic time, as long as\n$m=n^{2-\\epsilon}$ for some constant $\\epsilon > 0$. We complement this result\nwith a simple $\\epsilon$-close approximation algorithm for average-linkage in\nour framework that runs in $\\tilde{O}(m)$ time. As an application of our\nalgorithms, we consider clustering points in a metric space by first using\n$k$-NN to generate a graph from the point set, and then running our algorithms\non the resulting weighted graph. We validate the performance of our algorithms\non publicly available datasets, and show that our approach can speed up\nclustering of point datasets by a factor of 20.7--76.5x.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 09:29:05 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Dhulipala", "Laxman", ""], ["Eisenstat", "David", ""], ["\u0141\u0105cki", "Jakub", ""], ["Mirrokni", "Vahab", ""], ["Shi", "Jessica", ""]]}, {"id": "2106.05611", "submitter": "Ryota Yoshihashi", "authors": "Ryota Yoshihashi, Tomohiro Tanaka, Kenji Doi, Takumi Fujino, and\n  Naoaki Yamashita", "title": "Context-Free TextSpotter for Real-Time and Mobile End-to-End Text\n  Detection and Recognition", "comments": "To appear in ICDAR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the deployment of scene-text spotting systems on mobile platforms,\nlightweight models with low computation are preferable. In concept, end-to-end\n(E2E) text spotting is suitable for such purposes because it performs text\ndetection and recognition in a single model. However, current state-of-the-art\nE2E methods rely on heavy feature extractors, recurrent sequence modellings,\nand complex shape aligners to pursue accuracy, which means their computations\nare still heavy. We explore the opposite direction: How far can we go without\nbells and whistles in E2E text spotting? To this end, we propose a\ntext-spotting method that consists of simple convolutions and a few\npost-processes, named Context-Free TextSpotter. Experiments using standard\nbenchmarks show that Context-Free TextSpotter achieves real-time text spotting\non a GPU with only three million parameters, which is the smallest and fastest\namong existing deep text spotters, with an acceptable transcription quality\ndegradation compared to heavier ones. Further, we demonstrate that our text\nspotter can run on a smartphone with affordable latency, which is valuable for\nbuilding stand-alone OCR applications.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 09:32:52 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Yoshihashi", "Ryota", ""], ["Tanaka", "Tomohiro", ""], ["Doi", "Kenji", ""], ["Fujino", "Takumi", ""], ["Yamashita", "Naoaki", ""]]}, {"id": "2106.05616", "submitter": "Yicheng Deng", "authors": "Yicheng Deng, Yongqi Sun, Jiahui Zhu", "title": "SVMA: A GAN-based model for Monocular 3D Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recovering 3D human pose from 2D joints is a highly unconstrained problem,\nespecially without any video or multi-view information. We present an\nunsupervised GAN-based model to recover 3D human pose from 2D joint locations\nextracted from a single image. Our model uses a GAN to learn the mapping of\ndistribution from 2D poses to 3D poses, not the simple 2D-3D correspondence.\nConsidering the reprojection constraint, our model can estimate the camera so\nthat we can reproject the estimated 3D pose to the original 2D pose. Based on\nthis reprojection method, we can rotate and reproject the generated pose to get\nour \"new\" 2D pose and then use a weight sharing generator to estimate the \"new\"\n3D pose and a \"new\" camera. Through the above estimation process, we can define\nthe single-view-multi-angle consistency loss during training to simulate\nmulti-view consistency, which means the 3D poses and cameras estimated from two\nangles of a single view should be able to be mixed to generate rich 2D\nreprojections, and the 2D reprojections reprojected from the same 3D pose\nshould be consistent. The experimental results on Human3.6M show that our\nmethod outperforms all the state-of-the-art methods, and results on\nMPI-INF-3DHP show that our method outperforms state-of-the-art by approximately\n15.0%.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 09:43:57 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 05:21:21 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Deng", "Yicheng", ""], ["Sun", "Yongqi", ""], ["Zhu", "Jiahui", ""]]}, {"id": "2106.05618", "submitter": "Adri\\`a Molina Rodr\\'iguez", "authors": "Adri\\`a Molina and Pau Riba and Lluis Gomez and Oriol Ramos-Terrades\n  and Josep Llad\\'os", "title": "Date Estimation in the Wild of Scanned Historical Photos: An Image\n  Retrieval Approach", "comments": "Accepted at ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents a novel method for date estimation of historical\nphotographs from archival sources. The main contribution is to formulate the\ndate estimation as a retrieval task, where given a query, the retrieved images\nare ranked in terms of the estimated date similarity. The closer are their\nembedded representations the closer are their dates. Contrary to the\ntraditional models that design a neural network that learns a classifier or a\nregressor, we propose a learning objective based on the nDCG ranking metric. We\nhave experimentally evaluated the performance of the method in two different\ntasks: date estimation and date-sensitive image retrieval, using the DEW public\ndatabase, overcoming the baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 09:53:03 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Molina", "Adri\u00e0", ""], ["Riba", "Pau", ""], ["Gomez", "Lluis", ""], ["Ramos-Terrades", "Oriol", ""], ["Llad\u00f3s", "Josep", ""]]}, {"id": "2106.05656", "submitter": "Zhaowen Li", "authors": "Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong Zhu, Chaoyang\n  Zhao, Rui Deng, Liwei Wu, Rui Zhao, Ming Tang, Jinqiao Wang", "title": "MST: Masked Self-Supervised Transformer for Visual Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer has been widely used for self-supervised pre-training in Natural\nLanguage Processing (NLP) and achieved great success. However, it has not been\nfully explored in visual self-supervised learning. Meanwhile, previous methods\nonly consider the high-level feature and learning representation from a global\nperspective, which may fail to transfer to the downstream dense prediction\ntasks focusing on local features. In this paper, we present a novel Masked\nSelf-supervised Transformer approach named MST, which can explicitly capture\nthe local context of an image while preserving the global semantic information.\nSpecifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose\na masked token strategy based on the multi-head self-attention map, which\ndynamically masks some tokens of local patches without damaging the crucial\nstructure for self-supervised learning. More importantly, the masked tokens\ntogether with the remaining tokens are further recovered by a global image\ndecoder, which preserves the spatial information of the image and is more\nfriendly to the downstream dense prediction tasks. The experiments on multiple\ndatasets demonstrate the effectiveness and generality of the proposed method.\nFor instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using\n300-epoch pre-training by linear evaluation, which outperforms supervised\nmethods with the same epoch by 0.4% and its comparable variant DINO by 1.0\\%.\nFor dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object\ndetection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch\npre-training.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:05:18 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Li", "Zhaowen", ""], ["Chen", "Zhiyang", ""], ["Yang", "Fan", ""], ["Li", "Wei", ""], ["Zhu", "Yousong", ""], ["Zhao", "Chaoyang", ""], ["Deng", "Rui", ""], ["Wu", "Liwei", ""], ["Zhao", "Rui", ""], ["Tang", "Ming", ""], ["Wang", "Jinqiao", ""]]}, {"id": "2106.05657", "submitter": "Shashank Kotyan", "authors": "Shashank Kotyan and Danilo Vasconcellos Vargas", "title": "Deep neural network loses attention to adversarial images", "comments": "Accepted in Workshop on Artificial Intelligence Safety (AISafety\n  2021), IJCAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial algorithms have shown to be effective against neural networks for\na variety of tasks. Some adversarial algorithms perturb all the pixels in the\nimage minimally for the image classification task in image classification. In\ncontrast, some algorithms perturb few pixels strongly. However, very little\ninformation is available regarding why these adversarial samples so diverse\nfrom each other exist. Recently, Vargas et al. showed that the existence of\nthese adversarial samples might be due to conflicting saliency within the\nneural network. We test this hypothesis of conflicting saliency by analysing\nthe Saliency Maps (SM) and Gradient-weighted Class Activation Maps (Grad-CAM)\nof original and few different types of adversarial samples. We also analyse how\ndifferent adversarial samples distort the attention of the neural network\ncompared to original samples. We show that in the case of Pixel Attack,\nperturbed pixels either calls the network attention to themselves or divert the\nattention from them. Simultaneously, the Projected Gradient Descent Attack\nperturbs pixels so that intermediate layers inside the neural network lose\nattention for the correct class. We also show that both attacks affect the\nsaliency map and activation maps differently. Thus, shedding light on why some\ndefences successful against some attacks remain vulnerable against other\nattacks. We hope that this analysis will improve understanding of the existence\nand the effect of adversarial samples and enable the community to develop more\nrobust neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:06:17 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Kotyan", "Shashank", ""], ["Vargas", "Danilo Vasconcellos", ""]]}, {"id": "2106.05658", "submitter": "Tianlin Xu", "authors": "Tianlin Xu and Beatrice Acciaio", "title": "Quantized Conditional COT-GAN for Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal Optimal Transport (COT) results from imposing a temporal causality\nconstraint on classic optimal transport problems, which naturally generates a\nnew concept of distances between distributions on path spaces. The first\napplication of the COT theory for sequential learning was given in Xu et al.\n(2020), where COT-GAN was introduced as an adversarial algorithm to train\nimplicit generative models optimized for producing sequential data. Relying on\nXu et al. (2020), the contribution of the present paper is twofold. First, we\ndevelop a conditional version of COT-GAN suitable for sequence prediction. This\nmeans that the dataset is now used in order to learn how a sequence will evolve\ngiven the observation of its past evolution. Second, we improve on the\nconvergence results by working with modifications of the empirical measures via\na specific type of quantization due to Backhoff et al. (2020). The resulting\nquantized conditional COT-GAN algorithm is illustrated with an application for\nvideo prediction.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:10:53 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Xu", "Tianlin", ""], ["Acciaio", "Beatrice", ""]]}, {"id": "2106.05662", "submitter": "Filippos Kokkinos", "authors": "Filippos Kokkinos and Iasonas Kokkinos", "title": "To The Point: Correspondence-driven monocular 3D category reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present To The Point (TTP), a method for reconstructing 3D objects from a\nsingle image using 2D to 3D correspondences learned from weak supervision. We\nrecover a 3D shape from a 2D image by first regressing the 2D positions\ncorresponding to the 3D template vertices and then jointly estimating a rigid\ncamera transform and non-rigid template deformation that optimally explain the\n2D positions through the 3D shape projection. By relying on 3D-2D\ncorrespondences we use a simple per-sample optimization problem to replace\nCNN-based regression of camera pose and non-rigid deformation and thereby\nobtain substantially more accurate 3D reconstructions. We treat this\noptimization as a differentiable layer and train the whole system in an\nend-to-end manner. We report systematic quantitative improvements on multiple\ncategories and provide qualitative results comprising diverse shape, pose and\ntexture prediction examples. Project website:\nhttps://fkokkinos.github.io/to_the_point/.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:21:14 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Kokkinos", "Filippos", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "2106.05665", "submitter": "Anurag Ghosh", "authors": "Anurag Ghosh, Akshay Nambi, Aditya Singh, Harish YVS, Tanuja Ganu", "title": "Adaptive Streaming Perception using Deep Reinforcement Learning", "comments": "19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Executing computer vision models on streaming visual data, or streaming\nperception is an emerging problem, with applications in self-driving, embodied\nagents, and augmented/virtual reality. The development of such systems is\nlargely governed by the accuracy and latency of the processing pipeline. While\npast work has proposed numerous approximate execution frameworks, their\ndecision functions solely focus on optimizing latency, accuracy, or energy,\netc. This results in sub-optimum decisions, affecting the overall system\nperformance. We argue that the streaming perception systems should holistically\nmaximize the overall system performance (i.e., considering both accuracy and\nlatency simultaneously). To this end, we describe a new approach based on deep\nreinforcement learning to learn these tradeoffs at runtime for streaming\nperception. This tradeoff optimization is formulated as a novel deep contextual\nbandit problem and we design a new reward function that holistically integrates\nlatency and accuracy into a single metric. We show that our agent can learn a\ncompetitive policy across multiple decision dimensions, which outperforms\nstate-of-the-art policies on public datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:28:10 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Ghosh", "Anurag", ""], ["Nambi", "Akshay", ""], ["Singh", "Aditya", ""], ["YVS", "Harish", ""], ["Ganu", "Tanuja", ""]]}, {"id": "2106.05681", "submitter": "Chongwei Liu", "authors": "Chongwei Liu, Haojie Li, Shuchang Wang, Ming Zhu, Dong Wang, Xin Fan\n  and Zhihui Wang", "title": "A Dataset And Benchmark Of Underwater Object Detection For Robot Picking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Underwater object detection for robot picking has attracted a lot of\ninterest. However, it is still an unsolved problem due to several challenges.\nWe take steps towards making it more realistic by addressing the following\nchallenges. Firstly, the currently available datasets basically lack the test\nset annotations, causing researchers must compare their method with other SOTAs\non a self-divided test set (from the training set). Training other methods lead\nto an increase in workload and different researchers divide different datasets,\nresulting there is no unified benchmark to compare the performance of different\nalgorithms. Secondly, these datasets also have other shortcomings, e.g., too\nmany similar images or incomplete labels. Towards these challenges we introduce\na dataset, Detecting Underwater Objects (DUO), and a corresponding benchmark,\nbased on the collection and re-annotation of all relevant datasets. DUO\ncontains a collection of diverse underwater images with more rational\nannotations. The corresponding benchmark provides indicators of both efficiency\nand accuracy of SOTAs (under the MMDtection framework) for academic research\nand industrial applications, where JETSON AGX XAVIER is used to assess detector\nspeed to simulate the robot-embedded environment.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:56:19 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liu", "Chongwei", ""], ["Li", "Haojie", ""], ["Wang", "Shuchang", ""], ["Zhu", "Ming", ""], ["Wang", "Dong", ""], ["Fan", "Xin", ""], ["Wang", "Zhihui", ""]]}, {"id": "2106.05682", "submitter": "Youngtaek Oh", "authors": "Youngtaek Oh, Dong-Jin Kim, In So Kweon", "title": "Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced\n  Semi-Supervised Learning", "comments": "\"Code: https://github.com/ytaek-oh/daso\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The capability of the traditional semi-supervised learning (SSL) methods is\nfar from real-world application since they do not consider (1) class imbalance\nand (2) class distribution mismatch between labeled and unlabeled data. This\npaper addresses such a relatively under-explored problem, imbalanced\nsemi-supervised learning, where heavily biased pseudo-labels can harm the model\nperformance. Interestingly, we find that the semantic pseudo-labels from a\nsimilarity-based classifier in feature space and the traditional pseudo-labels\nfrom the linear classifier show the complementary property. To this end, we\npropose a general pseudo-labeling framework to address the bias motivated by\nthis observation. The key idea is to class-adaptively blend the semantic\npseudo-label to the linear one, depending on the current pseudo-label\ndistribution. Thereby, the increased semantic pseudo-label component suppresses\nthe false positives in the majority classes and vice versa. We term the novel\npseudo-labeling framework for imbalanced SSL as Distribution-Aware\nSemantics-Oriented (DASO) Pseudo-label. Extensive evaluation on CIFAR10/100-LT\nand STL10-LT shows that DASO consistently outperforms both recently proposed\nre-balancing methods for label and pseudo-label. Moreover, we demonstrate that\ntypical SSL algorithms can effectively benefit from unlabeled data with DASO,\nespecially when (1) class imbalance and (2) class distribution mismatch exist\nand even on recent real-world Semi-Aves benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 11:58:25 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Oh", "Youngtaek", ""], ["Kim", "Dong-Jin", ""], ["Kweon", "In So", ""]]}, {"id": "2106.05728", "submitter": "Riya Shah", "authors": "Riya Shah Rutva Shah", "title": "Face mask detection using convolution neural network", "comments": "4 PAGES, 3 FIGURES, 1 TABLE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the recent times, the Coronaviruses that are a big family of different\nviruses have become very common, contagious and dangerous to the whole human\nkind. It spreads human to human by exhaling the infection breath, which leaves\ndroplets of the virus on different surface which is then inhaled by other\nperson and catches the infection too. So it has become very important to\nprotect ourselves and the people around us from this situation. We can take\nprecautions such as social distancing, washing hands every two hours, using\nsanitizer, maintaining social distance and the most important wearing a mask.\nPublic use of wearing a masks has become very common everywhere in the whole\nworld now. From that the most affected and devastating condition is of India\ndue to its extreme population in small area. This paper proposes a method to\ndetect the face mask is put on or not for offices, or any other work place with\na lot of people coming to work. We have used convolutional neural network for\nthe same. The model is trained on a real world dataset and tested with live\nvideo streaming with a good accuracy. Further the accuracy of the model with\ndifferent hyper parameters and multiple people at different distance and\nlocation of the frame is done.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:18:57 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Shah", "Riya Shah Rutva", ""]]}, {"id": "2106.05735", "submitter": "Michela Antonelli", "authors": "Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani,\n  AnnetteKopp-Schneider, Bennett A. Landman, Geert Litjens, Bjoern Menze, Olaf\n  Ronneberger, Ronald M.Summers, Bram van Ginneken, Michel Bilello, Patrick\n  Bilic, Patrick F. Christ, Richard K. G. Do, Marc J. Gollub, Stephan H.\n  Heckers, Henkjan Huisman, William R. Jarnagin, Maureen K. McHugo, Sandy\n  Napel, Jennifer S. Goli Pernicka, Kawal Rhode, Catalina Tobon-Gomez, Eugene\n  Vorontsov, Henkjan Huisman, James A. Meakin, Sebastien Ourselin, Manuel\n  Wiesenfarth, Pablo Arbelaez, Byeonguk Bae, Sihong Chen, Laura Daza, Jianjiang\n  Feng, Baochun He, Fabian Isensee, Yuanfeng Ji, Fucang Jia, Namkug Kim, Ildoo\n  Kim, Dorit Merhof, Akshay Pai, Beomhee Park, Mathias Perslev, Ramin\n  Rezaiifar, Oliver Rippel, Ignacio Sarasua, Wei Shen, Jaemin Son, Christian\n  Wachinger, Liansheng Wang, Yan Wang, Yingda Xia, Daguang Xu, Zhanwei Xu,\n  Yefeng Zheng, Amber L. Simpson, Lena Maier-Hein, M. Jorge Cardoso", "title": "The Medical Segmentation Decathlon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  International challenges have become the de facto standard for comparative\nassessment of image analysis algorithms given a specific task. Segmentation is\nso far the most widely investigated medical image processing task, but the\nvarious segmentation challenges have typically been organized in isolation,\nsuch that algorithm development was driven by the need to tackle a single\nspecific clinical problem. We hypothesized that a method capable of performing\nwell on multiple tasks will generalize well to a previously unseen task and\npotentially outperform a custom-designed solution. To investigate the\nhypothesis, we organized the Medical Segmentation Decathlon (MSD) - a\nbiomedical image analysis challenge, in which algorithms compete in a multitude\nof both tasks and modalities. The underlying data set was designed to explore\nthe axis of difficulties typically encountered when dealing with medical\nimages, such as small data sets, unbalanced labels, multi-site data and small\nobjects. The MSD challenge confirmed that algorithms with a consistent good\nperformance on a set of tasks preserved their good average performance on a\ndifferent set of previously unseen tasks. Moreover, by monitoring the MSD\nwinner for two years, we found that this algorithm continued generalizing well\nto a wide range of other clinical problems, further confirming our hypothesis.\nThree main conclusions can be drawn from this study: (1) state-of-the-art image\nsegmentation algorithms are mature, accurate, and generalize well when\nretrained on unseen tasks; (2) consistent algorithmic performance across\nmultiple tasks is a strong surrogate of algorithmic generalizability; (3) the\ntraining of accurate AI segmentation models is now commoditized to non AI\nexperts.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:34:06 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Antonelli", "Michela", ""], ["Reinke", "Annika", ""], ["Bakas", "Spyridon", ""], ["Farahani", "Keyvan", ""], ["AnnetteKopp-Schneider", "", ""], ["Landman", "Bennett A.", ""], ["Litjens", "Geert", ""], ["Menze", "Bjoern", ""], ["Ronneberger", "Olaf", ""], ["Summers", "Ronald M.", ""], ["van Ginneken", "Bram", ""], ["Bilello", "Michel", ""], ["Bilic", "Patrick", ""], ["Christ", "Patrick F.", ""], ["Do", "Richard K. G.", ""], ["Gollub", "Marc J.", ""], ["Heckers", "Stephan H.", ""], ["Huisman", "Henkjan", ""], ["Jarnagin", "William R.", ""], ["McHugo", "Maureen K.", ""], ["Napel", "Sandy", ""], ["Pernicka", "Jennifer S. Goli", ""], ["Rhode", "Kawal", ""], ["Tobon-Gomez", "Catalina", ""], ["Vorontsov", "Eugene", ""], ["Huisman", "Henkjan", ""], ["Meakin", "James A.", ""], ["Ourselin", "Sebastien", ""], ["Wiesenfarth", "Manuel", ""], ["Arbelaez", "Pablo", ""], ["Bae", "Byeonguk", ""], ["Chen", "Sihong", ""], ["Daza", "Laura", ""], ["Feng", "Jianjiang", ""], ["He", "Baochun", ""], ["Isensee", "Fabian", ""], ["Ji", "Yuanfeng", ""], ["Jia", "Fucang", ""], ["Kim", "Namkug", ""], ["Kim", "Ildoo", ""], ["Merhof", "Dorit", ""], ["Pai", "Akshay", ""], ["Park", "Beomhee", ""], ["Perslev", "Mathias", ""], ["Rezaiifar", "Ramin", ""], ["Rippel", "Oliver", ""], ["Sarasua", "Ignacio", ""], ["Shen", "Wei", ""], ["Son", "Jaemin", ""], ["Wachinger", "Christian", ""], ["Wang", "Liansheng", ""], ["Wang", "Yan", ""], ["Xia", "Yingda", ""], ["Xu", "Daguang", ""], ["Xu", "Zhanwei", ""], ["Zheng", "Yefeng", ""], ["Simpson", "Amber L.", ""], ["Maier-Hein", "Lena", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "2106.05741", "submitter": "Elena Ericheva", "authors": "Ivan Drokin and Elena Ericheva", "title": "End-to-end lung nodule detection framework with model-based feature\n  projection block", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes novel end-to-end framework for detecting suspicious\npulmonary nodules in chest CT scans. The method core idea is a new nodule\nsegmentation architecture with a model-based feature projection block on\nthree-dimensional convolutions. This block acts as a preliminary feature\nextractor for a two-dimensional U-Net-like convolutional network. Using the\nproposed approach along with an axial, coronal, and sagittal projection\nanalysis makes it possible to abandon the widely used false positives reduction\nstep. The proposed method achieves SOTA on LUNA2016 with 0.959 average\nsensitivity, and 0.936 sensitivity if the false-positive level per scan is\n0.25. The paper describes the proposed approach and represents the experimental\nresults on LUNA2016 as well as ablation studies.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:42:59 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Drokin", "Ivan", ""], ["Ericheva", "Elena", ""]]}, {"id": "2106.05744", "submitter": "Ron Mokady", "authors": "Daniel Roich, Ron Mokady, Amit H. Bermano, and Daniel Cohen-Or", "title": "Pivotal Tuning for Latent-based Editing of Real Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recently, a surge of advanced facial editing techniques have been proposed\nthat leverage the generative power of a pre-trained StyleGAN. To successfully\nedit an image this way, one must first project (or invert) the image into the\npre-trained generator's domain. As it turns out, however, StyleGAN's latent\nspace induces an inherent tradeoff between distortion and editability, i.e.\nbetween maintaining the original appearance and convincingly altering some of\nits attributes. Practically, this means it is still challenging to apply\nID-preserving facial latent-space editing to faces which are out of the\ngenerator's domain. In this paper, we present an approach to bridge this gap.\nOur technique slightly alters the generator, so that an out-of-domain image is\nfaithfully mapped into an in-domain latent code. The key idea is pivotal tuning\n- a brief training process that preserves the editing quality of an in-domain\nlatent region, while changing its portrayed identity and appearance. In Pivotal\nTuning Inversion (PTI), an initial inverted latent code serves as a pivot,\naround which the generator is fined-tuned. At the same time, a regularization\nterm keeps nearby identities intact, to locally contain the effect. This\nsurgical training process ends up altering appearance features that represent\nmostly identity, without affecting editing capabilities. We validate our\ntechnique through inversion and editing metrics, and show preferable scores to\nstate-of-the-art methods. We further qualitatively demonstrate our technique by\napplying advanced edits (such as pose, age, or expression) to numerous images\nof well-known and recognizable identities. Finally, we demonstrate resilience\nto harder cases, including heavy make-up, elaborate hairstyles and/or headwear,\nwhich otherwise could not have been successfully inverted and edited by\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:47:59 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Roich", "Daniel", ""], ["Mokady", "Ron", ""], ["Bermano", "Amit H.", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2106.05746", "submitter": "Abby Stylianou", "authors": "Rashmi Kamath, Gregory Rolwes, Samuel Black and Abby Stylianou", "title": "The 2021 Hotel-ID to Combat Human Trafficking Competition Dataset", "comments": "CVPR 2021 Workshop on Fine-Grained Visual Categorization (FGVC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hotel recognition is an important task for human trafficking investigations\nsince victims are often photographed in hotel rooms. Identifying these hotels\nis vital to trafficking investigations since they can help track down current\nand future victims who might be taken to the same places. Hotel recognition is\na challenging fine grained visual classification task as there can be little\nsimilarity between different rooms within the same hotel, and high similarity\nbetween rooms from different hotels (especially if they are from the same\nchain). Hotel recognition to combat human trafficking poses additional\nchallenges as investigative images are often low quality, contain uncommon\ncamera angles and are highly occluded. Here, we present the 2021 Hotel-ID\ndataset to help raise awareness for this problem and generate novel approaches.\nThe dataset consists of hotel room images that have been crowd-sourced and\nuploaded through the TraffickCam mobile application. The quality of these\nimages is similar to investigative images and hence models trained on these\nimages have good chances of accurately narrowing down on the correct hotel.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:50:28 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 18:33:55 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kamath", "Rashmi", ""], ["Rolwes", "Gregory", ""], ["Black", "Samuel", ""], ["Stylianou", "Abby", ""]]}, {"id": "2106.05748", "submitter": "Abby Stylianou", "authors": "Chao Ren, Justin Dulay, Gregory Rolwes, Duke Pauli, Nadia Shakoor and\n  Abby Stylianou", "title": "Multi-resolution Outlier Pooling for Sorghum Classification", "comments": "CVPR 2021 Agriculture-Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated high throughput plant phenotyping involves leveraging sensors, such\nas RGB, thermal and hyperspectral cameras (among others), to make large scale\nand rapid measurements of the physical properties of plants for the purpose of\nbetter understanding the difference between crops and facilitating rapid plant\nbreeding programs. One of the most basic phenotyping tasks is to determine the\ncultivar, or species, in a particular sensor product. This simple phenotype can\nbe used to detect errors in planting and to learn the most differentiating\nfeatures between cultivars. It is also a challenging visual recognition task,\nas a large number of highly related crops are grown simultaneously, leading to\na classification problem with low inter-class variance. In this paper, we\nintroduce the Sorghum-100 dataset, a large dataset of RGB imagery of sorghum\ncaptured by a state-of-the-art gantry system, a multi-resolution network\narchitecture that learns both global and fine-grained features on the crops,\nand a new global pooling strategy called Dynamic Outlier Pooling which\noutperforms standard global pooling strategies on this task.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:57:33 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 18:58:30 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ren", "Chao", ""], ["Dulay", "Justin", ""], ["Rolwes", "Gregory", ""], ["Pauli", "Duke", ""], ["Shakoor", "Nadia", ""], ["Stylianou", "Abby", ""]]}, {"id": "2106.05779", "submitter": "Tejan Karmali", "authors": "Rahul Venkatesh, Tejan Karmali, Sarthak Sharma, Aurobrata Ghosh, R.\n  Venkatesh Babu, L\\'aszl\\'o A. Jeni, Maneesh Singh", "title": "Deep Implicit Surface Point Prediction Networks", "comments": "22 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural representations of 3D shapes as implicit functions have been\nshown to produce high fidelity models surpassing the resolution-memory\ntrade-off faced by the explicit representations using meshes and point clouds.\nHowever, most such approaches focus on representing closed shapes. Unsigned\ndistance function (UDF) based approaches have been proposed recently as a\npromising alternative to represent both open and closed shapes. However, since\nthe gradients of UDFs vanish on the surface, it is challenging to estimate\nlocal (differential) geometric properties like the normals and tangent planes\nwhich are needed for many downstream applications in vision and graphics. There\nare additional challenges in computing these properties efficiently with a\nlow-memory footprint. This paper presents a novel approach that models such\nsurfaces using a new class of implicit representations called the closest\nsurface-point (CSP) representation. We show that CSP allows us to represent\ncomplex surfaces of any topology (open or closed) with high fidelity. It also\nallows for accurate and efficient computation of local geometric properties. We\nfurther demonstrate that it leads to efficient implementation of downstream\nalgorithms like sphere-tracing for rendering the 3D surface as well as to\ncreate explicit mesh-based representations. Extensive experimental evaluation\non the ShapeNet dataset validate the above contributions with results\nsurpassing the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:31:54 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 03:26:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Venkatesh", "Rahul", ""], ["Karmali", "Tejan", ""], ["Sharma", "Sarthak", ""], ["Ghosh", "Aurobrata", ""], ["Babu", "R. Venkatesh", ""], ["Jeni", "L\u00e1szl\u00f3 A.", ""], ["Singh", "Maneesh", ""]]}, {"id": "2106.05786", "submitter": "Hezheng Lin", "authors": "Hezheng Lin, Xing Cheng, Xiangyu Wu, Fan Yang, Dong Shen, Zhongyuan\n  Wang, Qing Song, Wei Yuan", "title": "CAT: Cross Attention in Vision Transformer", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Transformer has found widespread use in NLP, the potential of\nTransformer in CV has been realized and has inspired many new approaches.\nHowever, the computation required for replacing word tokens with image patches\nfor Transformer after the tokenization of the image is vast(e.g., ViT), which\nbottlenecks model training and inference. In this paper, we propose a new\nattention mechanism in Transformer termed Cross Attention, which alternates\nattention inner the image patch instead of the whole image to capture local\ninformation and apply attention between image patches which are divided from\nsingle-channel feature maps capture global information. Both operations have\nless computation than standard self-attention in Transformer. By alternately\napplying attention inner patch and between patches, we implement cross\nattention to maintain the performance with lower computational cost and build a\nhierarchical network called Cross Attention Transformer(CAT) for other vision\ntasks. Our base model achieves state-of-the-arts on ImageNet-1K, and improves\nthe performance of other methods on COCO and ADE20K, illustrating that our\nnetwork has the potential to serve as general backbones. The code and models\nare available at \\url{https://github.com/linhezheng19/CAT}.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:38:32 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Lin", "Hezheng", ""], ["Cheng", "Xing", ""], ["Wu", "Xiangyu", ""], ["Yang", "Fan", ""], ["Shen", "Dong", ""], ["Wang", "Zhongyuan", ""], ["Song", "Qing", ""], ["Yuan", "Wei", ""]]}, {"id": "2106.05843", "submitter": "Saul Calderon Ramirez", "authors": "Willard Zamora-Cardenas, Mauro Mendez, Saul Calderon-Ramirez, Martin\n  Vargas, Gerardo Monge, Steve Quiros, David Elizondo, David Elizondo, Miguel\n  A. Molina-Cabello", "title": "Enforcing Morphological Information in Fully Convolutional Networks to\n  Improve Cell Instance Segmentation in Fluorescence Microscopy Images", "comments": "Accepted at the IWANN 2021 (International Work-Conference on\n  Artificial and Natural Neural Networks)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cell instance segmentation in fluorescence microscopy images is becoming\nessential for cancer dynamics and prognosis. Data extracted from cancer\ndynamics allows to understand and accurately model different metabolic\nprocesses such as proliferation. This enables customized and more precise\ncancer treatments. However, accurate cell instance segmentation, necessary for\nfurther cell tracking and behavior analysis, is still challenging in scenarios\nwith high cell concentration and overlapping edges. Within this framework, we\npropose a novel cell instance segmentation approach based on the well-known\nU-Net architecture. To enforce the learning of morphological information per\npixel, a deep distance transformer (DDT) acts as a back-bone model. The DDT\noutput is subsequently used to train a top-model. The following top-models are\nconsidered: a three-class (\\emph{e.g.,} foreground, background and cell border)\nU-net, and a watershed transform. The obtained results suggest a performance\nboost over traditional U-Net architectures. This opens an interesting research\nline around the idea of injecting morphological information into a fully\nconvolutional model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:54:38 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zamora-Cardenas", "Willard", ""], ["Mendez", "Mauro", ""], ["Calderon-Ramirez", "Saul", ""], ["Vargas", "Martin", ""], ["Monge", "Gerardo", ""], ["Quiros", "Steve", ""], ["Elizondo", "David", ""], ["Elizondo", "David", ""], ["Molina-Cabello", "Miguel A.", ""]]}, {"id": "2106.05844", "submitter": "Shruti Jadon", "authors": "Shruti Jadon", "title": "SemSegLoss: A python package of loss functions for semantic segmentation", "comments": "8 pages, 2 tables", "journal-ref": null, "doi": "10.1016/j.simpa.2021.100078", "report-no": "SIMPA_100078, Software Impacts Journal", "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image Segmentation has been an active field of research as it has a wide\nrange of applications, ranging from automated disease detection to self-driving\ncars. In recent years, various research papers proposed different loss\nfunctions used in case of biased data, sparse segmentation, and unbalanced\ndataset. In this paper, we introduce SemSegLoss, a python package consisting of\nsome of the well-known loss functions widely used for image segmentation. It is\ndeveloped with the intent to help researchers in the development of novel loss\nfunctions and perform an extensive set of experiments on model architectures\nfor various applications. The ease-of-use and flexibility of the presented\npackage have allowed reducing the development time and increased evaluation\nstrategies of machine learning models for semantic segmentation. Furthermore,\ndifferent applications that use image segmentation can use SemSegLoss because\nof the generality of its functions. This wide range of applications will lead\nto the development and growth of AI across all industries.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 04:27:06 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Jadon", "Shruti", ""]]}, {"id": "2106.05861", "submitter": "Debanjan Das", "authors": "Debanjan Das, Chirag Samal, Deewanshu Ukey, Gourav Chowdhary, and\n  Saraju P. Mohanty", "title": "CoviLearn: A Machine Learning Integrated Smart X-Ray Device in\n  Healthcare Cyber-Physical System for Automatic Initial Screening of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pandemic of novel Coronavirus Disease 2019 (COVID-19) is widespread all\nover the world causing serious health problems as well as serious impact on the\nglobal economy. Reliable and fast testing of the COVID-19 has been a challenge\nfor researchers and healthcare practitioners. In this work we present a novel\nmachine learning (ML) integrated X-ray device in Healthcare Cyber-Physical\nSystem (H-CPS) or smart healthcare framework (called CoviLearn) to allow\nhealthcare practitioners to perform automatic initial screening of COVID-19\npatients. We propose convolutional neural network (CNN) models of X-ray images\nintegrated into an X-ray device for automatic COVID-19 detection. The proposed\nCoviLearn device will be useful in detecting if a person is COVID-19 positive\nor negative by considering the chest X-ray image of individuals. CoviLearn will\nbe useful tool doctors to detect potential COVID-19 infections instantaneously\nwithout taking more intrusive healthcare data samples, such as saliva and\nblood. COVID-19 attacks the endothelium tissues that support respiratory tract,\nX-rays images can be used to analyze the health of a patient lungs. As all\nhealthcare centers have X-ray machines, it could be possible to use proposed\nCoviLearn X-rays to test for COVID-19 without the especial test kits. Our\nproposed automated analysis system CoviLearn which has 99% accuracy will be\nable to save valuable time of medical professionals as the X-ray machines come\nwith a drawback as it needed a radiology expert.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 03:40:16 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Das", "Debanjan", ""], ["Samal", "Chirag", ""], ["Ukey", "Deewanshu", ""], ["Chowdhary", "Gourav", ""], ["Mohanty", "Saraju P.", ""]]}, {"id": "2106.05897", "submitter": "Qingzhe Gao", "authors": "Qingzhe Gao, Bin Wang, Libin Liu, Baoquan Chen", "title": "Unsupervised Co-part Segmentation through Assembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-part segmentation is an important problem in computer vision for its rich\napplications. We propose an unsupervised learning approach for co-part\nsegmentation from images. For the training stage, we leverage motion\ninformation embedded in videos and explicitly extract latent representations to\nsegment meaningful object parts. More importantly, we introduce a dual\nprocedure of part-assembly to form a closed loop with part-segmentation,\nenabling an effective self-supervision. We demonstrate the effectiveness of our\napproach with a host of extensive experiments, ranging from human bodies,\nhands, quadruped, and robot arms. We show that our approach can achieve\nmeaningful and compact part segmentation, outperforming state-of-the-art\napproaches on diverse benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 16:22:53 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Gao", "Qingzhe", ""], ["Wang", "Bin", ""], ["Liu", "Libin", ""], ["Chen", "Baoquan", ""]]}, {"id": "2106.05903", "submitter": "Scott A. Hale", "authors": "Austin Botelho and Bertie Vidgen and Scott A. Hale", "title": "Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for\n  Multimodal Hate", "comments": "Please note the paper contains examples of hateful content", "journal-ref": "Findings of ACL, 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection and classification of online hate is a difficult task.\nImplicit hate is particularly challenging as such content tends to have unusual\nsyntax, polysemic words, and fewer markers of prejudice (e.g., slurs). This\nproblem is heightened with multimodal content, such as memes (combinations of\ntext and images), as they are often harder to decipher than unimodal content\n(e.g., text alone). This paper evaluates the role of semantic and multimodal\ncontext for detecting implicit and explicit hate. We show that both text- and\nvisual- enrichment improves model performance, with the multimodal model\n(0.771) outperforming other models' F1 scores (0.544, 0.737, and 0.754). While\nthe unimodal-text context-aware (transformer) model was the most accurate on\nthe subtask of implicit hate detection, the multimodal model outperformed it\noverall because of a lower propensity towards false positives. We find that all\nmodels perform better on content with full annotator agreement and that\nmultimodal models are best at classifying the content where annotators\ndisagree. To conduct these investigations, we undertook high-quality annotation\nof a sample of 5,000 multimodal entries. Tweets were annotated for primary\ncategory, modality, and strategy. We make this corpus, along with the codebook,\ncode, and final model, freely available.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 16:29:42 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Botelho", "Austin", ""], ["Vidgen", "Bertie", ""], ["Hale", "Scott A.", ""]]}, {"id": "2106.05915", "submitter": "Taufiq Hasan", "authors": "Uday Kamal, Mohammad Zunaed, Nusrat Binta Nizam, Taufiq Hasan", "title": "Anatomy X-Net: A Semi-Supervised Anatomy Aware Convolutional Neural\n  Network for Thoracic Disease Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thoracic disease detection from chest radiographs using deep learning methods\nhas been an active area of research in the last decade. Most previous methods\nattempt to focus on the diseased organs of the image by identifying spatial\nregions responsible for significant contributions to the model's prediction. In\ncontrast, expert radiologists first locate the prominent anatomical structures\nbefore determining if those regions are anomalous. Therefore, integrating\nanatomical knowledge within deep learning models could bring substantial\nimprovement in automatic disease classification. This work proposes an\nanatomy-aware attention-based architecture named Anatomy X-Net, that\nprioritizes the spatial features guided by the pre-identified anatomy regions.\nWe leverage a semi-supervised learning method using the JSRT dataset containing\norgan-level annotation to obtain the anatomical segmentation masks (for lungs\nand heart) for the NIH and CheXpert datasets. The proposed Anatomy X-Net uses\nthe pre-trained DenseNet-121 as the backbone network with two corresponding\nstructured modules, the Anatomy Aware Attention (AAA) and Probabilistic\nWeighted Average Pooling (PWAP), in a cohesive framework for anatomical\nattention learning. Our proposed method sets new state-of-the-art performance\non the official NIH test set with an AUC score of 0.8439, proving the efficacy\nof utilizing the anatomy segmentation knowledge to improve the thoracic disease\nclassification. Furthermore, the Anatomy X-Net yields an averaged AUC of 0.9020\non the Stanford CheXpert dataset, improving on existing methods that\ndemonstrate the generalizability of the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:01:23 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Kamal", "Uday", ""], ["Zunaed", "Mohammad", ""], ["Nizam", "Nusrat Binta", ""], ["Hasan", "Taufiq", ""]]}, {"id": "2106.05920", "submitter": "Tianwei Wang", "authors": "Tianwei Wang, Yuanzhi Zhu, Lianwen Jin, Dezhi Peng, Zhe Li, Mengchao\n  He, Yongpan Wang, Canjie Luo", "title": "Implicit Feature Alignment: Learn to Convert Text Recognizer to Text\n  Spotter", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text recognition is a popular research subject with many associated\nchallenges. Despite the considerable progress made in recent years, the text\nrecognition task itself is still constrained to solve the problem of reading\ncropped line text images and serves as a subtask of optical character\nrecognition (OCR) systems. As a result, the final text recognition result is\nlimited by the performance of the text detector. In this paper, we propose a\nsimple, elegant and effective paradigm called Implicit Feature Alignment (IFA),\nwhich can be easily integrated into current text recognizers, resulting in a\nnovel inference mechanism called IFAinference. This enables an ordinary text\nrecognizer to process multi-line text such that text detection can be\ncompletely freed. Specifically, we integrate IFA into the two most prevailing\ntext recognition streams (attention-based and CTC-based) and propose\nattention-guided dense prediction (ADP) and Extended CTC (ExCTC). Furthermore,\nthe Wasserstein-based Hollow Aggregation Cross-Entropy (WH-ACE) is proposed to\nsuppress negative predictions to assist in training ADP and ExCTC. We\nexperimentally demonstrate that IFA achieves state-of-the-art performance on\nend-to-end document recognition tasks while maintaining the fastest speed, and\nADP and ExCTC complement each other on the perspective of different application\nscenarios. Code will be available at\nhttps://github.com/WangTianwei/Implicit-feature-alignment.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:06:28 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wang", "Tianwei", ""], ["Zhu", "Yuanzhi", ""], ["Jin", "Lianwen", ""], ["Peng", "Dezhi", ""], ["Li", "Zhe", ""], ["He", "Mengchao", ""], ["Wang", "Yongpan", ""], ["Luo", "Canjie", ""]]}, {"id": "2106.05923", "submitter": "Sophia Bano", "authors": "Sophia Bano, Alessandro Casella, Francisco Vasconcelos, Sara Moccia,\n  George Attilakos, Ruwan Wimalasundera, Anna L. David, Dario Paladini, Jan\n  Deprest, Elena De Momi, Leonardo S. Mattos, Danail Stoyanov", "title": "FetReg: Placental Vessel Segmentation and Registration in Fetoscopy\n  Challenge Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fetoscopy laser photocoagulation is a widely used procedure for the treatment\nof Twin-to-Twin Transfusion Syndrome (TTTS), that occur in mono-chorionic\nmultiple pregnancies due to placental vascular anastomoses. This procedure is\nparticularly challenging due to limited field of view, poor manoeuvrability of\nthe fetoscope, poor visibility due to fluid turbidity, variability in light\nsource, and unusual position of the placenta. This may lead to increased\nprocedural time and incomplete ablation, resulting in persistent TTTS.\nComputer-assisted intervention may help overcome these challenges by expanding\nthe fetoscopic field of view through video mosaicking and providing better\nvisualization of the vessel network. However, the research and development in\nthis domain remain limited due to unavailability of high-quality data to encode\nthe intra- and inter-procedure variability. Through the \\textit{Fetoscopic\nPlacental Vessel Segmentation and Registration (FetReg)} challenge, we present\na large-scale multi-centre dataset for the development of generalized and\nrobust semantic segmentation and video mosaicking algorithms for the fetal\nenvironment with a focus on creating drift-free mosaics from long duration\nfetoscopy videos. In this paper, we provide an overview of the FetReg dataset,\nchallenge tasks, evaluation metrics and baseline methods for both segmentation\nand registration. Baseline methods results on the FetReg dataset shows that our\ndataset poses interesting challenges, offering large opportunity for the\ncreation of novel methods and models through a community effort initiative\nguided by the FetReg challenge.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:14:27 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 10:15:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Bano", "Sophia", ""], ["Casella", "Alessandro", ""], ["Vasconcelos", "Francisco", ""], ["Moccia", "Sara", ""], ["Attilakos", "George", ""], ["Wimalasundera", "Ruwan", ""], ["David", "Anna L.", ""], ["Paladini", "Dario", ""], ["Deprest", "Jan", ""], ["De Momi", "Elena", ""], ["Mattos", "Leonardo S.", ""], ["Stoyanov", "Danail", ""]]}, {"id": "2106.05946", "submitter": "S\\\"oren Becker", "authors": "S\\\"oren Becker, Thomas Wiegand, Sebastian Bosse", "title": "Curiously Effective Features for Image Quality Prediction", "comments": "To be published at ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of visual quality prediction models is commonly assumed to be\nclosely tied to their ability to capture perceptually relevant image aspects.\nModels are thus either based on sophisticated feature extractors carefully\ndesigned from extensive domain knowledge or optimized through feature learning.\nIn contrast to this, we find feature extractors constructed from random noise\nto be sufficient to learn a linear regression model whose quality predictions\nreach high correlations with human visual quality ratings, on par with a model\nwith learned features. We analyze this curious result and show that besides the\nquality of feature extractors also their quantity plays a crucial role - with\ntop performances only being achieved in highly overparameterized models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:44:04 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Becker", "S\u00f6ren", ""], ["Wiegand", "Thomas", ""], ["Bosse", "Sebastian", ""]]}, {"id": "2106.05953", "submitter": "Adrian Spurr", "authors": "Adrian Spurr, Aneesh Dahiya, Xucong Zhang, Xi Wang, Otmar Hilliges", "title": "Self-Supervised 3D Hand Pose Estimation from monocular RGB via\n  Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Acquiring accurate 3D annotated data for hand pose estimation is a\nnotoriously difficult problem. This typically requires complex multi-camera\nsetups and controlled conditions, which in turn creates a domain gap that is\nhard to bridge to fully unconstrained settings. Encouraged by the success of\ncontrastive learning on image classification tasks, we propose a new\nself-supervised method for the structured regression task of 3D hand pose\nestimation. Contrastive learning makes use of unlabeled data for the purpose of\nrepresentation learning via a loss formulation that encourages the learned\nfeature representations to be invariant under any image transformation. For 3D\nhand pose estimation, it too is desirable to have invariance to appearance\ntransformation such as color jitter. However, the task requires equivariance\nunder affine transformations, such as rotation and translation. To address this\nissue, we propose an equivariant contrastive objective and demonstrate its\neffectiveness in the context of 3D hand pose estimation. We experimentally\ninvestigate the impact of invariant and equivariant contrastive objectives and\nshow that learning equivariant features leads to better representations for the\ntask of 3D hand pose estimation. Furthermore, we show that a standard\nResNet-152, trained on additional unlabeled data, attains an improvement of\n$7.6\\%$ in PA-EPE on FreiHAND and thus achieves state-of-the-art performance\nwithout any task specific, specialized architectures.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:48:57 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 16:09:58 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Spurr", "Adrian", ""], ["Dahiya", "Aneesh", ""], ["Zhang", "Xucong", ""], ["Wang", "Xi", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2106.05954", "submitter": "Adrian Spurr", "authors": "Adrian Spurr, Pavlo Molchanov, Umar Iqbal, Jan Kautz, Otmar Hilliges", "title": "Adversarial Motion Modelling helps Semi-supervised Hand Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Hand pose estimation is difficult due to different environmental conditions,\nobject- and self-occlusion as well as diversity in hand shape and appearance.\nExhaustively covering this wide range of factors in fully annotated datasets\nhas remained impractical, posing significant challenges for generalization of\nsupervised methods. Embracing this challenge, we propose to combine ideas from\nadversarial training and motion modelling to tap into unlabeled videos. To this\nend we propose what to the best of our knowledge is the first motion model for\nhands and show that an adversarial formulation leads to better generalization\nproperties of the hand pose estimator via semi-supervised training on unlabeled\nvideo sequences. In this setting, the pose predictor must produce a valid\nsequence of hand poses, as determined by a discriminative adversary. This\nadversary reasons both on the structural as well as temporal domain,\neffectively exploiting the spatio-temporal structure in the task. The main\nadvantage of our approach is that we can make use of unpaired videos and joint\nsequence data both of which are much easier to attain than paired training\ndata. We perform extensive evaluation, investigating essential components\nneeded for the proposed framework and empirically demonstrate in two\nchallenging settings that the proposed approach leads to significant\nimprovements in pose estimation accuracy. In the lowest label setting, we\nattain an improvement of $40\\%$ in absolute mean joint error.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:50:19 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Spurr", "Adrian", ""], ["Molchanov", "Pavlo", ""], ["Iqbal", "Umar", ""], ["Kautz", "Jan", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2106.05956", "submitter": "Ekdeep Singh Lubana", "authors": "Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka", "title": "Beyond BatchNorm: Towards a General Understanding of Normalization in\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by BatchNorm, there has been an explosion of normalization layers\nfor deep neural networks (DNNs). However, these alternative normalization\nlayers have seen minimal use, partially due to a lack of guiding principles\nthat can help identify when these layers can serve as a replacement for\nBatchNorm. To address this problem, we take a theoretical approach,\ngeneralizing the known beneficial mechanisms of BatchNorm to several recently\nproposed normalization techniques. Our generalized theory leads to the\nfollowing set of principles: (i) similar to BatchNorm, activations-based\nnormalization layers can prevent exponential growth of activations in ResNets,\nbut parametric layers require explicit remedies; (ii) use of GroupNorm can\nensure informative forward propagation, with different samples being assigned\ndissimilar activations, but increasing group size results in increasingly\nindistinguishable activations for different samples, explaining slow\nconvergence speed in models with LayerNorm; (iii) small group sizes result in\nlarge gradient norm in earlier layers, hence explaining training instability\nissues in Instance Normalization and illustrating a speed-stability tradeoff in\nGroupNorm. Overall, our analysis reveals a unified set of mechanisms that\nunderpin the success of normalization methods in deep learning, providing us\nwith a compass to systematically explore the vast design space of DNN\nnormalization layers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:51:30 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 17:57:01 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lubana", "Ekdeep Singh", ""], ["Dick", "Robert P.", ""], ["Tanaka", "Hidenori", ""]]}, {"id": "2106.05961", "submitter": "Deng Weijian", "authors": "Weijian Deng, Stephen Gould, Liang Zheng", "title": "What Does Rotation Prediction Tell Us about Classifier Accuracy under\n  Varying Testing Environments?", "comments": "ICML 2021 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding classifier decision under novel environments is central to the\ncommunity, and a common practice is evaluating it on labeled test sets.\nHowever, in real-world testing, image annotations are difficult and expensive\nto obtain, especially when the test environment is changing. A natural question\nthen arises: given a trained classifier, can we evaluate its accuracy on\nvarying unlabeled test sets? In this work, we train semantic classification and\nrotation prediction in a multi-task way. On a series of datasets, we report an\ninteresting finding, i.e., the semantic classification accuracy exhibits a\nstrong linear relationship with the accuracy of the rotation prediction task\n(Pearson's Correlation r > 0.88). This finding allows us to utilize linear\nregression to estimate classifier performance from the accuracy of rotation\nprediction which can be obtained on the test set through the freely generated\nrotation labels.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:55:37 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Deng", "Weijian", ""], ["Gould", "Stephen", ""], ["Zheng", "Liang", ""]]}, {"id": "2106.05963", "submitter": "Jonas Wulff", "authors": "Manel Baradad, Jonas Wulff, Tongzhou Wang, Phillip Isola, Antonio\n  Torralba", "title": "Learning to See by Looking at Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current vision systems are trained on huge datasets, and these datasets come\nwith costs: curation is expensive, they inherit human biases, and there are\nconcerns over privacy and usage rights. To counter these costs, interest has\nsurged in learning from cheaper data sources, such as unlabeled images. In this\npaper we go a step further and ask if we can do away with real image datasets\nentirely, instead learning from noise processes. We investigate a suite of\nimage generation models that produce images from simple random processes. These\nare then used as training data for a visual representation learner with a\ncontrastive loss. We study two types of noise processes, statistical image\nmodels and deep generative models under different random initializations. Our\nfindings show that it is important for the noise to capture certain structural\nproperties of real data but that good performance can be achieved even with\nprocesses that are far from realistic. We also find that diversity is a key\nproperty to learn good representations. Datasets, models, and code are\navailable at https://mbaradad.github.io/learning_with_noise.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:56:46 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Baradad", "Manel", ""], ["Wulff", "Jonas", ""], ["Wang", "Tongzhou", ""], ["Isola", "Phillip", ""], ["Torralba", "Antonio", ""]]}, {"id": "2106.05965", "submitter": "Kieran Murphy", "authors": "Kieran Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam,\n  Ameesh Makadia", "title": "Implicit-PDF: Non-Parametric Representation of Probability Distributions\n  on the Rotation Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single image pose estimation is a fundamental problem in many vision and\nrobotics tasks, and existing deep learning approaches suffer by not completely\nmodeling and handling: i) uncertainty about the predictions, and ii) symmetric\nobjects with multiple (sometimes infinite) correct poses. To this end, we\nintroduce a method to estimate arbitrary, non-parametric distributions on\nSO(3). Our key idea is to represent the distributions implicitly, with a neural\nnetwork that estimates the probability given the input image and a candidate\npose. Grid sampling or gradient ascent can be used to find the most likely\npose, but it is also possible to evaluate the probability at any pose, enabling\nreasoning about symmetries and uncertainty. This is the most general way of\nrepresenting distributions on manifolds, and to showcase the rich expressive\npower, we introduce a dataset of challenging symmetric and nearly-symmetric\nobjects. We require no supervision on pose uncertainty -- the model trains only\nwith a single pose per example. Nonetheless, our implicit model is highly\nexpressive to handle complex distributions over 3D poses, while still obtaining\naccurate pose estimation on standard non-ambiguous environments, achieving\nstate-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:57:23 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Murphy", "Kieran", ""], ["Esteves", "Carlos", ""], ["Jampani", "Varun", ""], ["Ramalingam", "Srikumar", ""], ["Makadia", "Ameesh", ""]]}, {"id": "2106.05966", "submitter": "Jimuyang Zhang", "authors": "Jimuyang Zhang and Eshed Ohn-Bar", "title": "Learning by Watching", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When in a new situation or geographical location, human drivers have an\nextraordinary ability to watch others and learn maneuvers that they themselves\nmay have never performed. In contrast, existing techniques for learning to\ndrive preclude such a possibility as they assume direct access to an\ninstrumented ego-vehicle with fully known observations and expert driver\nactions. However, such measurements cannot be directly accessed for the non-ego\nvehicles when learning by watching others. Therefore, in an application where\ndata is regarded as a highly valuable asset, current approaches completely\ndiscard the vast portion of the training data that can be potentially obtained\nthrough indirect observation of surrounding vehicles. Motivated by this key\ninsight, we propose the Learning by Watching (LbW) framework which enables\nlearning a driving policy without requiring full knowledge of neither the state\nnor expert actions. To increase its data, i.e., with new perspectives and\nmaneuvers, LbW makes use of the demonstrations of other vehicles in a given\nscene by (1) transforming the ego-vehicle's observations to their points of\nview, and (2) inferring their expert actions. Our LbW agent learns more robust\ndriving policies while enabling data-efficient learning, including quick\nadaptation of the policy to rare and novel scenarios. In particular, LbW drives\nrobustly even with a fraction of available driving data required by existing\nmethods, achieving an average success rate of 92% on the original CARLA\nbenchmark with only 30 minutes of total driving data and 82% with only 10\nminutes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:58:34 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhang", "Jimuyang", ""], ["Ohn-Bar", "Eshed", ""]]}, {"id": "2106.05967", "submitter": "Wouter Van Gansbeke", "authors": "Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Luc Van\n  Gool", "title": "Revisiting Contrastive Methods for Unsupervised Learning of Visual\n  Representations", "comments": "Paper and supplementary (20 pages). Code:\n  https://github.com/wvangansbeke/Revisiting-Contrastive-SSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive self-supervised learning has outperformed supervised pretraining\non many downstream tasks like segmentation and object detection. However,\ncurrent methods are still primarily applied to curated datasets like ImageNet.\nIn this paper, we first study how biases in the dataset affect existing\nmethods. Our results show that current contrastive approaches work surprisingly\nwell across: (i) object- versus scene-centric, (ii) uniform versus long-tailed\nand (iii) general versus domain-specific datasets. Second, given the generality\nof the approach, we try to realize further gains with minor modifications. We\nshow that learning additional invariances -- through the use of multi-scale\ncropping, stronger augmentations and nearest neighbors -- improves the\nrepresentations. Finally, we observe that MoCo learns spatially structured\nrepresentations when trained with a multi-crop strategy. The representations\ncan be used for semantic segment retrieval and video instance segmentation\nwithout finetuning. Moreover, the results are on par with specialized models.\nWe hope this work will serve as a useful study for other researchers. The code\nand models will be available at\nhttps://github.com/wvangansbeke/Revisiting-Contrastive-SSL.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:59:13 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Van Gansbeke", "Wouter", ""], ["Vandenhende", "Simon", ""], ["Georgoulis", "Stamatios", ""], ["Van Gool", "Luc", ""]]}, {"id": "2106.05968", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Juan-Manuel Perez-Rua and Swathikiran Sudhakaran and\n  Brais Martinez and Georgios Tzimiropoulos", "title": "Space-time Mixing Attention for Video Transformer", "comments": "Updated results on SSv2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is on video recognition using Transformers. Very recent attempts\nin this area have demonstrated promising results in terms of recognition\naccuracy, yet they have been also shown to induce, in many cases, significant\ncomputational overheads due to the additional modelling of the temporal\ninformation. In this work, we propose a Video Transformer model the complexity\nof which scales linearly with the number of frames in the video sequence and\nhence induces no overhead compared to an image-based Transformer model. To\nachieve this, our model makes two approximations to the full space-time\nattention used in Video Transformers: (a) It restricts time attention to a\nlocal temporal window and capitalizes on the Transformer's depth to obtain full\ntemporal coverage of the video sequence. (b) It uses efficient space-time\nmixing to attend jointly spatial and temporal locations without inducing any\nadditional cost on top of a spatial-only attention model. We also show how to\nintegrate 2 very lightweight mechanisms for global temporal-only attention\nwhich provide additional accuracy improvements at minimal computational cost.\nWe demonstrate that our model produces very high recognition accuracy on the\nmost popular video recognition datasets while at the same time being\nsignificantly more efficient than other Video Transformer models. Code will be\nmade available.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:59:14 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 12:06:04 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Bulat", "Adrian", ""], ["Perez-Rua", "Juan-Manuel", ""], ["Sudhakaran", "Swathikiran", ""], ["Martinez", "Brais", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2106.05969", "submitter": "Zhengyi Luo", "authors": "Zhengyi Luo, Ryo Hachiuma, Ye Yuan, Kris Kitani", "title": "Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation", "comments": "Project page: https://zhengyiluo.github.io/projects/kin_poly/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a method for object-aware 3D egocentric pose estimation that\ntightly integrates kinematics modeling, dynamics modeling, and scene object\ninformation. Unlike prior kinematics or dynamics-based approaches where the two\ncomponents are used disjointly, we synergize the two approaches via\ndynamics-regulated training. At each timestep, a kinematic model is used to\nprovide a target pose using video evidence and simulation state. Then, a\nprelearned dynamics model attempts to mimic the kinematic pose in a physics\nsimulator. By comparing the pose instructed by the kinematic model against the\npose generated by the dynamics model, we can use their misalignment to further\nimprove the kinematic model. By factoring in the 6DoF pose of objects (e.g.,\nchairs, boxes) in the scene, we demonstrate for the first time, the ability to\nestimate physically-plausible 3D human-object interactions using a single\nwearable camera. We evaluate our egocentric pose estimation method in both\ncontrolled laboratory settings and real-world scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:59:50 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Luo", "Zhengyi", ""], ["Hachiuma", "Ryo", ""], ["Yuan", "Ye", ""], ["Kitani", "Kris", ""]]}, {"id": "2106.05970", "submitter": "Wanrong Zhu", "authors": "Wanrong Zhu, Xin Eric Wang, An Yan, Miguel Eckstein, William Yang Wang", "title": "ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural\n  Language Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic evaluations for natural language generation (NLG) conventionally\nrely on token-level or embedding-level comparisons with the text references.\nThis is different from human language processing, for which visual imaginations\noften improve comprehension. In this work, we propose ImaginE, an\nimagination-based automatic evaluation metric for natural language generation.\nWith the help of CLIP and DALL-E, two cross-modal models pre-trained on\nlarge-scale image-text pairs, we automatically generate an image as the\nembodied imagination for the text snippet and compute the imagination\nsimilarity using contextual embeddings. Experiments spanning several text\ngeneration tasks demonstrate that adding imagination with our ImaginE displays\ngreat potential in introducing multi-modal information into NLG evaluation, and\nimproves existing automatic metrics' correlations with human similarity\njudgments in many circumstances.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:59:52 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhu", "Wanrong", ""], ["Wang", "Xin Eric", ""], ["Yan", "An", ""], ["Eckstein", "Miguel", ""], ["Wang", "William Yang", ""]]}, {"id": "2106.05974", "submitter": "Carlos Riquelme Ruiz", "authors": "Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann,\n  Rodolphe Jenatton, Andr\\'e Susano Pinto, Daniel Keysers, Neil Houlsby", "title": "Scaling Vision with Sparse Mixture of Experts", "comments": "44 pages, 38 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent\nscalability in Natural Language Processing. In Computer Vision, however, almost\nall performant networks are \"dense\", that is, every input is processed by every\nparameter. We present a Vision MoE (V-MoE), a sparse version of the Vision\nTransformer, that is scalable and competitive with the largest dense networks.\nWhen applied to image recognition, V-MoE matches the performance of\nstate-of-the-art networks, while requiring as little as half of the compute at\ninference time. Further, we propose an extension to the routing algorithm that\ncan prioritize subsets of each input across the entire batch, leading to\nadaptive per-image compute. This allows V-MoE to trade-off performance and\ncompute smoothly at test-time. Finally, we demonstrate the potential of V-MoE\nto scale vision models, and train a 15B parameter model that attains 90.35% on\nImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:10:56 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Riquelme", "Carlos", ""], ["Puigcerver", "Joan", ""], ["Mustafa", "Basil", ""], ["Neumann", "Maxim", ""], ["Jenatton", "Rodolphe", ""], ["Pinto", "Andr\u00e9 Susano", ""], ["Keysers", "Daniel", ""], ["Houlsby", "Neil", ""]]}, {"id": "2106.06007", "submitter": "Zhen Wang", "authors": "Yunhao Ba, Zhen Wang, Kerim Doruk Karinca, Oyku Deniz Bozkurt, and\n  Achuta Kadambi", "title": "Overcoming Difficulty in Obtaining Dark-skinned Subjects for Remote-PPG\n  by Synthetic Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera-based remote photoplethysmography (rPPG) provides a non-contact way to\nmeasure physiological signals (e.g., heart rate) using facial videos. Recent\ndeep learning architectures have improved the accuracy of such physiological\nmeasurement significantly, yet they are restricted by the diversity of the\nannotated videos. The existing datasets MMSE-HR, AFRL, and UBFC-RPPG contain\nroughly 10%, 0%, and 5% of dark-skinned subjects respectively. The unbalanced\ntraining sets result in a poor generalization capability to unseen subjects and\nlead to unwanted bias toward different demographic groups. In Western academia,\nit is regrettably difficult in a university setting to collect data on these\ndark-skinned subjects. Here we show a first attempt to overcome the lack of\ndark-skinned subjects by synthetic augmentation. A joint optimization framework\nis utilized to translate real videos from light-skinned subjects to dark skin\ntones while retaining their pulsatile signals. In the experiment, our method\nexhibits around 31% reduction in mean absolute error for the dark-skinned group\nand 46% improvement on bias mitigation for all the groups, as compared with the\nprevious work trained with just real samples.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 19:00:08 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Ba", "Yunhao", ""], ["Wang", "Zhen", ""], ["Karinca", "Kerim Doruk", ""], ["Bozkurt", "Oyku Deniz", ""], ["Kadambi", "Achuta", ""]]}, {"id": "2106.06011", "submitter": "Yibo Guo", "authors": "Yibo Guo, Haidi Wang, Yiming Fan, Shunyao Li, Mingliang Xu", "title": "A self-adapting super-resolution structures framework for automatic\n  design of GAN", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of deep learning, the single super-resolution image\nreconstruction network models are becoming more and more complex. Small changes\nin hyperparameters of the models have a greater impact on model performance. In\nthe existing works, experts have gradually explored a set of optimal model\nparameters based on empirical values or performing brute-force search. In this\npaper, we introduce a new super-resolution image reconstruction generative\nadversarial network framework, and a Bayesian optimization method used to\noptimizing the hyperparameters of the generator and discriminator. The\ngenerator is made by self-calibrated convolution, and discriminator is made by\nconvolution lays. We have defined the hyperparameters such as the number of\nnetwork layers and the number of neurons. Our method adopts Bayesian\noptimization as a optimization policy of GAN in our model. Not only can find\nthe optimal hyperparameter solution automatically, but also can construct a\nsuper-resolution image reconstruction network, reducing the manual workload.\nExperiments show that Bayesian optimization can search the optimal solution\nearlier than the other two optimization algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 19:11:29 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Guo", "Yibo", ""], ["Wang", "Haidi", ""], ["Fan", "Yiming", ""], ["Li", "Shunyao", ""], ["Xu", "Mingliang", ""]]}, {"id": "2106.06012", "submitter": "Firas Laakom", "authors": "Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj", "title": "Within-layer Diversity Reduces Generalization Gap", "comments": "18 pages, 1 figure, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are composed of multiple layers arranged in a hierarchical\nstructure jointly trained with a gradient-based optimization, where the errors\nare back-propagated from the last layer back to the first one. At each\noptimization step, neurons at a given layer receive feedback from neurons\nbelonging to higher layers of the hierarchy. In this paper, we propose to\ncomplement this traditional 'between-layer' feedback with additional\n'within-layer' feedback to encourage diversity of the activations within the\nsame layer. To this end, we measure the pairwise similarity between the outputs\nof the neurons and use it to model the layer's overall diversity. By penalizing\nsimilarities and promoting diversity, we encourage each neuron to learn a\ndistinctive representation and, thus, to enrich the data representation learned\nwithin the layer and to increase the total capacity of the model. We\ntheoretically study how the within-layer activation diversity affects the\ngeneralization performance of a neural network and prove that increasing the\ndiversity of hidden activations reduces the estimation error. In addition to\nthe theoretical guarantees, we present an empirical study on three datasets\nconfirming that the proposed approach enhances the performance of\nstate-of-the-art neural network models and decreases the generalization gap.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 19:14:45 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Laakom", "Firas", ""], ["Raitoharju", "Jenni", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2106.06020", "submitter": "Maurice Weiler", "authors": "Maurice Weiler, Patrick Forr\\'e, Erik Verlinde, Max Welling", "title": "Coordinate Independent Convolutional Networks -- Isometry and Gauge\n  Equivariant Convolutions on Riemannian Manifolds", "comments": "The implementation of orientation independent M\\\"obius convolutions\n  is publicly available at https://github.com/mauriceweiler/MobiusCNNs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the vast success of deep convolutional networks, there is a\ngreat interest in generalizing convolutions to non-Euclidean manifolds. A major\ncomplication in comparison to flat spaces is that it is unclear in which\nalignment a convolution kernel should be applied on a manifold. The underlying\nreason for this ambiguity is that general manifolds do not come with a\ncanonical choice of reference frames (gauge). Kernels and features therefore\nhave to be expressed relative to arbitrary coordinates. We argue that the\nparticular choice of coordinatization should not affect a network's inference\n-- it should be coordinate independent. A simultaneous demand for coordinate\nindependence and weight sharing is shown to result in a requirement on the\nnetwork to be equivariant under local gauge transformations (changes of local\nreference frames). The ambiguity of reference frames depends thereby on the\nG-structure of the manifold, such that the necessary level of gauge\nequivariance is prescribed by the corresponding structure group G. Coordinate\nindependent convolutions are proven to be equivariant w.r.t. those isometries\nthat are symmetries of the G-structure. The resulting theory is formulated in a\ncoordinate free fashion in terms of fiber bundles. To exemplify the design of\ncoordinate independent convolutions, we implement a convolutional network on\nthe M\\\"obius strip. The generality of our differential geometric formulation of\nconvolutional networks is demonstrated by an extensive literature review which\nexplains a large number of Euclidean CNNs, spherical CNNs and CNNs on general\nsurfaces as specific instances of coordinate independent convolutions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 19:54:19 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Weiler", "Maurice", ""], ["Forr\u00e9", "Patrick", ""], ["Verlinde", "Erik", ""], ["Welling", "Max", ""]]}, {"id": "2106.06027", "submitter": "Tianlong Chen", "authors": "Mingkang Zhu, Tianlong Chen, Zhangyang Wang", "title": "Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse adversarial attacks can fool deep neural networks (DNNs) by only\nperturbing a few pixels (regularized by l_0 norm). Recent efforts combine it\nwith another l_infty imperceptible on the perturbation magnitudes. The\nresultant sparse and imperceptible attacks are practically relevant, and\nindicate an even higher vulnerability of DNNs that we usually imagined.\nHowever, such attacks are more challenging to generate due to the optimization\ndifficulty by coupling the l_0 regularizer and box constraints with a\nnon-convex objective. In this paper, we address this challenge by proposing a\nhomotopy algorithm, to jointly tackle the sparsity and the perturbation bound\nin one unified framework. Each iteration, the main step of our algorithm is to\noptimize an l_0-regularized adversarial loss, by leveraging the nonmonotone\nAccelerated Proximal Gradient Method (nmAPG) for nonconvex programming; it is\nfollowed by an l_0 change control step, and an optional post-attack step\ndesigned to escape bad local minima. We also extend the algorithm to handling\nthe structural sparsity regularizer. We extensively examine the effectiveness\nof our proposed homotopy attack for both targeted and non-targeted attack\nscenarios, on CIFAR-10 and ImageNet datasets. Compared to state-of-the-art\nmethods, our homotopy attack leads to significantly fewer perturbations, e.g.,\nreducing 42.91% on CIFAR-10 and 75.03% on ImageNet (average case, targeted\nattack), at similar maximal perturbation magnitudes, when still achieving 100%\nattack success rates. Our codes are available at:\nhttps://github.com/VITA-Group/SparseADV_Homotopy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 20:11:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhu", "Mingkang", ""], ["Chen", "Tianlong", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2106.06047", "submitter": "Liangqiong Qu", "authors": "Liangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, Li\n  Fei-Fei, Ehsan Adeli, Daniel Rubin", "title": "Rethinking Architecture Design for Tackling Data Heterogeneity in\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is an emerging research paradigm enabling collaborative\ntraining of machine learning models among different organizations while keeping\ndata private at each institution. Despite recent progress, there remain\nfundamental challenges such as lack of convergence and potential for\ncatastrophic forgetting in federated learning across real-world heterogeneous\ndevices. In this paper, we demonstrate that attention-based architectures\n(e.g., Transformers) are fairly robust to distribution shifts and hence improve\nfederated learning over heterogeneous data. Concretely, we conduct the first\nrigorous empirical investigation of different neural architectures across a\nrange of federated algorithms, real-world benchmarks, and heterogeneous data\nsplits. Our experiments show that simply replacing convolutional networks with\nTransformers can greatly reduce catastrophic forgetting of previous devices,\naccelerate convergence, and reach a better global model, especially when\ndealing with heterogeneous data. We will release our code and pretrained models\nat https://github.com/Liangqiong/ViT-FL-main to encourage future exploration in\nrobust architectures as an alternative to current research efforts on the\noptimization front.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 21:04:18 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Qu", "Liangqiong", ""], ["Zhou", "Yuyin", ""], ["Liang", "Paul Pu", ""], ["Xia", "Yingda", ""], ["Wang", "Feifei", ""], ["Fei-Fei", "Li", ""], ["Adeli", "Ehsan", ""], ["Rubin", "Daniel", ""]]}, {"id": "2106.06056", "submitter": "Linyi Li", "authors": "Jiawei Zhang and Linyi Li and Huichen Li and Xiaolu Zhang and Shuang\n  Yang and Bo Li", "title": "Progressive-Scale Boundary Blackbox Attack via Projective Gradient\n  Estimation", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Boundary based blackbox attack has been recognized as practical and\neffective, given that an attacker only needs to access the final model\nprediction. However, the query efficiency of it is in general high especially\nfor high dimensional image data. In this paper, we show that such efficiency\nhighly depends on the scale at which the attack is applied, and attacking at\nthe optimal scale significantly improves the efficiency. In particular, we\npropose a theoretical framework to analyze and show three key characteristics\nto improve the query efficiency. We prove that there exists an optimal scale\nfor projective gradient estimation. Our framework also explains the\nsatisfactory performance achieved by existing boundary black-box attacks. Based\non our theoretical framework, we propose Progressive-Scale enabled projective\nBoundary Attack (PSBA) to improve the query efficiency via progressive scaling\ntechniques. In particular, we employ Progressive-GAN to optimize the scale of\nprojections, which we call PSBA-PGAN. We evaluate our approach on both spatial\nand frequency scales. Extensive experiments on MNIST, CIFAR-10, CelebA, and\nImageNet against different models including a real-world face recognition API\nshow that PSBA-PGAN significantly outperforms existing baseline attacks in\nterms of query efficiency and attack success rate. We also observe relatively\nstable optimal scales for different models and datasets. The code is publicly\navailable at https://github.com/AI-secure/PSBA.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 21:13:41 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhang", "Jiawei", ""], ["Li", "Linyi", ""], ["Li", "Huichen", ""], ["Zhang", "Xiaolu", ""], ["Yang", "Shuang", ""], ["Li", "Bo", ""]]}, {"id": "2106.06059", "submitter": "Pankaj Roy", "authors": "Pankaj Raj Roy, Guillaume-Alexandre Bilodeau and Lama Seoud", "title": "Predicting Next Local Appearance for Video Anomaly Detection", "comments": "Accepted as an oral presentation for MVA'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a local anomaly detection method in videos. As opposed to most\nexisting methods that are computationally expensive and are not very\ngeneralizable across different video scenes, we propose an adversarial\nframework that learns the temporal local appearance variations by predicting\nthe appearance of a normally behaving object in the next frame of a scene by\nonly relying on its current and past appearances. In the presence of an\nabnormally behaving object, the reconstruction error between the real and the\npredicted next appearance of that object indicates the likelihood of an\nanomaly. Our method is competitive with the existing state-of-the-art while\nbeing significantly faster for both training and inference and being better at\ngeneralizing to unseen video scenes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 21:26:07 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Roy", "Pankaj Raj", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Seoud", "Lama", ""]]}, {"id": "2106.06072", "submitter": "Jeffri Murrugarra Llerena", "authors": "Jeffri M. Llerena, Luis Felipe Zeni, Lucas N. Kristen, Claudio Jung", "title": "Gaussian Bounding Boxes and Probabilistic Intersection-over-Union for\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most object detection methods use bounding boxes to encode and represent the\nobject shape and location. In this work, we explore a fuzzy representation of\nobject regions using Gaussian distributions, which provides an implicit binary\nrepresentation as (potentially rotated) ellipses. We also present a similarity\nmeasure for the Gaussian distributions based on the Hellinger Distance, which\ncan be viewed as a Probabilistic Intersection-over-Union (ProbIoU). Our\nexperimental results show that the proposed Gaussian representations are closer\nto annotated segmentation masks in publicly available datasets, and that loss\nfunctions based on ProbIoU can be successfully used to regress the parameters\nof the Gaussian representation. Furthermore, we present a simple mapping scheme\nfrom traditional (or rotated) bounding boxes to Gaussian representations,\nallowing the proposed ProbIoU-based losses to be seamlessly integrated into any\nobject detector.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 22:24:31 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Llerena", "Jeffri M.", ""], ["Zeni", "Luis Felipe", ""], ["Kristen", "Lucas N.", ""], ["Jung", "Claudio", ""]]}, {"id": "2106.06073", "submitter": "Nicolas Roth", "authors": "Nicolas Roth, Pia Bideau, Olaf Hellwich, Martin Rolfs, Klaus Obermayer", "title": "A modular framework for object-based saccadic decisions in dynamic\n  scenes", "comments": "Accepted for presentation at EPIC@CVPR2021 workshop, 4 pages, 2\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually exploring the world around us is not a passive process. Instead, we\nactively explore the world and acquire visual information over time. Here, we\npresent a new model for simulating human eye-movement behavior in dynamic\nreal-world scenes. We model this active scene exploration as a sequential\ndecision making process. We adapt the popular drift-diffusion model (DDM) for\nperceptual decision making and extend it towards multiple options, defined by\nobjects present in the scene. For each possible choice, the model integrates\nevidence over time and a decision (saccadic eye movement) is triggered as soon\nas evidence crosses a decision threshold. Drawing this explicit connection\nbetween decision making and object-based scene perception is highly relevant in\nthe context of active viewing, where decisions are made continuously while\ninteracting with an external environment. We validate our model with a\ncarefully designed ablation study and explore influences of our model\nparameters. A comparison on the VidCom dataset supports the plausibility of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 22:28:45 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Roth", "Nicolas", ""], ["Bideau", "Pia", ""], ["Hellwich", "Olaf", ""], ["Rolfs", "Martin", ""], ["Obermayer", "Klaus", ""]]}, {"id": "2106.06112", "submitter": "Jingyi Zhang", "authors": "Jingyi Zhang, Jiaxing Huang and Shijian Lu", "title": "Spectral Unsupervised Domain Adaptation for Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to learn a well-performed model in\nan unlabeled target domain by leveraging labeled data from one or multiple\nrelated source domains. It remains a great challenge due to 1) the lack of\nannotations in the target domain and 2) the rich discrepancy between the\ndistributions of source and target data. We propose Spectral UDA (SUDA), an\nefficient yet effective UDA technique that works in the spectral space and is\ngeneric across different visual recognition tasks in detection, classification\nand segmentation. SUDA addresses UDA challenges from two perspectives. First,\nit mitigates inter-domain discrepancies by a spectrum transformer (ST) that\nmaps source and target images into spectral space and learns to enhance\ndomain-invariant spectra while suppressing domain-variant spectra\nsimultaneously. To this end, we design novel adversarial multi-head spectrum\nattention that leverages contextual information to identify domain-variant and\ndomain-invariant spectra effectively. Second, it mitigates the lack of\nannotations in target domain by introducing multi-view spectral learning which\naims to learn comprehensive yet confident target representations by maximizing\nthe mutual information among multiple ST augmentations capturing different\nspectral views of each target sample. Extensive experiments over different\nvisual tasks (e.g., detection, classification and segmentation) show that SUDA\nachieves superior accuracy and it is also complementary with state-of-the-art\nUDA methods with consistent performance boosts but little extra computation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 01:31:52 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhang", "Jingyi", ""], ["Huang", "Jiaxing", ""], ["Lu", "Shijian", ""]]}, {"id": "2106.06129", "submitter": "Pavan Kumar Anasosalu Vasu", "authors": "Pavan Kumar Anasosalu Vasu, Shreyas Saxena, Oncel Tuzel", "title": "Instance-Level Task Parameters: A Robust Multi-task Weighting Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works have shown that deep neural networks benefit from multi-task\nlearning by learning a shared representation across several related tasks.\nHowever, performance of such systems depend on relative weighting between\nvarious losses involved during training. Prior works on loss weighting schemes\nassume that instances are equally easy or hard for all tasks. In order to break\nthis assumption, we let the training process dictate the optimal weighting of\ntasks for every instance in the dataset. More specifically, we equip every\ninstance in the dataset with a set of learnable parameters (instance-level task\nparameters) where the cardinality is equal to the number of tasks learned by\nthe model. These parameters model the weighting of each task for an instance.\nThey are updated by gradient descent and do not require hand-crafted rules. We\nconduct extensive experiments on SURREAL and CityScapes datasets, for human\nshape and pose estimation, depth estimation and semantic segmentation tasks. In\nthese tasks, our approach outperforms recent dynamic loss weighting approaches,\ne.g. reducing surface estimation errors by 8.97% on SURREAL. When applied to\ndatasets where one or more tasks can have noisy annotations, the proposed\nmethod learns to prioritize learning from clean labels for a given task, e.g.\nreducing surface estimation errors by up to 60%. We also show that we can\nreliably detect corrupt labels for a given task as a by-product from learned\ninstance-level task parameters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 02:35:42 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Vasu", "Pavan Kumar Anasosalu", ""], ["Saxena", "Shreyas", ""], ["Tuzel", "Oncel", ""]]}, {"id": "2106.06133", "submitter": "Xiao Zhang", "authors": "Xiao Zhang, Yixiao Ge, Yu Qiao, Hongsheng Li", "title": "Refining Pseudo Labels with Clustering Consensus over Generations for\n  Unsupervised Object Re-identification", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised object re-identification targets at learning discriminative\nrepresentations for object retrieval without any annotations. Clustering-based\nmethods conduct training with the generated pseudo labels and currently\ndominate this research direction. However, they still suffer from the issue of\npseudo label noise. To tackle the challenge, we propose to properly estimate\npseudo label similarities between consecutive training generations with\nclustering consensus and refine pseudo labels with temporally propagated and\nensembled pseudo labels. To the best of our knowledge, this is the first\nattempt to leverage the spirit of temporal ensembling to improve classification\nwith dynamically changing classes over generations. The proposed pseudo label\nrefinery strategy is simple yet effective and can be seamlessly integrated into\nexisting clustering-based unsupervised re-identification methods. With our\nproposed approach, state-of-the-art method can be further boosted with up to\n8.8% mAP improvements on the challenging MSMT17 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 02:42:42 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhang", "Xiao", ""], ["Ge", "Yixiao", ""], ["Qiao", "Yu", ""], ["Li", "Hongsheng", ""]]}, {"id": "2106.06138", "submitter": "Jieting Chen", "authors": "Ludan Ruan (1), Jieting Chen (1), Yuqing Song (1), Shizhe Chen (2),\n  Qin Jin (1) ((1) Renmin University of China, (2) INRIA)", "title": "Team RUC_AIM3 Technical Report at ActivityNet 2021: Entities Object\n  Localization", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entities Object Localization (EOL) aims to evaluate how grounded or faithful\na description is, which consists of caption generation and object grounding.\nPrevious works tackle this problem by jointly training the two modules in a\nframework, which limits the complexity of each module. Therefore, in this work,\nwe propose to divide these two modules into two stages and improve them\nrespectively to boost the whole system performance. For the caption generation,\nwe propose a Unified Multi-modal Pre-training Model (UMPM) to generate event\ndescriptions with rich objects for better localization. For the object\ngrounding, we fine-tune the state-of-the-art detection model MDETR and design a\npost processing method to make the grounding results more faithful. Our overall\nsystem achieves the state-of-the-art performances on both sub-tasks in Entities\nObject Localization challenge at Activitynet 2021, with 72.57 localization\naccuracy on the testing set of sub-task I and 0.2477 F1_all_per_sent on the\nhidden testing set of sub-task II.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 02:50:25 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Ruan", "Ludan", "", "Renmin University of China"], ["Chen", "Jieting", "", "Renmin University of China"], ["Song", "Yuqing", "", "Renmin University of China"], ["Chen", "Shizhe", "", "INRIA"], ["Jin", "Qin", "", "Renmin University of China"]]}, {"id": "2106.06158", "submitter": "Ahmed Gad", "authors": "Ahmed Fawzy Gad", "title": "PyGAD: An Intuitive Genetic Algorithm Python Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces PyGAD, an open-source easy-to-use Python library for\nbuilding the genetic algorithm. PyGAD supports a wide range of parameters to\ngive the user control over everything in its life cycle. This includes, but is\nnot limited to, population, gene value range, gene data type, parent selection,\ncrossover, and mutation. PyGAD is designed as a general-purpose optimization\nlibrary that allows the user to customize the fitness function. Its usage\nconsists of 3 main steps: build the fitness function, create an instance of the\npygad.GA class, and calling the pygad.GA.run() method. The library supports\ntraining deep learning models created either with PyGAD itself or with\nframeworks like Keras and PyTorch. Given its stable state, PyGAD is also in\nactive development to respond to the user's requested features and enhancement\nreceived on GitHub https://github.com/ahmedfgad/GeneticAlgorithmPython. PyGAD\ncomes with documentation https://pygad.readthedocs.io for further details and\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 04:08:30 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Gad", "Ahmed Fawzy", ""]]}, {"id": "2106.06159", "submitter": "Yanhai Gan", "authors": "Yanhai Gan, Xinghui Dong, Huiyu Zhou, Feng Gao, Junyu Dong", "title": "Learning the Precise Feature for Cluster Assignment", "comments": null, "journal-ref": null, "doi": "10.1109/TCYB.2021.3079914", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the fundamental tasks in computer vision and pattern\nrecognition. Recently, deep clustering methods (algorithms based on deep\nlearning) have attracted wide attention with their impressive performance. Most\nof these algorithms combine deep unsupervised representation learning and\nstandard clustering together. However, the separation of representation\nlearning and clustering will lead to suboptimal solutions because the two-stage\nstrategy prevents representation learning from adapting to subsequent tasks\n(e.g., clustering according to specific cues). To overcome this issue, efforts\nhave been made in the dynamic adaption of representation and cluster\nassignment, whereas current state-of-the-art methods suffer from heuristically\nconstructed objectives with representation and cluster assignment alternatively\noptimized. To further standardize the clustering problem, we audaciously\nformulate the objective of clustering as finding a precise feature as the cue\nfor cluster assignment. Based on this, we propose a general-purpose deep\nclustering framework which radically integrates representation learning and\nclustering into a single pipeline for the first time. The proposed framework\nexploits the powerful ability of recently developed generative models for\nlearning intrinsic features, and imposes an entropy minimization on the\ndistribution of the cluster assignment by a dedicated variational algorithm.\nExperimental results show that the performance of the proposed method is\nsuperior, or at least comparable to, the state-of-the-art methods on the\nhandwritten digit recognition, fashion recognition, face recognition and object\nrecognition benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 04:08:54 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Gan", "Yanhai", ""], ["Dong", "Xinghui", ""], ["Zhou", "Huiyu", ""], ["Gao", "Feng", ""], ["Dong", "Junyu", ""]]}, {"id": "2106.06181", "submitter": "Yuriy Anisimov", "authors": "Yuriy Anisimov, Gerd Reis, Didier Stricker", "title": "Calibration and Auto-Refinement for Light Field Cameras", "comments": "Presented on 29. International Conference on Computer Graphics,\n  Visualization and Computer Vision 2021 (WSCG 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to create an accurate three-dimensional reconstruction of a\ncaptured scene draws attention to the principles of light fields. This paper\npresents an approach for light field camera calibration and rectification,\nbased on pairwise pattern-based parameters extraction. It is followed by a\ncorrespondence-based algorithm for camera parameters refinement from arbitrary\nscenes using the triangulation filter and nonlinear optimization. The\neffectiveness of our approach is validated on both real and synthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 05:49:14 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Anisimov", "Yuriy", ""], ["Reis", "Gerd", ""], ["Stricker", "Didier", ""]]}, {"id": "2106.06195", "submitter": "Xing Cheng", "authors": "Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, Dong Shen, Zhongyuan\n  Wang, Nian Shi, Honglin Liu", "title": "MlTr: Multi-label Classification with Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of multi-label image classification is to recognize all the object\nlabels presented in an image. Though advancing for years, small objects,\nsimilar objects and objects with high conditional probability are still the\nmain bottlenecks of previous convolutional neural network(CNN) based models,\nlimited by convolutional kernels' representational capacity. Recent vision\ntransformer networks utilize the self-attention mechanism to extract the\nfeature of pixel granularity, which expresses richer local semantic\ninformation, while is insufficient for mining global spatial dependence. In\nthis paper, we point out the three crucial problems that CNN-based methods\nencounter and explore the possibility of conducting specific transformer\nmodules to settle them. We put forward a Multi-label Transformer\narchitecture(MlTr) constructed with windows partitioning, in-window pixel\nattention, cross-window attention, particularly improving the performance of\nmulti-label image classification tasks. The proposed MlTr shows\nstate-of-the-art results on various prevalent multi-label datasets such as\nMS-COCO, Pascal-VOC, and NUS-WIDE with 88.5%, 95.8%, and 65.5% respectively.\nThe code will be available soon at https://github.com/starmemda/MlTr/\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 06:53:09 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Cheng", "Xing", ""], ["Lin", "Hezheng", ""], ["Wu", "Xiangyu", ""], ["Yang", "Fan", ""], ["Shen", "Dong", ""], ["Wang", "Zhongyuan", ""], ["Shi", "Nian", ""], ["Liu", "Honglin", ""]]}, {"id": "2106.06234", "submitter": "Gennaro Vessio Dr.", "authors": "Giovanna Castellano, Gennaro Vessio", "title": "A deep learning approach to clustering visual arts", "comments": "Submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering artworks is difficult for several reasons. On the one hand,\nrecognizing meaningful patterns based on domain knowledge and visual perception\nis extremely hard. On the other hand, applying traditional clustering and\nfeature reduction techniques to the highly dimensional pixel space can be\nineffective. To address these issues, in this paper we propose DELIUS: a DEep\nlearning approach to cLustering vIsUal artS. The method uses a pre-trained\nconvolutional network to extract features and then feeds these features into a\ndeep embedded clustering model, where the task of mapping the raw input data to\na latent space is jointly optimized with the task of finding a set of cluster\ncentroids in this latent space. Quantitative and qualitative experimental\nresults show the effectiveness of the proposed method. DELIUS can be useful for\nseveral tasks related to art analysis, in particular visual link retrieval and\nhistorical knowledge discovery in painting datasets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 08:35:26 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Castellano", "Giovanna", ""], ["Vessio", "Gennaro", ""]]}, {"id": "2106.06237", "submitter": "Chenhong Zhou", "authors": "Chenhong Zhou, Feng Liu, Chen Gong, Tongliang Liu, Bo Han, William\n  Cheung", "title": "KRADA: Known-region-aware Domain Alignment for Open World Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semantic segmentation, we aim to train a pixel-level classifier to assign\ncategory labels to all pixels in an image, where labeled training images and\nunlabeled test images are from the same distribution and share the same label\nset. However, in an open world, the unlabeled test images probably contain\nunknown categories and have different distributions from the labeled images.\nHence, in this paper, we consider a new, more realistic, and more challenging\nproblem setting where the pixel-level classifier has to be trained with labeled\nimages and unlabeled open-world images -- we name it open world semantic\nsegmentation (OSS). In OSS, the trained classifier is expected to identify\nunknown-class pixels and classify known-class pixels well. To solve OSS, we\nfirst investigate which distribution that unknown-class pixels obey. Then,\nmotivated by the goodness-of-fit test, we use statistical measurements to show\nhow a pixel fits the distribution of an unknown class and select highly-fitted\npixels to form the unknown region in each image. Eventually, we propose an\nend-to-end learning framework, known-region-aware domain alignment (KRADA), to\ndistinguish unknown classes while aligning distributions of known classes in\nlabeled and unlabeled open-world images. The effectiveness of KRADA has been\nverified on two synthetic tasks and one COVID-19 segmentation task.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 08:43:59 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhou", "Chenhong", ""], ["Liu", "Feng", ""], ["Gong", "Chen", ""], ["Liu", "Tongliang", ""], ["Han", "Bo", ""], ["Cheung", "William", ""]]}, {"id": "2106.06250", "submitter": "Mingxiang Chen", "authors": "Mingxiang Chen, Zhanguo Chang, Haonan Lu, Bitao Yang, Zhuang Li,\n  Liufang Guo, Zhecheng Wang", "title": "AugNet: End-to-End Unsupervised Visual Representation Learning with\n  Image Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the achievements in artificial intelligence so far were accomplished\nby supervised learning which requires numerous annotated training data and thus\ncosts innumerable manpower for labeling. Unsupervised learning is one of the\neffective solutions to overcome such difficulties. In our work, we propose\nAugNet, a new deep learning training paradigm to learn image features from a\ncollection of unlabeled pictures. We develop a method to construct the\nsimilarities between pictures as distance metrics in the embedding space by\nleveraging the inter-correlation between augmented versions of samples. Our\nexperiments demonstrate that the method is able to represent the image in low\ndimensional space and performs competitively in downstream tasks such as image\nclassification and image similarity comparison. Specifically, we achieved over\n60% and 27% accuracy on the STL10 and CIFAR100 datasets with unsupervised\nclustering, respectively. Moreover, unlike many deep-learning-based image\nretrieval algorithms, our approach does not require access to external\nannotated datasets to train the feature extractor, but still shows comparable\nor even better feature representation ability and easy-to-use characteristics.\nIn our evaluations, the method outperforms all the state-of-the-art image\nretrieval algorithms on some out-of-domain image datasets. The code for the\nmodel implementation is available at\nhttps://github.com/chenmingxiang110/AugNet.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 09:02:30 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chen", "Mingxiang", ""], ["Chang", "Zhanguo", ""], ["Lu", "Haonan", ""], ["Yang", "Bitao", ""], ["Li", "Zhuang", ""], ["Guo", "Liufang", ""], ["Wang", "Zhecheng", ""]]}, {"id": "2106.06307", "submitter": "Usman Nazir", "authors": "Usman Nazir, He Wang and Murtaza Taj", "title": "Survey of Image Based Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this survey paper, we analyze image based graph neural networks and\npropose a three-step classification approach. We first convert the image into\nsuperpixels using the Quickshift algorithm so as to reduce 30% of the input\ndata. The superpixels are subsequently used to generate a region adjacency\ngraph. Finally, the graph is passed through a state-of-art graph convolutional\nneural network to get classification scores. We also analyze the spatial and\nspectral convolution filtering techniques in graph neural networks.\nSpectral-based models perform better than spatial-based models and classical\nCNN with lesser compute cost.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 10:56:43 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Nazir", "Usman", ""], ["Wang", "He", ""], ["Taj", "Murtaza", ""]]}, {"id": "2106.06313", "submitter": "Lixiang Lin", "authors": "Lixiang Lin and Jianke Zhu", "title": "Bridge the Gap Between Model-based and Model-free Human Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to directly estimate the geometry of human from a single\nimage due to the high diversity and complexity of body shapes with the various\nclothing styles. Most of model-based approaches are limited to predict the\nshape and pose of a minimally clothed body with over-smoothing surface.\nAlthough capturing the fine detailed geometries, the model-free methods are\nlack of the fixed mesh topology. To address these issues, we propose a novel\ntopology-preserved human reconstruction approach by bridging the gap between\nmodel-based and model-free human reconstruction. We present an end-to-end\nneural network that simultaneously predicts the pixel-aligned implicit surface\nand the explicit mesh model built by graph convolutional neural network.\nMoreover, an extra graph convolutional neural network is employed to estimate\nthe vertex offsets between the implicit surface and parametric mesh model.\nFinally, we suggest an efficient implicit registration method to refine the\nneural network output in implicit space. Experiments on DeepHuman dataset\nshowed that our approach is effective.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 11:13:42 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Lin", "Lixiang", ""], ["Zhu", "Jianke", ""]]}, {"id": "2106.06321", "submitter": "Tejas Bana", "authors": "Tejas Bana, Jatan Loya and Siddhant Kulkarni", "title": "ViT-Inception-GAN for Image Colourising", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Studies involving colourising images has been garnering researchers' keen\nattention over time, assisted by significant advances in various Machine\nLearning techniques and compute power availability. Traditionally, colourising\nimages have been an intricate task that gave a substantial degree of freedom\nduring the assignment of chromatic information. In our proposed method, we\nattempt to colourise images using Vision Transformer - Inception - Generative\nAdversarial Network (ViT-I-GAN), which has an Inception-v3 fusion embedding in\nthe generator. For a stable and robust network, we have used Vision Transformer\n(ViT) as the discriminator. We trained the model on the Unsplash and the COCO\ndataset for demonstrating the improvement made by the Inception-v3 embedding.\nWe have compared the results between ViT-GANs with and without Inception-v3\nembedding.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 11:41:14 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Bana", "Tejas", ""], ["Loya", "Jatan", ""], ["Kulkarni", "Siddhant", ""]]}, {"id": "2106.06340", "submitter": "Xuanhong Chen", "authors": "Renwang Chen, Xuanhong Chen, Bingbing Ni, Yanhao Ge", "title": "SimSwap: An Efficient Framework For High Fidelity Face Swapping", "comments": "Accepted by ACMMM 2020", "journal-ref": "Proceedings of the 28th ACM International Conference on\n  Multimedia. 2020", "doi": "10.1145/3394171.3413630", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an efficient framework, called Simple Swap (SimSwap), aiming for\ngeneralized and high fidelity face swapping. In contrast to previous approaches\nthat either lack the ability to generalize to arbitrary identity or fail to\npreserve attributes like facial expression and gaze direction, our framework is\ncapable of transferring the identity of an arbitrary source face into an\narbitrary target face while preserving the attributes of the target face. We\novercome the above defects in the following two ways. First, we present the ID\nInjection Module (IIM) which transfers the identity information of the source\nface into the target face at feature level. By using this module, we extend the\narchitecture of an identity-specific face swapping algorithm to a framework for\narbitrary face swapping. Second, we propose the Weak Feature Matching Loss\nwhich efficiently helps our framework to preserve the facial attributes in an\nimplicit way. Extensive experiments on wild faces demonstrate that our SimSwap\nis able to achieve competitive identity performance while preserving attributes\nbetter than previous state-of-the-art methods. The code is already available on\ngithub: https://github.com/neuralchen/SimSwap.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 12:23:10 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chen", "Renwang", ""], ["Chen", "Xuanhong", ""], ["Ni", "Bingbing", ""], ["Ge", "Yanhao", ""]]}, {"id": "2106.06351", "submitter": "Daan de Geus", "authors": "Daan de Geus, Panagiotis Meletis, Chenyang Lu, Xiaoxiao Wen, Gijs\n  Dubbelman", "title": "Part-aware Panoptic Segmentation", "comments": "CVPR 2021. Code and data: https://github.com/tue-mps/panoptic_parts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce the new scene understanding task of Part-aware\nPanoptic Segmentation (PPS), which aims to understand a scene at multiple\nlevels of abstraction, and unifies the tasks of scene parsing and part parsing.\nFor this novel task, we provide consistent annotations on two commonly used\ndatasets: Cityscapes and Pascal VOC. Moreover, we present a single metric to\nevaluate PPS, called Part-aware Panoptic Quality (PartPQ). For this new task,\nusing the metric and annotations, we set multiple baselines by merging results\nof existing state-of-the-art methods for panoptic segmentation and part\nsegmentation. Finally, we conduct several experiments that evaluate the\nimportance of the different levels of abstraction in this single task.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 12:48:07 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["de Geus", "Daan", ""], ["Meletis", "Panagiotis", ""], ["Lu", "Chenyang", ""], ["Wen", "Xiaoxiao", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "2106.06360", "submitter": "Feihong Shen", "authors": "Feihong Shen and Jun Liu and Ping Hu", "title": "Conterfactual Generative Zero-Shot Semantic Segmentation", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  zero-shot learning is an essential part of computer vision. As a classical\ndownstream task, zero-shot semantic segmentation has been studied because of\nits applicant value. One of the popular zero-shot semantic segmentation methods\nis based on the generative model Most new proposed works added structures on\nthe same architecture to enhance this model. However, we found that, from the\nview of causal inference, the result of the original model has been influenced\nby spurious statistical relationships. Thus the performance of the prediction\nshows severe bias. In this work, we consider counterfactual methods to avoid\nthe confounder in the original model. Based on this method, we proposed a new\nframework for zero-shot semantic segmentation. Our model is compared with\nbaseline models on two real-world datasets, Pascal-VOC and Pascal-Context. The\nexperiment results show proposed models can surpass previous confounded models\nand can still make use of additional structures to improve the performance. We\nalso design a simple structure based on Graph Convolutional Networks (GCN) in\nthis work.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:01:03 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Shen", "Feihong", ""], ["Liu", "Jun", ""], ["Hu", "Ping", ""]]}, {"id": "2106.06403", "submitter": "Christiane Plociennik", "authors": "Hooman Tavakoli, Snehal Walunj, Parsha Pahlevannejad, Christiane\n  Plociennik, and Martin Ruskowski", "title": "Small Object Detection for Near Real-Time Egocentric Perception in a\n  Manual Assembly Scenario", "comments": "Accepted for presentation at EPIC@CVPR2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting small objects in video streams of head-worn augmented reality\ndevices in near real-time is a huge challenge: training data is typically\nscarce, the input video stream can be of limited quality, and small objects are\nnotoriously hard to detect. In industrial scenarios, however, it is often\npossible to leverage contextual knowledge for the detection of small objects.\nFurthermore, CAD data of objects are typically available and can be used to\ngenerate synthetic training data. We describe a near real-time small object\ndetection pipeline for egocentric perception in a manual assembly scenario: We\ngenerate a training data set based on CAD data and realistic backgrounds in\nUnity. We then train a YOLOv4 model for a two-stage detection process: First,\nthe context is recognized, then the small object of interest is detected. We\nevaluate our pipeline on the augmented reality device Microsoft Hololens 2.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:59:44 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Tavakoli", "Hooman", ""], ["Walunj", "Snehal", ""], ["Pahlevannejad", "Parsha", ""], ["Plociennik", "Christiane", ""], ["Ruskowski", "Martin", ""]]}, {"id": "2106.06415", "submitter": "Stefan H\\\"ormann", "authors": "Stefan H\\\"ormann and Zeyuan Zhang and Martin Knoche and Torben Teepe\n  and Gerhard Rigoll", "title": "Attention-based Partial Face Recognition", "comments": "To be published in IEEE ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photos of faces captured in unconstrained environments, such as large crowds,\nstill constitute challenges for current face recognition approaches as often\nfaces are occluded by objects or people in the foreground. However, few studies\nhave addressed the task of recognizing partial faces. In this paper, we propose\na novel approach to partial face recognition capable of recognizing faces with\ndifferent occluded areas. We achieve this by combining attentional pooling of a\nResNet's intermediate feature maps with a separate aggregation module. We\nfurther adapt common losses to partial faces in order to ensure that the\nattention maps are diverse and handle occluded parts. Our thorough analysis\ndemonstrates that we outperform all baselines under multiple benchmark\nprotocols, including naturally and synthetically occluded partial faces. This\nsuggests that our method successfully focuses on the relevant parts of the\noccluded face.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:16:06 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:26:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["H\u00f6rmann", "Stefan", ""], ["Zhang", "Zeyuan", ""], ["Knoche", "Martin", ""], ["Teepe", "Torben", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "2106.06418", "submitter": "Tony Lindeberg", "authors": "Ylva Jansson and Tony Lindeberg", "title": "Scale-invariant scale-channel networks: Deep networks that generalise to\n  previously unseen scales", "comments": "29 pages, 14 figures, 6 tables. arXiv admin note: substantial text\n  overlap with arXiv:2004.01536", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to handle large scale variations is crucial for many real world\nvisual tasks. A straightforward approach for handling scale in a deep network\nis to process an image at several scales simultaneously in a set of scale\nchannels. Scale invariance can then, in principle, be achieved by using weight\nsharing between the scale channels together with max or average pooling over\nthe outputs from the scale channels. The ability of such scale channel networks\nto generalise to scales not present in the training set over significant scale\nranges has, however, not previously been explored.\n  In this paper, we present a systematic study of this methodology by\nimplementing different types of scale channel networks and evaluating their\nability to generalise to previously unseen scales. We develop a formalism for\nanalysing the covariance and invariance properties of scale channel networks,\nand explore how different design choices, unique to scaling transformations,\naffect the overall performance of scale channel networks. We first show that\ntwo previously proposed scale channel network designs do not generalise well to\nscales not present in the training set. We explain theoretically and\ndemonstrate experimentally why generalisation fails in these cases.\n  We then propose a new type of foveated scale channel architecture}, where the\nscale channels process increasingly larger parts of the image with decreasing\nresolution. This new type of scale channel network is shown to generalise\nextremely well, provided sufficient image resolution and the absence of\nboundary effects. Our proposed FovMax and FovAvg networks perform almost\nidentically over a scale range of 8, also when training on single scale\ntraining data, and do also give improved performance when learning from\ndatasets with large scale variations in the small sample regime.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:22:26 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Jansson", "Ylva", ""], ["Lindeberg", "Tony", ""]]}, {"id": "2106.06420", "submitter": "Davood Zabihzadeh", "authors": "Karrar Al-Kaabi, Reza Monsefi, Davood Zabihzadeh", "title": "A Framework to Enhance Generalization of Deep Metric Learning methods\n  using General Discriminative Feature Learning and Class Adversarial Neural\n  Networks", "comments": "Includes: 31 Pages, 5 Tables, 15 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning algorithms aim to learn a distance function that brings the\nsemantically similar data items together and keeps dissimilar ones at a\ndistance. The traditional Mahalanobis distance learning is equivalent to find a\nlinear projection. In contrast, Deep Metric Learning (DML) methods are proposed\nthat automatically extract features from data and learn a non-linear\ntransformation from input space to a semantically embedding space. Recently,\nmany DML methods are proposed focused to enhance the discrimination power of\nthe learned metric by providing novel sampling strategies or loss functions.\nThis approach is very helpful when both the training and test examples are\ncoming from the same set of categories. However, it is less effective in many\napplications of DML such as image retrieval and person-reidentification. Here,\nthe DML should learn general semantic concepts from observed classes and employ\nthem to rank or identify objects from unseen categories. Neglecting the\ngeneralization ability of the learned representation and just emphasizing to\nlearn a more discriminative embedding on the observed classes may lead to the\noverfitting problem. To address this limitation, we propose a framework to\nenhance the generalization power of existing DML methods in a Zero-Shot\nLearning (ZSL) setting by general yet discriminative representation learning\nand employing a class adversarial neural network. To learn a more general\nrepresentation, we propose to employ feature maps of intermediate layers in a\ndeep neural network and enhance their discrimination power through an attention\nmechanism. Besides, a class adversarial network is utilized to enforce the deep\nmodel to seek class invariant features for the DML task. We evaluate our work\non widely used machine vision datasets in a ZSL setting.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:24:40 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Al-Kaabi", "Karrar", ""], ["Monsefi", "Reza", ""], ["Zabihzadeh", "Davood", ""]]}, {"id": "2106.06439", "submitter": "Divakar Singh Mr", "authors": "Divakar Singh", "title": "An Image Forensic Technique Based on JPEG Ghosts", "comments": "8 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented growth in the easy availability of photo-editing tools has\nendangered the power of digital images.An image was supposed to be worth more\nthan a thousand words,but now this can be said only if it can be authenticated\northe integrity of the image can be proved to be intact. In thispaper, we\npropose a digital image forensic technique for JPEG images. It can detect any\nforgery in the image if the forged portion called a ghost image is having a\ncompression quality different from that of the cover image. It is based on\nresaving the JPEG image at different JPEG qualities, and the detection of the\nforged portion is maximum when it is saved at the same JPEG quality as the\ncover image. Also, we can precisely predictthe JPEG quality of the cover image\nby analyzing the similarity using Structural Similarity Index Measure (SSIM) or\nthe energyof the images. The first maxima in SSIM or the first minima inenergy\ncorrespond to the cover image JPEG quality. We created adataset for varying\nJPEG compression qualities of the ghost and the cover images and validated the\nscalability of the experimental results.We also, experimented with varied\nattack scenarios, e.g. high-quality ghost image embedded in low quality of\ncover image,low-quality ghost image embedded in high-quality of cover image,and\nghost image and cover image both at the same quality.The proposed method is\nable to localize the tampered portions accurately even for forgeries as small\nas 10x10 sized pixel blocks.Our technique is also robust against other attack\nscenarios like copy-move forgery, inserting text into image, rescaling\n(zoom-out/zoom-in) ghost image and then pasting on cover image.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:52:43 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 02:42:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Singh", "Divakar", ""]]}, {"id": "2106.06440", "submitter": "Mateusz Michalkiewicz", "authors": "Mateusz Michalkiewicz, Stavros Tsogkas, Sarah Parisot, Mahsa\n  Baktashmotlagh, Anders Eriksson, Eugene Belilovsky", "title": "Learning Compositional Shape Priors for Few-Shot 3D Reconstruction", "comments": "13 pages, 12 figures. arXiv admin note: substantial text overlap with\n  arXiv:2004.06302", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impressive performance of deep convolutional neural networks in\nsingle-view 3D reconstruction suggests that these models perform non-trivial\nreasoning about the 3D structure of the output space. Recent work has\nchallenged this belief, showing that, on standard benchmarks, complex\nencoder-decoder architectures perform similarly to nearest-neighbor baselines\nor simple linear decoder models that exploit large amounts of per-category\ndata. However, building large collections of 3D shapes for supervised training\nis a laborious process; a more realistic and less constraining task is\ninferring 3D shapes for categories with few available training examples,\ncalling for a model that can successfully generalize to novel object classes.\nIn this work we experimentally demonstrate that naive baselines fail in this\nfew-shot learning setting, in which the network must learn informative shape\npriors for inference of new categories. We propose three ways to learn a\nclass-specific global shape prior, directly from data. Using these techniques,\nwe are able to capture multi-scale information about the 3D shape, and account\nfor intra-class variability by virtue of an implicit compositional structure.\nExperiments on the popular ShapeNet dataset show that our method outperforms a\nzero-shot baseline by over 40%, and the current state-of-the-art by over 10%,\nin terms of relative performance, in the few-shot setting.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:55:49 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 11:18:32 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Michalkiewicz", "Mateusz", ""], ["Tsogkas", "Stavros", ""], ["Parisot", "Sarah", ""], ["Baktashmotlagh", "Mahsa", ""], ["Eriksson", "Anders", ""], ["Belilovsky", "Eugene", ""]]}, {"id": "2106.06442", "submitter": "Shan You", "authors": "Xiu Su, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, Changshui Zhang,\n  Chang Xu", "title": "K-shot NAS: Learnable Weight-Sharing for NAS with K-shot Supernets", "comments": "Accepted by ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In one-shot weight sharing for NAS, the weights of each operation (at each\nlayer) are supposed to be identical for all architectures (paths) in the\nsupernet. However, this rules out the possibility of adjusting operation\nweights to cater for different paths, which limits the reliability of the\nevaluation results. In this paper, instead of counting on a single supernet, we\nintroduce $K$-shot supernets and take their weights for each operation as a\ndictionary. The operation weight for each path is represented as a convex\ncombination of items in a dictionary with a simplex code. This enables a matrix\napproximation of the stand-alone weight matrix with a higher rank ($K>1$). A\n\\textit{simplex-net} is introduced to produce architecture-customized code for\neach path. As a result, all paths can adaptively learn how to share weights in\nthe $K$-shot supernets and acquire corresponding weights for better evaluation.\n$K$-shot supernets and simplex-net can be iteratively trained, and we further\nextend the search to the channel dimension. Extensive experiments on benchmark\ndatasets validate that K-shot NAS significantly improves the evaluation\naccuracy of paths and thus brings in impressive performance improvements.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:57:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Su", "Xiu", ""], ["You", "Shan", ""], ["Zheng", "Mingkai", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Zhang", "Changshui", ""], ["Xu", "Chang", ""]]}, {"id": "2106.06471", "submitter": "Xingyi Yang", "authors": "Xingyi Yang, Muchao Ye, Quanzeng You, Fenglong Ma", "title": "Writing by Memorizing: Hierarchical Retrieval-based Medical Report\n  Generation", "comments": "Accepted by ACL 2021, Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical report generation is one of the most challenging tasks in medical\nimage analysis. Although existing approaches have achieved promising results,\nthey either require a predefined template database in order to retrieve\nsentences or ignore the hierarchical nature of medical report generation. To\naddress these issues, we propose MedWriter that incorporates a novel\nhierarchical retrieval mechanism to automatically extract both report and\nsentence-level templates for clinically accurate report generation. MedWriter\nfirst employs the Visual-Language Retrieval~(VLR) module to retrieve the most\nrelevant reports for the given images. To guarantee the logical coherence\nbetween sentences, the Language-Language Retrieval~(LLR) module is introduced\nto retrieve relevant sentences based on the previous generated description. At\nlast, a language decoder fuses image features and features from retrieved\nreports and sentences to generate meaningful medical reports. We verified the\neffectiveness of our model by automatic evaluation and human evaluation on two\ndatasets, i.e., Open-I and MIMIC-CXR.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:47:23 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Yang", "Xingyi", ""], ["Ye", "Muchao", ""], ["You", "Quanzeng", ""], ["Ma", "Fenglong", ""]]}, {"id": "2106.06482", "submitter": "Emre Can Kaya", "authors": "Emre Can Kaya, Ioan Tabus", "title": "Neural Network Modeling of Probabilities for Coding the Octree\n  Representation of Point Clouds", "comments": "6 pages, 3 figures, Submitted to MMSP 2021, typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel lossless point cloud compression algorithm that\nuses a neural network for estimating the coding probabilities for the occupancy\nstatus of voxels, depending on wide three dimensional contexts around the voxel\nto be encoded. The point cloud is represented as an octree, with each\nresolution layer being sequentially encoded and decoded using arithmetic\ncoding, starting from the lowest resolution, until the final resolution is\nreached. The occupancy probability of each voxel of the splitting pattern at\neach node of the octree is modeled by a neural network, having at its input the\nalready encoded occupancy status of several octree nodes (belonging to the past\nand current resolutions), corresponding to a 3D context surrounding the node to\nbe encoded. The algorithm has a fast and a slow version, the fast version\nselecting differently several voxels of the context, which allows an increased\nparallelization by sending larger batches of templates to be estimated by the\nneural network, at both encoder and decoder. The proposed algorithms yield\nstate-of-the-art results on benchmark datasets. The implementation will be made\navailable at https://github.com/marmus12/nnctx\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 16:07:46 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 13:28:20 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 21:41:33 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kaya", "Emre Can", ""], ["Tabus", "Ioan", ""]]}, {"id": "2106.06485", "submitter": "Weichen Chen", "authors": "Weichen Chen (1) Xinyi Yu (1) Linlin Ou (1) ((1) Collage of\n  Information Engineering, Zhejiang University of Technology, Hangzhou, China)", "title": "Pedestrian Attribute Recognition in Video Surveillance Scenarios Based\n  on View-attribute Attention Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian attribute recognition in surveillance scenarios is still a\nchallenging task due to inaccurate localization of specific attributes. In this\npaper, we propose a novel view-attribute localization method based on attention\n(VALA), which relies on the strong relevance between attributes and views to\ncapture specific view-attributes and to localize attribute-corresponding areas\nby attention mechanism. A specific view-attribute is composed by the extracted\nattribute feature and four view scores which are predicted by view predictor as\nthe confidences for attribute from different views. View-attribute is then\ndelivered back to shallow network layers for supervising deep feature\nextraction. To explore the location of a view-attribute, regional attention is\nintroduced to aggregate spatial information of the input attribute feature in\nheight and width direction for constraining the image into a narrow range.\nMoreover, the inter-channel dependency of view-feature is embedded in the above\ntwo spatial directions. An attention attribute-specific region is gained after\nfining the narrow range by balancing the ratio of channel dependencies between\nheight and width branches. The final view-attribute recognition outcome is\nobtained by combining the output of regional attention with the view scores\nfrom view predictor. Experiments on three wide datasets (RAP, RAPv2, PETA, and\nPA-100K) demonstrate the effectiveness of our approach compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 16:09:31 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chen", "Weichen", ""], ["Yu", "Xinyi", ""], ["Ou", "Linlin", ""]]}, {"id": "2106.06489", "submitter": "John See", "authors": "Gen-Bing Liong, John See, Lai-Kuan Wong", "title": "Shallow Optical Flow Three-Stream CNN for Macro- and Micro-Expression\n  Spotting from Long Videos", "comments": "Accepted for publication in ICIP2021. 9 pages, including 3 pages of\n  supplemental notes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Facial expressions vary from the visible to the subtle. In recent years, the\nanalysis of micro-expressions $-$ a natural occurrence resulting from the\nsuppression of one's true emotions, has drawn the attention of researchers with\na broad range of potential applications. However, spotting microexpressions in\nlong videos becomes increasingly challenging when intertwined with normal or\nmacro-expressions. In this paper, we propose a shallow optical flow\nthree-stream CNN (SOFTNet) model to predict a score that captures the\nlikelihood of a frame being in an expression interval. By fashioning the\nspotting task as a regression problem, we introduce pseudo-labeling to\nfacilitate the learning process. We demonstrate the efficacy and efficiency of\nthe proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art\nperformance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM\nLong Videos.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 16:19:48 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Liong", "Gen-Bing", ""], ["See", "John", ""], ["Wong", "Lai-Kuan", ""]]}, {"id": "2106.06505", "submitter": "Rafael Gallardo-Garc\\'ia", "authors": "R. Gallardo Garc\\'ia and S. Jarqu\\'in Rodr\\'iguez and B. Beltr\\'an\n  Mart\\'inez and C. Hern\\'andez Gracidas and R. Mart\\'inez Torres", "title": "Efficient Deep Learning Architectures for Fast Identification of\n  Bacterial Strains in Resource-Constrained Devices", "comments": "22 pages, 2 figures, 5 tables. Submitted to Multimedia Tools and\n  Applications, issue 1218 - Engineering Tools and Applications in Medical\n  Imaging (currently in reviewing process)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents twelve fine-tuned deep learning architectures to solve the\nbacterial classification problem over the Digital Image of Bacterial Species\nDataset. The base architectures were mainly published as mobile or efficient\nsolutions to the ImageNet challenge, and all experiments presented in this work\nconsisted of making several modifications to the original designs, in order to\nmake them able to solve the bacterial classification problem by using\nfine-tuning and transfer learning techniques. This work also proposes a novel\ndata augmentation technique for this dataset, which is based on the idea of\nartificial zooming, strongly increasing the performance of every tested\narchitecture, even doubling it in some cases. In order to get robust and\ncomplete evaluations, all experiments were performed with 10-fold\ncross-validation and evaluated with five different metrics: top-1 and top-5\naccuracy, precision, recall, and F1 score. This paper presents a complete\ncomparison of the twelve different architectures, cross-validated with the\noriginal and the augmented version of the dataset, the results are also\ncompared with several literature methods. Overall, eight of the eleven\narchitectures surpassed the 0.95 scores in top-1 accuracy with our data\naugmentation method, being 0.9738 the highest top-1 accuracy. The impact of the\ndata augmentation technique is reported with relative improvement scores.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 16:59:22 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Garc\u00eda", "R. Gallardo", ""], ["Rodr\u00edguez", "S. Jarqu\u00edn", ""], ["Mart\u00ednez", "B. Beltr\u00e1n", ""], ["Gracidas", "C. Hern\u00e1ndez", ""], ["Torres", "R. Mart\u00ednez", ""]]}, {"id": "2106.06509", "submitter": "Haoran Wang", "authors": "Zhong Ji, Kexin Chen, Haoran Wang", "title": "Step-Wise Hierarchical Alignment Network for Image-Text Matching", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-text matching plays a central role in bridging the semantic gap between\nvision and language. The key point to achieve precise visual-semantic alignment\nlies in capturing the fine-grained cross-modal correspondence between image and\ntext. Most previous methods rely on single-step reasoning to discover the\nvisual-semantic interactions, which lacks the ability of exploiting the\nmulti-level information to locate the hierarchical fine-grained relevance.\nDifferent from them, in this work, we propose a step-wise hierarchical\nalignment network (SHAN) that decomposes image-text matching into multi-step\ncross-modal reasoning process. Specifically, we first achieve local-to-local\nalignment at fragment level, following by performing global-to-local and\nglobal-to-global alignment at context level sequentially. This progressive\nalignment strategy supplies our model with more complementary and sufficient\nsemantic clues to understand the hierarchical correlations between image and\ntext. The experimental results on two benchmark datasets demonstrate the\nsuperiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 17:05:56 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Ji", "Zhong", ""], ["Chen", "Kexin", ""], ["Wang", "Haoran", ""]]}, {"id": "2106.06523", "submitter": "Robert Citron", "authors": "Robert I. Citron, Peter Jenniskens, Christopher Watkins, Sravanthi\n  Sinha, Amar Shah, Chedy Raissi, Hadrien Devillepoix, Jim Albers", "title": "Recovery of Meteorites Using an Autonomous Drone and Machine Learning", "comments": "16 pages, 9 Figures", "journal-ref": "Meteoritics & Planetary Science (2021)", "doi": "10.1111/maps.13663", "report-no": null, "categories": "astro-ph.EP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recovery of freshly fallen meteorites from tracked and triangulated\nmeteors is critical to determining their source asteroid families. However,\nlocating meteorite fragments in strewn fields remains a challenge with very few\nmeteorites being recovered from the meteors triangulated in past and ongoing\nmeteor camera networks. We examined if locating meteorites can be automated\nusing machine learning and an autonomous drone. Drones can be programmed to fly\na grid search pattern and take systematic pictures of the ground over a large\nsurvey area. Those images can be analyzed using a machine learning classifier\nto identify meteorites in the field among many other features. Here, we\ndescribe a proof-of-concept meteorite classifier that deploys off-line a\ncombination of different convolution neural networks to recognize meteorites\nfrom images taken by drones in the field. The system was implemented in a\nconceptual drone setup and tested in the suspected strewn field of a recent\nmeteorite fall near Walker Lake, Nevada.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 17:36:33 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Citron", "Robert I.", ""], ["Jenniskens", "Peter", ""], ["Watkins", "Christopher", ""], ["Sinha", "Sravanthi", ""], ["Shah", "Amar", ""], ["Raissi", "Chedy", ""], ["Devillepoix", "Hadrien", ""], ["Albers", "Jim", ""]]}, {"id": "2106.06533", "submitter": "Anand Bhattad", "authors": "Anand Bhattad, Aysegul Dundar, Guilin Liu, Andrew Tao, Bryan Catanzaro", "title": "View Generalization for Single Image Textured 3D Models", "comments": "CVPR 2021. Project website:\n  https://nv-adlr.github.io/view-generalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can easily infer the underlying 3D geometry and texture of an object\nonly from a single 2D image. Current computer vision methods can do this, too,\nbut suffer from view generalization problems - the models inferred tend to make\npoor predictions of appearance in novel views. As for generalization problems\nin machine learning, the difficulty is balancing single-view accuracy (cf.\ntraining error; bias) with novel view accuracy (cf. test error; variance). We\ndescribe a class of models whose geometric rigidity is easily controlled to\nmanage this tradeoff. We describe a cycle consistency loss that improves view\ngeneralization (roughly, a model from a generated view should predict the\noriginal view well). View generalization of textures requires that models share\ntexture information, so a car seen from the back still has headlights because\nother cars have headlights. We describe a cycle consistency loss that\nencourages model textures to be aligned, so as to encourage sharing. We compare\nour method against the state-of-the-art method and show both qualitative and\nquantitative improvements.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:59:57 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Bhattad", "Anand", ""], ["Dundar", "Aysegul", ""], ["Liu", "Guilin", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2106.06560", "submitter": "Mingyu Ding", "authors": "Mingyu Ding, Xiaochen Lian, Linjie Yang, Peng Wang, Xiaojie Jin, Zhiwu\n  Lu, Ping Luo", "title": "HR-NAS: Searching Efficient High-Resolution Neural Architectures with\n  Lightweight Transformers", "comments": "Accepted by CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution representations (HR) are essential for dense prediction tasks\nsuch as segmentation, detection, and pose estimation. Learning HR\nrepresentations is typically ignored in previous Neural Architecture Search\n(NAS) methods that focus on image classification. This work proposes a novel\nNAS method, called HR-NAS, which is able to find efficient and accurate\nnetworks for different tasks, by effectively encoding multiscale contextual\ninformation while maintaining high-resolution representations. In HR-NAS, we\nrenovate the NAS search space as well as its searching strategy. To better\nencode multiscale image contexts in the search space of HR-NAS, we first\ncarefully design a lightweight transformer, whose computational complexity can\nbe dynamically changed with respect to different objective functions and\ncomputation budgets. To maintain high-resolution representations of the learned\nnetworks, HR-NAS adopts a multi-branch architecture that provides convolutional\nencoding of multiple feature resolutions, inspired by HRNet. Last, we proposed\nan efficient fine-grained search strategy to train HR-NAS, which effectively\nexplores the search space, and finds optimal architectures given various tasks\nand computation resources. HR-NAS is capable of achieving state-of-the-art\ntrade-offs between performance and FLOPs for three dense prediction tasks and\nan image classification task, given only small computational budgets. For\nexample, HR-NAS surpasses SqueezeNAS that is specially designed for semantic\nsegmentation while improving efficiency by 45.9%. Code is available at\nhttps://github.com/dingmyu/HR-NAS\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 18:11:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ding", "Mingyu", ""], ["Lian", "Xiaochen", ""], ["Yang", "Linjie", ""], ["Wang", "Peng", ""], ["Jin", "Xiaojie", ""], ["Lu", "Zhiwu", ""], ["Luo", "Ping", ""]]}, {"id": "2106.06561", "submitter": "Min Jin Chong", "authors": "Min Jin Chong, David Forsyth", "title": "GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation\n  (works for videos too!)", "comments": "code is here https://github.com/mchong6/GANsNRoses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to learn a map that takes a content code, derived from a face\nimage, and a randomly chosen style code to an anime image. We derive an\nadversarial loss from our simple and effective definitions of style and\ncontent. This adversarial loss guarantees the map is diverse -- a very wide\nrange of anime can be produced from a single content code. Under plausible\nassumptions, the map is not just diverse, but also correctly represents the\nprobability of an anime, conditioned on an input face. In contrast, current\nmultimodal generation procedures cannot capture the complex styles that appear\nin anime. Extensive quantitative experiments support the idea the map is\ncorrect. Extensive qualitative results show that the method can generate a much\nmore diverse range of styles than SOTA comparisons. Finally, we show that our\nformalization of content and style allows us to perform video to video\ntranslation without ever training on videos.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 18:23:00 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chong", "Min Jin", ""], ["Forsyth", "David", ""]]}, {"id": "2106.06579", "submitter": "Yeshwanth Venkatesha", "authors": "Yeshwanth Venkatesha, Youngeun Kim, Leandros Tassiulas, Priyadarshini\n  Panda", "title": "Federated Learning with Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural networks get widespread adoption in resource-constrained embedded\ndevices, there is a growing need for low-power neural systems. Spiking Neural\nNetworks (SNNs)are emerging to be an energy-efficient alternative to the\ntraditional Artificial Neural Networks (ANNs) which are known to be\ncomputationally intensive. From an application perspective, as federated\nlearning involves multiple energy-constrained devices, there is a huge scope to\nleverage energy efficiency provided by SNNs. Despite its importance, there has\nbeen little attention on training SNNs on a large-scale distributed system like\nfederated learning. In this paper, we bring SNNs to a more realistic federated\nlearning scenario. Specifically, we propose a federated learning framework for\ndecentralized and privacy-preserving training of SNNs. To validate the proposed\nfederated learning framework, we experimentally evaluate the advantages of SNNs\non various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks.\nWe observe that SNNs outperform ANNs in terms of overall accuracy by over 15%\nwhen the data is distributed across a large number of clients in the federation\nwhile providing up to5.3x energy efficiency. In addition to efficiency, we also\nanalyze the sensitivity of the proposed federated SNN framework to data\ndistribution among the clients, stragglers, and gradient noise and perform a\ncomprehensive comparison with ANNs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:00:58 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Venkatesha", "Yeshwanth", ""], ["Kim", "Youngeun", ""], ["Tassiulas", "Leandros", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2106.06583", "submitter": "Nathan Vance", "authors": "Jeremy Speth, Nathan Vance, Adam Czajka, Kevin W. Bowyer, Diane\n  Wright, Patrick Flynn", "title": "Deception Detection and Remote Physiological Monitoring: A Dataset and\n  Baseline Experimental Results", "comments": "The dataset will be available for download at\n  https://cvrl.nd.edu/projects/data/#deception-detection-and-physiological-monitoringddpm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Deception Detection and Physiological Monitoring (DDPM)\ndataset and initial baseline results on this dataset. Our application context\nis an interview scenario in which the interviewee attempts to deceive the\ninterviewer on selected responses. The interviewee is recorded in RGB,\nnear-infrared, and long-wave infrared, along with cardiac pulse, blood\noxygenation, and audio. After collection, data were annotated for\ninterviewer/interviewee, curated, ground-truthed, and organized into train /\ntest parts for a set of canonical deception detection experiments. Baseline\nexperiments found random accuracy for micro-expressions as an indicator of\ndeception, but that saccades can give a statistically significant response. We\nalso estimated subject heart rates from face videos (remotely) with a mean\nabsolute error as low as 3.16 bpm. The database contains almost 13 hours of\nrecordings of 70 subjects, and over 8 million visible-light, near-infrared, and\nthermal video frames, along with appropriate meta, audio and pulse oximeter\ndata. To our knowledge, this is the only collection offering recordings of five\nmodalities in an interview scenario that can be used in both deception\ndetection and remote photoplethysmography research.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:08:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Speth", "Jeremy", ""], ["Vance", "Nathan", ""], ["Czajka", "Adam", ""], ["Bowyer", "Kevin W.", ""], ["Wright", "Diane", ""], ["Flynn", "Patrick", ""]]}, {"id": "2106.06592", "submitter": "Ignacio Mu\\~noz", "authors": "Ignacio Mu\\~noz, Alfredo Bolt", "title": "Dise\\~no y desarrollo de aplicaci\\'on m\\'ovil para la clasificaci\\'on de\n  flora nativa chilena utilizando redes neuronales convolucionales", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Introduction: Mobile apps, through artificial vision, are capable of\nrecognizing vegetable species in real time. However, the existing species\nrecognition apps do not take in consideration the wide variety of endemic and\nnative (Chilean) species, which leads to wrong species predictions. This study\nintroduces the development of a chilean species dataset and an optimized\nclassification model implemented to a mobile app. Method: the data set was\nbuilt by putting together pictures of several species captured on the field and\nby selecting some pictures available from other datasets available online.\nConvolutional neural networks were used in order to develop the images\nprediction models. The networks were trained by performing a sensitivity\nanalysis, validating with k-fold cross validation and performing tests with\ndifferent hyper-parameters, optimizers, convolutional layers, and learning\nrates in order to identify and choose the best models and then put them\ntogether in one classification model. Results: The final data set was\ncompounded by 46 species, including native species, endemic and exotic from\nChile, with 6120 training pictures and 655 testing pictures. The best models\nwere implemented on a mobile app, obtaining a 95% correct prediction rate with\nrespect to the set of tests. Conclusion: The app developed in this study is\ncapable of classifying species with a high level of accuracy, depending on the\nstate of the art of the artificial vision and it can also show relevant\ninformation related to the classified species.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:43:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mu\u00f1oz", "Ignacio", ""], ["Bolt", "Alfredo", ""]]}, {"id": "2106.06593", "submitter": "Min Jin Chong", "authors": "Kedan Li, Min jin Chong, Jeffrey Zhang, Jingen Liu", "title": "Toward Accurate and Realistic Outfits Visualization with Attention to\n  Details", "comments": "Accepted to CVPR2021. Live demo here https://revery.ai/demo.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual try-on methods aim to generate images of fashion models wearing\narbitrary combinations of garments. This is a challenging task because the\ngenerated image must appear realistic and accurately display the interaction\nbetween garments. Prior works produce images that are filled with artifacts and\nfail to capture important visual details necessary for commercial applications.\nWe propose Outfit Visualization Net (OVNet) to capture these important details\n(e.g. buttons, shading, textures, realistic hemlines, and interactions between\ngarments) and produce high quality multiple-garment virtual try-on images.\nOVNet consists of 1) a semantic layout generator and 2) an image generation\npipeline using multiple coordinated warps. We train the warper to output\nmultiple warps using a cascade loss, which refines each successive warp to\nfocus on poorly generated regions of a previous warp and yields consistent\nimprovements in detail. In addition, we introduce a method for matching outfits\nwith the most suitable model and produce significant improvements for both our\nand other previous try-on methods. Through quantitative and qualitative\nanalysis, we demonstrate our method generates substantially higher-quality\nstudio images compared to prior works for multi-garment outfits. An interactive\ninterface powered by this method has been deployed on fashion e-commerce\nwebsites and received overwhelmingly positive feedback.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:53:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Li", "Kedan", ""], ["Chong", "Min jin", ""], ["Zhang", "Jeffrey", ""], ["Liu", "Jingen", ""]]}, {"id": "2106.06620", "submitter": "Saeid Asgari Taghanaki", "authors": "Saeid Asgari Taghanaki, Kristy Choi, Amir Khasahmadi, Anirudh Goyal", "title": "Robust Representation Learning via Perceptual Similarity Metrics", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A fundamental challenge in artificial intelligence is learning useful\nrepresentations of data that yield good performance on a downstream task,\nwithout overfitting to spurious input features. Extracting such task-relevant\npredictive information is particularly difficult for real-world datasets. In\nthis work, we propose Contrastive Input Morphing (CIM), a representation\nlearning framework that learns input-space transformations of the data to\nmitigate the effect of irrelevant input features on downstream performance. Our\nmethod leverages a perceptual similarity metric via a triplet loss to ensure\nthat the transformation preserves task-relevant information.Empirically, we\ndemonstrate the efficacy of our approach on tasks which typically suffer from\nthe presence of spurious correlations: classification with nuisance\ninformation, out-of-distribution generalization, and preservation of subgroup\naccuracies. We additionally show that CIM is complementary to other mutual\ninformation-based representation learning techniques, and demonstrate that it\nimproves the performance of variational information bottleneck (VIB) when used\ntogether.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 21:45:44 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Choi", "Kristy", ""], ["Khasahmadi", "Amir", ""], ["Goyal", "Anirudh", ""]]}, {"id": "2106.06623", "submitter": "Shivam Kalra", "authors": "Shivam Kalra, Mohammed Adnan, Sobhan Hemati, Taher Dehkharghanian,\n  Shahryar Rahnamayan, Hamid Tizhoosh", "title": "Pay Attention with Focus: A Novel Learning Scheme for Classification of\n  Whole Slide Images", "comments": "Accepted in MICCAI, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning methods such as convolutional neural networks (CNNs) are\ndifficult to directly utilize to analyze whole slide images (WSIs) due to the\nlarge image dimensions. We overcome this limitation by proposing a novel\ntwo-stage approach. First, we extract a set of representative patches (called\nmosaic) from a WSI. Each patch of a mosaic is encoded to a feature vector using\na deep network. The feature extractor model is fine-tuned using hierarchical\ntarget labels of WSIs, i.e., anatomic site and primary diagnosis. In the second\nstage, a set of encoded patch-level features from a WSI is used to compute the\nprimary diagnosis probability through the proposed Pay Attention with Focus\nscheme, an attention-weighted averaging of predicted probabilities for all\npatches of a mosaic modulated by a trainable focal factor. Experimental results\nshow that the proposed model can be robust, and effective for the\nclassification of WSIs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 21:59:02 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kalra", "Shivam", ""], ["Adnan", "Mohammed", ""], ["Hemati", "Sobhan", ""], ["Dehkharghanian", "Taher", ""], ["Rahnamayan", "Shahryar", ""], ["Tizhoosh", "Hamid", ""]]}, {"id": "2106.06629", "submitter": "Manolis Savva", "authors": "Jiaqi Tan, Weijie Lin, Angel X. Chang, Manolis Savva", "title": "Mirror3D: Depth Refinement for Mirror Surfaces", "comments": "Paper presented at CVPR 2021. For code, data and pretrained models,\n  see https://3dlg-hcvc.github.io/mirror3d/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress in depth sensing and 3D reconstruction, mirror\nsurfaces are a significant source of errors. To address this problem, we create\nthe Mirror3D dataset: a 3D mirror plane dataset based on three RGBD datasets\n(Matterport3D, NYUv2 and ScanNet) containing 7,011 mirror instance masks and 3D\nplanes. We then develop Mirror3DNet: a module that refines raw sensor depth or\nestimated depth to correct errors on mirror surfaces. Our key idea is to\nestimate the 3D mirror plane based on RGB input and surrounding depth context,\nand use this estimate to directly regress mirror surface depth. Our experiments\nshow that Mirror3DNet significantly mitigates errors from a variety of input\ndepth data, including raw sensor depth and depth estimation or completion\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 22:30:12 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tan", "Jiaqi", ""], ["Lin", "Weijie", ""], ["Chang", "Angel X.", ""], ["Savva", "Manolis", ""]]}, {"id": "2106.06637", "submitter": "Xiang Chen", "authors": "Xiang Chen, Yan Xia, Nishant Ravikumar, Alejandro F Frangi", "title": "CAR-Net: Unsupervised Co-Attention Guided Registration Network for Joint\n  Registration and Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is a fundamental building block for various applications\nin medical image analysis. To better explore the correlation between the fixed\nand moving images and improve registration performance, we propose a novel deep\nlearning network, Co-Attention guided Registration Network (CAR-Net). CAR-Net\nemploys a co-attention block to learn a new representation of the inputs, which\ndrives the registration of the fixed and moving images. Experiments on UK\nBiobank cardiac cine-magnetic resonance image data demonstrate that CAR-Net\nobtains higher registration accuracy and smoother deformation fields than\nstate-of-the-art unsupervised registration methods, while achieving comparable\nor better registration performance than corresponding weakly-supervised\nvariants. In addition, our approach can provide critical structural information\nof the input fixed and moving images simultaneously in a completely\nunsupervised manner.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 23:25:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Xiang", ""], ["Xia", "Yan", ""], ["Ravikumar", "Nishant", ""], ["Frangi", "Alejandro F", ""]]}, {"id": "2106.06649", "submitter": "Chuong Nguyen", "authors": "Thuy C. Nguyen, Tuan N. Tang, Nam LH. Phan, Chuong H. Nguyen, Masayuki\n  Yamazaki, Masao Yamanaka", "title": "1st Place Solution for YouTubeVOS Challenge 2021:Video Instance\n  Segmentation", "comments": "Accepted to CPVR 2021 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video Instance Segmentation (VIS) is a multi-task problem performing\ndetection, segmentation, and tracking simultaneously. Extended from image set\napplications, video data additionally induces the temporal information, which,\nif handled appropriately, is very useful to identify and predict object\nmotions. In this work, we design a unified model to mutually learn these tasks.\nSpecifically, we propose two modules, named Temporally Correlated Instance\nSegmentation (TCIS) and Bidirectional Tracking (BiTrack), to take the benefit\nof the temporal correlation between the object's instance masks across adjacent\nframes. On the other hand, video data is often redundant due to the frame's\noverlap. Our analysis shows that this problem is particularly severe for the\nYoutubeVOS-VIS2021 data. Therefore, we propose a Multi-Source Data (MSD)\ntraining mechanism to compensate for the data deficiency. By combining these\ntechniques with a bag of tricks, the network performance is significantly\nboosted compared to the baseline, and outperforms other methods by a\nconsiderable margin on the YoutubeVOS-VIS 2019 and 2021 datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 00:20:38 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 02:29:17 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Nguyen", "Thuy C.", ""], ["Tang", "Tuan N.", ""], ["Phan", "Nam LH.", ""], ["Nguyen", "Chuong H.", ""], ["Yamazaki", "Masayuki", ""], ["Yamanaka", "Masao", ""]]}, {"id": "2106.06650", "submitter": "Van Huy Vo", "authors": "Huy V. Vo, Elena Sizikova, Cordelia Schmid, Patrick P\\'erez, Jean\n  Ponce", "title": "Large-Scale Unsupervised Object Discovery", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing approaches to unsupervised object discovery (UOD) do not scale up to\nlarge datasets without approximations which compromise their performance. We\npropose a novel formulation of UOD as a ranking problem, amenable to the\narsenal of distributed methods available for eigenvalue problems and link\nanalysis. Extensive experiments with COCO and OpenImages demonstrate that, in\nthe single-object discovery setting where a single prominent object is sought\nin each image, the proposed LOD (Large-scale Object Discovery) approach is on\npar with, or better than the state of the art for medium-scale datasets (up to\n120K images), and over 37% better than the only other algorithms capable of\nscaling up to 1.7M images. In the multi-object discovery setting where multiple\nobjects are sought in each image, the proposed LOD is over 14% better in\naverage precision (AP) than all other methods for datasets ranging from 20K to\n1.7M images.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 00:29:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Vo", "Huy V.", ""], ["Sizikova", "Elena", ""], ["Schmid", "Cordelia", ""], ["P\u00e9rez", "Patrick", ""], ["Ponce", "Jean", ""]]}, {"id": "2106.06654", "submitter": "Ivan Evtimov", "authors": "Ivan Evtimov and Ian Covert and Aditya Kusupati and Tadayoshi Kohno", "title": "Disrupting Model Training with Adversarial Shortcuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data is publicly released for human consumption, it is unclear how to\nprevent its unauthorized usage for machine learning purposes. Successful model\ntraining may be preventable with carefully designed dataset modifications, and\nwe present a proof-of-concept approach for the image classification setting. We\npropose methods based on the notion of adversarial shortcuts, which encourage\nmodels to rely on non-robust signals rather than semantic features, and our\nexperiments demonstrate that these measures successfully prevent deep learning\nmodels from achieving high accuracy on real, unmodified data examples.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 01:04:41 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 21:48:44 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Evtimov", "Ivan", ""], ["Covert", "Ian", ""], ["Kusupati", "Aditya", ""], ["Kohno", "Tadayoshi", ""]]}, {"id": "2106.06664", "submitter": "Yanwei Fu", "authors": "Yanwei Fu, Lei Zhao, Haojie Zheng, Qiang Sun, Li Yang, Hong Li, Jiao\n  Xie, Xiangyang Xue, Feng Li, Yuan Li, Wei Wang, Yantao Pei, Jianmin Wang,\n  Xiuqi Wu, Yanhua Zheng, Hongxia Tian Mengwei Gu1", "title": "Rapid COVID-19 Risk Screening by Eye-region Manifestations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is still nontrivial to develop a new fast COVID-19 screening method with\nthe easier access and lower cost, due to the technical and cost limitations of\nthe current testing methods in the medical resource-poor districts. On the\nother hand, there are more and more ocular manifestations that have been\nreported in the COVID-19 patients as growing clinical evidence[1]. This\ninspired this project. We have conducted the joint clinical research since\nJanuary 2021 at the ShiJiaZhuang City, Heibei province, China, which approved\nby the ethics committee of The fifth hospital of ShiJiaZhuang of Hebei Medical\nUniversity. We undertake several blind tests of COVID-19 patients by Union\nHospital, Tongji Medical College, Huazhong University of Science and\nTechnology, Wuhan, China. Meantime as an important part of the ongoing globally\nCOVID-19 eye test program by AIMOMICS since February 2020, we propose a new\nfast screening method of analyzing the eye-region images, captured by common\nCCD and CMOS cameras. This could reliably make a rapid risk screening of\nCOVID-19 with the sustainable stable high performance in different countries\nand races. Our model for COVID-19 rapid prescreening have the merits of the\nlower cost, fully self-performed, non-invasive, importantly real-time, and thus\nenables the continuous health surveillance. We further implement it as the open\naccessible APIs, and provide public service to the world. Our pilot experiments\nshow that our model is ready to be usable to all kinds of surveillance\nscenarios, such as infrared temperature measurement device at airports and\nstations, or directly pushing to the target people groups smartphones as a\npackaged application.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 01:56:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Fu", "Yanwei", ""], ["Zhao", "Lei", ""], ["Zheng", "Haojie", ""], ["Sun", "Qiang", ""], ["Yang", "Li", ""], ["Li", "Hong", ""], ["Xie", "Jiao", ""], ["Xue", "Xiangyang", ""], ["Li", "Feng", ""], ["Li", "Yuan", ""], ["Wang", "Wei", ""], ["Pei", "Yantao", ""], ["Wang", "Jianmin", ""], ["Wu", "Xiuqi", ""], ["Zheng", "Yanhua", ""], ["Gu1", "Hongxia Tian Mengwei", ""]]}, {"id": "2106.06672", "submitter": "Shenao Zhang", "authors": "Shenao Zhang, Li Shen, Zhifeng Li, Wei Liu", "title": "Structure-Regularized Attention for Deformable Object Representation", "comments": "Published at NeurIPS 2020 Workshop on Object Representations for\n  Learning and Reasoning; code is available at\n  https://github.com/shenao-zhang/StRA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing contextual dependencies has proven useful to improve the\nrepresentational power of deep neural networks. Recent approaches that focus on\nmodeling global context, such as self-attention and non-local operation,\nachieve this goal by enabling unconstrained pairwise interactions between\nelements. In this work, we consider learning representations for deformable\nobjects which can benefit from context exploitation by modeling the structural\ndependencies that the data intrinsically possesses. To this end, we provide a\nnovel structure-regularized attention mechanism, which formalizes feature\ninteraction as structural factorization through the use of a pair of\nlight-weight operations. The instantiated building blocks can be directly\nincorporated into modern convolutional neural networks, to boost the\nrepresentational power in an efficient manner. Comprehensive studies on\nmultiple tasks and empirical comparisons with modern attention mechanisms\ndemonstrate the gains brought by our method in terms of both performance and\nmodel complexity. We further investigate its effect on feature representations,\nshowing that our trained models can capture diversified representations\ncharacterizing object parts without resorting to extra supervision.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 03:10:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhang", "Shenao", ""], ["Shen", "Li", ""], ["Li", "Zhifeng", ""], ["Liu", "Wei", ""]]}, {"id": "2106.06684", "submitter": "Joy Mazumder", "authors": "Joy Mazumder, Mohsen Zand, and Michael Greenspan", "title": "Multistream ValidNet: Improving 6D Object Pose Estimation by Automatic\n  Multistream Validation", "comments": "6 pages, 2 figures, 2 tables. To appear in the proceedings of the\n  28th IEEE International Conference on Image Processing (IEEE - ICIP),\n  September 19-22, 2021, Anchorage, Alaska, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel approach to improve the results of pose estimation\nby detecting and distinguishing between the occurrence of True and False\nPositive results. It achieves this by training a binary classifier on the\noutput of an arbitrary pose estimation algorithm, and returns a binary label\nindicating the validity of the result. We demonstrate that our approach\nimproves upon a state-of-the-art pose estimation result on the Sil\\'eane\ndataset, outperforming a variation of the alternative CullNet method by 4.15%\nin average class accuracy and 0.73% in overall accuracy at validation. Applying\nour method can also improve the pose estimation average precision results of\nOp-Net by 6.06% on average.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 04:11:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Mazumder", "Joy", ""], ["Zand", "Mohsen", ""], ["Greenspan", "Michael", ""]]}, {"id": "2106.06685", "submitter": "Francisco Messina", "authors": "Marine Picot, Francisco Messina, Malik Boudiaf, Fabrice Labeau, Ismail\n  Ben Ayed, and Pablo Piantanida", "title": "Adversarial Robustness via Fisher-Rao Regularization", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial robustness has become a topic of growing interest in machine\nlearning since it was observed that neural networks tend to be brittle. We\npropose an information-geometric formulation of adversarial defense and\nintroduce FIRE, a new Fisher-Rao regularization for the categorical\ncross-entropy loss, which is based on the geodesic distance between natural and\nperturbed input features. Based on the information-geometric properties of the\nclass of softmax distributions, we derive an explicit characterization of the\nFisher-Rao Distance (FRD) for the binary and multiclass cases, and draw some\ninteresting properties as well as connections with standard regularization\nmetrics. Furthermore, for a simple linear and Gaussian model, we show that all\nPareto-optimal points in the accuracy-robustness region can be reached by FIRE\nwhile other state-of-the-art methods fail. Empirically, we evaluate the\nperformance of various classifiers trained with the proposed loss on standard\ndatasets, showing up to 2\\% of improvements in terms of robustness while\nreducing the training time by 20\\% over the best-performing methods.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 04:12:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Picot", "Marine", ""], ["Messina", "Francisco", ""], ["Boudiaf", "Malik", ""], ["Labeau", "Fabrice", ""], ["Ayed", "Ismail Ben", ""], ["Piantanida", "Pablo", ""]]}, {"id": "2106.06694", "submitter": "Satoshi Tsutsui", "authors": "Satoshi Tsutsui, David Crandall, Chen Yu", "title": "Reverse-engineer the Distributional Structure of Infant Egocentric Views\n  for Training Generalizable Image Classifiers", "comments": "Accepted to 2021 CVPR Workshop on Egocentric Perception, Interaction\n  and Computing (EPIC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze egocentric views of attended objects from infants. This paper\nshows 1) empirical evidence that children's egocentric views have more diverse\ndistributions compared to adults' views, 2) we can computationally simulate the\ninfants' distribution, and 3) the distribution is beneficial for training more\ngeneralized image classifiers not only for infant egocentric vision but for\nthird-person computer vision.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 06:02:40 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tsutsui", "Satoshi", ""], ["Crandall", "David", ""], ["Yu", "Chen", ""]]}, {"id": "2106.06703", "submitter": "Matthew Gadd", "authors": "Matthew Gadd, Daniele De Martini, Paul Newman", "title": "Unsupervised Place Recognition with Deep Embedding Learning over Radar\n  Videos", "comments": "to be presented at the Workshop on Radar Perception for All-Weather\n  Autonomy at the IEEE International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn, in an unsupervised way, an embedding from sequences of radar images\nthat is suitable for solving place recognition problem using complex radar\ndata. We experiment on 280 km of data and show performance exceeding\nstate-of-the-art supervised approaches, localising correctly 98.38% of the time\nwhen using just the nearest database candidate.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 07:14:15 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gadd", "Matthew", ""], ["De Martini", "Daniele", ""], ["Newman", "Paul", ""]]}, {"id": "2106.06716", "submitter": "Bingzhi Chen", "authors": "Ailiang Lin, Bingzhi Chen, Jiayu Xu, Zheng Zhang, Guangming Lu", "title": "DS-TransUNet:Dual Swin Transformer U-Net for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic medical image segmentation has made great progress benefit from the\ndevelopment of deep learning. However, most existing methods are based on\nconvolutional neural networks (CNNs), which fail to build long-range\ndependencies and global context connections due to the limitation of receptive\nfield in convolution operation. Inspired by the success of Transformer in\nmodeling the long-range contextual information, some researchers have expended\nconsiderable efforts in designing the robust variants of Transformer-based\nU-Net. Moreover, the patch division used in vision transformers usually ignores\nthe pixel-level intrinsic structural features inside each patch. To alleviate\nthese problems, we propose a novel deep medical image segmentation framework\ncalled Dual Swin Transformer U-Net (DS-TransUNet), which might be the first\nattempt to concurrently incorporate the advantages of hierarchical Swin\nTransformer into both encoder and decoder of the standard U-shaped architecture\nto enhance the semantic segmentation quality of varying medical images. Unlike\nmany prior Transformer-based solutions, the proposed DS-TransUNet first adopts\ndual-scale encoder subnetworks based on Swin Transformer to extract the coarse\nand fine-grained feature representations of different semantic scales. As the\ncore component for our DS-TransUNet, a well-designed Transformer Interactive\nFusion (TIF) module is proposed to effectively establish global dependencies\nbetween features of different scales through the self-attention mechanism.\nFurthermore, we also introduce the Swin Transformer block into decoder to\nfurther explore the long-range contextual information during the up-sampling\nprocess. Extensive experiments across four typical tasks for medical image\nsegmentation demonstrate the effectiveness of DS-TransUNet, and show that our\napproach significantly outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 08:37:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lin", "Ailiang", ""], ["Chen", "Bingzhi", ""], ["Xu", "Jiayu", ""], ["Zhang", "Zheng", ""], ["Lu", "Guangming", ""]]}, {"id": "2106.06718", "submitter": "Nicol\\`o Oreste Pinciroli Vago", "authors": "Nicol\\`o Oreste Pinciroli Vago, Ibrahim A. Hameed and Michael\n  Kachelriess", "title": "Using Convolutional Neural Networks for the Helicity Classification of\n  Magnetic Fields", "comments": "14 pages, extended version of a contribution to the proceedings of\n  the 37.th ICRC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.HE cs.AI cs.CV cs.LG hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of non-zero helicity in intergalactic magnetic fields is a\nsmoking gun for their primordial origin since they have to be generated by\nprocesses that break CP invariance. As an experimental signature for the\npresence of helical magnetic fields, an estimator $Q$ based on the triple\nscalar product of the wave-vectors of photons generated in electromagnetic\ncascades from, e.g., TeV blazars, has been suggested previously. We propose to\napply deep learning to helicity classification employing Convolutional Neural\nNetworks and show that this method outperforms the $Q$ estimator.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 08:48:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Vago", "Nicol\u00f2 Oreste Pinciroli", ""], ["Hameed", "Ibrahim A.", ""], ["Kachelriess", "Michael", ""]]}, {"id": "2106.06726", "submitter": "Xuan Cheng", "authors": "Xuan Cheng, Tianshu Xie, Xiaomin Wang, Jiali Deng, Minghui Liu, Ming\n  Liu", "title": "Go Small and Similar: A Simple Output Decay Brings Better Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization and data augmentation methods have been widely used and become\nincreasingly indispensable in deep learning training. Researchers who devote\nthemselves to this have considered various possibilities. But so far, there has\nbeen little discussion about regularizing outputs of the model. This paper\nbegins with empirical observations that better performances are significantly\nassociated with output distributions, that have smaller average values and\nvariances. By audaciously assuming there is causality involved, we propose a\nnovel regularization term, called Output Decay, that enforces the model to\nassign smaller and similar output values on each class. Though being\ncounter-intuitive, such a small modification result in a remarkable improvement\non performance. Extensive experiments demonstrate the wide applicability,\nversatility, and compatibility of Output Decay.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 09:36:06 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cheng", "Xuan", ""], ["Xie", "Tianshu", ""], ["Wang", "Xiaomin", ""], ["Deng", "Jiali", ""], ["Liu", "Minghui", ""], ["Liu", "Ming", ""]]}, {"id": "2106.06733", "submitter": "Yanfei Liu", "authors": "Yi Lin, Yanfei Liu, Jingguang Liu, Guocai Liu, Kai Ma, Yefeng Zheng", "title": "LE-NAS: Learning-based Ensenble with NAS for Dose Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radiation therapy treatment planning is a complex process, as the target dose\nprescription and normal tissue sparing are conflicting objectives. Automated\nand accurate dose prediction for radiation therapy planning is in high demand.\nIn this study, we propose a novel learning-based ensemble approach, named\nLE-NAS, which integrates neural architecture search (NAS) with knowledge\ndistillation for 3D radiotherapy dose prediction. Specifically, the prediction\nnetwork first exhaustively searches each block from enormous architecture\nspace. Then, multiple architectures are selected with promising performance and\ndiversity. To reduce the inference time, we adopt the teacher-student paradigm\nby treating the combination of diverse outputs from multiple searched networks\nas supervisions to guide the student network training. In addition, we apply\nadversarial learning to optimize the student network to recover the knowledge\nin teacher networks. To the best of our knowledge, we are the first to\ninvestigate the combination of NAS and knowledge distillation. The proposed\nmethod has been evaluated on the public OpenKBP dataset, and experimental\nresults demonstrate the effectiveness of our method and its superior\nperformance to the state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 10:08:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lin", "Yi", ""], ["Liu", "Yanfei", ""], ["Liu", "Jingguang", ""], ["Liu", "Guocai", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2106.06736", "submitter": "Mathilde Brousmiche", "authors": "Mathilde Brousmiche and Jean Rouat and St\\'ephane Dupont", "title": "Multi-level Attention Fusion Network for Audio-visual Event Recognition", "comments": "Preprint submitted to the Information Fusion journal in August 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event classification is inherently sequential and multimodal. Therefore, deep\nneural models need to dynamically focus on the most relevant time window and/or\nmodality of a video. In this study, we propose the Multi-level Attention Fusion\nnetwork (MAFnet), an architecture that can dynamically fuse visual and audio\ninformation for event recognition. Inspired by prior studies in neuroscience,\nwe couple both modalities at different levels of visual and audio paths.\nFurthermore, the network dynamically highlights a modality at a given time\nwindow relevant to classify events. Experimental results in AVE (Audio-Visual\nEvent), UCF51, and Kinetics-Sounds datasets show that the approach can\neffectively improve the accuracy in audio-visual event classification. Code is\navailable at: https://github.com/numediart/MAFnet\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 10:24:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Brousmiche", "Mathilde", ""], ["Rouat", "Jean", ""], ["Dupont", "St\u00e9phane", ""]]}, {"id": "2106.06742", "submitter": "Chun-Mei Feng", "authors": "Chun-Mei Feng, Yunlu Yan, Huazhu Fu, Li Chen, and Yong Xu", "title": "Task Transformer Network for Joint MRI Reconstruction and\n  Super-Resolution", "comments": null, "journal-ref": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI2021)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The core problem of Magnetic Resonance Imaging (MRI) is the trade off between\nacceleration and image quality. Image reconstruction and super-resolution are\ntwo crucial techniques in Magnetic Resonance Imaging (MRI). Current methods are\ndesigned to perform these tasks separately, ignoring the correlations between\nthem. In this work, we propose an end-to-end task transformer network\n(T$^2$Net) for joint MRI reconstruction and super-resolution, which allows\nrepresentations and feature transmission to be shared between multiple task to\nachieve higher-quality, super-resolved and motion-artifacts-free images from\nhighly undersampled and degenerated MRI data. Our framework combines both\nreconstruction and super-resolution, divided into two sub-branches, whose\nfeatures are expressed as queries and keys. Specifically, we encourage joint\nfeature learning between the two tasks, thereby transferring accurate task\ninformation. We first use two separate CNN branches to extract task-specific\nfeatures. Then, a task transformer module is designed to embed and synthesize\nthe relevance between the two tasks. Experimental results show that our\nmulti-task model significantly outperforms advanced sequential methods, both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 10:59:46 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 19:39:51 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Feng", "Chun-Mei", ""], ["Yan", "Yunlu", ""], ["Fu", "Huazhu", ""], ["Chen", "Li", ""], ["Xu", "Yong", ""]]}, {"id": "2106.06743", "submitter": "Saber Malekzadeh", "authors": "Hossein Yousefi-Banaem, Saber Malekzadeh", "title": "Hippocampus segmentation in magnetic resonance images of Alzheimer's\n  patients using Deep machine learning", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.17986.91843", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background: Alzheimers disease is a progressive neurodegenerative disorder\nand the main cause of dementia in aging. Hippocampus is prone to changes in the\nearly stages of Alzheimers disease. Detection and observation of the\nhippocampus changes using magnetic resonance imaging (MRI) before the onset of\nAlzheimers disease leads to the faster preventive and therapeutic measures.\nObjective: The aim of this study was the segmentation of the hippocampus in\nmagnetic resonance (MR) images of Alzheimers patients using deep machine\nlearning method. Methods: U-Net architecture of convolutional neural network\nwas proposed to segment the hippocampus in the real MRI data. The MR images of\nthe 100 and 35 patients available in Alzheimers disease Neuroimaging Initiative\n(ADNI) dataset, was used for the train and test of the model, respectively. The\nperformance of the proposed method was compared with manual segmentation by\nmeasuring the similarity metrics. Results: The desired segmentation achieved\nafter 10 iterations. A Dice similarity coefficient (DSC) = 92.3%, sensitivity =\n96.5%, positive predicted value (PPV) = 90.4%, and Intersection over Union\n(IoU) value for the train 92.94 and test 92.93 sets were obtained which are\nacceptable. Conclusion: The proposed approach is promising and can be extended\nin the prognosis of Alzheimers disease by the prediction of the hippocampus\nvolume changes in the early stage of the disease.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 11:00:29 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 06:07:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Yousefi-Banaem", "Hossein", ""], ["Malekzadeh", "Saber", ""]]}, {"id": "2106.06744", "submitter": "Yujiao Wu", "authors": "Yujiao Wu, Jie Ma, Xiaoshui Huang, Sai Ho Ling, and Steven Weidong Su", "title": "DeepMMSA: A Novel Multimodal Deep Learning Method for Non-small Cell\n  Lung Cancer Survival Analysis", "comments": "7 Submitted to IEEE TBME", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Lung cancer is the leading cause of cancer death worldwide. The critical\nreason for the deaths is delayed diagnosis and poor prognosis. With the\naccelerated development of deep learning techniques, it has been successfully\napplied extensively in many real-world applications, including health sectors\nsuch as medical image interpretation and disease diagnosis. By combining more\nmodalities that being engaged in the processing of information, multimodal\nlearning can extract better features and improve predictive ability. The\nconventional methods for lung cancer survival analysis normally utilize\nclinical data and only provide a statistical probability. To improve the\nsurvival prediction accuracy and help prognostic decision-making in clinical\npractice for medical experts, we for the first time propose a multimodal deep\nlearning method for non-small cell lung cancer (NSCLC) survival analysis, named\nDeepMMSA. This method leverages CT images in combination with clinical data,\nenabling the abundant information hold within medical images to be associate\nwith lung cancer survival information. We validate our method on the data of\n422 NSCLC patients from The Cancer Imaging Archive (TCIA). Experimental results\nsupport our hypothesis that there is an underlying relationship between\nprognostic information and radiomic images. Besides, quantitative results\nshowing that the established multimodal model can be applied to traditional\nmethod and has the potential to break bottleneck of existing methods and\nincrease the the percentage of concordant pairs(right predicted pairs) in\noverall population by 4%.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 11:02:14 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wu", "Yujiao", ""], ["Ma", "Jie", ""], ["Huang", "Xiaoshui", ""], ["Ling", "Sai Ho", ""], ["Su", "Steven Weidong", ""]]}, {"id": "2106.06769", "submitter": "Junfu Chen", "authors": "Junfu Chen, Yang Chen, Bi Wang", "title": "Cross-Subject Domain Adaptation for Multi-Frame EEG Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working memory (WM) is a basic part of human cognition, which plays an\nimportant role in the study of human cognitive load. Among various brain\nimaging techniques, electroencephalography has shown its advantage on easy\naccess and reliability. However, one of the critical challenges is that\nindividual difference may cause the ineffective results, especially when the\nestablished model meets an unfamiliar subject. In this work, we propose a\ncross-subject deep adaptation model with spatial attention (CS-DASA) to\ngeneralize the workload classifications across subjects. First, we transform\ntime-series EEG data into multi-frame EEG images incorporating more\nspatio-temporal information. First, the subject-shared module in CS-DASA\nreceives multi-frame EEG image data from both source and target subjects and\nlearns the common feature representations. Then, in subject-specific module,\nthe maximum mean discrepancy is implemented to measure the domain distribution\ndivergence in a reproducing kernel Hilbert space, which can add an effective\npenalty loss for domain adaptation. Additionally, the subject-to-subject\nspatial attention mechanism is employed to focus on the most discriminative\nspatial feature in EEG image data. Experiments conducted on a public WM EEG\ndataset containing 13 subjects show that the proposed model is capable of\nachieve better performance than existing state-of-the art methods.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 13:04:46 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Junfu", ""], ["Chen", "Yang", ""], ["Wang", "Bi", ""]]}, {"id": "2106.06778", "submitter": "Longqing Ye", "authors": "Longqing Ye", "title": "Dynamic Clone Transformer for Efficient Convolutional Neural Netwoks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks (ConvNets) have shown impressive capability to solve\nvarious vision tasks. Nevertheless, the trade-off between performance and\nefficiency is still a challenge for a feasible model deployment on\nresource-constrained platforms. In this paper, we introduce a novel concept\ntermed multi-path fully connected pattern (MPFC) to rethink the\ninterdependencies of topology pattern, accuracy and efficiency for ConvNets.\nInspired by MPFC, we further propose a dual-branch module named dynamic clone\ntransformer (DCT) where one branch generates multiple replicas from inputs and\nanother branch reforms those clones through a series of difference vectors\nconditional on inputs itself to produce more variants. This operation allows\nthe self-expansion of channel-wise information in a data-driven way with little\ncomputational cost while providing sufficient learning capacity, which is a\npotential unit to replace computationally expensive pointwise convolution as an\nexpansion layer in the bottleneck structure.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 13:42:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ye", "Longqing", ""]]}, {"id": "2106.06792", "submitter": "Lingyun Gu", "authors": "Lingyun Gu, Lin Zhang, Zhaokui Wang", "title": "A One-Shot Texture-Perceiving Generative Adversarial Network for\n  Unsupervised Surface Inspection", "comments": "Accepted by ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual surface inspection is a challenging task owing to the highly diverse\nappearance of target surfaces and defective regions. Previous attempts heavily\nrely on vast quantities of training examples with manual annotation. However,\nin some practical cases, it is difficult to obtain a large number of samples\nfor inspection. To combat it, we propose a hierarchical texture-perceiving\ngenerative adversarial network (HTP-GAN) that is learned from the one-shot\nnormal image in an unsupervised scheme. Specifically, the HTP-GAN contains a\npyramid of convolutional GANs that can capture the global structure and\nfine-grained representation of an image simultaneously. This innovation helps\ndistinguishing defective surface regions from normal ones. In addition, in the\ndiscriminator, a texture-perceiving module is devised to capture the spatially\ninvariant representation of normal image via directional convolutions, making\nit more sensitive to defective areas. Experiments on a variety of datasets\nconsistently demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 15:05:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gu", "Lingyun", ""], ["Zhang", "Lin", ""], ["Wang", "Zhaokui", ""]]}, {"id": "2106.06795", "submitter": "Pravendra Singh", "authors": "Mohammed Asad Karim, Vinay Kumar Verma, Pravendra Singh, Vinay\n  Namboodiri, Piyush Rai", "title": "Knowledge Consolidation based Class Incremental Online Learning with\n  Limited Data", "comments": "International Joint Conference on Artificial Intelligence\n  (IJCAI-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach for class incremental online learning in a\nlimited data setting. This problem setting is challenging because of the\nfollowing constraints: (1) Classes are given incrementally, which necessitates\na class incremental learning approach; (2) Data for each class is given in an\nonline fashion, i.e., each training example is seen only once during training;\n(3) Each class has very few training examples; and (4) We do not use or assume\naccess to any replay/memory to store data from previous classes. Therefore, in\nthis setting, we have to handle twofold problems of catastrophic forgetting and\noverfitting. In our approach, we learn robust representations that are\ngeneralizable across tasks without suffering from the problems of catastrophic\nforgetting and overfitting to accommodate future classes with limited samples.\nOur proposed method leverages the meta-learning framework with knowledge\nconsolidation. The meta-learning framework helps the model for rapid learning\nwhen samples appear in an online fashion. Simultaneously, knowledge\nconsolidation helps to learn a robust representation against forgetting under\nonline updates to facilitate future learning. Our approach significantly\noutperforms other methods on several benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 15:18:29 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Karim", "Mohammed Asad", ""], ["Verma", "Vinay Kumar", ""], ["Singh", "Pravendra", ""], ["Namboodiri", "Vinay", ""], ["Rai", "Piyush", ""]]}, {"id": "2106.06801", "submitter": "Prashant Pandey", "authors": "Prashant Pandey, Ajey Pai, Nisarg Bhatt, Prasenjit Das, Govind\n  Makharia, Prathosh AP, Mausam", "title": "Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrastive Learning (CL) is a recent representation learning approach, which\nencourages inter-class separability and intra-class compactness in learned\nimage representations. Since medical images often contain multiple semantic\nclasses in an image, using CL to learn representations of local features (as\nopposed to global) is important. In this work, we present a novel\nsemi-supervised 2D medical segmentation solution that applies CL on image\npatches, instead of full images. These patches are meaningfully constructed\nusing the semantic information of different classes obtained via pseudo\nlabeling. We also propose a novel consistency regularization (CR) scheme, which\nworks in synergy with CL. It addresses the problem of confirmation bias, and\nencourages better clustering in the feature space. We evaluate our method on\nfour public medical segmentation datasets and a novel histopathology dataset\nthat we introduce. Our method obtains consistent improvements over\nstate-of-the-art semi-supervised segmentation approaches for all datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 15:43:24 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 03:45:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Pandey", "Prashant", ""], ["Pai", "Ajey", ""], ["Bhatt", "Nisarg", ""], ["Das", "Prasenjit", ""], ["Makharia", "Govind", ""], ["AP", "Prathosh", ""], ["Mausam", "", ""]]}, {"id": "2106.06804", "submitter": "Pietro Barbiero", "authors": "Pietro Barbiero, Gabriele Ciravegna, Francesco Giannini, Pietro Li\\'o,\n  Marco Gori, Stefano Melacci", "title": "Entropy-based Logic Explanations of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable artificial intelligence has rapidly emerged since lawmakers have\nstarted requiring interpretable models for safety-critical domains.\nConcept-based neural networks have arisen as explainable-by-design methods as\nthey leverage human-understandable symbols (i.e. concepts) to predict class\nmemberships. However, most of these approaches focus on the identification of\nthe most relevant concepts but do not provide concise, formal explanations of\nhow such concepts are leveraged by the classifier to make predictions. In this\npaper, we propose a novel end-to-end differentiable approach enabling the\nextraction of logic explanations from neural networks using the formalism of\nFirst-Order Logic. The method relies on an entropy-based criterion which\nautomatically identifies the most relevant concepts. We consider four different\ncase studies to demonstrate that: (i) this entropy-based criterion enables the\ndistillation of concise logic explanations in safety-critical domains from\nclinical data to computer vision; (ii) the proposed approach outperforms\nstate-of-the-art white-box models in terms of classification accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 15:50:47 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 09:52:47 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 14:06:17 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Barbiero", "Pietro", ""], ["Ciravegna", "Gabriele", ""], ["Giannini", "Francesco", ""], ["Li\u00f3", "Pietro", ""], ["Gori", "Marco", ""], ["Melacci", "Stefano", ""]]}, {"id": "2106.06817", "submitter": "Yize Jin", "authors": "Yize Jin, Anjul Patney, Alan Bovik", "title": "Evaluating Foveated Video Quality Using Entropic Differencing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality is regaining attention due to recent advancements in hardware\ntechnology. Immersive images / videos are becoming widely adopted to carry\nomnidirectional visual information. However, due to the requirements for higher\nspatial and temporal resolution of real video data, immersive videos require\nsignificantly larger bandwidth consumption. To reduce stresses on bandwidth,\nfoveated video compression is regaining popularity, whereby the space-variant\nspatial resolution of the retina is exploited. Towards advancing the progress\nof foveated video compression, we propose a full reference (FR) foveated image\nquality assessment algorithm, which we call foveated entropic differencing\n(FED), which employs the natural scene statistics of bandpass responses by\napplying differences of local entropies weighted by a foveation-based error\nsensitivity function. We evaluate the proposed algorithm by measuring the\ncorrelations of the predictions that FED makes against human judgements on the\nnewly created 2D and 3D LIVE-FBT-FCVR databases for Virtual Reality (VR). The\nperformance of the proposed algorithm yields state-of-the-art as compared with\nother existing full reference algorithms. Software for FED has been made\navailable at: http://live.ece.utexas.edu/research/Quality/FED.zip\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 16:29:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Jin", "Yize", ""], ["Patney", "Anjul", ""], ["Bovik", "Alan", ""]]}, {"id": "2106.06819", "submitter": "Jiaming Song", "authors": "Abhishek Sinha, Jiaming Song, Chenlin Meng, Stefano Ermon", "title": "D2C: Diffusion-Denoising Models for Few-shot Conditional Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional generative models of high-dimensional images have many\napplications, but supervision signals from conditions to images can be\nexpensive to acquire. This paper describes Diffusion-Decoding models with\nContrastive representations (D2C), a paradigm for training unconditional\nvariational autoencoders (VAEs) for few-shot conditional image generation. D2C\nuses a learned diffusion-based prior over the latent representations to improve\ngeneration and contrastive self-supervised learning to improve representation\nquality. D2C can adapt to novel generation tasks conditioned on labels or\nmanipulation constraints, by learning from as few as 100 labeled examples. On\nconditional generation from new labels, D2C achieves superior performance over\nstate-of-the-art VAEs and diffusion models. On conditional image manipulation,\nD2C generations are two orders of magnitude faster to produce over StyleGAN2\nones and are preferred by 50% - 60% of the human evaluators in a double-blind\nstudy.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 16:32:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sinha", "Abhishek", ""], ["Song", "Jiaming", ""], ["Meng", "Chenlin", ""], ["Ermon", "Stefano", ""]]}, {"id": "2106.06847", "submitter": "Jiezhang Cao", "authors": "Jiezhang Cao, Yawei Li, Kai Zhang, Luc Van Gool", "title": "Video Super-Resolution Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR), with the aim to restore a high-resolution video\nfrom its corresponding low-resolution version, is a spatial-temporal sequence\nprediction problem. Recently, Transformer has been gaining popularity due to\nits parallel computing ability for sequence-to-sequence modeling. Thus, it\nseems to be straightforward to apply the vision Transformer to solve VSR.\nHowever, the typical block design of Transformer with a fully connected\nself-attention layer and a token-wise feed-forward layer does not fit well for\nVSR due to the following two reasons. First, the fully connected self-attention\nlayer neglects to exploit the data locality because this layer relies on linear\nlayers to compute attention maps. Second, the token-wise feed-forward layer\nlacks the feature alignment which is important for VSR since this layer\nindependently processes each of the input token embeddings without any\ninteraction among them. In this paper, we make the first attempt to adapt\nTransformer for VSR. Specifically, to tackle the first issue, we present a\nspatial-temporal convolutional self-attention layer with a theoretical\nunderstanding to exploit the locality information. For the second issue, we\ndesign a bidirectional optical flow-based feed-forward layer to discover the\ncorrelations across different video frames and also align features. Extensive\nexperiments on several benchmark datasets demonstrate the effectiveness of our\nproposed method. The code will be available at\nhttps://github.com/caojiezhang/VSR-Transformer.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 20:00:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cao", "Jiezhang", ""], ["Li", "Yawei", ""], ["Zhang", "Kai", ""], ["Van Gool", "Luc", ""]]}, {"id": "2106.06856", "submitter": "Kha Gia Quach", "authors": "Kha Gia Quach, Pha Nguyen, Huu Le, Thanh-Dat Truong, Chi Nhan Duong,\n  Minh-Triet Tran, Khoa Luu", "title": "DyGLIP: A Dynamic Graph Model with Link Prediction for Accurate\n  Multi-Camera Multiple Object Tracking", "comments": "accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Camera Multiple Object Tracking (MC-MOT) is a significant computer\nvision problem due to its emerging applicability in several real-world\napplications. Despite a large number of existing works, solving the data\nassociation problem in any MC-MOT pipeline is arguably one of the most\nchallenging tasks. Developing a robust MC-MOT system, however, is still highly\nchallenging due to many practical issues such as inconsistent lighting\nconditions, varying object movement patterns, or the trajectory occlusions of\nthe objects between the cameras. To address these problems, this work,\ntherefore, proposes a new Dynamic Graph Model with Link Prediction (DyGLIP)\napproach to solve the data association task. Compared to existing methods, our\nnew model offers several advantages, including better feature representations\nand the ability to recover from lost tracks during camera transitions.\nMoreover, our model works gracefully regardless of the overlapping ratios\nbetween the cameras. Experimental results show that we outperform existing\nMC-MOT algorithms by a large margin on several practical datasets. Notably, our\nmodel works favorably on online settings but can be extended to an incremental\napproach for large-scale datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 20:22:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Quach", "Kha Gia", ""], ["Nguyen", "Pha", ""], ["Le", "Huu", ""], ["Truong", "Thanh-Dat", ""], ["Duong", "Chi Nhan", ""], ["Tran", "Minh-Triet", ""], ["Luu", "Khoa", ""]]}, {"id": "2106.06866", "submitter": "Pradyumna Reddy", "authors": "Pradyumna Reddy, Zhifei Zhang, Matthew Fisher, Hailin Jin, Zhaowen\n  Wang, Niloy J. Mitra", "title": "A Multi-Implicit Neural Representation for Fonts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Fonts are ubiquitous across documents and come in a variety of styles. They\nare either represented in a native vector format or rasterized to produce fixed\nresolution images. In the first case, the non-standard representation prevents\nbenefiting from latest network architectures for neural representations; while,\nin the latter case, the rasterized representation, when encoded via networks,\nresults in loss of data fidelity, as font-specific discontinuities like edges\nand corners are difficult to represent using neural networks. Based on the\nobservation that complex fonts can be represented by a superposition of a set\nof simpler occupancy functions, we introduce \\textit{multi-implicits} to\nrepresent fonts as a permutation-invariant set of learned implict functions,\nwithout losing features (e.g., edges and corners). However, while\nmulti-implicits locally preserve font features, obtaining supervision in the\nform of ground truth multi-channel signals is a problem in itself. Instead, we\npropose how to train such a representation with only local supervision, while\nthe proposed neural architecture directly finds globally consistent\nmulti-implicits for font families. We extensively evaluate the proposed\nrepresentation for various tasks including reconstruction, interpolation, and\nsynthesis to demonstrate clear advantages with existing alternatives.\nAdditionally, the representation naturally enables glyph completion, wherein a\nsingle characteristic font is used to synthesize a whole font family in the\ntarget style.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 21:40:11 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Reddy", "Pradyumna", ""], ["Zhang", "Zhifei", ""], ["Fisher", "Matthew", ""], ["Jin", "Hailin", ""], ["Wang", "Zhaowen", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2106.06882", "submitter": "Kyle Vedder", "authors": "Kyle Vedder and Eric Eaton", "title": "Sparse PointPillars: Exploiting Sparsity in Birds-Eye-View Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bird's Eye View (BEV) is a popular representation for processing 3D point\nclouds, and by its nature is fundamentally sparse. Motivated by the\ncomputational limitations of mobile robot platforms, we take a fast\nhigh-performance BEV 3D object detector - PointPillars - and modify its\nbackbone to exploit this sparsity, leading to decreased runtimes. We present\npreliminary results demonstrating decreased runtimes with either the same\nperformance or a modest decrease in performance, which we anticipate will be\nremedied by model specific hyperparameter tuning. Our work is a first step\ntowards a new class of 3D object detectors that exploit sparsity throughout\ntheir entire pipeline in order to reduce runtime and resource usage while\nmaintaining good detection performance.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 23:15:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Vedder", "Kyle", ""], ["Eaton", "Eric", ""]]}, {"id": "2106.06887", "submitter": "Pia Bideau", "authors": "Cheng Gu, Erik Learned-Miller, Daniel Sheldon, Guillermo Gallego, Pia\n  Bideau", "title": "The Spatio-Temporal Poisson Point Process: A Simple Model for the\n  Alignment of Event Camera Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event cameras, inspired by biological vision systems, provide a natural and\ndata efficient representation of visual information. Visual information is\nacquired in the form of events that are triggered by local brightness changes.\nEach pixel location of the camera's sensor records events asynchronously and\nindependently with very high temporal resolution. However, because most\nbrightness changes are triggered by relative motion of the camera and the\nscene, the events recorded at a single sensor location seldom correspond to the\nsame world point. To extract meaningful information from event cameras, it is\nhelpful to register events that were triggered by the same underlying world\npoint. In this work we propose a new model of event data that captures its\nnatural spatio-temporal structure. We start by developing a model for aligned\nevent data. That is, we develop a model for the data as though it has been\nperfectly registered already. In particular, we model the aligned data as a\nspatio-temporal Poisson point process. Based on this model, we develop a\nmaximum likelihood approach to registering events that are not yet aligned.\nThat is, we find transformations of the observed events that make them as\nlikely as possible under our model. In particular we extract the camera\nrotation that leads to the best event alignment. We show new state of the art\naccuracy for rotational velocity estimation on the DAVIS 240C dataset. In\naddition, our method is also faster and has lower computational complexity than\nseveral competing methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 00:43:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gu", "Cheng", ""], ["Learned-Miller", "Erik", ""], ["Sheldon", "Daniel", ""], ["Gallego", "Guillermo", ""], ["Bideau", "Pia", ""]]}, {"id": "2106.06896", "submitter": "Yunhao Gao", "authors": "Yunhao Gao, Wei Li, Mengmeng Zhang, Jianbu Wang, Weiwei Sun, Ran Tao,\n  Qian Du", "title": "Hyperspectral and Multispectral Classification for Coastal Wetland Using\n  Depthwise Feature Interaction Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The monitoring of coastal wetlands is of great importance to the protection\nof marine and terrestrial ecosystems. However, due to the complex environment,\nsevere vegetation mixture, and difficulty of access, it is impossible to\naccurately classify coastal wetlands and identify their species with\ntraditional classifiers. Despite the integration of multisource remote sensing\ndata for performance enhancement, there are still challenges with acquiring and\nexploiting the complementary merits from multisource data. In this paper, the\nDeepwise Feature Interaction Network (DFINet) is proposed for wetland\nclassification. A depthwise cross attention module is designed to extract\nself-correlation and cross-correlation from multisource feature pairs. In this\nway, meaningful complementary information is emphasized for classification.\nDFINet is optimized by coordinating consistency loss, discrimination loss, and\nclassification loss. Accordingly, DFINet reaches the standard solution-space\nunder the regularity of loss functions, while the spatial consistency and\nfeature discrimination are preserved. Comprehensive experimental results on two\nhyperspectral and multispectral wetland datasets demonstrate that the proposed\nDFINet outperforms other competitive methods in terms of overall accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 01:56:28 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 03:48:52 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gao", "Yunhao", ""], ["Li", "Wei", ""], ["Zhang", "Mengmeng", ""], ["Wang", "Jianbu", ""], ["Sun", "Weiwei", ""], ["Tao", "Ran", ""], ["Du", "Qian", ""]]}, {"id": "2106.06908", "submitter": "Chenxin Li", "authors": "Chenxin Li, Qi Qi, Xinghao Ding, Yue Huang, Dong Liang and Yizhou Yu", "title": "Domain Generalization on Medical Imaging Classification using Episodic\n  Training with Task Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical imaging datasets usually exhibit domain shift due to the variations\nof scanner vendors, imaging protocols, etc. This raises the concern about the\ngeneralization capacity of machine learning models. Domain generalization (DG),\nwhich aims to learn a model from multiple source domains such that it can be\ndirectly generalized to unseen test domains, seems particularly promising to\nmedical imaging community. To address DG, recent model-agnostic meta-learning\n(MAML) has been introduced, which transfers the knowledge from previous\ntraining tasks to facilitate the learning of novel testing tasks. However, in\nclinical practice, there are usually only a few annotated source domains\navailable, which decreases the capacity of training task generation and thus\nincreases the risk of overfitting to training tasks in the paradigm. In this\npaper, we propose a novel DG scheme of episodic training with task augmentation\non medical imaging classification. Based on meta-learning, we develop the\nparadigm of episodic training to construct the knowledge transfer from episodic\ntraining-task simulation to the real testing task of DG. Motivated by the\nlimited number of source domains in real-world medical deployment, we consider\nthe unique task-level overfitting and we propose task augmentation to enhance\nthe variety during training task generation to alleviate it. With the\nestablished learning framework, we further exploit a novel meta-objective to\nregularize the deep embedding of training domains. To validate the\neffectiveness of the proposed method, we perform experiments on\nhistopathological images and abdominal CT images.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 03:56:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Li", "Chenxin", ""], ["Qi", "Qi", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Liang", "Dong", ""], ["Yu", "Yizhou", ""]]}, {"id": "2106.06911", "submitter": "Yiqiao Yin", "authors": "Shaw-Hwa Lo, Yiqiao Yin", "title": "An Interaction-based Convolutional Neural Network (ICNN) Towards Better\n  Understanding of COVID-19 X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The field of Explainable Artificial Intelligence (XAI) aims to build\nexplainable and interpretable machine learning (or deep learning) methods\nwithout sacrificing prediction performance. Convolutional Neural Networks\n(CNNs) have been successful in making predictions, especially in image\nclassification. However, these famous deep learning models use tens of millions\nof parameters based on a large number of pre-trained filters which have been\nrepurposed from previous data sets. We propose a novel Interaction-based\nConvolutional Neural Network (ICNN) that does not make assumptions about the\nrelevance of local information. Instead, we use a model-free Influence Score\n(I-score) to directly extract the influential information from images to form\nimportant variable modules. We demonstrate that the proposed method produces\nstate-of-the-art prediction performance of 99.8% on a real-world data set\nclassifying COVID-19 Chest X-ray images without sacrificing the explanatory\npower of the model. This proposed design can efficiently screen COVID-19\npatients before human diagnosis, and will be the benchmark for addressing\nfuture XAI problems in large-scale data sets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 04:41:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lo", "Shaw-Hwa", ""], ["Yin", "Yiqiao", ""]]}, {"id": "2106.06921", "submitter": "Sixing Yu", "authors": "Sixing Yu, Phuong Nguyen, Ali Anwar, Ali Jannesari", "title": "Adaptive Dynamic Pruning for Non-IID Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning~(FL) has emerged as a new paradigm of training machine\nlearning models without sacrificing data security and privacy. Learning models\nat edge devices such as cell phones is one of the most common use case of FL.\nHowever, the limited computing power and energy constraints of edge devices\nhinder the adoption of FL for both model training and deployment, especially\nfor the resource-hungry Deep Neural Networks~(DNNs). To this end, many model\ncompression methods have been proposed and network pruning is among the most\nwell-known. However, a pruning policy for a given model is highly\ndataset-dependent, which is not suitable for non-Independent and Identically\nDistributed~(Non-IID) FL edge devices. In this paper, we present an adaptive\npruning scheme for edge devices in an FL system, which applies dataset-aware\ndynamic pruning for inference acceleration on Non-IID datasets. Our evaluation\nshows that the proposed method accelerates inference by $2\\times$~($50\\%$ FLOPs\nreduction) while maintaining the model's quality on edge devices.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 05:27:43 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yu", "Sixing", ""], ["Nguyen", "Phuong", ""], ["Anwar", "Ali", ""], ["Jannesari", "Ali", ""]]}, {"id": "2106.06924", "submitter": "Ching-Chun Chang", "authors": "Ching-Chun Chang, Xu Wang, Sisheng Chen, Isao Echizen, Victor Sanchez,\n  and Chang-Tsun Li", "title": "Deep Learning for Reversible Steganography: Principles and Insights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning\\textendash{centric} reversible steganography has emerged as a\npromising research paradigm. A direct way of applying deep learning to\nreversible steganography is to construct a pair of encoder and decoder, whose\nparameters are trained jointly, thereby learning the steganographic system as a\nwhole. This end-to-end framework, however, falls short of the reversibility\nrequirement because it is difficult for this kind of monolithic system, as a\nblack box, to create or duplicate intricate reversible mechanisms. In response\nto this issue, a recent approach is to carve up the steganographic system and\nwork on modules independently. In particular, neural networks are deployed in\nan analytics module to learn the data distribution, while an established\nmechanism is called upon to handle the remaining tasks. In this paper, we\ninvestigate the modular framework and deploy deep neural networks in a\nreversible steganographic scheme referred to as prediction-error modulation, in\nwhich an analytics module serves the purpose of pixel intensity prediction. The\nprimary focus of this study is on deep-learning\\textendash{based} context-aware\npixel intensity prediction. We address the unsolved issues reported in related\nliterature, including the impact of pixel initialisation on prediction accuracy\nand the influence of uncertainty propagation in dual-layer embedding.\nFurthermore, we establish a connection between context-aware pixel intensity\nprediction and low-level computer vision and analyse the performance of several\nadvanced neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 05:32:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chang", "Ching-Chun", ""], ["Wang", "Xu", ""], ["Chen", "Sisheng", ""], ["Echizen", "Isao", ""], ["Sanchez", "Victor", ""], ["Li", "Chang-Tsun", ""]]}, {"id": "2106.06927", "submitter": "Renan Rojas-Gomez", "authors": "Renan A. Rojas-Gomez, Raymond A. Yeh, Minh N. Do, Anh Nguyen", "title": "Inverting Adversarially Robust Networks for Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research in adversarially robust classifiers suggests their\nrepresentations tend to be aligned with human perception, which makes them\nattractive for image synthesis and restoration applications. Despite favorable\nempirical results on a few downstream tasks, their advantages are limited to\nslow and sensitive optimization-based techniques. Moreover, their use on\ngenerative models remains unexplored. This work proposes the use of robust\nrepresentations as a perceptual primitive for feature inversion models, and\nshow its benefits with respect to standard non-robust image features. We\nempirically show that adopting robust representations as an image prior\nsignificantly improves the reconstruction accuracy of CNN-based feature\ninversion models. Furthermore, it allows reconstructing images at multiple\nscales out-of-the-box. Following these findings, we propose an\nencoding-decoding network based on robust representations and show its\nadvantages for applications such as anomaly detection, style transfer and image\ndenoising.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 05:51:00 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Rojas-Gomez", "Renan A.", ""], ["Yeh", "Raymond A.", ""], ["Do", "Minh N.", ""], ["Nguyen", "Anh", ""]]}, {"id": "2106.06939", "submitter": "Shaobo Min", "authors": "Shaobo Min, Qi Dai, Hongtao Xie, Chuang Gan, Yongdong Zhang, Jingdong\n  Wang", "title": "Cross-Modal Attention Consistency for Video-Audio Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-modal correlation provides an inherent supervision for video\nunsupervised representation learning. Existing methods focus on distinguishing\ndifferent video clips by visual and audio representations. We human visual\nperception could attend to regions where sounds are made, and our auditory\nperception could also ground their frequencies of sounding objects, which we\ncall bidirectional local correspondence. Such supervision is intuitive but not\nwell explored in the contrastive learning framework. This paper introduces a\npretext task, Cross-Modal Attention Consistency (CMAC), for exploring the\nbidirectional local correspondence property. The CMAC approach aims to align\nthe regional attention generated purely from the visual signal with the target\nattention generated under the guidance of acoustic signal, and do a similar\nalignment for frequency grounding on the acoustic attention. Accompanied by a\nremoulded cross-modal contrastive loss where we consider additional\nwithin-modal interactions, the CMAC approach works effectively for enforcing\nthe bidirectional alignment. Extensive experiments on six downstream benchmarks\ndemonstrate that CMAC can improve the state-of-the-art performance on both\nvisual and audio modalities.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 07:41:15 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Min", "Shaobo", ""], ["Dai", "Qi", ""], ["Xie", "Hongtao", ""], ["Gan", "Chuang", ""], ["Zhang", "Yongdong", ""], ["Wang", "Jingdong", ""]]}, {"id": "2106.06942", "submitter": "Ziyuan Huang", "authors": "Zhiwu Qing, Ziyuan Huang, Xiang Wang, Yutong Feng, Shiwei Zhang,\n  Jianwen Jiang, Mingqian Tang, Changxin Gao, Marcelo H. Ang Jr, Nong Sang,", "title": "A Stronger Baseline for Ego-Centric Action Detection", "comments": "CVPRW21, EPIC-KITCHENS-100 Competition Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report analyzes an egocentric video action detection method we\nused in the 2021 EPIC-KITCHENS-100 competition hosted in CVPR2021 Workshop. The\ngoal of our task is to locate the start time and the end time of the action in\nthe long untrimmed video, and predict action category. We adopt sliding window\nstrategy to generate proposals, which can better adapt to short-duration\nactions. In addition, we show that classification and proposals are conflict in\nthe same network. The separation of the two tasks boost the detection\nperformance with high efficiency. By simply employing these strategy, we\nachieved 16.10\\% performance on the test set of EPIC-KITCHENS-100 Action\nDetection challenge using a single model, surpassing the baseline method by\n11.7\\% in terms of average mAP.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 08:11:31 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Qing", "Zhiwu", ""], ["Huang", "Ziyuan", ""], ["Wang", "Xiang", ""], ["Feng", "Yutong", ""], ["Zhang", "Shiwei", ""], ["Jiang", "Jianwen", ""], ["Tang", "Mingqian", ""], ["Gao", "Changxin", ""], ["Ang", "Marcelo H.", "Jr"], ["Sang", "Nong", ""]]}, {"id": "2106.06946", "submitter": "Mikl\\'os Z. Horv\\'ath", "authors": "Mikl\\'os Z. Horv\\'ath, Mark Niklas M\\\"uller, Marc Fischer, Martin\n  Vechev", "title": "Boosting Randomized Smoothing with Variance Reduced Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Randomized Smoothing (RS) is a promising method for obtaining robustness\ncertificates by evaluating a base model under noise. In this work we: (i)\ntheoretically motivate why ensembles are a particularly suitable choice as base\nmodels for RS, and (ii) empirically confirm this choice, obtaining state of the\nart results in multiple settings. The key insight of our work is that the\nreduced variance of ensembles over the perturbations introduced in RS leads to\nsignificantly more consistent classifications for a given input, in turn\nleading to substantially increased certifiable radii for difficult samples. We\nalso introduce key optimizations which enable an up to 50-fold decrease in\nsample complexity of RS, thus drastically reducing its computational overhead.\nExperimentally, we show that ensembles of only 3 to 10 classifiers consistently\nimprove on the strongest single model with respect to their average certified\nradius (ACR) by 5% to 21% on both CIFAR-10 and ImageNet. On the latter, we\nachieve a state-of-the-art ACR of 1.11. We release all code and models required\nto reproduce our results upon publication.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 08:40:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Horv\u00e1th", "Mikl\u00f3s Z.", ""], ["M\u00fcller", "Mark Niklas", ""], ["Fischer", "Marc", ""], ["Vechev", "Martin", ""]]}, {"id": "2106.06959", "submitter": "Changyeon Yoon", "authors": "Jaewoong Choi, Changyeon Yoon, Junho Lee, Jung Ho Park, Geonho Hwang,\n  Myungjoo Kang", "title": "Do Not Escape From the Manifold: Discovering the Local Coordinates on\n  the Latent Space of GANs", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to find local-geometry-aware traversal\ndirections on the intermediate latent space of Generative Adversarial Networks\n(GANs). These directions are defined as an ordered basis of tangent space at a\nlatent code. Motivated by the intrinsic sparsity of the latent space, the basis\nis discovered by solving the low-rank approximation problem of the differential\nof the partial network. Moreover, the local traversal basis leads to a natural\niterative traversal on the latent space. Iterative Curve-Traversal shows stable\ntraversal on images, since the trajectory of latent code stays close to the\nlatent space even under the strong perturbations compared to the linear\ntraversal. This stability provides far more diverse variations of the given\nimage. Although the proposed method can be applied to various GAN models, we\nfocus on the W-space of the StyleGAN2, which is renowned for showing the better\ndisentanglement of the latent factors of variation. Our quantitative and\nqualitative analysis provides evidence showing that the W-space is still\nglobally warped while showing a certain degree of global consistency of\ninterpretable variation. In particular, we introduce some metrics on the\nGrassmannian manifolds to quantify the global warpage of the W-space and the\nsubspace traversal to test the stability of traversal directions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 10:29:42 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Choi", "Jaewoong", ""], ["Yoon", "Changyeon", ""], ["Lee", "Junho", ""], ["Park", "Jung Ho", ""], ["Hwang", "Geonho", ""], ["Kang", "Myungjoo", ""]]}, {"id": "2106.06960", "submitter": "Mengmeng Cui", "authors": "Mengmeng Cui, Wei Wang, Jinjin Zhang, Liang Wang", "title": "Representation and Correlation Enhanced Encoder-Decoder Framework for\n  Scene Text Recognition", "comments": "15 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention-based encoder-decoder framework is widely used in the scene text\nrecognition task. However, for the current state-of-the-art(SOTA) methods,\nthere is room for improvement in terms of the efficient usage of local visual\nand global context information of the input text image, as well as the robust\ncorrelation between the scene processing module(encoder) and the text\nprocessing module(decoder). In this paper, we propose a Representation and\nCorrelation Enhanced Encoder-Decoder Framework(RCEED) to address these\ndeficiencies and break performance bottleneck. In the encoder module, local\nvisual feature, global context feature, and position information are aligned\nand fused to generate a small-size comprehensive feature map. In the decoder\nmodule, two methods are utilized to enhance the correlation between scene and\ntext feature space. 1) The decoder initialization is guided by the holistic\nfeature and global glimpse vector exported from the encoder. 2) The feature\nenriched glimpse vector produced by the Multi-Head General Attention is used to\nassist the RNN iteration and the character prediction at each time step.\nMeanwhile, we also design a Layernorm-Dropout LSTM cell to improve model's\ngeneralization towards changeable texts. Extensive experiments on the\nbenchmarks demonstrate the advantageous performance of RCEED in scene text\nrecognition tasks, especially the irregular ones.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 10:36:56 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cui", "Mengmeng", ""], ["Wang", "Wei", ""], ["Zhang", "Jinjin", ""], ["Wang", "Liang", ""]]}, {"id": "2106.06963", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, Yuexian Zou", "title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology\n  Report Generation", "comments": "Accepted by CVPR 2021 (2021 IEEE/CVF Conference on Computer Vision\n  and Pattern Recognition (CVPR2021))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically generating radiology reports can improve current clinical\npractice in diagnostic radiology. On one hand, it can relieve radiologists from\nthe heavy burden of report writing; On the other hand, it can remind\nradiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.\nYet, this task remains a challenging job for data-driven neural networks, due\nto the serious visual and textual data biases. To this end, we propose a\nPosterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to\nimitate the working patterns of radiologists, who will first examine the\nabnormal regions and assign the disease topic tags to the abnormal regions, and\nthen rely on the years of prior medical knowledge and prior working experience\naccumulations to write reports. Thus, the PPKED includes three modules:\nPosterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and\nMulti-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior\nknowledge, which provides explicit abnormal visual regions to alleviate visual\ndata bias; PrKE explores the prior knowledge from the prior medical knowledge\ngraph (medical knowledge) and prior radiology reports (working experience) to\nalleviate textual data bias. The explored knowledge is distilled by the MKD to\ngenerate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our\nmethod is able to outperform previous state-of-the-art models on these two\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 11:10:02 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 17:52:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Fenglin", ""], ["Wu", "Xian", ""], ["Ge", "Shen", ""], ["Fan", "Wei", ""], ["Zou", "Yuexian", ""]]}, {"id": "2106.06965", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang, Xu Sun", "title": "Contrastive Attention for Automatic Chest X-ray Report Generation", "comments": "Appear in Findings of ACL 2021 (The Joint Conference of the 59th\n  Annual Meeting of the Association for Computational Linguistics and the 11th\n  International Joint Conference on Natural Language Processing (ACL-IJCNLP\n  2021))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, chest X-ray report generation, which aims to automatically generate\ndescriptions of given chest X-ray images, has received growing research\ninterests. The key challenge of chest X-ray report generation is to accurately\ncapture and describe the abnormal regions. In most cases, the normal regions\ndominate the entire chest X-ray image, and the corresponding descriptions of\nthese normal regions dominate the final report. Due to such data bias,\nlearning-based models may fail to attend to abnormal regions. In this work, to\neffectively capture and describe abnormal regions, we propose the Contrastive\nAttention (CA) model. Instead of solely focusing on the current input image,\nthe CA model compares the current input image with normal images to distill the\ncontrastive information. The acquired contrastive information can better\nrepresent the visual features of abnormal regions. According to the experiments\non the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into\nseveral existing models can boost their performance across most metrics. In\naddition, according to the analysis, the CA model can help existing models\nbetter attend to the abnormal regions and provide more accurate descriptions\nwhich are crucial for an interpretable diagnosis. Specifically, we achieve the\nstate-of-the-art results on the two public datasets.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 11:20:31 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Fenglin", ""], ["Yin", "Changchang", ""], ["Wu", "Xian", ""], ["Ge", "Shen", ""], ["Zhang", "Ping", ""], ["Sun", "Xu", ""]]}, {"id": "2106.06966", "submitter": "Huapeng Wu", "authors": "Huapeng Wu, Jie Gui, Jun Zhang, James T. Kwok, Zhihui Wei", "title": "Feedback Pyramid Attention Networks for Single Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural network (CNN) based image super-resolution\n(SR) methods have achieved significant performance improvement. However, most\nCNN-based methods mainly focus on feed-forward architecture design and neglect\nto explore the feedback mechanism, which usually exists in the human visual\nsystem. In this paper, we propose feedback pyramid attention networks (FPAN) to\nfully exploit the mutual dependencies of features. Specifically, a novel\nfeedback connection structure is developed to enhance low-level feature\nexpression with high-level information. In our method, the output of each layer\nin the first stage is also used as the input of the corresponding layer in the\nnext state to re-update the previous low-level filters. Moreover, we introduce\na pyramid non-local structure to model global contextual information in\ndifferent scales and improve the discriminative representation of the network.\nExtensive experimental results on various datasets demonstrate the superiority\nof our FPAN in comparison with the state-of-the-art SR methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 11:32:53 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wu", "Huapeng", ""], ["Gui", "Jie", ""], ["Zhang", "Jun", ""], ["Kwok", "James T.", ""], ["Wei", "Zhihui", ""]]}, {"id": "2106.06971", "submitter": "Yingkun Hou", "authors": "Hao Hou, Yingkun Hou, Yuxuan Shi, Benzheng Wei, Jun Xu", "title": "NLHD: A Pixel-Level Non-Local Retinex Model for Low-Light Image\n  Enhancement", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinex model has been applied to low-light image enhancement in many\nexisting methods. More appropriate decomposition of a low-light image can help\nachieve better image enhancement. In this paper, we propose a new pixel-level\nnon-local Haar transform based illumination and reflectance decomposition\nmethod (NLHD). The unique low-frequency coefficient of Haar transform on each\nsimilar pixel group is used to reconstruct the illumination component, and the\nrest of all high-frequency coefficients are employed to reconstruct the\nreflectance component. The complete similarity of pixels in a matched similar\npixel group and the simple separable Haar transform help to obtain more\nappropriate image decomposition; thus, the image is hardly sharpened in the\nimage brightness enhancement procedure. The exponential transform and\nlogarithmic transform are respectively implemented on the illumination\ncomponent. Then a minimum fusion strategy on the results of these two\ntransforms is utilized to achieve more natural illumination component\nenhancement. It can alleviate the mosaic artifacts produced in the darker\nregions by the exponential transform with a gamma value less than 1 and reduce\ninformation loss caused by excessive enhancement of the brighter regions due to\nthe logarithmic transform. Finally, the Retinex model is applied to the\nenhanced illumination and reflectance to achieve image enhancement. We also\ndevelop a local noise level estimation based noise suppression method and a\nnon-local saturation reduction based color deviation correction method. These\ntwo methods can respectively attenuate noise or color deviation usually\npresented in the enhanced results of the extremely dark low-light images.\nExperiments on benchmark datasets show that the proposed method can achieve\nbetter low-light image enhancement results on subjective and objective\nevaluations than most existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 11:48:14 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 15:08:50 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Hou", "Hao", ""], ["Hou", "Yingkun", ""], ["Shi", "Yuxuan", ""], ["Wei", "Benzheng", ""], ["Xu", "Jun", ""]]}, {"id": "2106.06980", "submitter": "Mahesh Raveendranatha Panicker", "authors": "Mahesh Raveendranatha Panicker, Yale Tung Chen, Gayathri M,\n  Madhavanunni A N, Kiran Vishnu Narayan, C Kesavadas and A P Vinod", "title": "An Approach Towards Physics Informed Lung Ultrasound Image Scoring\n  Neural Network for Diagnostic Assistance in COVID-19", "comments": "8 pages, 8 figures, 3 tables, submitted to Springer SIVP Special\n  Issue for COVID19", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Ultrasound is fast becoming an inevitable diagnostic tool for regular and\ncontinuous monitoring of the lung with the recent outbreak of COVID-19. In this\nwork, a novel approach is presented to extract acoustic propagation-based\nfeatures to automatically highlight the region below pleura, which is an\nimportant landmark in lung ultrasound (LUS). Subsequently, a multichannel input\nformed by using the acoustic physics-based feature maps is fused to train a\nneural network, referred to as LUSNet, to classify the LUS images into five\nclasses of varying severity of lung infection to track the progression of\nCOVID-19. In order to ensure that the proposed approach is agnostic to the type\nof acquisition, the LUSNet, which consists of a U-net architecture is trained\nin an unsupervised manner with the acoustic feature maps to ensure that the\nencoder-decoder architecture is learning features in the pleural region of\ninterest. A novel combination of the U-net output and the U-net encoder output\nis employed for the classification of severity of infection in the lung. A\ndetailed analysis of the proposed approach on LUS images over the infection to\nfull recovery period of ten confirmed COVID-19 subjects shows an average\nfive-fold cross-validation accuracy, sensitivity, and specificity of 97%, 93%,\nand 98% respectively over 5000 frames of COVID-19 videos. The analysis also\nshows that, when the input dataset is limited and diverse as in the case of\nCOVID-19 pandemic, an aided effort of combining acoustic propagation-based\nfeatures along with the gray scale images, as proposed in this work, improves\nthe performance of the neural network significantly and also aids the labelling\nand triaging process.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 13:01:53 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Panicker", "Mahesh Raveendranatha", ""], ["Chen", "Yale Tung", ""], ["M", "Gayathri", ""], ["N", "Madhavanunni A", ""], ["Narayan", "Kiran Vishnu", ""], ["Kesavadas", "C", ""], ["Vinod", "A P", ""]]}, {"id": "2106.06987", "submitter": "Mahesh Raveendranatha Panicker", "authors": "Arpan Tripathi, Mahesh Raveendranatha Panicker, Abhilash R\n  Hareendranathan, Yale Tung Chen, Jacob L Jaremko, Kiran Vishnu Narayan and\n  Kesavadas C", "title": "Learning the Imaging Landmarks: Unsupervised Key point Detection in Lung\n  Ultrasound Videos", "comments": "5 pages, 6 figures, submitted to IEEE EMBC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Lung ultrasound (LUS) is an increasingly popular diagnostic imaging modality\nfor continuous and periodic monitoring of lung infection, given its advantages\nof non-invasiveness, non-ionizing nature, portability and easy disinfection.\nThe major landmarks assessed by clinicians for triaging using LUS are pleura, A\nand B lines. There have been many efforts for the automatic detection of these\nlandmarks. However, restricting to a few pre-defined landmarks may not reveal\nthe actual imaging biomarkers particularly in case of new pathologies like\nCOVID-19. Rather, the identification of key landmarks should be driven by data\ngiven the availability of a plethora of neural network algorithms. This work is\na first of its kind attempt towards unsupervised detection of the key LUS\nlandmarks in LUS videos of COVID-19 subjects during various stages of\ninfection. We adapted the relatively newer approach of transporter neural\nnetworks to automatically mark and track pleura, A and B lines based on their\nperiodic motion and relatively stable appearance in the videos. Initial results\non unsupervised pleura detection show an accuracy of 91.8% employing 1081 LUS\nvideo frames.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 13:27:12 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tripathi", "Arpan", ""], ["Panicker", "Mahesh Raveendranatha", ""], ["Hareendranathan", "Abhilash R", ""], ["Chen", "Yale Tung", ""], ["Jaremko", "Jacob L", ""], ["Narayan", "Kiran Vishnu", ""], ["C", "Kesavadas", ""]]}, {"id": "2106.06988", "submitter": "Weichuan Zhang", "authors": "Weichuan Zhang, Xuefang Liu, Zhe Xue, Yongsheng Gao, Changming Sun", "title": "NDPNet: A novel non-linear data projection network for few-shot\n  fine-grained image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric-based few-shot fine-grained image classification (FSFGIC) aims to\nlearn a transferable feature embedding network by estimating the similarities\nbetween query images and support classes from very few examples. In this work,\nwe propose, for the first time, to introduce the non-linear data projection\nconcept into the design of FSFGIC architecture in order to address the limited\nsample problem in few-shot learning and at the same time to increase the\ndiscriminability of the model for fine-grained image classification.\nSpecifically, we first design a feature re-abstraction embedding network that\nhas the ability to not only obtain the required semantic features for effective\nmetric learning but also re-enhance such features with finer details from input\nimages. Then the descriptors of the query images and the support classes are\nprojected into different non-linear spaces in our proposed similarity metric\nlearning network to learn discriminative projection factors. This design can\neffectively operate in the challenging and restricted condition of a FSFGIC\ntask for making the distance between the samples within the same class smaller\nand the distance between samples from different classes larger and for reducing\nthe coupling relationship between samples from different categories.\nFurthermore, a novel similarity measure based on the proposed non-linear data\nproject is presented for evaluating the relationships of feature information\nbetween a query image and a support set. It is worth to note that our proposed\narchitecture can be easily embedded into any episodic training mechanisms for\nend-to-end training from scratch. Extensive experiments on FSFGIC tasks\ndemonstrate the superiority of the proposed methods over the state-of-the-art\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 13:33:09 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 04:22:17 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 11:22:38 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zhang", "Weichuan", ""], ["Liu", "Xuefang", ""], ["Xue", "Zhe", ""], ["Gao", "Yongsheng", ""], ["Sun", "Changming", ""]]}, {"id": "2106.06992", "submitter": "Feihong Liu", "authors": "Liu Feihong, Yang Junwei, He Xiaowei, Zhou Luping, Feng Jun, Shen\n  Dinggang", "title": "Is Perfect Filtering Enough Leading to Perfect Phase Correction for dMRI\n  data?", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being complex-valued and low in signal-to-noise ratios, magnitude-based\ndiffusion MRI is confounded by the noise-floor that falsely elevates signal\nmagnitude and incurs bias to the commonly used diffusion indices, such as\nfractional anisotropy (FA). To avoid noise-floor, most existing phase\ncorrection methods explore improving filters to estimate the noise-free\nbackground phase. In this work, after diving into the phase correction\nprocedures, we argue that even a perfect filter is insufficient for phase\ncorrection because the correction procedures are incapable of distinguishing\nsign-symbols of noise, resulting in artifacts (\\textit{i.e.}, arbitrary signal\nloss). With this insight, we generalize the definition of noise-floor to a\ncomplex polar coordinate system and propose a calibration procedure that could\nconveniently distinguish noise sign symbols. The calibration procedure is\nconceptually simple and easy to implement without relying on any external\ntechnique while keeping distinctly effective.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 13:38:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Feihong", "Liu", ""], ["Junwei", "Yang", ""], ["Xiaowei", "He", ""], ["Luping", "Zhou", ""], ["Jun", "Feng", ""], ["Dinggang", "Shen", ""]]}, {"id": "2106.06996", "submitter": "Huapeng Wu", "authors": "Huapeng Wu, Jie Gui, Jun Zhang, James T. Kwok, Zhihui Wei", "title": "Pyramidal Dense Attention Networks for Lightweight Image\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural network methods have achieved an\nexcellent performance in image superresolution (SR), but they can not be easily\napplied to embedded devices due to large memory cost. To solve this problem, we\npropose a pyramidal dense attention network (PDAN) for lightweight image\nsuper-resolution in this paper. In our method, the proposed pyramidal dense\nlearning can gradually increase the width of the densely connected layer inside\na pyramidal dense block to extract deep features efficiently. Meanwhile, the\nadaptive group convolution that the number of groups grows linearly with dense\nconvolutional layers is introduced to relieve the parameter explosion. Besides,\nwe also present a novel joint attention to capture cross-dimension interaction\nbetween the spatial dimensions and channel dimension in an efficient way for\nproviding rich discriminative feature representations. Extensive experimental\nresults show that our method achieves superior performance in comparison with\nthe state-of-the-art lightweight SR methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 13:49:41 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wu", "Huapeng", ""], ["Gui", "Jie", ""], ["Zhang", "Jun", ""], ["Kwok", "James T.", ""], ["Wei", "Zhihui", ""]]}, {"id": "2106.07003", "submitter": "Ammar Abbas", "authors": "Ammar N. Abbas, Muhammad Asad Irshad, and Hossam Hassan Ammar", "title": "Experimental Analysis of Trajectory Control Using Computer Vision and\n  Artificial Intelligence for Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception of the lane boundaries is crucial for the tasks related to\nautonomous trajectory control. In this paper, several methodologies for lane\ndetection are discussed with an experimental illustration: Hough\ntransformation, Blob analysis, and Bird's eye view. Following the abstraction\nof lane marks from the boundary, the next approach is applying a control law\nbased on the perception to control steering and speed control. In the\nfollowing, a comparative analysis is made between an open-loop response, PID\ncontrol, and a neural network control law through graphical statistics. To get\nthe perception of the surrounding a wireless streaming camera connected to\nRaspberry Pi is used. After pre-processing the signal received by the camera\nthe output is sent back to the Raspberry Pi that processes the input and\ncommunicates the control to the motors through Arduino via serial\ncommunication.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 14:23:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Abbas", "Ammar N.", ""], ["Irshad", "Muhammad Asad", ""], ["Ammar", "Hossam Hassan", ""]]}, {"id": "2106.07009", "submitter": "Jong Chul Ye", "authors": "Kwanyoung Kim, Jong Chul Ye", "title": "Noise2Score: Tweedie's Approach to Self-Supervised Image Denoising\n  without Clean Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been extensive research interest in training deep\nnetworks to denoise images without clean reference. However, the representative\napproaches such as Noise2Noise, Noise2Void, Stein's unbiased risk estimator\n(SURE), etc. seem to differ from one another and it is difficult to find the\ncoherent mathematical structure. To address this, here we present a novel\napproach, called Noise2Score, which reveals a missing link in order to unite\nthese seemingly different approaches. Specifically, we show that image\ndenoising problems without clean images can be addressed by finding the mode of\nthe posterior distribution and that the Tweedie's formula offers an explicit\nsolution through the score function (i.e. the gradient of log likelihood). Our\nmethod then uses the recent finding that the score function can be stably\nestimated from the noisy images using the amortized residual denoising\nautoencoder, the method of which is closely related to Noise2Noise or\nNose2Void. Our Noise2Score approach is so universal that the same network\ntraining can be used to remove noises from images that are corrupted by any\nexponential family distributions and noise parameters. Using extensive\nexperiments with Gaussian, Poisson, and Gamma noises, we show that Noise2Score\nsignificantly outperforms the state-of-the-art self-supervised denoising\nmethods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 14:41:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kim", "Kwanyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2106.07015", "submitter": "Ammar Abbas", "authors": "Ammar N. Abbas and David Moser", "title": "Siamese Network Training Using Sampled Triplets and Image Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The device used in this work detects the objects over the surface of the\nwater using two thermal cameras which aid the users to detect and avoid the\nobjects in scenarios where the human eyes cannot (night, fog, etc.). To avoid\nthe obstacle collision autonomously, it is required to track the objects in\nreal-time and assign a specific identity to each object to determine its\ndynamics (trajectory, velocity, etc.) for making estimated collision\npredictions. In the following work, a Machine Learning (ML) approach for\nComputer Vision (CV) called Convolutional Neural Network (CNN) was used using\nTensorFlow as the high-level programming environment in Python. To validate the\nalgorithm a test set was generated using an annotation tool that was created\nduring the work for proper evaluation. Once validated, the algorithm was\ndeployed on the platform and tested with the sequence generated by the test\nboat.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 14:47:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Abbas", "Ammar N.", ""], ["Moser", "David", ""]]}, {"id": "2106.07020", "submitter": "Svetlana Illarionova", "authors": "Svetlana Illarionova, Dmitrii Shadrin, Alexey Trekin, Vladimir\n  Ignatiev, Ivan Oseledets", "title": "Generation of the NIR spectral Band for Satellite Images with\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The near-infrared (NIR) spectral range (from 780 to 2500 nm) of the\nmultispectral remote sensing imagery provides vital information for the\nlandcover classification, especially concerning the vegetation assessment.\nDespite the usefulness of NIR, common RGB is not always accompanied by it.\nModern achievements in image processing via deep neural networks allow\ngenerating artificial spectral information, such as for the image colorization\nproblem. In this research, we aim to investigate whether this approach can\nproduce not only visually similar images but also an artificial spectral band\nthat can improve the performance of computer vision algorithms for solving\nremote sensing tasks. We study the generative adversarial network (GAN)\napproach in the task of the NIR band generation using just RGB channels of\nhigh-resolution satellite imagery. We evaluate the impact of a generated\nchannel on the model performance for solving the forest segmentation task. Our\nresults show an increase in model accuracy when using generated NIR comparing\nto the baseline model that uses only RGB (0.947 and 0.914 F1-score\naccordingly). Conducted study shows the advantages of generating the extra band\nand its implementation in applied challenges reducing the required amount of\nlabeled data.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 15:14:57 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Illarionova", "Svetlana", ""], ["Shadrin", "Dmitrii", ""], ["Trekin", "Alexey", ""], ["Ignatiev", "Vladimir", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2106.07023", "submitter": "Younggeun Kim", "authors": "Jeeseung Park, Younggeun Kim", "title": "Styleformer: Transformer based Generative Adversarial Networks with\n  Style Vector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Styleformer, which is a style-based generator for GAN\narchitecture, but a convolution-free transformer-based generator. In our paper,\nwe explain how a transformer can generate high-quality images, overcoming the\ndisadvantage that convolution operations are difficult to capture global\nfeatures in an image. Furthermore, we change the demodulation of StyleGAN2 and\nmodify the existing transformer structure (e.g., residual connection, layer\nnormalization) to create a strong style-based generator with a convolution-free\nstructure. We also make Styleformer lighter by applying Linformer, enabling\nStyleformer to generate higher resolution images and result in improvements in\nterms of speed and memory. We experiment with the low-resolution image dataset\nsuch as CIFAR-10, as well as the high-resolution image dataset like\nLSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark\ndataset, which is comparable performance to the current state-of-the-art and\noutperforms all GAN-based generative models, including StyleGAN2-ADA with fewer\nparameters on the unconditional setting. We also both achieve new\nstate-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10\nand CelebA. We release our code at\nhttps://github.com/Jeeseung-Park/Styleformer.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 15:30:39 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 01:49:17 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Park", "Jeeseung", ""], ["Kim", "Younggeun", ""]]}, {"id": "2106.07026", "submitter": "Zhicheng Cai", "authors": "Zhicheng Cai, Kaizhu Huang, Chenglei Peng", "title": "Reborn Mechanism: Rethinking the Negative Phase Information Flow in\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel nonlinear activation mechanism typically for\nconvolutional neural network (CNN), named as reborn mechanism. In sharp\ncontrast to ReLU which cuts off the negative phase value, the reborn mechanism\nenjoys the capacity to reborn and reconstruct dead neurons. Compared to other\nimproved ReLU functions, reborn mechanism introduces a more proper way to\nutilize the negative phase information. Extensive experiments validate that\nthis activation mechanism is able to enhance the model representation ability\nmore significantly and make the better use of the input data information while\nmaintaining the advantages of the original ReLU function. Moreover, reborn\nmechanism enables a non-symmetry that is hardly achieved by traditional CNNs\nand can act as a channel compensation method, offering competitive or even\nbetter performance but with fewer learned parameters than traditional methods.\nReborn mechanism was tested on various benchmark datasets, all obtaining better\nperformance than previous nonlinear activation functions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 15:33:49 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 17:37:09 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Cai", "Zhicheng", ""], ["Huang", "Kaizhu", ""], ["Peng", "Chenglei", ""]]}, {"id": "2106.07049", "submitter": "Kangning Liu", "authors": "Kangning Liu, Yiqiu Shen, Nan Wu, Jakub Ch{\\l}\\k{e}dowski, Carlos\n  Fernandez-Granda, Krzysztof J. Geras", "title": "Weakly-supervised High-resolution Segmentation of Mammography Images for\n  Breast Cancer Diagnosis", "comments": "The last two authors contributed equally. Accepted to Medical Imaging\n  with Deep Learning (MIDL) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, deep learning classifiers have shown promising results\nin image-based medical diagnosis. However, interpreting the outputs of these\nmodels remains a challenge. In cancer diagnosis, interpretability can be\nachieved by localizing the region of the input image responsible for the\noutput, i.e. the location of a lesion. Alternatively, segmentation or detection\nmodels can be trained with pixel-wise annotations indicating the locations of\nmalignant lesions. Unfortunately, acquiring such labels is labor-intensive and\nrequires medical expertise. To overcome this difficulty, weakly-supervised\nlocalization can be utilized. These methods allow neural network classifiers to\noutput saliency maps highlighting the regions of the input most relevant to the\nclassification task (e.g. malignant lesions in mammograms) using only\nimage-level labels (e.g. whether the patient has cancer or not) during\ntraining. When applied to high-resolution images, existing methods produce\nlow-resolution saliency maps. This is problematic in applications in which\nsuspicious lesions are small in relation to the image size. In this work, we\nintroduce a novel neural network architecture to perform weakly-supervised\nsegmentation of high-resolution images. The proposed model selects regions of\ninterest via coarse-level localization, and then performs fine-grained\nsegmentation of those regions. We apply this model to breast cancer diagnosis\nwith screening mammography, and validate it on a large clinically-realistic\ndataset. Measured by Dice similarity score, our approach outperforms existing\nmethods by a large margin in terms of localization performance of benign and\nmalignant lesions, relatively improving the performance by 39.6% and 20.0%,\nrespectively. Code and the weights of some of the models are available at\nhttps://github.com/nyukat/GLAM\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:25:21 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 03:46:23 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Liu", "Kangning", ""], ["Shen", "Yiqiu", ""], ["Wu", "Nan", ""], ["Ch\u0142\u0119dowski", "Jakub", ""], ["Fernandez-Granda", "Carlos", ""], ["Geras", "Krzysztof J.", ""]]}, {"id": "2106.07068", "submitter": "Yash Sharma", "authors": "Yash Sharma, Lubaina Ehsan, Sana Syed, Donald E. Brown", "title": "HistoTransfer: Understanding Transfer Learning for Histopathology", "comments": "Accepted at IEEE International Conference on Biomedical and Health\n  Informatics (BHI'21). arXiv admin note: text overlap with arXiv:2103.10626", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advancement in digital pathology and artificial intelligence has enabled deep\nlearning-based computer vision techniques for automated disease diagnosis and\nprognosis. However, WSIs present unique computational and algorithmic\nchallenges. WSIs are gigapixel-sized, making them infeasible to be used\ndirectly for training deep neural networks. Hence, for modeling, a two-stage\napproach is adopted: Patch representations are extracted first, followed by the\naggregation for WSI prediction. These approaches require detailed pixel-level\nannotations for training the patch encoder. However, obtaining these\nannotations is time-consuming and tedious for medical experts. Transfer\nlearning is used to address this gap and deep learning architectures\npre-trained on ImageNet are used for generating patch-level representation.\nEven though ImageNet differs significantly from histopathology data,\npre-trained networks have been shown to perform impressively on histopathology\ndata. Also, progress in self-supervised and multi-task learning coupled with\nthe release of multiple histopathology data has led to the release of\nhistopathology-specific networks. In this work, we compare the performance of\nfeatures extracted from networks trained on ImageNet and histopathology data.\nWe use an attention pooling network over these extracted features for\nslide-level aggregation. We investigate if features learned using more complex\nnetworks lead to gain in performance. We use a simple top-k sampling approach\nfor fine-tuning framework and study the representation similarity between\nfrozen and fine-tuned networks using Centered Kernel Alignment. Further, to\nexamine if intermediate block representation is better suited for feature\nextraction and ImageNet architectures are unnecessarily large for\nhistopathology, we truncate the blocks of ResNet18 and DenseNet121 and examine\nthe performance.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 18:55:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sharma", "Yash", ""], ["Ehsan", "Lubaina", ""], ["Syed", "Sana", ""], ["Brown", "Donald E.", ""]]}, {"id": "2106.07075", "submitter": "Ivan Grubi\\v{s}i\\'c", "authors": "Ivan Grubi\\v{s}i\\'c, Marin Or\\v{s}i\\'c, Sini\\v{s}a \\v{S}egvi\\'c", "title": "A baseline for semi-supervised learning of efficient semantic\n  segmentation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning is especially interesting in the dense prediction\ncontext due to high cost of pixel-level ground truth. Unfortunately, most such\napproaches are evaluated on outdated architectures which hamper research due to\nvery slow training and high requirements on GPU RAM. We address this concern by\npresenting a simple and effective baseline which works very well both on\nstandard and efficient architectures. Our baseline is based on one-way\nconsistency and non-linear geometric and photometric perturbations. We show\nadvantage of perturbing only the student branch and present a plausible\nexplanation of such behaviour. Experiments on Cityscapes and CIFAR-10\ndemonstrate competitive performance with respect to prior work.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 19:31:59 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 09:08:19 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Grubi\u0161i\u0107", "Ivan", ""], ["Or\u0161i\u0107", "Marin", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "2106.07085", "submitter": "Humza Naveed", "authors": "Humza Naveed", "title": "Survey: Image Mixing and Deleting for Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data augmentation has been widely used to improve deep nerual networks\nperformance. Numerous approaches are suggested, for example, dropout,\nregularization and image augmentation, to avoid over-ftting and enhancing\ngeneralization of neural networks. One of the sub-area within data augmentation\nis image mixing and deleting. This specific type of augmentation either mixes\ntwo images or delete image regions to hide or make certain characteristics of\nimages confusing for the network to force it to emphasize on overall structure\nof object in image. The model trained with this approach has shown to perform\nand generalize well as compared to one trained without imgage mixing or\ndeleting. Additional benefit achieved with this method of training is\nrobustness against image corruptions. Due to its low compute cost and success\nin recent past, many techniques of image mixing and deleting are proposed. This\npaper provides detailed review on these devised approaches, dividing\naugmentation strategies in three main categories cut and delete, cut and mix\nand mixup. The second part of paper emprically evaluates these approaches for\nimage classification, finegrained image recognition and object detection where\nit is shown that this category of data augmentation improves the overall\nperformance for deep neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 20:32:24 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Naveed", "Humza", ""]]}, {"id": "2106.07091", "submitter": "Ramin Hasani", "authors": "Zahra Babaiee, Ramin Hasani, Mathias Lechner, Daniela Rus, Radu Grosu", "title": "On-Off Center-Surround Receptive Fields for Accurate and Robust Image\n  Classification", "comments": "21 Pages. Accepted for publication in the proceedings of the 38th\n  International Conference on Machine Learning (ICML) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robustness to variations in lighting conditions is a key objective for any\ndeep vision system. To this end, our paper extends the receptive field of\nconvolutional neural networks with two residual components, ubiquitous in the\nvisual processing system of vertebrates: On-center and off-center pathways,\nwith excitatory center and inhibitory surround; OOCS for short. The on-center\npathway is excited by the presence of a light stimulus in its center but not in\nits surround, whereas the off-center one is excited by the absence of a light\nstimulus in its center but not in its surround. We design OOCS pathways via a\ndifference of Gaussians, with their variance computed analytically from the\nsize of the receptive fields. OOCS pathways complement each other in their\nresponse to light stimuli, ensuring this way a strong edge-detection\ncapability, and as a result, an accurate and robust inference under challenging\nlighting conditions. We provide extensive empirical evidence showing that\nnetworks supplied with the OOCS edge representation gain accuracy and\nillumination-robustness compared to standard deep models.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 20:55:16 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Babaiee", "Zahra", ""], ["Hasani", "Ramin", ""], ["Lechner", "Mathias", ""], ["Rus", "Daniela", ""], ["Grosu", "Radu", ""]]}, {"id": "2106.07113", "submitter": "Sarah Chen", "authors": "Sarah Chen, Esther Cao, Anirudh Koul, Siddha Ganju, Satyarth Praveen,\n  Meher Anand Kasam", "title": "Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models\n  for NASA MODIS Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.EP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the nature of their pathways, NASA Terra and NASA Aqua satellites\ncapture imagery containing swath gaps, which are areas of no data. Swath gaps\ncan overlap the region of interest (ROI) completely, often rendering the entire\nimagery unusable by Machine Learning (ML) models. This problem is further\nexacerbated when the ROI rarely occurs (e.g. a hurricane) and, on occurrence,\nis partially overlapped with a swath gap. With annotated data as supervision, a\nmodel can learn to differentiate between the area of focus and the swath gap.\nHowever, annotation is expensive and currently the vast majority of existing\ndata is unannotated. Hence, we propose an augmentation technique that\nconsiderably removes the existence of swath gaps in order to allow CNNs to\nfocus on the ROI, and thus successfully use data with swath gaps for training.\nWe experiment on the UC Merced Land Use Dataset, where we add swath gaps\nthrough empty polygons (up to 20 percent areas) and then apply augmentation\ntechniques to fill the swath gaps. We compare the model trained with our\naugmentation techniques on the swath gap-filled data with the model trained on\nthe original swath gap-less data and note highly augmented performance.\nAdditionally, we perform a qualitative analysis using activation maps that\nvisualizes the effectiveness of our trained network in not paying attention to\nthe swath gaps. We also evaluate our results with a human baseline and show\nthat, in certain cases, the filled swath gaps look so realistic that even a\nhuman evaluator did not distinguish between original satellite images and swath\ngap-filled images. Since this method is aimed at unlabeled data, it is widely\ngeneralizable and impactful for large scale unannotated datasets from various\nspace data domains.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 23:50:05 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Sarah", ""], ["Cao", "Esther", ""], ["Koul", "Anirudh", ""], ["Ganju", "Siddha", ""], ["Praveen", "Satyarth", ""], ["Kasam", "Meher Anand", ""]]}, {"id": "2106.07115", "submitter": "Qi Lyu", "authors": "Qi Lyu, Xiao Fu, Weiran Wang and Songtao Lu", "title": "Latent Correlation-Based Multiview Learning and Self-Supervision: A\n  Unifying Perspective", "comments": "fixed some typos in the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple views of data, both naturally acquired (e.g., image and audio) and\nartificially produced (e.g., via adding different noise to data samples), have\nproven useful in enhancing representation learning. Natural views are often\nhandled by multiview analysis tools, e.g., (deep) canonical correlation\nanalysis [(D)CCA], while the artificial ones are frequently used in\nself-supervised learning (SSL) paradigms, e.g., SimCLR and Barlow Twins. Both\ntypes of approaches often involve learning neural feature extractors such that\nthe embeddings of data exhibit high cross-view correlations. Although\nintuitive, the effectiveness of correlation-based neural embedding is only\nempirically validated. This work puts forth a theory-backed framework for\nunsupervised multiview learning. Our development starts with proposing a\nmultiview model, where each view is a nonlinear mixture of shared and private\ncomponents. Consequently, the learning problem boils down to shared/private\ncomponent identification and disentanglement. Under this model, latent\ncorrelation maximization is shown to guarantee the extraction of the shared\ncomponents across views (up to certain ambiguities). In addition, the private\ninformation in each view can be provably disentangled from the shared using\nproper regularization design. The method is tested on a series of tasks, e.g.,\ndownstream clustering, which all show promising performance. Our development\nalso provides a unifying perspective for understanding various DCCA and SSL\nschemes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 00:12:36 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 16:51:29 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lyu", "Qi", ""], ["Fu", "Xiao", ""], ["Wang", "Weiran", ""], ["Lu", "Songtao", ""]]}, {"id": "2106.07134", "submitter": "Michael Hinczewski", "authors": "F. Ji, M. S. McMaster, S. Schwab, G. Singh, L. N. Smith, S. Adhikari,\n  M. O'Dwyer, F. Sayed, A. Ingrisano, D. Yoder, E. S. Bolman, I. T. Martin, M.\n  Hinczewski, K. D. Singer", "title": "Discerning the painter's hand: machine learning on surface topography", "comments": "main text: 24 pages, 6 figures; SI: 6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Attribution of paintings is a critical problem in art history. This study\nextends machine learning analysis to surface topography of painted works. A\ncontrolled study of positive attribution was designed with paintings produced\nby a class of art students. The paintings were scanned using a confocal optical\nprofilometer to produce surface data. The surface data were divided into\nvirtual patches and used to train an ensemble of convolutional neural networks\n(CNNs) for attribution. Over a range of patch sizes from 0.5 to 60 mm, the\nresulting attribution was found to be 60 to 96% accurate, and, when comparing\nregions of different color, was nearly twice as accurate as CNNs using color\nimages of the paintings. Remarkably, short length scales, as small as twice a\nbristle diameter, were the key to reliably distinguishing among artists. These\nresults show promise for real-world attribution, particularly in the case of\nworkshop practice.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 02:17:43 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ji", "F.", ""], ["McMaster", "M. S.", ""], ["Schwab", "S.", ""], ["Singh", "G.", ""], ["Smith", "L. N.", ""], ["Adhikari", "S.", ""], ["O'Dwyer", "M.", ""], ["Sayed", "F.", ""], ["Ingrisano", "A.", ""], ["Yoder", "D.", ""], ["Bolman", "E. S.", ""], ["Martin", "I. T.", ""], ["Hinczewski", "M.", ""], ["Singer", "K. D.", ""]]}, {"id": "2106.07136", "submitter": "Jingwei Song", "authors": "Jingwei Song, Qiuchen Zhu, Jianyu Lin, and Maani Ghaffari", "title": "Bayesian dense inverse searching algorithm for real-time stereo matching\n  in minimally invasive surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports a CPU-level real-time stereo matching method for surgical\nimages (10 Hz on 640 * 480 image with a single core of i5-9400). The proposed\nmethod is built on the fast ''dense inverse searching'' algorithm, which\nestimates the disparity of the stereo images. The overlapping image patches\n(arbitrary squared image segment) from the images at different scales are\naligned based on the photometric consistency presumption. We propose a Bayesian\nframework to evaluate the probability of the optimized patch disparity at\ndifferent scales. Moreover, we introduce a spatial Gaussian mixed probability\ndistribution to address the pixel-wise probability within the patch. In-vivo\nand synthetic experiments show that our method can handle ambiguities resulted\nfrom the textureless surfaces and the photometric inconsistency caused by the\nLambertian reflectance. Our Bayesian method correctly balances the probability\nof the patch for stereo images at different scales. Experiments indicate that\nthe estimated depth has higher accuracy and fewer outliers than the baseline\nmethods in the surgical scenario.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 02:26:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Song", "Jingwei", ""], ["Zhu", "Qiuchen", ""], ["Lin", "Jianyu", ""], ["Ghaffari", "Maani", ""]]}, {"id": "2106.07140", "submitter": "Jihyeong Yoo", "authors": "Jihyeong Yoo and Qifeng Chen", "title": "SinIR: Efficient General Image Manipulation with Single Image\n  Reconstruction", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SinIR, an efficient reconstruction-based framework trained on a\nsingle natural image for general image manipulation, including\nsuper-resolution, editing, harmonization, paint-to-image, photo-realistic style\ntransfer, and artistic style transfer. We train our model on a single image\nwith cascaded multi-scale learning, where each network at each scale is\nresponsible for image reconstruction. This reconstruction objective greatly\nreduces the complexity and running time of training, compared to the GAN\nobjective. However, the reconstruction objective also exacerbates the output\nquality. Therefore, to solve this problem, we further utilize simple random\npixel shuffling, which also gives control over manipulation, inspired by the\nDenoising Autoencoder. With quantitative evaluation, we show that SinIR has\ncompetitive performance on various image manipulation tasks. Moreover, with a\nmuch simpler training objective (i.e., reconstruction), SinIR is trained 33.5\ntimes faster than SinGAN (for 500 X 500 images) that solves similar tasks. Our\ncode is publicly available at github.com/YooJiHyeong/SinIR.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 02:41:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yoo", "Jihyeong", ""], ["Chen", "Qifeng", ""]]}, {"id": "2106.07141", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Esla Timothy Anzaku, Wesley De Neve, Arnout Van Messem", "title": "Selection of Source Images Heavily Influences the Effectiveness of\n  Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the adoption rate of deep neural networks (DNNs) has tremendously\nincreased in recent years, a solution for their vulnerability against\nadversarial examples has not yet been found. As a result, substantial research\nefforts are dedicated to fix this weakness, with many studies typically using a\nsubset of source images to generate adversarial examples, treating every image\nin this subset as equal. We demonstrate that, in fact, not every source image\nis equally suited for this kind of assessment. To do so, we devise a\nlarge-scale model-to-model transferability scenario for which we meticulously\nanalyze the properties of adversarial examples, generated from every suitable\nsource image in ImageNet by making use of two of the most frequently deployed\nattacks. In this transferability scenario, which involves seven distinct DNN\nmodels, including the recently proposed vision transformers, we reveal that it\nis possible to have a difference of up to $12.5\\%$ in model-to-model\ntransferability success, $1.01$ in average $L_2$ perturbation, and $0.03$\n($8/225$) in average $L_{\\infty}$ perturbation when $1,000$ source images are\nsampled randomly among all suitable candidates. We then take one of the first\nsteps in evaluating the robustness of images used to create adversarial\nexamples, proposing a number of simple but effective methods to identify\nunsuitable source images, thus making it possible to mitigate extreme cases in\nexperimentation and support high-quality benchmarking.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 02:45:45 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 02:32:16 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ozbulak", "Utku", ""], ["Anzaku", "Esla Timothy", ""], ["De Neve", "Wesley", ""], ["Van Messem", "Arnout", ""]]}, {"id": "2106.07159", "submitter": "Jingru Yi", "authors": "Jingru Yi, Pengxiang Wu, Hui Tang, Bo Liu, Qiaoying Huang, Hui Qu,\n  Lianyi Han, Wei Fan, Daniel J. Hoeppner, Dimitris N. Metaxas", "title": "Object-Guided Instance Segmentation With Auxiliary Feature Refinement\n  for Biological Images", "comments": "Accepted in TMI", "journal-ref": null, "doi": "10.1109/TMI.2021.3077285", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is of great importance for many biological\napplications, such as study of neural cell interactions, plant phenotyping, and\nquantitatively measuring how cells react to drug treatment. In this paper, we\npropose a novel box-based instance segmentation method. Box-based instance\nsegmentation methods capture objects via bounding boxes and then perform\nindividual segmentation within each bounding box region. However, existing\nmethods can hardly differentiate the target from its neighboring objects within\nthe same bounding box region due to their similar textures and low-contrast\nboundaries. To deal with this problem, in this paper, we propose an\nobject-guided instance segmentation method. Our method first detects the center\npoints of the objects, from which the bounding box parameters are then\npredicted. To perform segmentation, an object-guided coarse-to-fine\nsegmentation branch is built along with the detection branch. The segmentation\nbranch reuses the object features as guidance to separate target object from\nthe neighboring ones within the same bounding box region. To further improve\nthe segmentation quality, we design an auxiliary feature refinement module that\ndensely samples and refines point-wise features in the boundary regions.\nExperimental results on three biological image datasets demonstrate the\nadvantages of our method. The code will be available at\nhttps://github.com/yijingru/ObjGuided-Instance-Segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 04:35:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yi", "Jingru", ""], ["Wu", "Pengxiang", ""], ["Tang", "Hui", ""], ["Liu", "Bo", ""], ["Huang", "Qiaoying", ""], ["Qu", "Hui", ""], ["Han", "Lianyi", ""], ["Fan", "Wei", ""], ["Hoeppner", "Daniel J.", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "2106.07165", "submitter": "Fazil Altinel", "authors": "Ibrahim Batuhan Akkaya, Fazil Altinel, Ugur Halici", "title": "Self-training Guided Adversarial Domain Adaptation For Thermal Imagery", "comments": "Accepted to CVPR 2021 Perception Beyond the Visible Spectrum (PBVS)\n  workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep models trained on large-scale RGB image datasets have shown tremendous\nsuccess. It is important to apply such deep models to real-world problems.\nHowever, these models suffer from a performance bottleneck under illumination\nchanges. Thermal IR cameras are more robust against such changes, and thus can\nbe very useful for the real-world problems. In order to investigate efficacy of\ncombining feature-rich visible spectrum and thermal image modalities, we\npropose an unsupervised domain adaptation method which does not require\nRGB-to-thermal image pairs. We employ large-scale RGB dataset MS-COCO as source\ndomain and thermal dataset FLIR ADAS as target domain to demonstrate results of\nour method. Although adversarial domain adaptation methods aim to align the\ndistributions of source and target domains, simply aligning the distributions\ncannot guarantee perfect generalization to the target domain. To this end, we\npropose a self-training guided adversarial domain adaptation method to promote\ngeneralization capabilities of adversarial domain adaptation methods. To\nperform self-training, pseudo labels are assigned to the samples on the target\nthermal domain to learn more generalized representations for the target domain.\nExtensive experimental analyses show that our proposed method achieves better\nresults than the state-of-the-art adversarial domain adaptation methods. The\ncode and models are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 05:17:21 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Akkaya", "Ibrahim Batuhan", ""], ["Altinel", "Fazil", ""], ["Halici", "Ugur", ""]]}, {"id": "2106.07166", "submitter": "Yi Yu", "authors": "YiYu and XinyingWang and WeiHu and XunLuo and ChengLi", "title": "2rd Place Solutions in the HC-STVG track of Person in Context Challenge\n  2021", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we present our solution to localize a\nspatio-temporal person in an untrimmed video based on a sentence. We achieve\nthe second vIOU(0.30025) in the HC-STVG track of the 3rd Person in Context(PIC)\nChallenge. Our solution contains three parts: 1) human attributes information\nis extracted from the sentence, it is helpful to filter out tube proposals in\nthe testing phase and supervise our classifier to learn appearance information\nin the training phase. 2) we detect humans with YoloV5 and track humans based\non the DeepSort framework but replace the original ReID network with FastReID.\n3) a visual transformer is used to extract cross-modal representations for\nlocalizing a spatio-temporal tube of the target person.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 05:18:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["YiYu", "", ""], ["XinyingWang", "", ""], ["WeiHu", "", ""], ["XunLuo", "", ""], ["ChengLi", "", ""]]}, {"id": "2106.07186", "submitter": "Usman Cheema", "authors": "Usman Cheema and Seungbin Moon", "title": "Sejong Face Database: A Multi-Modal Disguise Face Database", "comments": "Database Access Link:\n  https://github.com/usmancheema89/SejongFaceDatabase", "journal-ref": "Computer Vision and Image Understanding, Volumes 208-209, 2021", "doi": "10.1016/j.cviu.2021.103218", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial application of facial recognition demands robustness to a variety\nof challenges such as illumination, occlusion, spoofing, disguise, etc.\nDisguised face recognition is one of the emerging issues for access control\nsystems, such as security checkpoints at the borders. However, the lack of\navailability of face databases with a variety of disguise addons limits the\ndevelopment of academic research in the area. In this paper, we present a\nmultimodal disguised face dataset to facilitate the disguised face recognition\nresearch. The presented database contains 8 facial add-ons and 7 additional\ncombinations of these add-ons to create a variety of disguised face images.\nEach facial image is captured in visible, visible plus infrared, infrared, and\nthermal spectra. Specifically, the database contains 100 subjects divided into\nsubset-A (30 subjects, 1 image per modality) and subset-B (70 subjects, 5 plus\nimages per modality). We also present baseline face detection results performed\non the proposed database to provide reference results and compare the\nperformance in different modalities. Qualitative and quantitative analysis is\nperformed to evaluate the challenging nature of disguise addons. The dataset\nwill be publicly available with the acceptance of the research article. The\ndatabase is available at: https://github.com/usmancheema89/SejongFaceDatabase.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 06:29:41 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cheema", "Usman", ""], ["Moon", "Seungbin", ""]]}, {"id": "2106.07190", "submitter": "Young-Ju Choi", "authors": "Young-Ju Choi, Young-Woon Lee, Byung-Gyu Kim", "title": "Group-based Bi-Directional Recurrent Wavelet Neural Networks for Video\n  Super-Resolution", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR) aims to estimate a high-resolution (HR) frame\nfrom a low-resolution (LR) frames. The key challenge for VSR lies in the\neffective exploitation of spatial correlation in an intra-frame and temporal\ndependency between consecutive frames. However, most of the previous methods\ntreat different types of the spatial features identically and extract spatial\nand temporal features from the separated modules. It leads to lack of obtaining\nmeaningful information and enhancing the fine details. In VSR, there are three\ntypes of temporal modeling frameworks: 2D convolutional neural networks (CNN),\n3D CNN, and recurrent neural networks (RNN). Among them, the RNN-based approach\nis suitable for sequential data. Thus the SR performance can be greatly\nimproved by using the hidden states of adjacent frames. However, at each of\ntime step in a recurrent structure, the RNN-based previous works utilize the\nneighboring features restrictively. Since the range of accessible motion per\ntime step is narrow, there are still limitations to restore the missing details\nfor dynamic or large motion. In this paper, we propose a group-based\nbi-directional recurrent wavelet neural networks (GBR-WNN) to exploit the\nsequential data and spatio-temporal information effectively for VSR. The\nproposed group-based bi-directional RNN (GBR) temporal modeling framework is\nbuilt on the well-structured process with the group of pictures (GOP). We\npropose a temporal wavelet attention (TWA) module, in which attention is\nadopted for both spatial and temporal features. Experimental results\ndemonstrate that the proposed method achieves superior performance compared\nwith state-of-the-art methods in both of quantitative and qualitative\nevaluations.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 06:36:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Choi", "Young-Ju", ""], ["Lee", "Young-Woon", ""], ["Kim", "Byung-Gyu", ""]]}, {"id": "2106.07204", "submitter": "Chih-Ting Liu", "authors": "Chih-Ting Liu, Man-Yu Lee, Tsai-Shien Chen, Shao-Yi Chien", "title": "Hard Samples Rectification for Unsupervised Cross-domain Person\n  Re-identification", "comments": "This paper was accepted by IEEE International Conference on Image\n  Processing (ICIP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) has received great success with the\nsupervised learning methods. However, the task of unsupervised cross-domain\nre-ID is still challenging. In this paper, we propose a Hard Samples\nRectification (HSR) learning scheme which resolves the weakness of original\nclustering-based methods being vulnerable to the hard positive and negative\nsamples in the target unlabelled dataset. Our HSR contains two parts, an\ninter-camera mining method that helps recognize a person under different views\n(hard positive) and a part-based homogeneity technique that makes the model\ndiscriminate different persons but with similar appearance (hard negative). By\nrectifying those two hard cases, the re-ID model can learn effectively and\nachieve promising results on two large-scale benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 07:38:42 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Chih-Ting", ""], ["Lee", "Man-Yu", ""], ["Chen", "Tsai-Shien", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "2106.07217", "submitter": "Seulki Park", "authors": "Seulki Park, Dae Ung Jo, and Jin Young Choi", "title": "Over-Fit: Noisy-Label Detection based on the Overfitted Model Property", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing need to handle the noisy label problem in a massive\ndataset, learning with noisy labels has received much attention in recent\nyears. As a promising approach, there have been recent studies to select clean\ntraining data by finding small-loss instances before a deep neural network\noverfits the noisy-label data. However, it is challenging to prevent\noverfitting. In this paper, we propose a novel noisy-label detection algorithm\nby employing the property of overfitting on individual data points. To this\nend, we present two novel criteria that statistically measure how much each\ntraining sample abnormally affects the model and clean validation data. Using\nthe criteria, our iterative algorithm removes noisy-label samples and retrains\nthe model alternately until no further performance improvement is made. In\nexperiments on multiple benchmark datasets, we demonstrate the validity of our\nalgorithm and show that our algorithm outperforms the state-of-the-art methods\nwhen the exact noise rates are not given. Furthermore, we show that our method\ncan not only be expanded to a real-world video dataset but also can be viewed\nas a regularization method to solve problems caused by overfitting.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:04:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Park", "Seulki", ""], ["Jo", "Dae Ung", ""], ["Choi", "Jin Young", ""]]}, {"id": "2106.07218", "submitter": "Niv Giladi", "authors": "Niv Giladi, Zvika Ben-Haim, Sella Nevo, Yossi Matias, Daniel Soudry", "title": "Physics-Aware Downsampling with Deep Learning for Scalable Flood\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Floods are the most common natural disaster in the world,\naffecting the lives of hundreds of millions. Flood forecasting is therefore a\nvitally important endeavor, typically achieved using physical water flow\nsimulations, which rely on accurate terrain elevation maps. However, such\nsimulations, based on solving partial differential equations, are\ncomputationally prohibitive on a large scale. This scalability issue is\ncommonly alleviated using a coarse grid representation of the elevation map,\nthough this representation may distort crucial terrain details, leading to\nsignificant inaccuracies in the simulation. Contributions: We train a deep\nneural network to perform physics-informed downsampling of the terrain map: we\noptimize the coarse grid representation of the terrain maps, so that the flood\nprediction will match the fine grid solution. For the learning process to\nsucceed, we configure a dataset specifically for this task. We demonstrate that\nwith this method, it is possible to achieve a significant reduction in\ncomputational cost, while maintaining an accurate solution. A reference\nimplementation accompanies the paper as well as documentation and code for\ndataset reproduction.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:05:14 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Giladi", "Niv", ""], ["Ben-Haim", "Zvika", ""], ["Nevo", "Sella", ""], ["Matias", "Yossi", ""], ["Soudry", "Daniel", ""]]}, {"id": "2106.07220", "submitter": "Wendong Zhang", "authors": "Wendong Zhang, Junwei Zhu, Ying Tai, Yunbo Wang, Wenqing Chu, Bingbing\n  Ni, Chengjie Wang and Xiaokang Yang", "title": "Context-Aware Image Inpainting with Learned Semantic Priors", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image inpainting have shown impressive results for\ngenerating plausible visual details on rather simple backgrounds. However, for\ncomplex scenes, it is still challenging to restore reasonable contents as the\ncontextual information within the missing regions tends to be ambiguous. To\ntackle this problem, we introduce pretext tasks that are semantically\nmeaningful to estimating the missing contents. In particular, we perform\nknowledge distillation on pretext models and adapt the features to image\ninpainting. The learned semantic priors ought to be partially invariant between\nthe high-level pretext task and low-level image inpainting, which not only help\nto understand the global context but also provide structural guidance for the\nrestoration of local textures. Based on the semantic priors, we further propose\na context-aware image inpainting model, which adaptively integrates global\nsemantics and local features in a unified image generator. The semantic learner\nand the image generator are trained in an end-to-end manner. We name the model\nSPL to highlight its ability to learn and leverage semantic priors. It achieves\nthe state of the art on Places2, CelebA, and Paris StreetView datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:09:43 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhang", "Wendong", ""], ["Zhu", "Junwei", ""], ["Tai", "Ying", ""], ["Wang", "Yunbo", ""], ["Chu", "Wenqing", ""], ["Ni", "Bingbing", ""], ["Wang", "Chengjie", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2106.07224", "submitter": "Rui Su", "authors": "Rui Su, Wenjing Huang, Haoyu Ma, Xiaowei Song, Jinglu Hu", "title": "SGE net: Video object detection with squeezed GRU and information\n  entropy map", "comments": "ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning based video object detection has attracted more and\nmore attention. Compared with object detection of static images, video object\ndetection is more challenging due to the motion of objects, while providing\nrich temporal information. The RNN-based algorithm is an effective way to\nenhance detection performance in videos with temporal information. However,\nmost studies in this area only focus on accuracy while ignoring the calculation\ncost and the number of parameters.\n  In this paper, we propose an efficient method that combines channel-reduced\nconvolutional GRU (Squeezed GRU), and Information Entropy map for video object\ndetection (SGE-Net). The experimental results validate the accuracy\nimprovement, computational savings of the Squeezed GRU, and superiority of the\ninformation entropy attention mechanism on the classification performance. The\nmAP has increased by 3.7 contrasted with the baseline, and the number of\nparameters has decreased from 6.33 million to 0.67 million compared with the\nstandard GRU.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:26:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Su", "Rui", ""], ["Huang", "Wenjing", ""], ["Ma", "Haoyu", ""], ["Song", "Xiaowei", ""], ["Hu", "Jinglu", ""]]}, {"id": "2106.07226", "submitter": "Federica Lago", "authors": "Federica Lago, Cecilia Pasquini, Rainer B\\\"ohme, H\\'el\\`ene Dumont,\n  Val\\'erie Goffaux and Giulia Boato", "title": "More Real than Real: A Study on Human Visual Perception of Synthetic\n  Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep fakes became extremely popular in the last years, also thanks to their\nincreasing realism. Therefore, there is the need to measures human's ability to\ndistinguish between real and synthetic face images when confronted with\ncutting-edge creation technologies. We describe the design and results of a\nperceptual experiment we have conducted, where a wide and diverse group of\nvolunteers has been exposed to synthetic face images produced by\nstate-of-the-art Generative Adversarial Networks (namely, PG-GAN, StyleGAN,\nStyleGAN2). The experiment outcomes reveal how strongly we should call into\nquestion our human ability to discriminate real faces from synthetic ones\ngenerated through modern AI.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:27:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lago", "Federica", ""], ["Pasquini", "Cecilia", ""], ["B\u00f6hme", "Rainer", ""], ["Dumont", "H\u00e9l\u00e8ne", ""], ["Goffaux", "Val\u00e9rie", ""], ["Boato", "Giulia", ""]]}, {"id": "2106.07228", "submitter": "Julien Nyambal", "authors": "Julien Nyambal, Richard Klein", "title": "Automated Parking Space Detection Using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/RoboMech.2017.8261114", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a parking space nowadays becomes an issue that is not to be\nneglected, it consumes time and energy. We have used computer vision techniques\nto infer the state of the parking lot given the data collected from the\nUniversity of The Witwatersrand. This paper presents an approach for a\nreal-time parking space classification based on Convolutional Neural Networks\n(CNN) using Caffe and Nvidia DiGITS framework. The training process has been\ndone using DiGITS and the output is a caffemodel used for predictions to detect\nvacant and occupied parking spots. The system checks a defined area whether a\nparking spot (bounding boxes defined at initialization of the system) is\ncontaining a car or not (occupied or vacant). Those bounding box coordinates\nare saved from a frame of the video of the parking lot in a JSON format, to be\nlater used by the system for sequential prediction on each parking spot. The\nsystem has been trained using the LeNet network with the Nesterov Accelerated\nGradient as solver and the AlexNet network with the Stochastic Gradient Descent\nas solver. We were able to get an accuracy on the validation set of 99\\% for\nboth networks. The accuracy on a foreign dataset(PKLot) returned as well 99\\%.\nThose are experimental results based on the training set shows how robust the\nsystem can be when the prediction has to take place in a different parking\nspace.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:30:38 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Nyambal", "Julien", ""], ["Klein", "Richard", ""]]}, {"id": "2106.07256", "submitter": "Ahmed Hussein", "authors": "Bryan Krauss, Gregory Schroeder, Marko Gustke, Ahmed Hussein", "title": "Deterministic Guided LiDAR Depth Map Completion", "comments": "Submitted to 2021 IEEE Intelligent Vehicles Symposium (IV21). This\n  work has been submitted to the IEEE for possible publication. Copyright may\n  be transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate dense depth estimation is crucial for autonomous vehicles to analyze\ntheir environment. This paper presents a non-deep learning-based approach to\ndensify a sparse LiDAR-based depth map using a guidance RGB image. To achieve\nthis goal the RGB image is at first cleared from most of the camera-LiDAR\nmisalignment artifacts. Afterward, it is over segmented and a plane for each\nsuperpixel is approximated. In the case a superpixel is not well represented by\na plane, a plane is approximated for a convex hull of the most inlier. Finally,\nthe pinhole camera model is used for the interpolation process and the\nremaining areas are interpolated. The evaluation of this work is executed using\nthe KITTI depth completion benchmark, which validates the proposed work and\nshows that it outperforms the state-of-the-art non-deep learning-based methods,\nin addition to several deep learning-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 09:19:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Krauss", "Bryan", ""], ["Schroeder", "Gregory", ""], ["Gustke", "Marko", ""], ["Hussein", "Ahmed", ""]]}, {"id": "2106.07283", "submitter": "Vidit Singh", "authors": "Vidit and Mathieu Salzmann", "title": "Attention-based Domain Adaptation for Single Stage Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While domain adaptation has been used to improve the performance of object\ndetectors when the training and test data follow different distributions,\nprevious work has mostly focused on two-stage detectors. This is because their\nuse of region proposals makes it possible to perform local adaptation, which\nhas been shown to significantly improve the adaptation effectiveness. Here, by\ncontrast, we target single-stage architectures, which are better suited to\nresource-constrained detection than two-stage ones but do not provide region\nproposals. To nonetheless benefit from the strength of local adaptation, we\nintroduce an attention mechanism that lets us identify the important regions on\nwhich adaptation should focus. Our approach is generic and can be integrated\ninto any single-stage detector. We demonstrate this on standard benchmark\ndatasets by applying it to both SSD and YOLO. Furthermore, for an equivalent\nsingle-stage architecture, our method outperforms the state-of-the-art domain\nadaptation technique even though it was designed specifically for this\nparticular detector.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 10:30:44 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Vidit", "", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2106.07286", "submitter": "Daniel Gehrig", "authors": "Stepan Tulyakov, Daniel Gehrig, Stamatios Georgoulis, Julius Erbach,\n  Mathias Gehrig, Yuanyou Li, Davide Scaramuzza", "title": "TimeLens: Event-based Video Frame Interpolation", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art frame interpolation methods generate intermediate frames by\ninferring object motions in the image from consecutive key-frames. In the\nabsence of additional information, first-order approximations, i.e. optical\nflow, must be used, but this choice restricts the types of motions that can be\nmodeled, leading to errors in highly dynamic scenarios. Event cameras are novel\nsensors that address this limitation by providing auxiliary visual information\nin the blind-time between frames. They asynchronously measure per-pixel\nbrightness changes and do this with high temporal resolution and low latency.\nEvent-based frame interpolation methods typically adopt a synthesis-based\napproach, where predicted frame residuals are directly applied to the\nkey-frames. However, while these approaches can capture non-linear motions they\nsuffer from ghosting and perform poorly in low-texture regions with few events.\nThus, synthesis-based and flow-based approaches are complementary. In this\nwork, we introduce Time Lens, a novel indicates equal contribution method that\nleverages the advantages of both. We extensively evaluate our method on three\nsynthetic and two real benchmarks where we show an up to 5.21 dB improvement in\nterms of PSNR over state-of-the-art frame-based and event-based methods.\nFinally, we release a new large-scale dataset in highly dynamic scenarios,\naimed at pushing the limits of existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 10:33:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tulyakov", "Stepan", ""], ["Gehrig", "Daniel", ""], ["Georgoulis", "Stamatios", ""], ["Erbach", "Julius", ""], ["Gehrig", "Mathias", ""], ["Li", "Yuanyou", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2106.07310", "submitter": "Xiangnan Yin", "authors": "Xiangnan Yin, Di Huang, Hongyu Yang, Zehua Fu, Yunhong Wang, Liming\n  Chen", "title": "Pixel Sampling for Style Preserving Face Pose Editing", "comments": null, "journal-ref": "IJCB,2020,pp. 1-10", "doi": "10.1109/IJCB48548.2020.9304867", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing auto-encoder based face pose editing methods primarily focus on\nmodeling the identity preserving ability during pose synthesis, but are less\nable to preserve the image style properly, which refers to the color,\nbrightness, saturation, etc. In this paper, we take advantage of the well-known\nfrontal/profile optical illusion and present a novel two-stage approach to\nsolve the aforementioned dilemma, where the task of face pose manipulation is\ncast into face inpainting. By selectively sampling pixels from the input face\nand slightly adjust their relative locations with the proposed ``Pixel\nAttention Sampling\" module, the face editing result faithfully keeps the\nidentity information as well as the image style unchanged. By leveraging\nhigh-dimensional embedding at the inpainting stage, finer details are\ngenerated. Further, with the 3D facial landmarks as guidance, our method is\nable to manipulate face pose in three degrees of freedom, i.e., yaw, pitch, and\nroll, resulting in more flexible face pose editing than merely controlling the\nyaw angle as usually achieved by the current state-of-the-art. Both the\nqualitative and quantitative evaluations validate the superiority of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 11:29:29 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Yin", "Xiangnan", ""], ["Huang", "Di", ""], ["Yang", "Hongyu", ""], ["Fu", "Zehua", ""], ["Wang", "Yunhong", ""], ["Chen", "Liming", ""]]}, {"id": "2106.07314", "submitter": "Lukas Bommes", "authors": "Lukas Bommes, Tobias Pickel, Claudia Buerhop-Lutz, Jens Hauch,\n  Christoph Brabec, Ian Marius Peters", "title": "Computer Vision Tool for Detection, Mapping and Fault Classification of\n  PV Modules in Aerial IR Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Increasing deployment of photovoltaics (PV) plants demands for cheap and fast\ninspection. A viable tool for this task is thermographic imaging by unmanned\naerial vehicles (UAV). In this work, we develop a computer vision tool for the\nsemi-automatic extraction of PV modules from thermographic UAV videos. We use\nit to curate a dataset containing 4.3 million IR images of 107842 PV modules\nfrom thermographic videos of seven different PV plants. To demonstrate its use\nfor automated PV plant inspection, we train a ResNet-50 to classify ten common\nmodule anomalies with more than 90 % test accuracy. Experiments show that our\ntool generalizes well to different PV plants. It successfully extracts PV\nmodules from 512 out of 561 plant rows. Failures are mostly due to an\ninappropriate UAV trajectory and erroneous module segmentation. Including all\nmanual steps our tool enables inspection of 3.5 MW p to 9 MW p of PV\ninstallations per day, potentially scaling to multi-gigawatt plants due to its\nparallel nature. While we present an effective method for automated PV plant\ninspection, we are also confident that our approach helps to meet the growing\ndemand for large thermographic datasets for machine learning tasks, such as\npower prediction or unsupervised defect identification.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 11:38:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Bommes", "Lukas", ""], ["Pickel", "Tobias", ""], ["Buerhop-Lutz", "Claudia", ""], ["Hauch", "Jens", ""], ["Brabec", "Christoph", ""], ["Peters", "Ian Marius", ""]]}, {"id": "2106.07327", "submitter": "Denny Mattern", "authors": "Denny Mattern, Darya Martyniuk, Henri Willems, Fabian Bergmann, Adrian\n  Paschke", "title": "Variational Quanvolutional Neural Networks with enhanced image encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image classification is an important task in various machine learning\napplications. In recent years, a number of classification methods based on\nquantum machine learning and different quantum image encoding techniques have\nbeen proposed. In this paper, we study the effect of three different quantum\nimage encoding approaches on the performance of a convolution-inspired hybrid\nquantum-classical image classification algorithm called quanvolutional neural\nnetwork (QNN). We furthermore examine the effect of variational - i.e.\ntrainable - quantum circuits on the classification results. Our experiments\nindicate that some image encodings are better suited for variational circuits.\nHowever, our experiments show as well that there is not one best image\nencoding, but that the choice of the encoding depends on the specific\nconstraints of the application.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 12:08:30 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 12:46:28 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Mattern", "Denny", ""], ["Martyniuk", "Darya", ""], ["Willems", "Henri", ""], ["Bergmann", "Fabian", ""], ["Paschke", "Adrian", ""]]}, {"id": "2106.07333", "submitter": "Yusuf Brima", "authors": "Yusuf Brima, Mossadek Hossain Kamal Tushar, Upama Kabir, Tariqul Islam", "title": "Deep Transfer Learning for Brain Magnetic Resonance Image Multi-class\n  Classification", "comments": "This work was carried out as a collaboration between the Department\n  of Computer Science and Engineering -- the University of Dhaka and the\n  National Institute of Neuroscience (NINS), Bangladesh. We created a novel\n  neurological discord dataset of 37 disease categories", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is a principal diagnostic approach used in\nthe field of radiology to create images of the anatomical and physiological\nstructure of patients. MRI is the prevalent medical imaging practice to find\nabnormalities in soft tissues. Traditionally they are analyzed by a radiologist\nto detect abnormalities in soft tissues, especially the brain. The process of\ninterpreting a massive volume of patient's MRI is laborious. Hence, the use of\nMachine Learning methodologies can aid in detecting abnormalities in soft\ntissues with considerable accuracy. In this research, we have curated a novel\ndataset and developed a framework that uses Deep Transfer Learning to perform a\nmulti-classification of tumors in the brain MRI images. In this paper, we\nadopted the Deep Residual Convolutional Neural Network (ResNet50) architecture\nfor the experiments along with discriminative learning techniques to train the\nmodel. Using the novel dataset and two publicly available MRI brain datasets,\nthis proposed approach attained a classification accuracy of 86.40% on the\ncurated dataset, 93.80% on the Harvard Whole Brain Atlas dataset, and 97.05%\naccuracy on the School of Biomedical Engineering dataset. Results of our\nexperiments significantly demonstrate our proposed framework for transfer\nlearning is a potential and effective method for brain tumor\nmulti-classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 12:19:27 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 16:01:46 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Brima", "Yusuf", ""], ["Tushar", "Mossadek Hossain Kamal", ""], ["Kabir", "Upama", ""], ["Islam", "Tariqul", ""]]}, {"id": "2106.07359", "submitter": "Zeyd Boukhers", "authors": "Zeyd Boukhers and Nada Beili and Timo Hartmann and Prantik Goswami and\n  Muhammad Arslan Zafar", "title": "MexPub: Deep Transfer Learning for Metadata Extraction from German\n  Publications", "comments": "A long version of an accepted paper @ JCDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CV cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Extracting metadata from scientific papers can be considered a solved problem\nin NLP due to the high accuracy of state-of-the-art methods. However, this does\nnot apply to German scientific publications, which have a variety of styles and\nlayouts. In contrast to most of the English scientific publications that follow\nstandard and simple layouts, the order, content, position and size of metadata\nin German publications vary greatly among publications. This variety makes\ntraditional NLP methods fail to accurately extract metadata from these\npublications. In this paper, we present a method that extracts metadata from\nPDF documents with different layouts and styles by viewing the document as an\nimage. We used Mask R-CNN that is trained on COCO dataset and finetuned with\nPubLayNet dataset that consists of ~200K PDF snapshots with five basic classes\n(e.g. text, figure, etc). We refine-tuned the model on our proposed synthetic\ndataset consisting of ~30K article snapshots to extract nine patterns (i.e.\nauthor, title, etc). Our synthetic dataset is generated using contents in both\nlanguages German and English and a finite set of challenging templates obtained\nfrom German publications. Our method achieved an average accuracy of around\n$90\\%$ which validates its capability to accurately extract metadata from a\nvariety of PDF documents with challenging templates.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:43:48 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Boukhers", "Zeyd", ""], ["Beili", "Nada", ""], ["Hartmann", "Timo", ""], ["Goswami", "Prantik", ""], ["Zafar", "Muhammad Arslan", ""]]}, {"id": "2106.07368", "submitter": "Lu Yang", "authors": "Lu Yang, Qing Song, Xueshi Xin, Zhiwei Liu", "title": "Quality-Aware Network for Face Parsing", "comments": "2nd place in Short-video Face Parsing Track of The 3rd Person in\n  Context (PIC) Workshop and Challenge at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is a very short technical report, which introduces the solution of the\nTeam BUPT-CASIA for Short-video Face Parsing Track of The 3rd Person in Context\n(PIC) Workshop and Challenge at CVPR 2021.\n  Face parsing has recently attracted increasing interest due to its numerous\napplication potentials. Generally speaking, it has a lot in common with human\nparsing, such as task setting, data characteristics, number of categories and\nso on. Therefore, this work applies state-of-the-art human parsing method to\nface parsing task to explore the similarities and differences between them. Our\nsubmission achieves 86.84% score and wins the 2nd place in the challenge.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 12:40:46 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yang", "Lu", ""], ["Song", "Qing", ""], ["Xin", "Xueshi", ""], ["Liu", "Zhiwei", ""]]}, {"id": "2106.07379", "submitter": "Emanoel Sabidussi", "authors": "E. R. Sabidussi, S. Klein, M. W. A. Caan, S. Bazrafkan, A. J. den\n  Dekker, J. Sijbers, W. J. Niessen, D. H. J. Poot", "title": "Recurrent Inference Machines as inverse problem solvers for MR\n  relaxometry", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose the use of Recurrent Inference Machines (RIMs) to\nperform T1 and T2 mapping. The RIM is a neural network framework that learns an\niterative inference process based on the signal model, similar to conventional\nstatistical methods for quantitative MRI (QMRI), such as the Maximum Likelihood\nEstimator (MLE). This framework combines the advantages of both data-driven and\nmodel-based methods, and, we hypothesize, is a promising tool for QMRI.\nPreviously, RIMs were used to solve linear inverse reconstruction problems.\nHere, we show that they can also be used to optimize non-linear problems and\nestimate relaxometry maps with high precision and accuracy. The developed RIM\nframework is evaluated in terms of accuracy and precision and compared to an\nMLE method and an implementation of the ResNet. The results show that the RIM\nimproves the quality of estimates compared to the other techniques in Monte\nCarlo experiments with simulated data, test-retest analysis of a system\nphantom, and in-vivo scans. Additionally, inference with the RIM is 150 times\nfaster than the MLE, and robustness to (slight) variations of scanning\nparameters is demonstrated. Hence, the RIM is a promising and flexible method\nfor QMRI. Coupled with an open-source training data generation tool, it\npresents a compelling alternative to previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:50:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sabidussi", "E. R.", ""], ["Klein", "S.", ""], ["Caan", "M. W. A.", ""], ["Bazrafkan", "S.", ""], ["Dekker", "A. J. den", ""], ["Sijbers", "J.", ""], ["Niessen", "W. J.", ""], ["Poot", "D. H. J.", ""]]}, {"id": "2106.07395", "submitter": "Cosmin Bonchis", "authors": "Ciprian Orhei, Victor Bogdan, Cosmin Bonchis", "title": "Dilated filters for edge detection algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edges are a basic and fundamental feature in image processing, that are used\ndirectly or indirectly in huge amount of applications. Inspired by the\nexpansion of image resolution and processing power dilated convolution\ntechniques appeared. Dilated convolution have impressive results in machine\nlearning, we discuss here the idea of dilating the standard filters which are\nused in edge detection algorithms. In this work we try to put together all our\nprevious and current results by using instead of the classical convolution\nfilters a dilated one. We compare the results of the edge detection algorithms\nusing the proposed dilation filters with original filters or custom variants.\nExperimental results confirm our statement that dilation of filters have\npositive impact for edge detection algorithms form simple to rather complex\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 12:52:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Orhei", "Ciprian", ""], ["Bogdan", "Victor", ""], ["Bonchis", "Cosmin", ""]]}, {"id": "2106.07409", "submitter": "Xiao Liu", "authors": "Xiao Liu, Xiaofei Si, Jiangtao Xie", "title": "3rd Place Solution for Short-video Face Parsing Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a short technical report introducing the solution of Team Rat for\nShort-video Parsing Face Parsing Track of The 3rd Person in Context (PIC)\nWorkshop and Challenge at CVPR 2021.\n  In this report, we propose an Edge-Aware Network (EANet) that uses edge\ninformation to refine the segmentation edge. To further obtain the finer edge\nresults, we introduce edge attention loss that only compute cross entropy on\nthe edges, it can effectively reduce the classification error around edge and\nget more smooth boundary. Benefiting from the edge information and edge\nattention loss, the proposed EANet achieves 86.16\\% accuracy in the Short-video\nFace Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge,\nranked the third place.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 13:22:19 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 14:18:27 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 11:06:17 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Liu", "Xiao", ""], ["Si", "Xiaofei", ""], ["Xie", "Jiangtao", ""]]}, {"id": "2106.07411", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian\n  Thieringer, Matthias Bethge, Felix A. Wichmann, Wieland Brendel", "title": "Partial success in closing the gap between human and machine vision", "comments": "A preliminary version of this work was presented as Oral at the 2020\n  NeurIPS workshop on \"Shared Visual Representations in Human & Machine\n  Intelligence\" (arXiv:2010.08377)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A few years ago, the first CNN surpassed human performance on ImageNet.\nHowever, it soon became clear that machines lack robustness on more challenging\ntest cases, a major obstacle towards deploying machines \"in the wild\" and\ntowards obtaining better computational models of human visual perception. Here\nwe ask: Are we making progress in closing the gap between human and machine\nvision? To answer this question, we tested human observers on a broad range of\nout-of-distribution (OOD) datasets, adding the \"missing human baseline\" by\nrecording 85,120 psychophysical trials across 90 participants. We then\ninvestigated a range of promising machine learning developments that crucially\ndeviate from standard supervised CNNs along three axes: objective function\n(self-supervised, adversarially trained, CLIP language-image training),\narchitecture (e.g. vision transformers), and dataset size (ranging from 1M to\n1B). Our findings are threefold. (1.) The longstanding robustness gap between\nhumans and CNNs is closing, with the best models now matching or exceeding\nhuman performance on most OOD datasets. (2.) There is still a substantial\nimage-level consistency gap, meaning that humans make different errors than\nmodels. In contrast, most models systematically agree in their categorisation\nerrors, even substantially different ones like contrastive self-supervised vs.\nstandard supervised models. (3.) In many cases, human-to-model consistency\nimproves when training dataset size is increased by one to three orders of\nmagnitude. Our results give reason for cautious optimism: While there is still\nmuch room for improvement, the behavioural difference between human and machine\nvision is narrowing. In order to measure future progress, 17 OOD datasets with\nimage-level human behavioural data are provided as a benchmark here:\nhttps://github.com/bethgelab/model-vs-human/\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 13:23:35 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Geirhos", "Robert", ""], ["Narayanappa", "Kantharaju", ""], ["Mitzkus", "Benjamin", ""], ["Thieringer", "Tizian", ""], ["Bethge", "Matthias", ""], ["Wichmann", "Felix A.", ""], ["Brendel", "Wieland", ""]]}, {"id": "2106.07441", "submitter": "Shiqi Liu", "authors": "Shiqi Liu, Jie Lian, Xuchen Zhan, Cong Liu, Yuze Tian, Hongwei Duan", "title": "Automatically eliminating seam lines with Poisson editing in complex\n  relative radiometric normalization mosaicking scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative radiometric normalization (RRN) mosaicking among multiple remote\nsensing images is crucial for the downstream tasks, including map-making, image\nrecognition, semantic segmentation, and change detection. However, there are\noften seam lines on the mosaic boundary and radiometric contrast left,\nespecially in complex scenarios, making the appearance of mosaic images\nunsightly and reducing the accuracy of the latter classification/recognition\nalgorithms. This paper renders a novel automatical approach to eliminate seam\nlines in complex RRN mosaicking scenarios. It utilizes the histogram matching\non the overlap area to alleviate radiometric contrast, Poisson editing to\nremove the seam lines, and merging procedure to determine the normalization\ntransfer order. Our method can handle the mosaicking seam lines with arbitrary\nshapes and images with extreme topological relationships (with a small\nintersection area). These conditions make the main feathering or blending\nmethods, e.g., linear weighted blending and Laplacian pyramid blending,\nunavailable. In the experiment, our approach visually surpasses the automatic\nmethods without Poisson editing and the manual blurring and feathering method\nusing GIMP software.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:06:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Shiqi", ""], ["Lian", "Jie", ""], ["Zhan", "Xuchen", ""], ["Liu", "Cong", ""], ["Tian", "Yuze", ""], ["Duan", "Hongwei", ""]]}, {"id": "2106.07445", "submitter": "Carl-Johann Simon-Gabriel", "authors": "Carl-Johann Simon-Gabriel and Noman Ahmed Sheikh and Andreas Krause", "title": "PopSkipJump: Decision-Based Attack for Probabilistic Classifiers", "comments": "ICML'21. Code available at https://github.com/cjsg/PopSkipJump . 9\n  pages & 7 figures in main part, 14 pages & 10 figures in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current classifiers are vulnerable to adversarial examples, small input\nperturbations that change the classification output. Many existing attack\nalgorithms cover various settings, from white-box to black-box classifiers, but\ntypically assume that the answers are deterministic and often fail when they\nare not. We therefore propose a new adversarial decision-based attack\nspecifically designed for classifiers with probabilistic outputs. It is based\non the HopSkipJump attack by Chen et al. (2019, arXiv:1904.02144v5 ), a strong\nand query efficient decision-based attack originally designed for deterministic\nclassifiers. Our P(robabilisticH)opSkipJump attack adapts its amount of queries\nto maintain HopSkipJump's original output quality across various noise levels,\nwhile converging to its query efficiency as the noise level decreases. We test\nour attack on various noise models, including state-of-the-art off-the-shelf\nrandomized defenses, and show that they offer almost no extra robustness to\ndecision-based attacks. Code is available at\nhttps://github.com/cjsg/PopSkipJump .\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:13:12 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Simon-Gabriel", "Carl-Johann", ""], ["Sheikh", "Noman Ahmed", ""], ["Krause", "Andreas", ""]]}, {"id": "2106.07448", "submitter": "Eysan Mehrbani", "authors": "Ezsan Mehrbani, Sezedeh Fatemeh Mirhoseini, Noushin Riahi", "title": "A Novel mapping for visual to auditory sensory substitution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CV eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  visual information can be converted into audio stream via sensory\nsubstitution devices in order to give visually impaired people the chance of\nperception of their surrounding easily and simultaneous to performing everyday\ntasks. In this study, visual environmental features namely, coordinate, type of\nobjects and their size are assigned to audio features related to music tones\nsuch as frequency, time duration and note permutations. Results demonstrated\nthat this new method has more training time efficiency in comparison with our\nprevious method named VBTones which sinusoidal tones were applied. Moreover,\nresults in blind object recognition for real objects was achieved 88.05 on\naverage.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:14:50 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Mehrbani", "Ezsan", ""], ["Mirhoseini", "Sezedeh Fatemeh", ""], ["Riahi", "Noushin", ""]]}, {"id": "2106.07470", "submitter": "Amine Bohi", "authors": "Amine Bohi, Guillaume Auzias and Julien Lef\\`evre", "title": "Comparing vector fields across surfaces: interest for characterizing the\n  orientations of cortical folds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG physics.bio-ph physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vectors fields defined on surfaces constitute relevant and useful\nrepresentations but are rarely used. One reason might be that comparing vector\nfields across two surfaces of the same genus is not trivial: it requires to\ntransport the vector fields from the original surfaces onto a common domain. In\nthis paper, we propose a framework to achieve this task by mapping the vector\nfields onto a common space, using some notions of differential geometry. The\nproposed framework enables the computation of statistics on vector fields. We\ndemonstrate its interest in practice with an application on real data with a\nquantitative assessment of the reproducibility of curvature directions that\ndescribe the complex geometry of cortical folding patterns. The proposed\nframework is general and can be applied to different types of vector fields and\nsurfaces, allowing for a large number of high potential applications in medical\nimaging.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:56:44 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Bohi", "Amine", ""], ["Auzias", "Guillaume", ""], ["Lef\u00e8vre", "Julien", ""]]}, {"id": "2106.07477", "submitter": "Ping Li", "authors": "Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, Ping Li", "title": "S$^2$-MLP: Spatial-Shift MLP Architecture for Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, visual Transformer (ViT) and its following works abandon the\nconvolution and exploit the self-attention operation, attaining a comparable or\neven higher accuracy than CNNs. More recently, MLP-Mixer abandons both the\nconvolution and the self-attention operation, proposing an architecture\ncontaining only MLP layers. To achieve cross-patch communications, it devises\nan additional token-mixing MLP besides the channel-mixing MLP. It achieves\npromising results when training on an extremely large-scale dataset. But it\ncannot achieve as outstanding performance as its CNN and ViT counterparts when\ntraining on medium-scale datasets such as ImageNet1K and ImageNet21K. The\nperformance drop of MLP-Mixer motivates us to rethink the token-mixing MLP. We\ndiscover that the token-mixing MLP is a variant of the depthwise convolution\nwith a global reception field and spatial-specific configuration. But the\nglobal reception field and the spatial-specific property make token-mixing MLP\nprone to over-fitting. In this paper, we propose a novel pure MLP architecture,\nspatial-shift MLP (S$^2$-MLP). Different from MLP-Mixer, our S$^2$-MLP only\ncontains channel-mixing MLP. We utilize a spatial-shift operation for\ncommunications between patches. It has a local reception field and is\nspatial-agnostic. It is parameter-free and efficient for computation. The\nproposed S$^2$-MLP attains higher recognition accuracy than MLP-Mixer when\ntraining on ImageNet-1K dataset. Meanwhile, S$^2$-MLP accomplishes as excellent\nperformance as ViT on ImageNet-1K dataset with considerably simpler\narchitecture and fewer FLOPs and parameters.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:05:11 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 17:58:04 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Yu", "Tan", ""], ["Li", "Xu", ""], ["Cai", "Yunfeng", ""], ["Sun", "Mingming", ""], ["Li", "Ping", ""]]}, {"id": "2106.07488", "submitter": "Pei Lv", "authors": "Pei Lv, Jianqi Fan, Xixi Nie, Weiming Dong, Xiaoheng Jiang, Bing Zhou,\n  Mingliang Xu and Changsheng Xu", "title": "User-Guided Personalized Image Aesthetic Assessment based on Deep\n  Reinforcement Learning", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Personalized image aesthetic assessment (PIAA) has recently become a hot\ntopic due to its usefulness in a wide variety of applications such as\nphotography, film and television, e-commerce, fashion design and so on. This\ntask is more seriously affected by subjective factors and samples provided by\nusers. In order to acquire precise personalized aesthetic distribution by small\namount of samples, we propose a novel user-guided personalized image aesthetic\nassessment framework. This framework leverages user interactions to retouch and\nrank images for aesthetic assessment based on deep reinforcement learning\n(DRL), and generates personalized aesthetic distribution that is more in line\nwith the aesthetic preferences of different users. It mainly consists of two\nstages. In the first stage, personalized aesthetic ranking is generated by\ninteractive image enhancement and manual ranking, meanwhile two policy networks\nwill be trained. The images will be pushed to the user for manual retouching\nand simultaneously to the enhancement policy network. The enhancement network\nutilizes the manual retouching results as the optimization goals of DRL. After\nthat, the ranking process performs the similar operations like the retouching\nmentioned before. These two networks will be trained iteratively and\nalternatively to help to complete the final personalized aesthetic assessment\nautomatically. In the second stage, these modified images are labeled with\naesthetic attributes by one style-specific classifier, and then the\npersonalized aesthetic distribution is generated based on the multiple\naesthetic attributes of these images, which conforms to the aesthetic\npreference of users better.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:19:48 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lv", "Pei", ""], ["Fan", "Jianqi", ""], ["Nie", "Xixi", ""], ["Dong", "Weiming", ""], ["Jiang", "Xiaoheng", ""], ["Zhou", "Bing", ""], ["Xu", "Mingliang", ""], ["Xu", "Changsheng", ""]]}, {"id": "2106.07524", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Anastasios Arsenos and Levon Soukissian and\n  Stefanos Kollias", "title": "MIA-COV19D: COVID-19 Detection through 3-D Chest CT Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Early and reliable COVID-19 diagnosis based on chest 3-D CT scans can assist\nmedical specialists in vital circumstances. Deep learning methodologies\nconstitute a main approach for chest CT scan analysis and disease prediction.\nHowever, large annotated databases are necessary for developing deep learning\nmodels that are able to provide COVID-19 diagnosis across various medical\nenvironments in different countries. Due to privacy issues, publicly available\nCOVID-19 CT datasets are highly difficult to obtain, which hinders the research\nand development of AI-enabled diagnosis methods of COVID-19 based on CT scans.\nIn this paper we present the COV19-CT-DB database which is annotated for\nCOVID-19, consisting of about 5,000 3-D CT scans, We have split the database in\ntraining, validation and test datasets. The former two datasets can be used for\ntraining and validation of machine learning models, while the latter will be\nused for evaluation of the developed models. We also present a deep learning\napproach, based on a CNN-RNN network and report its performance on the\nCOVID19-CT-DB database.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:48:14 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:00:36 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Arsenos", "Anastasios", ""], ["Soukissian", "Levon", ""], ["Kollias", "Stefanos", ""]]}, {"id": "2106.07545", "submitter": "Qi Chen", "authors": "Qi Chen, Sourabh Vora and Oscar Beijbom", "title": "PolarStream: Streaming Lidar Object Detection and Segmentation with\n  Polar Pillars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works recognized lidars as an inherently streaming data source and\nshowed that the end-to-end latency of lidar perception models can be reduced\nsignificantly by operating on wedge-shaped point cloud sectors rather then the\nfull point cloud. However, due to use of cartesian coordinate systems these\nmethods represent the sectors as rectangular regions, wasting memory and\ncompute. In this work we propose using a polar coordinate system and make two\nkey improvements on this design. First, we increase the spatial context by\nusing multi-scale padding from neighboring sectors: preceding sector from the\ncurrent scan and/or the following sector from the past scan. Second, we improve\nthe core polar convolutional architecture by introducing feature undistortion\nand range stratified convolutions. Experimental results on the nuScenes dataset\nshow significant improvements over other streaming based methods. We also\nachieve comparable results to existing non-streaming methods but with lower\nlatencies.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:11:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Qi", ""], ["Vora", "Sourabh", ""], ["Beijbom", "Oscar", ""]]}, {"id": "2106.07550", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "Abdul Mueed Hafiz, Shabir Ahmad Parah, Rouf Ul Alam Bhat", "title": "Attention mechanisms and deep learning for machine vision: A survey of\n  the state of the art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of state of the art nature-inspired pure attention based\nmodels i.e. transformers, and their success in natural language processing\n(NLP), their extension to machine vision (MV) tasks was inevitable and much\nfelt. Subsequently, vision transformers (ViTs) were introduced which are giving\nquite a challenge to the established deep learning based machine vision\ntechniques. However, pure attention based models/architectures like\ntransformers require huge data, large training times and large computational\nresources. Some recent works suggest that combinations of these two varied\nfields can prove to build systems which have the advantages of both these\nfields. Accordingly, this state of the art survey paper is introduced which\nhopefully will help readers get useful information about this interesting and\npotential research area. A gentle introduction to attention mechanisms is\ngiven, followed by a discussion of the popular attention based deep\narchitectures. Subsequently, the major categories of the intersection of\nattention mechanisms and deep learning for machine vision (MV) based are\ndiscussed. Afterwards, the major algorithms, issues and trends within the scope\nof the paper are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 10:23:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hafiz", "Abdul Mueed", ""], ["Parah", "Shabir Ahmad", ""], ["Bhat", "Rouf Ul Alam", ""]]}, {"id": "2106.07552", "submitter": "Aakash Kumar", "authors": "Aakash Kumar, Jyoti Kini, Mubarak Shah, Ajmal Mian", "title": "PC-DAN: Point Cloud based Deep Affinity Network for 3D Multi-Object\n  Tracking (Accepted as an extended abstract in JRDB-ACT Workshop at CVPR21)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent times, the scope of LIDAR (Light Detection and Ranging)\nsensor-based technology has spread across numerous fields. It is popularly used\nto map terrain and navigation information into reliable 3D point cloud data,\npotentially revolutionizing the autonomous vehicles and assistive robotic\nindustry. A point cloud is a dense compilation of spatial data in 3D\ncoordinates. It plays a vital role in modeling complex real-world scenes since\nit preserves structural information and avoids perspective distortion, unlike\nimage data, which is the projection of a 3D structure on a 2D plane. In order\nto leverage the intrinsic capabilities of the LIDAR data, we propose a\nPointNet-based approach for 3D Multi-Object Tracking (MOT).\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:36:39 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kumar", "Aakash", ""], ["Kini", "Jyoti", ""], ["Shah", "Mubarak", ""], ["Mian", "Ajmal", ""]]}, {"id": "2106.07554", "submitter": "Rakhmatulin Ildar", "authors": "R. Ildar", "title": "Dataset for eye-tracking tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In recent years many different deep neural networks were developed, but due\nto a large number of layers in deep networks, their training requires a long\ntime and a large number of datasets. Today is popular to use trained deep\nneural networks for various tasks, even for simple ones in which such deep\nnetworks are not required. The well-known deep networks such as YoloV3, SSD,\netc. are intended for tracking and monitoring various objects, therefore their\nweights are heavy and the overall accuracy for a specific task is low.\nEye-tracking tasks need to detect only one object - an iris in a given area.\nTherefore, it is logical to use a neural network only for this task. But the\nproblem is the lack of suitable datasets for training the model. In the\nmanuscript, we presented a dataset that is suitable for training custom models\nof convolutional neural networks for eye-tracking tasks. Using data set data,\neach user can independently pre-train the convolutional neural network models\nfor eye-tracking tasks. This dataset contains annotated 10,000 eye images in an\nextension of 416 by 416 pixels. The table with annotation information shows the\ncoordinates and radius of the eye for each image. This manuscript can be\nconsidered as a guide for the preparation of datasets for eye-tracking devices\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 23:54:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ildar", "R.", ""]]}, {"id": "2106.07556", "submitter": "Sravani Teeparthi", "authors": "Sravani Teeparthi", "title": "Long Term Object Detection and Tracking in Collaborative Learning\n  Environments", "comments": "Master's thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human activity recognition in videos is a challenging problem that has drawn\na lot of interest, particularly when the goal requires the analysis of a large\nvideo database. AOLME project provides a collaborative learning environment for\nmiddle school students to explore mathematics, computer science, and\nengineering by processing digital images and videos. As part of this project,\naround 2200 hours of video data was collected for analysis. Because of the size\nof the dataset, it is hard to analyze all the videos of the dataset manually.\nThus, there is a huge need for reliable computer-based methods that can detect\nactivities of interest. My thesis is focused on the development of accurate\nmethods for detecting and tracking objects in long videos. All the models are\nvalidated on videos from 7 different sessions, ranging from 45 minutes to 90\nminutes. The keyboard detector achieved a very high average precision (AP) of\n92% at 0.5 intersection over union (IoU). Furthermore, a combined system of the\ndetector with a fast tracker KCF (159fps) was developed so that the algorithm\nruns significantly faster without sacrificing accuracy. For a video of 23\nminutes having resolution 858X480 @ 30 fps, the detection alone runs at 4.7Xthe\nreal-time, and the combined algorithm runs at 21Xthe real-time for an average\nIoU of 0.84 and 0.82, respectively. The hand detector achieved average\nprecision (AP) of 72% at 0.5 IoU. The detection results were improved to 81%\nusing optimal data augmentation parameters. The hand detector runs at 4.7Xthe\nreal-time with AP of 81% at 0.5 IoU. The hand detection method was integrated\nwith projections and clustering for accurate proposal generation. This approach\nreduced the number of false-positive hand detections by 80%. The overall hand\ndetection system runs at 4Xthe real-time, capturing all the activity regions of\nthe current collaborative group.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 20:15:14 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Teeparthi", "Sravani", ""]]}, {"id": "2106.07557", "submitter": "Yinglin Zhang", "authors": "Yinglin Zhang, Risa Higashita, Huazhu Fu, Yanwu Xu, Yang Zhang,\n  Haofeng Liu, Jian Zhang, and Jiang Liu", "title": "A Multi-Branch Hybrid Transformer Networkfor Corneal Endothelial Cell\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corneal endothelial cell segmentation plays a vital role inquantifying\nclinical indicators such as cell density, coefficient of variation,and\nhexagonality. However, the corneal endothelium's uneven reflectionand the\nsubject's tremor and movement cause blurred cell edges in theimage, which is\ndifficult to segment, and need more details and contextinformation to release\nthis problem. Due to the limited receptive field oflocal convolution and\ncontinuous downsampling, the existing deep learn-ing segmentation methods\ncannot make full use of global context andmiss many details. This paper\nproposes a Multi-Branch hybrid Trans-former Network (MBT-Net) based on the\ntransformer and body-edgebranch. Firstly, We use the convolutional block to\nfocus on local tex-ture feature extraction and establish long-range\ndependencies over space,channel, and layer by the transformer and residual\nconnection. Besides,We use the body-edge branch to promote local consistency\nand to provideedge position information. On the self-collected dataset\nTM-EM3000 andpublic Alisarine dataset, compared with other State-Of-The-Art\n(SOTA)methods, the proposed method achieves an improvement.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:31:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhang", "Yinglin", ""], ["Higashita", "Risa", ""], ["Fu", "Huazhu", ""], ["Xu", "Yanwu", ""], ["Zhang", "Yang", ""], ["Liu", "Haofeng", ""], ["Zhang", "Jian", ""], ["Liu", "Jiang", ""]]}, {"id": "2106.07558", "submitter": "Jie Xu", "authors": "Jie Xu and Min Ding", "title": "Transparent Model of Unabridged Data (TMUD)", "comments": "6 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in computational power and algorithms have enabled\nunabridged data (e.g., raw images or audio) to be used as input in some models\n(e.g., deep learning). However, the black box nature of such models reduces\ntheir likelihood of adoption by marketing scholars. Our paradigm of analysis,\nthe Transparent Model of Unabridged Data (TMUD), enables researchers to\ninvestigate the inner workings of such black box models by incorporating an ex\nante filtration module and an ex post experimentation module. We empirically\ndemonstrate the TMUD by investigating the role of facial components and sexual\ndimorphism in face perceptions, which have implications for four marketing\ncontexts: advertisement (perceptions of approachability, trustworthiness, and\ncompetence), brand (perceptions of whether a face represents a brand's typical\ncustomer), category (perceptions of whether a face represents a category's\ntypical customer), and customer persona (perceptions of whether a face\nrepresents the persona of a brand's customer segment). Our results reveal new\nand useful findings that enrich the existing literature on face perception,\nmost of which is based on abridged attributes (e.g., width of mouth). The TMUD\nhas great potential to be a useful paradigm for generating theoretical insights\nand may encourage more marketing researchers and practitioners to use\nunabridged data.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 04:04:22 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xu", "Jie", ""], ["Ding", "Min", ""]]}, {"id": "2106.07559", "submitter": "Chengliang Tang", "authors": "Chengliang Tang, Mar\\'ia Uriarte, Helen Jin, Douglas C. Morton, Tian\n  Zheng", "title": "Artificial Perceptual Learning: Image Categorization with Weak\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has achieved much success on supervised learning tasks with\nlarge sets of well-annotated training samples. However, in many practical\nsituations, such strong and high-quality supervision provided by training data\nis unavailable due to the expensive and labor-intensive labeling process.\nAutomatically identifying and recognizing object categories in a large volume\nof unlabeled images with weak supervision remains an important, yet unsolved\nchallenge in computer vision. In this paper, we propose a novel machine\nlearning framework, artificial perceptual learning (APL), to tackle the problem\nof weakly supervised image categorization. The proposed APL framework is\nconstructed using state-of-the-art machine learning algorithms as building\nblocks to mimic the cognitive development process known as infant\ncategorization. We develop and illustrate the proposed framework by\nimplementing a wide-field fine-grain ecological survey of tree species over an\n8,000-hectare area of the El Yunque rainforest in Puerto Rico. It is based on\nunlabeled high-resolution aerial images of the tree canopy. Misplaced\nground-based labels were available for less than 1% of these images, which\nserve as the only weak supervision for this learning framework. We validate the\nproposed framework using a small set of images with high quality human\nannotations and show that the proposed framework attains human-level cognitive\neconomy.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 21:12:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tang", "Chengliang", ""], ["Uriarte", "Mar\u00eda", ""], ["Jin", "Helen", ""], ["Morton", "Douglas C.", ""], ["Zheng", "Tian", ""]]}, {"id": "2106.07561", "submitter": "Yanan Liu", "authors": "Yanan Liu, Jianing Chen, Laurie Bose, Piotr Dudek, Walterio\n  Mayol-Cuevas", "title": "Direct Servo Control from In-Sensor CNN Inference with A Pixel Processor\n  Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work demonstrates direct visual sensory-motor control using high-speed\nCNN inference via a SCAMP-5 Pixel Processor Array (PPA). We demonstrate how\nPPAs are able to efficiently bridge the gap between perception and action. A\nbinary Convolutional Neural Network (CNN) is used for a classic rock, paper,\nscissors classification problem at over 8000 FPS. Control instructions are\ndirectly sent to a servo motor from the PPA according to the CNN's\nclassification result without any other intermediate hardware.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 10:10:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Yanan", ""], ["Chen", "Jianing", ""], ["Bose", "Laurie", ""], ["Dudek", "Piotr", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "2106.07562", "submitter": "Xr L", "authors": "Xiangri Lu, Hongbin Ma, Jingcheng Zhang", "title": "Neural Network Structure Design based on N-Gauss Activation Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that the activation function of the convolutional\nneural network can meet the Lipschitz condition, then the corresponding\nconvolutional neural network structure can be constructed according to the\nscale of the data set, and the data set can be trained more deeply, more\naccurately and more effectively. In this article, we have accepted the\nexperimental results and introduced the core block N-Gauss, N-Gauss, and Swish\n(Conv1, Conv2, FC1) neural network structure design to train MNIST, CIFAR10,\nand CIFAR100 respectively. Experiments show that N-Gauss gives full play to the\nmain role of nonlinear modeling of activation functions, so that deep\nconvolutional neural networks have hierarchical nonlinear mapping learning\ncapabilities. At the same time, the training ability of N-Gauss on simple\none-dimensional channel small data sets is equivalent to the performance of\nReLU and Swish.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 11:16:37 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lu", "Xiangri", ""], ["Ma", "Hongbin", ""], ["Zhang", "Jingcheng", ""]]}, {"id": "2106.07563", "submitter": "Kunxian Shu", "authors": "Gao Xu (1), Yuanpeng Long (2), Siwei Liu (1), Lijia Yang (1), Shimei\n  Xu (3), Xiaoming Yao (1,3), Kunxian Shu (1) ((1) School of Computer Science\n  and Technology, Chongqing Key Laboratory on Big Data for Bio Intelligence,\n  Chongqing University of Posts and Telecommunications, Chongqing, China, (2)\n  School of Economic Information Engineering, Southwestern University of\n  Finance and Economics, Chengdu, China (3) 51yunjian.com, Hetie International\n  Square, Chengdu, Sichuan, China)", "title": "BPLF: A Bi-Parallel Linear Flow Model for Facial Expression Generation\n  from Emotion Set Images", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The flow-based generative model is a deep learning generative model, which\nobtains the ability to generate data by explicitly learning the data\ndistribution. Theoretically its ability to restore data is stronger than other\ngenerative models. However, its implementation has many limitations, including\nlimited model design, too many model parameters and tedious calculation. In\nthis paper, a bi-parallel linear flow model for facial emotion generation from\nemotion set images is constructed, and a series of improvements have been made\nin terms of the expression ability of the model and the convergence speed in\ntraining. The model is mainly composed of several coupling layers superimposed\nto form a multi-scale structure, in which each coupling layer contains 1*1\nreversible convolution and linear operation modules. Furthermore, this paper\nsorted out the current public data set of facial emotion images, made a new\nemotion data, and verified the model through this data set. The experimental\nresults show that, under the traditional convolutional neural network, the\n3-layer 3*3 convolution kernel is more conducive to extracte the features of\nthe face images. The introduction of principal component decomposition can\nimprove the convergence speed of the model.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 09:37:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xu", "Gao", ""], ["Long", "Yuanpeng", ""], ["Liu", "Siwei", ""], ["Yang", "Lijia", ""], ["Xu", "Shimei", ""], ["Yao", "Xiaoming", ""], ["Shu", "Kunxian", ""]]}, {"id": "2106.07564", "submitter": "Kunxian Shu", "authors": "Siwei Liu (1), Yuanpeng Long (2), Gao Xu (1), Lijia Yang (1), Shimei\n  Xu (3), Xiaoming Yao (1,3), Kunxian Shu (1) ((1) School of Computer Science\n  and Technology, Chongqing Key Laboratory on Big Data for Bio Intelligence,\n  Chongqing University of Posts and Telecommunications, Chongqing, China, (2)\n  School of Economic Information Engineering, Southwestern University of\n  Finance and Economics, Chengdu, China, (3) 51yunjian.com, Hetie International\n  Square, Chengdu, Sichuan, China)", "title": "An optimized Capsule-LSTM model for facial expression recognition with\n  video sequences", "comments": "14pages,4 figurews", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To overcome the limitations of convolutional neural network in the process of\nfacial expression recognition, a facial expression recognition model\nCapsule-LSTM based on video frame sequence is proposed. This model is composed\nof three networks includingcapsule encoders, capsule decoders and LSTM network.\nThe capsule encoder extracts the spatial information of facial expressions in\nvideo frames. Capsule decoder reconstructs the images to optimize the network.\nLSTM extracts the temporal information between video frames and analyzes the\ndifferences in expression changes between frames. The experimental results from\nthe MMI dataset show that the Capsule-LSTM model proposed in this paper can\neffectively improve the accuracy of video expression recognition.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:08:05 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Siwei", ""], ["Long", "Yuanpeng", ""], ["Xu", "Gao", ""], ["Yang", "Lijia", ""], ["Xu", "Shimei", ""], ["Yao", "Xiaoming", ""], ["Shu", "Kunxian", ""]]}, {"id": "2106.07565", "submitter": "David Ahmedt-Aristizabal", "authors": "Ziqing Wang, Mohammad Ali Armin, Simon Denman, Lars Petersson, David\n  Ahmedt-Aristizabal", "title": "Video-Based Inpatient Fall Risk Assessment: A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inpatient falls are a serious safety issue in hospitals and healthcare\nfacilities. Recent advances in video analytics for patient monitoring provide a\nnon-intrusive avenue to reduce this risk through continuous activity\nmonitoring. However, in-bed fall risk assessment systems have received less\nattention in the literature. The majority of prior studies have focused on fall\nevent detection, and do not consider the circumstances that may indicate an\nimminent inpatient fall. Here, we propose a video-based system that can monitor\nthe risk of a patient falling, and alert staff of unsafe behaviour to help\nprevent falls before they occur. We propose an approach that leverages recent\nadvances in human localisation and skeleton pose estimation to extract spatial\nfeatures from video frames recorded in a simulated environment. We demonstrate\nthat body positions can be effectively recognised and provide useful evidence\nfor fall risk assessment. This work highlights the benefits of video-based\nmodels for analysing behaviours of interest, and demonstrates how such a system\ncould enable sufficient lead time for healthcare professionals to respond and\naddress patient needs, which is necessary for the development of fall\nintervention programs.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:02:29 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Ziqing", ""], ["Armin", "Mohammad Ali", ""], ["Denman", "Simon", ""], ["Petersson", "Lars", ""], ["Ahmedt-Aristizabal", "David", ""]]}, {"id": "2106.07568", "submitter": "Boris Kovalerchuk", "authors": "Boris Kovalerchuk, Hoang Phan", "title": "Full interpretable machine learning in 2D with inline coordinates", "comments": "8 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a new methodology for machine learning in 2-dimensional\nspace (2-D ML) in inline coordinates. It is a full machine learning approach\nthat does not require to deal with n-dimensional data in n-dimensional space.\nIt allows discovering n-D patterns in 2-D space without loss of n-D information\nusing graph representations of n-D data in 2-D. Specifically, it can be done\nwith the inline based coordinates in different modifications, including static\nand dynamic ones. The classification and regression algorithms based on these\ninline coordinates were introduced. A successful case study based on a\nbenchmark data demonstrated the feasibility of the approach. This approach\nhelps to consolidate further a whole new area of full 2-D machine learning as a\npromising ML methodology. It has advantages of abilities to involve actively\nthe end-users into the discovering of models and their justification. Another\nadvantage is providing interpretable ML models.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:21:06 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 21:19:21 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kovalerchuk", "Boris", ""], ["Phan", "Hoang", ""]]}, {"id": "2106.07582", "submitter": "Eliya Nachmani", "authors": "Eliya Nachmani, Robin San Roman, Lior Wolf", "title": "Non Gaussian Denoising Diffusion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative diffusion processes are an emerging and effective tool for image\nand speech generation. In the existing methods, the underline noise\ndistribution of the diffusion process is Gaussian noise. However, fitting\ndistributions with more degrees of freedom, could help the performance of such\ngenerative models. In this work, we investigate other types of noise\ndistribution for the diffusion process. Specifically, we show that noise from\nGamma distribution provides improved results for image and speech generation.\nMoreover, we show that using a mixture of Gaussian noise variables in the\ndiffusion process improves the performance over a diffusion process that is\nbased on a single distribution. Our approach preserves the ability to\nefficiently sample state in the training diffusion process while using Gamma\nnoise and a mixture of noise.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:42:43 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Nachmani", "Eliya", ""], ["Roman", "Robin San", ""], ["Wolf", "Lior", ""]]}, {"id": "2106.07608", "submitter": "Xinzi He", "authors": "Xinzi He, Jia Guo, Xuzhe Zhang, Hanwen Bi, Sarah Gerard, David Kaczka,\n  Amin Motahari, Eric Hoffman, Joseph Reinhardt, R. Graham Barr, Elsa Angelini,\n  Andrew Laine", "title": "Recursive Refinement Network for Deformable Lung Registration between\n  Exhale and Inhale CT Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Unsupervised learning-based medical image registration approaches have\nwitnessed rapid development in recent years. We propose to revisit a commonly\nignored while simple and well-established principle: recursive refinement of\ndeformation vector fields across scales. We introduce a recursive refinement\nnetwork (RRN) for unsupervised medical image registration, to extract\nmulti-scale features, construct normalized local cost correlation volume and\nrecursively refine volumetric deformation vector fields. RRN achieves state of\nthe art performance for 3D registration of expiratory-inspiratory pairs of CT\nlung scans. On DirLab COPDGene dataset, RRN returns an average Target\nRegistration Error (TRE) of 0.83 mm, which corresponds to a 13% error reduction\nfrom the best result presented in the leaderboard. In addition to comparison\nwith conventional methods, RRN leads to 89% error reduction compared to\ndeep-learning-based peer approaches.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:14:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["He", "Xinzi", ""], ["Guo", "Jia", ""], ["Zhang", "Xuzhe", ""], ["Bi", "Hanwen", ""], ["Gerard", "Sarah", ""], ["Kaczka", "David", ""], ["Motahari", "Amin", ""], ["Hoffman", "Eric", ""], ["Reinhardt", "Joseph", ""], ["Barr", "R. Graham", ""], ["Angelini", "Elsa", ""], ["Laine", "Andrew", ""]]}, {"id": "2106.07615", "submitter": "Dipu Manandhar", "authors": "Dipu Manandhar, Hailin Jin, John Collomosse", "title": "Magic Layouts: Structural Prior for Component Detection in User\n  Interface Designs", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Magic Layouts; a method for parsing screenshots or hand-drawn\nsketches of user interface (UI) layouts. Our core contribution is to extend\nexisting detectors to exploit a learned structural prior for UI designs,\nenabling robust detection of UI components; buttons, text boxes and similar.\nSpecifically we learn a prior over mobile UI layouts, encoding common spatial\nco-occurrence relationships between different UI components. Conditioning\nregion proposals using this prior leads to performance gains on UI layout\nparsing for both hand-drawn UIs and app screenshots, which we demonstrate\nwithin the context an interactive application for rapidly acquiring digital\nprototypes of user experience (UX) designs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:20:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Manandhar", "Dipu", ""], ["Jin", "Hailin", ""], ["Collomosse", "John", ""]]}, {"id": "2106.07617", "submitter": "Chongzhi Zhang", "authors": "Chongzhi Zhang, Mingyuan Zhang, Shanghang Zhang, Daisheng Jin, Qiang\n  Zhou, Zhongang Cai, Haiyu Zhao, Shuai Yi, Xianglong Liu, Ziwei Liu", "title": "Delving Deep into the Generalization of Vision Transformers under\n  Distribution Shifts", "comments": "Our code is available at\n  https://github.com/Phoenix1153/ViT_OOD_generalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Vision Transformers (ViTs) have achieved impressive results on\nvarious vision tasks. Yet, their generalization ability under different\ndistribution shifts is rarely understood. In this work, we provide a\ncomprehensive study on the out-of-distribution generalization of ViTs. To\nsupport a systematic investigation, we first present a taxonomy of distribution\nshifts by categorizing them into five conceptual groups: corruption shift,\nbackground shift, texture shift, destruction shift, and style shift. Then we\nperform extensive evaluations of ViT variants under different groups of\ndistribution shifts and compare their generalization ability with CNNs. Several\nimportant observations are obtained: 1) ViTs generalize better than CNNs under\nmultiple distribution shifts. With the same or fewer parameters, ViTs are ahead\nof corresponding CNNs by more than 5% in top-1 accuracy under most distribution\nshifts. 2) Larger ViTs gradually narrow the in-distribution and\nout-of-distribution performance gap. To further improve the generalization of\nViTs, we design the Generalization-Enhanced ViTs by integrating adversarial\nlearning, information theory, and self-supervised learning. By investigating\nthree types of generalization-enhanced ViTs, we observe their\ngradient-sensitivity and design a smoother learning strategy to achieve a\nstable training process. With modified training schemes, we achieve\nimprovements on performance towards out-of-distribution data by 4% from vanilla\nViTs. We comprehensively compare three generalization-enhanced ViTs with their\ncorresponding CNNs, and observe that: 1) For the enhanced model, larger ViTs\nstill benefit more for the out-of-distribution generalization. 2)\ngeneralization-enhanced ViTs are more sensitive to the hyper-parameters than\ncorresponding CNNs. We hope our comprehensive study could shed light on the\ndesign of more generalizable learning architectures.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:21:41 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 16:48:04 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zhang", "Chongzhi", ""], ["Zhang", "Mingyuan", ""], ["Zhang", "Shanghang", ""], ["Jin", "Daisheng", ""], ["Zhou", "Qiang", ""], ["Cai", "Zhongang", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""], ["Liu", "Xianglong", ""], ["Liu", "Ziwei", ""]]}, {"id": "2106.07627", "submitter": "Laura E. Brandt", "authors": "Laura E. Brandt and William T. Freeman", "title": "Toward Automatic Interpretation of 3D Plots", "comments": "16 pages, 12 figures, accepted to the 16th International Conference\n  on Document Analysis and Recognition (ICDAR21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the challenge of teaching a machine how to\nreverse-engineer the grid-marked surfaces used to represent data in 3D surface\nplots of two-variable functions. These are common in scientific and economic\npublications; and humans can often interpret them with ease, quickly gleaning\ngeneral shape and curvature information from the simple collection of curves.\nWhile machines have no such visual intuition, they do have the potential to\naccurately extract the more detailed quantitative data that guided the\nsurface's construction. We approach this problem by synthesizing a new dataset\nof 3D grid-marked surfaces (SurfaceGrid) and training a deep neural net to\nestimate their shape. Our algorithm successfully recovers shape information\nfrom synthetic 3D surface plots that have had axes and shading information\nremoved, been rendered with a variety of grid types, and viewed from a range of\nviewpoints.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:32:53 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Brandt", "Laura E.", ""], ["Freeman", "William T.", ""]]}, {"id": "2106.07631", "submitter": "Long Zhao", "authors": "Long Zhao, Zizhao Zhang, Ting Chen, Dimitris N. Metaxas, Han Zhang", "title": "Improved Transformer for High-Resolution GANs", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based models, exemplified by the Transformer, can effectively model\nlong range dependency, but suffer from the quadratic complexity of\nself-attention operation, making them difficult to be adopted for\nhigh-resolution image generation based on Generative Adversarial Networks\n(GANs). In this paper, we introduce two key ingredients to Transformer to\naddress this challenge. First, in low-resolution stages of the generative\nprocess, standard global self-attention is replaced with the proposed\nmulti-axis blocked self-attention which allows efficient mixing of local and\nglobal attention. Second, in high-resolution stages, we drop self-attention\nwhile only keeping multi-layer perceptrons reminiscent of the implicit neural\nfunction. To further improve the performance, we introduce an additional\nself-modulation component based on cross-attention. The resulting model,\ndenoted as HiT, has a linear computational complexity with respect to the image\nsize and thus directly scales to synthesizing high definition images. We show\nin the experiments that the proposed HiT achieves state-of-the-art FID scores\nof 31.87 and 2.95 on unconditional ImageNet $128 \\times 128$ and FFHQ $256\n\\times 256$, respectively, with a reasonable throughput. We believe the\nproposed HiT is an important milestone for generators in GANs which are\ncompletely free of convolutions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:39:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhao", "Long", ""], ["Zhang", "Zizhao", ""], ["Chen", "Ting", ""], ["Metaxas", "Dimitris N.", ""], ["Zhang", "Han", ""]]}, {"id": "2106.07643", "submitter": "Deepak Pathak", "authors": "Boyuan Chen, Pieter Abbeel, Deepak Pathak", "title": "Unsupervised Learning of Visual 3D Keypoints for Control", "comments": "Accepted at ICML 2021. Videos and code at\n  https://buoyancy99.github.io/unsup-3d-keypoints/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning sensorimotor control policies from high-dimensional images crucially\nrelies on the quality of the underlying visual representations. Prior works\nshow that structured latent space such as visual keypoints often outperforms\nunstructured representations for robotic control. However, most of these\nrepresentations, whether structured or unstructured are learned in a 2D space\neven though the control tasks are usually performed in a 3D environment. In\nthis work, we propose a framework to learn such a 3D geometric structure\ndirectly from images in an end-to-end unsupervised manner. The input images are\nembedded into latent 3D keypoints via a differentiable encoder which is trained\nto optimize both a multi-view consistency loss and downstream task objective.\nThese discovered 3D keypoints tend to meaningfully capture robot joints as well\nas object movements in a consistent manner across both time and 3D space. The\nproposed approach outperforms prior state-of-art methods across a variety of\nreinforcement learning benchmarks. Code and videos at\nhttps://buoyancy99.github.io/unsup-3d-keypoints/\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:59:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Boyuan", ""], ["Abbeel", "Pieter", ""], ["Pathak", "Deepak", ""]]}, {"id": "2106.07696", "submitter": "Athira Nambiar Ph.D.", "authors": "Sinzith Tatikonda, Athira Nambiar and Anurag Mittal", "title": "Face Age Progression With Attribute Manipulation", "comments": "-", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face is one of the predominant means of person recognition. In the process of\nageing, human face is prone to many factors such as time, attributes, weather\nand other subject specific variations. The impact of these factors were not\nwell studied in the literature of face aging. In this paper, we propose a novel\nholistic model in this regard viz., ``Face Age progression With Attribute\nManipulation (FAWAM)\", i.e. generating face images at different ages while\nsimultaneously varying attributes and other subject specific characteristics.\nWe address the task in a bottom-up manner, as two submodules i.e. face age\nprogression and face attribute manipulation. For face aging, we use an\nattribute-conscious face aging model with a pyramidal generative adversarial\nnetwork that can model age-specific facial changes while maintaining intrinsic\nsubject specific characteristics. For facial attribute manipulation, the age\nprocessed facial image is manipulated with desired attributes while preserving\nother details unchanged, leveraging an attribute generative adversarial network\narchitecture. We conduct extensive analysis in standard large scale datasets\nand our model achieves significant performance both quantitatively and\nqualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 18:26:48 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Tatikonda", "Sinzith", ""], ["Nambiar", "Athira", ""], ["Mittal", "Anurag", ""]]}, {"id": "2106.07708", "submitter": "Geoffrey H Tison", "authors": "Robert Avram, Jeffrey E. Olgin, Alvin Wan, Zeeshan Ahmed, Louis\n  Verreault-Julien, Sean Abreau, Derek Wan, Joseph E. Gonzalez, Derek Y. So,\n  Krishan Soni, Geoffrey H. Tison", "title": "CathAI: Fully Automated Interpretation of Coronary Angiograms Using\n  Neural Networks", "comments": "62 pages, 3 main figures, 2 main tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Coronary heart disease (CHD) is the leading cause of adult death in the\nUnited States and worldwide, and for which the coronary angiography procedure\nis the primary gateway for diagnosis and clinical management decisions. The\nstandard-of-care for interpretation of coronary angiograms depends upon ad-hoc\nvisual assessment by the physician operator. However, ad-hoc visual\ninterpretation of angiograms is poorly reproducible, highly variable and bias\nprone. Here we show for the first time that fully-automated angiogram\ninterpretation to estimate coronary artery stenosis is possible using a\nsequence of deep neural network algorithms. The algorithmic pipeline we\ndeveloped--called CathAI--achieves state-of-the art performance across the\nsequence of tasks required to accomplish automated interpretation of\nunselected, real-world angiograms. CathAI (Algorithms 1-2) demonstrated\npositive predictive value, sensitivity and F1 score of >=90% to identify the\nprojection angle overall and >=93% for left or right coronary artery angiogram\ndetection, the primary anatomic structures of interest. To predict obstructive\ncoronary artery stenosis (>=70% stenosis), CathAI (Algorithm 4) exhibited an\narea under the receiver operating characteristic curve (AUC) of 0.862 (95% CI:\n0.843-0.880). When externally validated in a healthcare system in another\ncountry, CathAI AUC was 0.869 (95% CI: 0.830-0.907) to predict obstructive\ncoronary artery stenosis. Our results demonstrate that multiple purpose-built\nneural networks can function in sequence to accomplish the complex series of\ntasks required for automated analysis of real-world angiograms. Deployment of\nCathAI may serve to increase standardization and reproducibility in coronary\nstenosis assessment, while providing a robust foundation to accomplish future\ntasks for algorithmic angiographic interpretation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 18:58:09 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Avram", "Robert", ""], ["Olgin", "Jeffrey E.", ""], ["Wan", "Alvin", ""], ["Ahmed", "Zeeshan", ""], ["Verreault-Julien", "Louis", ""], ["Abreau", "Sean", ""], ["Wan", "Derek", ""], ["Gonzalez", "Joseph E.", ""], ["So", "Derek Y.", ""], ["Soni", "Krishan", ""], ["Tison", "Geoffrey H.", ""]]}, {"id": "2106.07714", "submitter": "Gianni Franchi", "authors": "Yufei Hu, Nacim Belkhir, Jesus Angulo, Angela Yao, Gianni Franchi", "title": "Learning Deep Morphological Networks with Neural Architecture Search", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) are generated by sequentially performing linear\nand non-linear processes. Using a combination of linear and non-linear\nprocedures is critical for generating a sufficiently deep feature space. The\nmajority of non-linear operators are derivations of activation functions or\npooling functions. Mathematical morphology is a branch of mathematics that\nprovides non-linear operators for a variety of image processing problems. We\ninvestigate the utility of integrating these operations in an end-to-end deep\nlearning framework in this paper. DNNs are designed to acquire a realistic\nrepresentation for a particular job. Morphological operators give topological\ndescriptors that convey salient information about the shapes of objects\ndepicted in images. We propose a method based on meta-learning to incorporate\nmorphological operators into DNNs. The learned architecture demonstrates how\nour novel morphological operations significantly increase DNN performance on\nvarious tasks, including picture classification and edge detection.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:19:48 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Hu", "Yufei", ""], ["Belkhir", "Nacim", ""], ["Angulo", "Jesus", ""], ["Yao", "Angela", ""], ["Franchi", "Gianni", ""]]}, {"id": "2106.07732", "submitter": "Changan Chen", "authors": "Changan Chen, Wei Sun, David Harwath, Kristen Grauman", "title": "Learning Audio-Visual Dereverberation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reverberation from audio reflecting off surfaces and objects in the\nenvironment not only degrades the quality of speech for human perception, but\nalso severely impacts the accuracy of automatic speech recognition. Prior work\nattempts to remove reverberation based on the audio modality only. Our idea is\nto learn to dereverberate speech from audio-visual observations. The visual\nenvironment surrounding a human speaker reveals important cues about the room\ngeometry, materials, and speaker location, all of which influence the precise\nreverberation effects in the audio stream. We introduce Visually-Informed\nDereverberation of Audio (VIDA), an end-to-end approach that learns to remove\nreverberation based on both the observed sounds and visual scene. In support of\nthis new task, we develop a large-scale dataset that uses realistic acoustic\nrenderings of speech in real-world 3D scans of homes offering a variety of room\nacoustics. Demonstrating our approach on both simulated and real imagery for\nspeech enhancement, speech recognition, and speaker identification, we show it\nachieves state-of-the-art performance and substantially improves over\ntraditional audio-only methods. Project page:\nhttp://vision.cs.utexas.edu/projects/learning-audio-visual-dereverberation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 20:01:24 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chen", "Changan", ""], ["Sun", "Wei", ""], ["Harwath", "David", ""], ["Grauman", "Kristen", ""]]}, {"id": "2106.07770", "submitter": "Aleksandar Vakanski", "authors": "Sujata Butte, Aleksandar Vakanski, Kasia Duellman, Haotian Wang, Amin\n  Mirkouei", "title": "Potato Crop Stress Identification in Aerial Images using Deep\n  Learning-based Object Detection", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on the application of remote sensing and deep learning-based\nanalysis in precision agriculture demonstrated a potential for improved crop\nmanagement and reduced environmental impacts of agricultural production.\nDespite the promising results, the practical relevance of these technologies\nfor actual field deployment requires novel algorithms that are customized for\nanalysis of agricultural images and robust to implementation on natural field\nimagery. The paper presents an approach for analyzing aerial images of a potato\ncrop using deep neural networks. The main objective is to demonstrate automated\nspatial recognition of a healthy versus stressed crop at a plant level.\nSpecifically, we examine premature plant senescence resulting in drought stress\non Russet Burbank potato plants. The proposed deep learning model, named\nRetina-UNet-Ag, is a variant of Retina-UNet (Jaeger et al., 2018) and includes\nconnections from low-level semantic dense representation maps to the feature\npyramid network. The paper also introduces a dataset of field images acquired\nwith a Parrot Sequoia camera carried by a Solo unmanned aerial vehicle.\nExperimental validation demonstrated the ability for distinguishing healthy and\nstressed plants in field images, achieving an average Dice score coefficient of\n0.74. A comparison to related state-of-the-art deep learning models for object\ndetection revealed that the presented approach is effective for the task at\nhand. The method applied here is conducive toward the assessment and\nrecognition of potato crop stress (early plant senescence resulting from\ndrought stress in this case) in natural aerial field images collected under\nreal conditions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 21:57:40 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 15:49:52 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Butte", "Sujata", ""], ["Vakanski", "Aleksandar", ""], ["Duellman", "Kasia", ""], ["Wang", "Haotian", ""], ["Mirkouei", "Amin", ""]]}, {"id": "2106.07771", "submitter": "Jian Ren", "authors": "Jian Ren, Menglei Chai, Oliver J. Woodford, Kyle Olszewski, Sergey\n  Tulyakov", "title": "Flow Guided Transformable Bottleneck Networks for Motion Retargeting", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human motion retargeting aims to transfer the motion of one person in a\n\"driving\" video or set of images to another person. Existing efforts leverage a\nlong training video from each target person to train a subject-specific motion\ntransfer model. However, the scalability of such methods is limited, as each\nmodel can only generate videos for the given target subject, and such training\nvideos are labor-intensive to acquire and process. Few-shot motion transfer\ntechniques, which only require one or a few images from a target, have recently\ndrawn considerable attention. Methods addressing this task generally use either\n2D or explicit 3D representations to transfer motion, and in doing so,\nsacrifice either accurate geometric modeling or the flexibility of an\nend-to-end learned representation. Inspired by the Transformable Bottleneck\nNetwork, which renders novel views and manipulations of rigid objects, we\npropose an approach based on an implicit volumetric representation of the image\ncontent, which can then be spatially manipulated using volumetric flow fields.\nWe address the challenging question of how to aggregate information across\ndifferent body poses, learning flow fields that allow for combining content\nfrom the appropriate regions of input images of highly non-rigid human subjects\nperforming complex motions into a single implicit volumetric representation.\nThis allows us to learn our 3D representation solely from videos of moving\npeople. Armed with both 3D object understanding and end-to-end learned\nrendering, this categorically novel representation delivers state-of-the-art\nimage generation quality, as shown by our quantitative and qualitative\nevaluations.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 21:58:30 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ren", "Jian", ""], ["Chai", "Menglei", ""], ["Woodford", "Oliver J.", ""], ["Olszewski", "Kyle", ""], ["Tulyakov", "Sergey", ""]]}, {"id": "2106.07791", "submitter": "Ufuk Efe", "authors": "Ufuk Efe, Kutalmis Gokalp Ince, A. Aydin Alatan", "title": "DFM: A Performance Baseline for Deep Feature Matching", "comments": "CVPR 2021 Image Matching Workshop Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel image matching method is proposed that utilizes learned features\nextracted by an off-the-shelf deep neural network to obtain a promising\nperformance. The proposed method uses pre-trained VGG architecture as a feature\nextractor and does not require any additional training specific to improve\nmatching. Inspired by well-established concepts in the psychology area, such as\nthe Mental Rotation paradigm, an initial warping is performed as a result of a\npreliminary geometric transformation estimate. These estimates are simply based\non dense matching of nearest neighbors at the terminal layer of VGG network\noutputs of the images to be matched. After this initial alignment, the same\napproach is repeated again between reference and aligned images in a\nhierarchical manner to reach a good localization and matching performance. Our\nalgorithm achieves 0.57 and 0.80 overall scores in terms of Mean Matching\nAccuracy (MMA) for 1 pixel and 2 pixels thresholds respectively on Hpatches\ndataset, which indicates a better performance than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 22:55:06 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Efe", "Ufuk", ""], ["Ince", "Kutalmis Gokalp", ""], ["Alatan", "A. Aydin", ""]]}, {"id": "2106.07806", "submitter": "Christopher Bridge", "authors": "Christopher P. Bridge, Chris Gorman, Steven Pieper, Sean W. Doyle,\n  Jochen K. Lennerz, Jayashree Kalpathy-Cramer, David A. Clunie, Andriy Y.\n  Fedorov, Markus D. Herrmann", "title": "Highdicom: A Python library for standardized encoding of image\n  annotations and machine learning model outputs in pathology and radiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Machine learning is revolutionizing image-based diagnostics in pathology and\nradiology. ML models have shown promising results in research settings, but\ntheir lack of interoperability has been a major barrier for clinical\nintegration and evaluation. The DICOM a standard specifies Information Object\nDefinitions and Services for the representation and communication of digital\nimages and related information, including image-derived annotations and\nanalysis results. However, the complexity of the standard represents an\nobstacle for its adoption in the ML community and creates a need for software\nlibraries and tools that simplify working with data sets in DICOM format. Here\nwe present the highdicom library, which provides a high-level application\nprogramming interface for the Python programming language that abstracts\nlow-level details of the standard and enables encoding and decoding of\nimage-derived information in DICOM format in a few lines of Python code. The\nhighdicom library ties into the extensive Python ecosystem for image processing\nand machine learning. Simultaneously, by simplifying creation and parsing of\nDICOM-compliant files, highdicom achieves interoperability with the medical\nimaging systems that hold the data used to train and run ML models, and\nultimately communicate and store model outputs for clinical use. We demonstrate\nthrough experiments with slide microscopy and computed tomography imaging,\nthat, by bridging these two ecosystems, highdicom enables developers to train\nand evaluate state-of-the-art ML models in pathology and radiology while\nremaining compliant with the DICOM standard and interoperable with clinical\nsystems at all stages. To promote standardization of ML research and streamline\nthe ML model development and deployment process, we made the library available\nfree and open-source.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 23:42:48 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Bridge", "Christopher P.", ""], ["Gorman", "Chris", ""], ["Pieper", "Steven", ""], ["Doyle", "Sean W.", ""], ["Lennerz", "Jochen K.", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Clunie", "David A.", ""], ["Fedorov", "Andriy Y.", ""], ["Herrmann", "Markus D.", ""]]}, {"id": "2106.07807", "submitter": "Ashraful Islam", "authors": "Ashraful Islam, Chun-Fu Chen, Rameswar Panda, Leonid Karlinsky,\n  Rogerio Feris, Richard J. Radke", "title": "Dynamic Distillation Network for Cross-Domain Few-Shot Recognition with\n  Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing works in few-shot learning rely on meta-learning the network on\na large base dataset which is typically from the same domain as the target\ndataset. We tackle the problem of cross-domain few-shot learning where there is\na large shift between the base and target domain. The problem of cross-domain\nfew-shot recognition with unlabeled target data is largely unaddressed in the\nliterature. STARTUP was the first method that tackles this problem using\nself-training. However, it uses a fixed teacher pretrained on a labeled base\ndataset to create soft labels for the unlabeled target samples. As the base\ndataset and unlabeled dataset are from different domains, projecting the target\nimages in the class-domain of the base dataset with a fixed pretrained model\nmight be sub-optimal. We propose a simple dynamic distillation-based approach\nto facilitate unlabeled images from the novel/base dataset. We impose\nconsistency regularization by calculating predictions from the weakly-augmented\nversions of the unlabeled images from a teacher network and matching it with\nthe strongly augmented versions of the same images from a student network. The\nparameters of the teacher network are updated as exponential moving average of\nthe parameters of the student network. We show that the proposed network learns\nrepresentation that can be easily adapted to the target domain even though it\nhas not been trained with target-specific classes during the pretraining phase.\nOur model outperforms the current state-of-the art method by 4.4% for 1-shot\nand 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows\ncompetitive performance on traditional in-domain few-shot learning task. Our\ncode will be available at: https://github.com/asrafulashiq/dynamic-cdfsl.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 23:44:34 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Islam", "Ashraful", ""], ["Chen", "Chun-Fu", ""], ["Panda", "Rameswar", ""], ["Karlinsky", "Leonid", ""], ["Feris", "Rogerio", ""], ["Radke", "Richard J.", ""]]}, {"id": "2106.07817", "submitter": "Stefan Winkler", "authors": "Vassilios Vonikakis and Stefan Winkler", "title": "Efficient Facial Expression Analysis For Dimensional Affect Recognition\n  Using Geometric Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite their continued popularity, categorical approaches to affect\nrecognition have limitations, especially in real-life situations. Dimensional\nmodels of affect offer important advantages for the recognition of subtle\nexpressions and more fine-grained analysis. We introduce a simple but effective\nfacial expression analysis (FEA) system for dimensional affect, solely based on\ngeometric features and Partial Least Squares (PLS) regression. The system\njointly learns to estimate Arousal and Valence ratings from a set of facial\nimages. The proposed approach is robust, efficient, and exhibits comparable\nperformance to contemporary deep learning models, while requiring a fraction of\nthe computational resources.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 00:28:16 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Vonikakis", "Vassilios", ""], ["Winkler", "Stefan", ""]]}, {"id": "2106.07822", "submitter": "David McNeely-White", "authors": "David McNeely-White, Ben Sattelberg, Nathaniel Blanchard, Ross\n  Beveridge", "title": "Canonical Face Embeddings", "comments": "10 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present evidence that many common convolutional neural networks (CNNs)\ntrained for face verification learn functions that are nearly equivalent under\nrotation. More specifically, we demonstrate that one face verification model's\nembeddings (i.e. last--layer activations) can be compared directly to another\nmodel's embeddings after only a rotation or linear transformation, with little\nperformance penalty. This finding is demonstrated using IJB-C 1:1 verification\nacross the combinations of ten modern off-the-shelf CNN-based face verification\nmodels which vary in training dataset, CNN architecture, way of using angular\nloss, or some combination of the 3, and achieve a mean true accept rate of 0.96\nat a false accept rate of 0.01. When instead evaluating embeddings generated\nfrom two CNNs, where one CNN's embeddings are mapped with a linear\ntransformation, the mean true accept rate drops to 0.95 using the same\nverification paradigm. Restricting these linear maps to only perform rotation\nproduces a mean true accept rate of 0.91. These mappings' existence suggests\nthat a common representation is learned by models with variation in training or\nstructure. A discovery such as this likely has broad implications, and we\nprovide an application in which face embeddings can be de-anonymized using a\nlimited number of samples.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 00:52:05 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 20:24:30 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["McNeely-White", "David", ""], ["Sattelberg", "Ben", ""], ["Blanchard", "Nathaniel", ""], ["Beveridge", "Ross", ""]]}, {"id": "2106.07833", "submitter": "Zhongyuan Hau", "authors": "Chengzeng You, Zhongyuan Hau, Soteris Demetriou", "title": "Temporal Consistency Checks to Detect LiDAR Spoofing Attacks on\n  Autonomous Vehicle Perception", "comments": "Accepted in 1st Workshop on Security and Privacy for Mobile AI (MAISP\n  2021)", "journal-ref": null, "doi": "10.1145/3469261.3469406", "report-no": null, "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR sensors are used widely in Autonomous Vehicles for better perceiving\nthe environment which enables safer driving decisions. Recent work has\ndemonstrated serious LiDAR spoofing attacks with alarming consequences. In\nparticular, model-level LiDAR spoofing attacks aim to inject fake depth\nmeasurements to elicit ghost objects that are erroneously detected by 3D Object\nDetectors, resulting in hazardous driving decisions. In this work, we explore\nthe use of motion as a physical invariant of genuine objects for detecting such\nattacks. Based on this, we propose a general methodology, 3D Temporal\nConsistency Check (3D-TC2), which leverages spatio-temporal information from\nmotion prediction to verify objects detected by 3D Object Detectors. Our\npreliminary design and implementation of a 3D-TC2 prototype demonstrates very\npromising performance, providing more than 98% attack detection rate with a\nrecall of 91% for detecting spoofed Vehicle (Car) objects, and is able to\nachieve real-time detection at 41Hz\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 01:36:40 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["You", "Chengzeng", ""], ["Hau", "Zhongyuan", ""], ["Demetriou", "Soteris", ""]]}, {"id": "2106.07846", "submitter": "Mingkun Li", "authors": "Mingkun Li, Chun-Guang Li, Jun Guo", "title": "Cluster-guided Asymmetric Contrastive Learning for Unsupervised Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised person re-identification (Re-ID) aims to match pedestrian images\nfrom different camera views in unsupervised setting. Existing methods for\nunsupervised person Re-ID are usually built upon the pseudo labels from\nclustering. However, the quality of clustering depends heavily on the quality\nof the learned features, which are overwhelmingly dominated by the colors in\nimages especially in the unsupervised setting. In this paper, we propose a\nCluster-guided Asymmetric Contrastive Learning (CACL) approach for unsupervised\nperson Re-ID, in which cluster structure is leveraged to guide the feature\nlearning in a properly designed asymmetric contrastive learning framework. To\nbe specific, we propose a novel cluster-level contrastive loss to help the\nsiamese network effectively mine the invariance in feature learning with\nrespect to the cluster structure within and between different data augmentation\nviews, respectively. Extensive experiments conducted on three benchmark\ndatasets demonstrate superior performance of our proposal.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 02:40:22 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Li", "Mingkun", ""], ["Li", "Chun-Guang", ""], ["Guo", "Jun", ""]]}, {"id": "2106.07847", "submitter": "Yujia Bao", "authors": "Yujia Bao, Shiyu Chang, Regina Barzilay", "title": "Learning Stable Classifiers by Transferring Unstable Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study transfer learning in the presence of spurious correlations. We\nexperimentally demonstrate that directly transferring the stable feature\nextractor learned on the source task may not eliminate these biases for the\ntarget task. However, we hypothesize that the unstable features in the source\ntask and those in the target task are directly related. By explicitly informing\nthe target classifier of the source task's unstable features, we can regularize\nthe biases in the target task. Specifically, we derive a representation that\nencodes the unstable features by contrasting different data environments in the\nsource task. On the target task, we cluster data from this representation, and\nachieve robustness by minimizing the worst-case risk across all clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask, outperforming the best baseline by 22.9% in absolute accuracy across 12\ntransfer settings. Our code is available at https://github.com/YujiaBao/Tofu.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 02:41:12 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Bao", "Yujia", ""], ["Chang", "Shiyu", ""], ["Barzilay", "Regina", ""]]}, {"id": "2106.07849", "submitter": "Cody Blakeney", "authors": "Cody Blakeney, Nathaniel Huish, Yan Yan, Ziliang Zong", "title": "Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks\n  with Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years the ubiquitous deployment of AI has posed great concerns in\nregards to algorithmic bias, discrimination, and fairness. Compared to\ntraditional forms of bias or discrimination caused by humans, algorithmic bias\ngenerated by AI is more abstract and unintuitive therefore more difficult to\nexplain and mitigate. A clear gap exists in the current literature on\nevaluating and mitigating bias in pruned neural networks. In this work, we\nstrive to tackle the challenging issues of evaluating, mitigating, and\nexplaining induced bias in pruned neural networks. Our paper makes three\ncontributions. First, we propose two simple yet effective metrics, Combined\nError Variance (CEV) and Symmetric Distance Error (SDE), to quantitatively\nevaluate the induced bias prevention quality of pruned models. Second, we\ndemonstrate that knowledge distillation can mitigate induced bias in pruned\nneural networks, even with unbalanced datasets. Third, we reveal that model\nsimilarity has strong correlations with pruning induced bias, which provides a\npowerful method to explain why bias occurs in pruned neural networks. Our code\nis available at https://github.com/codestar12/pruning-distilation-bias\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 02:59:32 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Blakeney", "Cody", ""], ["Huish", "Nathaniel", ""], ["Yan", "Yan", ""], ["Zong", "Ziliang", ""]]}, {"id": "2106.07852", "submitter": "Zhenyu Zhang Dr.", "authors": "Zhenyu Zhang, Yanhao Ge, Renwang Chen, Ying Tai, Yan Yan, Jian Yang,\n  Chengjie Wang, Jilin Li, Feiyue Huang", "title": "Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo\n  Collection", "comments": "CVPR 2021 Oral, 11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric face modeling aims to reconstruct 3D face only from images\nwithout shape assumptions. While plausible facial details are predicted, the\nmodels tend to over-depend on local color appearance and suffer from ambiguous\nnoise. To address such problem, this paper presents a novel Learning to\nAggregate and Personalize (LAP) framework for unsupervised robust 3D face\nmodeling. Instead of using controlled environment, the proposed method\nimplicitly disentangles ID-consistent and scene-specific face from\nunconstrained photo set. Specifically, to learn ID-consistent face, LAP\nadaptively aggregates intrinsic face factors of an identity based on a novel\ncurriculum learning approach with relaxed consistency loss. To adapt the face\nfor a personalized scene, we propose a novel attribute-refining network to\nmodify ID-consistent face with target attribute and details. Based on the\nproposed method, we make unsupervised 3D face modeling benefit from meaningful\nimage facial structure and possibly higher resolutions. Extensive experiments\non benchmarks show LAP recovers superior or competitive face shape and texture,\ncompared with state-of-the-art (SOTA) methods with or without prior and\nsupervision.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:10:17 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhang", "Zhenyu", ""], ["Ge", "Yanhao", ""], ["Chen", "Renwang", ""], ["Tai", "Ying", ""], ["Yan", "Yan", ""], ["Yang", "Jian", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "2106.07853", "submitter": "Lin Wan", "authors": "Lin Wan, Zongyuan Sun, Qianyan Jing, Yehansen Chen, Lijing Lu, and\n  Zhihang Li", "title": "G2DA: Geometry-Guided Dual-Alignment Learning for RGB-Infrared Person\n  Re-Identification", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-Infrared (IR) person re-identification aims to retrieve\nperson-of-interest from heterogeneous cameras, easily suffering from large\nimage modality discrepancy caused by different sensing wavelength ranges.\nExisting work usually minimizes such discrepancy by aligning domain\ndistribution of global features, while neglecting the intra-modality structural\nrelations between semantic parts. This could result in the network overly\nfocusing on local cues, without considering long-range body part dependencies,\nleading to meaningless region representations. In this paper, we propose a\ngraph-enabled distribution matching solution, dubbed Geometry-Guided\nDual-Alignment (G2DA) learning, for RGB-IR ReID. It can jointly encourage the\ncross-modal consistency between part semantics and structural relations for\nfine-grained modality alignment by solving a graph matching task within a\nmulti-scale skeleton graph that embeds human topology information.\nSpecifically, we propose to build a semantic-aligned complete graph into which\nall cross-modality images can be mapped via a pose-adaptive graph construction\nmechanism. This graph represents extracted whole-part features by nodes and\nexpresses the node-wise similarities with associated edges. To achieve the\ngraph-based dual-alignment learning, an Optimal Transport (OT) based structured\nmetric is further introduced to simultaneously measure point-wise relations and\ngroup-wise structural similarities across modalities. By minimizing the cost of\nan inter-modality transport plan, G2DA can learn a consistent and\ndiscriminative feature subspace for cross-modality image retrieval.\nFurthermore, we advance a Message Fusion Attention (MFA) mechanism to\nadaptively reweight the information flow of semantic propagation, effectively\nstrengthening the discriminability of extracted semantic features.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:14:31 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 02:19:17 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wan", "Lin", ""], ["Sun", "Zongyuan", ""], ["Jing", "Qianyan", ""], ["Chen", "Yehansen", ""], ["Lu", "Lijing", ""], ["Li", "Zhihang", ""]]}, {"id": "2106.07856", "submitter": "Akarsh Prabhakara", "authors": "Diana Zhang, Akarsh Prabhakara, Sirajum Munir, Aswin Sankaranarayanan,\n  Swarun Kumar", "title": "A Hybrid mmWave and Camera System for Long-Range Depth Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NI cs.RO eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  mmWave radars offer excellent depth resolution owing to their high bandwidth\nat mmWave radio frequencies. Yet, they suffer intrinsically from poor angular\nresolution, that is an order-of-magnitude worse than camera systems, and are\ntherefore not a capable 3-D imaging solution in isolation. We propose\nMetamoran, a system that combines the complimentary strengths of radar and\ncamera systems to obtain depth images at high azimuthal resolutions at\ndistances of several tens of meters with high accuracy, all from a single fixed\nvantage point. Metamoran enables rich long-range depth imaging outdoors with\napplications to roadside safety infrastructure, surveillance and wide-area\nmapping. Our key insight is to use the high azimuth resolution from cameras\nusing computer vision techniques, including image segmentation and monocular\ndepth estimation, to obtain object shapes and use these as priors for our novel\nspecular beamforming algorithm. We also design this algorithm to work in\ncluttered environments with weak reflections and in partially occluded\nscenarios. We perform a detailed evaluation of Metamoran's depth imaging and\nsensing capabilities in 200 diverse scenes at a major U.S. city. Our evaluation\nshows that Metamoran estimates the depth of an object up to 60~m away with a\nmedian error of 28~cm, an improvement of 13$\\times$ compared to a naive\nradar+camera baseline and 23$\\times$ compared to monocular depth estimation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:19:35 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhang", "Diana", ""], ["Prabhakara", "Akarsh", ""], ["Munir", "Sirajum", ""], ["Sankaranarayanan", "Aswin", ""], ["Kumar", "Swarun", ""]]}, {"id": "2106.07861", "submitter": "Jae Myung Kim", "authors": "Jae Myung Kim, Junsuk Choe, Zeynep Akata, and Seong Joon Oh", "title": "Keep CALM and Improve Visual Feature Attribution", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class activation mapping, or CAM, has been the cornerstone of feature\nattribution methods for multiple vision tasks. Its simplicity and effectiveness\nhave led to wide applications in the explanation of visual predictions and\nweakly-supervised localization tasks. However, CAM has its own shortcomings.\nThe computation of attribution maps relies on ad-hoc calibration steps that are\nnot part of the training computational graph, making it difficult for us to\nunderstand the real meaning of the attribution values. In this paper, we\nimprove CAM by explicitly incorporating a latent variable encoding the location\nof the cue for recognition in the formulation, thereby subsuming the\nattribution map into the training computational graph. The resulting model,\nclass activation latent mapping, or CALM, is trained with the\nexpectation-maximization algorithm. Our experiments show that CALM identifies\ndiscriminative attributes for image classifiers more accurately than CAM and\nother visual attribution baselines. CALM also shows performance improvements\nover prior arts on the weakly-supervised object localization benchmarks. Our\ncode is available at https://github.com/naver-ai/calm.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:33:25 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kim", "Jae Myung", ""], ["Choe", "Junsuk", ""], ["Akata", "Zeynep", ""], ["Oh", "Seong Joon", ""]]}, {"id": "2106.07862", "submitter": "Zhongzhou Zhang", "authors": "Zhongzhou Zhang, Lei Zhang", "title": "Domain Adaptive SiamRPN++ for Object Tracking in the Wild", "comments": "10 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefit from large-scale training data, recent advances in Siamese-based\nobject tracking have achieved compelling results on the normal sequences.\nWhilst Siamese-based trackers assume training and test data follow an identical\ndistribution. Suppose there is a set of foggy or rainy test sequences, it\ncannot be guaranteed that the trackers trained on the normal images perform\nwell on the data belonging to other domains. The problem of domain shift among\ntraining and test data has already been discussed in object detection and\nsemantic segmentation areas, which, however, has not been investigated for\nvisual tracking. To this end, based on SiamRPN++, we introduce a Domain\nAdaptive SiamRPN++, namely DASiamRPN++, to improve the cross-domain\ntransferability and robustness of a tracker. Inspired by A-distance theory, we\npresent two domain adaptive modules, Pixel Domain Adaptation (PDA) and Semantic\nDomain Adaptation (SDA). The PDA module aligns the feature maps of template and\nsearch region images to eliminate the pixel-level domain shift caused by\nweather, illumination, etc. The SDA module aligns the feature representations\nof the tracking target's appearance to eliminate the semantic-level domain\nshift. PDA and SDA modules reduce the domain disparity by learning domain\nclassifiers in an adversarial training manner. The domain classifiers enforce\nthe network to learn domain-invariant feature representations. Extensive\nexperiments are performed on the standard datasets of two different domains,\nincluding synthetic foggy and TIR sequences, which demonstrate the\ntransferability and domain adaptability of the proposed tracker.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:40:53 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhang", "Zhongzhou", ""], ["Zhang", "Lei", ""]]}, {"id": "2106.07867", "submitter": "Rajesh Kumar", "authors": "Mohit Agrawal and Pragyan Mehrotra and Rajesh Kumar and Rajiv Ratn\n  Shah", "title": "Defending Touch-based Continuous Authentication Systems from Active\n  Adversaries Using Generative Adversarial Networks", "comments": "2021 IEEE International Joint Conference on Biometrics (IJCB), 8\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previous studies have demonstrated that commonly studied (vanilla)\ntouch-based continuous authentication systems (V-TCAS) are susceptible to\npopulation attack. This paper proposes a novel Generative Adversarial Network\nassisted TCAS (G-TCAS) framework, which showed more resilience to the\npopulation attack. G-TCAS framework was tested on a dataset of 117 users who\ninteracted with a smartphone and tablet pair. On average, the increase in the\nfalse accept rates (FARs) for V-TCAS was much higher (22%) than G-TCAS (13%)\nfor the smartphone. Likewise, the increase in the FARs for V-TCAS was 25%\ncompared to G-TCAS (6%) for the tablet.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 04:04:58 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Agrawal", "Mohit", ""], ["Mehrotra", "Pragyan", ""], ["Kumar", "Rajesh", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2106.07873", "submitter": "Vishal Asnani", "authors": "Vishal Asnani, Xi Yin, Tal Hassner, Xiaoming Liu", "title": "Reverse Engineering of Generative Models: Inferring Model\n  Hyperparameters from Generated Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  State-of-the-art (SOTA) Generative Models (GMs) can synthesize\nphoto-realistic images that are hard for humans to distinguish from genuine\nphotos. We propose to perform reverse engineering of GMs to infer the model\nhyperparameters from the images generated by these models. We define a novel\nproblem, \"model parsing\", as estimating GM network architectures and training\nloss functions by examining their generated images -- a task seemingly\nimpossible for human beings. To tackle this problem, we propose a framework\nwith two components: a Fingerprint Estimation Network (FEN), which estimates a\nGM fingerprint from a generated image by training with four constraints to\nencourage the fingerprint to have desired properties, and a Parsing Network\n(PN), which predicts network architecture and loss functions from the estimated\nfingerprints. To evaluate our approach, we collect a fake image dataset with\n$100$K images generated by $100$ GMs. Extensive experiments show encouraging\nresults in parsing the hyperparameters of the unseen models. Finally, our\nfingerprint estimation can be leveraged for deepfake detection and image\nattribution, as we show by reporting SOTA results on both the recent Celeb-DF\nand image attribution benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 04:19:26 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Asnani", "Vishal", ""], ["Yin", "Xi", ""], ["Hassner", "Tal", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2106.07876", "submitter": "Chong Liu", "authors": "Chong Liu and Fengda Zhu and Xiaojun Chang and Xiaodan Liang and\n  Yi-Dong Shen", "title": "Vision-Language Navigation with Random Environmental Mixup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vision-language Navigation (VLN) tasks require an agent to navigate\nstep-by-step while perceiving the visual observations and comprehending a\nnatural language instruction. Large data bias, which is caused by the disparity\nratio between the small data scale and large navigation space, makes the VLN\ntask challenging. Previous works have proposed various data augmentation\nmethods to reduce data bias. However, these works do not explicitly reduce the\ndata bias across different house scenes. Therefore, the agent would overfit to\nthe seen scenes and achieve poor navigation performance in the unseen scenes.\nTo tackle this problem, we propose the Random Environmental Mixup (REM) method,\nwhich generates cross-connected house scenes as augmented data via mixuping\nenvironment. Specifically, we first select key viewpoints according to the room\nconnection graph for each scene. Then, we cross-connect the key views of\ndifferent scenes to construct augmented scenes. Finally, we generate augmented\ninstruction-path pairs in the cross-connected scenes. The experimental results\non benchmark datasets demonstrate that our augmentation data via REM help the\nagent reduce its performance gap between the seen and unseen environment and\nimprove the overall performance, making our model the best existing approach on\nthe standard VLN benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 04:34:26 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Liu", "Chong", ""], ["Zhu", "Fengda", ""], ["Chang", "Xiaojun", ""], ["Liang", "Xiaodan", ""], ["Shen", "Yi-Dong", ""]]}, {"id": "2106.07879", "submitter": "Md Adnan Arefeen", "authors": "Md Adnan Arefeen, Sumaiya Tabassum Nimi, Md Yusuf Sarwar Uddin, Zhu Li", "title": "A Lightweight ReLU-Based Feature Fusion for Aerial Scene Classification", "comments": "To be presented in IEEE ICIP'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a transfer-learning based model construction\ntechnique for the aerial scene classification problem. The core of our\ntechnique is a layer selection strategy, named ReLU-Based Feature Fusion\n(RBFF), that extracts feature maps from a pretrained CNN-based single-object\nimage classification model, namely MobileNetV2, and constructs a model for the\naerial scene classification task. RBFF stacks features extracted from the batch\nnormalization layer of a few selected blocks of MobileNetV2, where the\ncandidate blocks are selected based on the characteristics of the ReLU\nactivation layers present in those blocks. The feature vector is then\ncompressed into a low-dimensional feature space using dimension reduction\nalgorithms on which we train a low-cost SVM classifier for the classification\nof the aerial images. We validate our choice of selected features based on the\nsignificance of the extracted features with respect to our classification\npipeline. RBFF remarkably does not involve any training of the base CNN model\nexcept for a few parameters for the classifier, which makes the technique very\ncost-effective for practical deployments. The constructed model despite being\nlightweight outperforms several recently proposed models in terms of accuracy\nfor a number of aerial scene datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 04:43:41 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Arefeen", "Md Adnan", ""], ["Nimi", "Sumaiya Tabassum", ""], ["Uddin", "Md Yusuf Sarwar", ""], ["Li", "Zhu", ""]]}, {"id": "2106.07880", "submitter": "Insu Han", "authors": "Amir Zandieh, Insu Han, Haim Avron, Neta Shoham, Chaewon Kim, Jinwoo\n  Shin", "title": "Scaling Neural Tangent Kernels via Sketching and Random Features", "comments": "This is a merger of arXiv:2104.01351, arXiv:2104.00415", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide\nneural networks trained under least squares loss by gradient descent. Recent\nworks also report that NTK regression can outperform finitely-wide neural\nnetworks trained on small-scale datasets. However, the computational complexity\nof kernel methods has limited its use in large-scale learning tasks. To\naccelerate learning with NTK, we design a near input-sparsity time\napproximation algorithm for NTK, by sketching the polynomial expansions of\narc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK)\ncan transform any image using a linear runtime in the number of pixels.\nFurthermore, we prove a spectral approximation guarantee for the NTK matrix, by\ncombining random features (based on leverage score sampling) of the arc-cosine\nkernels with a sketching algorithm. We benchmark our methods on various\nlarge-scale regression and classification tasks and show that a linear\nregressor trained on our CNTK features matches the accuracy of exact CNTK on\nCIFAR-10 dataset while achieving 150x speedup.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 04:44:52 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zandieh", "Amir", ""], ["Han", "Insu", ""], ["Avron", "Haim", ""], ["Shoham", "Neta", ""], ["Kim", "Chaewon", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2106.07881", "submitter": "Christian Reul", "authors": "Christian Reul, Christoph Wick, Maximilian N\\\"oth, Andreas B\\\"uttner,\n  Maximilian Wehner, Uwe Springmann", "title": "Mixed Model OCR Training on Historical Latin Script for Out-of-the-Box\n  Recognition and Finetuning", "comments": "submitted to HIP'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to apply Optical Character Recognition (OCR) to historical printings\nof Latin script fully automatically, we report on our efforts to construct a\nwidely-applicable polyfont recognition model yielding text with a Character\nError Rate (CER) around 2% when applied out-of-the-box. Moreover, we show how\nthis model can be further finetuned to specific classes of printings with\nlittle manual and computational effort. The mixed or polyfont model is trained\non a wide variety of materials, in terms of age (from the 15th to the 19th\ncentury), typography (various types of Fraktur and Antiqua), and languages\n(among others, German, Latin, and French). To optimize the results we combined\nestablished techniques of OCR training like pretraining, data augmentation, and\nvoting. In addition, we used various preprocessing methods to enrich the\ntraining data and obtain more robust models. We also implemented a two-stage\napproach which first trains on all available, considerably unbalanced data and\nthen refines the output by training on a selected more balanced subset.\nEvaluations on 29 previously unseen books resulted in a CER of 1.73%,\noutperforming a widely used standard model with a CER of 2.84% by almost 40%.\nTraining a more specialized model for some unseen Early Modern Latin books\nstarting from our mixed model led to a CER of 1.47%, an improvement of up to\n50% compared to training from scratch and up to 30% compared to training from\nthe aforementioned standard model. Our new mixed model is made openly available\nto the community.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 04:51:54 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Reul", "Christian", ""], ["Wick", "Christoph", ""], ["N\u00f6th", "Maximilian", ""], ["B\u00fcttner", "Andreas", ""], ["Wehner", "Maximilian", ""], ["Springmann", "Uwe", ""]]}, {"id": "2106.07903", "submitter": "Changyeon Yoon", "authors": "Jaemoo Choi, Changyeon Yoon, Jeongwoo Bae, Myungjoo Kang", "title": "Robust Out-of-Distribution Detection on Deep Probabilistic Generative\n  Models", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out-of-distribution (OOD) detection is an important task in machine learning\nsystems for ensuring their reliability and safety. Deep probabilistic\ngenerative models facilitate OOD detection by estimating the likelihood of a\ndata sample. However, such models frequently assign a suspiciously high\nlikelihood to a specific outlier. Several recent works have addressed this\nissue by training a neural network with auxiliary outliers, which are generated\nby perturbing the input data. In this paper, we discover that these approaches\nfail for certain OOD datasets. Thus, we suggest a new detection metric that\noperates without outlier exposure. We observe that our metric is robust to\ndiverse variations of an image compared to the previous outlier-exposing\nmethods. Furthermore, our proposed score requires neither auxiliary models nor\nadditional training. Instead, this paper utilizes the likelihood ratio\nstatistic in a new perspective to extract genuine properties from the given\nsingle deep probabilistic generative model. We also apply a novel numerical\napproximation to enable fast implementation. Finally, we demonstrate\ncomprehensive experiments on various probabilistic generative models and show\nthat our method achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:36:10 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Choi", "Jaemoo", ""], ["Yoon", "Changyeon", ""], ["Bae", "Jeongwoo", ""], ["Kang", "Myungjoo", ""]]}, {"id": "2106.07905", "submitter": "Ziheng Jiao", "authors": "Rui Zhang and Ziheng Jiao and Hongyuan Zhang and Xuelong Li", "title": "Non-Gradient Manifold Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) generally takes thousands of iterations to optimize\nvia gradient descent and thus has a slow convergence. In addition, softmax, as\na decision layer, may ignore the distribution information of the data during\nclassification. Aiming to tackle the referred problems, we propose a novel\nmanifold neural network based on non-gradient optimization, i.e., the\nclosed-form solutions. Considering that the activation function is generally\ninvertible, we reconstruct the network via forward ridge regression and low\nrank backward approximation, which achieve the rapid convergence. Moreover, by\nunifying the flexible Stiefel manifold and adaptive support vector machine, we\ndevise the novel decision layer which efficiently fits the manifold structure\nof the data and label information. Consequently, a jointly non-gradient\noptimization method is designed to generate the network with closed-form\nresults. Eventually, extensive experiments validate the superior performance of\nthe model.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:39:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhang", "Rui", ""], ["Jiao", "Ziheng", ""], ["Zhang", "Hongyuan", ""], ["Li", "Xuelong", ""]]}, {"id": "2106.07910", "submitter": "Prasen Sharma", "authors": "Prasen Kumar Sharma, Ira Bisht, Arijit Sur", "title": "Wavelength-based Attributed Deep Neural Network for Underwater Image\n  Restoration", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater images, in general, suffer from low contrast and high color\ndistortions due to the non-uniform attenuation of the light as it propagates\nthrough the water. In addition, the degree of attenuation varies with the\nwavelength resulting in the asymmetric traversing of colors. Despite the\nprolific works for underwater image restoration (UIR) using deep learning, the\nabove asymmetricity has not been addressed in the respective network\nengineering. As the first novelty, this paper shows that attributing the right\nreceptive field size (context) based on the traversing range of the color\nchannel may lead to a substantial performance gain for the task of UIR.\nFurther, it is important to suppress the irrelevant multi-contextual features\nand increase the representational power of the model. Therefore, as a second\nnovelty, we have incorporated an attentive skip mechanism to adaptively refine\nthe learned multi-contextual features. The proposed framework, called Deep\nWaveNet, is optimized using the traditional pixel-wise and feature-based cost\nfunctions. An extensive set of experiments have been carried out to show the\nefficacy of the proposed scheme over existing best-published literature on\nbenchmark datasets. More importantly, we have demonstrated a comprehensive\nvalidation of enhanced images across various high-level vision tasks, e.g.,\nunderwater image semantic segmentation, and diver's 2D pose estimation. A\nsample video to exhibit our real-world performance is available at\n\\url{https://www.youtube.com/watch?v=8qtuegBdfac}.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:47:51 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sharma", "Prasen Kumar", ""], ["Bisht", "Ira", ""], ["Sur", "Arijit", ""]]}, {"id": "2106.07916", "submitter": "Thomas Duboudin", "authors": "Thomas Duboudin (imagine), Emmanuel Dellandr\\'ea, Corentin Abgrall,\n  Gilles H\\'enaff, Liming Chen", "title": "Encouraging Intra-Class Diversity Through a Reverse Contrastive Loss for\n  Better Single-Source Domain Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional deep learning algorithms often fail to generalize when they are\ntested outside of the domain of training data. Because data distributions can\nchange dynamically in real-life applications once a learned model is deployed,\nin this paper we are interested in single-source domain generalization (SDG)\nwhich aims to develop deep learning algorithms able to generalize from a single\ntraining domain where no information about the test domain is available at\ntraining time. Firstly, we design two simple MNISTbased SDG benchmarks, namely\nMNIST Color SDG-MP and MNIST Color SDG-UP, which highlight the two different\nfundamental SDG issues of increasing difficulties: 1) a class-correlated\npattern in the training domain is missing (SDG-MP), or 2) uncorrelated with the\nclass (SDG-UP), in the testing data domain. This is in sharp contrast with the\ncurrent domain generalization (DG) benchmarks which mix up different\ncorrelation and variation factors and thereby make hard to disentangle success\nor failure factors when benchmarking DG algorithms. We further evaluate several\nstate-of-the-art SDG algorithms through our simple benchmark, namely MNIST\nColor SDG-MP, and show that the issue SDG-MP is largely unsolved despite of a\ndecade of efforts in developing DG algorithms. Finally, we also propose a\npartially reversed contrastive loss to encourage intra-class diversity and find\nless strongly correlated patterns, to deal with SDG-MP and show that the\nproposed approach is very effective on our MNIST Color SDG-MP benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:04:39 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Duboudin", "Thomas", "", "imagine"], ["Dellandr\u00e9a", "Emmanuel", ""], ["Abgrall", "Corentin", ""], ["H\u00e9naff", "Gilles", ""], ["Chen", "Liming", ""]]}, {"id": "2106.07927", "submitter": "Boitumelo Ruf", "authors": "Boitumelo Ruf, Jonas Mohrs, Martin Weinmann, Stefan Hinz, J\\\"urgen\n  Beyerer", "title": "ReS2tAC -- UAV-Borne Real-Time SGM Stereo Optimized for Embedded ARM and\n  CUDA Devices", "comments": null, "journal-ref": "Sensors 2021, 21, 3938", "doi": "10.3390/s21113938", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the emergence of low-cost robotic systems, such as unmanned aerial\nvehicle, the importance of embedded high-performance image processing has\nincreased. For a long time, FPGAs were the only processing hardware that were\ncapable of high-performance computing, while at the same time preserving a low\npower consumption, essential for embedded systems. However, the recently\nincreasing availability of embedded GPU-based systems, such as the NVIDIA\nJetson series, comprised of an ARM CPU and a NVIDIA Tegra GPU, allows for\nmassively parallel embedded computing on graphics hardware. With this in mind,\nwe propose an approach for real-time embedded stereo processing on ARM and\nCUDA-enabled devices, which is based on the popular and widely used Semi-Global\nMatching algorithm. In this, we propose an optimization of the algorithm for\nembedded CUDA GPUs, by using massively parallel computing, as well as using the\nNEON intrinsics to optimize the algorithm for vectorized SIMD processing on\nembedded ARM CPUs. We have evaluated our approach with different configurations\non two public stereo benchmark datasets to demonstrate that they can reach an\nerror rate as low as 3.3%. Furthermore, our experiments show that the fastest\nconfiguration of our approach reaches up to 46 FPS on VGA image resolution.\nFinally, in a use-case specific qualitative evaluation, we have evaluated the\npower consumption of our approach and deployed it on the DJI Manifold 2-G\nattached to a DJI Matrix 210v2 RTK unmanned aerial vehicle (UAV), demonstrating\nits suitability for real-time stereo processing onboard a UAV.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:29:25 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ruf", "Boitumelo", ""], ["Mohrs", "Jonas", ""], ["Weinmann", "Martin", ""], ["Hinz", "Stefan", ""], ["Beyerer", "J\u00fcrgen", ""]]}, {"id": "2106.07929", "submitter": "Weichuan Zhang", "authors": "Junfeng Jing, Tian Gao, Weichuan Zhang, Yongsheng Gao, Changming Sun", "title": "Image Feature Information Extraction for Interest Point Detection: A\n  Comprehensive Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest point detection is one of the most fundamental and critical problems\nin computer vision and image processing. In this paper, we carry out a\ncomprehensive review on image feature information (IFI) extraction techniques\nfor interest point detection. To systematically introduce how the existing\ninterest point detection methods extract IFI from an input image, we propose a\ntaxonomy of the IFI extraction techniques for interest point detection.\nAccording to this taxonomy, we discuss different types of IFI extraction\ntechniques for interest point detection. Furthermore, we identify the main\nunresolved issues related to the existing IFI extraction techniques for\ninterest point detection and any interest point detection methods that have not\nbeen discussed before. The existing popular datasets and evaluation standards\nare provided and the performances for eighteen state-of-the-art approaches are\nevaluated and discussed. Moreover, future research directions on IFI extraction\ntechniques for interest point detection are elaborated.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:31:31 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 02:12:28 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 04:44:10 GMT"}, {"version": "v4", "created": "Tue, 6 Jul 2021 15:40:46 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Jing", "Junfeng", ""], ["Gao", "Tian", ""], ["Zhang", "Weichuan", ""], ["Gao", "Yongsheng", ""], ["Sun", "Changming", ""]]}, {"id": "2106.07941", "submitter": "S Deng", "authors": "Sen Deng, Yidan Feng, Mingqiang Wei, Haoran Xie, Yiping Chen, Jonathan\n  Li, Xiao-Ping Zhang and Jing Qin", "title": "Direction-aware Feature-level Frequency Decomposition for Single Image\n  Deraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel direction-aware feature-level frequency decomposition\nnetwork for single image deraining. Compared with existing solutions, the\nproposed network has three compelling characteristics. First, unlike previous\nalgorithms, we propose to perform frequency decomposition at feature-level\ninstead of image-level, allowing both low-frequency maps containing structures\nand high-frequency maps containing details to be continuously refined during\nthe training procedure. Second, we further establish communication channels\nbetween low-frequency maps and high-frequency maps to interactively capture\nstructures from high-frequency maps and add them back to low-frequency maps\nand, simultaneously, extract details from low-frequency maps and send them back\nto high-frequency maps, thereby removing rain streaks while preserving more\ndelicate features in the input image. Third, different from existing algorithms\nusing convolutional filters consistent in all directions, we propose a\ndirection-aware filter to capture the direction of rain streaks in order to\nmore effectively and thoroughly purge the input images of rain streaks. We\nextensively evaluate the proposed approach in three representative datasets and\nexperimental results corroborate our approach consistently outperforms\nstate-of-the-art deraining algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:51:19 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Deng", "Sen", ""], ["Feng", "Yidan", ""], ["Wei", "Mingqiang", ""], ["Xie", "Haoran", ""], ["Chen", "Yiping", ""], ["Li", "Jonathan", ""], ["Zhang", "Xiao-Ping", ""], ["Qin", "Jing", ""]]}, {"id": "2106.07955", "submitter": "Matteo Rizzo", "authors": "Matteo Rizzo, Cristina Conati, Daesik Jang, Hui Hu", "title": "Cascading Convolutional Temporal Colour Constancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Computational Colour Constancy (CCC) consists of estimating the colour of one\nor more illuminants in a scene and using them to remove unwanted chromatic\ndistortions. Much research has focused on illuminant estimation for CCC on\nsingle images, with few attempts of leveraging the temporal information\nintrinsic in sequences of correlated images (e.g., the frames in a video), a\ntask known as Temporal Colour Constancy (TCC). The state-of-the-art for TCC is\nTCCNet, a deep-learning architecture that uses a ConvLSTM for aggregating the\nencodings produced by CNN submodules for each image in a sequence. We extend\nthis architecture with different models obtained by (i) substituting the TCCNet\nsubmodules with C4, the state-of-the-art method for CCC targeting images; (ii)\nadding a cascading strategy to perform an iterative improvement of the estimate\nof the illuminant. We tested our models on the recently released TCC benchmark\nand achieved results that surpass the state-of-the-art. Analyzing the impact of\nthe number of frames involved in illuminant estimation on performance, we show\nthat it is possible to reduce inference time by training the models on few\nselected frames from the sequences while retaining comparable accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 08:17:30 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Rizzo", "Matteo", ""], ["Conati", "Cristina", ""], ["Jang", "Daesik", ""], ["Hu", "Hui", ""]]}, {"id": "2106.07959", "submitter": "Yibo Guo", "authors": "Yibo Guo, Yiming Fan, Zhiyang Xiang, Haidi Wang, Wenhua Meng,\n  Mingliang Xu", "title": "Zero-sample surface defect detection and classification based on\n  semantic feedback neural network", "comments": "28 pages 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Defect detection and classification technology has changed from traditional\nartificial visual inspection to current intelligent automated inspection, but\nmost of the current defect detection methods are training related detection\nmodels based on a data-driven approach, taking into account the difficulty of\ncollecting some sample data in the industrial field. We apply zero-shot\nlearning technology to the industrial field. Aiming at the problem of the\nexisting \"Latent Feature Guide Attribute Attention\" (LFGAA) zero-shot image\nclassification network, the output latent attributes and artificially defined\nattributes are different in the semantic space, which leads to the problem of\nmodel performance degradation, proposed an LGFAA network based on semantic\nfeedback, and improved model performance by constructing semantic embedded\nmodules and feedback mechanisms. At the same time, for the common domain shift\nproblem in zero-shot learning, based on the idea of co-training algorithm using\nthe difference information between different views of data to learn from each\nother, we propose an Ensemble Co-training algorithm, which adaptively reduces\nthe prediction error in image tag embedding from multiple angles. Various\nexperiments conducted on the zero-shot dataset and the cylinder liner dataset\nin the industrial field provide competitive results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 08:26:36 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Guo", "Yibo", ""], ["Fan", "Yiming", ""], ["Xiang", "Zhiyang", ""], ["Wang", "Haidi", ""], ["Meng", "Wenhua", ""], ["Xu", "Mingliang", ""]]}, {"id": "2106.07991", "submitter": "Risheng Liu", "authors": "Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang", "title": "A Value-Function-based Interior-point Method for Non-convex Bi-level\n  Optimization", "comments": "Accepted at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bi-level optimization model is able to capture a wide range of complex\nlearning tasks with practical interest. Due to the witnessed efficiency in\nsolving bi-level programs, gradient-based methods have gained popularity in the\nmachine learning community. In this work, we propose a new gradient-based\nsolution scheme, namely, the Bi-level Value-Function-based Interior-point\nMethod (BVFIM). Following the main idea of the log-barrier interior-point\nscheme, we penalize the regularized value function of the lower level problem\ninto the upper level objective. By further solving a sequence of differentiable\nunconstrained approximation problems, we consequently derive a sequential\nprogramming scheme. The numerical advantage of our scheme relies on the fact\nthat, when gradient methods are applied to solve the approximation problem, we\nsuccessfully avoid computing any expensive Hessian-vector or Jacobian-vector\nproduct. We prove the convergence without requiring any convexity assumption on\neither the upper level or the lower level objective. Experiments demonstrate\nthe efficiency of the proposed BVFIM on non-convex bi-level problems.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:10:40 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Liu", "Risheng", ""], ["Liu", "Xuan", ""], ["Yuan", "Xiaoming", ""], ["Zeng", "Shangzhi", ""], ["Zhang", "Jin", ""]]}, {"id": "2106.07995", "submitter": "Rinu Boney", "authors": "Rinu Boney, Alexander Ilin, Juho Kannala", "title": "End-to-End Learning of Keypoint Representations for Continuous Control\n  from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many control problems that include vision, optimal controls can be\ninferred from the location of the objects in the scene. This information can be\nrepresented using keypoints, which is a list of spatial locations in the input\nimage. Previous works show that keypoint representations learned during\nunsupervised pre-training using encoder-decoder architectures can provide good\nfeatures for control tasks. In this paper, we show that it is possible to learn\nefficient keypoint representations end-to-end, without the need for\nunsupervised pre-training, decoders, or additional losses. Our proposed\narchitecture consists of a differentiable keypoint extractor that feeds the\ncoordinates of the estimated keypoints directly to a soft actor-critic agent.\nThe proposed algorithm yields performance competitive to the state-of-the art\non DeepMind Control Suite tasks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:17:06 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Boney", "Rinu", ""], ["Ilin", "Alexander", ""], ["Kannala", "Juho", ""]]}, {"id": "2106.07998", "submitter": "Matthias Minderer", "authors": "Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis,\n  Xiaohua Zhai, Neil Houlsby, Dustin Tran, Mario Lucic", "title": "Revisiting the Calibration of Modern Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of predictive uncertainty (model calibration) is\nessential for the safe application of neural networks. Many instances of\nmiscalibration in modern neural networks have been reported, suggesting a trend\nthat newer, more accurate models produce poorly calibrated predictions. Here,\nwe revisit this question for recent state-of-the-art image classification\nmodels. We systematically relate model calibration and accuracy, and find that\nthe most recent models, notably those not using convolutions, are among the\nbest calibrated. Trends observed in prior model generations, such as decay of\ncalibration with distribution shift or model size, are less pronounced in\nrecent architectures. We also show that model size and amount of pretraining do\nnot fully explain these differences, suggesting that architecture is a major\ndeterminant of calibration properties.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:24:43 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Minderer", "Matthias", ""], ["Djolonga", "Josip", ""], ["Romijnders", "Rob", ""], ["Hubis", "Frances", ""], ["Zhai", "Xiaohua", ""], ["Houlsby", "Neil", ""], ["Tran", "Dustin", ""], ["Lucic", "Mario", ""]]}, {"id": "2106.08005", "submitter": "Jiankun Chen", "authors": "Jiankun Chen, Xiaolan Qiu, Chibiao Ding, Yirong Wu", "title": "SAR Image Classification Based on Spiking Neural Network through\n  Spike-Time Dependent Plasticity and Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  At present, the Synthetic Aperture Radar (SAR) image classification method\nbased on convolution neural network (CNN) has faced some problems such as poor\nnoise resistance and generalization ability. Spiking neural network (SNN) is\none of the core components of brain-like intelligence and has good application\nprospects. This article constructs a complete SAR image classifier based on\nunsupervised and supervised learning of SNN by using spike sequences with\ncomplex spatio-temporal information. We firstly expound the spiking neuron\nmodel, the receptive field of SNN, and the construction of spike sequence. Then\nwe put forward an unsupervised learning algorithm based on STDP and a\nsupervised learning algorithm based on gradient descent. The average\nclassification accuracy of single layer and bilayer unsupervised learning SNN\nin three categories images on MSTAR dataset is 80.8\\% and 85.1\\%, respectively.\nFurthermore, the convergent output spike sequences of unsupervised learning can\nbe used as teaching signals. Based on the TensorFlow framework, a single layer\nsupervised learning SNN is built from the bottom, and the classification\naccuracy reaches 90.05\\%. By comparing noise resistance and model parameters\nbetween SNNs and CNNs, the effectiveness and outstanding advantages of SNN are\nverified. Code to reproduce our experiments is available at\n\\url{https://github.com/Jiankun-chen/Supervised-SNN-with-GD}.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:36:04 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chen", "Jiankun", ""], ["Qiu", "Xiaolan", ""], ["Ding", "Chibiao", ""], ["Wu", "Yirong", ""]]}, {"id": "2106.08009", "submitter": "Alexander Black", "authors": "Alexander Black, Tu Bui, Long Mai, Hailin Jin, John Collomosse", "title": "Compositional Sketch Search", "comments": "ICIP 2021 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for searching image collections using free-hand\nsketches that describe the appearance and relative positions of multiple\nobjects. Sketch based image retrieval (SBIR) methods predominantly match\nqueries containing a single, dominant object invariant to its position within\nan image. Our work exploits drawings as a concise and intuitive representation\nfor specifying entire scene compositions. We train a convolutional neural\nnetwork (CNN) to encode masked visual features from sketched objects, pooling\nthese into a spatial descriptor encoding the spatial relationships and\nappearances of objects in the composition. Training the CNN backbone as a\nSiamese network under triplet loss yields a metric search embedding for\nmeasuring compositional similarity which may be efficiently leveraged for\nvisual search by applying product quantization.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:38:09 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Black", "Alexander", ""], ["Bui", "Tu", ""], ["Mai", "Long", ""], ["Jin", "Hailin", ""], ["Collomosse", "John", ""]]}, {"id": "2106.08017", "submitter": "Zhao Hengyuan", "authors": "Hengyuan Zhao, Wenhao Wu, Yihao Liu, Dongliang He", "title": "Color2Style: Real-Time Exemplar-Based Image Colorization with\n  Self-Reference Learning and Deep Feature Modulation", "comments": "16 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legacy black-and-white photos are riddled with people's nostalgia and\nglorious memories of the past. To better relive the elapsed frozen moments, in\nthis paper, we present a deep exemplar-based image colorization approach named\nColor2Style to resurrect these grayscale image media by filling them with\nvibrant colors. Generally, for exemplar-based colorization, unsupervised and\nunpaired training are usually adopted, due to the difficulty of obtaining input\nand ground truth image pairs. To train an exemplar-based colorization model,\ncurrent algorithms usually strive to achieve two procedures: i) retrieving a\nlarge number of reference images with high similarity in advance, which is\ninevitably time-consuming and tedious; ii) designing complicated modules to\ntransfer the colors of the reference image to the grayscale image, by\ncalculating and leveraging the deep semantic correspondence between them (e.g.,\nnon-local operation). Contrary to the previous methods, we solve and simplify\nthe above two steps in one end-to-end learning procedure. First, we adopt a\nself-augmented self-reference training scheme, where the reference image is\ngenerated by graphical transformations from the original colorful one whereby\nthe training can be formulated in a paired manner. Second, instead of computing\ncomplex and inexplicable correspondence maps, our method exploits a simple yet\neffective deep feature modulation (DFM) module, which injects the color\nembeddings extracted from the reference image into the deep representations of\nthe input grayscale image. Such design is much more lightweight and\nintelligible, achieving appealing performance with real-time processing speed.\nMoreover, our model does not require multifarious loss functions and\nregularization terms like existing methods, but only two widely used loss\nfunctions. Codes and models will be available at\nhttps://github.com/zhaohengyuan1/Color2Style.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:05:58 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 05:46:23 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhao", "Hengyuan", ""], ["Wu", "Wenhao", ""], ["Liu", "Yihao", ""], ["He", "Dongliang", ""]]}, {"id": "2106.08021", "submitter": "Soumyasis Gun", "authors": "Prathyusha Akundi, Soumyasis Gun, Jayanthi Sivaswamy", "title": "A Clinically Inspired Approach for Melanoma classification", "comments": "5 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Melanoma is a leading cause of deaths due to skin cancer deaths and hence,\nearly and effective diagnosis of melanoma is of interest. Current approaches\nfor automated diagnosis of melanoma either use pattern recognition or\nanalytical recognition like ABCDE (asymmetry, border, color, diameter and\nevolving) criterion. In practice however, a differential approach wherein\noutliers (ugly duckling) are detected and used to evaluate nevi/lesions.\nIncorporation of differential recognition in Computer Aided Diagnosis (CAD)\nsystems has not been explored but can be beneficial as it can provide a\nclinical justification for the derived decision. We present a method for\nidentifying and quantifying ugly ducklings by performing Intra-Patient\nComparative Analysis (IPCA) of neighboring nevi. This is then incorporated in a\nCAD system design for melanoma detection. This design ensures flexibility to\nhandle cases where IPCA is not possible. Our experiments on a public dataset\nshow that the outlier information helps boost the sensitivity of detection by\nat least 4.1 % and specificity by 4.0 % to 8.9 %, depending on the use of a\nstrong (EfficientNet) or moderately strong (VGG or ResNet) classifier.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:12:24 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Akundi", "Prathyusha", ""], ["Gun", "Soumyasis", ""], ["Sivaswamy", "Jayanthi", ""]]}, {"id": "2106.08038", "submitter": "Arsenii Ashukha", "authors": "Arsenii Ashukha, Andrei Atanov, Dmitry Vetrov", "title": "Mean Embeddings with Test-Time Data Augmentation for Ensembling of\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Averaging predictions over a set of models -- an ensemble -- is widely used\nto improve predictive performance and uncertainty estimation of deep learning\nmodels. At the same time, many machine learning systems, such as search,\nmatching, and recommendation systems, heavily rely on embeddings.\nUnfortunately, due to misalignment of features of independently trained models,\nembeddings, cannot be improved with a naive deep ensemble like approach. In\nthis work, we look at the ensembling of representations and propose mean\nembeddings with test-time augmentation (MeTTA) simple yet well-performing\nrecipe for ensembling representations. Empirically we demonstrate that MeTTA\nsignificantly boosts the quality of linear evaluation on ImageNet for both\nsupervised and self-supervised models. Even more exciting, we draw connections\nbetween MeTTA, image retrieval, and transformation invariant models. We believe\nthat spreading the success of ensembles to inference higher-quality\nrepresentations is the important step that will open many new applications of\nensembling.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:49:46 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 16:22:21 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ashukha", "Arsenii", ""], ["Atanov", "Andrei", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "2106.08042", "submitter": "Boris Tseytlin Mr", "authors": "Boris Tseytlin and Ilya Makarov", "title": "Hotel Recognition via Latent Image Embedding", "comments": "IWANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We approach the problem of hotel recognition with deep metric learning. We\noverview the existing approaches and propose a modification to Contrastive loss\ncalled Contrastive-Triplet loss. We construct a robust pipeline for\nbenchmarking metric learning models and perform experiments on Hotels-50K and\nCUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval\non Hotels-50k. We open-source our code.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:52:07 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Tseytlin", "Boris", ""], ["Makarov", "Ilya", ""]]}, {"id": "2106.08045", "submitter": "Timon H\\\"ofer", "authors": "Timon H\\\"ofer, Faranak Shamsafar, Nuri Benbarka and Andreas Zell", "title": "Object detection and Autoencoder-based 6D pose estimation for highly\n  cluttered Bin Picking", "comments": "5 pages, 1 page references. Accepted to ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bin picking is a core problem in industrial environments and robotics, with\nits main module as 6D pose estimation. However, industrial depth sensors have a\nlack of accuracy when it comes to small objects. Therefore, we propose a\nframework for pose estimation in highly cluttered scenes with small objects,\nwhich mainly relies on RGB data and makes use of depth information only for\npose refinement. In this work, we compare synthetic data generation approaches\nfor object detection and pose estimation and introduce a pose filtering\nalgorithm that determines the most accurate estimated poses. We will make our\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 11:01:07 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["H\u00f6fer", "Timon", ""], ["Shamsafar", "Faranak", ""], ["Benbarka", "Nuri", ""], ["Zell", "Andreas", ""]]}, {"id": "2106.08049", "submitter": "Pawel Drozdowski", "authors": "Pawel Drozdowski, Christian Rathgeb, Christoph Busch", "title": "Demographic Fairness in Face Identification: The Watchlist Imbalance\n  Effect", "comments": "dispute over contents of section 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, different researchers have found that the gallery composition of a\nface database can induce performance differentials to facial identification\nsystems in which a probe image is compared against up to all stored reference\nimages to reach a biometric decision. This negative effect is referred to as\n\"watchlist imbalance effect\". In this work, we present a method to\ntheoretically estimate said effect for a biometric identification system given\nits verification performance across demographic groups and the composition of\nthe used gallery. Further, we report results for identification experiments on\ndifferently composed demographic subsets, i.e. females and males, of the public\nacademic MORPH database using the open-source ArcFace face recognition system.\nIt is shown that the database composition has a huge impact on performance\ndifferentials in biometric identification systems, even if performance\ndifferentials are less pronounced in the verification scenario. This study\nrepresents the first detailed analysis of the watchlist imbalance effect which\nis expected to be of high interest for future research in the field of facial\nrecognition.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 11:09:06 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 07:45:48 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 07:20:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Drozdowski", "Pawel", ""], ["Rathgeb", "Christian", ""], ["Busch", "Christoph", ""]]}, {"id": "2106.08059", "submitter": "Franziska Mueller", "authors": "Franziska Mueller, Micah Davis, Florian Bernard, Oleksandr\n  Sotnychenko, Mickeal Verschoor, Miguel A. Otaduy, Dan Casas, Christian\n  Theobalt", "title": "Real-time Pose and Shape Reconstruction of Two Interacting Hands With a\n  Single Depth Camera", "comments": "ACM Transactions on Graphics (Proceedings SIGGRAPH 2019)", "journal-ref": null, "doi": "10.1145/3306346.3322958", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for real-time pose and shape reconstruction of two\nstrongly interacting hands. Our approach is the first two-hand tracking\nsolution that combines an extensive list of favorable properties, namely it is\nmarker-less, uses a single consumer-level depth camera, runs in real time,\nhandles inter- and intra-hand collisions, and automatically adjusts to the\nuser's hand shape. In order to achieve this, we embed a recent parametric hand\npose and shape model and a dense correspondence predictor based on a deep\nneural network into a suitable energy minimization framework. For training the\ncorrespondence prediction network, we synthesize a two-hand dataset based on\nphysical simulations that includes both hand pose and shape annotations while\nat the same time avoiding inter-hand penetrations. To achieve real-time rates,\nwe phrase the model fitting in terms of a nonlinear least-squares problem so\nthat the energy can be optimized based on a highly efficient GPU-based\nGauss-Newton optimizer. We show state-of-the-art results in scenes that exceed\nthe complexity level demonstrated by previous work, including tight two-hand\ngrasps, significant inter-hand occlusions, and gesture interaction.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 11:39:49 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Mueller", "Franziska", ""], ["Davis", "Micah", ""], ["Bernard", "Florian", ""], ["Sotnychenko", "Oleksandr", ""], ["Verschoor", "Mickeal", ""], ["Otaduy", "Miguel A.", ""], ["Casas", "Dan", ""], ["Theobalt", "Christian", ""]]}, {"id": "2106.08061", "submitter": "Yutong Feng", "authors": "Yutong Feng, Jianwen Jiang, Ziyuan Huang, Zhiwu Qing, Xiang Wang,\n  Shiwei Zhang, Mingqian Tang, Yue Gao", "title": "Relation Modeling in Spatio-Temporal Action Localization", "comments": "CVPR 2021 ActivityNet Workshop Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our solution to the AVA-Kinetics Crossover Challenge of\nActivityNet workshop at CVPR 2021. Our solution utilizes multiple types of\nrelation modeling methods for spatio-temporal action detection and adopts a\ntraining strategy to integrate multiple relation modeling in end-to-end\ntraining over the two large-scale video datasets. Learning with memory bank and\nfinetuning for long-tailed distribution are also investigated to further\nimprove the performance. In this paper, we detail the implementations of our\nsolution and provide experiments results and corresponding discussions. We\nfinally achieve 40.67 mAP on the test set of AVA-Kinetics.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 11:40:18 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 07:00:12 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Feng", "Yutong", ""], ["Jiang", "Jianwen", ""], ["Huang", "Ziyuan", ""], ["Qing", "Zhiwu", ""], ["Wang", "Xiang", ""], ["Zhang", "Shiwei", ""], ["Tang", "Mingqian", ""], ["Gao", "Yue", ""]]}, {"id": "2106.08073", "submitter": "Guangze Zheng", "authors": "Guangze Zheng, Changhong Fu, Junjie Ye, Fuling Lin, and Fangqiang Ding", "title": "Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with\n  Adaptive Hybrid Label", "comments": "Accepted by ICRA 2021, Github:\n  https://github.com/vision4robotics/MSCF-tracker", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicle (UAV) based visual tracking has been confronted with\nnumerous challenges, e.g., object motion and occlusion. These challenges\ngenerally introduce unexpected mutations of target appearance and result in\ntracking failure. However, prevalent discriminative correlation filter (DCF)\nbased trackers are insensitive to target mutations due to a predefined label,\nwhich concentrates on merely the centre of the training region. Meanwhile,\nappearance mutations caused by occlusion or similar objects usually lead to the\ninevitable learning of wrong information. To cope with appearance mutations,\nthis paper proposes a novel DCF-based method to enhance the sensitivity and\nresistance to mutations with an adaptive hybrid label, i.e., MSCF. The ideal\nlabel is optimized jointly with the correlation filter and remains temporal\nconsistency. Besides, a novel measurement of mutations called mutation threat\nfactor (MTF) is applied to correct the label dynamically. Considerable\nexperiments are conducted on widely used UAV benchmarks. The results indicate\nthat the performance of MSCF tracker surpasses other 26 state-of-the-art\nDCF-based and deep-based trackers. With a real-time speed of _38 frames/s, the\nproposed approach is sufficient for UAV tracking commissions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:04:22 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zheng", "Guangze", ""], ["Fu", "Changhong", ""], ["Ye", "Junjie", ""], ["Lin", "Fuling", ""], ["Ding", "Fangqiang", ""]]}, {"id": "2106.08077", "submitter": "Thiyanga Talagala Dr", "authors": "Jayani P. G. Lakshika, Thiyanga S. Talagala", "title": "Computer-aided Interpretable Features for Leaf Image Classification", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Plant species identification is time consuming, costly, and requires lots of\nefforts, and expertise knowledge. In recent, many researchers use deep learning\nmethods to classify plants directly using plant images. While deep learning\nmodels have achieved a great success, the lack of interpretability limit their\nwidespread application. To overcome this, we explore the use of interpretable,\nmeasurable and computer-aided features extracted from plant leaf images. Image\nprocessing is one of the most challenging, and crucial steps in\nfeature-extraction. The purpose of image processing is to improve the leaf\nimage by removing undesired distortion. The main image processing steps of our\nalgorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,\nii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove\nstalk, vi) Closing holes, and vii) Resize image. The next step after image\nprocessing is to extract features from plant leaf images. We introduced 52\ncomputationally efficient features to classify plant species. These features\nare mainly classified into four groups as: i) shape-based features, ii)\ncolor-based features, iii) texture-based features, and iv) scagnostic features.\nLength, width, area, texture correlation, monotonicity and scagnostics are to\nname few of them. We explore the ability of features to discriminate the\nclasses of interest under supervised learning and unsupervised learning\nsettings. For that, supervised dimensionality reduction technique, Linear\nDiscriminant Analysis (LDA), and unsupervised dimensionality reduction\ntechnique, Principal Component Analysis (PCA) are used to convert and visualize\nthe images from digital-image space to feature space. The results show that the\nfeatures are sufficient to discriminate the classes of interest under both\nsupervised and unsupervised learning settings.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:11:10 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lakshika", "Jayani P. G.", ""], ["Talagala", "Thiyanga S.", ""]]}, {"id": "2106.08091", "submitter": "Catherine Ordun", "authors": "Catherine Ordun, Edward Raff, Sanjay Purushotham", "title": "Generating Thermal Human Faces for Physiological Assessment Using\n  Thermal Sensor Auxiliary Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal images reveal medically important physiological information about\nhuman stress, signs of inflammation, and emotional mood that cannot be seen on\nvisible images. Providing a method to generate thermal faces from visible\nimages would be highly valuable for the telemedicine community in order to show\nthis medical information. To the best of our knowledge, there are limited works\non visible-to-thermal (VT) face translation, and many current works go the\nopposite direction to generate visible faces from thermal surveillance images\n(TV) for law enforcement applications. As a result, we introduce favtGAN, a VT\nGAN which uses the pix2pix image translation model with an auxiliary sensor\nlabel prediction network for generating thermal faces from visible images.\nSince most TV methods are trained on only one data source drawn from one\nthermal sensor, we combine datasets from faces and cityscapes. These combined\ndata are captured from similar sensors in order to bootstrap the training and\ntransfer learning task, especially valuable because visible-thermal face\ndatasets are limited. Experiments on these combined datasets show that favtGAN\ndemonstrates an increase in SSIM and PSNR scores of generated thermal faces,\ncompared to training on a single face dataset alone.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:32:52 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ordun", "Catherine", ""], ["Raff", "Edward", ""], ["Purushotham", "Sanjay", ""]]}, {"id": "2106.08094", "submitter": "Bram De Wilde", "authors": "Bram de Wilde, Richard P. G. ten Broek, Henkjan Huisman", "title": "Cine-MRI detection of abdominal adhesions with spatio-temporal deep\n  learning", "comments": "Accepted at MIDL 2021 as short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adhesions are an important cause of chronic pain following abdominal surgery.\nRecent developments in abdominal cine-MRI have enabled the non-invasive\ndiagnosis of adhesions. Adhesions are identified on cine-MRI by the absence of\nsliding motion during movement. Diagnosis and mapping of adhesions improves the\nmanagement of patients with pain. Detection of abdominal adhesions on cine-MRI\nis challenging from both a radiological and deep learning perspective. We focus\non classifying presence or absence of adhesions in sagittal abdominal cine-MRI\nseries. We experimented with spatio-temporal deep learning architectures\ncentered around a ConvGRU architecture. A hybrid architecture comprising a\nResNet followed by a ConvGRU model allows to classify a whole time-series.\nCompared to a stand-alone ResNet with a two time-point (inspiration/expiration)\ninput, we show an increase in classification performance (AUROC) from 0.74 to\n0.83 ($p<0.05$). Our full temporal classification approach adds only a small\namount (5%) of parameters to the entire architecture, which may be useful for\nother medical imaging problems with a temporal dimension.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:33:54 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["de Wilde", "Bram", ""], ["Broek", "Richard P. G. ten", ""], ["Huisman", "Henkjan", ""]]}, {"id": "2106.08107", "submitter": "Corinne Stucker", "authors": "Corinne Stucker, Konrad Schindler", "title": "ResDepth: A Deep Prior For 3D Reconstruction From High-resolution\n  Satellite Images", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern optical satellite sensors enable high-resolution stereo reconstruction\nfrom space. But the challenging imaging conditions when observing the Earth\nfrom space push stereo matching to its limits. In practice, the resulting\ndigital surface models (DSMs) are fairly noisy and often do not attain the\naccuracy needed for high-resolution applications such as 3D city modeling.\nArguably, stereo correspondence based on low-level image similarity is\ninsufficient and should be complemented with a-priori knowledge about the\nexpected surface geometry beyond basic local smoothness. To that end, we\nintroduce ResDepth, a convolutional neural network that learns such an\nexpressive geometric prior from example data. ResDepth refines an initial, raw\nstereo DSM while conditioning the refinement on the images. I.e., it acts as a\nsmart, learned post-processing filter and can seamlessly complement any stereo\nmatching pipeline. In a series of experiments, we find that the proposed method\nconsistently improves stereo DSMs both quantitatively and qualitatively. We\nshow that the prior encoded in the network weights captures meaningful\ngeometric characteristics of urban design, which also generalize across\ndifferent districts and even from one city to another. Moreover, we demonstrate\nthat, by training on a variety of stereo pairs, ResDepth can acquire a\nsufficient degree of invariance against variations in imaging conditions and\nacquisition geometry.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:51:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Stucker", "Corinne", ""], ["Schindler", "Konrad", ""]]}, {"id": "2106.08112", "submitter": "Da-Wei Zhou", "authors": "Han-Jia Ye, Da-Wei Zhou, Lanqing Hong, Zhenguo Li, Xiu-Shen Wei,\n  De-Chuan Zhan", "title": "Contextualizing Multiple Tasks via Learning to Decompose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One single instance could possess multiple portraits and reveal diverse\nrelationships with others according to different contexts. Those ambiguities\nincrease the difficulty of learning a generalizable model when there exists one\nconcept or mixed concepts in a task. We propose a general approach Learning to\nDecompose Network (LeadNet) for both two cases, which contextualizes a model\nthrough meta-learning multiple maps for concepts discovery -- the\nrepresentations of instances are decomposed and adapted conditioned on the\ncontexts. Through taking a holistic view over multiple latent components over\ninstances in a sampled pseudo task, LeadNet learns to automatically select the\nright concept via incorporating those rich semantics inside and between\nobjects. LeadNet demonstrates its superiority in various applications,\nincluding exploring multiple views of confusing tasks, out-of-distribution\nrecognition, and few-shot image classification.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:10:56 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ye", "Han-Jia", ""], ["Zhou", "Da-Wei", ""], ["Hong", "Lanqing", ""], ["Li", "Zhenguo", ""], ["Wei", "Xiu-Shen", ""], ["Zhan", "De-Chuan", ""]]}, {"id": "2106.08147", "submitter": "Fan Zhang Dr", "authors": "Di Ma, Mariana Afonso, Fan Zhang and David R. Bull", "title": "Perceptually-inspired super-resolution of compressed videos", "comments": null, "journal-ref": null, "doi": "10.1117/12.2530688", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Spatial resolution adaptation is a technique which has often been employed in\nvideo compression to enhance coding efficiency. This approach encodes a lower\nresolution version of the input video and reconstructs the original resolution\nduring decoding. Instead of using conventional up-sampling filters, recent work\nhas employed advanced super-resolution methods based on convolutional neural\nnetworks (CNNs) to further improve reconstruction quality. These approaches are\nusually trained to minimise pixel-based losses such as Mean-Squared Error\n(MSE), despite the fact that this type of loss metric does not correlate well\nwith subjective opinions. In this paper, a perceptually-inspired\nsuper-resolution approach (M-SRGAN) is proposed for spatial up-sampling of\ncompressed video using a modified CNN model, which has been trained using a\ngenerative adversarial network (GAN) on compressed content with perceptual loss\nfunctions. The proposed method was integrated with HEVC HM 16.20, and has been\nevaluated on the JVET Common Test Conditions (UHD test sequences) using the\nRandom Access configuration. The results show evident perceptual quality\nimprovement over the original HM 16.20, with an average bitrate saving of 35.6%\n(Bj{\\o}ntegaard Delta measurement) based on a perceptual quality metric, VMAF.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:50:24 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ma", "Di", ""], ["Afonso", "Mariana", ""], ["Zhang", "Fan", ""], ["Bull", "David R.", ""]]}, {"id": "2106.08148", "submitter": "Xiangnan Yin", "authors": "Xiangnan Yin, Di Huang, Zehua Fu, Yunhong Wang, Liming Chen", "title": "Weakly-Supervised Photo-realistic Texture Generation for 3D Face\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although much progress has been made recently in 3D face reconstruction, most\nprevious work has been devoted to predicting accurate and fine-grained 3D\nshapes. In contrast, relatively little work has focused on generating\nhigh-fidelity face textures. Compared with the prosperity of photo-realistic 2D\nface image generation, high-fidelity 3D face texture generation has yet to be\nstudied. In this paper, we proposed a novel UV map generation model that\npredicts the UV map from a single face image. The model consists of a UV\nsampler and a UV generator. By selectively sampling the input face image's\npixels and adjusting their relative locations, the UV sampler generates an\nincomplete UV map that could faithfully reconstruct the original face. Missing\ntextures in the incomplete UV map are further full-filled by the UV generator.\nThe training is based on pseudo ground truth blended by the 3DMM texture and\nthe input face texture, thus weakly supervised. To deal with the artifacts in\nthe imperfect pseudo UV map, multiple partial UV map discriminators are\nleveraged.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 12:34:35 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Yin", "Xiangnan", ""], ["Huang", "Di", ""], ["Fu", "Zehua", ""], ["Wang", "Yunhong", ""], ["Chen", "Liming", ""]]}, {"id": "2106.08151", "submitter": "Maja Schneider", "authors": "Maja Schneider, Amelie Broszeit, Marco K\\\"orner", "title": "EuroCrops: A Pan-European Dataset for Time Series Crop Type\n  Classification", "comments": "4 pages, website: https://www.eurocrops.tum.de/", "journal-ref": "Proc. of the 2021 conference on Big Data from Space (BiDS21),\n  2021, 5, 125-128", "doi": "10.2760/125905", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present EuroCrops, a dataset based on self-declared field annotations for\ntraining and evaluating methods for crop type classification and mapping,\ntogether with its process of acquisition and harmonisation. By this, we aim to\nenrich the research efforts and discussion for data-driven land cover\nclassification via Earth observation and remote sensing. Additionally, through\ninclusion of self-declarations gathered in the scope of subsidy control from\nall countries of the European Union (EU), this dataset highlights the\ndifficulties and pitfalls one comes across when operating on a transnational\nlevel. We, therefore, also introduce a new taxonomy scheme, HCAT-ID, that\naspires to capture all the aspects of reference data originating from\nadministrative and agency databases. To address researchers from both the\nremote sensing and the computer vision and machine learning communities, we\npublish the dataset in different formats and processing levels.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:21:50 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Schneider", "Maja", ""], ["Broszeit", "Amelie", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "2106.08170", "submitter": "Xavier Boix", "authors": "Vanessa D'Amario, Tomotake Sasaki, Xavier Boix", "title": "How Modular Should Neural Module Networks Be for Systematic\n  Generalization?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Module Networks (NMNs) aim at Visual Question Answering (VQA) via\ncomposition of modules that tackle a sub-task. NMNs are a promising strategy to\nachieve systematic generalization, i.e. overcoming biasing factors in the\ntraining distribution. However, the aspects of NMNs that facilitate systematic\ngeneralization are not fully understood. In this paper, we demonstrate that the\nstage and the degree at which modularity is defined has large influence on\nsystematic generalization. In a series of experiments on three VQA datasets\n(MNIST with multiple attributes, SQOOP, and CLEVR-CoGenT), our results reveal\nthat tuning the degree of modularity in the network, especially at the image\nencoder stage, reaches substantially higher systematic generalization. These\nfindings lead to new NMN architectures that outperform previous ones in terms\nof systematic generalization.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:13:47 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["D'Amario", "Vanessa", ""], ["Sasaki", "Tomotake", ""], ["Boix", "Xavier", ""]]}, {"id": "2106.08174", "submitter": "Netanell Avisdris", "authors": "Netanell Avisdris, Bossmat Yehuda, Ori Ben-Zvi, Daphna Link-Sourani,\n  Liat Ben-Sira, Elka Miller, Elena Zharkov, Dafna Ben Bashat and Leo Joskowicz", "title": "Automatic linear measurements of the fetal brain on MRI with deep neural\n  networks", "comments": "15 pages, 8 figures, presented in CARS 2020, submitted to IJCARS", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely, accurate and reliable assessment of fetal brain development is\nessential to reduce short and long-term risks to fetus and mother. Fetal MRI is\nincreasingly used for fetal brain assessment. Three key biometric linear\nmeasurements important for fetal brain evaluation are Cerebral Biparietal\nDiameter (CBD), Bone Biparietal Diameter (BBD), and Trans-Cerebellum Diameter\n(TCD), obtained manually by expert radiologists on reference slices, which is\ntime consuming and prone to human error. The aim of this study was to develop a\nfully automatic method computing the CBD, BBD and TCD measurements from fetal\nbrain MRI. The input is fetal brain MRI volumes which may include the fetal\nbody and the mother's abdomen. The outputs are the measurement values and\nreference slices on which the measurements were computed. The method, which\nfollows the manual measurements principle, consists of five stages: 1)\ncomputation of a Region Of Interest that includes the fetal brain with an\nanisotropic 3D U-Net classifier; 2) reference slice selection with a\nConvolutional Neural Network; 3) slice-wise fetal brain structures segmentation\nwith a multiclass U-Net classifier; 4) computation of the fetal brain\nmidsagittal line and fetal brain orientation, and; 5) computation of the\nmeasurements. Experimental results on 214 volumes for CBD, BBD and TCD\nmeasurements yielded a mean $L_1$ difference of 1.55mm, 1.45mm and 1.23mm\nrespectively, and a Bland-Altman 95% confidence interval ($CI_{95}$) of 3.92mm,\n3.98mm and 2.25mm respectively. These results are similar to the manual\ninter-observer variability. The proposed automatic method for computing\nbiometric linear measurements of the fetal brain from MR imaging achieves human\nlevel performance. It has the potential of being a useful method for the\nassessment of fetal brain biometry in normal and pathological cases, and of\nimproving routine clinical practice.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:20:11 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Avisdris", "Netanell", ""], ["Yehuda", "Bossmat", ""], ["Ben-Zvi", "Ori", ""], ["Link-Sourani", "Daphna", ""], ["Ben-Sira", "Liat", ""], ["Miller", "Elka", ""], ["Zharkov", "Elena", ""], ["Bashat", "Dafna Ben", ""], ["Joskowicz", "Leo", ""]]}, {"id": "2106.08176", "submitter": "David Wood", "authors": "David A. Wood, Sina Kafiabadi, Ayisha Al Busaidi, Emily Guilhem,\n  Antanas Montvila, Siddharth Agarwal, Jeremy Lynch, Matthew Townend, Gareth\n  Barker, Sebastien Ourselin, James H. Cole, Thomas C. Booth", "title": "Automated triaging of head MRI examinations using convolutional neural\n  networks", "comments": "Accepted as an oral presentation at Medical Imaging with Deep\n  Learning (MIDL) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing demand for head magnetic resonance imaging (MRI) examinations,\nalong with a global shortage of radiologists, has led to an increase in the\ntime taken to report head MRI scans around the world. For many neurological\nconditions, this delay can result in increased morbidity and mortality. An\nautomated triaging tool could reduce reporting times for abnormal examinations\nby identifying abnormalities at the time of imaging and prioritizing the\nreporting of these scans. In this work, we present a convolutional neural\nnetwork for detecting clinically-relevant abnormalities in\n$\\text{T}_2$-weighted head MRI scans. Using a validated neuroradiology report\nclassifier, we generated a labelled dataset of 43,754 scans from two large UK\nhospitals for model training, and demonstrate accurate classification (area\nunder the receiver operating curve (AUC) = 0.943) on a test set of 800 scans\nlabelled by a team of neuroradiologists. Importantly, when trained on scans\nfrom only a single hospital the model generalized to scans from the other\nhospital ($\\Delta$AUC $\\leq$ 0.02). A simulation study demonstrated that our\nmodel would reduce the mean reporting time for abnormal examinations from 28\ndays to 14 days and from 9 days to 5 days at the two hospitals, demonstrating\nfeasibility for use in a clinical triage environment.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:21:27 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wood", "David A.", ""], ["Kafiabadi", "Sina", ""], ["Busaidi", "Ayisha Al", ""], ["Guilhem", "Emily", ""], ["Montvila", "Antanas", ""], ["Agarwal", "Siddharth", ""], ["Lynch", "Jeremy", ""], ["Townend", "Matthew", ""], ["Barker", "Gareth", ""], ["Ourselin", "Sebastien", ""], ["Cole", "James H.", ""], ["Booth", "Thomas C.", ""]]}, {"id": "2106.08186", "submitter": "Dung Hoang", "authors": "Dung Anh Hoang and Bo Chen and Tat-Jun Chin", "title": "A Spacecraft Dataset for Detection, Segmentation and Parts Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtually all aspects of modern life depend on space technology. Thanks to\nthe great advancement of computer vision in general and deep learning-based\ntechniques in particular, over the decades, the world witnessed the growing use\nof deep learning in solving problems for space applications, such as\nself-driving robot, tracers, insect-like robot on cosmos and health monitoring\nof spacecraft. These are just some prominent examples that has advanced space\nindustry with the help of deep learning. However, the success of deep learning\nmodels requires a lot of training data in order to have decent performance,\nwhile on the other hand, there are very limited amount of publicly available\nspace datasets for the training of deep learning models. Currently, there is no\npublic datasets for space-based object detection or instance segmentation,\npartly because manually annotating object segmentation masks is very time\nconsuming as they require pixel-level labelling, not to mention the challenge\nof obtaining images from space. In this paper, we aim to fill this gap by\nreleasing a dataset for spacecraft detection, instance segmentation and part\nrecognition. The main contribution of this work is the development of the\ndataset using images of space stations and satellites, with rich annotations\nincluding bounding boxes of spacecrafts and masks to the level of object parts,\nwhich are obtained with a mixture of automatic processes and manual efforts. We\nalso provide evaluations with state-of-the-art methods in object detection and\ninstance segmentation as a benchmark for the dataset. The link for downloading\nthe proposed dataset can be found on\nhttps://github.com/Yurushia1998/SatelliteDataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:36:56 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Hoang", "Dung Anh", ""], ["Chen", "Bo", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "2106.08188", "submitter": "Dawood Al Chanti", "authors": "Dawood Al Chanti and Diana Mateus", "title": "Optimal Latent Vector Alignment for Unsupervised Domain Adaptation in\n  Medical Image Segmentation", "comments": "10 pages, 3 figures, conference MICCAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the domain shift problem for segmentation. As a\nsolution, we propose OLVA, a novel and lightweight unsupervised domain\nadaptation method based on a Variational Auto-Encoder (VAE) and Optimal\nTransport (OT) theory. Thanks to the VAE, our model learns a shared\ncross-domain latent space that follows a normal distribution, which reduces the\ndomain shift. To guarantee valid segmentations, our shared latent space is\ndesigned to model the shape rather than the intensity variations. We further\nrely on an OT loss to match and align the remaining discrepancy between the two\ndomains in the latent space. We demonstrate OLVA's effectiveness for the\nsegmentation of multiple cardiac structures on the public Multi-Modality Whole\nHeart Segmentation (MM-WHS) dataset, where the source domain consists of\nannotated 3D MR images and the unlabelled target domain of 3D CTs. Our results\nshow remarkable improvements with an additional margin of 12.5\\% dice score\nover concurrent generative training approaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:41:09 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chanti", "Dawood Al", ""], ["Mateus", "Diana", ""]]}, {"id": "2106.08208", "submitter": "Feihu Huang", "authors": "Feihu Huang, Junyi Li and Heng Huang", "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients", "comments": "18 pages, 5 figures. We add the detailed proofs and correct some\n  typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adaptive gradient methods have shown excellent performance for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using specific adaptive learning rates. It\nis desired to design a universal framework for practical algorithms of adaptive\ngradients with theoretical guarantee to solve general problems. To fill this\ngap, we propose a faster and universal framework of adaptive gradients (i.e.,\nSUPER-ADAM) by introducing a universal adaptive matrix that includes most\nexisting adaptive gradient forms. Moreover, our framework can flexibly\nintegrates the momentum and variance reduced techniques. In particular, our\nnovel framework provides the convergence analysis support for adaptive gradient\nmethods under the nonconvex setting. In theoretical analysis, we prove that our\nnew algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 15:16:28 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:10:11 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Huang", "Feihu", ""], ["Li", "Junyi", ""], ["Huang", "Heng", ""]]}, {"id": "2106.08233", "submitter": "Steffen Czolbe", "authors": "Steffen Czolbe, Aasa Feragen, Oswin Krause", "title": "Spot the Difference: Topological Anomaly Detection via Geometric\n  Alignment", "comments": "Preprint, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Geometric alignment appears in a variety of applications, ranging from domain\nadaptation, optimal transport, and normalizing flows in machine learning;\noptical flow and learned augmentation in computer vision and deformable\nregistration within biomedical imaging. A recurring challenge is the alignment\nof domains whose topology is not the same; a problem that is routinely ignored,\npotentially introducing bias in downstream analysis. As a first step towards\nsolving such alignment problems, we propose an unsupervised topological\ndifference detection algorithm. The model is based on a conditional variational\nauto-encoder and detects topological anomalies with regards to a reference\nalongside the registration step. We consider both a) topological changes in the\nimage under spatial variation and b) unexpected transformations. Our approach\nis validated on a proxy task of unsupervised anomaly detection in images.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 11:49:23 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Czolbe", "Steffen", ""], ["Feragen", "Aasa", ""], ["Krause", "Oswin", ""]]}, {"id": "2106.08254", "submitter": "Li Dong", "authors": "Hangbo Bao, Li Dong, Furu Wei", "title": "BEiT: BERT Pre-Training of Image Transformers", "comments": "A Path to the BERT Moment of CV. Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a self-supervised vision representation model BEiT, which stands\nfor Bidirectional Encoder representation from Image Transformers. Following\nBERT developed in the natural language processing area, we propose a masked\nimage modeling task to pretrain vision Transformers. Specifically, each image\nhas two views in our pre-training, i.e, image patches (such as 16x16 pixels),\nand visual tokens (i.e., discrete tokens). We first \"tokenize\" the original\nimage into visual tokens. Then we randomly mask some image patches and fed them\ninto the backbone Transformer. The pre-training objective is to recover the\noriginal visual tokens based on the corrupted image patches. After pre-training\nBEiT, we directly fine-tune the model parameters on downstream tasks by\nappending task layers upon the pretrained encoder. Experimental results on\nimage classification and semantic segmentation show that our model achieves\ncompetitive results with previous pre-training methods. For example, base-size\nBEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming\nfrom-scratch DeiT training (81.8%) with the same setup. Moreover, large-size\nBEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with\nsupervised pre-training on ImageNet-22K (85.2%). The code and pretrained models\nare available at https://aka.ms/beit.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:02:37 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Bao", "Hangbo", ""], ["Dong", "Li", ""], ["Wei", "Furu", ""]]}, {"id": "2106.08261", "submitter": "Daniel Bear", "authors": "Daniel M. Bear, Elias Wang, Damian Mrowca, Felix J. Binder, Hsiau-Yu\n  Fish Tung, R.T. Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun\n  Sun, Li Fei-Fei, Nancy Kanwisher, Joshua B. Tenenbaum, Daniel L.K. Yamins,\n  Judith E. Fan", "title": "Physion: Evaluating Physical Prediction from Vision in Humans and\n  Machines", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While machine learning algorithms excel at many challenging visual tasks, it\nis unclear that they can make predictions about commonplace real world physical\nevents. Here, we present a visual and physical prediction benchmark that\nprecisely measures this capability. In realistically simulating a wide variety\nof physical phenomena -- rigid and soft-body collisions, stable multi-object\nconfigurations, rolling and sliding, projectile motion -- our dataset presents\na more comprehensive challenge than existing benchmarks. Moreover, we have\ncollected human responses for our stimuli so that model predictions can be\ndirectly compared to human judgments. We compare an array of algorithms --\nvarying in their architecture, learning objective, input-output structure, and\ntraining data -- on their ability to make diverse physical predictions. We find\nthat graph neural networks with access to the physical state best capture human\nbehavior, whereas among models that receive only visual input, those with\nobject-centric representations or pretraining do best but fall far short of\nhuman accuracy. This suggests that extracting physically meaningful\nrepresentations of scenes is the main bottleneck to achieving human-like visual\nprediction. We thus demonstrate how our benchmark can identify areas for\nimprovement and measure progress on this key aspect of physical understanding.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:13:39 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 17:20:27 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Bear", "Daniel M.", ""], ["Wang", "Elias", ""], ["Mrowca", "Damian", ""], ["Binder", "Felix J.", ""], ["Tung", "Hsiau-Yu Fish", ""], ["Pramod", "R. T.", ""], ["Holdaway", "Cameron", ""], ["Tao", "Sirui", ""], ["Smith", "Kevin", ""], ["Sun", "Fan-Yun", ""], ["Fei-Fei", "Li", ""], ["Kanwisher", "Nancy", ""], ["Tenenbaum", "Joshua B.", ""], ["Yamins", "Daniel L. K.", ""], ["Fan", "Judith E.", ""]]}, {"id": "2106.08265", "submitter": "Karsten Roth", "authors": "Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\\\"olkopf,\n  Thomas Brox, Peter Gehler", "title": "Towards Total Recall in Industrial Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to spot defective parts is a critical component in large-scale\nindustrial manufacturing. A particular challenge that we address in this work\nis the cold-start problem: fit a model using nominal (non-defective) example\nimages only. While handcrafted solutions per class are possible, the goal is to\nbuild systems that work well simultaneously on many different tasks\nautomatically. The best peforming approaches combine embeddings from ImageNet\nmodels with an outlier detection model. In this paper, we extend on this line\nof work and propose PatchCore, which uses a maximally representative memory\nbank of nominal patch-features. PatchCore offers competitive inference times\nwhile achieving state-of-the-art performance for both detection and\nlocalization. On the standard dataset MVTec AD, PatchCore achieves an\nimage-level anomaly detection AUROC score of $99.1\\%$, more than halving the\nerror compared to the next best competitor. We further report competitive\nresults on two additional datasets and also find competitive results in the few\nsamples regime.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:27:02 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Roth", "Karsten", ""], ["Pemula", "Latha", ""], ["Zepeda", "Joaquin", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Brox", "Thomas", ""], ["Gehler", "Peter", ""]]}, {"id": "2106.08267", "submitter": "Mesay Samuel", "authors": "Mesay Samuel Gondere, Lars Schmidt-Thieme, Durga Prasad Sharma,\n  Randolf Scholz", "title": "Multi-script Handwritten Digit Recognition Using Multi-task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handwritten digit recognition is one of the extensively studied area in\nmachine learning. Apart from the wider research on handwritten digit\nrecognition on MNIST dataset, there are many other research works on various\nscript recognition. However, it is not very common for multi-script digit\nrecognition which encourage the development of robust and multipurpose systems.\nAdditionally working on multi-script digit recognition enables multi-task\nlearning, considering the script classification as a related task for instance.\nIt is evident that multi-task learning improves model performance through\ninductive transfer using the information contained in related tasks. Therefore,\nin this study multi-script handwritten digit recognition using multi-task\nlearning will be investigated. As a specific case of demonstrating the solution\nto the problem, Amharic handwritten character recognition will also be\nexperimented. The handwritten digits of three scripts including Latin, Arabic\nand Kannada are studied to show that multi-task models with reformulation of\nthe individual tasks have shown promising results. In this study a novel way of\nusing the individual tasks predictions was proposed to help classification\nperformance and regularize the different loss for the purpose of the main task.\nThis finding has outperformed the baseline and the conventional multi-task\nlearning models. More importantly, it avoided the need for weighting the\ndifferent losses of the tasks, which is one of the challenges in multi-task\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:30:37 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Gondere", "Mesay Samuel", ""], ["Schmidt-Thieme", "Lars", ""], ["Sharma", "Durga Prasad", ""], ["Scholz", "Randolf", ""]]}, {"id": "2106.08269", "submitter": "Luis Felipe M\\\"uller de Oliveira Henriques", "authors": "Luis Felipe Henriques, S\\'ergio Colcher, Ruy Luiz Milidi\\'u, Andr\\'e\n  Bulc\\~ao, Pablo Barros", "title": "Generating Data Augmentation samples for Semantic Segmentation of Salt\n  Bodies in a Synthetic Seismic Image Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, subsurface salt body localization and delineation, also called\nsemantic segmentation of salt bodies, are among the most challenging\ngeophysicist tasks. Thus, identifying large salt bodies is notoriously tricky\nand is crucial for identifying hydrocarbon reservoirs and drill path planning.\nThis work proposes a Data Augmentation method based on training two generative\nmodels to augment the number of samples in a seismic image dataset for the\nsemantic segmentation of salt bodies. Our method uses deep learning models to\ngenerate pairs of seismic image patches and their respective salt masks for the\nData Augmentation. The first model is a Variational Autoencoder and is\nresponsible for generating patches of salt body masks. The second is a\nConditional Normalizing Flow model, which receives the generated masks as\ninputs and generates the associated seismic image patches. We evaluate the\nproposed method by comparing the performance of ten distinct state-of-the-art\nmodels for semantic segmentation, trained with and without the generated\naugmentations, in a dataset from two synthetic seismic images. The proposed\nmethodology yields an average improvement of 8.57% in the IoU metric across all\ncompared models. The best result is achieved by a DeeplabV3+ model variant,\nwhich presents an IoU score of 95.17% when trained with our augmentations.\nAdditionally, our proposal outperformed six selected data augmentation methods,\nand the most significant improvement in the comparison, of 9.77%, is achieved\nby composing our DA with augmentations from an elastic transformation. At last,\nwe show that the proposed method is adaptable for a larger context size by\nachieving results comparable to the obtained on the smaller context size.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:32:32 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 17:06:52 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Henriques", "Luis Felipe", ""], ["Colcher", "S\u00e9rgio", ""], ["Milidi\u00fa", "Ruy Luiz", ""], ["Bulc\u00e3o", "Andr\u00e9", ""], ["Barros", "Pablo", ""]]}, {"id": "2106.08285", "submitter": "Tim Prangemeier", "authors": "Christoph Reich, Tim Prangemeier, Christian Wildner and Heinz Koeppl", "title": "Multi-StyleGAN: Towards Image-Based Simulation of Time-Lapse Live-Cell\n  Microscopy", "comments": "revised -- accepted to MICCAI 2021. (Tim Prangemeier and Christoph\n  Reich --- both authors contributed equally)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-lapse fluorescent microscopy (TLFM) combined with predictive\nmathematical modelling is a powerful tool to study the inherently dynamic\nprocesses of life on the single-cell level. Such experiments are costly,\ncomplex and labour intensive. A complimentary approach and a step towards in\nsilico experimentation, is to synthesise the imagery itself. Here, we propose\nMulti-StyleGAN as a descriptive approach to simulate time-lapse fluorescence\nmicroscopy imagery of living cells, based on a past experiment. This novel\ngenerative adversarial network synthesises a multi-domain sequence of\nconsecutive timesteps. We showcase Multi-StyleGAN on imagery of multiple live\nyeast cells in microstructured environments and train on a dataset recorded in\nour laboratory. The simulation captures underlying biophysical factors and time\ndependencies, such as cell morphology, growth, physical interactions, as well\nas the intensity of a fluorescent reporter protein. An immediate application is\nto generate additional training and validation data for feature extraction\nalgorithms or to aid and expedite development of advanced experimental\ntechniques such as online monitoring or control of cells.\n  Code and dataset is available at\nhttps://git.rwth-aachen.de/bcs/projects/tp/multi-stylegan.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:51:16 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 14:37:52 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Reich", "Christoph", ""], ["Prangemeier", "Tim", ""], ["Wildner", "Christian", ""], ["Koeppl", "Heinz", ""]]}, {"id": "2106.08295", "submitter": "Marios Fournarakis", "authors": "Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko,\n  Mart van Baalen, Tijmen Blankevoort", "title": "A White Paper on Neural Network Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While neural networks have advanced the frontiers in many applications, they\noften come at a high computational cost. Reducing the power and latency of\nneural network inference is key if we want to integrate modern networks into\nedge devices with strict power and compute requirements. Neural network\nquantization is one of the most effective ways of achieving these savings but\nthe additional noise it induces can lead to accuracy degradation. In this white\npaper, we introduce state-of-the-art algorithms for mitigating the impact of\nquantization noise on the network's performance while maintaining low-bit\nweights and activations. We start with a hardware motivated introduction to\nquantization and then consider two main classes of algorithms: Post-Training\nQuantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no\nre-training or labelled data and is thus a lightweight push-button approach to\nquantization. In most cases, PTQ is sufficient for achieving 8-bit quantization\nwith close to floating-point accuracy. QAT requires fine-tuning and access to\nlabeled training data but enables lower bit quantization with competitive\nresults. For both solutions, we provide tested pipelines based on existing\nliterature and extensive experimentation that lead to state-of-the-art\nperformance for common deep learning models and tasks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:12:42 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Nagel", "Markus", ""], ["Fournarakis", "Marios", ""], ["Amjad", "Rana Ali", ""], ["Bondarenko", "Yelysei", ""], ["van Baalen", "Mart", ""], ["Blankevoort", "Tijmen", ""]]}, {"id": "2106.08301", "submitter": "Sheng Lin", "authors": "Sheng Lin, Wei Jiang, Wei Wang, Kaidi Xu, Yanzhi Wang, Shan Liu and\n  Songnan Li", "title": "Efficient Micro-Structured Weight Unification and Pruning for Neural\n  Network Compression", "comments": "10 pages, 3 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing Deep Neural Network (DNN) models to alleviate the storage and\ncomputation requirements is essential for practical applications, especially\nfor resource limited devices. Although capable of reducing a reasonable amount\nof model parameters, previous unstructured or structured weight pruning methods\ncan hardly truly accelerate inference, either due to the poor hardware\ncompatibility of the unstructured sparsity or due to the low sparse rate of the\nstructurally pruned network. Aiming at reducing both storage and computation,\nas well as preserving the original task performance, we propose a generalized\nweight unification framework at a hardware compatible micro-structured level to\nachieve high amount of compression and acceleration. Weight coefficients of a\nselected micro-structured block are unified to reduce the storage and\ncomputation of the block without changing the neuron connections, which turns\nto a micro-structured pruning special case when all unified coefficients are\nset to zero, where neuron connections (hence storage and computation) are\ncompletely removed. In addition, we developed an effective training framework\nbased on the alternating direction method of multipliers (ADMM), which converts\nour complex constrained optimization into separately solvable subproblems.\nThrough iteratively optimizing the subproblems, the desired micro-structure can\nbe ensured with high compression ratio and low performance degradation. We\nextensively evaluated our method using a variety of benchmark models and\ndatasets for different applications. Experimental results demonstrate\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:22:59 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 16:43:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Lin", "Sheng", ""], ["Jiang", "Wei", ""], ["Wang", "Wei", ""], ["Xu", "Kaidi", ""], ["Wang", "Yanzhi", ""], ["Liu", "Shan", ""], ["Li", "Songnan", ""]]}, {"id": "2106.08318", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Dimitrios Vytiniotis and Grzegorz Swirszcz and\n  Viorica Patraucean and Joao Carreira", "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling", "comments": "Accepted to CVPR 2021. arXiv admin note: text overlap with\n  arXiv:2001.06232", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can neural networks be trained on large-volume temporal data efficiently?\nTo compute the gradients required to update parameters, backpropagation blocks\ncomputations until the forward and backward passes are completed. For temporal\nsignals, this introduces high latency and hinders real-time learning. It also\ncreates a coupling between consecutive layers, which limits model parallelism\nand increases memory consumption. In this paper, we build upon Sideways, which\navoids blocking by propagating approximate gradients forward in time, and we\npropose mechanisms for temporal integration of information based on different\nvariants of skip connections. We also show how to decouple computation and\ndelegate individual neural modules to different devices, allowing distributed\nand parallel training. The proposed Skip-Sideways achieves low latency\ntraining, model parallelism, and, importantly, is capable of extracting\ntemporal features, leading to more stable training and improved performance on\nreal-world action recognition video datasets such as HMDB51, UCF101, and the\nlarge-scale Kinetics-600. Finally, we also show that models trained with\nSkip-Sideways generate better future frames than Sideways models, and hence\nthey can better utilize motion cues.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:50:22 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 12:52:29 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Vytiniotis", "Dimitrios", ""], ["Swirszcz", "Grzegorz", ""], ["Patraucean", "Viorica", ""], ["Carreira", "Joao", ""]]}, {"id": "2106.08320", "submitter": "Danica J. Sutherland", "authors": "Yazhe Li and Roman Pogodin and Danica J. Sutherland and Arthur Gretton", "title": "Self-Supervised Learning with Kernel Dependence Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We approach self-supervised learning of image representations from a\nstatistical dependence perspective, proposing Self-Supervised Learning with the\nHilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes\ndependence between representations of transformed versions of an image and the\nimage identity, while minimizing the kernelized variance of those features.\nThis self-supervised learning framework yields a new understanding of InfoNCE,\na variational lower bound on the mutual information (MI) between different\ntransformations. While the MI itself is known to have pathologies which can\nresult in meaningless representations being learned, its bound is much better\nbehaved: we show that it implicitly approximates SSL-HSIC (with a slightly\ndifferent regularizer). Our approach also gives us insight into BYOL, since\nSSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to\ndirectly optimize statistical dependence in time linear in the batch size,\nwithout restrictive data assumptions or indirect mutual information estimators.\nTrained with or without a target network, SSL-HSIC matches the current\nstate-of-the-art for standard linear evaluation on ImageNet, semi-supervised\nlearning and transfer to other classification and vision tasks such as semantic\nsegmentation, depth estimation and object recognition.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:51:16 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Li", "Yazhe", ""], ["Pogodin", "Roman", ""], ["Sutherland", "Danica J.", ""], ["Gretton", "Arthur", ""]]}, {"id": "2106.08322", "submitter": "Xiyang Dai", "authors": "Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu\n  Yuan, Lei Zhang", "title": "Dynamic Head: Unifying Object Detection Heads with Attentions", "comments": "CVPR 2021 camera ready with extensions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The complex nature of combining localization and classification in object\ndetection has resulted in the flourished development of methods. Previous works\ntried to improve the performance in various object detection heads but failed\nto present a unified view. In this paper, we present a novel dynamic head\nframework to unify object detection heads with attentions. By coherently\ncombining multiple self-attention mechanisms between feature levels for\nscale-awareness, among spatial locations for spatial-awareness, and within\noutput channels for task-awareness, the proposed approach significantly\nimproves the representation ability of object detection heads without any\ncomputational overhead. Further experiments demonstrate that the effectiveness\nand efficiency of the proposed dynamic head on the COCO benchmark. With a\nstandard ResNeXt-101-DCN backbone, we largely improve the performance over\npopular object detectors and achieve a new state-of-the-art at 54.0 AP.\nFurthermore, with latest transformer backbone and extra data, we can push\ncurrent best COCO result to a new record at 60.6 AP. The code will be released\nat https://github.com/microsoft/DynamicHead.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:55:22 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Dai", "Xiyang", ""], ["Chen", "Yinpeng", ""], ["Xiao", "Bin", ""], ["Chen", "Dongdong", ""], ["Liu", "Mengchen", ""], ["Yuan", "Lu", ""], ["Zhang", "Lei", ""]]}, {"id": "2106.08323", "submitter": "Johan Edstedt", "authors": "Johan Edstedt, Johan Karlsson, Francisca Benavente, Anette Novak,\n  Amanda Berg, Michael Felsberg", "title": "Is this Harmful? Learning to Predict Harmfulness Ratings from Video", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically identifying harmful content in video is an important task with\na wide range of applications. However, due to the difficulty of collecting\nhigh-quality labels as well as demanding computational requirements, the task\nhas not had a satisfying general approach. Typically, only small subsets of the\nproblem are considered, such as identifying violent content. In cases where the\ngeneral problem is tackled, rough approximations and simplifications are made\nto deal with the lack of labels and computational complexity. In this work, we\nidentify and tackle the two main obstacles. First, we create a dataset of\napproximately 4000 video clips, annotated by professionals in the field.\nSecondly, we demonstrate that advances in video recognition enable training\nmodels on our dataset that consider the full context of the scene. We conduct\nan in-depth study on our modeling choices and find that we greatly benefit from\ncombining the visual and audio modality and that pretraining on large-scale\nvideo recognition datasets and class balanced sampling further improves\nperformance. We additionally perform a qualitative study that reveals the\nheavily multi-modal nature of our dataset. Our dataset will be made available\nupon publication.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:57:12 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Edstedt", "Johan", ""], ["Karlsson", "Johan", ""], ["Benavente", "Francisca", ""], ["Novak", "Anette", ""], ["Berg", "Amanda", ""], ["Felsberg", "Michael", ""]]}, {"id": "2106.08366", "submitter": "Dipesh Tamboli", "authors": "Dipesh Tamboli", "title": "Explaining decision of model from its prediction", "comments": "Literature review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document summarizes different visual explanations methods such as CAM,\nGrad-CAM, Localization using Multiple Instance Learning - Saliency-based\nmethods, Saliency-driven Class-Impressions, Muting pixels in input image -\nAdversarial methods and Activation visualization, Convolution filter\nvisualization - Feature-based methods. We have also shown the results produced\nby different methods and a comparison between CAM, GradCAM, and Guided\nBackpropagation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 18:36:54 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Tamboli", "Dipesh", ""]]}, {"id": "2106.08372", "submitter": "Anthony Ngo", "authors": "Anthony Ngo, Max Paul Bauer and Michael Resch", "title": "A Multi-Layered Approach for Measuring the Simulation-to-Reality Gap of\n  Radar Perception for Autonomous Driving", "comments": "Accepted at the 24th IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing safety validation requirements for the release of a\nself-driving car, alternative approaches, such as simulation-based testing, are\nemerging in addition to conventional real-world testing. In order to rely on\nvirtual tests the employed sensor models have to be validated. For this reason,\nit is necessary to quantify the discrepancy between simulation and reality in\norder to determine whether a certain fidelity is sufficient for a desired\nintended use. There exists no sound method to measure this\nsimulation-to-reality gap of radar perception for autonomous driving. We\naddress this problem by introducing a multi-layered evaluation approach, which\nconsists of a combination of an explicit and an implicit sensor model\nevaluation. The former directly evaluates the realism of the synthetically\ngenerated sensor data, while the latter refers to an evaluation of a downstream\ntarget application. In order to demonstrate the method, we evaluated the\nfidelity of three typical radar model types (ideal, data-driven, ray\ntracing-based) and their applicability for virtually testing radar-based\nmulti-object tracking. We have shown the effectiveness of the proposed approach\nin terms of providing an in-depth sensor model assessment that renders existing\ndisparities visible and enables a realistic estimation of the overall model\nfidelity across different scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 18:51:39 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 08:13:14 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ngo", "Anthony", ""], ["Bauer", "Max Paul", ""], ["Resch", "Michael", ""]]}, {"id": "2106.08382", "submitter": "Abhinav Sagar", "authors": "Abhinav Sagar", "title": "DMSANet: Dual Multi Scale Attention Network", "comments": "11 pages, 3 figures, 8 tables, Submitted to Neurips 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention mechanism of late has been quite popular in the computer vision\ncommunity. A lot of work has been done to improve the performance of the\nnetwork, although almost always it results in increased computational\ncomplexity. In this paper, we propose a new attention module that not only\nachieves the best performance but also has lesser parameters compared to most\nexisting models. Our attention module can easily be integrated with other\nconvolutional neural networks because of its lightweight nature. The proposed\nnetwork named Dual Multi Scale Attention Network (DMSANet) is comprised of two\nparts: the first part is used to extract features at various scales and\naggregate them, the second part uses spatial and channel attention modules in\nparallel to adaptively integrate local features with their global dependencies.\nWe benchmark our network performance for Image Classification on ImageNet\ndataset, Object Detection and Instance Segmentation both on MS COCO dataset.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 10:31:31 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Sagar", "Abhinav", ""]]}, {"id": "2106.08385", "submitter": "Praveen Krishnan", "authors": "Praveen Krishnan, Rama Kovvuri, Guan Pang, Boris Vassilev, Tal Hassner", "title": "TextStyleBrush: Transfer of Text Aesthetics from a Single Example", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for disentangling the content of a text image\nfrom all aspects of its appearance. The appearance representation we derive can\nthen be applied to new content, for one-shot transfer of the source style to\nnew content. We learn this disentanglement in a self-supervised manner. Our\nmethod processes entire word boxes, without requiring segmentation of text from\nbackground, per-character processing, or making assumptions on string lengths.\nWe show results in different text domains which were previously handled by\nspecialized methods, e.g., scene text, handwritten text. To these ends, we make\na number of technical contributions: (1) We disentangle the style and content\nof a textual image into a non-parametric, fixed-dimensional vector. (2) We\npropose a novel approach inspired by StyleGAN but conditioned over the example\nstyle at different resolution and content. (3) We present novel self-supervised\ntraining criteria which preserve both source style and target content using a\npre-trained font classifier and text recognizer. Finally, (4) we also introduce\nImgur5K, a new challenging dataset for handwritten word images. We offer\nnumerous qualitative photo-realistic results of our method. We further show\nthat our method surpasses previous work in quantitative tests on scene text and\nhandwriting datasets, as well as in a user study.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 19:28:49 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Krishnan", "Praveen", ""], ["Kovvuri", "Rama", ""], ["Pang", "Guan", ""], ["Vassilev", "Boris", ""], ["Hassner", "Tal", ""]]}, {"id": "2106.08408", "submitter": "Mingmin Zhao", "authors": "Mingmin Zhao, Peder A. Olsen, Ranveer Chandra", "title": "Seeing Through Clouds in Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a neural-network-based solution to recover pixels\noccluded by clouds in satellite images. We leverage radio frequency (RF)\nsignals in the ultra/super-high frequency band that penetrate clouds to help\nreconstruct the occluded regions in multispectral images. We introduce the\nfirst multi-modal multi-temporal cloud removal model. Our model uses publicly\navailable satellite observations and produces daily cloud-free images.\nExperimental results show that our system significantly outperforms baselines\nby 8dB in PSNR. We also demonstrate use cases of our system in digital\nagriculture, flood monitoring, and wildfire detection. We will release the\nprocessed dataset to facilitate future research.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 20:01:27 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhao", "Mingmin", ""], ["Olsen", "Peder A.", ""], ["Chandra", "Ranveer", ""]]}, {"id": "2106.08417", "submitter": "Jonathon Shlens", "authors": "Jiquan Ngiam, Benjamin Caine, Vijay Vasudevan, Zhengdong Zhang,\n  Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi\n  Liu, Ashish Venugopal, David Weiss, Ben Sapp, Zhifeng Chen, Jonathon Shlens", "title": "Scene Transformer: A unified multi-task model for behavior prediction\n  and planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future motion of multiple agents is necessary for planning in\ndynamic environments. This task is challenging for autonomous driving since\nagents (e.g., vehicles and pedestrians) and their associated behaviors may be\ndiverse and influence each other. Most prior work has focused on first\npredicting independent futures for each agent based on all past motion, and\nthen planning against these independent predictions. However, planning against\nfixed predictions can suffer from the inability to represent the future\ninteraction possibilities between different agents, leading to sub-optimal\nplanning. In this work, we formulate a model for predicting the behavior of all\nagents jointly in real-world driving environments in a unified manner. Inspired\nby recent language modeling approaches, we use a masking strategy as the query\nto our model, enabling one to invoke a single model to predict agent behavior\nin many ways, such as potentially conditioned on the goal or full future\ntrajectory of the autonomous vehicle or the behavior of other agents in the\nenvironment. Our model architecture fuses heterogeneous world state in a\nunified Transformer architecture by employing attention across road elements,\nagent interactions and time steps. We evaluate our approach on autonomous\ndriving datasets for behavior prediction, and achieve state-of-the-art\nperformance. Our work demonstrates that formulating the problem of behavior\nprediction in a unified architecture with a masking strategy may allow us to\nhave a single model that can perform multiple motion prediction and planning\nrelated tasks effectively.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 20:20:44 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ngiam", "Jiquan", ""], ["Caine", "Benjamin", ""], ["Vasudevan", "Vijay", ""], ["Zhang", "Zhengdong", ""], ["Chiang", "Hao-Tien Lewis", ""], ["Ling", "Jeffrey", ""], ["Roelofs", "Rebecca", ""], ["Bewley", "Alex", ""], ["Liu", "Chenxi", ""], ["Venugopal", "Ashish", ""], ["Weiss", "David", ""], ["Sapp", "Ben", ""], ["Chen", "Zhifeng", ""], ["Shlens", "Jonathon", ""]]}, {"id": "2106.08445", "submitter": "Silvia Seidlitz", "authors": "Maximilian Dietrich (1) and Silvia Seidlitz (2, 3), Nicholas Schreck\n  (4), Manuel Wiesenfarth (4), Patrick Godau (2, 3), Minu Tizabi (2), Jan\n  Sellner (2, 3), Sebastian Marx (1), Samuel Kn\\\"odler (5), Michael M. Allers\n  (5), Leonardo Ayala (2, 7), Karsten Schmidt (8), Thorsten Brenner (8),\n  Alexander Studier-Fischer (5), Felix Nickel (5), Beat P. M\\\"uller-Stich (5),\n  Annette Kopp-Schneider (4), Markus A. Weigand (1) and Lena Maier-Hein (2, 6,\n  7) ((1) Department of Anesthesiology, Heidelberg University Hospital,\n  Heidelberg, Germany, (2) Division of Computer Assisted Medical Interventions,\n  German Cancer Research Center (DKFZ), Heidelberg, Germany, (3) HIDSS4Health -\n  Helmholtz Information and Data Science School for Health,\n  Karlsruhe/Heidelberg, Germany (4) Division of Biostatistics, German Cancer\n  Research Center (DKFZ), Heidelberg, Germany, (5) Department of General,\n  Visceral, and Transplantation Surgery, Heidelberg University Hospital,\n  Heidelberg, Germany, (6) Faculty of Mathematics and Computer Science,\n  Heidelberg University, Heidelberg, Germany, (7) Medical Faculty, Heidelberg\n  University, Heidelberg, Germany, (8) Department of Anesthesiology and\n  Intensive Care Medicine, University Hospital Essen, University\n  Duisburg-Essen, Essen, Germany)", "title": "Machine learning-based analysis of hyperspectral images for automated\n  sepsis diagnosis", "comments": "Maximilian Dietrich and Silvia Seidlitz contributed equally. Markus\n  A. Weigand and Lena Maier-Hein contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sepsis is a leading cause of mortality and critical illness worldwide. While\nrobust biomarkers for early diagnosis are still missing, recent work indicates\nthat hyperspectral imaging (HSI) has the potential to overcome this bottleneck\nby monitoring microcirculatory alterations. Automated machine learning-based\ndiagnosis of sepsis based on HSI data, however, has not been explored to date.\nGiven this gap in the literature, we leveraged an existing data set to (1)\ninvestigate whether HSI-based automated diagnosis of sepsis is possible and (2)\nput forth a list of possible confounders relevant for HSI-based tissue\nclassification. While we were able to classify sepsis with an accuracy of over\n$98\\,\\%$ using the existing data, our research also revealed several subject-,\ntherapy- and imaging-related confounders that may lead to an overestimation of\nalgorithm performance when not balanced across the patient groups. We conclude\nthat further prospective studies, carefully designed with respect to these\nconfounders, are necessary to confirm the preliminary results obtained in this\nstudy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 21:33:59 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Dietrich", "Maximilian", ""], ["Seidlitz", "Silvia", ""], ["Schreck", "Nicholas", ""], ["Wiesenfarth", "Manuel", ""], ["Godau", "Patrick", ""], ["Tizabi", "Minu", ""], ["Sellner", "Jan", ""], ["Marx", "Sebastian", ""], ["Kn\u00f6dler", "Samuel", ""], ["Allers", "Michael M.", ""], ["Ayala", "Leonardo", ""], ["Schmidt", "Karsten", ""], ["Brenner", "Thorsten", ""], ["Studier-Fischer", "Alexander", ""], ["Nickel", "Felix", ""], ["M\u00fcller-Stich", "Beat P.", ""], ["Kopp-Schneider", "Annette", ""], ["Weigand", "Markus A.", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2106.08462", "submitter": "Vikram Voleti", "authors": "Vikram Voleti, Chris Finlay, Adam Oberman, Christopher Pal", "title": "Multi-Resolution Continuous Normalizing Flows", "comments": "9 pages, 5 figures, 3 tables, 18 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that Neural Ordinary Differential Equations (ODEs) can\nserve as generative models of images using the perspective of Continuous\nNormalizing Flows (CNFs). Such models offer exact likelihood calculation, and\ninvertible generation/density estimation. In this work we introduce a\nMulti-Resolution variant of such models (MRCNF), by characterizing the\nconditional distribution over the additional information required to generate a\nfine image that is consistent with the coarse image. We introduce a\ntransformation between resolutions that allows for no change in the log\nlikelihood. We show that this approach yields comparable likelihood values for\nvarious image datasets, with improved performance at higher resolutions, with\nfewer parameters, using only 1 GPU. Further, we examine the out-of-distribution\nproperties of (Multi-Resolution) Continuous Normalizing Flows, and find that\nthey are similar to those of other likelihood-based generative models.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:14:56 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 02:15:55 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 16:34:31 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Voleti", "Vikram", ""], ["Finlay", "Chris", ""], ["Oberman", "Adam", ""], ["Pal", "Christopher", ""]]}, {"id": "2106.08486", "submitter": "WeiQin Chuah", "authors": "WeiQin Chuah, Ruwan Tennakoon, Alireza Bab-Hadiashar, David Suter", "title": "Achieving Domain Robustness in Stereo Matching Networks by Removing\n  Shortcut Learning", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based stereo matching and depth estimation networks currently excel\non public benchmarks with impressive results. However, state-of-the-art\nnetworks often fail to generalize from synthetic imagery to more challenging\nreal data domains. This paper is an attempt to uncover hidden secrets of\nachieving domain robustness and in particular, discovering the important\ningredients of generalization success of stereo matching networks by analyzing\nthe effect of synthetic image learning on real data performance. We provide\nevidence that demonstrates that learning of features in the synthetic domain by\na stereo matching network is heavily influenced by two \"shortcuts\" presented in\nthe synthetic data: (1) identical local statistics (RGB colour features)\nbetween matching pixels in the synthetic stereo images and (2) lack of realism\nin synthetic textures on 3D objects simulated in game engines. We will show\nthat by removing such shortcuts, we can achieve domain robustness in the\nstate-of-the-art stereo matching frameworks and produce a remarkable\nperformance on multiple realistic datasets, despite the fact that the networks\nwere trained on synthetic data, only. Our experimental results point to the\nfact that eliminating shortcuts from the synthetic data is key to achieve\ndomain-invariant generalization between synthetic and real data domains.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 23:22:54 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Chuah", "WeiQin", ""], ["Tennakoon", "Ruwan", ""], ["Bab-Hadiashar", "Alireza", ""], ["Suter", "David", ""]]}, {"id": "2106.08493", "submitter": "Junshen Xu", "authors": "Junshen Xu, Eric Z. Chen, Xiao Chen, Terrence Chen, Shanhui Sun", "title": "Multi-scale Neural ODEs for 3D Medical Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration plays an important role in medical image analysis.\nConventional optimization based methods provide an accurate estimation due to\nthe iterative process at the cost of expensive computation. Deep learning\nmethods such as learn-to-map are much faster but either iterative or\ncoarse-to-fine approach is required to improve accuracy for handling large\nmotions. In this work, we proposed to learn a registration optimizer via a\nmulti-scale neural ODE model. The inference consists of iterative gradient\nupdates similar to a conventional gradient descent optimizer but in a much\nfaster way, because the neural ODE learns from the training data to adapt the\ngradient efficiently at each iteration. Furthermore, we proposed to learn a\nmodal-independent similarity metric to address image appearance variations\nacross different image contrasts. We performed evaluations through extensive\nexperiments in the context of multi-contrast 3D MR images from both public and\nprivate data sources and demonstrate the superior performance of our proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 00:26:53 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 21:05:42 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Xu", "Junshen", ""], ["Chen", "Eric Z.", ""], ["Chen", "Xiao", ""], ["Chen", "Terrence", ""], ["Sun", "Shanhui", ""]]}, {"id": "2106.08497", "submitter": "Ruinian Xu", "authors": "Ruinian Xu, Fu-Jen Chu and Patricio A. Vela", "title": "GKNet: grasp keypoint network for grasp candidates detection", "comments": "24 pages, 12 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary grasp detection approaches employ deep learning to achieve\nrobustness to sensor and object model uncertainty. The two dominant approaches\ndesign either grasp-quality scoring or anchor-based grasp recognition networks.\nThis paper presents a different approach to grasp detection by treating it as\nkeypoint detection. The deep network detects each grasp candidate as a pair of\nkeypoints, convertible to the grasp representation g = {x, y, w, {\\theta}}^T,\nrather than a triplet or quartet of corner points. Decreasing the detection\ndifficulty by grouping keypoints into pairs boosts performance. To further\npromote dependencies between keypoints, the general non-local module is\nincorporated into the proposed learning framework. A final filtering strategy\nbased on discrete and continuous orientation prediction removes false\ncorrespondences and further improves grasp detection performance. GKNet, the\napproach presented here, achieves the best balance of accuracy and speed on the\nCornell and the abridged Jacquard dataset (96.9% and 98.39% at 41.67 and 23.26\nfps). Follow-up experiments on a manipulator evaluate GKNet using 4 types of\ngrasping experiments reflecting different nuisance sources: static grasping,\ndynamic grasping, grasping at varied camera angles, and bin picking. GKNet\noutperforms reference baselines in static and dynamic grasping experiments\nwhile showing robustness to varied camera viewpoints and bin picking\nexperiments. The results confirm the hypothesis that grasp keypoints are an\neffective output representation for deep grasp networks that provide robustness\nto expected nuisance factors.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 00:34:55 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Xu", "Ruinian", ""], ["Chu", "Fu-Jen", ""], ["Vela", "Patricio A.", ""]]}, {"id": "2106.08499", "submitter": "Celso A. M. Lopes Junior", "authors": "Celso A. M. Lopes Junior, Ricardo B. das Neves Junior, Byron L. D.\n  Bezerra, Alejandro H. Toselli, Donato Impedovo", "title": "ICDAR 2021 Competition on Components Segmentation Task of Document\n  Photos", "comments": "15 pages; 5 figures; Accepted at ICDAR 2021: 16th International\n  Conference on Document Analysis and Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes the short-term competition on the Components\nSegmentation Task of Document Photos that was prepared in the context of the\n16th International Conference on Document Analysis and Recognition (ICDAR\n2021). This competition aims to bring together researchers working in the field\nof identification document image processing and provides them a suitable\nbenchmark to compare their techniques on the component segmentation task of\ndocument images. Three challenge tasks were proposed entailing different\nsegmentation assignments to be performed on a provided dataset. The collected\ndata are from several types of Brazilian ID documents, whose personal\ninformation was conveniently replaced. There were 16 participants whose results\nobtained for some or all the three tasks show different rates for the adopted\nmetrics, like Dice Similarity Coefficient ranging from 0.06 to 0.99. Different\nDeep Learning models were applied by the entrants with diverse strategies to\nachieve the best results in each of the tasks. Obtained results show that the\ncurrently applied methods for solving one of the proposed tasks (document\nboundary detection) are already well established. However, for the other two\nchallenge tasks (text zone and handwritten sign detection) research and\ndevelopment of more robust approaches are still required to achieve acceptable\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 00:49:58 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 01:40:34 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Junior", "Celso A. M. Lopes", ""], ["Junior", "Ricardo B. das Neves", ""], ["Bezerra", "Byron L. D.", ""], ["Toselli", "Alejandro H.", ""], ["Impedovo", "Donato", ""]]}, {"id": "2106.08503", "submitter": "Dora Zhao", "authors": "Dora Zhao and Angelina Wang and Olga Russakovsky", "title": "Understanding and Evaluating Racial Biases in Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image captioning is an important task for benchmarking visual reasoning and\nfor enabling accessibility for people with vision impairments. However, as in\nmany machine learning settings, social biases can influence image captioning in\nundesirable ways. In this work, we study bias propagation pathways within image\ncaptioning, focusing specifically on the COCO dataset. Prior work has analyzed\ngender bias in captions using automatically-derived gender labels; here we\nexamine racial and intersectional biases using manual annotations. Our first\ncontribution is in annotating the perceived gender and skin color of 28,315 of\nthe depicted people after obtaining IRB approval. Using these annotations, we\ncompare racial biases present in both manual and automatically-generated image\ncaptions. We demonstrate differences in caption performance, sentiment, and\nword choice between images of lighter versus darker-skinned people. Further, we\nfind the magnitude of these differences to be greater in modern captioning\nsystems compared to older ones, thus leading to concerns that without proper\nconsideration and mitigation these differences will only become increasingly\nprevalent. Code and data is available at\nhttps://princetonvisualai.github.io/imagecaptioning-bias .\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 01:07:24 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhao", "Dora", ""], ["Wang", "Angelina", ""], ["Russakovsky", "Olga", ""]]}, {"id": "2106.08505", "submitter": "Lanlan Liu", "authors": "Lanlan Liu, Yuting Zhang, Jia Deng, Stefano Soatto", "title": "Dynamically Grown Generative Adversarial Networks", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work introduced progressive network growing as a promising way to ease\nthe training for large GANs, but the model design and architecture-growing\nstrategy still remain under-explored and needs manual design for different\nimage data. In this paper, we propose a method to dynamically grow a GAN during\ntraining, optimizing the network architecture and its parameters together with\nautomation. The method embeds architecture search techniques as an interleaving\nstep with gradient-based training to periodically seek the optimal\narchitecture-growing strategy for the generator and discriminator. It enjoys\nthe benefits of both eased training because of progressive growing and improved\nperformance because of broader architecture design space. Experimental results\ndemonstrate new state-of-the-art of image generation. Observations in the\nsearch procedure also provide constructive insights into the GAN model design\nsuch as generator-discriminator balance and convolutional layer choices.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 01:25:51 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Liu", "Lanlan", ""], ["Zhang", "Yuting", ""], ["Deng", "Jia", ""], ["Soatto", "Stefano", ""]]}, {"id": "2106.08512", "submitter": "Yueyu Hu", "authors": "Yueyu Hu, Wenhan Yang, Haofeng Huang, Jiaying Liu", "title": "Revisit Visual Representation in Analytics Taxonomy: A Compression\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analytics have played an increasingly critical role in the Internet of\nThings, where massive visual signals have to be compressed and fed into\nmachines. But facing such big data and constrained bandwidth capacity, existing\nimage/video compression methods lead to very low-quality representations, while\nexisting feature compression techniques fail to support diversified visual\nanalytics applications/tasks with low-bit-rate representations. In this paper,\nwe raise and study the novel problem of supporting multiple machine vision\nanalytics tasks with the compressed visual representation, namely, the\ninformation compression problem in analytics taxonomy. By utilizing the\nintrinsic transferability among different tasks, our framework successfully\nconstructs compact and expressive representations at low bit-rates to support a\ndiversified set of machine vision tasks, including both high-level\nsemantic-related tasks and mid-level geometry analytic tasks. In order to\nimpose compactness in the representations, we propose a codebook-based\nhyperprior, which helps map the representation into a low-dimensional manifold.\nAs it well fits the signal structure of the deep visual feature, it facilitates\nmore accurate entropy estimation, and results in higher compression efficiency.\nWith the proposed framework and the codebook-based hyperprior, we further\ninvestigate the relationship of different task features owning different levels\nof abstraction granularity. Experimental results demonstrate that with the\nproposed scheme, a set of diversified tasks can be supported at a significantly\nlower bit-rate, compared with existing compression schemes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 01:44:32 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Hu", "Yueyu", ""], ["Yang", "Wenhan", ""], ["Huang", "Haofeng", ""], ["Liu", "Jiaying", ""]]}, {"id": "2106.08513", "submitter": "Mahdi Kalayeh", "authors": "Mahdi M. Kalayeh, Nagendra Kamath, Lingyi Liu and Ashok Chandrashekar", "title": "Watching Too Much Television is Good: Self-Supervised Audio-Visual\n  Representation Learning from Movies and TV Shows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The abundance and ease of utilizing sound, along with the fact that auditory\nclues reveal so much about what happens in the scene, make the audio-visual\nspace a perfectly intuitive choice for self-supervised representation learning.\nHowever, the current literature suggests that training on \\textit{uncurated}\ndata yields considerably poorer representations compared to the\n\\textit{curated} alternatives collected in supervised manner, and the gap only\nnarrows when the volume of data significantly increases. Furthermore, the\nquality of learned representations is known to be heavily influenced by the\nsize and taxonomy of the curated datasets used for self-supervised training.\nThis begs the question of whether we are celebrating too early on catching up\nwith supervised learning when our self-supervised efforts still rely almost\nexclusively on curated data. In this paper, we study the efficacy of learning\nfrom Movies and TV Shows as forms of uncurated data for audio-visual\nself-supervised learning. We demonstrate that a simple model based on\ncontrastive learning, trained on a collection of movies and TV shows, not only\ndramatically outperforms more complex methods which are trained on orders of\nmagnitude larger uncurated datasets, but also performs very competitively with\nthe state-of-the-art that learns from large-scale curated data. We identify\nthat audiovisual patterns like the appearance of the main character or\nprominent scenes and mise-en-sc\\`ene which frequently occur through the whole\nduration of a movie, lead to an overabundance of easy negative instances in the\ncontrastive learning formulation. Capitalizing on such observation, we propose\na hierarchical sampling policy, which despite its simplicity, effectively\nimproves the performance, particularly when learning from TV shows which\nnaturally face less semantic diversity.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 02:00:11 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Kalayeh", "Mahdi M.", ""], ["Kamath", "Nagendra", ""], ["Liu", "Lingyi", ""], ["Chandrashekar", "Ashok", ""]]}, {"id": "2106.08523", "submitter": "Chaofan Chen", "authors": "Chaofan Chen, Xiaoshan Yang, Changsheng Xu, Xuhui Huang, Zhe Ma", "title": "ECKPN: Explicit Class Knowledge Propagation Network for Transductive\n  Few-shot Learning", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the transductive graph-based methods have achieved great success in\nthe few-shot classification task. However, most existing methods ignore\nexploring the class-level knowledge that can be easily learned by humans from\njust a handful of samples. In this paper, we propose an Explicit Class\nKnowledge Propagation Network (ECKPN), which is composed of the comparison,\nsqueeze and calibration modules, to address this problem. Specifically, we\nfirst employ the comparison module to explore the pairwise sample relations to\nlearn rich sample representations in the instance-level graph. Then, we squeeze\nthe instance-level graph to generate the class-level graph, which can help\nobtain the class-level visual knowledge and facilitate modeling the relations\nof different classes. Next, the calibration module is adopted to characterize\nthe relations of the classes explicitly to obtain the more discriminative\nclass-level knowledge representations. Finally, we combine the class-level\nknowledge with the instance-level sample representations to guide the inference\nof the query samples. We conduct extensive experiments on four few-shot\nclassification benchmarks, and the experimental results show that the proposed\nECKPN significantly outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 02:29:43 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Chen", "Chaofan", ""], ["Yang", "Xiaoshan", ""], ["Xu", "Changsheng", ""], ["Huang", "Xuhui", ""], ["Ma", "Zhe", ""]]}, {"id": "2106.08543", "submitter": "Sangmin Woo", "authors": "Sangmin Woo, Junhyug Noh, Kangil Kim", "title": "Tackling the Challenges in Scene Graph Generation with Local-to-Global\n  Interactions", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we seek new insights into the underlying challenges of the\nScene Graph Generation (SGG) task. Quantitative and qualitative analysis of the\nVisual Genome dataset implies -- 1) Ambiguity: even if inter-object\nrelationship contains the same object (or predicate), they may not be visually\nor semantically similar, 2) Asymmetry: despite the nature of the relationship\nthat embodied the direction, it was not well addressed in previous studies, and\n3) Higher-order contexts: leveraging the identities of certain graph elements\ncan help to generate accurate scene graphs. Motivated by the analysis, we\ndesign a novel SGG framework, Local-to-Global Interaction Networks (LOGIN).\nLocally, interactions extract the essence between three instances - subject,\nobject, and background - while baking direction awareness into the network by\nconstraining the input order. Globally, interactions encode the contexts\nbetween every graph components -- nodes and edges. Also we introduce Attract &\nRepel loss which finely adjusts predicate embeddings. Our framework enables\npredicting the scene graph in a local-to-global manner by design, leveraging\nthe possible complementariness. To quantify how much LOGIN is aware of\nrelational direction, we propose a new diagnostic task called Bidirectional\nRelationship Classification (BRC). We see that LOGIN can successfully\ndistinguish relational direction than existing methods (in BRC task) while\nshowing state-of-the-art results on the Visual Genome benchmark (in SGG task).\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 03:58:21 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Woo", "Sangmin", ""], ["Noh", "Junhyug", ""], ["Kim", "Kangil", ""]]}, {"id": "2106.08557", "submitter": "Hidetoshi Matsuo", "authors": "Hidetoshi Matsuo (1), Mizuho Nishio (1), Munenobu Nogami (1), Feibi\n  Zeng (1), Takako Kurimoto (2), Sandeep Kaushik (3), Florian Wiesinger (3),\n  Atsushi K Kono (1), and Takamichi Murakami (1) ((1) Department of Radiology,\n  Kobe University Graduate School of Medicine, Kobe, Japan, (2) GE Healthcare,\n  Hino, Japan and (3) GE Healthcare, Munich, Germany)", "title": "Unsupervised-learning-based method for chest MRI-CT transformation using\n  structure constrained unsupervised generative attention networks", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The integrated positron emission tomography/magnetic resonance imaging\n(PET/MRI) scanner facilitates the simultaneous acquisition of metabolic\ninformation via PET and morphological information with high soft-tissue\ncontrast using MRI. Although PET/MRI facilitates the capture of high-accuracy\nfusion images, its major drawback can be attributed to the difficulty\nencountered when performing attenuation correction, which is necessary for\nquantitative PET evaluation. The combined PET/MRI scanning requires the\ngeneration of attenuation-correction maps from MRI owing to no direct\nrelationship between the gamma-ray attenuation information and MRIs. While\nMRI-based bone-tissue segmentation can be readily performed for the head and\npelvis regions, the realization of accurate bone segmentation via chest CT\ngeneration remains a challenging task. This can be attributed to the\nrespiratory and cardiac motions occurring in the chest as well as its\nanatomically complicated structure and relatively thin bone cortex. This paper\npresents a means to minimise the anatomical structural changes without human\nannotation by adding structural constraints using a modality-independent\nneighbourhood descriptor (MIND) to a generative adversarial network (GAN) that\ncan transform unpaired images. The results obtained in this study revealed the\nproposed U-GAT-IT + MIND approach to outperform all other competing approaches.\nThe findings of this study hint towards possibility of synthesising clinically\nacceptable CT images from chest MRI without human annotation, thereby\nminimising the changes in the anatomical structure.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 05:22:27 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Matsuo", "Hidetoshi", ""], ["Nishio", "Mizuho", ""], ["Nogami", "Munenobu", ""], ["Zeng", "Feibi", ""], ["Kurimoto", "Takako", ""], ["Kaushik", "Sandeep", ""], ["Wiesinger", "Florian", ""], ["Kono", "Atsushi K", ""], ["Murakami", "Takamichi", ""]]}, {"id": "2106.08565", "submitter": "Poorya Aghdaie", "authors": "Poorya Aghdaie, Baaria Chaudhary, Sobhan Soleymani, Jeremy Dawson,\n  Nasser M. Nasrabadi", "title": "Detection of Morphed Face Images Using Discriminative Wavelet Sub-bands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work investigates the well-known problem of morphing attacks, which has\ndrawn considerable attention in the biometrics community. Morphed images have\nexposed face recognition systems' susceptibility to false acceptance, resulting\nin dire consequences, especially for national security applications. To detect\nmorphing attacks, we propose a method which is based on a discriminative 2D\nDiscrete Wavelet Transform (2D-DWT). A discriminative wavelet sub-band can\nhighlight inconsistencies between a real and a morphed image. We observe that\nthere is a salient discrepancy between the entropy of a given sub-band in a\nbona fide image, and the same sub-band's entropy in a morphed sample.\nConsidering this dissimilarity between these two entropy values, we find the\nKullback-Leibler divergence between the two distributions, namely the entropy\nof the bona fide and the corresponding morphed images. The most discriminative\nwavelet sub-bands are those with the highest corresponding KL-divergence\nvalues. Accordingly, 22 sub-bands are selected as the most discriminative ones\nin terms of morph detection. We show that a Deep Neural Network (DNN) trained\non the 22 discriminative sub-bands can detect morphed samples precisely. Most\nimportantly, the effectiveness of our algorithm is validated through\nexperiments on three datasets: VISAPP17, LMA, and MorGAN. We also performed an\nablation study on the sub-band selection.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 06:03:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Aghdaie", "Poorya", ""], ["Chaudhary", "Baaria", ""], ["Soleymani", "Sobhan", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2106.08570", "submitter": "Boyang Wan Mr", "authors": "Boyang Wan and Wenhui Jiang and Yuming Fang and Zhiyuan Luo and\n  Guanqun Ding", "title": "Anomaly Detection in Video Sequences: A Benchmark and Computational\n  Model", "comments": "Publication in IET Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anomaly detection has attracted considerable search attention. However,\nexisting anomaly detection databases encounter two major problems. Firstly,\nthey are limited in scale. Secondly, training sets contain only video-level\nlabels indicating the existence of an abnormal event during the full video\nwhile lacking annotations of precise time durations. To tackle these problems,\nwe contribute a new Large-scale Anomaly Detection (LAD) database as the\nbenchmark for anomaly detection in video sequences, which is featured in two\naspects. 1) It contains 2000 video sequences including normal and abnormal\nvideo clips with 14 anomaly categories including crash, fire, violence, etc.\nwith large scene varieties, making it the largest anomaly analysis database to\ndate. 2) It provides the annotation data, including video-level labels\n(abnormal/normal video, anomaly type) and frame-level labels (abnormal/normal\nvideo frame) to facilitate anomaly detection. Leveraging the above benefits\nfrom the LAD database, we further formulate anomaly detection as a\nfully-supervised learning problem and propose a multi-task deep neural network\nto solve it. We first obtain the local spatiotemporal contextual feature by\nusing an Inflated 3D convolutional (I3D) network. Then we construct a recurrent\nconvolutional neural network fed the local spatiotemporal contextual feature to\nextract the spatiotemporal contextual feature. With the global spatiotemporal\ncontextual feature, the anomaly type and score can be computed simultaneously\nby a multi-task neural network. Experimental results show that the proposed\nmethod outperforms the state-of-the-art anomaly detection methods on our\ndatabase and other public databases of anomaly detection. Codes are available\nat https://github.com/wanboyang/anomaly_detection_LAD2000.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 06:34:38 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wan", "Boyang", ""], ["Jiang", "Wenhui", ""], ["Fang", "Yuming", ""], ["Luo", "Zhiyuan", ""], ["Ding", "Guanqun", ""]]}, {"id": "2106.08573", "submitter": "Yuan-Chen Guo", "authors": "Ying-Tian Liu, Yuan-Chen Guo, Yi-Xiao Li, Chen Wang, Song-Hai Zhang", "title": "Learning Implicit Glyph Shape Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel implicit glyph shape representation, which\nmodels glyphs as shape primitives enclosed by quadratic curves, and naturally\nenables generating glyph images at arbitrary high resolutions. Experiments on\nfont reconstruction and interpolation tasks verified that this structured\nimplicit representation is suitable for describing both structure and style\nfeatures of glyphs. Furthermore, based on the proposed representation, we\ndesign a simple yet effective disentangled network for the challenging one-shot\nfont style transfer problem, and achieve the best results comparing to\nstate-of-the-art alternatives in both quantitative and qualitative comparisons.\nBenefit from this representation, our generated glyphs have the potential to be\nconverted to vector fonts through post-processing, reducing the gap between\nrasterized images and vector graphics. We hope this work can provide a powerful\ntool for 2D shape analysis and synthesis, and inspire further exploitation in\nimplicit representations for 2D shape modeling.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 06:42:55 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Liu", "Ying-Tian", ""], ["Guo", "Yuan-Chen", ""], ["Li", "Yi-Xiao", ""], ["Wang", "Chen", ""], ["Zhang", "Song-Hai", ""]]}, {"id": "2106.08575", "submitter": "Shadrokh Samavi", "authors": "Eric J. Nunn, Pejman Khadivi, Shadrokh Samavi", "title": "Compound Frechet Inception Distance for Quality Assessment of GAN\n  Created Images", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks or GANs are a type of generative modeling\nframework. GANs involve a pair of neural networks engaged in a competition in\niteratively creating fake data, indistinguishable from the real data. One\nnotable application of GANs is developing fake human faces, also known as \"deep\nfakes,\" due to the deep learning algorithms at the core of the GAN framework.\nMeasuring the quality of the generated images is inherently subjective but\nattempts to objectify quality using standardized metrics have been made. One\nexample of objective metrics is the Frechet Inception Distance (FID), which\nmeasures the difference between distributions of feature vectors for two\nseparate datasets of images. There are situations that images with low\nperceptual qualities are not assigned appropriate FID scores. We propose to\nimprove the robustness of the evaluation process by integrating lower-level\nfeatures to cover a wider array of visual defects. Our proposed method\nintegrates three levels of feature abstractions to evaluate the quality of\ngenerated images. Experimental evaluations show better performance of the\nproposed method for distorted images.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 06:53:27 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Nunn", "Eric J.", ""], ["Khadivi", "Pejman", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2106.08590", "submitter": "Zhipeng Luo", "authors": "Zhipeng Luo, Xiaobing Zhang, Shijian Lu, Shuai Yi", "title": "Domain Consistency Regularization for Unsupervised Multi-source Domain\n  Adaptive Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based multi-source unsupervised domain adaptation (MUDA) has\nbeen actively studied in recent years. Compared with single-source unsupervised\ndomain adaptation (SUDA), domain shift in MUDA exists not only between the\nsource and target domains but also among multiple source domains. Most existing\nMUDA algorithms focus on extracting domain-invariant representations among all\ndomains whereas the task-specific decision boundaries among classes are largely\nneglected. In this paper, we propose an end-to-end trainable network that\nexploits domain Consistency Regularization for unsupervised Multi-source domain\nAdaptive classification (CRMA). CRMA aligns not only the distributions of each\npair of source and target domains but also that of all domains. For each pair\nof source and target domains, we employ an intra-domain consistency to\nregularize a pair of domain-specific classifiers to achieve intra-domain\nalignment. In addition, we design an inter-domain consistency that targets\njoint inter-domain alignment among all domains. To address different\nsimilarities between multiple source domains and the target domain, we design\nan authorization strategy that assigns different authorities to domain-specific\nclassifiers adaptively for optimal pseudo label prediction and self-training.\nExtensive experiments show that CRMA tackles unsupervised domain adaptation\neffectively under a multi-source setup and achieves superior adaptation\nconsistently across multiple MUDA datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 07:29:27 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Luo", "Zhipeng", ""], ["Zhang", "Xiaobing", ""], ["Lu", "Shijian", ""], ["Yi", "Shuai", ""]]}, {"id": "2106.08596", "submitter": "Van Thong Huynh", "authors": "VanThong Huynh, Guee-Sang Lee, Hyung-Jeong Yang, Soo-Huyng Kim", "title": "Temporal Convolution Networks with Positional Encoding for Evoked\n  Expression Estimation", "comments": "Oral presentation at AUVi Workshop - CVPR 2021\n  (https://sites.google.com/view/auvi-cvpr2021/program). Source code available\n  at https://github.com/th2l/EvokedExpression-tcnpe", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an approach for Evoked Expressions from Videos (EEV)\nchallenge, which aims to predict evoked facial expressions from video. We take\nadvantage of pre-trained models on large-scale datasets in computer vision and\naudio signals to extract the deep representation of timestamps in the video. A\ntemporal convolution network, rather than an RNN like architecture, is used to\nexplore temporal relationships due to its advantage in memory consumption and\nparallelism. Furthermore, to address the missing annotations of some\ntimestamps, positional encoding is employed to ensure continuity of input data\nwhen discarding these timestamps during training. We achieved state-of-the-art\nresults on the EEV challenge with a Pearson correlation coefficient of 0.05477,\nthe first ranked performance in the EEV 2021 challenge.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 07:49:36 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Huynh", "VanThong", ""], ["Lee", "Guee-Sang", ""], ["Yang", "Hyung-Jeong", ""], ["Kim", "Soo-Huyng", ""]]}, {"id": "2106.08599", "submitter": "Hankyu Moon", "authors": "Hankyu Moon, Heng Hao, Sima Didari, Jae Oh Woo, Patrick Bangert", "title": "PatchNet: Unsupervised Object Discovery based on Patch Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We demonstrate that frequently appearing objects can be discovered by\ntraining randomly sampled patches from a small number of images (100 to 200) by\nself-supervision. Key to this approach is the pattern space, a latent space of\npatterns that represents all possible sub-images of the given image data. The\ndistance structure in the pattern space captures the co-occurrence of patterns\ndue to the frequent objects. The pattern space embedding is learned by\nminimizing the contrastive loss between randomly generated adjacent patches. To\nprevent the embedding from learning the background, we modulate the contrastive\nloss by color-based object saliency and background dissimilarity. The learned\ndistance structure serves as object memory, and the frequent objects are simply\ndiscovered by clustering the pattern vectors from the random patches sampled\nfor inference. Our image representation based on image patches naturally\nhandles the position and scale invariance property that is crucial to\nmulti-object discovery. The method has been proven surprisingly effective, and\nsuccessfully applied to finding multiple human faces and bodies from natural\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 07:56:19 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Moon", "Hankyu", ""], ["Hao", "Heng", ""], ["Didari", "Sima", ""], ["Woo", "Jae Oh", ""], ["Bangert", "Patrick", ""]]}, {"id": "2106.08600", "submitter": "Quande Liu", "authors": "Quande Liu, Hongzheng Yang, Qi Dou, Pheng-Ann Heng", "title": "Federated Semi-supervised Medical Image Classification via Inter-client\n  Relation Matching", "comments": "Accepted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning (FL) has emerged with increasing popularity to collaborate\ndistributed medical institutions for training deep networks. However, despite\nexisting FL algorithms only allow the supervised training setting, most\nhospitals in realistic usually cannot afford the intricate data labeling due to\nabsence of budget or expertise. This paper studies a practical yet challenging\nFL problem, named \\textit{Federated Semi-supervised Learning} (FSSL), which\naims to learn a federated model by jointly utilizing the data from both labeled\nand unlabeled clients (i.e., hospitals). We present a novel approach for this\nproblem, which improves over traditional consistency regularization mechanism\nwith a new inter-client relation matching scheme. The proposed learning scheme\nexplicitly connects the learning across labeled and unlabeled clients by\naligning their extracted disease relationships, thereby mitigating the\ndeficiency of task knowledge at unlabeled clients and promoting discriminative\ninformation from unlabeled samples. We validate our method on two large-scale\nmedical image classification datasets. The effectiveness of our method has been\ndemonstrated with the clear improvements over state-of-the-arts as well as the\nthorough ablation analysis on both tasks\\footnote{Code will be made available\nat \\url{https://github.com/liuquande/FedIRM}}.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 07:58:00 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Liu", "Quande", ""], ["Yang", "Hongzheng", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2106.08601", "submitter": "Liang Hou", "authors": "Liang Hou, Huawei Shen, Qi Cao, Xueqi Cheng", "title": "Self-supervised GANs with Label Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, transformation-based self-supervised learning has been applied to\ngenerative adversarial networks (GANs) to mitigate the catastrophic forgetting\nproblem of discriminator by learning stable representations. However, the\nseparate self-supervised tasks in existing self-supervised GANs cause an\ninconsistent goal with generative modeling due to the learning of the generator\nfrom their generator distribution-agnostic classifiers. To address this issue,\nwe propose a novel self-supervised GANs framework with label augmentation,\ni.e., augmenting the GAN labels (real or fake) with the self-supervised\npseudo-labels. In particular, the discriminator and the self-supervised\nclassifier are unified to learn a single task that predicts the augmented label\nsuch that the discriminator/classifier is aware of the generator distribution,\nwhile the generator tries to confuse the discriminator/classifier by optimizing\nthe discrepancy between the transformed real and generated distributions.\nTheoretically, we prove that the generator, at the equilibrium point, converges\nto replicate the data distribution. Empirically, we demonstrate that the\nproposed method significantly outperforms competitive baselines on both\ngenerative modeling and representation learning across benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 07:58:00 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Hou", "Liang", ""], ["Shen", "Huawei", ""], ["Cao", "Qi", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2106.08605", "submitter": "Zihan Ye", "authors": "Zihan Ye, Fuyuan Hu, Fan Lyu, Linyan Li, Kaizhu Huang", "title": "Disentangling Semantic-to-visual Confusion for Zero-shot Learning", "comments": "Accepted by IEEE TRANSACTIONS ON MULTIMEDIA (TMM) in 2021", "journal-ref": null, "doi": "10.1109/TMM.2021.3089017", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using generative models to synthesize visual features from semantic\ndistribution is one of the most popular solutions to ZSL image classification\nin recent years. The triplet loss (TL) is popularly used to generate realistic\nvisual distributions from semantics by automatically searching discriminative\nrepresentations. However, the traditional TL cannot search reliable unseen\ndisentangled representations due to the unavailability of unseen classes in\nZSL. To alleviate this drawback, we propose in this work a multi-modal triplet\nloss (MMTL) which utilizes multimodal information to search a disentangled\nrepresentation space. As such, all classes can interplay which can benefit\nlearning disentangled class representations in the searched space. Furthermore,\nwe develop a novel model called Disentangling Class Representation Generative\nAdversarial Network (DCR-GAN) focusing on exploiting the disentangled\nrepresentations in training, feature synthesis, and final recognition stages.\nBenefiting from the disentangled representations, DCR-GAN could fit a more\nrealistic distribution over both seen and unseen features. Extensive\nexperiments show that our proposed model can lead to superior performance to\nthe state-of-the-arts on four benchmark datasets. Our code is available at\nhttps://github.com/FouriYe/DCRGAN-TMM.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 08:04:11 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ye", "Zihan", ""], ["Hu", "Fuyuan", ""], ["Lyu", "Fan", ""], ["Li", "Linyan", ""], ["Huang", "Kaizhu", ""]]}, {"id": "2106.08613", "submitter": "Chaewon Park", "authors": "Chaewon Park, MyeongAh Cho, Minhyeok Lee, Sangyoun Lee", "title": "FastAno: Fast Anomaly Detection via Spatio-temporal Patch Transformation", "comments": "10 pages, 2022WACV under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video anomaly detection has gained significant attention due to the\nincreasing requirements of automatic monitoring for surveillance videos.\nEspecially, the prediction based approach is one of the most studied methods to\ndetect anomalies by predicting frames that include abnormal events in the test\nset after learning with the normal frames of the training set. However, a lot\nof prediction networks are computationally expensive owing to the use of\npre-trained optical flow networks, or fail to detect abnormal situations\nbecause of their strong generative ability to predict even the anomalies. To\naddress these shortcomings, we propose spatial rotation transformation (SRT)\nand temporal mixing transformation (TMT) to generate irregular patch cuboids\nwithin normal frame cuboids in order to enhance the learning of normal\nfeatures. Additionally, the proposed patch transformation is used only during\nthe training phase, allowing our model to detect abnormal frames at fast speed\nduring inference. Our model is evaluated on three anomaly detection benchmarks,\nachieving competitive accuracy and surpassing all the previous works in terms\nof speed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 08:14:31 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 04:51:43 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Park", "Chaewon", ""], ["Cho", "MyeongAh", ""], ["Lee", "Minhyeok", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2106.08615", "submitter": "Minhyeok Lee", "authors": "Minhyeok Lee, Sangwon Hwang, Chaewon Park, Sangyoun Lee", "title": "EdgeConv with Attention Module for Monocular Depth Estimation", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation is an especially important task in robotics and\nautonomous driving, where 3D structural information is essential. However,\nextreme lighting conditions and complex surface objects make it difficult to\npredict depth in a single image. Therefore, to generate accurate depth maps, it\nis important for the model to learn structural information about the scene. We\npropose a novel Patch-Wise EdgeConv Module (PEM) and EdgeConv Attention Module\n(EAM) to solve the difficulty of monocular depth estimation. The proposed\nmodules extract structural information by learning the relationship between\nimage patches close to each other in space using edge convolution. Our method\nis evaluated on two popular datasets, the NYU Depth V2 and the KITTI Eigen\nsplit, achieving state-of-the-art performance. We prove that the proposed model\npredicts depth robustly in challenging scenes through various comparative\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 08:15:20 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Lee", "Minhyeok", ""], ["Hwang", "Sangwon", ""], ["Park", "Chaewon", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2106.08617", "submitter": "Jianhua Yang", "authors": "Jianhua Yang, Yan Huang, Zhanyu Ma, Liang Wang", "title": "CMF: Cascaded Multi-model Fusion for Referring Image Segmentation", "comments": "Accepted by ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we address the task of referring image segmentation (RIS),\nwhich aims at predicting a segmentation mask for the object described by a\nnatural language expression. Most existing methods focus on establishing\nunidirectional or directional relationships between visual and linguistic\nfeatures to associate two modalities together, while the multi-scale context is\nignored or insufficiently modeled. Multi-scale context is crucial to localize\nand segment those objects that have large scale variations during the\nmulti-modal fusion process. To solve this problem, we propose a simple yet\neffective Cascaded Multi-modal Fusion (CMF) module, which stacks multiple\natrous convolutional layers in parallel and further introduces a cascaded\nbranch to fuse visual and linguistic features. The cascaded branch can\nprogressively integrate multi-scale contextual information and facilitate the\nalignment of two modalities during the multi-modal fusion process. Experimental\nresults on four benchmark datasets demonstrate that our method outperforms most\nstate-of-the-art methods. Code is available at\nhttps://github.com/jianhua2022/CMF-Refseg.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 08:18:39 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Yang", "Jianhua", ""], ["Huang", "Yan", ""], ["Ma", "Zhanyu", ""], ["Wang", "Liang", ""]]}, {"id": "2106.08624", "submitter": "Wenqing Zheng", "authors": "Wenqing Zheng, Jiyang Xie, Weidong Liu, Zhanyu Ma", "title": "Structured DropConnect for Uncertainty Inference in Image Classification", "comments": "5 pages,1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the complexity of the network structure, uncertainty inference has\nbecome an important task to improve the classification accuracy for artificial\nintelligence systems. For image classification tasks, we propose a structured\nDropConnect (SDC) framework to model the output of a deep neural network by a\nDirichlet distribution. We introduce a DropConnect strategy on weights in the\nfully connected layers during training. In test, we split the network into\nseveral sub-networks, and then model the Dirichlet distribution by match its\nmoments with the mean and variance of the outputs of these sub-networks. The\nentropy of the estimated Dirichlet distribution is finally utilized for\nuncertainty inference. In this paper, this framework is implemented on LeNet$5$\nand VGG$16$ models for misclassification detection and out-of-distribution\ndetection on MNIST and CIFAR-$10$ datasets. Experimental results show that the\nperformance of the proposed SDC can be comparable to other uncertainty\ninference methods. Furthermore, the SDC is adapted well to different network\nstructures with certain generalization capabilities and research prospects.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 08:31:14 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zheng", "Wenqing", ""], ["Xie", "Jiyang", ""], ["Liu", "Weidong", ""], ["Ma", "Zhanyu", ""]]}, {"id": "2106.08650", "submitter": "Zilong Huang", "authors": "Rui Zhang, Yang Han, Zilong Huang, Pei Cheng, Guozhong Luo, Gang Yu,\n  Bin Fu", "title": "Shuffle Transformer with Feature Alignment for Video Face Parsing", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This is a short technical report introducing the solution of the Team\nTCParser for Short-video Face Parsing Track of The 3rd Person in Context (PIC)\nWorkshop and Challenge at CVPR 2021. In this paper, we introduce a strong\nbackbone which is cross-window based Shuffle Transformer for presenting\naccurate face parsing representation. To further obtain the finer segmentation\nresults, especially on the edges, we introduce a Feature Alignment Aggregation\n(FAA) module. It can effectively relieve the feature misalignment issue caused\nby multi-resolution feature aggregation. Benefiting from the stronger backbone\nand better feature aggregation, the proposed method achieves 86.9519% score in\nthe Short-video Face Parsing track of the 3rd Person in Context (PIC) Workshop\nand Challenge, ranked the first place.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 09:25:33 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhang", "Rui", ""], ["Han", "Yang", ""], ["Huang", "Zilong", ""], ["Cheng", "Pei", ""], ["Luo", "Guozhong", ""], ["Yu", "Gang", ""], ["Fu", "Bin", ""]]}, {"id": "2106.08693", "submitter": "Alexander Tsaregorodtsev", "authors": "Alexander Tsaregorodtsev, Vasileios Belagiannis", "title": "ParticleAugment: Sampling-Based Data Augmentation", "comments": "11 pages. Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automated data augmentation approach for image classification.\nWe formulate the problem as Monte Carlo sampling where our goal is to\napproximate the optimal augmentation policies. We propose a particle filtering\nformulation to find optimal augmentation policies and their schedules during\nmodel training. Our performance measurement procedure relies on a validation\nsubset of our training set, while the policy transition model depends on a\nGaussian prior and an optional augmentation velocity parameter. In our\nexperiments, we show that our formulation for automated augmentation reaches\npromising results on CIFAR-10, CIFAR-100, and ImageNet datasets using the\nstandard network architectures for this problem. By comparing with the related\nwork, we also show that our method reaches a balance between the computational\ncost of policy search and the model performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 10:56:02 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Tsaregorodtsev", "Alexander", ""], ["Belagiannis", "Vasileios", ""]]}, {"id": "2106.08706", "submitter": "Ahmed Arif", "authors": "Laxmi Pandey, Ahmed Sabbir Arif", "title": "Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in\n  Real-Time MRI", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.HC cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speech sounds of spoken language are obtained by varying configuration of the\narticulators surrounding the vocal tract. They contain abundant information\nthat can be utilized to better understand the underlying mechanism of human\nspeech production. We propose a novel deep neural network-based learning\nframework that understands acoustic information in the variable-length sequence\nof vocal tract shaping during speech production, captured by real-time magnetic\nresonance imaging (rtMRI), and translate it into text. The proposed framework\ncomprises of spatiotemporal convolutions, a recurrent network, and the\nconnectionist temporal classification loss, trained entirely end-to-end. On the\nUSC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better\ncompared to the existing models. To the best of our knowledge, this is the\nfirst study that demonstrates the recognition of entire spoken sentence based\non an individual's articulatory motions captured by rtMRI video. We also\nperformed an analysis of variations in the geometry of articulation in each\nsub-regions of the vocal tract (i.e., pharyngeal, velar and dorsal, hard\npalate, labial constriction region) with respect to different emotions and\ngenders. Results suggest that each sub-regions distortion is affected by both\nemotion and gender.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:20:02 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Pandey", "Laxmi", ""], ["Arif", "Ahmed Sabbir", ""]]}, {"id": "2106.08710", "submitter": "Jacky Cao", "authors": "Jacky Cao, Kit-Yung Lam, Lik-Hang Lee, Xiaoli Liu, Pan Hui, Xiang Su", "title": "Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence", "comments": "This work is currently under review in an international journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Augmented Reality (MAR) integrates computer-generated virtual objects\nwith physical environments for mobile devices. MAR systems enable users to\ninteract with MAR devices, such as smartphones and head-worn wearables, and\nperforms seamless transitions from the physical world to a mixed world with\ndigital entities. These MAR systems support user experiences by using MAR\ndevices to provide universal accessibility to digital contents. Over the past\n20 years, a number of MAR systems have been developed, however, the studies and\ndesign of MAR frameworks have not yet been systematically reviewed from the\nperspective of user-centric design. This article presents the first effort of\nsurveying existing MAR frameworks (count: 37) and further discusses the latest\nstudies on MAR through a top-down approach: 1) MAR applications; 2) MAR\nvisualisation techniques adaptive to user mobility and contexts; 3) systematic\nevaluation of MAR frameworks including supported platforms and corresponding\nfeatures such as tracking, feature extraction plus sensing capabilities; and 4)\nunderlying machine learning approaches supporting intelligent operations within\nMAR systems. Finally, we summarise the development of emerging research fields,\ncurrent state-of-the-art, and discuss the important open challenges and\npossible theoretical and technical directions. This survey aims to benefit both\nresearchers and MAR system developers alike.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:26:37 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cao", "Jacky", ""], ["Lam", "Kit-Yung", ""], ["Lee", "Lik-Hang", ""], ["Liu", "Xiaoli", ""], ["Hui", "Pan", ""], ["Su", "Xiang", ""]]}, {"id": "2106.08713", "submitter": "Xiaolin Song", "authors": "Yueming Zhang, Xiaolin Song, Bing Bai, Tengfei Xing, Chao Liu, Xin\n  Gao, Zhihui Wang, Yawei Wen, Haojin Liao, Guoshan Zhang, Pengfei Xu", "title": "2nd Place Solution for Waymo Open Dataset Challenge -- Real-time 2D\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an autonomous driving system, it is essential to recognize vehicles,\npedestrians and cyclists from images. Besides the high accuracy of the\nprediction, the requirement of real-time running brings new challenges for\nconvolutional network models. In this report, we introduce a real-time method\nto detect the 2D objects from images. We aggregate several popular one-stage\nobject detectors and train the models of variety input strategies\nindependently, to yield better performance for accurate multi-scale detection\nof each category, especially for small objects. For model acceleration, we\nleverage TensorRT to optimize the inference time of our detection pipeline. As\nshown in the leaderboard, our proposed detection framework ranks the 2nd place\nwith 75.00% L1 mAP and 69.72% L2 mAP in the real-time 2D detection track of the\nWaymo Open Dataset Challenges, while our framework achieves the latency of\n45.8ms/frame on an Nvidia Tesla V100 GPU.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:32:03 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhang", "Yueming", ""], ["Song", "Xiaolin", ""], ["Bai", "Bing", ""], ["Xing", "Tengfei", ""], ["Liu", "Chao", ""], ["Gao", "Xin", ""], ["Wang", "Zhihui", ""], ["Wen", "Yawei", ""], ["Liao", "Haojin", ""], ["Zhang", "Guoshan", ""], ["Xu", "Pengfei", ""]]}, {"id": "2106.08727", "submitter": "Lei Li", "authors": "Lei Li and Veronika A. Zimmer and Julia A. Schnabel and Xiahai Zhuang", "title": "AtrialGeneral: Domain Generalization for Left Atrial Segmentation of\n  Multi-Center LGE MRIs", "comments": "10 pages, 4 figures, MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Left atrial (LA) segmentation from late gadolinium enhanced magnetic\nresonance imaging (LGE MRI) is a crucial step needed for planning the treatment\nof atrial fibrillation. However, automatic LA segmentation from LGE MRI is\nstill challenging, due to the poor image quality, high variability in LA\nshapes, and unclear LA boundary. Though deep learning-based methods can provide\npromising LA segmentation results, they often generalize poorly to unseen\ndomains, such as data from different scanners and/or sites. In this work, we\ncollect 210 LGE MRIs from different centers with different levels of image\nquality. To evaluate the domain generalization ability of models on the LA\nsegmentation task, we employ four commonly used semantic segmentation networks\nfor the LA segmentation from multi-center LGE MRIs. Besides, we investigate\nthree domain generalization strategies, i.e., histogram matching, mutual\ninformation based disentangled representation, and random style transfer, where\na simple histogram matching is proved to be most effective.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:58:11 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 01:26:28 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 03:00:37 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Li", "Lei", ""], ["Zimmer", "Veronika A.", ""], ["Schnabel", "Julia A.", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2106.08749", "submitter": "Tianyun Yang", "authors": "Tianyun Yang, Juan Cao, Qiang Sheng, Lei Li, Jiaqi Ji, Xirong Li,\n  Sheng Tang", "title": "Learning to Disentangle GAN Fingerprint for Fake Image Attribution", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid pace of generative models has brought about new threats to visual\nforensics such as malicious personation and digital copyright infringement,\nwhich promotes works on fake image attribution. Existing works on fake image\nattribution mainly rely on a direct classification framework. Without\nadditional supervision, the extracted features could include many\ncontent-relevant components and generalize poorly. Meanwhile, how to obtain an\ninterpretable GAN fingerprint to explain the decision remains an open question.\nAdopting a multi-task framework, we propose a GAN Fingerprint Disentangling\nNetwork (GFD-Net) to simultaneously disentangle the fingerprint from\nGAN-generated images and produce a content-irrelevant representation for fake\nimage attribution. A series of constraints are provided to guarantee the\nstability and discriminability of the fingerprint, which in turn helps\ncontent-irrelevant feature extraction. Further, we perform comprehensive\nanalysis on GAN fingerprint, providing some clues about the properties of GAN\nfingerprint and which factors dominate the fingerprint in GAN architecture.\nExperiments show that our GFD-Net achieves superior fake image attribution\nperformance in both closed-world and open-world testing. We also apply our\nmethod in binary fake image detection and exhibit a significant generalization\nability on unseen generators.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 12:50:40 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Yang", "Tianyun", ""], ["Cao", "Juan", ""], ["Sheng", "Qiang", ""], ["Li", "Lei", ""], ["Ji", "Jiaqi", ""], ["Li", "Xirong", ""], ["Tang", "Sheng", ""]]}, {"id": "2106.08752", "submitter": "Fuping Wu", "authors": "Fuping Wu and Xiahai Zhuang", "title": "Unsupervised Domain Adaptation with Variational Approximation for\n  Cardiac Segmentation", "comments": "accepted by IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation is useful in medical image segmentation.\nParticularly, when ground truths of the target images are not available, domain\nadaptation can train a target-specific model by utilizing the existing labeled\nimages from other modalities. Most of the reported works mapped images of both\nthe source and target domains into a common latent feature space, and then\nreduced their discrepancy either implicitly with adversarial training or\nexplicitly by directly minimizing a discrepancy metric. In this work, we\npropose a new framework, where the latent features of both domains are driven\ntowards a common and parameterized variational form, whose conditional\ndistribution given the image is Gaussian. This is achieved by two networks\nbased on variational auto-encoders (VAEs) and a regularization for this\nvariational approximation. Both of the VAEs, each for one domain, contain a\nsegmentation module, where the source segmentation is trained in a supervised\nmanner, while the target one is trained unsupervisedly. We validated the\nproposed domain adaptation method using two cardiac segmentation tasks, i.e.,\nthe cross-modality (CT and MR) whole heart segmentation and the cross-sequence\ncardiac MR segmentation. Results show that the proposed method achieved better\naccuracies compared to two state-of-the-art approaches and demonstrated good\npotential for cardiac segmentation. Furthermore, the proposed explicit\nregularization was shown to be effective and efficient in narrowing down the\ndistribution gap between domains, which is useful for unsupervised domain\nadaptation. Our code and data has been released via\nhttps://zmiclab.github.io/projects.html.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:00:39 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wu", "Fuping", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2106.08756", "submitter": "Xiaomeng Dong", "authors": "Xiaomeng Dong, Michael Potter, Gaurav Kumar, Yun-Chan Tsai, V. Ratna\n  Saripalli", "title": "Automating Augmentation Through Random Unidimensional Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is no secret amongst deep learning researchers that finding the right data\naugmentation strategy during training can mean the difference between a\nstate-of-the-art result and a run-of-the-mill ranking. To that end, the\ncommunity has seen many efforts to automate the process of finding the perfect\naugmentation procedure for any task at hand. Unfortunately, even recent\ncutting-edge methods bring massive computational overhead, requiring as many as\n100 full model trainings to settle on an ideal configuration. We show how to\nachieve even better performance in just 7: with Random Unidimensional\nAugmentation. Source code is available at https://github.com/fastestimator/RUA\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:07:59 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Dong", "Xiaomeng", ""], ["Potter", "Michael", ""], ["Kumar", "Gaurav", ""], ["Tsai", "Yun-Chan", ""], ["Saripalli", "V. Ratna", ""]]}, {"id": "2106.08761", "submitter": "Luke Guerdan", "authors": "Luke Guerdan, Alex Raymond, and Hatice Gunes", "title": "Toward Affective XAI: Facial Affect Analysis for Understanding\n  Explainable Human-AI Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As machine learning approaches are increasingly used to augment human\ndecision-making, eXplainable Artificial Intelligence (XAI) research has\nexplored methods for communicating system behavior to humans. However, these\napproaches often fail to account for the emotional responses of humans as they\ninteract with explanations. Facial affect analysis, which examines human facial\nexpressions of emotions, is one promising lens for understanding how users\nengage with explanations. Therefore, in this work, we aim to (1) identify which\nfacial affect features are pronounced when people interact with XAI interfaces,\nand (2) develop a multitask feature embedding for linking facial affect signals\nwith participants' use of explanations. Our analyses and results show that the\noccurrence and values of facial AU1 and AU4, and Arousal are heightened when\nparticipants fail to use explanations effectively. This suggests that facial\naffect analysis should be incorporated into XAI to personalize explanations to\nindividuals' interaction styles and to adapt explanations based on the\ndifficulty of the task performed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:14:21 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Guerdan", "Luke", ""], ["Raymond", "Alex", ""], ["Gunes", "Hatice", ""]]}, {"id": "2106.08762", "submitter": "Denys Rozumnyi", "authors": "Denys Rozumnyi, Martin R. Oswald, Vittorio Ferrari, Marc Pollefeys", "title": "Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving\n  Objects", "comments": "15 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address the novel task of jointly reconstructing the 3D shape, texture,\nand motion of an object from a single motion-blurred image. While previous\napproaches address the deblurring problem only in the 2D image domain, our\nproposed rigorous modeling of all object properties in the 3D domain enables\nthe correct description of arbitrary object motion. This leads to significantly\nbetter image decomposition and sharper deblurring results. We model the\nobserved appearance of a motion-blurred object as a combination of the\nbackground and a 3D object with constant translation and rotation. Our method\nminimizes a loss on reconstructing the input image via differentiable rendering\nwith suitable regularizers. This enables estimating the textured 3D mesh of the\nblurred object with high fidelity. Our method substantially outperforms\ncompeting approaches on several benchmarks for fast moving objects deblurring.\nQualitative results show that the reconstructed 3D mesh generates high-quality\ntemporal super-resolution and novel views of the deblurred object.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:18:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Rozumnyi", "Denys", ""], ["Oswald", "Martin R.", ""], ["Ferrari", "Vittorio", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2106.08795", "submitter": "Muhammad Jehanzeb Mirza", "authors": "Muhammad Jehanzeb Mirza, Cornelius Buerkle, Julio Jarquin, Michael\n  Opitz, Fabian Oboril, Kay-Ulrich Scholl, Horst Bischof", "title": "Robustness of Object Detectors in Degrading Weather Conditions", "comments": "Accepted for publication at ITSC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art object detection systems for autonomous driving achieve\npromising results in clear weather conditions. However, such autonomous safety\ncritical systems also need to work in degrading weather conditions, such as\nrain, fog and snow. Unfortunately, most approaches evaluate only on the KITTI\ndataset, which consists only of clear weather scenes. In this paper we address\nthis issue and perform one of the most detailed evaluation on single and dual\nmodality architectures on data captured in real weather conditions. We analyse\nthe performance degradation of these architectures in degrading weather\nconditions. We demonstrate that an object detection architecture performing\ngood in clear weather might not be able to handle degrading weather conditions.\nWe also perform ablation studies on the dual modality architectures and show\ntheir limitations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:56:07 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Mirza", "Muhammad Jehanzeb", ""], ["Buerkle", "Cornelius", ""], ["Jarquin", "Julio", ""], ["Opitz", "Michael", ""], ["Oboril", "Fabian", ""], ["Scholl", "Kay-Ulrich", ""], ["Bischof", "Horst", ""]]}, {"id": "2106.08798", "submitter": "Jongmin Yu", "authors": "Jongmin Yu and Hyeontaek Oh", "title": "Unsupervised Person Re-identification via Multi-Label Prediction and\n  Classification based on Graph-Structural Insight", "comments": "submitted to ICCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper addresses unsupervised person re-identification (Re-ID) using\nmulti-label prediction and classification based on graph-structural insight.\nOur method extracts features from person images and produces a graph that\nconsists of the features and a pairwise similarity of them as nodes and edges,\nrespectively. Based on the graph, the proposed graph structure based\nmulti-label prediction (GSMLP) method predicts multi-labels by considering the\npairwise similarity and the adjacency node distribution of each node. The\nmulti-labels created by GSMLP are applied to the proposed selective multi-label\nclassification (SMLC) loss. SMLC integrates a hard-sample mining scheme and a\nmulti-label classification. The proposed GSMLP and SMLC boost the performance\nof unsupervised person Re-ID without any pre-labelled dataset. Experimental\nresults justify the superiority of the proposed method in unsupervised person\nRe-ID by producing state-of-the-art performance. The source code for this paper\nis publicly available on 'https://github.com/uknownpioneer/GSMLP-SMLC.git'.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:00:40 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Yu", "Jongmin", ""], ["Oh", "Hyeontaek", ""]]}, {"id": "2106.08808", "submitter": "Pietro Gori", "authors": "Benoit Dufumier, Pietro Gori, Julie Victor, Antoine Grigis, Michel\n  Wessa, Paolo Brambilla, Pauline Favre, Mircea Polosan, Colm McDonald, Camille\n  Marie Piguet, Edouard Duchesnay", "title": "Contrastive Learning with Continuous Proxy Meta-Data for 3D MRI\n  Classification", "comments": "MICCAI 2021", "journal-ref": "MICCAI 2021", "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional supervised learning with deep neural networks requires a\ntremendous amount of labelled data to converge to a good solution. For 3D\nmedical images, it is often impractical to build a large homogeneous annotated\ndataset for a specific pathology. Self-supervised methods offer a new way to\nlearn a representation of the images in an unsupervised manner with a neural\nnetwork. In particular, contrastive learning has shown great promises by\n(almost) matching the performance of fully-supervised CNN on vision tasks.\nNonetheless, this method does not take advantage of available meta-data, such\nas participant's age, viewed as prior knowledge. Here, we propose to leverage\ncontinuous proxy metadata, in the contrastive learning framework, by\nintroducing a new loss called y-Aware InfoNCE loss. Specifically, we improve\nthe positive sampling during pre-training by adding more positive examples with\nsimilar proxy meta-data with the anchor, assuming they share similar\ndiscriminative semantic features.With our method, a 3D CNN model pre-trained on\n$10^4$ multi-site healthy brain MRI scans can extract relevant features for\nthree classification tasks: schizophrenia, bipolar diagnosis and Alzheimer's\ndetection. When fine-tuned, it also outperforms 3D CNN trained from scratch on\nthese tasks, as well as state-of-the-art self-supervised methods. Our code is\nmade publicly available here.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:17:04 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Dufumier", "Benoit", ""], ["Gori", "Pietro", ""], ["Victor", "Julie", ""], ["Grigis", "Antoine", ""], ["Wessa", "Michel", ""], ["Brambilla", "Paolo", ""], ["Favre", "Pauline", ""], ["Polosan", "Mircea", ""], ["McDonald", "Colm", ""], ["Piguet", "Camille Marie", ""], ["Duchesnay", "Edouard", ""]]}, {"id": "2106.08816", "submitter": "Ziang Cao", "authors": "Ziang Cao, Changhong Fu, Junjie Ye, Bowen Li, and Yiming Li", "title": "SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the Siamese-based method has stood out from multitudinous tracking\nmethods owing to its state-of-the-art (SOTA) performance. Nevertheless, due to\nvarious special challenges in UAV tracking, \\textit{e.g.}, severe occlusion,\nand fast motion, most existing Siamese-based trackers hardly combine superior\nperformance with high efficiency. To this concern, in this paper, a novel\nattentional Siamese tracker (SiamAPN++) is proposed for real-time UAV tracking.\nBy virtue of the attention mechanism, the attentional aggregation network (AAN)\nis conducted with self-AAN and cross-AAN, raising the expression ability of\nfeatures eventually. The former AAN aggregates and models the self-semantic\ninterdependencies of the single feature map via spatial and channel dimensions.\nThe latter aims to aggregate the cross-interdependencies of different semantic\nfeatures including the location information of anchors. In addition, the dual\nfeatures version of the anchor proposal network is proposed to raise the\nrobustness of proposing anchors, increasing the perception ability to objects\nwith various scales. Experiments on two well-known authoritative benchmarks are\nconducted, where SiamAPN++ outperforms its baseline SiamAPN and other SOTA\ntrackers. Besides, real-world tests onboard a typical embedded platform\ndemonstrate that SiamAPN++ achieves promising tracking results with real-time\nspeed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:28:57 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cao", "Ziang", ""], ["Fu", "Changhong", ""], ["Ye", "Junjie", ""], ["Li", "Bowen", ""], ["Li", "Yiming", ""]]}, {"id": "2106.08817", "submitter": "Pietro Gori", "authors": "Anton Fran\\c{c}ois, Pietro Gori, Joan Glaun\\`es", "title": "Metamorphic image registration using a semi-Lagrangian scheme", "comments": "SEE GSI 2021", "journal-ref": "Geometric Science for Information 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an implementation of both Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) and Metamorphosis image registration using\na semi-Lagrangian scheme for geodesic shooting. We propose to solve both\nproblems as an inexact matching providing a single and unifying cost function.\nWe demonstrate that for image registration the use of a semi-Lagrangian scheme\nis more stable than a standard Eulerian scheme. Our GPU implementation is based\non PyTorch, which greatly simplifies and accelerates the computations thanks to\nits powerful automatic differentiation engine. It will be freely available at\nhttps://github.com/antonfrancois/Demeter_metamorphosis.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:29:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Fran\u00e7ois", "Anton", ""], ["Gori", "Pietro", ""], ["Glaun\u00e8s", "Joan", ""]]}, {"id": "2106.08827", "submitter": "Mahsa Ehsanpour", "authors": "Mahsa Ehsanpour, Fatemeh Saleh, Silvio Savarese, Ian Reid, Hamid\n  Rezatofighi", "title": "JRDB-Act: A Large-scale Multi-modal Dataset for Spatio-temporal Action,\n  Social Group and Activity Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large-scale video action understanding datasets has\nfacilitated advances in the interpretation of visual scenes containing people.\nHowever, learning to recognize human activities in an unconstrained real-world\nenvironment, with potentially highly unbalanced and long-tailed distributed\ndata remains a significant challenge, not least owing to the lack of a\nreflective large-scale dataset. Most existing large-scale datasets are either\ncollected from a specific or constrained environment, e.g. kitchens or rooms,\nor video sharing platforms such as YouTube. In this paper, we introduce\nJRDB-Act, a multi-modal dataset, as an extension of the existing JRDB, which is\ncaptured by asocial mobile manipulator and reflects a real distribution of\nhuman daily life actions in a university campus environment. JRDB-Act has been\ndensely annotated with atomic actions, comprises over 2.8M action labels,\nconstituting a large-scale spatio-temporal action detection dataset. Each human\nbounding box is labelled with one pose-based action label and multiple\n(optional) interaction-based action labels. Moreover JRDB-Act comes with social\ngroup identification annotations conducive to the task of grouping individuals\nbased on their interactions in the scene to infer their social activities\n(common activities in each social group).\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:43:46 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ehsanpour", "Mahsa", ""], ["Saleh", "Fatemeh", ""], ["Savarese", "Silvio", ""], ["Reid", "Ian", ""], ["Rezatofighi", "Hamid", ""]]}, {"id": "2106.08829", "submitter": "Gullal Singh Cheema", "authors": "Gullal S. Cheema and Sherzod Hakimov and Eric M\\\"uller-Budack and\n  Ralph Ewerth", "title": "A Fair and Comprehensive Comparison of Multimodal Tweet Sentiment\n  Analysis Methods", "comments": "Accepted in Workshop on Multi-ModalPre-Training for Multimedia\n  Understanding (MMPT 2021), co-located with ICMR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Opinion and sentiment analysis is a vital task to characterize subjective\ninformation in social media posts. In this paper, we present a comprehensive\nexperimental evaluation and comparison with six state-of-the-art methods, from\nwhich we have re-implemented one of them. In addition, we investigate different\ntextual and visual feature embeddings that cover different aspects of the\ncontent, as well as the recently introduced multimodal CLIP embeddings.\nExperimental results are presented for two different publicly available\nbenchmark datasets of tweets and corresponding images. In contrast to the\nevaluation methodology of previous work, we introduce a reproducible and fair\nevaluation scheme to make results comparable. Finally, we conduct an error\nanalysis to outline the limitations of the methods and possibilities for the\nfuture work.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:44:48 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cheema", "Gullal S.", ""], ["Hakimov", "Sherzod", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2106.08851", "submitter": "Shaoxiong Wang", "authors": "Shaoxiong Wang, Yu She, Branden Romero, Edward Adelson", "title": "GelSight Wedge: Measuring High-Resolution 3D Contact Geometry with a\n  Compact Robot Finger", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based tactile sensors have the potential to provide important contact\ngeometry to localize the objective with visual occlusion. However, it is\nchallenging to measure high-resolution 3D contact geometry for a compact robot\nfinger, to simultaneously meet optical and mechanical constraints. In this\nwork, we present the GelSight Wedge sensor, which is optimized to have a\ncompact shape for robot fingers, while achieving high-resolution 3D\nreconstruction. We evaluate the 3D reconstruction under different lighting\nconfigurations, and extend the method from 3 lights to 1 or 2 lights. We\ndemonstrate the flexibility of the design by shrinking the sensor to the size\nof a human finger for fine manipulation tasks. We also show the effectiveness\nand potential of the reconstructed 3D geometry for pose tracking in the 3D\nspace.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:15:29 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Shaoxiong", ""], ["She", "Yu", ""], ["Romero", "Branden", ""], ["Adelson", "Edward", ""]]}, {"id": "2106.08856", "submitter": "Stanislaw Szymanowicz", "authors": "Stanislaw Szymanowicz, James Charles, Roberto Cipolla", "title": "X-MAN: Explaining multiple sources of anomalies in video", "comments": "In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops, June 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our objective is to detect anomalies in video while also automatically\nexplaining the reason behind the detector's response. In a practical sense,\nexplainability is crucial for this task as the required response to an anomaly\ndepends on its nature and severity. However, most leading methods (based on\ndeep neural networks) are not interpretable and hide the decision making\nprocess in uninterpretable feature representations. In an effort to tackle this\nproblem we make the following contributions: (1) we show how to build\ninterpretable feature representations suitable for detecting anomalies with\nstate of the art performance, (2) we propose an interpretable probabilistic\nanomaly detector which can describe the reason behind it's response using high\nlevel concepts, (3) we are the first to directly consider object interactions\nfor anomaly detection and (4) we propose a new task of explaining anomalies and\nrelease a large dataset for evaluating methods on this task. Our method\ncompetes well with the state of the art on public datasets while also providing\nanomaly explanation based on objects and their interactions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:25:50 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Szymanowicz", "Stanislaw", ""], ["Charles", "James", ""], ["Cipolla", "Roberto", ""]]}, {"id": "2106.08886", "submitter": "Pengfei Guo", "authors": "Pengfei Guo, Jeya Maria Jose Valanarasu, Puyang Wang, Jinyuan Zhou,\n  Shanshan Jiang, Vishal M. Patel", "title": "Over-and-Under Complete Convolutional RNN for MRI Reconstruction", "comments": "Accepted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing magnetic resonance (MR) images from undersampled data is a\nchallenging problem due to various artifacts introduced by the under-sampling\noperation. Recent deep learning-based methods for MR image reconstruction\nusually leverage a generic auto-encoder architecture which captures low-level\nfeatures at the initial layers and high-level features at the deeper layers.\nSuch networks focus much on global features which may not be optimal to\nreconstruct the fully-sampled image. In this paper, we propose an\nOver-and-Under Complete Convolutional Recurrent Neural Network (OUCR), which\nconsists of an overcomplete and an undercomplete Convolutional Recurrent Neural\nNetwork(CRNN). The overcomplete branch gives special attention in learning\nlocal structures by restraining the receptive field of the network. Combining\nit with the undercomplete branch leads to a network which focuses more on\nlow-level features without losing out on the global structures. Extensive\nexperiments on two datasets demonstrate that the proposed method achieves\nsignificant improvements over the compressed sensing and popular deep\nlearning-based methods with less number of trainable parameters.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:56:34 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 02:22:42 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Guo", "Pengfei", ""], ["Valanarasu", "Jeya Maria Jose", ""], ["Wang", "Puyang", ""], ["Zhou", "Jinyuan", ""], ["Jiang", "Shanshan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2106.08897", "submitter": "Dezhen Song", "authors": "Shuangyu Xie, Chengsong Hu, Muthukumar Bagavathiannan, and Dezhen Song", "title": "Toward Robotic Weed Control: Detection of Nutsedge Weed in Bermudagrass\n  Turf Using Inaccurate and Insufficient Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To enable robotic weed control, we develop algorithms to detect nutsedge weed\nfrom bermudagrass turf. Due to the similarity between the weed and the\nbackground turf, manual data labeling is expensive and error-prone.\nConsequently, directly applying deep learning methods for object detection\ncannot generate satisfactory results. Building on an instance detection\napproach (i.e. Mask R-CNN), we combine synthetic data with raw data to train\nthe network. We propose an algorithm to generate high fidelity synthetic data,\nadopting different levels of annotations to reduce labeling cost. Moreover, we\nconstruct a nutsedge skeleton-based probabilistic map (NSPM) as the neural\nnetwork input to reduce the reliance on pixel-wise precise labeling. We also\nmodify loss function from cross entropy to Kullback-Leibler divergence which\naccommodates uncertainty in the labeling process. We implement the proposed\nalgorithm and compare it with both Faster R-CNN and Mask R-CNN. The results\nshow that our design can effectively overcome the impact of imprecise and\ninsufficient training sample issues and significantly outperform the Faster\nR-CNN counterpart with a false negative rate of only 0.4%. In particular, our\napproach also reduces labeling time by 95% while achieving better performance\nif comparing with the original Mask R-CNN approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:58:00 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Xie", "Shuangyu", ""], ["Hu", "Chengsong", ""], ["Bagavathiannan", "Muthukumar", ""], ["Song", "Dezhen", ""]]}, {"id": "2106.08905", "submitter": "Shuyi Qu", "authors": "Shuyi Qu, Zhenxing Niu, Kaizhu Huang, Jianke Zhu, Matan Protter, Gadi\n  Zimerman, Yinghui Xu", "title": "Structure First Detail Next: Image Inpainting with Pyramid Generator", "comments": "ICCV'21 under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep generative models have achieved promising performance in image\ninpainting. However, it is still very challenging for a neural network to\ngenerate realistic image details and textures, due to its inherent spectral\nbias. By our understanding of how artists work, we suggest to adopt a\n`structure first detail next' workflow for image inpainting. To this end, we\npropose to build a Pyramid Generator by stacking several sub-generators, where\nlower-layer sub-generators focus on restoring image structures while the\nhigher-layer sub-generators emphasize image details. Given an input image, it\nwill be gradually restored by going through the entire pyramid in a bottom-up\nfashion. Particularly, our approach has a learning scheme of progressively\nincreasing hole size, which allows it to restore large-hole images. In\naddition, our method could fully exploit the benefits of learning with\nhigh-resolution images, and hence is suitable for high-resolution image\ninpainting. Extensive experimental results on benchmark datasets have validated\nthe effectiveness of our approach compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:00:16 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Qu", "Shuyi", ""], ["Niu", "Zhenxing", ""], ["Huang", "Kaizhu", ""], ["Zhu", "Jianke", ""], ["Protter", "Matan", ""], ["Zimerman", "Gadi", ""], ["Xu", "Yinghui", ""]]}, {"id": "2106.08914", "submitter": "Hung Le", "authors": "Hung Le, Nancy F. Chen, Steven C.H. Hoi", "title": "$C^3$: Compositional Counterfactual Constrastive Learning for\n  Video-grounded Dialogues", "comments": "22 pages, 11 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-grounded dialogue systems aim to integrate video understanding and\ndialogue understanding to generate responses that are relevant to both the\ndialogue and video context. Most existing approaches employ deep learning\nmodels and have achieved remarkable performance, given the relatively small\ndatasets available. However, the results are partly accomplished by exploiting\nbiases in the datasets rather than developing multimodal reasoning, resulting\nin limited generalization. In this paper, we propose a novel approach of\nCompositional Counterfactual Contrastive Learning ($C^3$) to develop\ncontrastive training between factual and counterfactual samples in\nvideo-grounded dialogues. Specifically, we design factual/counterfactual\nsampling based on the temporal steps in videos and tokens in dialogues and\npropose contrastive loss functions that exploit object-level or action-level\nvariance. Different from prior approaches, we focus on contrastive hidden state\nrepresentations among compositional output tokens to optimize the\nrepresentation space in a generation setting. We achieved promising performance\ngains on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark and showed the\nbenefits of our approach in grounding video and dialogue context.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:05:27 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Le", "Hung", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2106.08917", "submitter": "James Tompkin", "authors": "Numair Khan, Min H. Kim, James Tompkin", "title": "Differentiable Diffusion for Dense Depth Estimation from Multi-view\n  Images", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to estimate dense depth by optimizing a sparse set of\npoints such that their diffusion into a depth map minimizes a multi-view\nreprojection error from RGB supervision. We optimize point positions, depths,\nand weights with respect to the loss by differential splatting that models\npoints as Gaussians with analytic transmittance. Further, we develop an\nefficient optimization routine that can simultaneously optimize the 50k+ points\nrequired for complex scene reconstruction. We validate our routine using ground\ntruth data and show high reconstruction quality. Then, we apply this to light\nfield and wider baseline images via self supervision, and show improvements in\nboth average and outlier error for depth maps diffused from inaccurate sparse\npoints. Finally, we compare qualitative and quantitative results to image\nprocessing and deep learning methods. http://visual.cs.brown.edu/diffdiffdepth\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:17:34 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 15:43:24 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Khan", "Numair", ""], ["Kim", "Min H.", ""], ["Tompkin", "James", ""]]}, {"id": "2106.08921", "submitter": "Kinjal Pravinbhai Patel Ms", "authors": "Kinjal Patel, Eric Hunsberger, Sean Batir, and Chris Eliasmith", "title": "A Spiking Neural Network for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to investigate the scalability of neuromorphic computing for computer\nvision, with the objective of replicating non-neuromorphic performance on\ncomputer vision tasks while reducing power consumption. We convert the deep\nArtificial Neural Network (ANN) architecture U-Net to a Spiking Neural Network\n(SNN) architecture using the Nengo framework. Both rate-based and spike-based\nmodels are trained and optimized for benchmarking performance and power, using\na modified version of the ISBI 2D EM Segmentation dataset consisting of\nmicroscope images of cells. We propose a partitioning method to optimize\ninter-chip communication to improve speed and energy efficiency when deploying\nmulti-chip networks on the Loihi neuromorphic chip. We explore the advantages\nof regularizing firing rates of Loihi neurons for converting ANN to SNN with\nminimum accuracy loss and optimized energy consumption. We propose a percentile\nbased regularization loss function to limit the spiking rate of the neuron\nbetween a desired range. The SNN is converted directly from the corresponding\nANN, and demonstrates similar semantic segmentation as the ANN using the same\nnumber of neurons and weights. However, the neuromorphic implementation on the\nIntel Loihi neuromorphic chip is over 2x more energy-efficient than\nconventional hardware (CPU, GPU) when running online (one image at a time).\nThese power improvements are achieved without sacrificing the task performance\naccuracy of the network, and when all weights (Loihi, CPU, and GPU networks)\nare quantized to 8 bits.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:23:18 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Patel", "Kinjal", ""], ["Hunsberger", "Eric", ""], ["Batir", "Sean", ""], ["Eliasmith", "Chris", ""]]}, {"id": "2106.08936", "submitter": "Luka Murn", "authors": "Luka Murn, Saverio Blasi, Alan F. Smeaton and Marta Mrak", "title": "Improved CNN-based Learning of Interpolation Filters for Low-Complexity\n  Inter Prediction in Video Coding", "comments": "IEEE Open Journal of Signal Processing Special Issue on Applied AI\n  and Machine Learning for Video Coding and Streaming, June 2021", "journal-ref": null, "doi": "10.1109/OJSP.2021.3089439", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The versatility of recent machine learning approaches makes them ideal for\nimprovement of next generation video compression solutions. Unfortunately,\nthese approaches typically bring significant increases in computational\ncomplexity and are difficult to interpret into explainable models, affecting\ntheir potential for implementation within practical video coding applications.\nThis paper introduces a novel explainable neural network-based inter-prediction\nscheme, to improve the interpolation of reference samples needed for fractional\nprecision motion compensation. The approach requires a single neural network to\nbe trained from which a full quarter-pixel interpolation filter set is derived,\nas the network is easily interpretable due to its linear structure. A novel\ntraining framework enables each network branch to resemble a specific\nfractional shift. This practical solution makes it very efficient to use\nalongside conventional video coding schemes. When implemented in the context of\nthe state-of-the-art Versatile Video Coding (VVC) test model, 0.77%, 1.27% and\n2.25% BD-rate savings can be achieved on average for lower resolution sequences\nunder the random access, low-delay B and low-delay P configurations,\nrespectively, while the complexity of the learned interpolation schemes is\nsignificantly reduced compared to the interpolation with full CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:48:01 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Murn", "Luka", ""], ["Blasi", "Saverio", ""], ["Smeaton", "Alan F.", ""], ["Mrak", "Marta", ""]]}, {"id": "2106.08970", "submitter": "Hossein Souri", "authors": "Hossein Souri, Micah Goldblum, Liam Fowl, Rama Chellappa, Tom\n  Goldstein", "title": "Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks\n  Trained from Scratch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the curation of data for machine learning becomes increasingly automated,\ndataset tampering is a mounting threat. Backdoor attackers tamper with training\ndata to embed a vulnerability in models that are trained on that data. This\nvulnerability is then activated at inference time by placing a \"trigger\" into\nthe model's input. Typical backdoor attacks insert the trigger directly into\nthe training data, although the presence of such an attack may be visible upon\ninspection. In contrast, the Hidden Trigger Backdoor Attack achieves poisoning\nwithout placing a trigger into the training data at all. However, this hidden\ntrigger attack is ineffective at poisoning neural networks trained from\nscratch. We develop a new hidden trigger attack, Sleeper Agent, which employs\ngradient matching, data selection, and target model re-training during the\ncrafting process. Sleeper Agent is the first hidden trigger backdoor attack to\nbe effective against neural networks trained from scratch. We demonstrate its\neffectiveness on ImageNet and in black-box settings. Our implementation code\ncan be found at https://github.com/hsouri/Sleeper-Agent.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:09:55 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Souri", "Hossein", ""], ["Goldblum", "Micah", ""], ["Fowl", "Liam", ""], ["Chellappa", "Rama", ""], ["Goldstein", "Tom", ""]]}, {"id": "2106.08983", "submitter": "Tarlan Suleymanov", "authors": "Tarlan Suleymanov, Matthew Gadd, Daniele De Martini, Paul Newman", "title": "The Oxford Road Boundaries Dataset", "comments": "Accepted for publication at the workshop \"3D-DLAD: 3D-Deep Learning\n  for Autonomous Driving\" (WS15), Intelligent Vehicles Symposium (IV 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the Oxford Road Boundaries Dataset, designed for\ntraining and testing machine-learning-based road-boundary detection and\ninference approaches. We have hand-annotated two of the 10 km-long forays from\nthe Oxford Robotcar Dataset and generated from other forays several thousand\nfurther examples with semi-annotated road-boundary masks. To boost the number\nof training samples in this way, we used a vision-based localiser to project\nlabels from the annotated datasets to other traversals at different times and\nweather conditions. As a result, we release 62605 labelled samples, of which\n47639 samples are curated. Each of these samples contains both raw and\nclassified masks for left and right lenses. Our data contains images from a\ndiverse set of scenarios such as straight roads, parked cars, junctions, etc.\nFiles for download and tools for manipulating the labelled data are available\nat: oxford-robotics-institute.github.io/road-boundaries-dataset\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:23:34 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Suleymanov", "Tarlan", ""], ["Gadd", "Matthew", ""], ["De Martini", "Daniele", ""], ["Newman", "Paul", ""]]}, {"id": "2106.08992", "submitter": "Giuseppe Alessio D'Inverno", "authors": "Giuseppe Alessio D'Inverno, Monica Bianchini, Maria Lucia Sampoli,\n  Franco Scarselli", "title": "A unifying point of view on expressive power of GNNs", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) are a wide class of connectionist models for\ngraph processing. They perform an iterative message passing operation on each\nnode and its neighbors, to solve classification/ clustering tasks -- on some\nnodes or on the whole graph -- collecting all such messages, regardless of\ntheir order. Despite the differences among the various models belonging to this\nclass, most of them adopt the same computation scheme, based on a local\naggregation mechanism and, intuitively, the local computation framework is\nmainly responsible for the expressive power of GNNs. In this paper, we prove\nthat the Weisfeiler--Lehman test induces an equivalence relationship on the\ngraph nodes that exactly corresponds to the unfolding equivalence, defined on\nthe original GNN model. Therefore, the results on the expressive power of the\noriginal GNNs can be extended to general GNNs which, under mild conditions, can\nbe proved capable of approximating, in probability and up to any precision, any\nfunction on graphs that respects the unfolding equivalence.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:46:51 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 07:22:22 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["D'Inverno", "Giuseppe Alessio", ""], ["Bianchini", "Monica", ""], ["Sampoli", "Maria Lucia", ""], ["Scarselli", "Franco", ""]]}, {"id": "2106.09003", "submitter": "Yiran Zhong", "authors": "Jiajun Zha, Yiran Zhong, Jing Zhang, Richard Hartley, Liang Zheng", "title": "Invertible Attention", "comments": "19 pages. The code is available at\n  https://github.com/Schwartz-Zha/InvertibleAttention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention has been proved to be an efficient mechanism to capture long-range\ndependencies. However, so far it has not been deployed in invertible networks.\nThis is due to the fact that in order to make a network invertible, every\ncomponent within the network needs to be a bijective transformation, but a\nnormal attention block is not. In this paper, we propose invertible attention\nthat can be plugged into existing invertible models. We mathematically and\nexperimentally prove that the invertibility of an attention model can be\nachieved by carefully constraining its Lipschitz constant. We validate the\ninvertibility of our invertible attention on image reconstruction task with 3\npopular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible\nattention achieves similar performance in comparison with normal non-invertible\nattention on dense prediction tasks. The code is available at\nhttps://github.com/Schwartz-Zha/InvertibleAttention\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:55:02 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 13:01:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zha", "Jiajun", ""], ["Zhong", "Yiran", ""], ["Zhang", "Jing", ""], ["Hartley", "Richard", ""], ["Zheng", "Liang", ""]]}, {"id": "2106.09011", "submitter": "Paola Cascante-Bonilla", "authors": "Paola Cascante-Bonilla, Arshdeep Sekhon, Yanjun Qi, Vicente Ordonez", "title": "Evolving Image Compositions for Feature Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional neural networks for visual recognition require large amounts of\ntraining samples and usually benefit from data augmentation. This paper\nproposes PatchMix, a data augmentation method that creates new samples by\ncomposing patches from pairs of images in a grid-like pattern. These new\nsamples' ground truth labels are set as proportional to the number of patches\nfrom each image. We then add a set of additional losses at the patch-level to\nregularize and to encourage good representations at both the patch and image\nlevels. A ResNet-50 model trained on ImageNet using PatchMix exhibits superior\ntransfer learning capabilities across a wide array of benchmarks. Although\nPatchMix can rely on random pairings and random grid-like patterns for mixing,\nwe explore evolutionary search as a guiding strategy to discover optimal\ngrid-like patterns and image pairing jointly. For this purpose, we conceive a\nfitness function that bypasses the need to re-train a model to evaluate each\nchoice. In this way, PatchMix outperforms a base model on CIFAR-10 (+1.91),\nCIFAR-100 (+5.31), Tiny Imagenet (+3.52), and ImageNet (+1.16) by significant\nmargins, also outperforming previous state-of-the-art pairwise augmentation\nstrategies.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:57:18 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cascante-Bonilla", "Paola", ""], ["Sekhon", "Arshdeep", ""], ["Qi", "Yanjun", ""], ["Ordonez", "Vicente", ""]]}, {"id": "2106.09015", "submitter": "Ke Li", "authors": "Shichong Peng, Alireza Moazeni, Ke Li", "title": "Cascading Modular Network (CAM-Net) for Multimodal Image Synthesis", "comments": "Videos available as ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models such as GANs have driven impressive advances in\nconditional image synthesis in recent years. A persistent challenge has been to\ngenerate diverse versions of output images from the same input image, due to\nthe problem of mode collapse: because only one ground truth output image is\ngiven per input image, only one mode of the conditional distribution is\nmodelled. In this paper, we focus on this problem of multimodal conditional\nimage synthesis and build on the recently proposed technique of Implicit\nMaximum Likelihood Estimation (IMLE). Prior IMLE-based methods required\ndifferent architectures for different tasks, which limit their applicability,\nand were lacking in fine details in the generated images. We propose CAM-Net, a\nunified architecture that can be applied to a broad range of tasks.\nAdditionally, it is capable of generating convincing high frequency details,\nachieving a reduction of the Frechet Inception Distance (FID) by up to 45.3%\ncompared to the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:58:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Peng", "Shichong", ""], ["Moazeni", "Alireza", ""], ["Li", "Ke", ""]]}, {"id": "2106.09016", "submitter": "Marco De Nadai", "authors": "Yahui Liu, Enver Sangineto, Yajing Chen, Linchao Bao, Haoxian Zhang,\n  Nicu Sebe, Bruno Lepri, Wei Wang and Marco De Nadai", "title": "Smoothing the Disentangled Latent Style Space for Unsupervised\n  Image-to-Image Translation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-Image (I2I) multi-domain translation models are usually evaluated\nalso using the quality of their semantic interpolation results. However,\nstate-of-the-art models frequently show abrupt changes in the image appearance\nduring interpolation, and usually perform poorly in interpolations across\ndomains. In this paper, we propose a new training protocol based on three\nspecific losses which help a translation network to learn a smooth and\ndisentangled latent style space in which: 1) Both intra- and inter-domain\ninterpolations correspond to gradual changes in the generated images and 2) The\ncontent of the source image is better preserved during the translation.\nMoreover, we propose a novel evaluation metric to properly measure the\nsmoothness of latent style space of I2I translation models. The proposed method\ncan be plugged into existing translation approaches, and our extensive\nexperiments on different datasets show that it can significantly boost the\nquality of the generated images and the graduality of the interpolations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:58:21 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Liu", "Yahui", ""], ["Sangineto", "Enver", ""], ["Chen", "Yajing", ""], ["Bao", "Linchao", ""], ["Zhang", "Haoxian", ""], ["Sebe", "Nicu", ""], ["Lepri", "Bruno", ""], ["Wang", "Wei", ""], ["De Nadai", "Marco", ""]]}, {"id": "2106.09017", "submitter": "Haoxiang Wang", "authors": "Haoxiang Wang, Han Zhao, Bo Li", "title": "Bridging Multi-Task Learning and Meta-Learning: Towards Efficient\n  Training and Effective Adaptation", "comments": "ICML 2021 camera-ready version. Code is released at\n  https://github.com/AI-secure/multi-task-learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) aims to improve the generalization of several\nrelated tasks by learning them jointly. As a comparison, in addition to the\njoint training scheme, modern meta-learning allows unseen tasks with limited\nlabels during the test phase, in the hope of fast adaptation over them. Despite\nthe subtle difference between MTL and meta-learning in the problem formulation,\nboth learning paradigms share the same insight that the shared structure\nbetween existing training tasks could lead to better generalization and\nadaptation. In this paper, we take one important step further to understand the\nclose connection between these two learning paradigms, through both theoretical\nanalysis and empirical investigation. Theoretically, we first demonstrate that\nMTL shares the same optimization formulation with a class of gradient-based\nmeta-learning (GBML) algorithms. We then prove that for over-parameterized\nneural networks with sufficient depth, the learned predictive functions of MTL\nand GBML are close. In particular, this result implies that the predictions\ngiven by these two models are similar over the same unseen task. Empirically,\nwe corroborate our theoretical findings by showing that, with proper\nimplementation, MTL is competitive against state-of-the-art GBML algorithms on\na set of few-shot image classification benchmarks. Since existing GBML\nalgorithms often involve costly second-order bi-level optimization, our\nfirst-order MTL method is an order of magnitude faster on large-scale datasets\nsuch as mini-ImageNet. We believe this work could help bridge the gap between\nthese two learning paradigms, and provide a computationally efficient\nalternative to GBML that also supports fast task adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:58:23 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Haoxiang", ""], ["Zhao", "Han", ""], ["Li", "Bo", ""]]}, {"id": "2106.09018", "submitter": "Zheng Zhang", "authors": "Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan Wang, Fangyun\n  Wei, Xiang Bai, Zicheng Liu", "title": "End-to-End Semi-Supervised Object Detection with Soft Teacher", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an end-to-end semi-supervised object detection approach,\nin contrast to previous more complex multi-stage methods. The end-to-end\ntraining gradually improves pseudo label qualities during the curriculum, and\nthe more and more accurate pseudo labels in turn benefit object detection\ntraining. We also propose two simple yet effective techniques within this\nframework: a soft teacher mechanism where the classification loss of each\nunlabeled bounding box is weighed by the classification score produced by the\nteacher network; a box jittering approach to select reliable pseudo boxes for\nthe learning of box regression. On COCO benchmark, the proposed approach\noutperforms previous methods by a large margin under various labeling ratios,\ni.e. 1\\%, 5\\% and 10\\%. Moreover, our approach proves to perform also well when\nthe amount of labeled data is relatively large. For example, it can improve a\n40.9 mAP baseline detector trained using the full COCO training set by +3.6\nmAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the\nstate-of-the-art Swin Transformer-based object detector (58.9 mAP on test-dev),\nit can still significantly improve the detection accuracy by +1.5 mAP, reaching\n60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching\n52.4 mAP, pushing the new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:59:30 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 16:59:32 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xu", "Mengde", ""], ["Zhang", "Zheng", ""], ["Hu", "Han", ""], ["Wang", "Jianfeng", ""], ["Wang", "Lijuan", ""], ["Wei", "Fangyun", ""], ["Bai", "Xiang", ""], ["Liu", "Zicheng", ""]]}, {"id": "2106.09035", "submitter": "Tony Bonnaire", "authors": "Tony Bonnaire, Aur\\'elien Decelle, Nabila Aghanim", "title": "Regularization of Mixture Models for Robust Principal Graph Learning", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regularized version of Mixture Models is proposed to learn a principal\ngraph from a distribution of $D$-dimensional data points. In the particular\ncase of manifold learning for ridge detection, we assume that the underlying\nmanifold can be modeled as a graph structure acting like a topological prior\nfor the Gaussian clusters turning the problem into a maximum a posteriori\nestimation. Parameters of the model are iteratively estimated through an\nExpectation-Maximization procedure making the learning of the structure\ncomputationally efficient with guaranteed convergence for any graph prior in a\npolynomial time. We also embed in the formalism a natural way to make the\nalgorithm robust to outliers of the pattern and heteroscedasticity of the\nmanifold sampling coherently with the graph structure. The method uses a graph\nprior given by the minimum spanning tree that we extend using random\nsub-samplings of the dataset to take into account cycles that can be observed\nin the spatial distribution.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:00:02 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Bonnaire", "Tony", ""], ["Decelle", "Aur\u00e9lien", ""], ["Aghanim", "Nabila", ""]]}, {"id": "2106.09051", "submitter": "Paul Henderson", "authors": "Paul Henderson, Christoph H. Lampert, Bernd Bickel", "title": "Unsupervised Video Prediction from a Single Frame by Estimating 3D\n  Dynamic Scene Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal in this work is to generate realistic videos given just one initial\nframe as input. Existing unsupervised approaches to this task do not consider\nthe fact that a video typically shows a 3D environment, and that this should\nremain coherent from frame to frame even as the camera and objects move. We\naddress this by developing a model that first estimates the latent 3D structure\nof the scene, including the segmentation of any moving objects. It then\npredicts future frames by simulating the object and camera dynamics, and\nrendering the resulting views. Importantly, it is trained end-to-end using only\nthe unsupervised objective of predicting future frames, without any 3D\ninformation nor segmentation annotations. Experiments on two challenging\ndatasets of natural videos show that our model can estimate 3D structure and\nmotion segmentation from a single frame, and hence generate plausible and\nvaried predictions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:00:12 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Henderson", "Paul", ""], ["Lampert", "Christoph H.", ""], ["Bickel", "Bernd", ""]]}, {"id": "2106.09064", "submitter": "Mert Seker", "authors": "Mert Seker, Anssi M\\\"annist\\\"o, Alexandros Iosifidis and Jenni\n  Raitoharju", "title": "Automatic Main Character Recognition for Photographic Studies", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Main characters in images are the most important humans that catch the\nviewer's attention upon first look, and they are emphasized by properties such\nas size, position, color saturation, and sharpness of focus. Identifying the\nmain character in images plays an important role in traditional photographic\nstudies and media analysis, but the task is performed manually and can be slow\nand laborious. Furthermore, selection of main characters can be sometimes\nsubjective. In this paper, we analyze the feasibility of solving the main\ncharacter recognition needed for photographic studies automatically and propose\na method for identifying the main characters. The proposed method uses machine\nlearning based human pose estimation along with traditional computer vision\napproaches for this task. We approach the task as a binary classification\nproblem where each detected human is classified either as a main character or\nnot. To evaluate both the subjectivity of the task and the performance of our\nmethod, we collected a dataset of 300 varying images from multiple sources and\nasked five people, a photographic researcher and four other persons, to\nannotate the main characters. Our analysis showed a relatively high agreement\nbetween different annotators. The proposed method achieved a promising F1 score\nof 0.83 on the full image set and 0.96 on a subset evaluated as most clear and\nimportant cases by the photographic researcher.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:14:45 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Seker", "Mert", ""], ["M\u00e4nnist\u00f6", "Anssi", ""], ["Iosifidis", "Alexandros", ""], ["Raitoharju", "Jenni", ""]]}, {"id": "2106.09065", "submitter": "Lucas Caccia", "authors": "Lucas Caccia, Joelle Pineau", "title": "SPeCiaL: Self-Supervised Pretraining for Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents SPeCiaL: a method for unsupervised pretraining of\nrepresentations tailored for continual learning. Our approach devises a\nmeta-learning objective that differentiates through a sequential learning\nprocess. Specifically, we train a linear model over the representations to\nmatch different augmented views of the same image together, each view presented\nsequentially. The linear model is then evaluated on both its ability to\nclassify images it just saw, and also on images from previous iterations. This\ngives rise to representations that favor quick knowledge retention with minimal\nforgetting. We evaluate SPeCiaL in the Continual Few-Shot Learning setting, and\nshow that it can match or outperform other supervised pretraining approaches.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:15:15 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Caccia", "Lucas", ""], ["Pineau", "Joelle", ""]]}, {"id": "2106.09076", "submitter": "Saad Nadeem", "authors": "Donghoon Lee, Sadegh R Alam, Jue Jiang, Pengpeng Zhang, Saad Nadeem\n  and Yu-Chi Hu", "title": "Deformation Driven Seq2Seq Longitudinal Tumor and Organs-at-Risk\n  Prediction for Radiotherapy", "comments": "Medical Physics 2021, Saad Nadeem and Yu-Chi Hu contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Radiotherapy presents unique challenges and clinical requirements\nfor longitudinal tumor and organ-at-risk (OAR) prediction during treatment. The\nchallenges include tumor inflammation/edema and radiation-induced changes in\norgan geometry, whereas the clinical requirements demand flexibility in\ninput/output sequence timepoints to update the predictions on rolling basis and\nthe grounding of all predictions in relationship to the pre-treatment imaging\ninformation for response and toxicity assessment in adaptive radiotherapy.\nMethods: To deal with the aforementioned challenges and to comply with the\nclinical requirements, we present a novel 3D sequence-to-sequence model based\non Convolution Long Short Term Memory (ConvLSTM) that makes use of series of\ndeformation vector fields (DVF) between individual timepoints and reference\npre-treatment/planning CTs to predict future anatomical deformations and\nchanges in gross tumor volume as well as critical OARs. High-quality DVF\ntraining data is created by employing hyper-parameter optimization on the\nsubset of the training data with DICE coefficient and mutual information\nmetric. We validated our model on two radiotherapy datasets: a publicly\navailable head-and-neck dataset (28 patients with manually contoured pre-,\nmid-, and post-treatment CTs), and an internal non-small cell lung cancer\ndataset (63 patients with manually contoured planning CT and 6 weekly CBCTs).\nResults: The use of DVF representation and skip connections overcomes the\nblurring issue of ConvLSTM prediction with the traditional image\nrepresentation. The mean and standard deviation of DICE for predictions of lung\nGTV at week 4, 5, and 6 were 0.83$\\pm$0.09, 0.82$\\pm$0.08, and 0.81$\\pm$0.10,\nrespectively, and for post-treatment ipsilateral and contralateral parotids,\nwere 0.81$\\pm$0.06 and 0.85$\\pm$0.02.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:29:16 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 01:58:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Lee", "Donghoon", ""], ["Alam", "Sadegh R", ""], ["Jiang", "Jue", ""], ["Zhang", "Pengpeng", ""], ["Nadeem", "Saad", ""], ["Hu", "Yu-Chi", ""]]}, {"id": "2106.09121", "submitter": "Jiahao Su", "authors": "Jiahao Su, Wonmin Byeon, Furong Huang", "title": "Scaling-up Diverse Orthogonal Convolutional Networks with a Paraunitary\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enforcing orthogonality in neural networks is an antidote for gradient\nvanishing/exploding problems, sensitivity by adversarial perturbation, and\nbounding generalization errors. However, many previous approaches are\nheuristic, and the orthogonality of convolutional layers is not systematically\nstudied: some of these designs are not exactly orthogonal, while others only\nconsider standard convolutional layers and propose specific classes of their\nrealizations. To address this problem, we propose a theoretical framework for\northogonal convolutional layers, which establishes the equivalence between\nvarious orthogonal convolutional layers in the spatial domain and the\nparaunitary systems in the spectral domain. Since there exists a complete\nspectral factorization of paraunitary systems, any orthogonal convolution layer\ncan be parameterized as convolutions of spatial filters. Our framework endows\nhigh expressive power to various convolutional layers while maintaining their\nexact orthogonality. Furthermore, our layers are memory and computationally\nefficient for deep networks compared to previous designs. Our versatile\nframework, for the first time, enables the study of architecture designs for\ndeep orthogonal networks, such as choices of skip connection, initialization,\nstride, and dilation. Consequently, we scale up orthogonal networks to deep\narchitectures, including ResNet, WideResNet, and ShuffleNet, substantially\nincreasing the performance over the traditional shallow orthogonal networks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 20:50:59 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Su", "Jiahao", ""], ["Byeon", "Wonmin", ""], ["Huang", "Furong", ""]]}, {"id": "2106.09141", "submitter": "Aida Nematzadeh", "authors": "Lisa Anne Hendricks and Aida Nematzadeh", "title": "Probing Image-Language Transformers for Verb Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal image-language transformers have achieved impressive results on a\nvariety of tasks that rely on fine-tuning (e.g., visual question answering and\nimage retrieval). We are interested in shedding light on the quality of their\npretrained representations -- in particular, if these models can distinguish\ndifferent types of verbs or if they rely solely on nouns in a given sentence.\nTo do so, we collect a dataset of image-sentence pairs (in English) consisting\nof 421 verbs that are either visual or commonly found in the pretraining data\n(i.e., the Conceptual Captions dataset). We use this dataset to evaluate\npretrained image-language transformers and find that they fail more in\nsituations that require verb understanding compared to other parts of speech.\nWe also investigate what category of verbs are particularly challenging.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 21:36:36 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Nematzadeh", "Aida", ""]]}, {"id": "2106.09157", "submitter": "Dewen Zeng", "authors": "Dewen Zeng, Yawen Wu, Xinrong Hu, Xiaowei Xu, Haiyun Yuan, Meiping\n  Huang, Jian Zhuang, Jingtong Hu and Yiyu Shi", "title": "Positional Contrastive Learning for Volumetric Medical Image\n  Segmentation", "comments": "8 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning heavily depends on the availability of large\nlabeled training sets. However, it is hard to get large labeled datasets in\nmedical image domain because of the strict privacy concern and costly labeling\nefforts. Contrastive learning, an unsupervised learning technique, has been\nproved powerful in learning image-level representations from unlabeled data.\nThe learned encoder can then be transferred or fine-tuned to improve the\nperformance of downstream tasks with limited labels. A critical step in\ncontrastive learning is the generation of contrastive data pairs, which is\nrelatively simple for natural image classification but quite challenging for\nmedical image segmentation due to the existence of the same tissue or organ\nacross the dataset. As a result, when applied to medical image segmentation,\nmost state-of-the-art contrastive learning frameworks inevitably introduce a\nlot of false-negative pairs and result in degraded segmentation quality. To\naddress this issue, we propose a novel positional contrastive learning (PCL)\nframework to generate contrastive data pairs by leveraging the position\ninformation in volumetric medical images. Experimental results on CT and MRI\ndatasets demonstrate that the proposed PCL method can substantially improve the\nsegmentation performance compared to existing methods in both semi-supervised\nsetting and transfer learning setting.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 22:15:28 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 03:49:32 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zeng", "Dewen", ""], ["Wu", "Yawen", ""], ["Hu", "Xinrong", ""], ["Xu", "Xiaowei", ""], ["Yuan", "Haiyun", ""], ["Huang", "Meiping", ""], ["Zhuang", "Jian", ""], ["Hu", "Jingtong", ""], ["Shi", "Yiyu", ""]]}, {"id": "2106.09171", "submitter": "Rodrigo Mira", "authors": "Pingchuan Ma, Rodrigo Mira, Stavros Petridis, Bj\\\"orn W. Schuller and\n  Maja Pantic", "title": "LiRA: Learning Visual Speech Representations from Audio through\n  Self-supervision", "comments": "Accepted for publication at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large amount of audiovisual content being shared online today has drawn\nsubstantial attention to the prospect of audiovisual self-supervised learning.\nRecent works have focused on each of these modalities separately, while others\nhave attempted to model both simultaneously in a cross-modal fashion. However,\ncomparatively little attention has been given to leveraging one modality as a\ntraining objective to learn from the other. In this work, we propose Learning\nvisual speech Representations from Audio via self-supervision (LiRA).\nSpecifically, we train a ResNet+Conformer model to predict acoustic features\nfrom unlabelled visual speech. We find that this pre-trained model can be\nleveraged towards word-level and sentence-level lip-reading through feature\nextraction and fine-tuning experiments. We show that our approach significantly\noutperforms other self-supervised methods on the Lip Reading in the Wild (LRW)\ndataset and achieves state-of-the-art performance on Lip Reading Sentences 2\n(LRS2) using only a fraction of the total labelled data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 23:20:06 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ma", "Pingchuan", ""], ["Mira", "Rodrigo", ""], ["Petridis", "Stavros", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Pantic", "Maja", ""]]}, {"id": "2106.09177", "submitter": "Alexander Wong", "authors": "Alexander Wong, Adam Dorfman, Paul McInnis, and Hayden Gunraj", "title": "Insights into Data through Model Behaviour: An Explainability-driven\n  Strategy for Data Auditing for Responsible Computer Vision Applications", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we take a departure and explore an explainability-driven\nstrategy to data auditing, where actionable insights into the data at hand are\ndiscovered through the eyes of quantitative explainability on the behaviour of\na dummy model prototype when exposed to data. We demonstrate this strategy by\nauditing two popular medical benchmark datasets, and discover hidden data\nquality issues that lead deep learning models to make predictions for the wrong\nreasons. The actionable insights gained from this explainability driven data\nauditing strategy is then leveraged to address the discovered issues to enable\nthe creation of high-performing deep learning models with appropriate\nprediction behaviour. The hope is that such an explainability-driven strategy\ncan be complimentary to data-driven strategies to facilitate for more\nresponsible development of machine learning algorithms for computer vision\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 23:46:39 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wong", "Alexander", ""], ["Dorfman", "Adam", ""], ["McInnis", "Paul", ""], ["Gunraj", "Hayden", ""]]}, {"id": "2106.09178", "submitter": "Justin Kay", "authors": "Justin Kay and Matt Merrifield", "title": "The Fishnet Open Images Database: A Dataset for Fish Detection and\n  Fine-Grained Categorization in Fisheries", "comments": "In 8th Workshop on Fine-Grained Visual Categorization at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera-based electronic monitoring (EM) systems are increasingly being\ndeployed onboard commercial fishing vessels to collect essential data for\nfisheries management and regulation. These systems generate large quantities of\nvideo data which must be reviewed on land by human experts. Computer vision can\nassist this process by automatically detecting and classifying fish species,\nhowever the lack of existing public data in this domain has hindered progress.\nTo address this, we present the Fishnet Open Images Database, a large dataset\nof EM imagery for fish detection and fine-grained categorization onboard\ncommercial fishing vessels. The dataset consists of 86,029 images containing 34\nobject classes, making it the largest and most diverse public dataset of\nfisheries EM imagery to-date. It includes many of the characteristic challenges\nof EM data: visual similarity between species, skewed class distributions,\nharsh weather conditions, and chaotic crew activity. We evaluate the\nperformance of existing detection and classification algorithms and demonstrate\nthat the dataset can serve as a challenging benchmark for development of\ncomputer vision algorithms in fisheries. The dataset is available at\nhttps://www.fishnet.ai/.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 23:53:18 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kay", "Justin", ""], ["Merrifield", "Matt", ""]]}, {"id": "2106.09198", "submitter": "Haoran Xie", "authors": "Haoran Xie and Yuki Fujita and Kazunori Miyata", "title": "Learning Perceptual Manifold of Fonts", "comments": "9 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Along the rapid development of deep learning techniques in generative models,\nit is becoming an urgent issue to combine machine intelligence with human\nintelligence to solve the practical applications. Motivated by this\nmethodology, this work aims to adjust the machine generated character fonts\nwith the effort of human workers in the perception study. Although numerous\nfonts are available online for public usage, it is difficult and challenging to\ngenerate and explore a font to meet the preferences for common users. To solve\nthe specific issue, we propose the perceptual manifold of fonts to visualize\nthe perceptual adjustment in the latent space of a generative model of fonts.\nIn our framework, we adopt the variational autoencoder network for the font\ngeneration. Then, we conduct a perceptual study on the generated fonts from the\nmulti-dimensional latent space of the generative model. After we obtained the\ndistribution data of specific preferences, we utilize manifold learning\napproach to visualize the font distribution. In contrast to the conventional\nuser interface in our user study, the proposed font-exploring user interface is\nefficient and helpful in the designated user preference.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 01:22:52 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xie", "Haoran", ""], ["Fujita", "Yuki", ""], ["Miyata", "Kazunori", ""]]}, {"id": "2106.09199", "submitter": "Jicheng Li", "authors": "Jicheng Li, Anjana Bhat, Roghayeh Barmaki", "title": "A Two-stage Multi-modal Affect Analysis Framework for Children with\n  Autism Spectrum Disorder", "comments": "8 pages including reference; 8 figures", "journal-ref": "The AAAI-21 Workshop On Affective Content Analysis; 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autism spectrum disorder (ASD) is a developmental disorder that influences\nthe communication and social behavior of a person in a way that those in the\nspectrum have difficulty in perceiving other people's facial expressions, as\nwell as presenting and communicating emotions and affect via their own faces\nand bodies. Some efforts have been made to predict and improve children with\nASD's affect states in play therapy, a common method to improve children's\nsocial skills via play and games. However, many previous works only used\npre-trained models on benchmark emotion datasets and failed to consider the\ndistinction in emotion between typically developing children and children with\nautism. In this paper, we present an open-source two-stage multi-modal approach\nleveraging acoustic and visual cues to predict three main affect states of\nchildren with ASD's affect states (positive, negative, and neutral) in\nreal-world play therapy scenarios, and achieved an overall accuracy of 72:40%.\nThis work presents a novel way to combine human expertise and machine\nintelligence for ASD affect recognition by proposing a two-stage schema.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 01:28:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Li", "Jicheng", ""], ["Bhat", "Anjana", ""], ["Barmaki", "Roghayeh", ""]]}, {"id": "2106.09201", "submitter": "Ghada Zamzmi", "authors": "Ghada Zamzmi, Vandana Sachdev, and Sameer Antani", "title": "Trilateral Attention Network for Real-time Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate segmentation of medical images into anatomically meaningful regions\nis critical for the extraction of quantitative indices or biomarkers. The\ncommon pipeline for segmentation comprises regions of interest detection stage\nand segmentation stage, which are independent of each other and typically\nperformed using separate deep learning networks. The performance of the\nsegmentation stage highly relies on the extracted set of spatial features and\nthe receptive fields. In this work, we propose an end-to-end network, called\nTrilateral Attention Network (TaNet), for real-time detection and segmentation\nin medical images. TaNet has a module for region localization, and three\nsegmentation pathways: 1) handcrafted pathway with hand-designed convolutional\nkernels, 2) detail pathway with regular convolutional kernels, and 3) a global\npathway to enlarge the receptive field. The first two pathways encode rich\nhandcrafted and low-level features extracted by hand-designed and regular\nkernels while the global pathway encodes high-level context information. By\njointly training the network for localization and segmentation using different\nsets of features, TaNet achieved superior performance, in terms of accuracy and\nspeed, when evaluated on an echocardiography dataset for cardiac segmentation.\nThe code and models will be made publicly available in TaNet Github page.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 01:46:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zamzmi", "Ghada", ""], ["Sachdev", "Vandana", ""], ["Antani", "Sameer", ""]]}, {"id": "2106.09212", "submitter": "Jue Wang", "authors": "Jue Wang, Gedas Bertasius, Du Tran, Lorenzo Torresani", "title": "Long-Short Temporal Contrastive Learning of Video Transformers", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video transformers have recently emerged as a competitive alternative to 3D\nCNNs for video understanding. However, due to their large number of parameters\nand reduced inductive biases, these models require supervised pretraining on\nlarge-scale image datasets to achieve top performance. In this paper, we\nempirically demonstrate that self-supervised pretraining of video transformers\non video-only datasets can lead to action recognition results that are on par\nor better than those obtained with supervised pretraining on large-scale image\ndatasets, even massive ones such as ImageNet-21K. Since transformer-based\nmodels are effective at capturing dependencies over extended temporal spans, we\npropose a simple learning procedure that forces the model to match a long-term\nview to a short-term view of the same video. Our approach, named Long-Short\nTemporal Contrastive Learning (LSTCL), enables video transformers to learn an\neffective clip-level representation by predicting temporal context captured\nfrom a longer temporal extent. To demonstrate the generality of our findings,\nwe implement and validate our approach under three different self-supervised\ncontrastive learning frameworks (MoCo v3, BYOL, SimSiam) using two distinct\nvideo-transformer architectures, including an improved variant of the Swin\nTransformer augmented with space-time attention. We conduct a thorough ablation\nstudy and show that LSTCL achieves competitive performance on multiple video\nbenchmarks and represents a convincing alternative to supervised image-based\npretraining.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 02:30:26 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 01:13:15 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wang", "Jue", ""], ["Bertasius", "Gedas", ""], ["Tran", "Du", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "2106.09222", "submitter": "Ousmane Dia", "authors": "Ousmane Amadou Dia, Theofanis Karaletsos, Caner Hazirbas, Cristian\n  Canton Ferrer, Ilknur Kaynar Kabul, Erik Meijer", "title": "Localized Uncertainty Attacks", "comments": "CVPR 2021 Workshop on Adversarial Machine Learning in Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The susceptibility of deep learning models to adversarial perturbations has\nstirred renewed attention in adversarial examples resulting in a number of\nattacks. However, most of these attacks fail to encompass a large spectrum of\nadversarial perturbations that are imperceptible to humans. In this paper, we\npresent localized uncertainty attacks, a novel class of threat models against\ndeterministic and stochastic classifiers. Under this threat model, we create\nadversarial examples by perturbing only regions in the inputs where a\nclassifier is uncertain. To find such regions, we utilize the predictive\nuncertainty of the classifier when the classifier is stochastic or, we learn a\nsurrogate model to amortize the uncertainty when it is deterministic. Unlike\n$\\ell_p$ ball or functional attacks which perturb inputs indiscriminately, our\ntargeted changes can be less perceptible. When considered under our threat\nmodel, these attacks still produce strong adversarial examples; with the\nexamples retaining a greater degree of similarity with the inputs.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 03:07:22 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Dia", "Ousmane Amadou", ""], ["Karaletsos", "Theofanis", ""], ["Hazirbas", "Caner", ""], ["Ferrer", "Cristian Canton", ""], ["Kabul", "Ilknur Kaynar", ""], ["Meijer", "Erik", ""]]}, {"id": "2106.09223", "submitter": "Yutian Pang", "authors": "Yutian Pang, Sheng Cheng, Jueming Hu, Yongming Liu", "title": "Evaluating the Robustness of Bayesian Neural Networks Against Different\n  Types of Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To evaluate the robustness gain of Bayesian neural networks on image\nclassification tasks, we perform input perturbations, and adversarial attacks\nto the state-of-the-art Bayesian neural networks, with a benchmark CNN model as\nreference. The attacks are selected to simulate signal interference and\ncyberattacks towards CNN-based machine learning systems. The result shows that\na Bayesian neural network achieves significantly higher robustness against\nadversarial attacks generated against a deterministic neural network model,\nwithout adversarial training. The Bayesian posterior can act as the safety\nprecursor of ongoing malicious activities. Furthermore, we show that the\nstochastic classifier after the deterministic CNN extractor has sufficient\nrobustness enhancement rather than a stochastic feature extractor before the\nstochastic classifier. This advises on utilizing stochastic layers in building\ndecision-making pipelines within a safety-critical domain.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 03:18:59 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Pang", "Yutian", ""], ["Cheng", "Sheng", ""], ["Hu", "Jueming", ""], ["Liu", "Yongming", ""]]}, {"id": "2106.09229", "submitter": "Levy Chaves", "authors": "Levy Chaves, Alceu Bissoto, Eduardo Valle and Sandra Avila", "title": "An Evaluation of Self-Supervised Pre-Training for Skin-Lesion Analysis", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised pre-training appears as an advantageous alternative to\nsupervised pre-trained for transfer learning. By synthesizing annotations on\npretext tasks, self-supervision allows to pre-train models on large amounts of\npseudo-labels before fine-tuning them on the target task. In this work, we\nassess self-supervision for the diagnosis of skin lesions, comparing three\nself-supervised pipelines to a challenging supervised baseline, on five test\ndatasets comprising in- and out-of-distribution samples. Our results show that\nself-supervision is competitive both in improving accuracies and in reducing\nthe variability of outcomes. Self-supervision proves particularly useful for\nlow training data scenarios ($<1\\,500$ and $<150$ samples), where its ability\nto stabilize the outcomes is essential to provide sound results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 03:47:36 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 23:15:36 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chaves", "Levy", ""], ["Bissoto", "Alceu", ""], ["Valle", "Eduardo", ""], ["Avila", "Sandra", ""]]}, {"id": "2106.09244", "submitter": "Chengjun Lu", "authors": "Rui Zhang, Chengjun Lu, Ziheng Jiao and Xuelong Li", "title": "Deep Contrastive Graph Representation via Adaptive Homotopy Learning", "comments": "9 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homotopy model is an excellent tool exploited by diverse research works in\nthe field of machine learning. However, its flexibility is limited due to lack\nof adaptiveness, i.e., manual fixing or tuning the appropriate homotopy\ncoefficients. To address the problem above, we propose a novel adaptive\nhomotopy framework (AH) in which the Maclaurin duality is employed, such that\nthe homotopy parameters can be adaptively obtained. Accordingly, the proposed\nAH can be widely utilized to enhance the homotopy-based algorithm. In\nparticular, in this paper, we apply AH to contrastive learning (AHCL) such that\nit can be effectively transferred from weak-supervised learning (given label\npriori) to unsupervised learning, where soft labels of contrastive learning are\ndirectly and adaptively learned. Accordingly, AHCL has the adaptive ability to\nextract deep features without any sort of prior information. Consequently, the\naffinity matrix formulated by the related adaptive labels can be constructed as\nthe deep Laplacian graph that incorporates the topology of deep representations\nfor the inputs. Eventually, extensive experiments on benchmark datasets\nvalidate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 04:46:04 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhang", "Rui", ""], ["Lu", "Chengjun", ""], ["Jiao", "Ziheng", ""], ["Li", "Xuelong", ""]]}, {"id": "2106.09246", "submitter": "Jong Chul Ye", "authors": "Joonyoung Song, Jong Chul Ye", "title": "Federated CycleGAN for Privacy-Preserving Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation methods such as CycleGAN learn to\nconvert images from one domain to another using unpaired training data sets\nfrom different domains. Unfortunately, these approaches still require centrally\ncollected unpaired records, potentially violating privacy and security issues.\nAlthough the recent federated learning (FL) allows a neural network to be\ntrained without data exchange, the basic assumption of the FL is that all\nclients have their own training data from a similar domain, which is different\nfrom our image-to-image translation scenario in which each client has images\nfrom its unique domain and the goal is to learn image translation between\ndifferent domains without accessing the target domain data. To address this,\nhere we propose a novel federated CycleGAN architecture that can learn image\ntranslation in an unsupervised manner while maintaining the data privacy.\nSpecifically, our approach arises from a novel observation that CycleGAN loss\ncan be decomposed into the sum of client specific local objectives that can be\nevaluated using only their data. This local objective decomposition allows\nmultiple clients to participate in federated CycleGAN training without\nsacrificing performance. Furthermore, our method employs novel switchable\ngenerator and discriminator architecture using Adaptive Instance Normalization\n(AdaIN) that significantly reduces the band-width requirement of the federated\nlearning. Our experimental results on various unsupervised image translation\ntasks show that our federated CycleGAN provides comparable performance compared\nto the non-federated counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 05:01:59 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Song", "Joonyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2106.09249", "submitter": "Ningfei Wang", "authors": "Yulong Cao*, Ningfei Wang*, Chaowei Xiao*, Dawei Yang*, Jin Fang,\n  Ruigang Yang, Qi Alfred Chen, Mingyan Liu, Bo Li (*co-first authors)", "title": "Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion\n  based Perception in Autonomous Driving Under Physical-World Attacks", "comments": "Accepted by IEEE S&P 2021", "journal-ref": null, "doi": "10.1109/SP40001.2021.00076", "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Autonomous Driving (AD) systems, perception is both security and safety\ncritical. Despite various prior studies on its security issues, all of them\nonly consider attacks on camera- or LiDAR-based AD perception alone. However,\nproduction AD systems today predominantly adopt a Multi-Sensor Fusion (MSF)\nbased design, which in principle can be more robust against these attacks under\nthe assumption that not all fusion sources are (or can be) attacked at the same\ntime. In this paper, we present the first study of security issues of MSF-based\nperception in AD systems. We directly challenge the basic MSF design assumption\nabove by exploring the possibility of attacking all fusion sources\nsimultaneously. This allows us for the first time to understand how much\nsecurity guarantee MSF can fundamentally provide as a general defense strategy\nfor AD perception.\n  We formulate the attack as an optimization problem to generate a\nphysically-realizable, adversarial 3D-printed object that misleads an AD system\nto fail in detecting it and thus crash into it. We propose a novel attack\npipeline that addresses two main design challenges: (1) non-differentiable\ntarget camera and LiDAR sensing systems, and (2) non-differentiable cell-level\naggregated features popularly used in LiDAR-based AD perception. We evaluate\nour attack on MSF included in representative open-source industry-grade AD\nsystems in real-world driving scenarios. Our results show that the attack\nachieves over 90% success rate across different object types and MSF. Our\nattack is also found stealthy, robust to victim positions, transferable across\nMSF algorithms, and physical-world realizable after being 3D-printed and\ncaptured by LiDAR and camera devices. To concretely assess the end-to-end\nsafety impact, we further perform simulation evaluation and show that it can\ncause a 100% vehicle collision rate for an industry-grade AD system.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 05:11:07 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Cao*", "Yulong", "", "*co-first authors"], ["Wang*", "Ningfei", "", "*co-first authors"], ["Xiao*", "Chaowei", "", "*co-first authors"], ["Yang*", "Dawei", "", "*co-first authors"], ["Fang", "Jin", "", "*co-first authors"], ["Yang", "Ruigang", "", "*co-first authors"], ["Chen", "Qi Alfred", "", "*co-first authors"], ["Liu", "Mingyan", "", "*co-first authors"], ["Li", "Bo", "", "*co-first authors"]]}, {"id": "2106.09251", "submitter": "Bo Hu", "authors": "Bo Hu, Bryan Seybold, Shan Yang, David Ross, Avneesh Sud, Graham Ruby,\n  Yi Liu", "title": "Optical Mouse: 3D Mouse Pose From Single-View Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to infer the 3D pose of mice, including the limbs and\nfeet, from monocular videos. Many human clinical conditions and their\ncorresponding animal models result in abnormal motion, and accurately measuring\n3D motion at scale offers insights into health. The 3D poses improve\nclassification of health-related attributes over 2D representations. The\ninferred poses are accurate enough to estimate stride length even when the feet\nare mostly occluded. This method could be applied as part of a continuous\nmonitoring system to non-invasively measure animal health.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 05:12:36 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Hu", "Bo", ""], ["Seybold", "Bryan", ""], ["Yang", "Shan", ""], ["Ross", "David", ""], ["Sud", "Avneesh", ""], ["Ruby", "Graham", ""], ["Liu", "Yi", ""]]}, {"id": "2106.09259", "submitter": "Yun-Hao Cao", "authors": "Yun-Hao Cao and Jianxin Wu", "title": "A Random CNN Sees Objects: One Inductive Bias of CNN and Its\n  Applications", "comments": "17 pages, 9 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper starts by revealing a surprising finding: without any learning, a\nrandomly initialized CNN can localize objects surprisingly well. That is, a CNN\nhas an inductive bias to naturally focus on objects, named as Tobias (``The\nobject is at sight'') in this paper. This empirical inductive bias is further\nanalyzed and successfully applied to self-supervised learning. A CNN is\nencouraged to learn representations that focus on the foreground object, by\ntransforming every image into various versions with different backgrounds,\nwhere the foreground and background separation is guided by Tobias.\nExperimental results show that the proposed Tobias significantly improves\ndownstream tasks, especially for object detection. This paper also shows that\nTobias has consistent improvements on training sets of different sizes, and is\nmore resilient to changes in image augmentations. Our codes will be available\nat https://github.com/CupidJay/Tobias.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 06:07:49 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Cao", "Yun-Hao", ""], ["Wu", "Jianxin", ""]]}, {"id": "2106.09300", "submitter": "Wei Mao", "authors": "Wei Mao, Miaomiao Liu, Mathieu Salzmann, Hongdong Li", "title": "Multi-level Motion Attention for Human Motion Prediction", "comments": "Accepted by IJCV. arXiv admin note: substantial text overlap with\n  arXiv:2007.11755", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction aims to forecast future human poses given a\nhistorical motion. Whether based on recurrent or feed-forward neural networks,\nexisting learning based methods fail to model the observation that human motion\ntends to repeat itself, even for complex sports actions and cooking activities.\nHere, we introduce an attention based feed-forward network that explicitly\nleverages this observation. In particular, instead of modeling frame-wise\nattention via pose similarity, we propose to extract motion attention to\ncapture the similarity between the current motion context and the historical\nmotion sub-sequences. In this context, we study the use of different types of\nattention, computed at joint, body part, and full pose levels. Aggregating the\nrelevant past motions and processing the result with a graph convolutional\nnetwork allows us to effectively exploit motion patterns from the long-term\nhistory to predict the future poses. Our experiments on Human3.6M, AMASS and\n3DPW validate the benefits of our approach for both periodical and\nnon-periodical actions. Thanks to our attention model, it yields\nstate-of-the-art results on all three datasets. Our code is available at\nhttps://github.com/wei-mao-2019/HisRepItself.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 08:08:11 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mao", "Wei", ""], ["Liu", "Miaomiao", ""], ["Salzmann", "Mathieu", ""], ["Li", "Hongdong", ""]]}, {"id": "2106.09302", "submitter": "Tobias Ross", "authors": "Tobias Ro{\\ss}, Pierangela Bruno, Annika Reinke, Manuel Wiesenfarth,\n  Lisa Koeppel, Peter M. Full, B\\\"unyamin Pekdemir, Patrick Godau, Darya\n  Trofimova, Fabian Isensee, Sara Moccia, Francesco Calimeri, Beat P.\n  M\\\"uller-Stich, Annette Kopp-Schneider, Lena Maier-Hein", "title": "How can we learn (more) from challenges? A statistical approach to\n  driving future algorithm development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Challenges have become the state-of-the-art approach to benchmark image\nanalysis algorithms in a comparative manner. While the validation on identical\ndata sets was a great step forward, results analysis is often restricted to\npure ranking tables, leaving relevant questions unanswered. Specifically,\nlittle effort has been put into the systematic investigation on what\ncharacterizes images in which state-of-the-art algorithms fail. To address this\ngap in the literature, we (1) present a statistical framework for learning from\nchallenges and (2) instantiate it for the specific task of instrument instance\nsegmentation in laparoscopic videos. Our framework relies on the semantic meta\ndata annotation of images, which serves as foundation for a General Linear\nMixed Models (GLMM) analysis. Based on 51,542 meta data annotations performed\non 2,728 images, we applied our approach to the results of the Robust Medical\nInstrument Segmentation Challenge (ROBUST-MIS) challenge 2019 and revealed\nunderexposure, motion and occlusion of instruments as well as the presence of\nsmoke or other objects in the background as major sources of algorithm failure.\nOur subsequent method development, tailored to the specific remaining issues,\nyielded a deep learning model with state-of-the-art overall performance and\nspecific strengths in the processing of images in which previous methods tended\nto fail. Due to the objectivity and generic applicability of our approach, it\ncould become a valuable tool for validation in the field of medical image\nanalysis and beyond. and segmentation of small, crossing, moving and\ntransparent instrument(s) (parts).\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 08:12:37 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ro\u00df", "Tobias", ""], ["Bruno", "Pierangela", ""], ["Reinke", "Annika", ""], ["Wiesenfarth", "Manuel", ""], ["Koeppel", "Lisa", ""], ["Full", "Peter M.", ""], ["Pekdemir", "B\u00fcnyamin", ""], ["Godau", "Patrick", ""], ["Trofimova", "Darya", ""], ["Isensee", "Fabian", ""], ["Moccia", "Sara", ""], ["Calimeri", "Francesco", ""], ["M\u00fcller-Stich", "Beat P.", ""], ["Kopp-Schneider", "Annette", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2106.09303", "submitter": "Salima Bourbia", "authors": "Salima Bourbia, Ayoub Karine, Aladine Chetouani, Mohammed El Hassouni", "title": "A Multi-task convolutional neural network for blind stereoscopic image\n  quality assessment using naturalness analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of blind stereoscopic image quality\nassessment (NR-SIQA) using a new multi-task deep learning based-method. In the\nfield of stereoscopic vision, the information is fairly distributed between the\nleft and right views as well as the binocular phenomenon. In this work, we\npropose to integrate these characteristics to estimate the quality of\nstereoscopic images without reference through a convolutional neural network.\nOur method is based on two main tasks: the first task predicts naturalness\nanalysis based features adapted to stereo images, while the second task\npredicts the quality of such images. The former, so-called auxiliary task, aims\nto find more robust and relevant features to improve the quality prediction. To\ndo this, we compute naturalness-based features using a Natural Scene Statistics\n(NSS) model in the complex wavelet domain. It allows to capture the statistical\ndependency between pairs of the stereoscopic images. Experiments are conducted\non the well known LIVE PHASE I and LIVE PHASE II databases. The results\nobtained show the relevance of our method when comparing with those of the\nstate-of-the-art. Our code is available online on\nhttps://github.com/Bourbia-Salima/multitask-cnn-nrsiqa_2021.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 08:13:51 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 14:53:26 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 12:33:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Bourbia", "Salima", ""], ["Karine", "Ayoub", ""], ["Chetouani", "Aladine", ""], ["Hassouni", "Mohammed El", ""]]}, {"id": "2106.09309", "submitter": "Niv Zehngut", "authors": "Amir Ben Dror, Niv Zehngut, Avraham Raviv, Evgeny Artyomov, Ran Vitek\n  and Roy Jevnisek", "title": "Layer Folding: Neural Network Depth Reduction using Activation\n  Linearization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the increasing prevalence of deep neural networks, their\napplicability in resource-constrained devices is limited due to their\ncomputational load. While modern devices exhibit a high level of parallelism,\nreal-time latency is still highly dependent on networks' depth. Although recent\nworks show that below a certain depth, the width of shallower networks must\ngrow exponentially, we presume that neural networks typically exceed this\nminimal depth to accelerate convergence and incrementally increase accuracy.\nThis motivates us to transform pre-trained deep networks that already exploit\nsuch advantages into shallower forms. We propose a method that learns whether\nnon-linear activations can be removed, allowing to fold consecutive linear\nlayers into one. We apply our method to networks pre-trained on CIFAR-10 and\nCIFAR-100 and find that they can all be transformed into shallower forms that\nshare a similar depth. Finally, we use our method to provide more efficient\nalternatives to MobileNetV2 and EfficientNet-Lite architectures on the ImageNet\nclassification task.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 08:22:46 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Dror", "Amir Ben", ""], ["Zehngut", "Niv", ""], ["Raviv", "Avraham", ""], ["Artyomov", "Evgeny", ""], ["Vitek", "Ran", ""], ["Jevnisek", "Roy", ""]]}, {"id": "2106.09311", "submitter": "Florian Cassayre", "authors": "Haley Owsianko, Florian Cassayre and Qiyuan Liang", "title": "Controllable Confidence-Based Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image denoising is a classic restoration problem. Yet, current deep learning\nmethods are subject to the problems of generalization and interpretability. To\nmitigate these problems, in this project, we present a framework that is\ncapable of controllable, confidence-based noise removal. The framework is based\non the fusion between two different denoised images, both derived from the same\nnoisy input. One of the two is denoised using generic algorithms (e.g.\nGaussian), which make few assumptions on the input images, therefore,\ngeneralize in all scenarios. The other is denoised using deep learning,\nperforming well on seen datasets. We introduce a set of techniques to fuse the\ntwo components smoothly in the frequency domain. Beyond that, we estimate the\nconfidence of a deep learning denoiser to allow users to interpret the output,\nand provide a fusion strategy that safeguards them against out-of-distribution\ninputs. Through experiments, we demonstrate the effectiveness of the proposed\nframework in different use cases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 08:25:12 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Owsianko", "Haley", ""], ["Cassayre", "Florian", ""], ["Liang", "Qiyuan", ""]]}, {"id": "2106.09336", "submitter": "Andrei Zanfir", "authors": "Mihai Zanfir, Andrei Zanfir, Eduard Gabriel Bazavan, William T.\n  Freeman, Rahul Sukthankar and Cristian Sminchisescu", "title": "THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present THUNDR, a transformer-based deep neural network methodology to\nreconstruct the 3d pose and shape of people, given monocular RGB images. Key to\nour methodology is an intermediate 3d marker representation, where we aim to\ncombine the predictive power of model-free-output architectures and the\nregularizing, anthropometrically-preserving properties of a statistical human\nsurface model like GHUM -- a recently introduced, expressive full body\nstatistical 3d human model, trained end-to-end. Our novel transformer-based\nprediction pipeline can focus on image regions relevant to the task, supports\nself-supervised regimes, and ensures that solutions are consistent with human\nanthropometry. We show state-of-the-art results on Human3.6M and 3DPW, for both\nthe fully-supervised and the self-supervised models, for the task of inferring\n3d human shape, joint positions, and global translation. Moreover, we observe\nvery solid 3d reconstruction performance for difficult human poses collected in\nthe wild.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 09:09:24 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zanfir", "Mihai", ""], ["Zanfir", "Andrei", ""], ["Bazavan", "Eduard Gabriel", ""], ["Freeman", "William T.", ""], ["Sukthankar", "Rahul", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "2106.09358", "submitter": "Gagan Kanojia", "authors": "Sudhakar Kumawat, Gagan Kanojia, and Shanmuganathan Raman", "title": "ShuffleBlock: Shuffle to Regularize Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have enormous representational power which leads them to\noverfit on most datasets. Thus, regularizing them is important in order to\nreduce overfitting and enhance their generalization capabilities. Recently,\nchannel shuffle operation has been introduced for mixing channels in group\nconvolutions in resource efficient networks in order to reduce memory and\ncomputations. This paper studies the operation of channel shuffle as a\nregularization technique in deep convolutional networks. We show that while\nrandom shuffling of channels during training drastically reduce their\nperformance, however, randomly shuffling small patches between channels\nsignificantly improves their performance. The patches to be shuffled are picked\nfrom the same spatial locations in the feature maps such that a patch, when\ntransferred from one channel to another, acts as structured noise for the later\nchannel. We call this method \"ShuffleBlock\". The proposed ShuffleBlock module\nis easy to implement and improves the performance of several baseline networks\non the task of image classification on CIFAR and ImageNet datasets. It also\nachieves comparable and in many cases better performance than many other\nregularization methods. We provide several ablation studies on selecting\nvarious hyperparameters of the ShuffleBlock module and propose a new scheduling\nmethod that further enhances its performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:23:00 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kumawat", "Sudhakar", ""], ["Kanojia", "Gagan", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2106.09369", "submitter": "Moritz Wolter", "authors": "Moritz Wolter and Felix Blanke and Charles Tapley Hoyt and Jochen\n  Garcke", "title": "Wavelet-Packet Powered Deepfake Image Detection", "comments": "Source code is available at\n  https://github.com/gan-police/frequency-forensics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As neural networks become more able to generate realistic artificial images,\nthey have the potential to improve movies, music, video games and make the\ninternet an even more creative and inspiring place. Yet, at the same time, the\nlatest technology potentially enables new digital ways to lie. In response, the\nneed for a diverse and reliable toolbox arises to identify artificial images\nand other content. Previous work primarily relies on pixel-space CNN or the\nFourier transform. To the best of our knowledge, wavelet-based gan analysis and\ndetection methods have been absent thus far. This paper aims to fill this gap\nand describes a wavelet-based approach to gan-generated image analysis and\ndetection. We evaluate our method on FFHQ, CelebA, and LSUN source\nidentification problems and find improved or competitive performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:41:44 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wolter", "Moritz", ""], ["Blanke", "Felix", ""], ["Hoyt", "Charles Tapley", ""], ["Garcke", "Jochen", ""]]}, {"id": "2106.09385", "submitter": "Aditya Singh", "authors": "Aditya Singh, Alessandro Bay, Biswa Sengupta, Andrea Mirabile", "title": "On the Dark Side of Calibration for Modern Neural Networks", "comments": "15 pages including references and supplemental", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Modern neural networks are highly uncalibrated. It poses a significant\nchallenge for safety-critical systems to utilise deep neural networks (DNNs),\nreliably. Many recently proposed approaches have demonstrated substantial\nprogress in improving DNN calibration. However, they hardly touch upon\nrefinement, which historically has been an essential aspect of calibration.\nRefinement indicates separability of a network's correct and incorrect\npredictions. This paper presents a theoretically and empirically supported\nexposition for reviewing a model's calibration and refinement. Firstly, we show\nthe breakdown of expected calibration error (ECE), into predicted confidence\nand refinement. Connecting with this result, we highlight that regularisation\nbased calibration only focuses on naively reducing a model's confidence. This\nlogically has a severe downside to a model's refinement. We support our claims\nthrough rigorous empirical evaluations of many state of the art calibration\napproaches on standard datasets. We find that many calibration approaches with\nthe likes of label smoothing, mixup etc. lower the utility of a DNN by\ndegrading its refinement. Even under natural data shift, this\ncalibration-refinement trade-off holds for the majority of calibration methods.\nThese findings call for an urgent retrospective into some popular pathways\ntaken for modern DNN calibration.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:04:14 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Singh", "Aditya", ""], ["Bay", "Alessandro", ""], ["Sengupta", "Biswa", ""], ["Mirabile", "Andrea", ""]]}, {"id": "2106.09388", "submitter": "Yongchun Zhu", "authors": "Yongchun Zhu, Fuzhen Zhuang, Jindong Wang, Guolin Ke, Jingwu Chen,\n  Jiang Bian, Hui Xiong and Qing He", "title": "Deep Subdomain Adaptation Network for Image Classification", "comments": "published on TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2020.2988928", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a target task where labeled data is unavailable, domain adaptation can\ntransfer a learner from a different source domain. Previous deep domain\nadaptation methods mainly learn a global domain shift, i.e., align the global\nsource and target distributions without considering the relationships between\ntwo subdomains within the same category of different domains, leading to\nunsatisfying transfer learning performance without capturing the fine-grained\ninformation. Recently, more and more researchers pay attention to Subdomain\nAdaptation which focuses on accurately aligning the distributions of the\nrelevant subdomains. However, most of them are adversarial methods which\ncontain several loss functions and converge slowly. Based on this, we present\nDeep Subdomain Adaptation Network (DSAN) which learns a transfer network by\naligning the relevant subdomain distributions of domain-specific layer\nactivations across different domains based on a local maximum mean discrepancy\n(LMMD). Our DSAN is very simple but effective which does not need adversarial\ntraining and converges fast. The adaptation can be achieved easily with most\nfeed-forward network models by extending them with LMMD loss, which can be\ntrained efficiently via back-propagation. Experiments demonstrate that DSAN can\nachieve remarkable results on both object recognition tasks and digit\nclassification tasks. Our code will be available at:\nhttps://github.com/easezyc/deep-transfer-learning\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:07:21 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhu", "Yongchun", ""], ["Zhuang", "Fuzhen", ""], ["Wang", "Jindong", ""], ["Ke", "Guolin", ""], ["Chen", "Jingwu", ""], ["Bian", "Jiang", ""], ["Xiong", "Hui", ""], ["He", "Qing", ""]]}, {"id": "2106.09393", "submitter": "Yi Zhou", "authors": "Yi Zhou, Heikki Huttunen, Tapio Elomaa", "title": "using multiple losses for accurate facial age estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Age estimation is an essential challenge in computer vision. With the\nadvances of convolutional neural networks, the performance of age estimation\nhas been dramatically improved. Existing approaches usually treat age\nestimation as a classification problem. However, the age labels are ambiguous,\nthus make the classification task difficult. In this paper, we propose a simple\nyet effective approach for age estimation, which improves the performance\ncompared to classification-based methods. The method combines four\nclassification losses and one regression loss representing different class\ngranularities together, and we name it as Age-Granularity-Net. We validate the\nAge-Granularity-Net framework on the CVPR Chalearn 2016 dataset, and extensive\nexperiments show that the proposed approach can reduce the prediction error\ncompared to any individual loss. The source code link is\nhttps://github.com/yipersevere/age-estimation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:18:16 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhou", "Yi", ""], ["Huttunen", "Heikki", ""], ["Elomaa", "Tapio", ""]]}, {"id": "2106.09398", "submitter": "Fangbing Liu", "authors": "Fangbing Liu and Qing Wang", "title": "Episode Adaptive Embedding Networks for Few-shot Learning", "comments": null, "journal-ref": "PAKDD 2021", "doi": "10.1007/978-3-030-75768-7_1", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Few-shot learning aims to learn a classifier using a few labelled instances\nfor each class. Metric-learning approaches for few-shot learning embed\ninstances into a high-dimensional space and conduct classification based on\ndistances among instance embeddings. However, such instance embeddings are\nusually shared across all episodes and thus lack the discriminative power to\ngeneralize classifiers according to episode-specific features. In this paper,\nwe propose a novel approach, namely \\emph{Episode Adaptive Embedding Network}\n(EAEN), to learn episode-specific embeddings of instances. By leveraging the\nprobability distributions of all instances in an episode at each channel-pixel\nembedding dimension, EAEN can not only alleviate the overfitting issue\nencountered in few-shot learning tasks, but also capture discriminative\nfeatures specific to an episode. To empirically verify the effectiveness and\nrobustness of EAEN, we have conducted extensive experiments on three widely\nused benchmark datasets, under various combinations of different generic\nembedding backbones and different classifiers. The results show that EAEN\nsignificantly improves classification accuracy about $10\\%$ to $20\\%$ in\ndifferent settings over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:29:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liu", "Fangbing", ""], ["Wang", "Qing", ""]]}, {"id": "2106.09402", "submitter": "Konda Reddy Mopuri", "authors": "Harsh Rangwani, Konda Reddy Mopuri, and R. Venkatesh Babu", "title": "Class Balancing GAN with a Classifier in the Loop", "comments": "UAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have swiftly evolved to imitate\nincreasingly complex image distributions. However, majority of the developments\nfocus on performance of GANs on balanced datasets. We find that the existing\nGANs and their training regimes which work well on balanced datasets fail to be\neffective in case of imbalanced (i.e. long-tailed) datasets. In this work we\nintroduce a novel theoretically motivated Class Balancing regularizer for\ntraining GANs. Our regularizer makes use of the knowledge from a pre-trained\nclassifier to ensure balanced learning of all the classes in the dataset. This\nis achieved via modelling the effective class frequency based on the\nexponential forgetting observed in neural networks and encouraging the GAN to\nfocus on underrepresented classes. We demonstrate the utility of our\nregularizer in learning representations for long-tailed distributions via\nachieving better performance than existing approaches over multiple datasets.\nSpecifically, when applied to an unconditional GAN, it improves the FID from\n$13.03$ to $9.01$ on the long-tailed iNaturalist-$2019$ dataset.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:41:30 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Rangwani", "Harsh", ""], ["Mopuri", "Konda Reddy", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2106.09431", "submitter": "Marvin Eisenberger", "authors": "Marvin Eisenberger, David Novotny, Gael Kerchenbaum, Patrick Labatut,\n  Natalia Neverova, Daniel Cremers, Andrea Vedaldi", "title": "NeuroMorph: Unsupervised Shape Interpolation and Correspondence in One\n  Go", "comments": "Published at the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NeuroMorph, a new neural network architecture that takes as input\ntwo 3D shapes and produces in one go, i.e. in a single feed forward pass, a\nsmooth interpolation and point-to-point correspondences between them. The\ninterpolation, expressed as a deformation field, changes the pose of the source\nshape to resemble the target, but leaves the object identity unchanged.\nNeuroMorph uses an elegant architecture combining graph convolutions with\nglobal feature pooling to extract local features. During training, the model is\nincentivized to create realistic deformations by approximating geodesics on the\nunderlying shape space manifold. This strong geometric prior allows to train\nour model end-to-end and in a fully unsupervised manner without requiring any\nmanual correspondence annotations. NeuroMorph works well for a large variety of\ninput shapes, including non-isometric pairs from different object categories.\nIt obtains state-of-the-art results for both shape correspondence and\ninterpolation tasks, matching or surpassing the performance of recent\nunsupervised and supervised methods on multiple benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 12:25:44 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Eisenberger", "Marvin", ""], ["Novotny", "David", ""], ["Kerchenbaum", "Gael", ""], ["Labatut", "Patrick", ""], ["Neverova", "Natalia", ""], ["Cremers", "Daniel", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2106.09432", "submitter": "Matthias Springstein", "authors": "Matthias Springstein and Eric M\\\"uller-Budack and Ralph Ewerth", "title": "Unsupervised Training Data Generation of Handwritten Formulas using\n  Generative Adversarial Networks with Self-Attention", "comments": "Accepted for publication in: ACM International Conference on\n  Multimedia Retrieval (ICMR) Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of handwritten mathematical expressions in images and video\nframes is a difficult and unsolved problem yet. Deep convectional neural\nnetworks are basically a promising approach, but typically require a large\namount of labeled training data. However, such a large training dataset does\nnot exist for the task of handwritten formula recognition. In this paper, we\nintroduce a system that creates a large set of synthesized training examples of\nmathematical expressions which are derived from LaTeX documents. For this\npurpose, we propose a novel attention-based generative adversarial network to\ntranslate rendered equations to handwritten formulas. The datasets generated by\nthis approach contain hundreds of thousands of formulas, making it ideal for\npretraining or the design of more complex models. We evaluate our synthesized\ndataset and the recognition approach on the CROHME 2014 benchmark dataset.\nExperimental results demonstrate the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 12:27:18 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Springstein", "Matthias", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2106.09436", "submitter": "Yuanen Zhou", "authors": "Yuanen Zhou, Yong Zhang, Zhenzhen Hu, Meng Wang", "title": "Semi-Autoregressive Transformer for Image Captioning", "comments": "arXiv admin note: text overlap with arXiv:2005.04690 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art image captioning models adopt autoregressive\ndecoders, \\ie they generate each word by conditioning on previously generated\nwords, which leads to heavy latency during inference. To tackle this issue,\nnon-autoregressive image captioning models have recently been proposed to\nsignificantly accelerate the speed of inference by generating all words in\nparallel. However, these non-autoregressive models inevitably suffer from large\ngeneration quality degradation since they remove words dependence excessively.\nTo make a better trade-off between speed and quality, we introduce a\nsemi-autoregressive model for image captioning~(dubbed as SATIC), which keeps\nthe autoregressive property in global but generates words parallelly in local.\nBased on Transformer, there are only a few modifications needed to implement\nSATIC. Extensive experiments on the MSCOCO image captioning benchmark show that\nSATIC can achieve a better trade-off without bells and whistles. Code is\navailable at {\\color{magenta}\\url{https://github.com/YuanEZhou/satic}}.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 12:36:33 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Zhou", "Yuanen", ""], ["Zhang", "Yong", ""], ["Hu", "Zhenzhen", ""], ["Wang", "Meng", ""]]}, {"id": "2106.09453", "submitter": "Sanghyun Woo", "authors": "Sanghyun Woo, Dahun Kim, Joon-Young Lee, In So Kweon", "title": "Learning to Associate Every Segment for Video Panoptic Segmentation", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal correspondence - linking pixels or objects across frames - is a\nfundamental supervisory signal for the video models. For the panoptic\nunderstanding of dynamic scenes, we further extend this concept to every\nsegment. Specifically, we aim to learn coarse segment-level matching and fine\npixel-level matching together. We implement this idea by designing two novel\nlearning objectives. To validate our proposals, we adopt a deep siamese model\nand train the model to learn the temporal correspondence on two different\nlevels (i.e., segment and pixel) along with the target task. At inference time,\nthe model processes each frame independently without any extra computation and\npost-processing. We show that our per-frame inference model can achieve new\nstate-of-the-art results on Cityscapes-VPS and VIPER datasets. Moreover, due to\nits high efficiency, the model runs in a fraction of time (3x) compared to the\nprevious state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:06:24 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Woo", "Sanghyun", ""], ["Kim", "Dahun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "2106.09486", "submitter": "Demetris Marnerides", "authors": "Demetris Marnerides, Thomas Bashford-Rogers, Kurt Debattista", "title": "Deep HDR Hallucination for Inverse Tone Mapping", "comments": null, "journal-ref": "Sensors 2021, 21, 4032", "doi": "10.3390/s21124032", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Inverse Tone Mapping (ITM) methods attempt to reconstruct High Dynamic Range\n(HDR) information from Low Dynamic Range (LDR) image content. The dynamic range\nof well-exposed areas must be expanded and any missing information due to\nover/under-exposure must be recovered (hallucinated). The majority of methods\nfocus on the former and are relatively successful, while most attempts on the\nlatter are not of sufficient quality, even ones based on Convolutional Neural\nNetworks (CNNs). A major factor for the reduced inpainting quality in some\nworks is the choice of loss function. Work based on Generative Adversarial\nNetworks (GANs) shows promising results for image synthesis and LDR inpainting,\nsuggesting that GAN losses can improve inverse tone mapping results. This work\npresents a GAN-based method that hallucinates missing information from badly\nexposed areas in LDR images and compares its efficacy with alternative\nvariations. The proposed method is quantitatively competitive with\nstate-of-the-art inverse tone mapping methods, providing good dynamic range\nexpansion for well-exposed areas and plausible hallucinations for saturated and\nunder-exposed areas. A density-based normalisation method, targeted for HDR\ncontent, is also proposed, as well as an HDR data augmentation method targeted\nfor HDR hallucination.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:35:40 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Marnerides", "Demetris", ""], ["Bashford-Rogers", "Thomas", ""], ["Debattista", "Kurt", ""]]}, {"id": "2106.09516", "submitter": "Imtiaz Ziko", "authors": "Imtiaz Masud Ziko, Malik Boudiaf, Jose Dolz, Eric Granger and Ismail\n  Ben Ayed", "title": "Transductive Few-Shot Learning: Clustering is All You Need?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate a general formulation for clustering and transductive few-shot\nlearning, which integrates prototype-based objectives, Laplacian regularization\nand supervision constraints from a few labeled data points. We propose a\nconcave-convex relaxation of the problem, and derive a computationally\nefficient block-coordinate bound optimizer, with convergence guarantee. At each\niteration,our optimizer computes independent (parallel) updates for each\npoint-to-cluster assignment. Therefore, it could be trivially distributed for\nlarge-scale clustering and few-shot tasks. Furthermore, we provides a thorough\nconvergence analysis based on point-to-set maps. Were port comprehensive\nclustering and few-shot learning experiments over various data sets, showing\nthat our method yields competitive performances, in term of accuracy and\noptimization quality, while scaling up to large problems. Using standard\ntraining on the base classes, without resorting to complex meta-learning and\nepisodic-training strategies, our approach outperforms state-of-the-art\nfew-shot methods by significant margins, across various models, settings and\ndata sets. Surprisingly, we found that even standard clustering procedures\n(e.g., K-means), which correspond to particular, non-regularized cases of our\ngeneral model, already achieve competitive performances in comparison to the\nstate-of-the-art in few-shot learning. These surprising results point to the\nlimitations of the current few-shot benchmarks, and question the viability of a\nlarge body of convoluted few-shot learning techniques in the recent literature.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:14:01 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ziko", "Imtiaz Masud", ""], ["Boudiaf", "Malik", ""], ["Dolz", "Jose", ""], ["Granger", "Eric", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2106.09517", "submitter": "Guangyu Ren", "authors": "Guangyu Ren, Tania Stathaki", "title": "Dynamic Knowledge Distillation with A Single Stream Structure for RGB-D\n  Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-D salient object detection(SOD) demonstrates its superiority on detecting\nin complex environments due to the additional depth information introduced in\nthe data. Inevitably, an independent stream is introduced to extract features\nfrom depth images, leading to extra computation and parameters. This\nmethodology which sacrifices the model size to improve the detection accuracy\nmay impede the practical application of SOD problems. To tackle this dilemma,\nwe propose a dynamic distillation method along with a lightweight framework,\nwhich significantly reduces the parameters. This method considers the factors\nof both teacher and student performance within the training stage and\ndynamically assigns the distillation weight instead of applying a fixed weight\non the student model. Extensive experiments are conducted on five public\ndatasets to demonstrate that our method can achieve competitive performance\ncompared to 10 prior methods through a 78.2MB lightweight structure.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 14:07:25 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 21:18:33 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ren", "Guangyu", ""], ["Stathaki", "Tania", ""]]}, {"id": "2106.09534", "submitter": "Kaihua Tang", "authors": "Kaihua Tang, Mingyuan Tao, Hanwang Zhang", "title": "Adversarial Visual Robustness by Causal Intervention", "comments": "Codes are available at\n  https://github.com/KaihuaTang/Adversarial-Robustness-by-Causal-Intervention.pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is the de facto most promising defense against\nadversarial examples. Yet, its passive nature inevitably prevents it from being\nimmune to unknown attackers. To achieve a proactive defense, we need a more\nfundamental understanding of adversarial examples, beyond the popular bounded\nthreat model. In this paper, we provide a causal viewpoint of adversarial\nvulnerability: the cause is the confounder ubiquitously existing in learning,\nwhere attackers are precisely exploiting the confounding effect. Therefore, a\nfundamental solution for adversarial robustness is causal intervention. As the\nconfounder is unobserved in general, we propose to use the instrumental\nvariable that achieves intervention without the need for confounder\nobservation. We term our robust training method as Causal intervention by\ninstrumental Variable (CiiV). It has a differentiable retinotopic sampling\nlayer and a consistency loss, which is stable and guaranteed not to suffer from\ngradient obfuscation. Extensive experiments on a wide spectrum of attackers and\nsettings applied in MNIST, CIFAR-10, and mini-ImageNet datasets empirically\ndemonstrate that CiiV is robust to adaptive attacks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 14:23:54 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Tang", "Kaihua", ""], ["Tao", "Mingyuan", ""], ["Zhang", "Hanwang", ""]]}, {"id": "2106.09548", "submitter": "Wenpeng Xing", "authors": "Wenpeng Xing, Jie Chen, Zaifeng Yang and Qiang Wang", "title": "Scale-Consistent Fusion: from Heterogeneous Local Sampling to Global\n  Immersive Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based geometric modeling and novel view synthesis based on sparse,\nlarge-baseline samplings are challenging but important tasks for emerging\nmultimedia applications such as virtual reality and immersive telepresence.\nExisting methods fail to produce satisfactory results due to the limitation on\ninferring reliable depth information over such challenging reference\nconditions. With the popularization of commercial light field (LF) cameras,\ncapturing LF images (LFIs) is as convenient as taking regular photos, and\ngeometry information can be reliably inferred. This inspires us to use a sparse\nset of LF captures to render high-quality novel views globally. However, fusion\nof LF captures from multiple angles is challenging due to the scale\ninconsistency caused by various capture settings. To overcome this challenge,\nwe propose a novel scale-consistent volume rescaling algorithm that robustly\naligns the disparity probability volumes (DPV) among different captures for\nscale-consistent global geometry fusion. Based on the fused DPV projected to\nthe target camera frustum, novel learning-based modules have been proposed\n(i.e., the attention-guided multi-scale residual fusion module, and the\ndisparity field guided deep re-regularization module) which comprehensively\nregularize noisy observations from heterogeneous captures for high-quality\nrendering of novel LFIs. Both quantitative and qualitative experiments over the\nStanford Lytro Multi-view LF dataset show that the proposed method outperforms\nstate-of-the-art methods significantly under different experiment settings for\ndisparity inference and LF synthesis.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 14:27:08 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xing", "Wenpeng", ""], ["Chen", "Jie", ""], ["Yang", "Zaifeng", ""], ["Wang", "Qiang", ""]]}, {"id": "2106.09563", "submitter": "Lucas Caccia", "authors": "Lucas Caccia, Jing Xu, Myle Ott, Marc'Aurelio Ranzato, Ludovic Denoyer", "title": "On Anytime Learning at Macroscale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical machine learning frameworks assume access to a possibly large\ndataset in order to train a predictive model. In many practical applications\nhowever, data does not arrive all at once, but in batches over time. This\ncreates a natural trade-off between accuracy of a model and time to obtain such\na model. A greedy predictor could produce non-trivial predictions by\nimmediately training on batches as soon as these become available but, it may\nalso make sub-optimal use of future data. On the other hand, a tardy predictor\ncould wait for a long time to aggregate several batches into a larger dataset,\nbut ultimately deliver a much better performance. In this work, we consider\nsuch a streaming learning setting, which we dub {\\em anytime learning at\nmacroscale} (ALMA). It is an instance of anytime learning applied not at the\nlevel of a single chunk of data, but at the level of the entire sequence of\nlarge batches. We first formalize this learning setting, we then introduce\nmetrics to assess how well learners perform on the given task for a given\nmemory and compute budget, and finally we test several baseline approaches on\nstandard benchmarks repurposed for anytime learning at macroscale. The general\nfinding is that bigger models always generalize better. In particular, it is\nimportant to grow model capacity over time if the initial model is relatively\nsmall. Moreover, updating the model at an intermediate rate strikes the best\ntrade off between accuracy and time to obtain a useful predictor.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 14:45:22 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Caccia", "Lucas", ""], ["Xu", "Jing", ""], ["Ott", "Myle", ""], ["Ranzato", "Marc'Aurelio", ""], ["Denoyer", "Ludovic", ""]]}, {"id": "2106.09564", "submitter": "Pietro Gori", "authors": "Minhao Hu, Matthis Maillard, Ya Zhang, Tommaso Ciceri, Giammarco La\n  Barbera, Isabelle Bloch, Pietro Gori", "title": "Knowledge distillation from multi-modal to mono-modal segmentation\n  networks", "comments": "MICCAI 2020", "journal-ref": "MICCAI 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint use of multiple imaging modalities for medical image segmentation\nhas been widely studied in recent years. The fusion of information from\ndifferent modalities has demonstrated to improve the segmentation accuracy,\nwith respect to mono-modal segmentations, in several applications. However,\nacquiring multiple modalities is usually not possible in a clinical setting due\nto a limited number of physicians and scanners, and to limit costs and scan\ntime. Most of the time, only one modality is acquired. In this paper, we\npropose KD-Net, a framework to transfer knowledge from a trained multi-modal\nnetwork (teacher) to a mono-modal one (student). The proposed method is an\nadaptation of the generalized distillation framework where the student network\nis trained on a subset (1 modality) of the teacher's inputs (n modalities). We\nillustrate the effectiveness of the proposed framework in brain tumor\nsegmentation with the BraTS 2018 dataset. Using different architectures, we\nshow that the student network effectively learns from the teacher and always\noutperforms the baseline mono-modal network in terms of segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 14:46:57 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Hu", "Minhao", ""], ["Maillard", "Matthis", ""], ["Zhang", "Ya", ""], ["Ciceri", "Tommaso", ""], ["La Barbera", "Giammarco", ""], ["Bloch", "Isabelle", ""], ["Gori", "Pietro", ""]]}, {"id": "2106.09584", "submitter": "Fabio Bellavia", "authors": "Fabio Bellavia", "title": "SIFT Matching by Context Exposed", "comments": "Early pre-release", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates how to step up local image descriptor matching by\nexploiting matching context information. Two main contexts are identified,\noriginated respectively from the descriptor space and from the keypoint space.\nThe former is generally used to design the actual matching strategy while the\nlatter to filter matches according to the local spatial consistency. On this\nbasis, a new matching strategy and a novel local spatial filter, named\nrespectively blob matching and Delaunay Triangulation Matching (DTM) are\ndevised. Blob matching provides a general matching framework by merging\ntogether several strategies, including pre-filtering as well as many-to-many\nand symmetric matching, enabling to achieve a global improvement upon each\nindividual strategy. DTM alternates between Delaunay triangulation contractions\nand expansions to figure out and adjust keypoint neighborhood consistency.\nExperimental evaluation shows that DTM is comparable or better than the\nstate-of-the-art in terms of matching accuracy and robustness, especially for\nnon-planar scenes. Evaluation is carried out according to a new benchmark\ndevised for analyzing the matching pipeline in terms of correct correspondences\non both planar and non-planar scenes, including state-of-the-art methods as\nwell as the common SIFT matching approach for reference. This evaluation can be\nof assistance for future research in this field.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:10:59 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 08:34:15 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Bellavia", "Fabio", ""]]}, {"id": "2106.09614", "submitter": "Chunlu Li", "authors": "Chunlu Li, Andreas Morel-Forster, Thomas Vetter, Bernhard Egger, Adam\n  Kortylewski", "title": "To fit or not to fit: Model-based Face Reconstruction and Occlusion\n  Segmentation from Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face reconstruction from a single image is challenging due to its\nill-posed nature. Model-based face autoencoders address this issue effectively\nby fitting a face model to the target image in a weakly supervised manner.\nHowever, in unconstrained environments occlusions distort the face\nreconstruction because the model often erroneously tries to adapt to occluded\nface regions. Supervised occlusion segmentation is a viable solution to avoid\nthe fitting of occluded face regions, but it requires a large amount of\nannotated training data. In this work, we enable model-based face autoencoders\nto segment occluders accurately without requiring any additional supervision\nduring training, and this separates regions where the model will be fitted from\nthose where it will not be fitted. To achieve this, we extend face autoencoders\nwith a segmentation network. The segmentation network decides which regions the\nmodel should adapt to by reaching balances in a trade-off between including\npixels and adapting the model to them, and excluding pixels so that the model\nfitting is not negatively affected and reaches higher overall reconstruction\naccuracy on pixels showing the face. This leads to a synergistic effect, in\nwhich the occlusion segmentation guides the training of the face autoencoder to\nconstrain the fitting in the non-occluded regions, while the improved fitting\nenables the segmentation model to better predict the occluded face regions.\nQualitative and quantitative experiments on the CelebA-HQ database and the AR\ndatabase verify the effectiveness of our model in improving 3D face\nreconstruction under occlusions and in enabling accurate occlusion segmentation\nfrom weak supervision only. Code available at\nhttps://github.com/unibas-gravis/Occlusion-Robust-MoFA.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:52:19 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Li", "Chunlu", ""], ["Morel-Forster", "Andreas", ""], ["Vetter", "Thomas", ""], ["Egger", "Bernhard", ""], ["Kortylewski", "Adam", ""]]}, {"id": "2106.09621", "submitter": "Salman Seyedi", "authors": "Salman Seyedi, Zifan Jiang, Allan Levey, Gari D. Clifford", "title": "Privacy-Preserving Eye-tracking Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The expanding usage of complex machine learning methods like deep learning\nhas led to an explosion in human activity recognition, particularly applied to\nhealth. In particular, as part of a larger body sensor network system, face and\nfull-body analysis is becoming increasingly common for evaluating health\nstatus. However, complex models which handle private and sometimes protected\ndata, raise concerns about the potential leak of identifiable data. In this\nwork, we focus on the case of a deep network model trained on images of\nindividual faces. Full-face video recordings taken from 493 individuals\nundergoing an eye-tracking based evaluation of neurological function were used.\nOutputs, gradients, intermediate layer outputs, loss, and labels were used as\ninputs for a deep network with an added support vector machine emission layer\nto recognize membership in the training data. The inference attack method and\nassociated mathematical analysis indicate that there is a low likelihood of\nunintended memorization of facial features in the deep learning model. In this\nstudy, it is showed that the named model preserves the integrity of training\ndata with reasonable confidence. The same process can be implemented in similar\nconditions for different models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:58:01 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 20:24:53 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Seyedi", "Salman", ""], ["Jiang", "Zifan", ""], ["Levey", "Allan", ""], ["Clifford", "Gari D.", ""]]}, {"id": "2106.09637", "submitter": "Tiago Barros", "authors": "Tiago Barros, Lu\\'is Garrote, Ricardo Pereira, Cristiano Premebida,\n  Urbano J. Nunes", "title": "AttDLNet: Attention-based DL Network for 3D LiDAR Place Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep networks have been progressively adapted to new sensor modalities,\nnamely to 3D LiDAR, which led to unprecedented achievements in autonomous\nvehicle-related applications such as place recognition. One of the main\nchallenges of deep models in place recognition is to extract efficient and\ndescriptive feature representations that relate places based on their\nsimilarity. To address the problem of place recognition using LiDAR data, this\npaper proposes a novel 3D LiDAR-based deep learning network (named AttDLNet)\nthat comprises an encoder network and exploits an attention mechanism to\nselectively focus on long-range context and interfeature relationships. The\nproposed network is trained and validated on the KITTI dataset, using the\ncosine loss for training and a retrieval-based place recognition pipeline for\nvalidation. Additionally, an ablation study is presented to assess the best\nnetwork configuration. Results show that the encoder network features are\nalready very descriptive, but adding attention to the network further improves\nperformance. From the ablation study, results indicate that the middle encoder\nlayers have the highest mean performance, while deeper layers are more robust\nto orientation change. The code is publicly available on the project website:\nhttps://github.com/Cybonic/ AttDLNet\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 16:34:37 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Barros", "Tiago", ""], ["Garrote", "Lu\u00eds", ""], ["Pereira", "Ricardo", ""], ["Premebida", "Cristiano", ""], ["Nunes", "Urbano J.", ""]]}, {"id": "2106.09662", "submitter": "Davood Karimi", "authors": "Golnoosh Samei, Davood Karimi, Claudia Kesch, Septimiu Salcudean", "title": "Automatic Segmentation of the Prostate on 3D Trans-rectal Ultrasound\n  Images using Statistical Shape Models and Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we propose to segment the prostate on a challenging dataset of\ntrans-rectal ultrasound (TRUS) images using convolutional neural networks\n(CNNs) and statistical shape models (SSMs). TRUS is commonly used for a number\nof image-guided interventions on the prostate. Fast and accurate segmentation\non the organ in these images is crucial to planning and fusion with other\nmodalities such as magnetic resonance images (MRIs) . However, TRUS has limited\nsoft tissue contrast and signal to noise ratio which makes the task of\nsegmenting the prostate challenging and subject to inter-observer and\nintra-observer variability. This is especially problematic at the base and apex\nwhere the gland boundary is hard to define. In this paper, we aim to tackle\nthis problem by taking advantage of shape priors learnt on an MR dataset which\nhas higher soft tissue contrast allowing the prostate to be contoured more\naccurately. We use this shape prior in combination with a prostate tissue\nprobability map computed by a CNN for segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:11:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Samei", "Golnoosh", ""], ["Karimi", "Davood", ""], ["Kesch", "Claudia", ""], ["Salcudean", "Septimiu", ""]]}, {"id": "2106.09669", "submitter": "Efthymios Tzinis", "authors": "Efthymios Tzinis, Scott Wisdom, Tal Remez, John R. Hershey", "title": "Improving On-Screen Sound Separation for Open Domain Videos with\n  Audio-Visual Self-attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a state-of-the-art audio-visual on-screen sound separation\nsystem which is capable of learning to separate sounds and associate them with\non-screen objects by looking at in-the-wild videos. We identify limitations of\nprevious work on audiovisual on-screen sound separation, including the\nsimplicity and coarse resolution of spatio-temporal attention, and poor\nconvergence of the audio separation model. Our proposed model addresses these\nissues using cross-modal and self-attention modules that capture audio-visual\ndependencies at a finer resolution over time, and by unsupervised pre-training\nof audio separation model. These improvements allow the model to generalize to\na much wider set of unseen videos. For evaluation and semi-supervised training,\nwe collected human annotations of on-screen audio from a large database of\nin-the-wild videos (YFCC100M). Our results show marked improvements in\non-screen separation performance, in more general conditions than previous\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:23:44 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Tzinis", "Efthymios", ""], ["Wisdom", "Scott", ""], ["Remez", "Tal", ""], ["Hershey", "John R.", ""]]}, {"id": "2106.09670", "submitter": "Puspita Majumdar", "authors": "Shiksha Mishra, Puspita Majumdar, Richa Singh, Mayank Vatsa", "title": "Indian Masked Faces in the Wild Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the COVID-19 pandemic, wearing face masks has become a mandate in\npublic places worldwide. Face masks occlude a significant portion of the facial\nregion. Additionally, people wear different types of masks, from simple ones to\nones with graphics and prints. These pose new challenges to face recognition\nalgorithms. Researchers have recently proposed a few masked face datasets for\ndesigning algorithms to overcome the challenges of masked face recognition.\nHowever, existing datasets lack the cultural diversity and collection in the\nunrestricted settings. Country like India with attire diversity, people are not\nlimited to wearing traditional masks but also clothing like a thin cotton\nprinted towel (locally called as ``gamcha''), ``stoles'', and ``handkerchiefs''\nto cover their faces. In this paper, we present a novel \\textbf{Indian Masked\nFaces in the Wild (IMFW)} dataset which contains images with variations in\npose, illumination, resolution, and the variety of masks worn by the subjects.\nWe have also benchmarked the performance of existing face recognition models on\nthe proposed IMFW dataset. Experimental results demonstrate the limitations of\nexisting algorithms in presence of diverse conditions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:23:54 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mishra", "Shiksha", ""], ["Majumdar", "Puspita", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "2106.09672", "submitter": "Matthijs Douze", "authors": "Matthijs Douze and Giorgos Tolias and Ed Pizzi and Zo\\\"e Papakipos and\n  Lowik Chanussot and Filip Radenovic and Tomas Jenicek and Maxim Maximov and\n  Laura Leal-Taix\\'e and Ismail Elezi and Ond\\v{r}ej Chum and Cristian Canton\n  Ferrer", "title": "The 2021 Image Similarity Dataset and Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new benchmark for large-scale image similarity\ndetection. This benchmark is used for the Image Similarity Challenge at\nNeurIPS'21 (ISC2021). The goal is to determine whether a query image is a\nmodified copy of any image in a reference corpus of size 1~million. The\nbenchmark features a variety of image transformations such as automated\ntransformations, hand-crafted image edits and machine-learning based\nmanipulations. This mimics real-life cases appearing in social media, for\nexample for integrity-related problems dealing with misinformation and\nobjectionable content. The strength of the image manipulations, and therefore\nthe difficulty of the benchmark, is calibrated according to the performance of\na set of baseline approaches. Both the query and reference set contain a\nmajority of \"distractor\" images that do not match, which corresponds to a\nreal-life needle-in-haystack setting, and the evaluation metric reflects that.\nWe expect the DISC21 benchmark to promote image copy detection as an important\nand challenging computer vision task and refresh the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:23:59 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 20:58:36 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Douze", "Matthijs", ""], ["Tolias", "Giorgos", ""], ["Pizzi", "Ed", ""], ["Papakipos", "Zo\u00eb", ""], ["Chanussot", "Lowik", ""], ["Radenovic", "Filip", ""], ["Jenicek", "Tomas", ""], ["Maximov", "Maxim", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Elezi", "Ismail", ""], ["Chum", "Ond\u0159ej", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "2106.09678", "submitter": "Linxi Fan", "authors": "Linxi Fan, Guanzhi Wang, De-An Huang, Zhiding Yu, Li Fei-Fei, Yuke\n  Zhu, Anima Anandkumar", "title": "SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual\n  Policies", "comments": "ICML 2021. Website: https://linxifan.github.io/secant-site/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization has been a long-standing challenge for reinforcement learning\n(RL). Visual RL, in particular, can be easily distracted by irrelevant factors\nin high-dimensional observation space. In this work, we consider robust policy\nlearning which targets zero-shot generalization to unseen visual environments\nwith large distributional shift. We propose SECANT, a novel self-expert cloning\ntechnique that leverages image augmentation in two stages to decouple robust\nrepresentation learning from policy optimization. Specifically, an expert\npolicy is first trained by RL from scratch with weak augmentations. A student\nnetwork then learns to mimic the expert policy by supervised learning with\nstrong augmentations, making its representation more robust against visual\nvariations compared to the expert. Extensive experiments demonstrate that\nSECANT significantly advances the state of the art in zero-shot generalization\nacross 4 challenging domains. Our average reward improvements over prior SOTAs\nare: DeepMind Control (+26.5%), robotic manipulation (+337.8%), vision-based\nautonomous driving (+47.7%), and indoor object navigation (+15.8%). Code\nrelease and video are available at https://linxifan.github.io/secant-site/.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:28:18 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Fan", "Linxi", ""], ["Wang", "Guanzhi", ""], ["Huang", "De-An", ""], ["Yu", "Zhiding", ""], ["Fei-Fei", "Li", ""], ["Zhu", "Yuke", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2106.09679", "submitter": "Ron Mokady", "authors": "Ron Mokady, Rotem Tzaban, Sagie Benaim, Amit H. Bermano and Daniel\n  Cohen-Or", "title": "JOKR: Joint Keypoint Representation for Unsupervised Cross-Domain Motion\n  Retargeting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The task of unsupervised motion retargeting in videos has seen substantial\nadvancements through the use of deep neural networks. While early works\nconcentrated on specific object priors such as a human face or body, recent\nwork considered the unsupervised case. When the source and target videos,\nhowever, are of different shapes, current methods fail. To alleviate this\nproblem, we introduce JOKR - a JOint Keypoint Representation that captures the\nmotion common to both the source and target videos, without requiring any\nobject prior or data collection. By employing a domain confusion term, we\nenforce the unsupervised keypoint representations of both videos to be\nindistinguishable. This encourages disentanglement between the parts of the\nmotion that are common to the two domains, and their distinctive appearance and\nmotion, enabling the generation of videos that capture the motion of the one\nwhile depicting the style of the other. To enable cases where the objects are\nof different proportions or orientations, we apply a learned affine\ntransformation between the JOKRs. This augments the representation to be affine\ninvariant, and in practice broadens the variety of possible retargeting pairs.\nThis geometry-driven representation enables further intuitive control, such as\ntemporal coherence and manual editing. Through comprehensive experimentation,\nwe demonstrate the applicability of our method to different challenging\ncross-domain video pairs. We evaluate our method both qualitatively and\nquantitatively, and demonstrate that our method handles various cross-domain\nscenarios, such as different animals, different flowers, and humans. We also\ndemonstrate superior temporal coherency and visual quality compared to\nstate-of-the-art alternatives, through statistical metrics and a user study.\nSource code and videos can be found at https://rmokady.github.io/JOKR/ .\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:32:32 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mokady", "Ron", ""], ["Tzaban", "Rotem", ""], ["Benaim", "Sagie", ""], ["Bermano", "Amit H.", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2106.09681", "submitter": "Alaaeldin El-Nouby", "authors": "Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski,\n  Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel\n  Synnaeve, Jakob Verbeek, Herv\\'e Jegou", "title": "XCiT: Cross-Covariance Image Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Following their success in natural language processing, transformers have\nrecently shown much promise for computer vision. The self-attention operation\nunderlying transformers yields global interactions between all tokens ,i.e.\nwords or image patches, and enables flexible modelling of image data beyond the\nlocal interactions of convolutions. This flexibility, however, comes with a\nquadratic complexity in time and memory, hindering application to long\nsequences and high-resolution images. We propose a \"transposed\" version of\nself-attention that operates across feature channels rather than tokens, where\nthe interactions are based on the cross-covariance matrix between keys and\nqueries. The resulting cross-covariance attention (XCA) has linear complexity\nin the number of tokens, and allows efficient processing of high-resolution\nimages. Our cross-covariance image transformer (XCiT) is built upon XCA. It\ncombines the accuracy of conventional transformers with the scalability of\nconvolutional architectures. We validate the effectiveness and generality of\nXCiT by reporting excellent results on multiple vision benchmarks, including\nimage classification and self-supervised feature learning on ImageNet-1k,\nobject detection and instance segmentation on COCO, and semantic segmentation\non ADE20k.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:33:35 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 15:33:31 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["El-Nouby", "Alaaeldin", ""], ["Touvron", "Hugo", ""], ["Caron", "Mathilde", ""], ["Bojanowski", "Piotr", ""], ["Douze", "Matthijs", ""], ["Joulin", "Armand", ""], ["Laptev", "Ivan", ""], ["Neverova", "Natalia", ""], ["Synnaeve", "Gabriel", ""], ["Verbeek", "Jakob", ""], ["Jegou", "Herv\u00e9", ""]]}, {"id": "2106.09693", "submitter": "Koushik Biswas", "authors": "Koushik Biswas, Shilpak Banerjee, Ashish Kumar Pandey", "title": "Orthogonal-Pad\\'e Activation Functions: Trainable Activation functions\n  for smooth and faster convergence in deep networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have proposed orthogonal-Pad\\'e activation functions, which are trainable\nactivation functions and show that they have faster learning capability and\nimproves the accuracy in standard deep learning datasets and models. Based on\nour experiments, we have found two best candidates out of six orthogonal-Pad\\'e\nactivations, which we call safe Hermite-Pade (HP) activation functions, namely\nHP-1 and HP-2. When compared to ReLU, HP-1 and HP-2 has an increment in top-1\naccuracy by 5.06% and 4.63% respectively in PreActResNet-34, by 3.02% and 2.75%\nrespectively in MobileNet V2 model on CIFAR100 dataset while on CIFAR10 dataset\ntop-1 accuracy increases by 2.02% and 1.78% respectively in PreActResNet-34, by\n2.24% and 2.06% respectively in LeNet, by 2.15% and 2.03% respectively in\nEfficientnet B0.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:47:01 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Biswas", "Koushik", ""], ["Banerjee", "Shilpak", ""], ["Pandey", "Ashish Kumar", ""]]}, {"id": "2106.09696", "submitter": "Abhinanda Ranjit Punnakkal", "authors": "Abhinanda R. Punnakkal (1), Arjun Chandrasekaran (1), Nikos Athanasiou\n  (1), Alejandra Quiros-Ramirez (2), Michael J. Black (1) ((1) Max Planck\n  Institute for Intelligent Systems, (2) Universitat Konstanz)", "title": "BABEL: Bodies, Action and Behavior with English Labels", "comments": "11 pages, 4 figures, Accepted in CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the semantics of human movement -- the what, how and why of the\nmovement -- is an important problem that requires datasets of human actions\nwith semantic labels. Existing datasets take one of two approaches. Large-scale\nvideo datasets contain many action labels but do not contain ground-truth 3D\nhuman motion. Alternatively, motion-capture (mocap) datasets have precise body\nmotions but are limited to a small number of actions. To address this, we\npresent BABEL, a large dataset with language labels describing the actions\nbeing performed in mocap sequences. BABEL consists of action labels for about\n43 hours of mocap sequences from AMASS. Action labels are at two levels of\nabstraction -- sequence labels describe the overall action in the sequence, and\nframe labels describe all actions in every frame of the sequence. Each frame\nlabel is precisely aligned with the duration of the corresponding action in the\nmocap sequence, and multiple actions can overlap. There are over 28k sequence\nlabels, and 63k frame labels in BABEL, which belong to over 250 unique action\ncategories. Labels from BABEL can be leveraged for tasks like action\nrecognition, temporal action localization, motion synthesis, etc. To\ndemonstrate the value of BABEL as a benchmark, we evaluate the performance of\nmodels on 3D action recognition. We demonstrate that BABEL poses interesting\nlearning challenges that are applicable to real-world scenarios, and can serve\nas a useful benchmark of progress in 3D action recognition. The dataset,\nbaseline method, and evaluation code is made available, and supported for\nacademic research purposes at https://babel.is.tue.mpg.de/.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:51:14 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 21:03:06 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Punnakkal", "Abhinanda R.", ""], ["Chandrasekaran", "Arjun", ""], ["Athanasiou", "Nikos", ""], ["Quiros-Ramirez", "Alejandra", ""], ["Black", "Michael J.", ""]]}, {"id": "2106.09701", "submitter": "James Smith", "authors": "James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen, Hongxia Jin,\n  Zsolt Kira", "title": "Always Be Dreaming: A New Approach for Data-Free Class-Incremental\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer vision applications suffer from catastrophic forgetting when\nincrementally learning new concepts over time. The most successful approaches\nto alleviate this forgetting require extensive replay of previously seen data,\nwhich is problematic when memory constraints or data legality concerns exist.\nIn this work, we consider the high-impact problem of Data-Free\nClass-Incremental Learning (DFCIL), where an incremental learning agent must\nlearn new concepts over time without storing generators or training data from\npast tasks. One approach for DFCIL is to replay synthetic images produced by\ninverting a frozen copy of the learner's classification model, but we show this\napproach fails for common class-incremental benchmarks when using standard\ndistillation strategies. We diagnose the cause of this failure and propose a\nnovel incremental distillation strategy for DFCIL, contributing a modified\ncross-entropy training and importance-weighted feature distillation, and show\nthat our method results in up to a 25.1% increase in final task accuracy\n(absolute difference) compared to SOTA DFCIL methods for common\nclass-incremental benchmarks. Our method even outperforms several standard\nreplay based methods which store a coreset of images.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:56:08 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Smith", "James", ""], ["Hsu", "Yen-Chang", ""], ["Balloch", "Jonathan", ""], ["Shen", "Yilin", ""], ["Jin", "Hongxia", ""], ["Kira", "Zsolt", ""]]}, {"id": "2106.09703", "submitter": "Fanyi Xiao", "authors": "Fanyi Xiao and Joseph Tighe and Davide Modolo", "title": "MoDist: Motion Distillation for Self-supervised Video Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MoDist as a novel method to explicitly distill motion information\ninto self-supervised video representations. Compared to previous video\nrepresentation learning methods that mostly focus on learning motion cues\nimplicitly from RGB inputs, we show that the representation learned with our\nMoDist method focus more on foreground motion regions and thus generalizes\nbetter to downstream tasks. To achieve this, MoDist enriches standard\ncontrastive learning objectives for RGB video clips with a cross-modal learning\nobjective between a Motion pathway and a Visual pathway. We evaluate MoDist on\nseveral datasets for both action recognition (UCF101/HMDB51/SSv2) as well as\naction detection (AVA), and demonstrate state-of-the-art self-supervised\nperformance on all datasets. Furthermore, we show that MoDist representation\ncan be as effective as (in some cases even better than) representations learned\nwith full supervision. Given its simplicity, we hope MoDist could serve as a\nstrong baseline for future research in self-supervised video representation\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:57:11 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xiao", "Fanyi", ""], ["Tighe", "Joseph", ""], ["Modolo", "Davide", ""]]}, {"id": "2106.09707", "submitter": "Khoi Pham", "authors": "Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan\n  Tran, Abhinav Shrivastava", "title": "Learning to Predict Visual Attributes in the Wild", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attributes constitute a large portion of information contained in a\nscene. Objects can be described using a wide variety of attributes which\nportray their visual appearance (color, texture), geometry (shape, size,\nposture), and other intrinsic properties (state, action). Existing work is\nmostly limited to study of attribute prediction in specific domains. In this\npaper, we introduce a large-scale in-the-wild visual attribute prediction\ndataset consisting of over 927K attribute annotations for over 260K object\ninstances. Formally, object attribute prediction is a multi-label\nclassification problem where all attributes that apply to an object must be\npredicted. Our dataset poses significant challenges to existing methods due to\nlarge number of attributes, label sparsity, data imbalance, and object\nocclusion. To this end, we propose several techniques that systematically\ntackle these challenges, including a base model that utilizes both low- and\nhigh-level CNN features with multi-hop attention, reweighting and resampling\ntechniques, a novel negative label expansion scheme, and a novel supervised\nattribute-aware contrastive learning algorithm. Using these techniques, we\nachieve near 3.7 mAP and 5.7 overall F1 points improvement over the current\nstate of the art. Further details about the VAW dataset can be found at\nhttp://vawdataset.com/.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:58:02 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Pham", "Khoi", ""], ["Kafle", "Kushal", ""], ["Lin", "Zhe", ""], ["Ding", "Zhihong", ""], ["Cohen", "Scott", ""], ["Tran", "Quan", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2106.09708", "submitter": "Elijah Cole", "authors": "Elijah Cole, Oisin Mac Aodha, Titouan Lorieul, Pietro Perona, Dan\n  Morris, Nebojsa Jojic", "title": "Multi-Label Learning from Single Positive Labels", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting all applicable labels for a given image is known as multi-label\nclassification. Compared to the standard multi-class case (where each image has\nonly one label), it is considerably more challenging to annotate training data\nfor multi-label classification. When the number of potential labels is large,\nhuman annotators find it difficult to mention all applicable labels for each\ntraining image. Furthermore, in some settings detection is intrinsically\ndifficult e.g. finding small object instances in high resolution images. As a\nresult, multi-label training data is often plagued by false negatives. We\nconsider the hardest version of this problem, where annotators provide only one\nrelevant label for each image. As a result, training sets will have only one\npositive label per image and no confirmed negatives. We explore this special\ncase of learning from missing labels across four different multi-label image\nclassification datasets for both linear classifiers and end-to-end fine-tuned\ndeep networks. We extend existing multi-label losses to this setting and\npropose novel variants that constrain the number of expected positive labels\nduring training. Surprisingly, we show that in some cases it is possible to\napproach the performance of fully labeled classifiers despite training with\nsignificantly fewer confirmed labels.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:58:04 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Cole", "Elijah", ""], ["Mac Aodha", "Oisin", ""], ["Lorieul", "Titouan", ""], ["Perona", "Pietro", ""], ["Morris", "Dan", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "2106.09711", "submitter": "Hugo Germain", "authors": "Hugo Germain and Vincent Lepetit and Guillaume Bourmaud", "title": "Visual Correspondence Hallucination: Towards Geometric Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pair of partially overlapping source and target images and a keypoint\nin the source image, the keypoint's correspondent in the target image can be\neither visible, occluded or outside the field of view. Local feature matching\nmethods are only able to identify the correspondent's location when it is\nvisible, while humans can also hallucinate its location when it is occluded or\noutside the field of view through geometric reasoning. In this paper, we bridge\nthis gap by training a network to output a peaked probability distribution over\nthe correspondent's location, regardless of this correspondent being visible,\noccluded, or outside the field of view. We experimentally demonstrate that this\nnetwork is indeed able to hallucinate correspondences on unseen pairs of\nimages. We also apply this network to a camera pose estimation problem and find\nit is significantly more robust than state-of-the-art local feature\nmatching-based competitors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:58:35 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Germain", "Hugo", ""], ["Lepetit", "Vincent", ""], ["Bourmaud", "Guillaume", ""]]}, {"id": "2106.09712", "submitter": "Christoph Emunds", "authors": "Christoph Emunds, Nicolas Pauen, Veronika Richter, J\\'er\\^ome Frisch,\n  Christoph van Treeck", "title": "IFCNet: A Benchmark Dataset for IFC Entity Classification", "comments": "To be presented at EG-ICE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancing interoperability and information exchange between domain-specific\nsoftware products for BIM is an important aspect in the Architecture,\nEngineering, Construction and Operations industry. Recent research started\ninvestigating methods from the areas of machine and deep learning for semantic\nenrichment of BIM models. However, training and evaluation of these machine\nlearning algorithms requires sufficiently large and comprehensive datasets.\nThis work presents IFCNet, a dataset of single-entity IFC files spanning a\nbroad range of IFC classes containing both geometric and semantic information.\nUsing only the geometric information of objects, the experiments show that\nthree different deep learning models are able to achieve good classification\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:59:00 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Emunds", "Christoph", ""], ["Pauen", "Nicolas", ""], ["Richter", "Veronika", ""], ["Frisch", "J\u00e9r\u00f4me", ""], ["van Treeck", "Christoph", ""]]}, {"id": "2106.09748", "submitter": "Liang-Chieh Chen", "authors": "Mark Weber, Huiyu Wang, Siyuan Qiao, Jun Xie, Maxwell D. Collins,\n  Yukun Zhu, Liangzhe Yuan, Dahun Kim, Qihang Yu, Daniel Cremers, Laura\n  Leal-Taixe, Alan L. Yuille, Florian Schroff, Hartwig Adam, Liang-Chieh Chen", "title": "DeepLab2: A TensorFlow Library for Deep Labeling", "comments": "4-page technical report. The first three authors contributed equally\n  to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeepLab2 is a TensorFlow library for deep labeling, aiming to provide a\nstate-of-the-art and easy-to-use TensorFlow codebase for general dense pixel\nprediction problems in computer vision. DeepLab2 includes all our recently\ndeveloped DeepLab model variants with pretrained checkpoints as well as model\ntraining and evaluation code, allowing the community to reproduce and further\nimprove upon the state-of-art systems. To showcase the effectiveness of\nDeepLab2, our Panoptic-DeepLab employing Axial-SWideRNet as network backbone\nachieves 68.0% PQ or 83.5% mIoU on Cityscaspes validation set, with only\nsingle-scale inference and ImageNet-1K pretrained checkpoints. We hope that\npublicly sharing our library could facilitate future research on dense pixel\nlabeling tasks and envision new applications of this technology. Code is made\npublicly available at \\url{https://github.com/google-research/deeplab2}.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:04:53 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Weber", "Mark", ""], ["Wang", "Huiyu", ""], ["Qiao", "Siyuan", ""], ["Xie", "Jun", ""], ["Collins", "Maxwell D.", ""], ["Zhu", "Yukun", ""], ["Yuan", "Liangzhe", ""], ["Kim", "Dahun", ""], ["Yu", "Qihang", ""], ["Cremers", "Daniel", ""], ["Leal-Taixe", "Laura", ""], ["Yuille", "Alan L.", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "2106.09756", "submitter": "Haiping Lu", "authors": "Haiping Lu, Xianyuan Liu, Robert Turner, Peizhen Bai, Raivo E Koot,\n  Shuo Zhou, Mustafa Chasmai, Lawrence Schobs", "title": "PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python", "comments": "This library is available at https://github.com/pykale/pykale", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a general-purpose technology holding promises for many\ninterdisciplinary research problems. However, significant barriers exist in\ncrossing disciplinary boundaries when most machine learning tools are developed\nin different areas separately. We present Pykale - a Python library for\nknowledge-aware machine learning on graphs, images, texts, and videos to enable\nand accelerate interdisciplinary research. We formulate new green machine\nlearning guidelines based on standard software engineering practices and\npropose a novel pipeline-based application programming interface (API). PyKale\nfocuses on leveraging knowledge from multiple sources for accurate and\ninterpretable prediction, thus supporting multimodal learning and transfer\nlearning (particularly domain adaptation) with latest deep learning and\ndimensionality reduction models. We build PyKale on PyTorch and leverage the\nrich PyTorch ecosystem. Our pipeline-based API design enforces standardization\nand minimalism, embracing green machine learning concepts via reducing\nrepetitions and redundancy, reusing existing resources, and recycling learning\nmodels across areas. We demonstrate its interdisciplinary nature via examples\nin bioinformatics, knowledge graph, image/video recognition, and medical\nimaging.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:35:37 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Lu", "Haiping", ""], ["Liu", "Xianyuan", ""], ["Turner", "Robert", ""], ["Bai", "Peizhen", ""], ["Koot", "Raivo E", ""], ["Zhou", "Shuo", ""], ["Chasmai", "Mustafa", ""], ["Schobs", "Lawrence", ""]]}, {"id": "2106.09758", "submitter": "Artsiom Sanakoyeu", "authors": "Natalia Neverova, Artsiom Sanakoyeu, Patrick Labatut, David Novotny,\n  Andrea Vedaldi", "title": "Discovering Relationships between Object Categories via Universal\n  Canonical Maps", "comments": "Accepted at CVPR 2021; Project page:\n  https://gdude.de/discovering-3d-obj-rel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the problem of learning the geometry of multiple categories of\ndeformable objects jointly. Recent work has shown that it is possible to learn\na unified dense pose predictor for several categories of related objects.\nHowever, training such models requires to initialize inter-category\ncorrespondences by hand. This is suboptimal and the resulting models fail to\nmaintain correct correspondences as individual categories are learned. In this\npaper, we show that improved correspondences can be learned automatically as a\nnatural byproduct of learning category-specific dense pose predictors. To do\nthis, we express correspondences between different categories and between\nimages and categories using a unified embedding. Then, we use the latter to\nenforce two constraints: symmetric inter-category cycle consistency and a new\nasymmetric image-to-category cycle consistency. Without any manual annotations\nfor the inter-category correspondences, we obtain state-of-the-art alignment\nresults, outperforming dedicated methods for matching 3D shapes. Moreover, the\nnew model is also better at the task of dense pose prediction than prior work.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:38:18 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Neverova", "Natalia", ""], ["Sanakoyeu", "Artsiom", ""], ["Labatut", "Patrick", ""], ["Novotny", "David", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2106.09759", "submitter": "A. Ben Hamza", "authors": "Hasib Zunair and A. Ben Hamza", "title": "Synthetic COVID-19 Chest X-ray Dataset for Computer-Aided Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new dataset called Synthetic COVID-19 Chest X-ray Dataset for\ntraining machine learning models. The dataset consists of 21,295 synthetic\nCOVID-19 chest X-ray images to be used for computer-aided diagnosis. These\nimages, generated via an unsupervised domain adaptation approach, are of high\nquality. We find that the synthetic images not only improve performance of\nvarious deep learning architectures when used as additional training data under\nheavy imbalance conditions, but also detect the target class with high\nconfidence. We also find that comparable performance can also be achieved when\ntrained only on synthetic images. Further, salient features of the synthetic\nCOVID-19 images indicate that the distribution is significantly different from\nNon-COVID-19 classes, enabling a proper decision boundary. We hope the\navailability of such high fidelity chest X-ray images of COVID-19 will\nencourage advances in the development of diagnostic and/or management tools.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:39:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zunair", "Hasib", ""], ["Hamza", "A. Ben", ""]]}, {"id": "2106.09785", "submitter": "Chunyuan Li", "authors": "Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang\n  Dai, Lu Yuan, Jianfeng Gao", "title": "Efficient Self-supervised Vision Transformers for Representation\n  Learning", "comments": "24 pages, 12 figures, file size 13.6MB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates two techniques for developing efficient\nself-supervised vision transformers (EsViT) for visual representation learning.\nFirst, we show through a comprehensive empirical study that multi-stage\narchitectures with sparse self-attentions can significantly reduce modeling\ncomplexity but with a cost of losing the ability to capture fine-grained\ncorrespondences between image regions. Second, we propose a new pre-training\ntask of region matching which allows the model to capture fine-grained region\ndependencies and as a result significantly improves the quality of the learned\nvision representations. Our results show that combining the two techniques,\nEsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation,\noutperforming prior arts with around an order magnitude of higher throughput.\nWhen transferring to downstream linear classification tasks, EsViT outperforms\nits supervised counterpart on 17 out of 18 datasets. The code and models will\nbe publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 19:57:33 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Li", "Chunyuan", ""], ["Yang", "Jianwei", ""], ["Zhang", "Pengchuan", ""], ["Gao", "Mei", ""], ["Xiao", "Bin", ""], ["Dai", "Xiyang", ""], ["Yuan", "Lu", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2106.09788", "submitter": "Andrei Kapishnikov", "authors": "Andrei Kapishnikov, Subhashini Venugopalan, Besim Avci, Ben Wedin,\n  Michael Terry, Tolga Bolukbasi", "title": "Guided Integrated Gradients: An Adaptive Path Method for Removing Noise", "comments": "13 pages, 11 figures, for implementation sources see\n  https://github.com/PAIR-code/saliency", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2021, pp. 5050-5058", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated Gradients (IG) is a commonly used feature attribution method for\ndeep neural networks. While IG has many desirable properties, the method often\nproduces spurious/noisy pixel attributions in regions that are not related to\nthe predicted class when applied to visual models. While this has been\npreviously noted, most existing solutions are aimed at addressing the symptoms\nby explicitly reducing the noise in the resulting attributions. In this work,\nwe show that one of the causes of the problem is the accumulation of noise\nalong the IG path. To minimize the effect of this source of noise, we propose\nadapting the attribution path itself -- conditioning the path not just on the\nimage but also on the model being explained. We introduce Adaptive Path Methods\n(APMs) as a generalization of path methods, and Guided IG as a specific\ninstance of an APM. Empirically, Guided IG creates saliency maps better aligned\nwith the model's prediction and the input image that is being explained. We\nshow through qualitative and quantitative experiments that Guided IG\noutperforms other, related methods in nearly every experiment.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 20:00:55 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Kapishnikov", "Andrei", ""], ["Venugopalan", "Subhashini", ""], ["Avci", "Besim", ""], ["Wedin", "Ben", ""], ["Terry", "Michael", ""], ["Bolukbasi", "Tolga", ""]]}, {"id": "2106.09794", "submitter": "Shuyue Guan", "authors": "Shuyue Guan, Murray Loew", "title": "A Distance-based Separability Measure for Internal Cluster Validation", "comments": "It is an extended version of the paper: arXiv:2009.01328", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To evaluate clustering results is a significant part of cluster analysis.\nSince there are no true class labels for clustering in typical unsupervised\nlearning, many internal cluster validity indices (CVIs), which use predicted\nlabels and data, have been created. Without true labels, to design an effective\nCVI is as difficult as to create a clustering method. And it is crucial to have\nmore CVIs because there are no universal CVIs that can be used to measure all\ndatasets and no specific methods of selecting a proper CVI for clusters without\ntrue labels. Therefore, to apply a variety of CVIs to evaluate clustering\nresults is necessary. In this paper, we propose a novel internal CVI -- the\nDistance-based Separability Index (DSI), based on a data separability measure.\nWe compared the DSI with eight internal CVIs including studies from early Dunn\n(1974) to most recent CVDD (2019) and an external CVI as ground truth, by using\nclustering results of five clustering algorithms on 12 real and 97 synthetic\ndatasets. Results show DSI is an effective, unique, and competitive CVI to\nother compared CVIs. We also summarized the general process to evaluate CVIs\nand created the rank-difference metric for comparison of CVIs' results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 20:19:50 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Guan", "Shuyue", ""], ["Loew", "Murray", ""]]}, {"id": "2106.09812", "submitter": "Hrithwik Shalu", "authors": "Joseph Stember, Hrithwik Shalu", "title": "Deep reinforcement learning with automated label extraction from\n  clinical reports accurately classifies 3D MRI brain volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Image classification is perhaps the most fundamental task in imaging\nAI. However, labeling images is time-consuming and tedious. We have recently\ndemonstrated that reinforcement learning (RL) can classify 2D slices of MRI\nbrain images with high accuracy. Here we make two important steps toward\nspeeding image classification: Firstly, we automatically extract class labels\nfrom the clinical reports. Secondly, we extend our prior 2D classification work\nto fully 3D image volumes from our institution. Hence, we proceed as follows:\nin Part 1, we extract labels from reports automatically using the SBERT natural\nlanguage processing approach. Then, in Part 2, we use these labels with RL to\ntrain a classification Deep-Q Network (DQN) for 3D image volumes.\n  Methods: For Part 1, we trained SBERT with 90 radiology report impressions.\nWe then used the trained SBERT to predict class labels for use in Part 2. In\nPart 2, we applied multi-step image classification to allow for combined Deep-Q\nlearning using 3D convolutions and TD(0) Q learning. We trained on a set of 90\nimages. We tested on a separate set of 61 images, again using the classes\npredicted from patient reports by the trained SBERT in Part 1. For comparison,\nwe also trained and tested a supervised deep learning classification network on\nthe same set of training and testing images using the same labels.\n  Results: Part 1: Upon training with the corpus of radiology reports, the\nSBERT model had 100% accuracy for both normal and metastasis-containing scans.\nPart 2: Then, using these labels, whereas the supervised approach quickly\noverfit the training data and as expected performed poorly on the testing set\n(66% accuracy, just over random guessing), the reinforcement learning approach\nachieved an accuracy of 92%. The results were found to be statistically\nsignificant, with a p-value of 3.1 x 10^-5.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 20:53:41 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Stember", "Joseph", ""], ["Shalu", "Hrithwik", ""]]}, {"id": "2106.09832", "submitter": "Nicol\\'as Gaggion", "authors": "Nicol\\'as Gaggion, Lucas Mansilla, Diego Milone, Enzo Ferrante", "title": "Hybrid graph convolutional neural networks for landmark-based anatomical\n  segmentation", "comments": "Accepted for publication at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work we address the problem of landmark-based segmentation for\nanatomical structures. We propose HybridGNet, an encoder-decoder neural\narchitecture which combines standard convolutions for image feature encoding,\nwith graph convolutional neural networks to decode plausible representations of\nanatomical structures. We benchmark the proposed architecture considering other\nstandard landmark and pixel-based models for anatomical segmentation in chest\nx-ray images, and found that HybridGNet is more robust to image occlusions. We\nalso show that it can be used to construct landmark-based segmentations from\npixel level annotations. Our experimental results suggest that HybridGNet\nproduces accurate and anatomically plausible landmark-based segmentations, by\nnaturally incorporating shape constraints within the decoding process via\nspectral convolutions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 22:04:44 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Gaggion", "Nicol\u00e1s", ""], ["Mansilla", "Lucas", ""], ["Milone", "Diego", ""], ["Ferrante", "Enzo", ""]]}, {"id": "2106.09834", "submitter": "Ge Wang Dr.", "authors": "Weiwen Wu, Chuang Niu, Shadi Ebrahimian, Hengyong Yu, Mannu Kalra, Ge\n  Wang", "title": "AI-Enabled Ultra-Low-Dose CT Reconstruction", "comments": "19 pages, 10 figures, 1 table, 44 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By the ALARA (As Low As Reasonably Achievable) principle, ultra-low-dose CT\nreconstruction is a holy grail to minimize cancer risks and genetic damages,\nespecially for children. With the development of medical CT technologies, the\niterative algorithms are widely used to reconstruct decent CT images from a\nlow-dose scan. Recently, artificial intelligence (AI) techniques have shown a\ngreat promise in further reducing CT radiation dose to the next level. In this\npaper, we demonstrate that AI-powered CT reconstruction offers diagnostic image\nquality at an ultra-low-dose level comparable to that of radiography.\nSpecifically, here we develop a Split Unrolled Grid-like Alternative\nReconstruction (SUGAR) network, in which deep learning, physical modeling and\nimage prior are integrated. The reconstruction results from clinical datasets\nshow that excellent images can be reconstructed using SUGAR from 36\nprojections. This approach has a potential to change future healthcare.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 22:13:11 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wu", "Weiwen", ""], ["Niu", "Chuang", ""], ["Ebrahimian", "Shadi", ""], ["Yu", "Hengyong", ""], ["Kalra", "Mannu", ""], ["Wang", "Ge", ""]]}, {"id": "2106.09835", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Mostafa El-Khamy, Jungwon Lee", "title": "Dual-Teacher Class-Incremental Learning With Data-Free Generative Replay", "comments": "CVPR 2021 Workshop on Continual Learning in Computer Vision\n  (CLVision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes two novel knowledge transfer techniques for\nclass-incremental learning (CIL). First, we propose data-free generative replay\n(DF-GR) to mitigate catastrophic forgetting in CIL by using synthetic samples\nfrom a generative model. In the conventional generative replay, the generative\nmodel is pre-trained for old data and shared in extra memory for later\nincremental learning. In our proposed DF-GR, we train a generative model from\nscratch without using any training data, based on the pre-trained\nclassification model from the past, so we curtail the cost of sharing\npre-trained generative models. Second, we introduce dual-teacher information\ndistillation (DT-ID) for knowledge distillation from two teachers to one\nstudent. In CIL, we use DT-ID to learn new classes incrementally based on the\npre-trained model for old classes and another model (pre-)trained on the new\ndata for new classes. We implemented the proposed schemes on top of one of the\nstate-of-the-art CIL methods and showed the performance improvement on\nCIFAR-100 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 22:13:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Choi", "Yoojin", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "2106.09857", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Minghai Qin, Fei Sun, Zejiang Hou, Kun Yuan, Yi Xu,\n  Yanzhi Wang, Yen-Kuang Chen, Rong Jin, Yuan Xie", "title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNNs) are effective in solving many real-world\nproblems. Larger DNN models usually exhibit better quality (e.g., accuracy) but\ntheir excessive computation results in long training and inference time. Model\nsparsification can reduce the computation and memory cost while maintaining\nmodel quality. Most existing sparsification algorithms unidirectionally remove\nweights, while others randomly or greedily explore a small subset of weights in\neach layer. The inefficiency of the algorithms reduces the achievable sparsity\nlevel. In addition, many algorithms still require pre-trained dense models and\nthus suffer from large memory footprint and long training time. In this paper,\nwe propose a novel scheduled grow-and-prune (GaP) methodology without\npre-training the dense models. It addresses the shortcomings of the previous\nworks by repeatedly growing a subset of layers to dense and then pruning back\nto sparse after some training. Experiments have shown that such models can\nmatch or beat the quality of highly optimized dense models at 80% sparsity on a\nvariety of tasks, such as image classification, objective detection, 3D object\npart segmentation, and translation. They also outperform other state-of-the-art\n(SOTA) pruning methods, including pruning from pre-trained dense models. As an\nexample, a 90% sparse ResNet-50 obtained via GaP achieves 77.9% top-1 accuracy\non ImageNet, improving the SOTA results by 1.5%.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 01:03:13 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ma", "Xiaolong", ""], ["Qin", "Minghai", ""], ["Sun", "Fei", ""], ["Hou", "Zejiang", ""], ["Yuan", "Kun", ""], ["Xu", "Yi", ""], ["Wang", "Yanzhi", ""], ["Chen", "Yen-Kuang", ""], ["Jin", "Rong", ""], ["Xie", "Yuan", ""]]}, {"id": "2106.09859", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Thomas Lukasiewicz, Xiaolin Hu, Jianfei Cai, Zhenghua\n  Xu", "title": "RSG: A Simple but Effective Module for Learning Imbalanced Datasets", "comments": "To appear at CVPR 2021. We propose a flexible data generation/data\n  augmentation module for long-tailed classification. Codes are available at:\n  https://github.com/Jianf-Wang/RSG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imbalanced datasets widely exist in practice and area great challenge for\ntraining deep neural models with agood generalization on infrequent classes. In\nthis work, wepropose a new rare-class sample generator (RSG) to solvethis\nproblem. RSG aims to generate some new samplesfor rare classes during training,\nand it has in particularthe following advantages: (1) it is convenient to use\nandhighly versatile, because it can be easily integrated intoany kind of\nconvolutional neural network, and it works wellwhen combined with different\nloss functions, and (2) it isonly used during the training phase, and\ntherefore, no ad-ditional burden is imposed on deep neural networks duringthe\ntesting phase. In extensive experimental evaluations, weverify the\neffectiveness of RSG. Furthermore, by leveragingRSG, we obtain competitive\nresults on Imbalanced CIFARand new state-of-the-art results on Places-LT,\nImageNet-LT, and iNaturalist 2018. The source code is available at\nhttps://github.com/Jianf-Wang/RSG.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 01:10:27 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wang", "Jianfeng", ""], ["Lukasiewicz", "Thomas", ""], ["Hu", "Xiaolin", ""], ["Cai", "Jianfei", ""], ["Xu", "Zhenghua", ""]]}, {"id": "2106.09862", "submitter": "Lei Li", "authors": "Lei Li and Veronika A. Zimmer and Julia A. Schnabel and Xiahai Zhuang", "title": "Medical Image Analysis on Left Atrial LGE MRI for Atrial Fibrillation\n  Studies: A Review", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is commonly\nused to visualize and quantify left atrial (LA) scars. The position and extent\nof scars provide important information of the pathophysiology and progression\nof atrial fibrillation (AF). Hence, LA scar segmentation and quantification\nfrom LGE MRI can be useful in computer-assisted diagnosis and treatment\nstratification of AF patients. Since manual delineation can be time-consuming\nand subject to intra- and inter-expert variability, automating this computing\nis highly desired, which nevertheless is still challenging and\nunder-researched.\n  This paper aims to provide a systematic review on computing methods for LA\ncavity, wall, scar and ablation gap segmentation and quantification from LGE\nMRI, and the related literature for AF studies. Specifically, we first\nsummarize AF-related imaging techniques, particularly LGE MRI. Then, we review\nthe methodologies of the four computing tasks in detail, and summarize the\nvalidation strategies applied in each task. Finally, the possible future\ndevelopments are outlined, with a brief survey on the potential clinical\napplications of the aforementioned methods. The review shows that the research\ninto this topic is still in early stages. Although several methods have been\nproposed, especially for LA segmentation, there is still large scope for\nfurther algorithmic developments due to performance issues related to the high\nvariability of enhancement appearance and differences in image acquisition.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 01:31:06 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 08:47:34 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Li", "Lei", ""], ["Zimmer", "Veronika A.", ""], ["Schnabel", "Julia A.", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2106.09872", "submitter": "Lina Wang", "authors": "Lina Wang, Xingshu Chen, Yulong Wang, Yawei Yue, Yi Zhu, Xuemei Zeng,\n  Wei Wang", "title": "Analyzing Adversarial Robustness of Deep Neural Networks in Pixel Space:\n  a Semantic Perspective", "comments": "13 pages, 6figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vulnerability of deep neural networks to adversarial examples, which are\ncrafted maliciously by modifying the inputs with imperceptible perturbations to\nmisled the network produce incorrect outputs, reveals the lack of robustness\nand poses security concerns. Previous works study the adversarial robustness of\nimage classifiers on image level and use all the pixel information in an image\nindiscriminately, lacking of exploration of regions with different semantic\nmeanings in the pixel space of an image. In this work, we fill this gap and\nexplore the pixel space of the adversarial image by proposing an algorithm to\nlooking for possible perturbations pixel by pixel in different regions of the\nsegmented image. The extensive experimental results on CIFAR-10 and ImageNet\nverify that searching for the modified pixel in only some pixels of an image\ncan successfully launch the one-pixel adversarial attacks without requiring all\nthe pixels of the entire image, and there exist multiple vulnerable points\nscattered in different regions of an image. We also demonstrate that the\nadversarial robustness of different regions on the image varies with the amount\nof semantic information contained.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 02:16:01 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wang", "Lina", ""], ["Chen", "Xingshu", ""], ["Wang", "Yulong", ""], ["Yue", "Yawei", ""], ["Zhu", "Yi", ""], ["Zeng", "Xuemei", ""], ["Wang", "Wei", ""]]}, {"id": "2106.09874", "submitter": "Zhao Kang", "authors": "Zhengrui Ma, Zhao Kang, Guangchun Luo, Ling Tian", "title": "Towards Clustering-friendly Representations: Subspace Clustering via\n  Graph Filtering", "comments": "Published in ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Finding a suitable data representation for a specific task has been shown to\nbe crucial in many applications. The success of subspace clustering depends on\nthe assumption that the data can be separated into different subspaces.\nHowever, this simple assumption does not always hold since the raw data might\nnot be separable into subspaces. To recover the ``clustering-friendly''\nrepresentation and facilitate the subsequent clustering, we propose a graph\nfiltering approach by which a smooth representation is achieved. Specifically,\nit injects graph similarity into data features by applying a low-pass filter to\nextract useful data representations for clustering. Extensive experiments on\nimage and document clustering datasets demonstrate that our method improves\nupon state-of-the-art subspace clustering techniques. Especially, its\ncomparable performance with deep learning methods emphasizes the effectiveness\nof the simple graph filtering scheme for many real-world applications. An\nablation study shows that graph filtering can remove noise, preserve structure\nin the image, and increase the separability of classes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 02:21:36 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ma", "Zhengrui", ""], ["Kang", "Zhao", ""], ["Luo", "Guangchun", ""], ["Tian", "Ling", ""]]}, {"id": "2106.09875", "submitter": "Zhao Kang", "authors": "Peng Chen, Liang Liu, Zhengrui Ma, Zhao Kang", "title": "Smoothed Multi-View Subspace Clustering", "comments": "Accepted by International Conference on Neural Computing for Advanced\n  Applications 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, multi-view subspace clustering has achieved impressive\nperformance due to the exploitation of complementary imformation across\nmultiple views. However, multi-view data can be very complicated and are not\neasy to cluster in real-world applications. Most existing methods operate on\nraw data and may not obtain the optimal solution. In this work, we propose a\nnovel multi-view clustering method named smoothed multi-view subspace\nclustering (SMVSC) by employing a novel technique, i.e., graph filtering, to\nobtain a smooth representation for each view, in which similar data points have\nsimilar feature values. Specifically, it retains the graph geometric features\nthrough applying a low-pass filter. Consequently, it produces a\n``clustering-friendly\" representation and greatly facilitates the downstream\nclustering task. Extensive experiments on benchmark datasets validate the\nsuperiority of our approach. Analysis shows that graph filtering increases the\nseparability of classes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 02:24:19 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chen", "Peng", ""], ["Liu", "Liang", ""], ["Ma", "Zhengrui", ""], ["Kang", "Zhao", ""]]}, {"id": "2106.09886", "submitter": "Qigong Sun", "authors": "Qigong Sun, Xiufang Li, Fanhua Shang, Hongying Liu, Kang Yang, Licheng\n  Jiao, and Zhouchen Lin", "title": "Quantized Neural Networks via {-1, +1} Encoding Decomposition and\n  Acceleration", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.13389", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of deep neural networks (DNNs) always requires intensive\nresources for both computation and data storage. Thus, DNNs cannot be\nefficiently applied to mobile phones and embedded devices, which severely\nlimits their applicability in industrial applications. To address this issue,\nwe propose a novel encoding scheme using {-1, +1} to decompose quantized neural\nnetworks (QNNs) into multi-branch binary networks, which can be efficiently\nimplemented by bitwise operations (i.e., xnor and bitcount) to achieve model\ncompression, computational acceleration, and resource saving. By using our\nmethod, users can achieve different encoding precisions arbitrarily according\nto their requirements and hardware resources. The proposed mechanism is highly\nsuitable for the use of FPGA and ASIC in terms of data storage and computation,\nwhich provides a feasible idea for smart chips. We validate the effectiveness\nof our method on large-scale image classification (e.g., ImageNet), object\ndetection, and semantic segmentation tasks. In particular, our method with\nlow-bit encoding can still achieve almost the same performance as its high-bit\ncounterparts.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:11:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Sun", "Qigong", ""], ["Li", "Xiufang", ""], ["Shang", "Fanhua", ""], ["Liu", "Hongying", ""], ["Yang", "Kang", ""], ["Jiao", "Licheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "2106.09887", "submitter": "Lin Wang", "authors": "Lin Wang, Lie Ju, Donghao Zhang, Xin Wang, Wanji He, Yelin Huang,\n  Zhiwen Yang, Xuan Yao, Xin Zhao, Xiufen Ye, Zongyuan Ge", "title": "Medical Matting: A New Perspective on Medical Segmentation with\n  Uncertainty", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical image segmentation, it is difficult to mark ambiguous areas\naccurately with binary masks, especially when dealing with small lesions.\nTherefore, it is a challenge for radiologists to reach a consensus by using\nbinary masks under the condition of multiple annotations. However, these areas\nmay contain anatomical structures that are conducive to diagnosis. Uncertainty\nis introduced to study these situations. Nevertheless, the uncertainty is\nusually measured by the variances between predictions in a multiple trial way.\nIt is not intuitive, and there is no exact correspondence in the image.\nInspired by image matting, we introduce matting as a soft segmentation method\nand a new perspective to deal with and represent uncertain regions into medical\nscenes, namely medical matting. More specifically, because there is no\navailable medical matting dataset, we first labeled two medical datasets with\nalpha matte. Secondly, the matting method applied to the natural image is not\nsuitable for the medical scene, so we propose a new architecture to generate\nbinary masks and alpha matte in a row. Thirdly, the uncertainty map is\nintroduced to highlight the ambiguous regions from the binary results and\nimprove the matting performance. Evaluated on these datasets, the proposed\nmodel outperformed state-of-the-art matting algorithms by a large margin, and\nalpha matte is proved to be a more efficient labeling form than a binary mask.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:13:52 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 11:40:12 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wang", "Lin", ""], ["Ju", "Lie", ""], ["Zhang", "Donghao", ""], ["Wang", "Xin", ""], ["He", "Wanji", ""], ["Huang", "Yelin", ""], ["Yang", "Zhiwen", ""], ["Yao", "Xuan", ""], ["Zhao", "Xin", ""], ["Ye", "Xiufen", ""], ["Ge", "Zongyuan", ""]]}, {"id": "2106.09889", "submitter": "Lin Su", "authors": "Lin Su and Nan Duan and Edward Cui and Lei Ji and Chenfei Wu and\n  Huaishao Luo and Yongfei Liu and Ming Zhong and Taroon Bharti and Arun\n  Sacheti", "title": "GEM: A General Evaluation Benchmark for Multimodal Tasks", "comments": "Accepted by Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present GEM as a General Evaluation benchmark for\nMultimodal tasks. Different from existing datasets such as GLUE, SuperGLUE,\nXGLUE and XTREME that mainly focus on natural language tasks, GEM is a\nlarge-scale vision-language benchmark, which consists of GEM-I for\nimage-language tasks and GEM-V for video-language tasks. Comparing with\nexisting multimodal datasets such as MSCOCO and Flicker30K for image-language\ntasks, YouCook2 and MSR-VTT for video-language tasks, GEM is not only the\nlargest vision-language dataset covering image-language tasks and\nvideo-language tasks at the same time, but also labeled in multiple languages.\nWe also provide two baseline models for this benchmark. We will release the\ndataset, code and baseline models, aiming to advance the development of\nmultilingual multimodal research.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:14:13 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Su", "Lin", ""], ["Duan", "Nan", ""], ["Cui", "Edward", ""], ["Ji", "Lei", ""], ["Wu", "Chenfei", ""], ["Luo", "Huaishao", ""], ["Liu", "Yongfei", ""], ["Zhong", "Ming", ""], ["Bharti", "Taroon", ""], ["Sacheti", "Arun", ""]]}, {"id": "2106.09894", "submitter": "Ryan Kim", "authors": "Ryan Kim", "title": "Development of a conversing and body temperature scanning autonomously\n  navigating robot to help screen for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Throughout the COVID-19 pandemic, the most common symptom displayed by\npatients has been a fever, leading to the use of temperature scanning as a\npreemptive measure to detect potential carriers of the virus. Human employees\nwith handheld thermometers have been used to fulfill this task, however this\nputs them at risk as they cannot be physically distanced and the sequential\nnature of this method leads to great inconveniences and inefficiency. The\nproposed solution is an autonomously navigating robot capable of conversing and\nscanning people's temperature to detect fevers and help screen for COVID-19. To\nsatisfy this objective, the robot must be able to (1) navigate autonomously,\n(2) detect and track people, and (3) get individuals' temperature reading and\nconverse with them if it exceeds 38{\\deg}C. An autonomously navigating mobile\nrobot is used with a manipulator controlled using a face tracking algorithm,\nand an end effector consisting of a thermal camera, smartphone, and chatbot.\nThe goal is to develop a functioning solution that performs the above tasks. In\naddition, technical challenges encountered and their engineering solutions will\nbe presented, and recommendations will be made for enhancements that could be\nincorporated when approaching commercialization.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:30:11 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Kim", "Ryan", ""]]}, {"id": "2106.09908", "submitter": "Kyulim Kim", "authors": "Kyulim Kim, JeongSoo Kim, Seungri Song, Jun-Ho Choi, Chulmin Joo,\n  Jong-Seok Lee", "title": "Light Lies: Optical Adversarial Attack", "comments": "11 pages, 4 figures, author names corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A significant amount of work has been done on adversarial attacks that inject\nimperceptible noise to images to deteriorate the image classification\nperformance of deep models. However, most of the existing studies consider\nattacks in the digital (pixel) domain where an image acquired by an image\nsensor with sampling and quantization has been recorded. This paper, for the\nfirst time, introduces an optical adversarial attack, which physically alters\nthe light field information arriving at the image sensor so that the\nclassification model yields misclassification. More specifically, we modulate\nthe phase of the light in the Fourier domain using a spatial light modulator\nplaced in the photographic system. The operative parameters of the modulator\nare obtained by gradient-based optimization to maximize cross-entropy and\nminimize distortions. We present experiments based on both simulation and a\nreal hardware optical system, from which the feasibility of the proposed\noptical attack is demonstrated. It is also verified that the proposed attack is\ncompletely different from common optical-domain distortions such as spherical\naberration, defocus, and astigmatism in terms of both perturbation patterns and\nclassification results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 04:20:49 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 05:57:41 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Kim", "Kyulim", ""], ["Kim", "JeongSoo", ""], ["Song", "Seungri", ""], ["Choi", "Jun-Ho", ""], ["Joo", "Chulmin", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2106.09914", "submitter": "Tomoki Watanabe", "authors": "Tomoki Watanabe, Paolo Favaro", "title": "A Unified Generative Adversarial Network Training via Self-Labeling and\n  Self-Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel GAN training scheme that can handle any level of labeling\nin a unified manner. Our scheme introduces a form of artificial labeling that\ncan incorporate manually defined labels, when available, and induce an\nalignment between them. To define the artificial labels, we exploit the\nassumption that neural network generators can be trained more easily to map\nnearby latent vectors to data with semantic similarities, than across separate\ncategories. We use generated data samples and their corresponding artificial\nconditioning labels to train a classifier. The classifier is then used to\nself-label real data. To boost the accuracy of the self-labeling, we also use\nthe exponential moving average of the classifier. However, because the\nclassifier might still make mistakes, especially at the beginning of the\ntraining, we also refine the labels through self-attention, by using the\nlabeling of real data samples only when the classifier outputs a high\nclassification probability score. We evaluate our approach on CIFAR-10, STL-10\nand SVHN, and show that both self-labeling and self-attention consistently\nimprove the quality of generated data. More surprisingly, we find that the\nproposed scheme can even outperform class-conditional GANs.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 04:40:26 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Watanabe", "Tomoki", ""], ["Favaro", "Paolo", ""]]}, {"id": "2106.09932", "submitter": "Awad Abdelhalim", "authors": "Awad Abdelhalim, Montasir Abbas, Bhavi Bharat Kotha, Alfred Wicks", "title": "A Framework for Real-time Traffic Trajectory Tracking, Speed Estimation,\n  and Driver Behavior Calibration at Urban Intersections Using Virtual Traffic\n  Lanes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous study, we presented VT-Lane, a three-step framework for\nreal-time vehicle detection, tracking, and turn movement classification at\nurban intersections. In this study, we present a case study incorporating the\nhighly accurate trajectories and movement classification obtained via VT-Lane\nfor the purpose of speed estimation and driver behavior calibration for traffic\nat urban intersections. First, we use a highly instrumented vehicle to verify\nthe estimated speeds obtained from video inference. The results of the speed\nvalidation show that our method can estimate the average travel speed of\ndetected vehicles in real-time with an error of 0.19 m/sec, which is equivalent\nto 2% of the average observed travel speeds in the intersection of the study.\nInstantaneous speeds (at the resolution of 30 Hz) were found to be estimated\nwith an average error of 0.21 m/sec and 0.86 m/sec respectively for\nfree-flowing and congested traffic conditions. We then use the estimated speeds\nto calibrate the parameters of a driver behavior model for the vehicles in the\narea of study. The results show that the calibrated model replicates the\ndriving behavior with an average error of 0.45 m/sec, indicating the high\npotential for using this framework for automated, large-scale calibration of\ncar-following models from roadside traffic video data, which can lead to\nsubstantial improvements in traffic modeling via microscopic simulation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 06:15:53 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Abdelhalim", "Awad", ""], ["Abbas", "Montasir", ""], ["Kotha", "Bhavi Bharat", ""], ["Wicks", "Alfred", ""]]}, {"id": "2106.09946", "submitter": "Sauptik Dhar", "authors": "Sauptik Dhar, Javad Heydari, Samarth Tripathi, Unmesh Kurup, Mohak\n  Shah", "title": "Evolving GANs: When Contradictions Turn into Compliance", "comments": "Generative Adversarial Networks, Universum Learning, Semi-Supervised\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited availability of labeled-data makes any supervised learning problem\nchallenging. Alternative learning settings like semi-supervised and universum\nlearning alleviate the dependency on labeled data, but still require a large\namount of unlabeled data, which may be unavailable or expensive to acquire.\nGAN-based synthetic data generation methods have recently shown promise by\ngenerating synthetic samples to improve task at hand. However, these samples\ncannot be used for other purposes. In this paper, we propose a GAN game which\nprovides improved discriminator accuracy under limited data settings, while\ngenerating realistic synthetic data. This provides the added advantage that now\nthe generated data can be used for other similar tasks. We provide the\ntheoretical guarantees and empirical results in support of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 06:51:35 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Dhar", "Sauptik", ""], ["Heydari", "Javad", ""], ["Tripathi", "Samarth", ""], ["Kurup", "Unmesh", ""], ["Shah", "Mohak", ""]]}, {"id": "2106.09947", "submitter": "Maura Pintor", "authors": "Maura Pintor, Luca Demetrio, Angelo Sotgiu, Giovanni Manca, Ambra\n  Demontis, Nicholas Carlini, Battista Biggio, Fabio Roli", "title": "Indicators of Attack Failure: Debugging and Improving Optimization of\n  Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating robustness of machine-learning models to adversarial examples is a\nchallenging problem. Many defenses have been shown to provide a false sense of\nsecurity by causing gradient-based attacks to fail, and they have been broken\nunder more rigorous evaluations. Although guidelines and best practices have\nbeen suggested to improve current adversarial robustness evaluations, the lack\nof automatic testing and debugging tools makes it difficult to apply these\nrecommendations in a systematic manner. In this work, we overcome these\nlimitations by (i) defining a set of quantitative indicators which unveil\ncommon failures in the optimization of gradient-based attacks, and (ii)\nproposing specific mitigation strategies within a systematic evaluation\nprotocol. Our extensive experimental analysis shows that the proposed\nindicators of failure can be used to visualize, debug and improve current\nadversarial robustness evaluations, providing a first concrete step towards\nautomatizing and systematizing current adversarial robustness evaluations. Our\nopen-source code is available at:\nhttps://github.com/pralab/IndicatorsOfAttackFailure.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 06:57:58 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Pintor", "Maura", ""], ["Demetrio", "Luca", ""], ["Sotgiu", "Angelo", ""], ["Manca", "Giovanni", ""], ["Demontis", "Ambra", ""], ["Carlini", "Nicholas", ""], ["Biggio", "Battista", ""], ["Roli", "Fabio", ""]]}, {"id": "2106.09958", "submitter": "Chengwei Chen", "authors": "Chengwei Chen, Yuan Xie, Shaohui Lin, Ruizhi Qiao, Jian Zhou, Xin Tan,\n  Yi Zhang and Lizhuang Ma", "title": "Novelty Detection via Contrastive Learning with Negative Data\n  Augmentation", "comments": null, "journal-ref": "IJCAI2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection is the process of determining whether a query example\ndiffers from the learned training distribution. Previous methods attempt to\nlearn the representation of the normal samples via generative adversarial\nnetworks (GANs). However, they will suffer from instability training, mode\ndropping, and low discriminative ability. Recently, various pretext tasks (e.g.\nrotation prediction and clustering) have been proposed for self-supervised\nlearning in novelty detection. However, the learned latent features are still\nlow discriminative. We overcome such problems by introducing a novel\ndecoder-encoder framework. Firstly, a generative network (a.k.a. decoder)\nlearns the representation by mapping the initialized latent vector to an image.\nIn particular, this vector is initialized by considering the entire\ndistribution of training data to avoid the problem of mode-dropping. Secondly,\na contrastive network (a.k.a. encoder) aims to ``learn to compare'' through\nmutual information estimation, which directly helps the generative network to\nobtain a more discriminative representation by using a negative data\naugmentation strategy. Extensive experiments show that our model has\nsignificant superiority over cutting-edge novelty detectors and achieves new\nstate-of-the-art results on some novelty detection benchmarks, e.g. CIFAR10 and\nDCASE. Moreover, our model is more stable for training in a non-adversarial\nmanner, compared to other adversarial based novelty detection methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 07:26:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chen", "Chengwei", ""], ["Xie", "Yuan", ""], ["Lin", "Shaohui", ""], ["Qiao", "Ruizhi", ""], ["Zhou", "Jian", ""], ["Tan", "Xin", ""], ["Zhang", "Yi", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2106.09964", "submitter": "Baoming Yan", "authors": "Baoming Yan, Lin Wang, Ke Gao, Bo Gao, Xiao Liu, Chao Ban, Jiang Yang,\n  Xiaobo Li", "title": "Multi-Granularity Network with Modal Attention for Dense Affective\n  Understanding", "comments": "Oral presentation at AUVi Workshop - CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video affective understanding, which aims to predict the evoked expressions\nby the video content, is desired for video creation and recommendation. In the\nrecent EEV challenge, a dense affective understanding task is proposed and\nrequires frame-level affective prediction. In this paper, we propose a\nmulti-granularity network with modal attention (MGN-MA), which employs\nmulti-granularity features for better description of the target frame.\nSpecifically, the multi-granularity features could be divided into frame-level,\nclips-level and video-level features, which corresponds to visual-salient\ncontent, semantic-context and video theme information. Then the modal attention\nfusion module is designed to fuse the multi-granularity features and emphasize\nmore affection-relevant modals. Finally, the fused feature is fed into a\nMixtures Of Experts (MOE) classifier to predict the expressions. Further\nemploying model-ensemble post-processing, the proposed method achieves the\ncorrelation score of 0.02292 in the EEV challenge.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 07:37:06 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yan", "Baoming", ""], ["Wang", "Lin", ""], ["Gao", "Ke", ""], ["Gao", "Bo", ""], ["Liu", "Xiao", ""], ["Ban", "Chao", ""], ["Yang", "Jiang", ""], ["Li", "Xiaobo", ""]]}, {"id": "2106.09965", "submitter": "Xu Chen", "authors": "Yuhan Wang, Xu Chen, Junwei Zhu, Wenqing Chu, Ying Tai, Chengjie Wang,\n  Jilin Li, Yongjian Wu, Feiyue Huang and Rongrong Ji", "title": "HifiFace: 3D Shape and Semantic Prior Guided High Fidelity Face Swapping", "comments": "Accepted to IJCAI 2021, project website: https://johann.wang/HifiFace", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a high fidelity face swapping method, called\nHifiFace, which can well preserve the face shape of the source face and\ngenerate photo-realistic results. Unlike other existing face swapping works\nthat only use face recognition model to keep the identity similarity, we\npropose 3D shape-aware identity to control the face shape with the geometric\nsupervision from 3DMM and 3D face reconstruction method. Meanwhile, we\nintroduce the Semantic Facial Fusion module to optimize the combination of\nencoder and decoder features and make adaptive blending, which makes the\nresults more photo-realistic. Extensive experiments on faces in the wild\ndemonstrate that our method can preserve better identity, especially on the\nface shape, and can generate more photo-realistic results than previous\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 07:39:09 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wang", "Yuhan", ""], ["Chen", "Xu", ""], ["Zhu", "Junwei", ""], ["Chu", "Wenqing", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""]]}, {"id": "2106.09982", "submitter": "Chen Li", "authors": "Chen Li, Jinzhe Jiang, Xin Zhang, Tonghuan Zhang, Yaqian Zhao,\n  Dongdong Jiang and RenGang Li", "title": "Towards interpreting computer vision based on transformation invariant\n  optimization", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpreting how does deep neural networks (DNNs) make predictions is a vital\nfield in artificial intelligence, which hinders wide applications of DNNs.\nVisualization of learned representations helps we humans understand the vision\nof DNNs. In this work, visualized images that can activate the neural network\nto the target classes are generated by back-propagation method. Here, rotation\nand scaling operations are applied to introduce the transformation invariance\nin the image generating process, which we find a significant improvement on\nvisualization effect. Finally, we show some cases that such method can help us\nto gain insight into neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:04:10 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Li", "Chen", ""], ["Jiang", "Jinzhe", ""], ["Zhang", "Xin", ""], ["Zhang", "Tonghuan", ""], ["Zhao", "Yaqian", ""], ["Jiang", "Dongdong", ""], ["Li", "RenGang", ""]]}, {"id": "2106.09987", "submitter": "Daniil Tropin", "authors": "D.V. Tropin, A.M. Ershov, D.P. Nikolaev and V.V. Arlazarov", "title": "Advanced Hough-based method for on-device document localization", "comments": "This is a preprint of the article submitted for publication in the\n  journal \"Computer Optics\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for on-device document recognition systems increases in\nconjunction with the emergence of more strict privacy and security\nrequirements. In such systems, there is no data transfer from the end device to\na third-party information processing servers. The response time is vital to the\nuser experience of on-device document recognition. Combined with the\nunavailability of discrete GPUs, powerful CPUs, or a large RAM capacity on\nconsumer-grade end devices such as smartphones, the time limitations put\nsignificant constraints on the computational complexity of the applied\nalgorithms for on-device execution.\n  In this work, we consider document location in an image without prior\nknowledge of the document content or its internal structure. In accordance with\nthe published works, at least 5 systems offer solutions for on-device document\nlocation. All these systems use a location method which can be considered\nHough-based. The precision of such systems seems to be lower than that of the\nstate-of-the-art solutions which were not designed to account for the limited\ncomputational resources.\n  We propose an advanced Hough-based method. In contrast with other approaches,\nit accounts for the geometric invariants of the central projection model and\ncombines both edge and color features for document boundary detection. The\nproposed method allowed for the second best result for SmartDoc dataset in\nterms of precision, surpassed by U-net like neural network. When evaluated on a\nmore challenging MIDV-500 dataset, the proposed algorithm guaranteed the best\nprecision compared to published methods. Our method retained the applicability\nto on-device computations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:17:45 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Tropin", "D. V.", ""], ["Ershov", "A. M.", ""], ["Nikolaev", "D. P.", ""], ["Arlazarov", "V. V.", ""]]}, {"id": "2106.09993", "submitter": "Tianyu Pang", "authors": "Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, Jun Zhu", "title": "Accumulative Poisoning Attacks on Real-time Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting training data from untrusted sources exposes machine learning\nservices to poisoning adversaries, who maliciously manipulate training data to\ndegrade the model accuracy. When trained on offline datasets, poisoning\nadversaries have to inject the poisoned data in advance before training, and\nthe order of feeding these poisoned batches into the model is stochastic. In\ncontrast, practical systems are more usually trained/fine-tuned on sequentially\ncaptured real-time data, in which case poisoning adversaries could dynamically\npoison each data batch according to the current model state. In this paper, we\nfocus on the real-time settings and propose a new attacking strategy, which\naffiliates an accumulative phase with poisoning attacks to secretly (i.e.,\nwithout affecting accuracy) magnify the destructive effect of a (poisoned)\ntrigger batch. By mimicking online learning and federated learning on CIFAR-10,\nwe show that the model accuracy will significantly drop by a single update step\non the trigger batch after the accumulative phase. Our work validates that a\nwell-designed but straightforward attacking strategy can dramatically amplify\nthe poisoning effects, with no need to explore complex techniques.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:29:53 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Pang", "Tianyu", ""], ["Yang", "Xiao", ""], ["Dong", "Yinpeng", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""]]}, {"id": "2106.09996", "submitter": "Sungwon Hwang", "authors": "Sungwon Hwang, Hyungtae Lim and Hyun Myung", "title": "Equivariance-bridged SO(2)-Invariant Representation Learning using Graph\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training a Convolutional Neural Network (CNN) to be robust against rotation\nhas mostly been done with data augmentation. In this paper, another progressive\nvision of research direction is highlighted to encourage less dependence on\ndata augmentation by achieving structural rotational invariance of a network.\nThe deep equivariance-bridged SO(2) invariant network is proposed to echo such\nvision. First, Self-Weighted Nearest Neighbors Graph Convolutional Network\n(SWN-GCN) is proposed to implement Graph Convolutional Network (GCN) on the\ngraph representation of an image to acquire rotationally equivariant\nrepresentation, as GCN is more suitable for constructing deeper network than\nspectral graph convolution-based approaches. Then, invariant representation is\neventually obtained with Global Average Pooling (GAP), a permutation-invariant\noperation suitable for aggregating high-dimensional representations, over the\nequivariant set of vertices retrieved from SWN-GCN. Our method achieves the\nstate-of-the-art image classification performance on rotated MNIST and CIFAR-10\nimages, where the models are trained with a non-augmented dataset only.\nQuantitative validations over invariance of the representations also\ndemonstrate strong invariance of deep representations of SWN-GCN over\nrotations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:37:45 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Hwang", "Sungwon", ""], ["Lim", "Hyungtae", ""], ["Myung", "Hyun", ""]]}, {"id": "2106.10000", "submitter": "Huan Yin", "authors": "Huan Yin, Yue Wang and Rong Xiong", "title": "Improved Radar Localization on Lidar Maps Using Shared Embedding", "comments": "Extended abstract. Spotlight Talk at Radar Perception for All-Weather\n  Autonomy Workshop of ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a heterogeneous localization framework for solving radar global\nlocalization and pose tracking on pre-built lidar maps. To bridge the gap of\nsensing modalities, deep neural networks are constructed to create shared\nembedding space for radar scans and lidar maps. Herein learned feature\nembeddings are supportive for similarity measurement, thus improving map\nretrieval and data matching respectively. In RobotCar and MulRan datasets, we\ndemonstrate the effectiveness of the proposed framework with the comparison to\nScan Context and RaLL. In addition, the proposed pose tracking pipeline is with\nless neural networks compared to the original RaLL.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:40:04 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yin", "Huan", ""], ["Wang", "Yue", ""], ["Xiong", "Rong", ""]]}, {"id": "2106.10013", "submitter": "Aqi Gao", "authors": "Aqi Gao, Jiale Cao, Yanwei Pang", "title": "Shape Prior Non-Uniform Sampling Guided Real-time Stereo 3D Object\n  Detection", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-LiDAR based 3D object detectors have gained popularity due to their\nhigh accuracy. However, these methods need dense depth supervision and suffer\nfrom inferior speed. To solve these two issues, a recently introduced RTS3D\nbuilds an efficient 4D Feature-Consistency Embedding (FCE) space for the\nintermediate representation of object without depth supervision. FCE space\nsplits the entire object region into 3D uniform grid latent space for feature\nsampling point generation, which ignores the importance of different object\nregions. However, we argue that, compared with the inner region, the outer\nregion plays a more important role for accurate 3D detection. To encode more\ninformation from the outer region, we propose a shape prior non-uniform\nsampling strategy that performs dense sampling in outer region and sparse\nsampling in inner region. As a result, more points are sampled from the outer\nregion and more useful features are extracted for 3D detection. Further, to\nenhance the feature discrimination of each sampling point, we propose a\nhigh-level semantic enhanced FCE module to exploit more contextual information\nand suppress noise better. Experiments on the KITTI dataset are performed to\nshow the effectiveness of the proposed method. Compared with the baseline\nRTS3D, our proposed method has 2.57% improvement on AP3d almost without extra\nnetwork parameters. Moreover, our proposed method outperforms the\nstate-of-the-art methods without extra supervision at a real-time speed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 09:14:55 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 01:55:56 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 03:35:10 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gao", "Aqi", ""], ["Cao", "Jiale", ""], ["Pang", "Yanwei", ""]]}, {"id": "2106.10026", "submitter": "Yifei Huang", "authors": "Lijin Yang, Yifei Huang, Yusuke Sugano, Yoichi Sato", "title": "EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action\n  Recognition 2021: Team M3EM Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we describe the technical details of our submission to the\n2021 EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action\nRecognition. Leveraging multiple modalities has been proved to benefit the\nUnsupervised Domain Adaptation (UDA) task. In this work, we present Multi-Modal\nMutual Enhancement Module (M3EM), a deep module for jointly considering\ninformation from multiple modalities to find the most transferable\nrepresentations across domains. We achieve this by implementing two sub-modules\nfor enhancing each modality using the context of other modalities. The first\nsub-module exchanges information across modalities through the semantic space,\nwhile the second sub-module finds the most transferable spatial region based on\nthe consensus of all modalities.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:03:30 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 12:40:26 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 02:56:46 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yang", "Lijin", ""], ["Huang", "Yifei", ""], ["Sugano", "Yusuke", ""], ["Sato", "Yoichi", ""]]}, {"id": "2106.10031", "submitter": "Jiabao Lei", "authors": "Jiabao Lei, Kui Jia, Yi Ma", "title": "Learning and Meshing from Deep Implicit Surface Networks Using an\n  Efficient Implementation of Analytic Marching", "comments": "arXiv admin note: text overlap with arXiv:2002.06597", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of object or scene surfaces has tremendous applications in\ncomputer vision, computer graphics, and robotics. In this paper, we study a\nfundamental problem in this context about recovering a surface mesh from an\nimplicit field function whose zero-level set captures the underlying surface.\nTo achieve the goal, existing methods rely on traditional meshing algorithms;\nwhile promising, they suffer from loss of precision learned in the implicit\nsurface networks, due to the use of discrete space sampling in marching cubes.\nGiven that an MLP with activations of Rectified Linear Unit (ReLU) partitions\nits input space into a number of linear regions, we are motivated to connect\nthis local linearity with a same property owned by the desired result of\npolygon mesh. More specifically, we identify from the linear regions,\npartitioned by an MLP based implicit function, the analytic cells and analytic\nfaces that are associated with the function's zero-level isosurface. We prove\nthat under mild conditions, the identified analytic faces are guaranteed to\nconnect and form a closed, piecewise planar surface. Based on the theorem, we\npropose an algorithm of analytic marching, which marches among analytic cells\nto exactly recover the mesh captured by an implicit surface network. We also\nshow that our theory and algorithm are equally applicable to advanced MLPs with\nshortcut connections and max pooling. Given the parallel nature of analytic\nmarching, we contribute AnalyticMesh, a software package that supports\nefficient meshing of implicit surface networks via CUDA parallel computing, and\nmesh simplification for efficient downstream processing. We apply our method to\ndifferent settings of generative shape modeling using implicit surface\nnetworks. Extensive experiments demonstrate our advantages over existing\nmethods in terms of both meshing accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:06:28 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Lei", "Jiabao", ""], ["Jia", "Kui", ""], ["Ma", "Yi", ""]]}, {"id": "2106.10044", "submitter": "Kanchana Vaishnavi Gandikota", "authors": "Kanchana Vaishnavi Gandikota, Jonas Geiping, Zorah L\\\"ahner, Adam\n  Czapli\\'nski, Michael Moeller", "title": "Training or Architecture? How to Incorporate Invariance in Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require the robustness, or ideally the invariance, of a\nneural network to certain transformations of input data. Most commonly, this\nrequirement is addressed by either augmenting the training data, using\nadversarial training, or defining network architectures that include the\ndesired invariance automatically. Unfortunately, the latter often relies on the\nability to enlist all possible transformations, which make such approaches\nlargely infeasible for infinite sets of transformations, such as arbitrary\nrotations or scaling. In this work, we propose a method for provably invariant\nnetwork architectures with respect to group actions by choosing one element\nfrom a (possibly continuous) orbit based on a fixed criterion. In a nutshell,\nwe intend to 'undo' any possible transformation before feeding the data into\nthe actual network. We analyze properties of such approaches, extend them to\nequivariant networks, and demonstrate their advantages in terms of robustness\nas well as computational efficiency in several numerical examples. In\nparticular, we investigate the robustness with respect to rotations of images\n(which can possibly hold up to discretization artifacts only) as well as the\nprovable rotational and scaling invariance of 3D point cloud classification.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:31:00 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Gandikota", "Kanchana Vaishnavi", ""], ["Geiping", "Jonas", ""], ["L\u00e4hner", "Zorah", ""], ["Czapli\u0144ski", "Adam", ""], ["Moeller", "Michael", ""]]}, {"id": "2106.10046", "submitter": "Chang Liu", "authors": "Chang Liu, Xiaolin Wu", "title": "Light Pollution Reduction in Nighttime Photography", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nighttime photographers are often troubled by light pollution of unwanted\nartificial lights. Artificial lights, after scattered by aerosols in the\natmosphere, can inundate the starlight and degrade the quality of nighttime\nimages, by reducing contrast and dynamic range and causing hazes. In this paper\nwe develop a physically-based light pollution reduction (LPR) algorithm that\ncan substantially alleviate the aforementioned degradations of perceptual\nquality and restore the pristine state of night sky. The key to the success of\nthe proposed LPR algorithm is an inverse method to estimate the spatial\nradiance distribution and spectral signature of ground artificial lights.\nExtensive experiments are carried out to evaluate the efficacy and limitations\nof the LPR algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:38:13 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Liu", "Chang", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2106.10060", "submitter": "Chintan Trivedi", "authors": "Chintan Trivedi, Antonios Liapis and Georgios N. Yannakakis", "title": "Contrastive Learning of Generalized Game Representations", "comments": "8 pages, 7 figures, CoG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing games through their pixels offers a promising approach for\nbuilding general-purpose and versatile game models. While games are not merely\nimages, neural network models trained on game pixels often capture differences\nof the visual style of the image rather than the content of the game. As a\nresult, such models cannot generalize well even within similar games of the\nsame genre. In this paper we build on recent advances in contrastive learning\nand showcase its benefits for representation learning in games. Learning to\ncontrast images of games not only classifies games in a more efficient manner;\nit also yields models that separate games in a more meaningful fashion by\nignoring the visual style and focusing, instead, on their content. Our results\nin a large dataset of sports video games containing 100k images across 175\ngames and 10 game genres suggest that contrastive learning is better suited for\nlearning generalized game representations compared to conventional supervised\nlearning. The findings of this study bring us closer to universal visual\nencoders for games that can be reused across previously unseen games without\nrequiring retraining or fine-tuning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 11:17:54 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Trivedi", "Chintan", ""], ["Liapis", "Antonios", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "2106.10070", "submitter": "Nanqing Dong", "authors": "Nanqing Dong, Matteo Maggioni, Yongxin Yang, Eduardo\n  P\\'erez-Pellitero, Ales Leonardis, Steven McDonagh", "title": "Residual Contrastive Learning for Joint Demosaicking and Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The breakthrough of contrastive learning (CL) has fueled the recent success\nof self-supervised learning (SSL) in high-level vision tasks on RGB images.\nHowever, CL is still ill-defined for low-level vision tasks, such as joint\ndemosaicking and denoising (JDD), in the RAW domain. To bridge this\nmethodological gap, we present a novel CL approach on RAW images, residual\ncontrastive learning (RCL), which aims to learn meaningful representations for\nJDD. Our work is built on the assumption that noise contained in each RAW image\nis signal-dependent, thus two crops from the same RAW image should have more\nsimilar noise distribution than two crops from different RAW images. We use\nresiduals as a discriminative feature and the earth mover's distance to measure\nthe distribution divergence for the contrastive loss. To evaluate the proposed\nCL strategy, we simulate a series of unsupervised JDD experiments with\nlarge-scale data corrupted by synthetic signal-dependent noise, where we set a\nnew benchmark for unsupervised JDD tasks with unknown (random) noise variance.\nOur empirical study not only validates that CL can be applied on distributions\n(c.f. features), but also exposes the lack of robustness of previous non-ML and\nSSL JDD methods when the statistics of the noise are unknown, thus providing\nsome further insight into signal-dependent noise problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 11:37:05 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Dong", "Nanqing", ""], ["Maggioni", "Matteo", ""], ["Yang", "Yongxin", ""], ["P\u00e9rez-Pellitero", "Eduardo", ""], ["Leonardis", "Ales", ""], ["McDonagh", "Steven", ""]]}, {"id": "2106.10077", "submitter": "Indrajit Kurmi", "authors": "Indrajit Kurmi, David C. Schedl, and Oliver Bimber", "title": "Combined Person Classification with Airborne Optical Sectioning", "comments": "9 Pages, 7 Figures, 1 Table. This work has been submitted to the IEEE\n  for possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fully autonomous drones have been demonstrated to find lost or injured\npersons under strongly occluding forest canopy. Airborne Optical Sectioning\n(AOS), a novel synthetic aperture imaging technique, together with\ndeep-learning-based classification enables high detection rates under realistic\nsearch-and-rescue conditions. We demonstrate that false detections can be\nsignificantly suppressed and true detections boosted by combining\nclassifications from multiple AOS rather than single integral images. This\nimproves classification rates especially in the presence of occlusion. To make\nthis possible, we modified the AOS imaging process to support large overlaps\nbetween subsequent integrals, enabling real-time and on-board scanning and\nprocessing of groundspeeds up to 10 m/s.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 11:56:17 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Kurmi", "Indrajit", ""], ["Schedl", "David C.", ""], ["Bimber", "Oliver", ""]]}, {"id": "2106.10080", "submitter": "Peibei Cao", "authors": "Cao Peibei, Wang Zhangyang, Ma Kede", "title": "Debiased Subjective Assessment of Real-World Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In real-world image enhancement, it is often challenging (if not impossible)\nto acquire ground-truth data, preventing the adoption of distance metrics for\nobjective quality assessment. As a result, one often resorts to subjective\nquality assessment, the most straightforward and reliable means of evaluating\nimage enhancement. Conventional subjective testing requires manually\npre-selecting a small set of visual examples, which may suffer from three\nsources of biases: 1) sampling bias due to the extremely sparse distribution of\nthe selected samples in the image space; 2) algorithmic bias due to potential\noverfitting the selected samples; 3) subjective bias due to further potential\ncherry-picking test results. This eventually makes the field of real-world\nimage enhancement more of an art than a science. Here we take steps towards\ndebiasing conventional subjective assessment by automatically sampling a set of\nadaptive and diverse images for subsequent testing. This is achieved by casting\nsample selection into a joint maximization of the discrepancy between the\nenhancers and the diversity among the selected input images. Careful visual\ninspection on the resulting enhanced images provides a debiased ranking of the\nenhancement algorithms. We demonstrate our subjective assessment method using\nthree popular and practically demanding image enhancement tasks: dehazing,\nsuper-resolution, and low-light enhancement.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 12:03:35 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 03:42:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Peibei", "Cao", ""], ["Zhangyang", "Wang", ""], ["Kede", "Ma", ""]]}, {"id": "2106.10090", "submitter": "Ayush K. Rai", "authors": "Ayush K Rai, Tarun Krishna, Julia Dietlmeier, Kevin McGuinness, Alan F\n  Smeaton, Noel E O'Connor", "title": "Discerning Generic Event Boundaries in Long-Form Wild Videos", "comments": "Technical Report for Generic Event Boundary Challenge - LOVEU\n  Challenge (CVPR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting generic, taxonomy-free event boundaries invideos represents a major\nstride forward towards holisticvideo understanding. In this paper we present a\ntechnique forgeneric event boundary detection based on a two stream in-flated\n3D convolutions architecture, which can learn spatio-temporal features from\nvideos. Our work is inspired from theGeneric Event Boundary Detection Challenge\n(part of CVPR2021 Long Form Video Understanding- LOVEU Workshop).Throughout the\npaper we provide an in-depth analysis ofthe experiments performed along with an\ninterpretation ofthe results obtained.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 12:28:19 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Rai", "Ayush K", ""], ["Krishna", "Tarun", ""], ["Dietlmeier", "Julia", ""], ["McGuinness", "Kevin", ""], ["Smeaton", "Alan F", ""], ["O'Connor", "Noel E", ""]]}, {"id": "2106.10102", "submitter": "Ci Li", "authors": "Ci Li, Nima Ghorbani, Sofia Broom\\'e, Maheen Rashid, Michael J. Black,\n  Elin Hernlund, Hedvig Kjellstr\\\"om, Silvia Zuffi", "title": "hSMAL: Detailed Horse Shape and Pose Reconstruction for Motion Pattern\n  Recognition", "comments": "CV4Animals Workshop in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our preliminary work on model-based behavioral\nanalysis of horse motion. Our approach is based on the SMAL model, a 3D\narticulated statistical model of animal shape. We define a novel SMAL model for\nhorses based on a new template, skeleton and shape space learned from $37$\nhorse toys. We test the accuracy of our hSMAL model in reconstructing a horse\nfrom 3D mocap data and images. We apply the hSMAL model to the problem of\nlameness detection from video, where we fit the model to images to recover 3D\npose and train an ST-GCN network on pose data. A comparison with the same\nnetwork trained on mocap points illustrates the benefit of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 12:53:40 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Li", "Ci", ""], ["Ghorbani", "Nima", ""], ["Broom\u00e9", "Sofia", ""], ["Rashid", "Maheen", ""], ["Black", "Michael J.", ""], ["Hernlund", "Elin", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Zuffi", "Silvia", ""]]}, {"id": "2106.10110", "submitter": "Fangwei Zhong", "authors": "Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang", "title": "Towards Distraction-Robust Active Visual Tracking", "comments": "To appear in ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In active visual tracking, it is notoriously difficult when distracting\nobjects appear, as distractors often mislead the tracker by occluding the\ntarget or bringing a confusing appearance. To address this issue, we propose a\nmixed cooperative-competitive multi-agent game, where a target and multiple\ndistractors form a collaborative team to play against a tracker and make it\nfail to follow. Through learning in our game, diverse distracting behaviors of\nthe distractors naturally emerge, thereby exposing the tracker's weakness,\nwhich helps enhance the distraction-robustness of the tracker. For effective\nlearning, we then present a bunch of practical methods, including a reward\nfunction for distractors, a cross-modal teacher-student learning strategy, and\na recurrent attention mechanism for the tracker. The experimental results show\nthat our tracker performs desired distraction-robust active visual tracking and\ncan be well generalized to unseen environments. We also show that the\nmulti-agent game can be used to adversarially test the robustness of trackers.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:05:25 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zhong", "Fangwei", ""], ["Sun", "Peng", ""], ["Luo", "Wenhan", ""], ["Yan", "Tingyun", ""], ["Wang", "Yizhou", ""]]}, {"id": "2106.10118", "submitter": "Alireza Ahmadi", "authors": "Alireza Ahmadi, Michael Halstead, and Chris McCool", "title": "Virtual Temporal Samples for Recurrent Neural Networks: applied to\n  semantic segmentation in agriculture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the potential for performing temporal semantic\nsegmentation in the context of agricultural robotics without temporally\nlabelled data. We achieve this by proposing to generate virtual temporal\nsamples from labelled still images. This allows us, with no extra annotation\neffort, to generate virtually labelled temporal sequences. Normally, to train a\nrecurrent neural network (RNN), labelled samples from a video (temporal)\nsequence are required which is laborious and has stymied work in this\ndirection. By generating virtual temporal samples, we demonstrate that it is\npossible to train a lightweight RNN to perform semantic segmentation on two\nchallenging agricultural datasets. Our results show that by training a temporal\nsemantic segmenter using virtual samples we can increase the performance by an\nabsolute amount of 4.6 and 4.9 on sweet pepper and sugar beet datasets,\nrespectively. This indicates that our virtual data augmentation technique is\nable to accurately classify agricultural images temporally without the use of\ncomplicated synthetic data generation techniques nor with the overhead of\nlabelling large amounts of temporal sequences.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:15:54 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ahmadi", "Alireza", ""], ["Halstead", "Michael", ""], ["McCool", "Chris", ""]]}, {"id": "2106.10137", "submitter": "Martine Toering", "authors": "Martine Toering, Ioannis Gatopoulos, Maarten Stol, Vincent Tao Hu", "title": "Self-supervised Video Representation Learning with Cross-Stream\n  Prototypical Contrasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance-level contrastive learning techniques, which rely on data\naugmentation and a contrastive loss function, have found great success in the\ndomain of visual representation learning. They are not suitable for exploiting\nthe rich dynamical structure of video however, as operations are done on many\naugmented instances. In this paper we propose \"Video Cross-Stream Prototypical\nContrasting\", a novel method which predicts consistent prototype assignments\nfrom both RGB and optical flow views, operating on sets of samples.\nSpecifically, we alternate the optimization process; while optimizing one of\nthe streams, all views are mapped to one set of stream prototype vectors. Each\nof the assignments is predicted with all views except the one matching the\nprediction, pushing representations closer to their assigned prototypes. As a\nresult, more efficient video embeddings with ingrained motion information are\nlearned, without the explicit need for optical flow computation during\ninference. We obtain state-of-the-art results on nearest neighbour video\nretrieval and action recognition, outperforming previous best by +3.2% on\nUCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and\n+15.1% on HMDB51 using the R(2+1)D backbone.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:57:51 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 09:41:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Toering", "Martine", ""], ["Gatopoulos", "Ioannis", ""], ["Stol", "Maarten", ""], ["Hu", "Vincent Tao", ""]]}, {"id": "2106.10153", "submitter": "Carmelo Scribano", "authors": "Carmelo Scribano, Davide Sapienza, Giorgia Franchini, Micaela Verucchi\n  and Marko Bertogna", "title": "All You Can Embed: Natural Language based Vehicle Retrieval with\n  Spatio-Temporal Transformers", "comments": "CVPR 2021 AI CITY CHALLENGE Natural Language-Based Vehicle Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining Natural Language with Vision represents a unique and interesting\nchallenge in the domain of Artificial Intelligence. The AI City Challenge Track\n5 for Natural Language-Based Vehicle Retrieval focuses on the problem of\ncombining visual and textual information, applied to a smart-city use case. In\nthis paper, we present All You Can Embed (AYCE), a modular solution to\ncorrelate single-vehicle tracking sequences with natural language. The main\nbuilding blocks of the proposed architecture are (i) BERT to provide an\nembedding of the textual descriptions, (ii) a convolutional backbone along with\na Transformer model to embed the visual information. For the training of the\nretrieval model, a variation of the Triplet Margin Loss is proposed to learn a\ndistance measure between the visual and language embeddings. The code is\npublicly available at https://github.com/cscribano/AYCE_2021.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:38:51 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Scribano", "Carmelo", ""], ["Sapienza", "Davide", ""], ["Franchini", "Giorgia", ""], ["Verucchi", "Micaela", ""], ["Bertogna", "Marko", ""]]}, {"id": "2106.10155", "submitter": "Frederik Schubert", "authors": "Maren Awiszus, Frederik Schubert, Bodo Rosenhahn", "title": "World-GAN: a Generative Model for Minecraft Worlds", "comments": "8 pages, 8 figures, IEEE Conference on Games (CoG) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces World-GAN, the first method to perform data-driven\nProcedural Content Generation via Machine Learning in Minecraft from a single\nexample. Based on a 3D Generative Adversarial Network (GAN) architecture, we\nare able to create arbitrarily sized world snippets from a given sample. We\nevaluate our approach on creations from the community as well as structures\ngenerated with the Minecraft World Generator. Our method is motivated by the\ndense representations used in Natural Language Processing (NLP) introduced with\nword2vec [1]. The proposed block2vec representations make World-GAN independent\nfrom the number of different blocks, which can vary a lot in Minecraft, and\nenable the generation of larger levels. Finally, we demonstrate that changing\nthis new representation space allows us to change the generated style of an\nalready trained generator. World-GAN enables its users to generate Minecraft\nworlds based on parts of their creations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:45:39 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Awiszus", "Maren", ""], ["Schubert", "Frederik", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2106.10160", "submitter": "Jibinraj Antony", "authors": "Jibinraj Antony, Dr. Florian Schlather, Georgij Safronov, Markus\n  Schmitz, Prof. Dr. Kristof Van Laerhoven", "title": "Toward Fault Detection in Industrial Welding Processes with Deep\n  Learning and Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the rise of deep learning models in the field of computer vision, new\npossibilities for their application in industrial processes proves to return\ngreat benefits. Nevertheless, the actual fit of machine learning for highly\nstandardised industrial processes is still under debate. This paper addresses\nthe challenges on the industrial realization of the AI tools, considering the\nuse case of Laser Beam Welding quality control as an example. We use object\ndetection algorithms from the TensorFlow object detection API and adapt them to\nour use case using transfer learning. The baseline models we develop are used\nas benchmarks and evaluated and compared to models that undergo dataset scaling\nand hyperparameter tuning. We find that moderate scaling of the dataset via\nimage augmentation leads to improvements in intersection over union (IoU) and\nrecall, whereas high levels of augmentation and scaling may lead to\ndeterioration of results. Finally, we put our results into perspective of the\nunderlying use case and evaluate their fit.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:52:49 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Antony", "Jibinraj", ""], ["Schlather", "Dr. Florian", ""], ["Safronov", "Georgij", ""], ["Schmitz", "Markus", ""], ["Van Laerhoven", "Prof. Dr. Kristof", ""]]}, {"id": "2106.10163", "submitter": "Erik Jenner", "authors": "Erik Jenner, Maurice Weiler", "title": "Steerable Partial Differential Operators for Equivariant Neural Networks", "comments": "43 pages, 4 figures, code available at\n  https://github.com/ejnnr/steerable_pdos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in equivariant deep learning bears strong similarities to\nphysics. Fields over a base space are fundamental entities in both subjects, as\nare equivariant maps between these fields. In deep learning, however, these\nmaps are usually defined by convolutions with a kernel, whereas they are\npartial differential operators (PDOs) in physics. Developing the theory of\nequivariant PDOs in the context of deep learning could bring these subjects\neven closer together and lead to a stronger flow of ideas. In this work, we\nderive a $G$-steerability constraint that completely characterizes when a PDO\nbetween feature vector fields is equivariant, for arbitrary symmetry groups\n$G$. We then fully solve this constraint for several important groups. We use\nour solutions as equivariant drop-in replacements for convolutional layers and\nbenchmark them in that role. Finally, we develop a framework for equivariant\nmaps based on Schwartz distributions that unifies classical convolutions and\ndifferential operators and gives insight about the relation between the two.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:58:19 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Jenner", "Erik", ""], ["Weiler", "Maurice", ""]]}, {"id": "2106.10195", "submitter": "Tobias Uelwer", "authors": "Tobias Uelwer and Tobias Hoffmann and Stefan Harmeling", "title": "Non-Iterative Phase Retrieval With Cascaded Neural Networks", "comments": "Accepted at the 30th International Conference on Artificial Neural\n  Networks (ICANN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier phase retrieval is the problem of reconstructing a signal given only\nthe magnitude of its Fourier transformation. Optimization-based approaches,\nlike the well-established Gerchberg-Saxton or the hybrid input output\nalgorithm, struggle at reconstructing images from magnitudes that are not\noversampled. This motivates the application of learned methods, which allow\nreconstruction from non-oversampled magnitude measurements after a learning\nphase. In this paper, we want to push the limits of these learned methods by\nmeans of a deep neural network cascade that reconstructs the image successively\non different resolutions from its non-oversampled Fourier magnitude. We\nevaluate our method on four different datasets (MNIST, EMNIST, Fashion-MNIST,\nand KMNIST) and demonstrate that it yields improved performance over other\nnon-iterative methods and optimization-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 15:52:12 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Uelwer", "Tobias", ""], ["Hoffmann", "Tobias", ""], ["Harmeling", "Stefan", ""]]}, {"id": "2106.10197", "submitter": "Muhammad Monjurul Karim", "authors": "Muhammad Monjurul Karim, Yu Li, Ruwen Qin, Zhaozheng Yin", "title": "A Dynamic Spatial-temporal Attention Network for Early Anticipation of\n  Traffic Accidents", "comments": "10 pages, 4 figures, submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, autonomous vehicles and those equipped with an Advanced Driver\nAssistance System (ADAS) are emerging. They share the road with regular ones\noperated by human drivers entirely. To ensure guaranteed safety for passengers\nand other road users, it becomes essential for autonomous vehicles and ADAS to\nanticipate traffic accidents from natural driving scenes. The dynamic\nspatial-temporal interaction of the traffic agents is complex, and visual cues\nfor predicting a future accident are embedded deeply in dashcam video data.\nTherefore, early anticipation of traffic accidents remains a challenge. To this\nend, the paper presents a dynamic spatial-temporal attention (DSTA) network for\nearly anticipation of traffic accidents from dashcam videos. The proposed\nDSTA-network learns to select discriminative temporal segments of a video\nsequence with a module named Dynamic Temporal Attention (DTA). It also learns\nto focus on the informative spatial regions of frames with another module named\nDynamic Spatial Attention (DSA). The spatial-temporal relational features of\naccidents, along with scene appearance features, are learned jointly with a\nGated Recurrent Unit (GRU) network. The experimental evaluation of the\nDSTA-network on two benchmark datasets confirms that it has exceeded the\nstate-of-the-art performance. A thorough ablation study evaluates the\ncontributions of individual components of the DSTA-network, revealing how the\nnetwork achieves such performance. Furthermore, this paper proposes a new\nstrategy that fuses the prediction scores from two complementary models and\nverifies its effectiveness in further boosting the performance of early\naccident anticipation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 15:58:53 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Karim", "Muhammad Monjurul", ""], ["Li", "Yu", ""], ["Qin", "Ruwen", ""], ["Yin", "Zhaozheng", ""]]}, {"id": "2106.10212", "submitter": "Hossein Aboutalebi", "authors": "Hossein Aboutalebi, Mohammad Javad Shafiee, Michelle Karg, Christian\n  Scharfenberger, Alexander Wong", "title": "Residual Error: a New Performance Measure for Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant advances in deep learning over the past decade, a\nmajor challenge that limits the wide-spread adoption of deep learning has been\ntheir fragility to adversarial attacks. This sensitivity to making erroneous\npredictions in the presence of adversarially perturbed data makes deep neural\nnetworks difficult to adopt for certain real-world, mission-critical\napplications. While much of the research focus has revolved around adversarial\nexample creation and adversarial hardening, the area of performance measures\nfor assessing adversarial robustness is not well explored. Motivated by this,\nthis study presents the concept of residual error, a new performance measure\nfor not only assessing the adversarial robustness of a deep neural network at\nthe individual sample level, but also can be used to differentiate between\nadversarial and non-adversarial examples to facilitate for adversarial example\ndetection. Furthermore, we introduce a hybrid model for approximating the\nresidual error in a tractable manner. Experimental results using the case of\nimage classification demonstrates the effectiveness and efficacy of the\nproposed residual error metric for assessing several well-known deep neural\nnetwork architectures. These results thus illustrate that the proposed measure\ncould be a useful tool for not only assessing the robustness of deep neural\nnetworks used in mission-critical scenarios, but also in the design of\nadversarially robust models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 16:34:23 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Aboutalebi", "Hossein", ""], ["Shafiee", "Mohammad Javad", ""], ["Karg", "Michelle", ""], ["Scharfenberger", "Christian", ""], ["Wong", "Alexander", ""]]}, {"id": "2106.10213", "submitter": "Bin-Bin Gao", "authors": "Feng Luo, Bin-Bin Gao, Jiangpeng Yan, Xiu Li", "title": "A Coarse-to-Fine Instance Segmentation Network with Learning Boundary\n  Representation", "comments": "8 pages, Accepted by IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundary-based instance segmentation has drawn much attention since of its\nattractive efficiency. However, existing methods suffer from the difficulty in\nlong-distance regression. In this paper, we propose a coarse-to-fine module to\naddress the problem. Approximate boundary points are generated at the coarse\nstage and then features of these points are sampled and fed to a refined\nregressor for fine prediction. It is end-to-end trainable since differential\nsampling operation is well supported in the module. Furthermore, we design a\nholistic boundary-aware branch and introduce instance-agnostic supervision to\nassist regression. Equipped with ResNet-101, our approach achieves 31.7\\% mask\nAP on COCO dataset with single-scale training and testing, outperforming the\nbaseline 1.3\\% mask AP with less than 1\\% additional parameters and GFLOPs.\nExperiments also show that our proposed method achieves competitive performance\ncompared to existing boundary-based methods with a lightweight design and a\nsimple pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 16:37:28 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Luo", "Feng", ""], ["Gao", "Bin-Bin", ""], ["Yan", "Jiangpeng", ""], ["Li", "Xiu", ""]]}, {"id": "2106.10230", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra, Ankur Singh", "title": "CT Image Synthesis Using Weakly Supervised Segmentation and Geometric\n  Inter-Label Relations For COVID Image Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.14119;\n  text overlap with arXiv:1908.10555, arXiv:2004.14133 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While medical image segmentation is an important task for computer aided\ndiagnosis, the high expertise requirement for pixelwise manual annotations\nmakes it a challenging and time consuming task. Since conventional data\naugmentations do not fully represent the underlying distribution of the\ntraining set, the trained models have varying performance when tested on images\ncaptured from different sources. Most prior work on image synthesis for data\naugmentation ignore the interleaved geometric relationship between different\nanatomical labels. We propose improvements over previous GAN-based medical\nimage synthesis methods by learning the relationship between different\nanatomical labels. We use a weakly supervised segmentation method to obtain\npixel level semantic label map of images which is used learn the intrinsic\nrelationship of geometry and shape across semantic labels. Latent space\nvariable sampling results in diverse generated images from a base image and\nimproves robustness. We use the synthetic images from our method to train\nnetworks for segmenting COVID-19 infected areas from lung CT images. The\nproposed method outperforms state-of-the-art segmentation methods on a public\ndataset. Ablation studies also demonstrate benefits of integrating geometry and\ndiversity.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:21:24 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Mahapatra", "Dwarikanath", ""], ["Singh", "Ankur", ""]]}, {"id": "2106.10240", "submitter": "Maksym Ivashechkin", "authors": "Maksym Ivashechkin, Daniel Barath, Jiri Matas", "title": "VSAC: Efficient and Accurate Estimator for H and F", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present VSAC, a RANSAC-type robust estimator with a number of novelties.\nIt benefits from the introduction of the concept of independent inliers that\nimproves significantly the efficacy of the dominant plane handling and, also,\nallows near error-free rejection of incorrect models, without false positives.\nThe local optimization process and its application is improved so that it is\nrun on average only once. Further technical improvements include adaptive\nsequential hypothesis verification and efficient model estimation via Gaussian\nelimination. Experiments on four standard datasets show that VSAC is\nsignificantly faster than all its predecessors and runs on average in 1-2 ms,\non a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++,\nthe currently most accurate estimator of two-view geometry. In the repeated\nruns on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 17:04:57 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ivashechkin", "Maksym", ""], ["Barath", "Daniel", ""], ["Matas", "Jiri", ""]]}, {"id": "2106.10258", "submitter": "Marco Fornoni", "authors": "Marco Fornoni, Chaochao Yan, Liangchen Luo, Kimberly Wilber, Alex\n  Stark, Yin Cui, Boqing Gong, Andrew Howard", "title": "Bridging the Gap Between Object Detection and User Intent via\n  Query-Modulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When interacting with objects through cameras, or pictures, users often have\na specific intent. For example, they may want to perform a visual search.\nHowever, most object detection models ignore the user intent, relying on image\npixels as their only input. This often leads to incorrect results, such as lack\nof a high-confidence detection on the object of interest, or detection with a\nwrong class label. In this paper we investigate techniques to modulate standard\nobject detectors to explicitly account for the user intent, expressed as an\nembedding of a simple query. Compared to standard object detectors,\nquery-modulated detectors show superior performance at detecting objects for a\ngiven label of interest. Thanks to large-scale training data synthesized from\nstandard object detection annotations, query-modulated detectors can also\noutperform specialized referring expression recognition systems. Furthermore,\nthey can be simultaneously trained to solve for both query-modulated detection\nand standard object detection.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 17:47:53 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Fornoni", "Marco", ""], ["Yan", "Chaochao", ""], ["Luo", "Liangchen", ""], ["Wilber", "Kimberly", ""], ["Stark", "Alex", ""], ["Cui", "Yin", ""], ["Gong", "Boqing", ""], ["Howard", "Andrew", ""]]}, {"id": "2106.10270", "submitter": "Xiaohua Zhai", "authors": "Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman,\n  Jakob Uszkoreit, Lucas Beyer", "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision\n  Transformers", "comments": "Andreas, Alex, Xiaohua and Lucas contributed equally. We release more\n  than 50'000 ViT models trained under diverse settings on various datasets. We\n  believe this to be a treasure trove for model analysis. Available at\n  https://github.com/google-research/vision_transformer and\n  https://github.com/rwightman/pytorch-image-models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision Transformers (ViT) have been shown to attain highly competitive\nperformance for a wide range of vision applications, such as image\nclassification, object detection and semantic image segmentation. In comparison\nto convolutional neural networks, the Vision Transformer's weaker inductive\nbias is generally found to cause an increased reliance on model regularization\nor data augmentation (``AugReg'' for short) when training on smaller training\ndatasets. We conduct a systematic empirical study in order to better understand\nthe interplay between the amount of training data, AugReg, model size and\ncompute budget. As one result of this study we find that the combination of\nincreased compute and AugReg can yield models with the same performance as\nmodels trained on an order of magnitude more training data: we train ViT models\nof various sizes on the public ImageNet-21k dataset which either match or\noutperform their counterparts trained on the larger, but not publicly available\nJFT-300M dataset.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 17:58:20 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Steiner", "Andreas", ""], ["Kolesnikov", "Alexander", ""], ["Zhai", "Xiaohua", ""], ["Wightman", "Ross", ""], ["Uszkoreit", "Jakob", ""], ["Beyer", "Lucas", ""]]}, {"id": "2106.10271", "submitter": "Xiaolong Liu", "authors": "Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Song Bai, Xiang Bai", "title": "End-to-end Temporal Action Detection with Transformer", "comments": "Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal action detection (TAD) aims to determine the semantic label and the\nboundaries of every action instance in an untrimmed video. It is a fundamental\nand challenging task in video understanding and significant progress has been\nmade. Previous methods involve multiple stages or networks and hand-designed\nrules or operations, which fall short in efficiency and flexibility. In this\npaper, we propose an end-to-end framework for TAD upon Transformer, termed\n\\textit{TadTR}, which maps a set of learnable embeddings to action instances in\nparallel. TadTR is able to adaptively extract temporal context information\nrequired for making action predictions, by selectively attending to a sparse\nset of snippets in a video. As a result, it simplifies the pipeline of TAD and\nrequires lower computation cost than previous detectors, while preserving\nremarkable detection performance. TadTR achieves state-of-the-art performance\non HACS Segments (+3.35% average mAP). As a single-network detector, TadTR runs\n10$\\times$ faster than its comparable competitor. It outperforms existing\nsingle-network detectors by a large margin on THUMOS14 (+5.0% average mAP) and\nActivityNet (+7.53% average mAP). When combined with other detectors, it\nreports 54.1% mAP at IoU=0.5 on THUMOS14, and 34.55% average mAP on\nActivityNet-1.3. Our code will be released at\n\\url{https://github.com/xlliu7/TadTR}.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 17:58:34 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 14:54:58 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Liu", "Xiaolong", ""], ["Wang", "Qimeng", ""], ["Hu", "Yao", ""], ["Tang", "Xu", ""], ["Bai", "Song", ""], ["Bai", "Xiang", ""]]}, {"id": "2106.10309", "submitter": "Peri Akiva", "authors": "Peri Akiva and Kristin Dana", "title": "Towards Single Stage Weakly Supervised Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The costly process of obtaining semantic segmentation labels has driven\nresearch towards weakly supervised semantic segmentation (WSSS) methods, using\nonly image-level, point, or box labels. The lack of dense scene representation\nrequires methods to increase complexity to obtain additional semantic\ninformation about the scene, often done through multiple stages of training and\nrefinement. Current state-of-the-art (SOTA) models leverage image-level labels\nto produce class activation maps (CAMs) which go through multiple stages of\nrefinement before they are thresholded to make pseudo-masks for supervision.\nThe multi-stage approach is computationally expensive, and dependency on\nimage-level labels for CAMs generation lacks generalizability to more complex\nscenes. In contrary, our method offers a single-stage approach generalizable to\narbitrary dataset, that is trainable from scratch, without any dependency on\npre-trained backbones, classification, or separate refinement tasks. We utilize\npoint annotations to generate reliable, on-the-fly pseudo-masks through refined\nand filtered features. While our method requires point annotations that are\nonly slightly more expensive than image-level annotations, we are to\ndemonstrate SOTA performance on benchmark datasets (PascalVOC 2012), as well as\nsignificantly outperform other SOTA WSSS methods on recent real-world datasets\n(CRAID, CityPersons, IAD).\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 18:34:50 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 15:23:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Akiva", "Peri", ""], ["Dana", "Kristin", ""]]}, {"id": "2106.10319", "submitter": "Muhammad Monjurul Karim", "authors": "Muhammad Monjurul Karim, Yu Li, Ruwen Qin, Zhaozheng Yin", "title": "A system of vision sensor based deep neural networks for complex driving\n  scene analysis in support of crash risk assessment and prevention", "comments": "11 Pages, 8 Figures, Presented in TRB conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To assist human drivers and autonomous vehicles in assessing crash risks,\ndriving scene analysis using dash cameras on vehicles and deep learning\nalgorithms is of paramount importance. Although these technologies are\nincreasingly available, driving scene analysis for this purpose still remains a\nchallenge. This is mainly due to the lack of annotated large image datasets for\nanalyzing crash risk indicators and crash likelihood, and the lack of an\neffective method to extract lots of required information from complex driving\nscenes. To fill the gap, this paper develops a scene analysis system. The\nMulti-Net of the system includes two multi-task neural networks that perform\nscene classification to provide four labels for each scene. The DeepLab v3 and\nYOLO v3 are combined by the system to detect and locate risky pedestrians and\nthe nearest vehicles. All identified information can provide the situational\nawareness to autonomous vehicles or human drivers for identifying crash risks\nfrom the surrounding traffic. To address the scarcity of annotated image\ndatasets for studying traffic crashes, two completely new datasets have been\ndeveloped by this paper and made available to the public, which were proved to\nbe effective in training the proposed deep neural networks. The paper further\nevaluates the performance of the Multi-Net and the efficiency of the developed\nsystem. Comprehensive scene analysis is further illustrated with representative\nexamples. Results demonstrate the effectiveness of the developed system and\ndatasets for driving scene analysis, and their supportiveness for crash risk\nassessment and crash prevention.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:07:59 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Karim", "Muhammad Monjurul", ""], ["Li", "Yu", ""], ["Qin", "Ruwen", ""], ["Yin", "Zhaozheng", ""]]}, {"id": "2106.10335", "submitter": "Xiaohan Fei", "authors": "Xiaohan Fei, Henry Wang, Xiangyu Zeng, Lin Lee Cheong, Meng Wang,\n  Joseph Tighe", "title": "Single View Physical Distance Estimation using Human Pose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a fully automated system that simultaneously estimates the camera\nintrinsics, the ground plane, and physical distances between people from a\nsingle RGB image or video captured by a camera viewing a 3-D scene from a fixed\nvantage point. To automate camera calibration and distance estimation, we\nleverage priors about human pose and develop a novel direct formulation for\npose-based auto-calibration and distance estimation, which shows\nstate-of-the-art performance on publicly available datasets. The proposed\napproach enables existing camera systems to measure physical distances without\nneeding a dedicated calibration process or range sensors, and is applicable to\na broad range of use cases such as social distancing and workplace safety.\nFurthermore, to enable evaluation and drive research in this area, we\ncontribute to the publicly available MEVA dataset with additional distance\nannotations, resulting in MEVADA -- the first evaluation benchmark in the world\nfor the pose-based auto-calibration and distance estimation problem.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:50:40 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fei", "Xiaohan", ""], ["Wang", "Henry", ""], ["Zeng", "Xiangyu", ""], ["Cheong", "Lin Lee", ""], ["Wang", "Meng", ""], ["Tighe", "Joseph", ""]]}, {"id": "2106.10359", "submitter": "Kuang Gong", "authors": "Kuang Gong, Ciprian Catana, Jinyi Qi and Quanzheng Li", "title": "Direct Reconstruction of Linear Parametric Images from Dynamic PET Using\n  Nonlocal Deep Image Prior", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct reconstruction methods have been developed to estimate parametric\nimages directly from the measured PET sinograms by combining the PET imaging\nmodel and tracer kinetics in an integrated framework. Due to limited counts\nreceived, signal-to-noise-ratio (SNR) and resolution of parametric images\nproduced by direct reconstruction frameworks are still limited. Recently\nsupervised deep learning methods have been successfully applied to medical\nimaging denoising/reconstruction when large number of high-quality training\nlabels are available. For static PET imaging, high-quality training labels can\nbe acquired by extending the scanning time. However, this is not feasible for\ndynamic PET imaging, where the scanning time is already long enough. In this\nwork, we proposed an unsupervised deep learning framework for direct parametric\nreconstruction from dynamic PET, which was tested on the Patlak model and the\nrelative equilibrium Logan model. The patient's anatomical prior image, which\nis readily available from PET/CT or PET/MR scans, was supplied as the network\ninput to provide a manifold constraint, and also utilized to construct a kernel\nlayer to perform non-local feature denoising. The linear kinetic model was\nembedded in the network structure as a 1x1 convolution layer. The training\nobjective function was based on the PET statistical model. Evaluations based on\ndynamic datasets of 18F-FDG and 11C-PiB tracers show that the proposed\nframework can outperform the traditional and the kernel method-based direct\nreconstruction methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 21:30:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gong", "Kuang", ""], ["Catana", "Ciprian", ""], ["Qi", "Jinyi", ""], ["Li", "Quanzheng", ""]]}, {"id": "2106.10377", "submitter": "Jason Parham", "authors": "Charles V. Stewart, Jason R. Parham, Jason Holmberg and Tanya Y.\n  Berger-Wolf", "title": "The Animal ID Problem: Continual Curation", "comments": "4 pages, 2 figures, non-archival in 2021 CVPR workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Hoping to stimulate new research in individual animal identification from\nimages, we propose to formulate the problem as the human-machine Continual\nCuration of images and animal identities. This is an open world recognition\nproblem, where most new animals enter the system after its algorithms are\ninitially trained and deployed. Continual Curation, as defined here, requires\n(1) an improvement in the effectiveness of current recognition methods, (2) a\npairwise verification algorithm that allows the possibility of no decision, and\n(3) an algorithmic decision mechanism that seeks human input to guide the\ncuration process. Error metrics must evaluate the ability of recognition\nalgorithms to identify not only animals that have been seen just once or twice\nbut also recognize new animals not in the database. An important measure of\noverall system performance is accuracy as a function of the amount of human\ninput required.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 22:32:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Stewart", "Charles V.", ""], ["Parham", "Jason R.", ""], ["Holmberg", "Jason", ""], ["Berger-Wolf", "Tanya Y.", ""]]}, {"id": "2106.10393", "submitter": "Sarah Ostadabbas", "authors": "Amirreza Farnoosh, Sarah Ostadabbas", "title": "Dynamical Deep Generative Latent Modeling of 3D Skeletal Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose a Bayesian switching dynamical model for\nsegmentation of 3D pose data over time that uncovers interpretable patterns in\nthe data and is generative. Our model decomposes highly correlated skeleton\ndata into a set of few spatial basis of switching temporal processes in a\nlow-dimensional latent framework. We parameterize these temporal processes with\nregard to a switching deep vector autoregressive prior in order to accommodate\nboth multimodal and higher-order nonlinear inter-dependencies. This results in\na dynamical deep generative latent model that parses the meaningful intrinsic\nstates in the dynamics of 3D pose data using approximate variational inference,\nand enables a realistic low-level dynamical generation and segmentation of\ncomplex skeleton movements. Our experiments on four biological motion data\ncontaining bat flight, salsa dance, walking, and golf datasets substantiate\nsuperior performance of our model in comparison with the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 23:58:49 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Farnoosh", "Amirreza", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "2106.10404", "submitter": "Shiwei Liu", "authors": "Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin,\n  Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, Decebal Constantin\n  Mocanu", "title": "Sparse Training via Boosting Pruning Plasticity with Neuroregeneration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Works on lottery ticket hypothesis (LTH) and single-shot network pruning\n(SNIP) have raised a lot of attention currently on post-training pruning\n(iterative magnitude pruning), and before-training pruning (pruning at\ninitialization). The former method suffers from an extremely large computation\ncost and the latter category of methods usually struggles with insufficient\nperformance. In comparison, during-training pruning, a class of pruning methods\nthat simultaneously enjoys the training/inference efficiency and the comparable\nperformance, temporarily, has been less explored. To better understand\nduring-training pruning, we quantitatively study the effect of pruning\nthroughout training from the perspective of pruning plasticity (the ability of\nthe pruned networks to recover the original performance). Pruning plasticity\ncan help explain several other empirical observations about neural network\npruning in literature. We further find that pruning plasticity can be\nsubstantially improved by injecting a brain-inspired mechanism called\nneuroregeneration, i.e., to regenerate the same number of connections as\npruned. Based on the insights from pruning plasticity, we design a novel\ngradual magnitude pruning (GMP) method, named gradual pruning with zero-cost\nneuroregeneration (GraNet), and its dynamic sparse training (DST) variant\n(GraNet-ST). Both of them advance state of the art. Perhaps most impressively,\nthe latter for the first time boosts the sparse-to-sparse training performance\nover various dense-to-sparse methods by a large margin with ResNet-50 on\nImageNet. We will release all codes.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 02:09:25 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Liu", "Shiwei", ""], ["Chen", "Tianlong", ""], ["Chen", "Xiaohan", ""], ["Atashgahi", "Zahra", ""], ["Yin", "Lu", ""], ["Kou", "Huanyu", ""], ["Shen", "Li", ""], ["Pechenizkiy", "Mykola", ""], ["Wang", "Zhangyang", ""], ["Mocanu", "Decebal Constantin", ""]]}, {"id": "2106.10409", "submitter": "Jingtao Xu", "authors": "Jingtao Xu and Yali Li and Shengjin Wang", "title": "AdaZoom: Adaptive Zoom Network for Multi-Scale Object Detection in Large\n  Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection in large-scale scenes is a challenging problem due to small objects\nand extreme scale variation. It is essential to focus on the image regions of\nsmall objects. In this paper, we propose a novel Adaptive Zoom (AdaZoom)\nnetwork as a selective magnifier with flexible shape and focal length to\nadaptively zoom the focus regions for object detection. Based on policy\ngradient, we construct a reinforcement learning framework for focus region\ngeneration, with the reward formulated by object distributions. The scales and\naspect ratios of the generated regions are adaptive to the scales and\ndistribution of objects inside. We apply variable magnification according to\nthe scale of the region for adaptive multi-scale detection. We further propose\ncollaborative training to complementarily promote the performance of AdaZoom\nand the detection network. To validate the effectiveness, we conduct extensive\nexperiments on VisDrone2019, UAVDT, and DOTA datasets. The experiments show\nAdaZoom brings a consistent and significant improvement over different\ndetection networks, achieving state-of-the-art performance on these datasets,\nespecially outperforming the existing methods by AP of 4.64% on Vis-Drone2019.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 03:30:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xu", "Jingtao", ""], ["Li", "Yali", ""], ["Wang", "Shengjin", ""]]}, {"id": "2106.10410", "submitter": "Yuling Jiao", "authors": "Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, Can Yang", "title": "Deep Generative Learning via Schr\\\"{o}dinger Bridge", "comments": null, "journal-ref": "ICML, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn a generative model via entropy interpolation with a\nSchr\\\"{o}dinger Bridge. The generative learning task can be formulated as\ninterpolating between a reference distribution and a target distribution based\non the Kullback-Leibler divergence. At the population level, this entropy\ninterpolation is characterized via an SDE on $[0,1]$ with a time-varying drift\nterm. At the sample level, we derive our Schr\\\"{o}dinger Bridge algorithm by\nplugging the drift term estimated by a deep score estimator and a deep density\nratio estimator into the Euler-Maruyama method. Under some mild smoothness\nassumptions of the target distribution, we prove the consistency of both the\nscore estimator and the density ratio estimator, and then establish the\nconsistency of the proposed Schr\\\"{o}dinger Bridge approach. Our theoretical\nresults guarantee that the distribution learned by our approach converges to\nthe target distribution. Experimental results on multimodal synthetic data and\nbenchmark data support our theoretical findings and indicate that the\ngenerative model via Schr\\\"{o}dinger Bridge is comparable with state-of-the-art\nGANs, suggesting a new formulation of generative learning. We demonstrate its\nusefulness in image interpolation and image inpainting.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 03:35:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Gefei", ""], ["Jiao", "Yuling", ""], ["Xu", "Qian", ""], ["Wang", "Yang", ""], ["Yang", "Can", ""]]}, {"id": "2106.10430", "submitter": "Brijesh Singh", "authors": "Brijesh Singh, Arijit Sur, and Pinaki Mitra", "title": "Multi-Contextual Design of Convolutional Neural Network for Steganalysis", "comments": "This work has been submitted to the IEEE Transactions on Information\n  Forensics and Security (IEEE-TIFS) for possible publication. Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, deep learning-based steganalysis classifiers became popular\ndue to their state-of-the-art performance. Most deep steganalysis classifiers\nusually extract noise residuals using high-pass filters as preprocessing steps\nand feed them to their deep model for classification. It is observed that\nrecent steganographic embedding does not always restrict their embedding in the\nhigh-frequency zone; instead, they distribute it as per embedding policy.\nTherefore, besides noise residual, learning the embedding zone is another\nchallenging task. In this work, unlike the conventional approaches, the\nproposed model first extracts the noise residual using learned denoising\nkernels to boost the signal-to-noise ratio. After preprocessing, the sparse\nnoise residuals are fed to a novel Multi-Contextual Convolutional Neural\nNetwork (M-CNET) that uses heterogeneous context size to learn the sparse and\nlow-amplitude representation of noise residuals. The model performance is\nfurther improved by incorporating the Self-Attention module to focus on the\nareas prone to steganalytic embedding. A set of comprehensive experiments is\nperformed to show the proposed scheme's efficacy over the prior arts. Besides,\nan ablation study is given to justify the contribution of various modules of\nthe proposed architecture.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 05:38:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Singh", "Brijesh", ""], ["Sur", "Arijit", ""], ["Mitra", "Pinaki", ""]]}, {"id": "2106.10432", "submitter": "Muhamad Amin Husni Abdul Haris", "authors": "Muhamad Amin Husni Abdul Haris, Sin Liang Lim", "title": "Neural Network Facial Authentication for Public Electric Vehicle\n  Charging Station", "comments": null, "journal-ref": "JETAP Vol.3 No.1 (2021) 17-21", "doi": "10.33093/jetap.2021.3.1.4", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study is to investigate and compare the facial recognition accuracy\nperformance of Dlib ResNet against a K-Nearest Neighbour (KNN) classifier.\nParticularly when used against a dataset from an Asian ethnicity as Dlib ResNet\nwas reported to have an accuracy deficiency when it comes to Asian faces. The\ncomparisons are both implemented on the facial vectors extracted using the\nHistogram of Oriented Gradients (HOG) method and use the same dataset for a\nfair comparison. Authentication of a user by facial recognition in an electric\nvehicle (EV) charging station demonstrates a practical use case for such an\nauthentication system.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 05:48:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Haris", "Muhamad Amin Husni Abdul", ""], ["Lim", "Sin Liang", ""]]}, {"id": "2106.10437", "submitter": "Sieun Park", "authors": "Sieun Park, Eunho Lee", "title": "One-to-many Approach for Improving Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Super-resolution (SR) is a one-to-many task with multiple possible solutions.\nHowever, previous works were not concerned about this characteristic. For a\none-to-many pipeline, the generator should be able to generate multiple\nestimates of the reconstruction, and not be penalized for generating similar\nand equally realistic images. To achieve this, we propose adding weighted\npixel-wise noise after every Residual-in-Residual Dense Block (RRDB) to enable\nthe generator to generate various images. We modify the strict content loss to\nnot penalize the stochastic variation in reconstructed images as long as it has\nconsistent content. Additionally, we observe that there are out-of-focus\nregions in the DIV2K, DIV8K datasets that provide unhelpful guidelines. We\nfilter blurry regions in the training data using the method of [10]. Finally,\nwe modify the discriminator to receive the low-resolution image as a reference\nimage along with the target image to provide better feedback to the generator.\nUsing our proposed methods, we were able to improve the performance of ESRGAN\nin x4 perceptual SR and achieve the state-of-the-art LPIPS score in x16\nperceptual extreme SR.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 06:41:29 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 01:25:11 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Park", "Sieun", ""], ["Lee", "Eunho", ""]]}, {"id": "2106.10446", "submitter": "Gicheon Kang", "authors": "Ahjeong Seo, Gi-Cheon Kang, Joonhan Park, Byoung-Tak Zhang", "title": "Attend What You Need: Motion-Appearance Synergistic Networks for Video\n  Question Answering", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Question Answering is a task which requires an AI agent to answer\nquestions grounded in video. This task entails three key challenges: (1)\nunderstand the intention of various questions, (2) capturing various elements\nof the input video (e.g., object, action, causality), and (3) cross-modal\ngrounding between language and vision information. We propose Motion-Appearance\nSynergistic Networks (MASN), which embed two cross-modal features grounded on\nmotion and appearance information and selectively utilize them depending on the\nquestion's intentions. MASN consists of a motion module, an appearance module,\nand a motion-appearance fusion module. The motion module computes the\naction-oriented cross-modal joint representations, while the appearance module\nfocuses on the appearance aspect of the input video. Finally, the\nmotion-appearance fusion module takes each output of the motion module and the\nappearance module as input, and performs question-guided fusion. As a result,\nMASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA\ndatasets. We also conduct qualitative analysis by visualizing the inference\nresults of MASN. The code is available at\nhttps://github.com/ahjeongseo/MASN-pytorch.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 07:48:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Seo", "Ahjeong", ""], ["Kang", "Gi-Cheon", ""], ["Park", "Joonhan", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "2106.10452", "submitter": "Vidit Goel", "authors": "Vidit Goel, Jiachen Li, Shubhika Garg, Harsh Maheshwari, Humphrey Shi", "title": "MSN: Efficient Online Mask Selection Network for Video Instance\n  Segmentation", "comments": "3rd Place Solution to the YouTube-VIS Challenge at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel solution for Video Instance\nSegmentation(VIS), that is automatically generating instance level segmentation\nmasks along with object class and tracking them in a video. Our method improves\nthe masks from segmentation and propagation branches in an online manner using\nthe Mask Selection Network (MSN) hence limiting the noise accumulation during\nmask tracking. We propose an effective design of MSN by using patch-based\nconvolutional neural network. The network is able to distinguish between very\nsubtle differences between the masks and choose the better masks out of the\nassociated masks accurately. Further, we make use of temporal consistency and\nprocess the video sequences in both forward and reverse manner as a post\nprocessing step to recover lost objects. The proposed method can be used to\nadapt any video object segmentation method for the task of VIS. Our method\nachieves a score of 49.1 mAP on 2021 YouTube-VIS Challenge and was ranked third\nplace among more than 30 global teams. Our code will be available at\nhttps://github.com/SHI-Labs/Mask-Selection-Networks.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 08:33:29 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Goel", "Vidit", ""], ["Li", "Jiachen", ""], ["Garg", "Shubhika", ""], ["Maheshwari", "Harsh", ""], ["Shi", "Humphrey", ""]]}, {"id": "2106.10456", "submitter": "Yihe Tang", "authors": "Yihe Tang, Weifeng Chen, Yijun Luo, Yuting Zhang", "title": "Humble Teachers Teach Better Students for Semi-Supervised Object\n  Detection", "comments": "CVPR 2021 camera-ready. Code: https://github.com/lryta/HumbleTeacher", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semi-supervised approach for contemporary object detectors\nfollowing the teacher-student dual model framework. Our method is featured with\n1) the exponential moving averaging strategy to update the teacher from the\nstudent online, 2) using plenty of region proposals and soft pseudo-labels as\nthe student's training targets, and 3) a light-weighted detection-specific data\nensemble for the teacher to generate more reliable pseudo-labels. Compared to\nthe recent state-of-the-art -- STAC, which uses hard labels on sparsely\nselected hard pseudo samples, the teacher in our model exposes richer\ninformation to the student with soft-labels on many proposals. Our model\nachieves COCO-style AP of 53.04% on VOC07 val set, 8.4% better than STAC, when\nusing VOC12 as unlabeled data. On MS-COCO, it outperforms prior work when only\na small percentage of data is taken as labeled. It also reaches 53.8% AP on\nMS-COCO test-dev with 3.1% gain over the fully supervised ResNet-152 Cascaded\nR-CNN, by tapping into unlabeled data of a similar size to the labeled data.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 09:05:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Tang", "Yihe", ""], ["Chen", "Weifeng", ""], ["Luo", "Yijun", ""], ["Zhang", "Yuting", ""]]}, {"id": "2106.10458", "submitter": "Tiago Barros", "authors": "Tiago Barros, Ricardo Pereira, Lu\\'is Garrote, Cristiano Premebida,\n  Urbano J. Nunes", "title": "Place recognition survey: An update on deep learning approaches", "comments": "Under review in IEEE Transactions on Intelligent Vehicles. This work\n  was submitted on the 13/01/2021 to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. Upon acceptance of the article by IEEE, the preprint\n  article will be replaced with the accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous Vehicles (AV) are becoming more capable of navigating in complex\nenvironments with dynamic and changing conditions. A key component that enables\nthese intelligent vehicles to overcome such conditions and become more\nautonomous is the sophistication of the perception and localization systems. As\npart of the localization system, place recognition has benefited from recent\ndevelopments in other perception tasks such as place categorization or object\nrecognition, namely with the emergence of deep learning (DL) frameworks. This\npaper surveys recent approaches and methods used in place recognition,\nparticularly those based on deep learning. The contributions of this work are\ntwofold: surveying recent sensors such as 3D LiDARs and RADARs, applied in\nplace recognition; and categorizing the various DL-based place recognition\nworks into supervised, unsupervised, semi-supervised, parallel, and\nhierarchical categories. First, this survey introduces key place recognition\nconcepts to contextualize the reader. Then, sensor characteristics are\naddressed. This survey proceeds by elaborating on the various DL-based works,\npresenting summaries for each framework. Some lessons learned from this survey\ninclude: the importance of NetVLAD for supervised end-to-end learning; the\nadvantages of unsupervised approaches in place recognition, namely for\ncross-domain applications; or the increasing tendency of recent works to seek,\nnot only for higher performance but also for higher efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 09:17:15 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 10:31:04 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Barros", "Tiago", ""], ["Pereira", "Ricardo", ""], ["Garrote", "Lu\u00eds", ""], ["Premebida", "Cristiano", ""], ["Nunes", "Urbano J.", ""]]}, {"id": "2106.10464", "submitter": "Stanis{\\l}aw Ka\\'zmierczak", "authors": "Stanis{\\l}aw Ka\\'zmierczak, Zofia Juszka, Piotr Fudalej, Jacek\n  Ma\\'ndziuk", "title": "Prediction of the facial growth direction with Machine Learning methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  First attempts of prediction of the facial growth (FG) direction were made\nover half of a century ago. Despite numerous attempts and elapsed time, a\nsatisfactory method has not been established yet and the problem still poses a\nchallenge for medical experts. To our knowledge, this paper is the first\nMachine Learning approach to the prediction of FG direction. Conducted data\nanalysis reveals the inherent complexity of the problem and explains the\nreasons of difficulty in FG direction prediction based on 2D X-ray images. To\nperform growth forecasting, we employ a wide range of algorithms, from logistic\nregression, through tree ensembles to neural networks and consider three,\nslightly different, problem formulations. The resulting classification accuracy\nvaries between 71% and 75%.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 10:12:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ka\u017amierczak", "Stanis\u0142aw", ""], ["Juszka", "Zofia", ""], ["Fudalej", "Piotr", ""], ["Ma\u0144dziuk", "Jacek", ""]]}, {"id": "2106.10465", "submitter": "Chih-Ting Liu", "authors": "Chun-Tse Lin, Wei-Chih Tu, Chih-Ting Liu, Shao-Yi Chien", "title": "Interactive Object Segmentation with Dynamic Click Transform", "comments": "This paper was accepted by IEEE International Conference on Image\n  Processing (ICIP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the interactive segmentation, users initially click on the target object\nto segment the main body and then provide corrections on mislabeled regions to\niteratively refine the segmentation masks. Most existing methods transform\nthese user-provided clicks into interaction maps and concatenate them with\nimage as the input tensor. Typically, the interaction maps are determined by\nmeasuring the distance of each pixel to the clicked points, ignoring the\nrelation between clicks and mislabeled regions. We propose a Dynamic Click\nTransform Network~(DCT-Net), consisting of Spatial-DCT and Feature-DCT, to\nbetter represent user interactions. Spatial-DCT transforms each user-provided\nclick with individual diffusion distance according to the target scale, and\nFeature-DCT normalizes the extracted feature map to a specific distribution\npredicted from the clicked points. We demonstrate the effectiveness of our\nproposed method and achieve favorable performance compared to the\nstate-of-the-art on three standard benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 10:13:37 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lin", "Chun-Tse", ""], ["Tu", "Wei-Chih", ""], ["Liu", "Chih-Ting", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "2106.10472", "submitter": "Zhenyue Qin", "authors": "Zhenyue Qin and Dongwoo Kim and Tom Gedeon", "title": "Informative Class Activation Maps", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.10688", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to evaluate the quantitative information content of a region\nwithin an image for a particular label. To this end, we bridge class activation\nmaps with information theory. We develop an informative class activation map\n(infoCAM). Given a classification task, infoCAM depict how to accumulate\ninformation of partial regions to that of the entire image toward a label.\nThus, we can utilise infoCAM to locate the most informative features for a\nlabel. When applied to an image classification task, infoCAM performs better\nthan the traditional classification map in the weakly supervised object\nlocalisation task. We achieve state-of-the-art results on Tiny-ImageNet.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 11:02:59 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Qin", "Zhenyue", ""], ["Kim", "Dongwoo", ""], ["Gedeon", "Tom", ""]]}, {"id": "2106.10479", "submitter": "Yang Tan", "authors": "Yang Tan, Yang Li, Shao-Lun Huang", "title": "Practical Transferability Estimation for Image Classification Tasks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferability estimation is an essential problem in transfer learning to\npredict how good the performance is when transferring a source model (or source\ntask) to a target task. Recent analytical transferability metrics have been\nwidely used for source model selection and multi-task learning. A major\nchallenge is how to make transfereability estimation robust under the\ncross-domain cross-task settings. The recently proposed OTCE score solves this\nproblem by considering both domain and task differences, with the help of\ntransfer experiences on auxiliary tasks, which causes an efficiency overhead.\nIn this work, we propose a practical transferability metric called JC-NCE score\nthat dramatically improves the robustness of the task difference estimation in\nOTCE, thus removing the need for auxiliary tasks. Specifically, we build the\njoint correspondences between source and target data via solving an optimal\ntransport problem with a ground cost considering both the sample distance and\nlabel distance, and then compute the transferability score as the negative\nconditional entropy of the matched labels. Extensive validations under the\nintra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE\nscore outperforms the auxiliary-task free version of OTCE for 7% and 12%,\nrespectively, and is also more robust than other existing transferability\nmetrics on average.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 11:59:11 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 10:26:37 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Tan", "Yang", ""], ["Li", "Yang", ""], ["Huang", "Shao-Lun", ""]]}, {"id": "2106.10482", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Yingchen Yu, Kaiwen Cui, Gongjie Zhang, Shijian Lu,\n  Jianxiong Pan, Changgong Zhang, Feiying Ma, Xuansong Xie, Chunyan Miao", "title": "Unbalanced Feature Transport for Exemplar-based Image Translation", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the great success of GANs in images translation with different\nconditioned inputs such as semantic segmentation and edge maps, generating\nhigh-fidelity realistic images with reference styles remains a grand challenge\nin conditional image-to-image translation. This paper presents a general image\ntranslation framework that incorporates optimal transport for feature alignment\nbetween conditional inputs and style exemplars in image translation. The\nintroduction of optimal transport mitigates the constraint of many-to-one\nfeature matching significantly while building up accurate semantic\ncorrespondences between conditional inputs and exemplars. We design a novel\nunbalanced optimal transport to address the transport between features with\ndeviational distributions which exists widely between conditional inputs and\nexemplars. In addition, we design a semantic-activation normalization scheme\nthat injects style features of exemplars into the image translation process\nsuccessfully. Extensive experiments over multiple image translation tasks show\nthat our method achieves superior image translation qualitatively and\nquantitatively as compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 12:07:48 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhan", "Fangneng", ""], ["Yu", "Yingchen", ""], ["Cui", "Kaiwen", ""], ["Zhang", "Gongjie", ""], ["Lu", "Shijian", ""], ["Pan", "Jianxiong", ""], ["Zhang", "Changgong", ""], ["Ma", "Feiying", ""], ["Xie", "Xuansong", ""], ["Miao", "Chunyan", ""]]}, {"id": "2106.10486", "submitter": "Chen Zhang", "authors": "Chen Zhang, Yinghao Xu, Yujun Shen", "title": "CompConv: A Compact Convolution Module for Efficient Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have achieved remarkable success in\nvarious computer vision tasks but rely on tremendous computational cost. To\nsolve this problem, existing approaches either compress well-trained\nlarge-scale models or learn lightweight models with carefully designed network\nstructures. In this work, we make a close study of the convolution operator,\nwhich is the basic unit used in CNNs, to reduce its computing load. In\nparticular, we propose a compact convolution module, called CompConv, to\nfacilitate efficient feature learning. With the divide-and-conquer strategy,\nCompConv is able to save a great many computations as well as parameters to\nproduce a certain dimensional feature map. Furthermore, CompConv discreetly\nintegrates the input features into the outputs to efficiently inherit the input\ninformation. More importantly, the novel CompConv is a plug-and-play module\nthat can be directly applied to modern CNN structures to replace the vanilla\nconvolution layers without further effort. Extensive experimental results\nsuggest that CompConv can adequately compress the benchmark CNN structures yet\nbarely sacrifice the performance, surpassing other competitors.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 12:31:57 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 11:03:35 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Chen", ""], ["Xu", "Yinghao", ""], ["Shen", "Yujun", ""]]}, {"id": "2106.10493", "submitter": "Jianyun Xu", "authors": "Jianyun Xu, Xin Tang, Jian Dou, Xu Shu, Yushi Zhu", "title": "CenterAtt: Fast 2-stage Center Attention Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we introduce the methods of HIKVISION_LiDAR_Det in\nthe challenge of waymo open dataset real-time 3D detection. Our solution for\nthe competition are built upon Centerpoint 3D detection framework. Several\nvariants of CenterPoint are explored, including center attention head and\nfeature pyramid network neck. In order to achieve real time detection, methods\nlike batchnorm merge, half-precision floating point network and GPU-accelerated\nvoxelization process are adopted. By using these methods, our team ranks 6th\namong all the methods on real-time 3D detection challenge in the waymo open\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 13:03:14 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xu", "Jianyun", ""], ["Tang", "Xin", ""], ["Dou", "Jian", ""], ["Shu", "Xu", ""], ["Zhu", "Yushi", ""]]}, {"id": "2106.10506", "submitter": "Yichao Yan", "authors": "Yichao Yan, Jinpeng Li, Shengcai Liao, Jie Qin, Bingbing Ni, Xiaokang\n  Yang, and Ling Shao", "title": "Exploring Visual Context for Weakly Supervised Person Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search has recently emerged as a challenging task that jointly\naddresses pedestrian detection and person re-identification. Existing\napproaches follow a fully supervised setting where both bounding box and\nidentity annotations are available. However, annotating identities is\nlabor-intensive, limiting the practicability and scalability of current\nframeworks. This paper inventively considers weakly supervised person search\nwith only bounding box annotations. We proposed the first framework to address\nthis novel task, namely Context-Guided Person Search (CGPS), by investigating\nthree levels of context clues (i.e., detection, memory and scene) in\nunconstrained natural images. The first two are employed to promote local and\nglobal discriminative capabilities, while the latter enhances clustering\naccuracy. Despite its simple design, our CGPS boosts the baseline model by 8.3%\nin mAP on CUHK-SYSU. Surprisingly, it even achieves comparable performance to\ntwo-step person search models, while displaying higher efficiency. Our code is\navailable at https://github.com/ljpadam/CGPS.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 14:47:13 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Yan", "Yichao", ""], ["Li", "Jinpeng", ""], ["Liao", "Shengcai", ""], ["Qin", "Jie", ""], ["Ni", "Bingbing", ""], ["Yang", "Xiaokang", ""], ["Shao", "Ling", ""]]}, {"id": "2106.10507", "submitter": "Yufei Li", "authors": "Ke Chen, Yufei Li, Yingfeng Chen, Changjie Fan, Zhipeng Hu, Wei Yang", "title": "GLIB: Towards Automated Test Oracle for Graphically-Rich Applications", "comments": "ESEC/FSE 2021", "journal-ref": null, "doi": "10.1145/3468264.3468586", "report-no": null, "categories": "cs.SE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Graphically-rich applications such as games are ubiquitous with attractive\nvisual effects of Graphical User Interface (GUI) that offers a bridge between\nsoftware applications and end-users. However, various types of graphical\nglitches may arise from such GUI complexity and have become one of the main\ncomponent of software compatibility issues. Our study on bug reports from game\ndevelopment teams in NetEase Inc. indicates that graphical glitches frequently\noccur during the GUI rendering and severely degrade the quality of\ngraphically-rich applications such as video games. Existing automated testing\ntechniques for such applications focus mainly on generating various GUI test\nsequences and check whether the test sequences can cause crashes. These\ntechniques require constant human attention to captures non-crashing bugs such\nas bugs causing graphical glitches. In this paper, we present the first step in\nautomating the test oracle for detecting non-crashing bugs in graphically-rich\napplications. Specifically, we propose \\texttt{GLIB} based on a code-based data\naugmentation technique to detect game GUI glitches. We perform an evaluation of\n\\texttt{GLIB} on 20 real-world game apps (with bug reports available) and the\nresult shows that \\texttt{GLIB} can achieve 100\\% precision and 99.5\\% recall\nin detecting non-crashing bugs such as game GUI glitches. Practical application\nof \\texttt{GLIB} on another 14 real-world games (without bug reports) further\ndemonstrates that \\texttt{GLIB} can effectively uncover GUI glitches, with 48\nof 53 bugs reported by \\texttt{GLIB} having been confirmed and fixed so far.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 14:50:43 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 17:22:48 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 22:39:34 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Chen", "Ke", ""], ["Li", "Yufei", ""], ["Chen", "Yingfeng", ""], ["Fan", "Changjie", ""], ["Hu", "Zhipeng", ""], ["Yang", "Wei", ""]]}, {"id": "2106.10528", "submitter": "Tianrui Liu", "authors": "Tianrui Liu, Qingjie Meng, Jun-Jie Huang, Athanasios Vlontzos, Daniel\n  Rueckert, Bernhard Kainz", "title": "Video Summarization through Reinforcement Learning with a 3D\n  Spatio-Temporal U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent video summarization algorithms allow to quickly convey the most\nrelevant information in videos through the identification of the most essential\nand explanatory content while removing redundant video frames. In this paper,\nwe introduce the 3DST-UNet-RL framework for video summarization. A 3D\nspatio-temporal U-Net is used to efficiently encode spatio-temporal information\nof the input videos for downstream reinforcement learning (RL). An RL agent\nlearns from spatio-temporal latent scores and predicts actions for keeping or\nrejecting a video frame in a video summary. We investigate if real/inflated 3D\nspatio-temporal CNN features are better suited to learn representations from\nvideos than commonly used 2D image features. Our framework can operate in both,\na fully unsupervised mode and a supervised training mode. We analyse the impact\nof prescribed summary lengths and show experimental evidence for the\neffectiveness of 3DST-UNet-RL on two commonly used general video summarization\nbenchmarks. We also applied our method on a medical video summarization task.\nThe proposed video summarization method has the potential to save storage costs\nof ultrasound screening videos as well as to increase efficiency when browsing\npatient video data during retrospective analysis or audit without loosing\nessential information\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 16:27:19 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Liu", "Tianrui", ""], ["Meng", "Qingjie", ""], ["Huang", "Jun-Jie", ""], ["Vlontzos", "Athanasios", ""], ["Rueckert", "Daniel", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2106.10542", "submitter": "Arun Jose", "authors": "Arun Jose, Abraham Francis", "title": "Reversible Colour Density Compression of Images using cGANs", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image compression using colour densities is historically impractical to\ndecompress losslessly. We examine the use of conditional generative adversarial\nnetworks in making this transformation more feasible, through learning a\nmapping between the images and a loss function to train on. We show that this\nmethod is effective at producing visually lossless generations, indicating that\nefficient colour compression is viable.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 17:44:39 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Jose", "Arun", ""], ["Francis", "Abraham", ""]]}, {"id": "2106.10548", "submitter": "Argho Sarkar", "authors": "Argho Sarkar, Maryam Rahnemoonfar", "title": "VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment\n  and Analysis", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering system integrated with Unmanned Aerial Vehicle\n(UAV) has a lot of potentials to advance the post-disaster damage assessment\npurpose. Providing assistance to affected areas is highly dependent on\nreal-time data assessment and analysis. Scope of the Visual Question Answering\nis to understand the scene and provide query related answer which certainly\nfaster the recovery process after any disaster. In this work, we address the\nimportance of \\textit{visual question answering (VQA)} task for post-disaster\ndamage assessment by presenting our recently developed VQA dataset called\n\\textit{HurMic-VQA} collected during hurricane Michael, and comparing the\nperformances of baseline VQA models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 18:28:16 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sarkar", "Argho", ""], ["Rahnemoonfar", "Maryam", ""]]}, {"id": "2106.10581", "submitter": "Fouad Yacef", "authors": "Faiza Mekhalfa and Fouad Yacef", "title": "Supervised learning for crop/weed classification based on color and\n  texture features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer vision techniques have attracted a great interest in precision\nagriculture, recently. The common goal of all computer vision-based precision\nagriculture tasks is to detect the objects of interest (e.g., crop, weed) and\ndiscriminating them from the background. The Weeds are unwanted plants growing\namong crops competing for nutrients, water, and sunlight, causing losses to\ncrop yields. Weed detection and mapping is critical for site-specific weed\nmanagement to reduce the cost of labor and impact of herbicides. This paper\ninvestigates the use of color and texture features for discrimination of\nSoybean crops and weeds. Feature extraction methods including two color spaces\n(RGB, HSV), gray level Co-occurrence matrix (GLCM), and Local Binary Pattern\n(LBP) are used to train the Support Vector Machine (SVM) classifier. The\nexperiment was carried out on image dataset of soybean crop, obtained from an\nunmanned aerial vehicle (UAV), which is publicly available. The results from\nthe experiment showed that the highest accuracy (above 96%) was obtained from\nthe combination of color and LBP features.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 22:31:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mekhalfa", "Faiza", ""], ["Yacef", "Fouad", ""]]}, {"id": "2106.10587", "submitter": "Kerem Turgutlu", "authors": "Marcos V. Conde and Kerem Turgutlu", "title": "Exploring Vision Transformers for Fine-grained Classification", "comments": "4 pages, 5 figures, 4 tables. Published in IEEE Computer Society\n  Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) 2021\n  - FGVC8. For code see https://github.com/mv-lab/ViT-FGVC8 and for other\n  workshop papers see https://sites.google.com/view/fgvc8/papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing computer vision research in categorization struggles with\nfine-grained attributes recognition due to the inherently high intra-class\nvariances and low inter-class variances. SOTA methods tackle this challenge by\nlocating the most informative image regions and rely on them to classify the\ncomplete image. The most recent work, Vision Transformer (ViT), shows its\nstrong performance in both traditional and fine-grained classification tasks.\nIn this work, we propose a multi-stage ViT framework for fine-grained image\nclassification tasks, which localizes the informative image regions without\nrequiring architectural changes using the inherent multi-head self-attention\nmechanism. We also introduce attention-guided augmentations for improving the\nmodel's capabilities. We demonstrate the value of our approach by experimenting\nwith four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,\nStanford Dogs, and FGVC7 Plant Pathology. We also prove our model's\ninterpretability via qualitative results.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 23:57:31 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 01:04:12 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Conde", "Marcos V.", ""], ["Turgutlu", "Kerem", ""]]}, {"id": "2106.10588", "submitter": "Abhinav Goel", "authors": "Abhinav Goel, Caleb Tung, Xiao Hu, Haobo Wang, James C. Davis, George\n  K. Thiruvathukal, Yung-Hsiang Lu", "title": "Low-Power Multi-Camera Object Re-Identification using Hierarchical\n  Neural Networks", "comments": "Accepted to ISLPED 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-power computer vision on embedded devices has many applications. This\npaper describes a low-power technique for the object re-identification (reID)\nproblem: matching a query image against a gallery of previously seen images.\nState-of-the-art techniques rely on large, computationally-intensive Deep\nNeural Networks (DNNs). We propose a novel hierarchical DNN architecture that\nuses attribute labels in the training dataset to perform efficient object reID.\nAt each node in the hierarchy, a small DNN identifies a different attribute of\nthe query image. The small DNN at each leaf node is specialized to re-identify\na subset of the gallery: only the images with the attributes identified along\nthe path from the root to a leaf. Thus, a query image is re-identified\naccurately after processing with a few small DNNs. We compare our method with\nstate-of-the-art object reID techniques. With a 4% loss in accuracy, our\napproach realizes significant resource savings: 74% less memory, 72% fewer\noperations, and 67% lower query latency, yielding 65% less energy consumption.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 23:59:26 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Goel", "Abhinav", ""], ["Tung", "Caleb", ""], ["Hu", "Xiao", ""], ["Wang", "Haobo", ""], ["Davis", "James C.", ""], ["Thiruvathukal", "George K.", ""], ["Lu", "Yung-Hsiang", ""]]}, {"id": "2106.10598", "submitter": "Wenyuan Xue", "authors": "Wenyuan Xue and Baosheng Yu and Wen Wang and Dacheng Tao and Qingyong\n  Li", "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A table arranging data in rows and columns is a very effective data\nstructure, which has been widely used in business and scientific research.\nConsidering large-scale tabular data in online and offline documents, automatic\ntable recognition has attracted increasing attention from the document analysis\ncommunity. Though human can easily understand the structure of tables, it\nremains a challenge for machines to understand that, especially due to a\nvariety of different table layouts and styles. Existing methods usually model a\ntable as either the markup sequence or the adjacency matrix between different\ntable cells, failing to address the importance of the logical location of table\ncells, e.g., a cell is located in the first row and the second column of the\ntable. In this paper, we reformulate the problem of table structure recognition\nas the table graph reconstruction, and propose an end-to-end trainable table\ngraph reconstruction network (TGRNet) for table structure recognition.\nSpecifically, the proposed method has two main branches, a cell detection\nbranch and a cell logical location branch, to jointly predict the spatial\nlocation and the logical location of different cells. Experimental results on\nthree popular table recognition datasets and a new dataset with table graph\nannotations (TableGraph-350K) demonstrate the effectiveness of the proposed\nTGRNet for table structure recognition. Code and annotations will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 01:57:05 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 03:08:37 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Xue", "Wenyuan", ""], ["Yu", "Baosheng", ""], ["Wang", "Wen", ""], ["Tao", "Dacheng", ""], ["Li", "Qingyong", ""]]}, {"id": "2106.10601", "submitter": "Yaxiong Wang", "authors": "Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu and Yi Yang", "title": "ReGO: Reference-Guided Outpainting for Scenery Image", "comments": "Image outpainting, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We aim to tackle the challenging yet practical scenery image outpainting task\nin this work. Recently, generative adversarial learning has significantly\nadvanced the image outpainting by producing semantic consistent content for the\ngiven image. However, the existing methods always suffer from the blurry\ntexture and the artifacts of the generative part, making the overall\noutpainting results lack authenticity. To overcome the weakness, this work\ninvestigates a principle way to synthesize texture-rich results by borrowing\npixels from its neighbors (\\ie, reference images), named\n\\textbf{Re}ference-\\textbf{G}uided \\textbf{O}utpainting (ReGO). Particularly,\nthe ReGO designs an Adaptive Content Selection (ACS) module to transfer the\npixel of reference images for texture compensating of the target one. To\nprevent the style of the generated part from being affected by the reference\nimages, a style ranking loss is further proposed to augment the ReGO to\nsynthesize style-consistent results. Extensive experiments on two popular\nbenchmarks, NS6K~\\cite{yangzx} and NS8K~\\cite{wang}, well demonstrate the\neffectiveness of our ReGO.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 02:34:55 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 11:33:31 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Wang", "Yaxiong", ""], ["Wei", "Yunchao", ""], ["Qian", "Xueming", ""], ["Zhu", "Li", ""], ["Yang", "Yi", ""]]}, {"id": "2106.10605", "submitter": "Haifeng Li", "authors": "Haifeng Li, Yi Li, Guo Zhang, Ruoyun Liu, Haozhe Huang, Qing Zhu, Chao\n  Tao", "title": "Remote Sensing Images Semantic Segmentation with General Remote Sensing\n  Vision Model via a Self-Supervised Contrastive Learning Method", "comments": "13 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new learning paradigm, self-supervised learning (SSL), can be used to solve\nsuch problems by pre-training a general model with large unlabeled images and\nthen fine-tuning on a downstream task with very few labeled samples.\nContrastive learning is a typical method of SSL, which can learn general\ninvariant features. However, most of the existing contrastive learning is\ndesigned for classification tasks to obtain an image-level representation,\nwhich may be sub-optimal for semantic segmentation tasks requiring pixel-level\ndiscrimination. Therefore, we propose Global style and Local matching\nContrastive Learning Network (GLCNet) for remote sensing semantic segmentation.\nSpecifically, the global style contrastive module is used to learn an\nimage-level representation better, as we consider the style features can better\nrepresent the overall image features; The local features matching contrastive\nmodule is designed to learn representations of local regions which is\nbeneficial for semantic segmentation. We evaluate four remote sensing semantic\nsegmentation datasets, and the experimental results show that our method mostly\noutperforms state-of-the-art self-supervised methods and ImageNet pre-training.\nSpecifically, with 1\\% annotation from the original dataset, our approach\nimproves Kappa by 6\\% on the ISPRS Potsdam dataset and 3\\% on Deep Globe Land\nCover Classification dataset relative to the existing baseline. Moreover, our\nmethod outperforms supervised learning when there are some differences between\nthe datasets of upstream tasks and downstream tasks. Our study promotes the\ndevelopment of self-supervised learning in the field of remote sensing semantic\nsegmentation. The source code is available at\nhttps://github.com/GeoX-Lab/G-RSIM.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 03:03:40 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Haifeng", ""], ["Li", "Yi", ""], ["Zhang", "Guo", ""], ["Liu", "Ruoyun", ""], ["Huang", "Haozhe", ""], ["Zhu", "Qing", ""], ["Tao", "Chao", ""]]}, {"id": "2106.10606", "submitter": "Naveed Akhtar Dr.", "authors": "Naveed Akhtar, Muhammad A. A. K. Jalwana, Mohammed Bennamoun, Ajmal\n  Mian", "title": "Attack to Fool and Explain Deep Networks", "comments": "To appear in IEEE TPAMI. arXiv admin note: text overlap with\n  arXiv:1905.11544", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep visual models are susceptible to adversarial perturbations to inputs.\nAlthough these signals are carefully crafted, they still appear noise-like\npatterns to humans. This observation has led to the argument that deep visual\nrepresentation is misaligned with human perception. We counter-argue by\nproviding evidence of human-meaningful patterns in adversarial perturbations.\nWe first propose an attack that fools a network to confuse a whole category of\nobjects (source class) with a target label. Our attack also limits the\nunintended fooling by samples from non-sources classes, thereby circumscribing\nhuman-defined semantic notions for network fooling. We show that the proposed\nattack not only leads to the emergence of regular geometric patterns in the\nperturbations, but also reveals insightful information about the decision\nboundaries of deep models. Exploring this phenomenon further, we alter the\n`adversarial' objective of our attack to use it as a tool to `explain' deep\nvisual representation. We show that by careful channeling and projection of the\nperturbations computed by our method, we can visualize a model's understanding\nof human-defined semantic notions. Finally, we exploit the explanability\nproperties of our perturbations to perform image generation, inpainting and\ninteractive image manipulation by attacking adversarialy robust\n`classifiers'.In all, our major contribution is a novel pragmatic adversarial\nattack that is subsequently transformed into a tool to interpret the visual\nmodels. The article also makes secondary contributions in terms of establishing\nthe utility of our attack beyond the adversarial objective with multiple\ninteresting applications.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 03:07:36 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Akhtar", "Naveed", ""], ["Jalwana", "Muhammad A. A. K.", ""], ["Bennamoun", "Mohammed", ""], ["Mian", "Ajmal", ""]]}, {"id": "2106.10634", "submitter": "Chaolei Tan", "authors": "Chaolei Tan, Zihang Lin, Jian-Fang Hu, Xiang Li, Wei-Shi Zheng", "title": "Augmented 2D-TAN: A Two-stage Approach for Human-centric Spatio-Temporal\n  Video Grounding", "comments": "A technical report on our solution for Person in Context(PIC)\n  Challenge HCVG track at CVPR 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective two-stage approach to tackle the problem of\nlanguage-based Human-centric Spatio-Temporal Video Grounding (HC-STVG) task. In\nthe first stage, we propose an Augmented 2D Temporal Adjacent Network\n(Augmented 2D-TAN) to temporally ground the target moment corresponding to the\ngiven description. Primarily, we improve the original 2D-TAN from two aspects:\nFirst, a temporal context-aware Bi-LSTM Aggregation Module is developed to\naggregate clip-level representations, replacing the original max-pooling.\nSecond, we propose to employ Random Concatenation Augmentation (RCA) mechanism\nduring the training phase. In the second stage, we use pretrained MDETR model\nto generate per-frame bounding boxes via language query, and design a set of\nhand-crafted rules to select the best matching bounding box outputted by MDETR\nfor each frame within the grounded moment.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 06:35:40 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Tan", "Chaolei", ""], ["Lin", "Zihang", ""], ["Hu", "Jian-Fang", ""], ["Li", "Xiang", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "2106.10635", "submitter": "Yijie Wu", "authors": "Yijie Wu and Fan Xue", "title": "FloorPP-Net: Reconstructing Floor Plans using Point Pillars for\n  Scan-to-BIM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep learning-based point cloud processing method named\nFloorPP-Net for the task of Scan-to-BIM (building information model).\nFloorPP-Net first converts the input point cloud of a building story into point\npillars (PP), then predicts the corners and edges to output the floor plan.\nAltogether, FloorPP-Net establishes an end-to-end supervised learning framework\nfor the Scan-to-Floor-Plan (Scan2FP) task. In the 1st International Scan-to-BIM\nChallenge held in conjunction with CVPR 2021, FloorPP-Net was ranked the second\nrunner-up in the floor plan reconstruction track. Future work includes general\nedge proposals, 2D plan regularization, and 3D BIM reconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 06:45:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wu", "Yijie", ""], ["Xue", "Fan", ""]]}, {"id": "2106.10637", "submitter": "Yijiang Li", "authors": "Yijiang Li, Wentian Cai, Ying Gao and Xiping Hu", "title": "More than Encoder: Introducing Transformer Decoder to Upsample", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General segmentation models downsample images and then upsample to restore\nresolution for pixel level prediction. In such schema, upsample technique is\nvital in maintaining information for better performance. In this paper, we\npresent a new upsample approach, Attention Upsample (AU), that could serve as\ngeneral upsample method and be incorporated into any segmentation model that\npossesses lateral connections. AU leverages pixel-level attention to model long\nrange dependency and global information for better reconstruction. It consists\nof Attention Decoder (AD) and bilinear upsample as residual connection to\ncomplement the upsampled features. AD adopts the idea of decoder from\ntransformer which upsamples features conditioned on local and detailed\ninformation from contracting path. Moreover, considering the extensive memory\nand computation cost of pixel-level attention, we further propose to use window\nattention scheme to restrict attention computation in local windows instead of\nglobal range. Incorporating window attention, we denote our decoder as Window\nAttention Decoder (WAD) and our upsample method as Window Attention Upsample\n(WAU). We test our method on classic U-Net structure with lateral connection to\ndeliver information from contracting path and achieve state-of-the-arts\nperformance on Synapse (80.30 DSC and 23.12 HD) and MSD Brain (74.75 DSC)\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 06:58:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Yijiang", ""], ["Cai", "Wentian", ""], ["Gao", "Ying", ""], ["Hu", "Xiping", ""]]}, {"id": "2106.10641", "submitter": "Zeyu Gao", "authors": "Zeyu Gao, Jiangbo Shi, Xianli Zhang, Yang Li, Haichuan Zhang, Jialun\n  Wu, Chunbao Wang, Deyu Meng, Chen Li", "title": "Nuclei Grading of Clear Cell Renal Cell Carcinoma in Histopathological\n  Image by Composite High-Resolution Network", "comments": "Accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The grade of clear cell renal cell carcinoma (ccRCC) is a critical prognostic\nfactor, making ccRCC nuclei grading a crucial task in RCC pathology analysis.\nComputer-aided nuclei grading aims to improve pathologists' work efficiency\nwhile reducing their misdiagnosis rate by automatically identifying the grades\nof tumor nuclei within histopathological images. Such a task requires precisely\nsegment and accurately classify the nuclei. However, most of the existing\nnuclei segmentation and classification methods can not handle the inter-class\nsimilarity property of nuclei grading, thus can not be directly applied to the\nccRCC grading task. In this paper, we propose a Composite High-Resolution\nNetwork for ccRCC nuclei grading. Specifically, we propose a segmentation\nnetwork called W-Net that can separate the clustered nuclei. Then, we recast\nthe fine-grained classification of nuclei to two cross-category classification\ntasks, based on two high-resolution feature extractors (HRFEs) which are\nproposed for learning these two tasks. The two HRFEs share the same backbone\nencoder with W-Net by a composite connection so that meaningful features for\nthe segmentation task can be inherited for the classification task. Last, a\nhead-fusion block is applied to generate the predicted label of each nucleus.\nFurthermore, we introduce a dataset for ccRCC nuclei grading, containing 1000\nimage patches with 70945 annotated nuclei. We demonstrate that our proposed\nmethod achieves state-of-the-art performance compared to existing methods on\nthis large ccRCC grading dataset.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 07:32:26 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gao", "Zeyu", ""], ["Shi", "Jiangbo", ""], ["Zhang", "Xianli", ""], ["Li", "Yang", ""], ["Zhang", "Haichuan", ""], ["Wu", "Jialun", ""], ["Wang", "Chunbao", ""], ["Meng", "Deyu", ""], ["Li", "Chen", ""]]}, {"id": "2106.10642", "submitter": "Aroof Aimen", "authors": "Aroof Aimen, Sahil Sidheekh, Narayanan C. Krishnan", "title": "Task Attended Meta-Learning for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning (ML) has emerged as a promising direction in learning models\nunder constrained resource settings like few-shot learning. The popular\napproaches for ML either learn a generalizable initial model or a generic\nparametric optimizer through episodic training. The former approaches leverage\nthe knowledge from a batch of tasks to learn an optimal prior. In this work, we\nstudy the importance of a batch for ML. Specifically, we first incorporate a\nbatch episodic training regimen to improve the learning of the generic\nparametric optimizer. We also hypothesize that the common assumption in batch\nepisodic training that each task in a batch has an equal contribution to\nlearning an optimal meta-model need not be true. We propose to weight the tasks\nin a batch according to their \"importance\" in improving the meta-model's\nlearning. To this end, we introduce a training curriculum motivated by\nselective focus in humans, called task attended meta-training, to weight the\ntasks in a batch. Task attention is a standalone module that can be integrated\nwith any batch episodic training regimen. The comparisons of the models with\ntheir non-task-attended counterparts on complex datasets like miniImageNet and\ntieredImageNet validate its effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 07:34:37 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Aimen", "Aroof", ""], ["Sidheekh", "Sahil", ""], ["Krishnan", "Narayanan C.", ""]]}, {"id": "2106.10649", "submitter": "Naveed Akhtar Dr.", "authors": "Mohammad A. A. K. Jalwana, Naveed Akhtar, Mohammed Bennamoun, Ajmal\n  Mian", "title": "CAMERAS: Enhanced Resolution And Sanity preserving Class Activation\n  Mapping for image saliency", "comments": "IEEE CVPR 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backpropagation image saliency aims at explaining model predictions by\nestimating model-centric importance of individual pixels in the input. However,\nclass-insensitivity of the earlier layers in a network only allows saliency\ncomputation with low resolution activation maps of the deeper layers, resulting\nin compromised image saliency. Remedifying this can lead to sanity failures. We\npropose CAMERAS, a technique to compute high-fidelity backpropagation saliency\nmaps without requiring any external priors and preserving the map sanity. Our\nmethod systematically performs multi-scale accumulation and fusion of the\nactivation maps and backpropagated gradients to compute precise saliency maps.\nFrom accurate image saliency to articulation of relative importance of input\nfeatures for different models, and precise discrimination between model\nperception of visually similar objects, our high-resolution mapping offers\nmultiple novel insights into the black-box deep visual models, which are\npresented in the paper. We also demonstrate the utility of our saliency maps in\nadversarial setup by drastically reducing the norm of attack signals by\nfocusing them on the precise regions identified by our maps. Our method also\ninspires new evaluation metrics and a sanity check for this developing research\ndirection. Code is available here https://github.com/VisMIL/CAMERAS\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 08:20:56 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Jalwana", "Mohammad A. A. K.", ""], ["Akhtar", "Naveed", ""], ["Bennamoun", "Mohammed", ""], ["Mian", "Ajmal", ""]]}, {"id": "2106.10651", "submitter": "Dennis N\\'u\\~nez-Fern\\'andez", "authors": "Carlos Rojas-Azabache, Karen Vilca-Janampa, Renzo Guerrero-Huayta,\n  Dennis N\\'u\\~nez-Fern\\'andez", "title": "Implementing a Detection System for COVID-19 based on Lung Ultrasound\n  Imaging and Deep Learning", "comments": "Beyond Fairness Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic started in China in December 2019 and quickly spread to\nseveral countries. The consequences of this pandemic are incalculable, causing\nthe death of millions of people and damaging the global economy. To achieve\nlarge-scale control of this pandemic, fast tools for detection and treatment of\npatients are needed. Thus, the demand for alternative tools for the diagnosis\nof COVID-19 has increased dramatically since accurated and automated tools are\nnot available. In this paper we present the ongoing work on a system for\nCOVID-19 detection using ultrasound imaging and using Deep Learning techniques.\nFurthermore, such a system is implemented on a Raspberry Pi to make it portable\nand easy to use in remote regions without an Internet connection.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 08:33:33 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rojas-Azabache", "Carlos", ""], ["Vilca-Janampa", "Karen", ""], ["Guerrero-Huayta", "Renzo", ""], ["N\u00fa\u00f1ez-Fern\u00e1ndez", "Dennis", ""]]}, {"id": "2106.10653", "submitter": "Xuhong Li", "authors": "Xuanyu Wu, Xuhong Li, Haoyi Xiong, Xiao Zhang, Siyu Huang, Dejing Dou", "title": "Practical Assessment of Generalization Performance Robustness for Deep\n  Networks via Contrastive Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training images with data transformations have been suggested as contrastive\nexamples to complement the testing set for generalization performance\nevaluation of deep neural networks (DNNs). In this work, we propose a practical\nframework ContRE (The word \"contre\" means \"against\" or \"versus\" in French.)\nthat uses Contrastive examples for DNN geneRalization performance Estimation.\nSpecifically, ContRE follows the assumption in contrastive learning that robust\nDNN models with good generalization performance are capable of extracting a\nconsistent set of features and making consistent predictions from the same\nimage under varying data transformations. Incorporating with a set of\nrandomized strategies for well-designed data transformations over the training\nset, ContRE adopts classification errors and Fisher ratios on the generated\ncontrastive examples to assess and analyze the generalization performance of\ndeep models in complement with a testing set. To show the effectiveness and the\nefficiency of ContRE, extensive experiments have been done using various DNN\nmodels on three open source benchmark datasets with thorough ablation studies\nand applicability analyses. Our experiment results confirm that (1) behaviors\nof deep models on contrastive examples are strongly correlated to what on the\ntesting set, and (2) ContRE is a robust measure of generalization performance\ncomplementing to the testing set in various settings.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 08:46:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wu", "Xuanyu", ""], ["Li", "Xuhong", ""], ["Xiong", "Haoyi", ""], ["Zhang", "Xiao", ""], ["Huang", "Siyu", ""], ["Dou", "Dejing", ""]]}, {"id": "2106.10658", "submitter": "Fenglin Liu", "authors": "Fenglin Liu, Meng Gao, Tianhao Zhang, Yuexian Zou", "title": "Exploring Semantic Relationships for Unpaired Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, image captioning has aroused great interest in both academic and\nindustrial worlds. Most existing systems are built upon large-scale datasets\nconsisting of image-sentence pairs, which, however, are time-consuming to\nconstruct. In addition, even for the most advanced image captioning systems, it\nis still difficult to realize deep image understanding. In this work, we\nachieve unpaired image captioning by bridging the vision and the language\ndomains with high-level semantic information. The motivation stems from the\nfact that the semantic concepts with the same modality can be extracted from\nboth images and descriptions. To further improve the quality of captions\ngenerated by the model, we propose the Semantic Relationship Explorer, which\nexplores the relationships between semantic concepts for better understanding\nof the image. Extensive experiments on MSCOCO dataset show that we can generate\ndesirable captions without paired datasets. Furthermore, the proposed approach\nboosts five strong baselines under the paired setting, where the most\nsignificant improvement in CIDEr score reaches 8%, demonstrating that it is\neffective and generalizes well to a wide range of models.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 09:10:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Liu", "Fenglin", ""], ["Gao", "Meng", ""], ["Zhang", "Tianhao", ""], ["Zou", "Yuexian", ""]]}, {"id": "2106.10681", "submitter": "Jiapeng Wang", "authors": "Jiapeng Wang, Tianwei Wang, Guozhi Tang, Lianwen Jin, Weihong Ma, Kai\n  Ding, Yichao Huang", "title": "Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for\n  Visual Information Extraction using Sequences", "comments": "IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual information extraction (VIE) has attracted increasing attention in\nrecent years. The existing methods usually first organized optical character\nrecognition (OCR) results into plain texts and then utilized token-level entity\nannotations as supervision to train a sequence tagging model. However, it\nexpends great annotation costs and may be exposed to label confusion, and the\nOCR errors will also significantly affect the final performance. In this paper,\nwe propose a unified weakly-supervised learning framework called TCPN (Tag,\nCopy or Predict Network), which introduces 1) an efficient encoder to\nsimultaneously model the semantic and layout information in 2D OCR results; 2)\na weakly-supervised training strategy that utilizes only key information\nsequences as supervision; and 3) a flexible and switchable decoder which\ncontains two inference modes: one (Copy or Predict Mode) is to output key\ninformation sequences of different categories by copying a token from the input\nor predicting one in each time step, and the other (Tag Mode) is to directly\ntag the input sequence in a single forward pass. Our method shows new\nstate-of-the-art performance on several public benchmarks, which fully proves\nits effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 11:56:46 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Jiapeng", ""], ["Wang", "Tianwei", ""], ["Tang", "Guozhi", ""], ["Jin", "Lianwen", ""], ["Ma", "Weihong", ""], ["Ding", "Kai", ""], ["Huang", "Yichao", ""]]}, {"id": "2106.10683", "submitter": "Jia-Xin Zhuang", "authors": "Yuqiao Xian, Jia-Xin Zhuang, Fufu Yu", "title": "Solution for Large-scale Long-tailed Recognition with Noisy Labels", "comments": "3 pages", "journal-ref": "CVPR 2021 AliProducts Challenge: CVPR 2021 AliProducts\n  Challenge:Large-scale Product Recognition, Technical Report", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a technical report for CVPR 2021 AliProducts Challenge. AliProducts\nChallenge is a competition proposed for studying the large-scale and\nfine-grained commodity image recognition problem encountered by worldleading\necommerce companies. The large-scale product recognition simultaneously meets\nthe challenge of noisy annotations, imbalanced (long-tailed) data distribution\nand fine-grained classification. In our solution, we adopt stateof-the-art\nmodel architectures of both CNNs and Transformer, including ResNeSt,\nEfficientNetV2, and DeiT. We found that iterative data cleaning, classifier\nweight normalization, high-resolution finetuning, and test time augmentation\nare key components to improve the performance of training with the noisy and\nimbalanced dataset. Finally, we obtain 6.4365% mean class error rate in the\nleaderboard with our ensemble model.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 12:09:38 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xian", "Yuqiao", ""], ["Zhuang", "Jia-Xin", ""], ["Yu", "Fufu", ""]]}, {"id": "2106.10686", "submitter": "Liulei Li", "authors": "Tianfei Zhou, Liulei Li, Gustav Bredell, Jianwu Li, Ender Konukoglu", "title": "Quality-Aware Memory Network for Interactive Volumetric Image\n  Segmentation", "comments": "MICCAI 2021. Code: https://github.com/0liliulei/Mem3D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent progress of automatic medical image segmentation techniques,\nfully automatic results usually fail to meet the clinical use and typically\nrequire further refinement. In this work, we propose a quality-aware memory\nnetwork for interactive segmentation of 3D medical images. Provided by user\nguidance on an arbitrary slice, an interaction network is firstly employed to\nobtain an initial 2D segmentation. The quality-aware memory network\nsubsequently propagates the initial segmentation estimation bidirectionally\nover the entire volume. Subsequent refinement based on additional user guidance\non other slices can be incorporated in the same manner. To further facilitate\ninteractive segmentation, a quality assessment module is introduced to suggest\nthe next slice to segment based on the current segmentation quality of each\nslice. The proposed network has two appealing characteristics: 1) The\nmemory-augmented network offers the ability to quickly encode past segmentation\ninformation, which will be retrieved for the segmentation of other slices; 2)\nThe quality assessment module enables the model to directly estimate the\nqualities of segmentation predictions, which allows an active learning paradigm\nwhere users preferentially label the lowest-quality slice for multi-round\nrefinement. The proposed network leads to a robust interactive segmentation\nengine, which can generalize well to various types of user annotations (e.g.,\nscribbles, boxes). Experimental results on various medical datasets demonstrate\nthe superiority of our approach in comparison with existing techniques.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 12:34:19 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 07:52:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhou", "Tianfei", ""], ["Li", "Liulei", ""], ["Bredell", "Gustav", ""], ["Li", "Jianwu", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2106.10689", "submitter": "Peng Wang", "authors": "Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura,\n  Wenping Wang", "title": "NeuS: Learning Neural Implicit Surfaces by Volume Rendering for\n  Multi-view Reconstruction", "comments": "22 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a novel neural surface reconstruction method, called NeuS, for\nreconstructing objects and scenes with high fidelity from 2D image inputs.\nExisting neural surface reconstruction approaches, such as DVR and IDR, require\nforeground mask as supervision, easily get trapped in local minima, and\ntherefore struggle with the reconstruction of objects with severe\nself-occlusion or thin structures. Meanwhile, recent neural methods for novel\nview synthesis, such as NeRF and its variants, use volume rendering to produce\na neural scene representation with robustness of optimization, even for highly\ncomplex objects. However, extracting high-quality surfaces from this learned\nimplicit representation is difficult because there are not sufficient surface\nconstraints in the representation. In NeuS, we propose to represent a surface\nas the zero-level set of a signed distance function (SDF) and develop a new\nvolume rendering method to train a neural SDF representation. We observe that\nthe conventional volume rendering method causes inherent geometric errors (i.e.\nbias) for surface reconstruction, and therefore propose a new formulation that\nis free of bias in the first order of approximation, thus leading to more\naccurate surface reconstruction even without the mask supervision. Experiments\non the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the\nstate-of-the-arts in high-quality surface reconstruction, especially for\nobjects and scenes with complex structures and self-occlusion.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 12:59:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Peng", ""], ["Liu", "Lingjie", ""], ["Liu", "Yuan", ""], ["Theobalt", "Christian", ""], ["Komura", "Taku", ""], ["Wang", "Wenping", ""]]}, {"id": "2106.10698", "submitter": "Pranesh Kulkarni", "authors": "Pranesh Kulkarni, Atharva Karwande, Tejas Kolhe, Soham Kamble, Akshay\n  Joshi, Medha Wyawahare", "title": "Plant Disease Detection Using Image Processing and Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the important and tedious task in agricultural practices is the\ndetection of the disease on crops. It requires huge time as well as skilled\nlabor. This paper proposes a smart and efficient technique for detection of\ncrop disease which uses computer vision and machine learning techniques. The\nproposed system is able to detect 20 different diseases of 5 common plants with\n93% accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 14:11:24 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kulkarni", "Pranesh", ""], ["Karwande", "Atharva", ""], ["Kolhe", "Tejas", ""], ["Kamble", "Soham", ""], ["Joshi", "Akshay", ""], ["Wyawahare", "Medha", ""]]}, {"id": "2106.10705", "submitter": "Ping Liu", "authors": "Ping Liu", "title": "Automated Deepfake Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to utilize Automated Machine Learning to\nautomatically search architecture for deepfake detection. Unlike previous\nworks, our method benefits from the superior capability of deep learning while\nrelieving us from the high labor cost in the manual network design process. It\nis experimentally proved that our proposed method not only outperforms previous\nnon-deep learning methods but achieves comparable or even better prediction\naccuracy compared to previous deep learning methods. To improve the generality\nof our method, especially when training data and testing data are manipulated\nby different methods, we propose a multi-task strategy in our network learning\nprocess, making it estimate potential manipulation regions in given samples as\nwell as predict whether the samples are real. Comparing to previous works using\nsimilar strategies, our method depends much less on prior knowledge, such as no\nneed to know which manipulation method is utilized and whether it is utilized\nalready. Extensive experimental results on two benchmark datasets demonstrate\nthe effectiveness of our proposed method on deepfake detection.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 14:48:50 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Liu", "Ping", ""]]}, {"id": "2106.10718", "submitter": "Junlin Han", "authors": "Junlin Han, Mehrdad Shoeiby, Tim Malthus, Elizabeth Botha, Janet\n  Anstee, Saeed Anwar, Ran Wei, Mohammad Ali Armin, Hongdong Li, Lars Petersson", "title": "Underwater Image Restoration via Contrastive Learning and a Real-world\n  Dataset", "comments": "In submission, code/dataset are at https://github.com/JunlinHan/CWR.\n  arXiv admin note: text overlap with arXiv:2103.09697", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater image restoration is of significant importance in unveiling the\nunderwater world. Numerous techniques and algorithms have been developed in the\npast decades. However, due to fundamental difficulties associated with\nimaging/sensing, lighting, and refractive geometric distortions, in capturing\nclear underwater images, no comprehensive evaluations have been conducted of\nunderwater image restoration. To address this gap, we have constructed a\nlarge-scale real underwater image dataset, dubbed `HICRD' (Heron Island Coral\nReef Dataset), for the purpose of benchmarking existing methods and supporting\nthe development of new deep-learning based methods. We employ accurate water\nparameter (diffuse attenuation coefficient) in generating reference images.\nThere are 2000 reference restored images and 6003 original underwater images in\nthe unpaired training set. Further, we present a novel method for underwater\nimage restoration based on unsupervised image-to-image translation framework.\nOur proposed method leveraged contrastive learning and generative adversarial\nnetworks to maximize the mutual information between raw and restored images.\nExtensive experiments with comparisons to recent approaches further demonstrate\nthe superiority of our proposed method. Our code and dataset are publicly\navailable at GitHub.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 16:06:26 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Han", "Junlin", ""], ["Shoeiby", "Mehrdad", ""], ["Malthus", "Tim", ""], ["Botha", "Elizabeth", ""], ["Anstee", "Janet", ""], ["Anwar", "Saeed", ""], ["Wei", "Ran", ""], ["Armin", "Mohammad Ali", ""], ["Li", "Hongdong", ""], ["Petersson", "Lars", ""]]}, {"id": "2106.10731", "submitter": "Zhun Zhong", "authors": "Zhun Zhong, Enrico Fini, Subhankar Roy, Zhiming Luo, Elisa Ricci, Nicu\n  Sebe", "title": "Neighborhood Contrastive Learning for Novel Class Discovery", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address Novel Class Discovery (NCD), the task of unveiling\nnew classes in a set of unlabeled samples given a labeled dataset with known\nclasses. We exploit the peculiarities of NCD to build a new framework, named\nNeighborhood Contrastive Learning (NCL), to learn discriminative\nrepresentations that are important to clustering performance. Our contribution\nis twofold. First, we find that a feature extractor trained on the labeled set\ngenerates representations in which a generic query sample and its neighbors are\nlikely to share the same class. We exploit this observation to retrieve and\naggregate pseudo-positive pairs with contrastive learning, thus encouraging the\nmodel to learn more discriminative representations. Second, we notice that most\nof the instances are easily discriminated by the network, contributing less to\nthe contrastive loss. To overcome this issue, we propose to generate hard\nnegatives by mixing labeled and unlabeled samples in the feature space. We\nexperimentally demonstrate that these two ingredients significantly contribute\nto clustering performance and lead our model to outperform state-of-the-art\nmethods by a large margin (e.g., clustering accuracy +13% on CIFAR-100 and +8%\non ImageNet).\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 17:34:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhong", "Zhun", ""], ["Fini", "Enrico", ""], ["Roy", "Subhankar", ""], ["Luo", "Zhiming", ""], ["Ricci", "Elisa", ""], ["Sebe", "Nicu", ""]]}, {"id": "2106.10733", "submitter": "Armstrong Aboah", "authors": "Armstrong Aboah, Michael Boeding, Yaw Adu-Gyamfi", "title": "Mobile Sensing for Multipurpose Applications in Transportation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Routine and consistent data collection is required to address contemporary\ntransportation issues.The cost of data collection increases significantly when\nsophisticated machines are used to collect data. Due to this constraint, State\nDepartments of Transportation struggles to collect consistent data for\nanalyzing and resolving transportation problems in a timely manner. Recent\nadvancements in the sensors integrated into smartphones have resulted in a more\naffordable method of data collection.The primary objective of this study is to\ndevelop and implement a smartphone application for data collection.The\ncurrently designed app consists of three major modules: a frontend graphical\nuser interface (GUI), a sensor module, and a backend module. While the frontend\nuser interface enables interaction with the app, the sensor modules collect\nrelevant data such as video and accelerometer readings while the app is in use.\nThe backend, on the other hand, is made up of firebase storage, which is used\nto store the gathered data.In comparison to other developed apps for collecting\npavement information, this current app is not overly reliant on the internet\nenabling the app to be used in areas of restricted internet access.The\ndeveloped application was evaluated by collecting data on the i70W highway\nconnecting Columbia, Missouri, and Kansas City, Missouri.The data was analyzed\nfor a variety of purposes, including calculating the International Roughness\nIndex (IRI), identifying pavement distresses, and understanding driver's\nbehaviour and environment .The results of the application indicate that the\ndata collected by the app is of high quality.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 17:56:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Aboah", "Armstrong", ""], ["Boeding", "Michael", ""], ["Adu-Gyamfi", "Yaw", ""]]}, {"id": "2106.10766", "submitter": "Satyaki Chakraborty", "authors": "Satyaki Chakraborty, Martial Hebert", "title": "Learning to Track Object Position through Occlusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Occlusion is one of the most significant challenges encountered by object\ndetectors and trackers. While both object detection and tracking has received a\nlot of attention in the past, most existing methods in this domain do not\ntarget detecting or tracking objects when they are occluded. However, being\nable to detect or track an object of interest through occlusion has been a long\nstanding challenge for different autonomous tasks. Traditional methods that\nemploy visual object trackers with explicit occlusion modeling experience drift\nand make several fundamental assumptions about the data. We propose to address\nthis with a `tracking-by-detection` approach that builds upon the success of\nregion based video object detectors. Our video level object detector uses a\nnovel recurrent computational unit at its core that enables long term\npropagation of object features even under occlusion. Finally, we compare our\napproach with existing state-of-the-art video object detectors and show that\nour approach achieves superior results on a dataset of furniture assembly\nvideos collected from the internet, where small objects like screws, nuts, and\nbolts often get occluded from the camera viewpoint.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 22:29:46 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chakraborty", "Satyaki", ""], ["Hebert", "Martial", ""]]}, {"id": "2106.10777", "submitter": "Mengyu Dai", "authors": "Mengyu Dai and Haibin Hang", "title": "Manifold Matching via Deep Metric Learning for Generative Modeling", "comments": "Accepted to ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a manifold matching approach to generative models which includes a\ndistribution generator (or data generator) and a metric generator. In our\nframework, we view the real data set as some manifold embedded in a\nhigh-dimensional Euclidean space. The distribution generator aims at generating\nsamples that follow some distribution condensed around the real data manifold.\nIt is achieved by matching two sets of points using their geometric shape\ndescriptors, such as centroid and $p$-diameter, with learned distance metric;\nthe metric generator utilizes both real data and generated samples to learn a\ndistance metric which is close to some intrinsic geodesic distance on the real\ndata manifold. The produced distance metric is further used for manifold\nmatching. The two networks are learned simultaneously during the training\nprocess. We apply the approach on both unsupervised and supervised learning\ntasks: in unconditional image generation task, the proposed method obtains\ncompetitive results compared with existing generative models; in\nsuper-resolution task, we incorporate the framework in perception-based models\nand improve visual qualities by producing samples with more natural textures.\nBoth theoretical analysis and real data experiments demonstrate the feasibility\nand effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 23:25:01 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 03:36:54 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Dai", "Mengyu", ""], ["Hang", "Haibin", ""]]}, {"id": "2106.10795", "submitter": "Ran Lu", "authors": "Ran Lu, Aleksandar Zlateski and H. Sebastian Seung", "title": "Large-scale image segmentation based on distributed clustering\n  algorithms", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many approaches to 3D image segmentation are based on hierarchical clustering\nof supervoxels into image regions. Here we describe a distributed algorithm\ncapable of handling a tremendous number of supervoxels. The algorithm works\nrecursively, the regions are divided into chunks that are processed\nindependently in parallel by multiple workers. At each round of the recursive\nprocedure, the chunk size in all dimensions are doubled until a single chunk\nencompasses the entire image. The final result is provably independent of the\nchunking scheme, and the same as if the entire image were processed without\ndivision into chunks. This is nontrivial because a pair of adjacent regions is\nscored by some statistical property (e.g. mean or median) of the affinities at\nthe interface, and the interface may extend over arbitrarily many chunks. The\ntrick is to delay merge decisions for regions that touch chunk boundaries, and\nonly complete them in a later round after the regions are fully contained\nwithin a chunk. We demonstrate the algorithm by clustering an affinity graph\nwith over 1.5 trillion edges between 135 billion supervoxels derived from a 3D\nelectron microscopic brain image.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 01:11:49 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lu", "Ran", ""], ["Zlateski", "Aleksandar", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "2106.10811", "submitter": "Yizhak Ben-Shabat", "authors": "Yizhak Ben-Shabat, Chamin Hewa Koneputugodage, Stephen Gould", "title": "DiGS : Divergence guided shape implicit neural representation for\n  unoriented point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural shape representations have recently shown to be effective in shape\nanalysis and reconstruction tasks. Existing neural network methods require\npoint coordinates and corresponding normal vectors to learn the implicit level\nsets of the shape. Normal vectors are often not provided as raw data,\ntherefore, approximation and reorientation are required as pre-processing\nstages, both of which can introduce noise. In this paper, we propose a\ndivergence guided shape representation learning approach that does not require\nnormal vectors as input. We show that incorporating a soft constraint on the\ndivergence of the distance function favours smooth solutions that reliably\norients gradients to match the unknown normal at each point, in some cases even\nbetter than approaches that use ground truth normal vectors directly.\nAdditionally, we introduce a novel geometric initialization method for\nsinusoidal shape representation networks that further improves convergence to\nthe desired solution. We evaluate the effectiveness of our approach on the task\nof surface reconstruction and show state-of-the-art performance compared to\nother unoriented methods and on-par performance compared to oriented methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 02:10:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ben-Shabat", "Yizhak", ""], ["Koneputugodage", "Chamin Hewa", ""], ["Gould", "Stephen", ""]]}, {"id": "2106.10812", "submitter": "Guoqiang Wei", "authors": "Guoqiang Wei, Cuiling Lan, Wenjun Zeng, Zhibo Chen", "title": "ToAlign: Task-oriented Alignment for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptive classification intends to improve\ntheclassification performance on unlabeled target domain. To alleviate the\nadverse effect of domain shift, many approaches align the source and target\ndomains in the feature space. However, a feature is usually taken as a whole\nfor alignment without explicitly making domain alignment proactively serve the\nclassification task, leading to sub-optimal solution. What sub-feature should\nbe aligned for better adaptation is under-explored. In this paper, we propose\nan effective Task-oriented Alignment (ToAlign) for unsupervised domain\nadaptation (UDA). We study what features should be aligned across domains and\npropose to make the domain alignment proactively serve classification by\nperforming feature decomposition and alignment under the guidance of the prior\nknowledge induced from the classification taskitself. Particularly, we\nexplicitly decompose a feature in the source domain intoa\ntask-related/discriminative feature that should be aligned, and a\ntask-irrelevant feature that should be avoided/ignored, based on the\nclassification meta-knowledge. Extensive experimental results on various\nbenchmarks (e.g., Office-Home, Visda-2017, and DomainNet) under different\ndomain adaptation settings demonstrate theeffectiveness of ToAlign which helps\nachieve the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 02:17:48 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wei", "Guoqiang", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""]]}, {"id": "2106.10815", "submitter": "Limin Wang", "authors": "Yao Teng, Limin Wang", "title": "Structured Sparse R-CNN for Direct Scene Graph Generation", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scene graph generation (SGG) is to detect entity pairs with their relations\nin an image. Existing SGG approaches often use multi-stage pipelines to\ndecompose this task into object detection, relation graph construction, and\ndense or dense-to-sparse relation prediction. Instead, from a perspective on\nSGG as a direct set prediction, this paper presents a simple, sparse, and\nunified framework for relation detection, termed as Structured Sparse R-CNN.\nThe key to our method is a set of learnable triplet queries and structured\ntriplet detectors which could be jointly optimized from the training set in an\nend-to-end manner. Specifically, the triplet queries encode the general prior\nfor entity pair locations, categories, and their relations, and provide an\ninitial guess of relation detection for subsequent refinement. The triplet\ndetector presents a cascaded dynamic head design to progressively refine the\nresults of relation detection. In addition, to relieve the training difficulty\nof Structured Sparse R-CNN, we propose a relaxed and enhanced training strategy\nbased on knowledge distillation from a Siamese Sparse R-CNN. We also propose\nadaptive focusing parameter and average logit approach for imbalance data\ndistribution. We perform experiments on two benchmarks: Visual Genome and Open\nImages, and the results demonstrate that our method achieves the\nstate-of-the-art performance. Meanwhile, we perform in-depth ablation studies\nto provide insights on our structured modeling in triplet detector design and\ntraining strategies.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 02:24:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Teng", "Yao", ""], ["Wang", "Limin", ""]]}, {"id": "2106.10823", "submitter": "Rui Qian", "authors": "Rui Qian, Xin Lai, Xirong Li", "title": "3D Object Detection for Autonomous Driving: A Survey", "comments": "3D object detection, Autonomous driving, Point clouds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous driving is regarded as one of the most promising remedies to\nshield human beings from severe crashes. To this end, 3D object detection\nserves as the core basis of such perception system especially for the sake of\npath planning, motion prediction, collision avoidance, etc. Generally, stereo\nor monocular images with corresponding 3D point clouds are already standard\nlayout for 3D object detection, out of which point clouds are increasingly\nprevalent with accurate depth information being provided. Despite existing\nefforts, 3D object detection on point clouds is still in its infancy due to\nhigh sparseness and irregularity of point clouds by nature, misalignment view\nbetween camera view and LiDAR bird's eye of view for modality synergies,\nocclusions and scale variations at long distances, etc. Recently, profound\nprogress has been made in 3D object detection, with a large body of literature\nbeing investigated to address this vision task. As such, we present a\ncomprehensive review of the latest progress in this field covering all the main\ntopics including sensors, fundamentals, and the recent state-of-the-art\ndetection methods with their pros and cons. Furthermore, we introduce metrics\nand provide quantitative comparisons on popular public datasets. The avenues\nfor future work are going to be judiciously identified after an in-deep\nanalysis of the surveyed works. Finally, we conclude this paper.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 03:17:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Qian", "Rui", ""], ["Lai", "Xin", ""], ["Li", "Xirong", ""]]}, {"id": "2106.10829", "submitter": "Yuanhao Zhai", "authors": "Yuanhao Zhai, Le Wang, David Doermann, Junsong Yuan", "title": "Two-Stream Consensus Network: Submission to HACS Challenge 2021\n  Weakly-Supervised Learning Track", "comments": "Second place solution to the HACS Weakly-Supervised Temporal Action\n  Localization Challenge 2021. arXiv admin note: text overlap with\n  arXiv:2010.11594", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This technical report presents our solution to the HACS Temporal Action\nLocalization Challenge 2021, Weakly-Supervised Learning Track. The goal of\nweakly-supervised temporal action localization is to temporally locate and\nclassify action of interest in untrimmed videos given only video-level labels.\nWe adopt the two-stream consensus network (TSCN) as the main framework in this\nchallenge. The TSCN consists of a two-stream base model training procedure and\na pseudo ground truth learning procedure. The base model training encourages\nthe model to predict reliable predictions based on single modality (i.e., RGB\nor optical flow), based on the fusion of which a pseudo ground truth is\ngenerated and in turn used as supervision to train the base models. On the HACS\nv1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our\nmethod achieves 22.20% on the validation set and 21.68% on the testing set in\nterms of average mAP. Our solution ranked the 2rd in this challenge, and we\nhope our method can serve as a baseline for future academic research.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 03:36:36 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 06:35:57 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhai", "Yuanhao", ""], ["Wang", "Le", ""], ["Doermann", "David", ""], ["Yuan", "Junsong", ""]]}, {"id": "2106.10834", "submitter": "Yingying Hua", "authors": "Yingying Hua, Daichi Zhang, Pengju Wang, Shiming Ge", "title": "Interpretable Face Manipulation Detection via Feature Whitening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why should we trust the detections of deep neural networks for manipulated\nfaces? Understanding the reasons is important for users in improving the\nfairness, reliability, privacy and trust of the detection models. In this work,\nwe propose an interpretable face manipulation detection approach to achieve the\ntrustworthy and accurate inference. The approach could make the face\nmanipulation detection process transparent by embedding the feature whitening\nmodule. This module aims to whiten the internal working mechanism of deep\nnetworks through feature decorrelation and feature constraint. The experimental\nresults demonstrate that our proposed approach can strike a balance between the\ndetection accuracy and the model interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 03:51:43 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hua", "Yingying", ""], ["Zhang", "Daichi", ""], ["Wang", "Pengju", ""], ["Ge", "Shiming", ""]]}, {"id": "2106.10836", "submitter": "Yuya Senzaki", "authors": "Yuya Senzaki, Christian Hamelain", "title": "Active Learning for Deep Neural Networks on Edge Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  When dealing with deep neural network (DNN) applications on edge devices,\ncontinuously updating the model is important. Although updating a model with\nreal incoming data is ideal, using all of them is not always feasible due to\nlimits, such as labeling and communication costs. Thus, it is necessary to\nfilter and select the data to use for training (i.e., active learning) on the\ndevice. In this paper, we formalize a practical active learning problem for\nDNNs on edge devices and propose a general task-agnostic framework to tackle\nthis problem, which reduces it to a stream submodular maximization. This\nframework is light enough to be run with low computational resources, yet\nprovides solutions whose quality is theoretically guaranteed thanks to the\nsubmodular property. Through this framework, we can configure data selection\ncriteria flexibly, including using methods proposed in previous active learning\nstudies. We evaluate our approach on both classification and object detection\ntasks in a practical setting to simulate a real-life scenario. The results of\nour study show that the proposed framework outperforms all other methods in\nboth tasks, while running at a practical speed on real devices.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 03:55:33 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Senzaki", "Yuya", ""], ["Hamelain", "Christian", ""]]}, {"id": "2106.10846", "submitter": "Guizhong Liu", "authors": "Jianyi Li and Guizhong Liu", "title": "Trainable Class Prototypes for Few-Shot Learning", "comments": "8 pages, 2 figures,and 3 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2008.09942", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Metric learning is a widely used method for few shot learning in which the\nquality of prototypes plays a key role in the algorithm. In this paper we\npropose the trainable prototypes for distance measure instead of the artificial\nones within the meta-training and task-training framework. Also to avoid the\ndisadvantages that the episodic meta-training brought, we adopt non-episodic\nmeta-training based on self-supervised learning. Overall we solve the few-shot\ntasks in two phases: meta-training a transferable feature extractor via\nself-supervised learning and training the prototypes for metric classification.\nIn addition, the simple attention mechanism is used in both meta-training and\ntask-training. Our method achieves state-of-the-art performance in a variety of\nestablished few-shot tasks on the standard few-shot visual classification\ndataset, with about 20% increase compared to the available unsupervised\nfew-shot learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 04:19:56 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Jianyi", ""], ["Liu", "Guizhong", ""]]}, {"id": "2106.10850", "submitter": "Ayman Mukhaimar Mr", "authors": "Ayman Mukhaimar, Ruwan Tennakoon, Chow Yin Lai, Reza Hoseinnezhad,\n  AlirezaBab-Hadiashar", "title": "Robust Pooling through the Data Mode", "comments": "under consideration at Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of learning from point cloud data is always challenging due to the\noften occurrence of noise and outliers in the data. Such data inaccuracies can\nsignificantly influence the performance of state-of-the-art deep learning\nnetworks and their ability to classify or segment objects. While there are some\nrobust deep learning approaches, they are computationally too expensive for\nreal-time applications. This paper proposes a deep learning solution that\nincludes a novel robust pooling layer which greatly enhances network robustness\nand performs significantly faster than state-of-the-art approaches. The\nproposed pooling layer looks for data a mode/cluster using two methods, RANSAC,\nand histogram, as clusters are indicative of models. We tested the pooling\nlayer into frameworks such as Point-based and graph-based neural networks, and\nthe tests showed enhanced robustness as compared to robust state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 04:35:24 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mukhaimar", "Ayman", ""], ["Tennakoon", "Ruwan", ""], ["Lai", "Chow Yin", ""], ["Hoseinnezhad", "Reza", ""], ["AlirezaBab-Hadiashar", "", ""]]}, {"id": "2106.10852", "submitter": "Swati .", "authors": "Swati Jindal, Xin Eric Wang", "title": "CUDA-GR: Controllable Unsupervised Domain Adaptation for Gaze\n  Redirection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of gaze redirection is to manipulate the gaze in an image to the\ndesired direction. However, existing methods are inadequate in generating\nperceptually reasonable images. Advancement in generative adversarial networks\nhas shown excellent results in generating photo-realistic images. Though, they\nstill lack the ability to provide finer control over different image\nattributes. To enable such fine-tuned control, one needs to obtain ground truth\nannotations for the training data which can be very expensive. In this paper,\nwe propose an unsupervised domain adaptation framework, called CUDA-GR, that\nlearns to disentangle gaze representations from the labeled source domain and\ntransfers them to an unlabeled target domain. Our method enables fine-grained\ncontrol over gaze directions while preserving the appearance information of the\nperson. We show that the generated image-labels pairs in the target domain are\neffective in knowledge transfer and can boost the performance of the downstream\ntasks. Extensive experiments on the benchmarking datasets show that the\nproposed method can outperform state-of-the-art techniques in both quantitative\nand qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 04:39:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Jindal", "Swati", ""], ["Wang", "Xin Eric", ""]]}, {"id": "2106.10859", "submitter": "Ching Yu Hsu", "authors": "Ching-Yu Hsu, Cheng Sun, Hwann-Tzong Chen", "title": "Moving in a 360 World: Synthesizing Panoramic Parallaxes from a Single\n  Panorama", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Omnidirectional Neural Radiance Fields (OmniNeRF), the first\nmethod to the application of parallax-enabled novel panoramic view synthesis.\nRecent works for novel view synthesis focus on perspective images with limited\nfield-of-view and require sufficient pictures captured in a specific condition.\nConversely, OmniNeRF can generate panorama images for unknown viewpoints given\na single equirectangular image as training data. To this end, we propose to\naugment the single RGB-D panorama by projecting back and forth between a 3D\nworld and different 2D panoramic coordinates at different virtual camera\npositions. By doing so, we are able to optimize an Omnidirectional Neural\nRadiance Field with visible pixels collecting from omnidirectional viewing\nangles at a fixed center for the estimation of new viewing angles from varying\ncamera positions. As a result, the proposed OmniNeRF achieves convincing\nrenderings of novel panoramic views that exhibit the parallax effect. We\nshowcase the effectiveness of each of our proposals on both synthetic and\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 05:08:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hsu", "Ching-Yu", ""], ["Sun", "Cheng", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "2106.10875", "submitter": "Rina Buoy", "authors": "Rina Buoy and Sokchea Kor and Nguonly Taing", "title": "An End-to-End Khmer Optical Character Recognition using\n  Sequence-to-Sequence with Attention", "comments": "16 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an end-to-end deep convolutional recurrent neural network\nsolution for Khmer optical character recognition (OCR) task. The proposed\nsolution uses a sequence-to-sequence (Seq2Seq) architecture with attention\nmechanism. The encoder extracts visual features from an input text-line image\nvia layers of residual convolutional blocks and a layer of gated recurrent\nunits (GRU). The features are encoded in a single context vector and a sequence\nof hidden states which are fed to the decoder for decoding one character at a\ntime until a special end-of-sentence (EOS) token is reached. The attention\nmechanism allows the decoder network to adaptively select parts of the input\nimage while predicting a target character. The Seq2Seq Khmer OCR network was\ntrained on a large collection of computer-generated text-line images for seven\ncommon Khmer fonts. The proposed model's performance outperformed the\nstate-of-art Tesseract OCR engine for Khmer language on the 3000-images test\nset by achieving a character error rate (CER) of 1% vs 3%.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:18:23 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Buoy", "Rina", ""], ["Kor", "Sokchea", ""], ["Taing", "Nguonly", ""]]}, {"id": "2106.10876", "submitter": "Hao Tang", "authors": "Hao Tang, Nicu Sebe", "title": "Total Generate: Cycle in Cycle Generative Adversarial Networks for\n  Generating Human Faces, Hands, Bodies, and Natural Scenes", "comments": "Accepted to TMM, an extended version of a paper published in ACM MM\n  2019. arXiv admin note: substantial text overlap with arXiv:1908.00999", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and unified Cycle in Cycle Generative Adversarial Network\n(C2GAN) for generating human faces, hands, bodies, and natural scenes. Our\nproposed C2GAN is a cross-modal model exploring the joint exploitation of the\ninput image data and guidance data in an interactive manner. C2GAN contains two\ndifferent generators, i.e., an image-generation generator and a\nguidance-generation generator. Both generators are mutually connected and\ntrained in an end-to-end fashion and explicitly form three cycled subnets,\ni.e., one image generation cycle and two guidance generation cycles. Each cycle\naims at reconstructing the input domain and simultaneously produces a useful\noutput involved in the generation of another cycle. In this way, the cycles\nconstrain each other implicitly providing complementary information from both\nimage and guidance modalities and bringing an extra supervision gradient across\nthe cycles, facilitating a more robust optimization of the whole model.\nExtensive results on four guided image-to-image translation subtasks\ndemonstrate that the proposed C2GAN is effective in generating more realistic\nimages compared with state-of-the-art models. The code is available at\nhttps://github.com/Ha0Tang/C2GAN.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:20:16 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Tang", "Hao", ""], ["Sebe", "Nicu", ""]]}, {"id": "2106.10882", "submitter": "Ali Abedi", "authors": "Ali Abedi and Shehroz Khan", "title": "Affect-driven Engagement Measurement from Videos", "comments": "13 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In education and intervention programs, person's engagement has been\nidentified as a major factor in successful program completion. Automatic\nmeasurement of person's engagement provides useful information for instructors\nto meet program objectives and individualize program delivery. In this paper,\nwe present a novel approach for video-based engagement measurement in virtual\nlearning programs. We propose to use affect states, continuous values of\nvalence and arousal extracted from consecutive video frames, along with a new\nlatent affective feature vector and behavioral features for engagement\nmeasurement. Deep learning-based temporal, and traditional\nmachine-learning-based non-temporal models are trained and validated on\nframe-level, and video-level features, respectively. In addition to the\nconventional centralized learning, we also implement the proposed method in a\ndecentralized federated learning setting and study the effect of model\npersonalization in engagement measurement. We evaluated the performance of the\nproposed method on the only two publicly available video engagement measurement\ndatasets, DAiSEE and EmotiW, containing videos of students in online learning\nprograms. Our experiments show a state-of-the-art engagement level\nclassification accuracy of 63.3% and correctly classifying disengagement videos\nin the DAiSEE dataset and a regression mean squared error of 0.0673 on the\nEmotiW dataset. Our ablation study shows the effectiveness of incorporating\naffect states in engagement measurement. We interpret the findings from the\nexperimental results based on psychology concepts in the field of engagement.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:49:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Abedi", "Ali", ""], ["Khan", "Shehroz", ""]]}, {"id": "2106.10885", "submitter": "Haoran Zhao", "authors": "Haoran Zhao, Xin Sun, Junyu Dong, Zihe Dong and Qiong Li", "title": "Knowledge Distillation via Instance-level Sequence Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, distillation approaches are suggested to extract general knowledge\nfrom a teacher network to guide a student network. Most of the existing methods\ntransfer knowledge from the teacher network to the student via feeding the\nsequence of random mini-batches sampled uniformly from the data. Instead, we\nargue that the compact student network should be guided gradually using samples\nordered in a meaningful sequence. Thus, it can bridge the gap of feature\nrepresentation between the teacher and student network step by step. In this\nwork, we provide a curriculum learning knowledge distillation framework via\ninstance-level sequence learning. It employs the student network of the early\nepoch as a snapshot to create a curriculum for the student network's next\ntraining phase. We carry out extensive experiments on CIFAR-10, CIFAR-100, SVHN\nand CINIC-10 datasets. Compared with several state-of-the-art methods, our\nframework achieves the best performance with fewer iterations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:58:26 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhao", "Haoran", ""], ["Sun", "Xin", ""], ["Dong", "Junyu", ""], ["Dong", "Zihe", ""], ["Li", "Qiong", ""]]}, {"id": "2106.10887", "submitter": "Yixin Wang", "authors": "Yixin Wang, Zihao Lin, Jiang Tian, Zhongchao Shi, Yang Zhang, Jianping\n  Fan, Zhiqiang He", "title": "Confidence-Guided Radiology Report Generation", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging plays a pivotal role in diagnosis and treatment in clinical\npractice. Inspired by the significant progress in automatic image captioning,\nvarious deep learning (DL)-based architectures have been proposed for\ngenerating radiology reports for medical images. However, model uncertainty\n(i.e., model reliability/confidence on report generation) is still an\nunder-explored problem. In this paper, we propose a novel method to explicitly\nquantify both the visual uncertainty and the textual uncertainty for the task\nof radiology report generation. Such multi-modal uncertainties can sufficiently\ncapture the model confidence scores at both the report-level and the\nsentence-level, and thus they are further leveraged to weight the losses for\nachieving more comprehensive model optimization. Our experimental results have\ndemonstrated that our proposed method for model uncertainty characterization\nand estimation can provide more reliable confidence scores for radiology report\ngeneration, and our proposed uncertainty-weighted losses can achieve more\ncomprehensive model optimization and result in state-of-the-art performance on\na public radiology report dataset.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 07:02:12 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 01:53:55 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Yixin", ""], ["Lin", "Zihao", ""], ["Tian", "Jiang", ""], ["Shi", "Zhongchao", ""], ["Zhang", "Yang", ""], ["Fan", "Jianping", ""], ["He", "Zhiqiang", ""]]}, {"id": "2106.10889", "submitter": "Maedeh Sadat Fasihi", "authors": "Maedeh Sadat Fasihi (1) and Wasfy B. Mikhael (1) ((1) Department of\n  Electrical Engineering and Computer Science, University of Central Florida,\n  Orlando, FL)", "title": "Brain tumor grade classification Using LSTM Neural Networks with Domain\n  Pre-Transforms", "comments": "4 pages, 5 figures, MWSCAS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of image classification methodsheavily relies on the\nhigh-quality annotations, which are noteasily affordable, particularly for\nmedical data. To alleviate thislimitation, in this study, we propose a weakly\nsupervised imageclassification method based on combination of\nhand-craftedfeatures. We hypothesize that integration of these\nhand-craftedfeatures alongside Long short-term memory (LSTM) classifiercan\nreduce the adverse effects of weak labels in classificationaccuracy. Our\nproposed algorithm is based on selecting theappropriate domain representations\nof the data in Wavelet andDiscrete Cosine Transform (DCT) domains. This\ninformationis then fed into LSTM network to account for the sequentialnature of\nthe data. The proposed efficient, low dimensionalfeatures exploit the power of\nshallow deep learning modelsto achieve higher performance with lower\ncomputational cost.In order to show efficacy of the proposed strategy, we\nhaveexperimented classification of brain tumor grades and achievedthe state of\nthe art performance with the resolution of 256 x 256. We also conducted a\ncomprehensive set of experiments toanalyze the effect of each component on the\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 07:04:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fasihi", "Maedeh Sadat", ""], ["Mikhael", "Wasfy B.", ""]]}, {"id": "2106.10893", "submitter": "Yuwei Li", "authors": "Yuwei Li, Minye Wu, Yuyao Zhang, Lan Xu, Jingyi Yu", "title": "PIANO: A Parametric Hand Bone Model from Magnetic Resonance Imaging", "comments": "Accepted to IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand modeling is critical for immersive VR/AR, action understanding, or human\nhealthcare. Existing parametric models account only for hand shape, pose, or\ntexture, without modeling the anatomical attributes like bone, which is\nessential for realistic hand biomechanics analysis. In this paper, we present\nPIANO, the first parametric bone model of human hands from MRI data. Our PIANO\nmodel is biologically correct, simple to animate, and differentiable, achieving\nmore anatomically precise modeling of the inner hand kinematic structure in a\ndata-driven manner than the traditional hand models based on the outer surface\nonly. Furthermore, our PIANO model can be applied in neural network layers to\nenable training with a fine-grained semantic loss, which opens up the new task\nof data-driven fine-grained hand bone anatomic and semantic understanding from\nMRI or even RGB images. We make our model publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 07:21:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Yuwei", ""], ["Wu", "Minye", ""], ["Zhang", "Yuyao", ""], ["Xu", "Lan", ""], ["Yu", "Jingyi", ""]]}, {"id": "2106.10900", "submitter": "Xin Li", "authors": "Xin Li, Wenjie Pei, Zikun Zhou, Zhenyu He, Huchuan Lu, Ming-Hsuan Yang", "title": "Crop-Transform-Paste: Self-Supervised Learning for Visual Tracking", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep-learning based methods for visual tracking have achieved\nsubstantial progress, these schemes entail large-scale and high-quality\nannotated data for sufficient training. To eliminate expensive and exhaustive\nannotation, we study self-supervised learning for visual tracking. In this\nwork, we develop the Crop-Transform-Paste operation, which is able to\nsynthesize sufficient training data by simulating various kinds of scene\nvariations during tracking, including appearance variations of objects and\nbackground changes. Since the object state is known in all synthesized data,\nexisting deep trackers can be trained in routine ways without human annotation.\nDifferent from typical self-supervised learning methods performing visual\nrepresentation learning as an individual step, the proposed self-supervised\nlearning mechanism can be seamlessly integrated into any existing tracking\nframework to perform training. Extensive experiments show that our method 1)\nachieves favorable performance than supervised learning in few-shot tracking\nscenarios; 2) can deal with various tracking challenges such as object\ndeformation, occlusion, or background clutter due to its design; 3) can be\ncombined with supervised learning to further boost the performance,\nparticularly effective in few-shot tracking scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 07:40:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Xin", ""], ["Pei", "Wenjie", ""], ["Zhou", "Zikun", ""], ["He", "Zhenyu", ""], ["Lu", "Huchuan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2106.10916", "submitter": "Deepak Alapatt", "authors": "Pietro Mascagni and Deepak Alapatt, Alain Garcia, Nariaki Okamoto,\n  Armine Vardazaryan, Guido Costamagna, Bernard Dallemagne, Nicolas Padoy", "title": "Surgical data science for safe cholecystectomy: a protocol for\n  segmentation of hepatocystic anatomy and assessment of the critical view of\n  safety", "comments": "24 pages, 34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Minimally invasive image-guided surgery heavily relies on vision. Deep\nlearning models for surgical video analysis could therefore support visual\ntasks such as assessing the critical view of safety (CVS) in laparoscopic\ncholecystectomy (LC), potentially contributing to surgical safety and\nefficiency. However, the performance, reliability and reproducibility of such\nmodels are deeply dependent on the quality of data and annotations used in\ntheir development. Here, we present a protocol, checklists, and visual examples\nto promote consistent annotation of hepatocystic anatomy and CVS criteria. We\nbelieve that sharing annotation guidelines can help build trustworthy\nmulticentric datasets for assessing generalizability of performance, thus\naccelerating the clinical translation of deep learning models for surgical\nvideo analysis.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 08:27:38 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mascagni", "Pietro", ""], ["Alapatt", "Deepak", ""], ["Garcia", "Alain", ""], ["Okamoto", "Nariaki", ""], ["Vardazaryan", "Armine", ""], ["Costamagna", "Guido", ""], ["Dallemagne", "Bernard", ""], ["Padoy", "Nicolas", ""]]}, {"id": "2106.10920", "submitter": "Chenyu Guo", "authors": "Chenyu Guo, Jiyang Xie, Kongming Liang, Xian Sun, Zhanyu Ma", "title": "Cross-layer Navigation Convolutional Neural Network for Fine-grained\n  Visual Classification", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual classification (FGVC) aims to classify sub-classes of\nobjects in the same super-class (e.g., species of birds, models of cars). For\nthe FGVC tasks, the essential solution is to find discriminative subtle\ninformation of the target from local regions. TraditionalFGVC models preferred\nto use the refined features,i.e., high-level semantic information for\nrecognition and rarely use low-level in-formation. However, it turns out that\nlow-level information which contains rich detail information also has effect on\nimproving performance. Therefore, in this paper, we propose cross-layer\nnavigation convolutional neural network for feature fusion. First, the feature\nmaps extracted by the backbone network are fed into a convolutional long\nshort-term memory model sequentially from high-level to low-level to perform\nfeature aggregation. Then, attention mechanisms are used after feature fusion\nto extract spatial and channel information while linking the high-level\nsemantic information and the low-level texture features, which can better\nlocate the discriminative regions for the FGVC. In the experiments, three\ncommonly used FGVC datasets, including CUB-200-2011, Stanford-Cars,\nandFGVC-Aircraft datasets, are used for evaluation and we demonstrate the\nsuperiority of the proposed method by comparing it with other referred FGVC\nmethods to show that this method achieves superior results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 08:38:27 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Guo", "Chenyu", ""], ["Xie", "Jiyang", ""], ["Liang", "Kongming", ""], ["Sun", "Xian", ""], ["Ma", "Zhanyu", ""]]}, {"id": "2106.10923", "submitter": "Tomoya Sakai", "authors": "Tomoya Sakai", "title": "Unsupervised Deep Learning by Injecting Low-Rank and Sparse Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  What if deep neural networks can learn from sparsity-inducing priors? When\nthe networks are designed by combining layer modules (CNN, RNN, etc), engineers\nless exploit the inductive bias, i.e., existing well-known rules or prior\nknowledge, other than annotated training data sets. We focus on employing\nsparsity-inducing priors in deep learning to encourage the network to concisely\ncapture the nature of high-dimensional data in an unsupervised way. In order to\nuse non-differentiable sparsity-inducing norms as loss functions, we plug their\nproximal mappings into the automatic differentiation framework. We demonstrate\nunsupervised learning of U-Net for background subtraction using low-rank and\nsparse priors. The U-Net can learn moving objects in a training sequence\nwithout any annotation, and successfully detect the foreground objects in test\nsequences.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 08:41:02 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sakai", "Tomoya", ""]]}, {"id": "2106.10936", "submitter": "Zhihao Fan", "authors": "Zhihao Fan, Zhongyu Wei, Siyuan Wang, Ruize Wang, Zejun Li, Haijun\n  Shan, Xuanjing Huang", "title": "TCIC: Theme Concepts Learning Cross Language and Vision for Image\n  Captioning", "comments": "IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing research for image captioning usually represents an image using a\nscene graph with low-level facts (objects and relations) and fails to capture\nthe high-level semantics. In this paper, we propose a Theme Concepts extended\nImage Captioning (TCIC) framework that incorporates theme concepts to represent\nhigh-level cross-modality semantics. In practice, we model theme concepts as\nmemory vectors and propose Transformer with Theme Nodes (TTN) to incorporate\nthose vectors for image captioning. Considering that theme concepts can be\nlearned from both images and captions, we propose two settings for their\nrepresentations learning based on TTN. On the vision side, TTN is configured to\ntake both scene graph based features and theme concepts as input for visual\nrepresentation learning. On the language side, TTN is configured to take both\ncaptions and theme concepts as input for text representation re-construction.\nBoth settings aim to generate target captions with the same transformer-based\ndecoder. During the training, we further align representations of theme\nconcepts learned from images and corresponding captions to enforce the\ncross-modality learning. Experimental results on MS COCO show the effectiveness\nof our approach compared to some state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 09:12:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fan", "Zhihao", ""], ["Wei", "Zhongyu", ""], ["Wang", "Siyuan", ""], ["Wang", "Ruize", ""], ["Li", "Zejun", ""], ["Shan", "Haijun", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2106.10938", "submitter": "Xu Cheng", "authors": "Xu Cheng, Chuntung Chu, Yi Zheng, Jie Ren, Quanshi Zhang", "title": "A Game-Theoretic Taxonomy of Visual Concepts in DNNs", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we rethink how a DNN encodes visual concepts of different\ncomplexities from a new perspective, i.e. the game-theoretic multi-order\ninteractions between pixels in an image. Beyond the categorical taxonomy of\nobjects and the cognitive taxonomy of textures and shapes, we provide a new\ntaxonomy of visual concepts, which helps us interpret the encoding of shapes\nand textures, in terms of concept complexities. In this way, based on\nmulti-order interactions, we find three distinctive signal-processing behaviors\nof DNNs encoding textures. Besides, we also discover the flexibility for a DNN\nto encode shapes is lower than the flexibility of encoding textures.\nFurthermore, we analyze how DNNs encode outlier samples, and explore the\nimpacts of network architectures on interactions. Additionally, we clarify the\ncrucial role of the multi-order interactions in real-world applications. The\ncode will be released when the paper is accepted.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 09:16:51 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Cheng", "Xu", ""], ["Chu", "Chuntung", ""], ["Zheng", "Yi", ""], ["Ren", "Jie", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2106.10944", "submitter": "Bartosz W\\'ojcik", "authors": "Bartosz W\\'ojcik, Mateusz \\.Zarski, Kamil Ksi\\k{a}\\.zek, Jaros{\\l}aw\n  Adam Miszczak, Miros{\\l}aw Jan Skibniewski", "title": "Hard hat wearing detection based on head keypoint localization", "comments": "15 pages, 9 figures and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, a lot of attention is paid to deep learning methods in the\ncontext of vision-based construction site safety systems, especially regarding\npersonal protective equipment. However, despite all this attention, there is\nstill no reliable way to establish the relationship between workers and their\nhard hats. To answer this problem a combination of deep learning, object\ndetection and head keypoint localization, with simple rule-based reasoning is\nproposed in this article. In tests, this solution surpassed the previous\nmethods based on the relative bounding box position of different instances, as\nwell as direct detection of hard hat wearers and non-wearers. The results show\nthat the conjunction of novel deep learning methods with humanly-interpretable\nrule-based systems can result in a solution that is both reliable and can\nsuccessfully mimic manual, on-site supervision. This work is the next step in\nthe development of fully autonomous construction site safety systems and shows\nthat there is still room for improvement in this area.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 09:31:33 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["W\u00f3jcik", "Bartosz", ""], ["\u017barski", "Mateusz", ""], ["Ksi\u0105\u017cek", "Kamil", ""], ["Miszczak", "Jaros\u0142aw Adam", ""], ["Skibniewski", "Miros\u0142aw Jan", ""]]}, {"id": "2106.10947", "submitter": "Martin Charachon", "authors": "Martin Charachon, Paul-Henry Courn\\`ede, C\\'eline Hudelot and Roberto\n  Ardon", "title": "Leveraging Conditional Generative Models in a General Explanation\n  Framework of Classifier Decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Providing a human-understandable explanation of classifiers' decisions has\nbecome imperative to generate trust in their use for day-to-day tasks. Although\nmany works have addressed this problem by generating visual explanation maps,\nthey often provide noisy and inaccurate results forcing the use of heuristic\nregularization unrelated to the classifier in question. In this paper, we\npropose a new general perspective of the visual explanation problem overcoming\nthese limitations. We show that visual explanation can be produced as the\ndifference between two generated images obtained via two specific conditional\ngenerative models. Both generative models are trained using the classifier to\nexplain and a database to enforce the following properties: (i) All images\ngenerated by the first generator are classified similarly to the input image,\nwhereas the second generator's outputs are classified oppositely. (ii)\nGenerated images belong to the distribution of real images. (iii) The distances\nbetween the input image and the corresponding generated images are minimal so\nthat the difference between the generated elements only reveals relevant\ninformation for the studied classifier. Using symmetrical and cyclic\nconstraints, we present two different approximations and implementations of the\ngeneral formulation. Experimentally, we demonstrate significant improvements\nw.r.t the state-of-the-art on three different public data sets. In particular,\nthe localization of regions influencing the classifier is consistent with human\nannotations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 09:41:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Charachon", "Martin", ""], ["Courn\u00e8de", "Paul-Henry", ""], ["Hudelot", "C\u00e9line", ""], ["Ardon", "Roberto", ""]]}, {"id": "2106.10950", "submitter": "Andreu Girbau Xalabarder", "authors": "Andreu Girbau, Xavier Gir\\'o-i-Nieto, Ignasi Rius, Ferran Marqu\\'es", "title": "Multiple Object Tracking with Mixture Density Networks for Trajectory\n  Estimation", "comments": "Best paper runner up on CVPR 2021 RVSU workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple object tracking faces several challenges that may be alleviated with\ntrajectory information. Knowing the posterior locations of an object helps\ndisambiguating and solving situations such as occlusions, re-identification,\nand identity switching. In this work, we show that trajectory estimation can\nbecome a key factor for tracking, and present TrajE, a trajectory estimator\nbased on recurrent mixture density networks, as a generic module that can be\nadded to existing object trackers. To provide several trajectory hypotheses,\nour method uses beam search. Also, relying on the same estimated trajectory, we\npropose to reconstruct a track after an occlusion occurs. We integrate TrajE\ninto two state of the art tracking algorithms, CenterTrack [63] and Tracktor\n[3]. Their respective performances in the MOTChallenge 2017 test set are\nboosted 6.3 and 0.3 points in MOTA score, and 1.8 and 3.1 in IDF1, setting a\nnew state of the art for the CenterTrack+TrajE configuration\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 09:45:27 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 01:55:48 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Girbau", "Andreu", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Rius", "Ignasi", ""], ["Marqu\u00e9s", "Ferran", ""]]}, {"id": "2106.10962", "submitter": "I\\~nigo Martinez", "authors": "Urtzi Otamendi and I\\~nigo Martinez and Marco Quartulli and Igor G.\n  Olaizola and Elisabeth Viles and Werther Cambarau", "title": "Segmentation of cell-level anomalies in electroluminescence images of\n  photovoltaic modules", "comments": "16 pages, 14 figures", "journal-ref": "Solar Energy, Volume 220, 2021", "doi": "10.1016/j.solener.2021.03.058", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the operation & maintenance (O&M) of photovoltaic (PV) plants, the early\nidentification of failures has become crucial to maintain productivity and\nprolong components' life. Of all defects, cell-level anomalies can lead to\nserious failures and may affect surrounding PV modules in the long run. These\nfine defects are usually captured with high spatial resolution\nelectroluminescence (EL) imaging. The difficulty of acquiring such images has\nlimited the availability of data. For this work, multiple data resources and\naugmentation techniques have been used to surpass this limitation. Current\nstate-of-the-art detection methods extract barely low-level information from\nindividual PV cell images, and their performance is conditioned by the\navailable training data. In this article, we propose an end-to-end deep\nlearning pipeline that detects, locates and segments cell-level anomalies from\nentire photovoltaic modules via EL images. The proposed modular pipeline\ncombines three deep learning techniques: 1. object detection (modified\nFaster-RNN), 2. image classification (EfficientNet) and 3. weakly supervised\nsegmentation (autoencoder). The modular nature of the pipeline allows to\nupgrade the deep learning models to the further improvements in the\nstate-of-the-art and also extend the pipeline towards new functionalities.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 10:17:40 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Otamendi", "Urtzi", ""], ["Martinez", "I\u00f1igo", ""], ["Quartulli", "Marco", ""], ["Olaizola", "Igor G.", ""], ["Viles", "Elisabeth", ""], ["Cambarau", "Werther", ""]]}, {"id": "2106.10980", "submitter": "Ariel Caputo", "authors": "Ariel Caputo, Andrea Giachetti, Simone Soso, Deborah Pintani, Andrea\n  D'Eusanio, Stefano Pini, Guido Borghi, Alessandro Simoni, Roberto Vezzani,\n  Rita Cucchiara, Andrea Ranieri, Franca Giannini, Katia Lupinetti, Marina\n  Monti, Mehran Maghoumi, Joseph J. LaViola Jr, Minh-Quan Le, Hai-Dang Nguyen,\n  Minh-Triet Tran", "title": "SHREC 2021: Track on Skeleton-based Hand Gesture Recognition in the Wild", "comments": "12 pages, to be published on Computers & Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gesture recognition is a fundamental tool to enable novel interaction\nparadigms in a variety of application scenarios like Mixed Reality\nenvironments, touchless public kiosks, entertainment systems, and more.\nRecognition of hand gestures can be nowadays performed directly from the stream\nof hand skeletons estimated by software provided by low-cost trackers\n(Ultraleap) and MR headsets (Hololens, Oculus Quest) or by video processing\nsoftware modules (e.g. Google Mediapipe). Despite the recent advancements in\ngesture and action recognition from skeletons, it is unclear how well the\ncurrent state-of-the-art techniques can perform in a real-world scenario for\nthe recognition of a wide set of heterogeneous gestures, as many benchmarks do\nnot test online recognition and use limited dictionaries. This motivated the\nproposal of the SHREC 2021: Track on Skeleton-based Hand Gesture Recognition in\nthe Wild. For this contest, we created a novel dataset with heterogeneous\ngestures featuring different types and duration. These gestures have to be\nfound inside sequences in an online recognition scenario. This paper presents\nthe result of the contest, showing the performances of the techniques proposed\nby four research groups on the challenging task compared with a simple baseline\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 10:57:49 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Caputo", "Ariel", ""], ["Giachetti", "Andrea", ""], ["Soso", "Simone", ""], ["Pintani", "Deborah", ""], ["D'Eusanio", "Andrea", ""], ["Pini", "Stefano", ""], ["Borghi", "Guido", ""], ["Simoni", "Alessandro", ""], ["Vezzani", "Roberto", ""], ["Cucchiara", "Rita", ""], ["Ranieri", "Andrea", ""], ["Giannini", "Franca", ""], ["Lupinetti", "Katia", ""], ["Monti", "Marina", ""], ["Maghoumi", "Mehran", ""], ["LaViola", "Joseph J.", "Jr"], ["Le", "Minh-Quan", ""], ["Nguyen", "Hai-Dang", ""], ["Tran", "Minh-Triet", ""]]}, {"id": "2106.10989", "submitter": "Jiaming Zhang", "authors": "Jiaming Zhang, Jitao Sang, Qi Yi, Huiwen Dong, Jian Yu", "title": "Pre-training also Transfers Non-Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training has enabled many state-of-the-art results on many tasks. In\nspite of its recognized contribution to generalization, we observed in this\nstudy that pre-training also transfers the non-robustness from pre-trained\nmodel into the fine-tuned model. Using image classification as an example, we\nfirst conducted experiments on various datasets and network backbones to\nexplore the factors influencing robustness. Further analysis is conducted on\nexamining the difference between the fine-tuned model and standard model to\nuncover the reason leading to the non-robustness transfer. Finally, we\nintroduce a simple robust pre-training solution by regularizing the difference\nbetween target and source tasks. Results validate the effectiveness in\nalleviating non-robustness and preserving generalization.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 11:16:13 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhang", "Jiaming", ""], ["Sang", "Jitao", ""], ["Yi", "Qi", ""], ["Dong", "Huiwen", ""], ["Yu", "Jian", ""]]}, {"id": "2106.10992", "submitter": "Richard Shaw", "authors": "Richard Shaw, Carole H. Sudre, Sebastien Ourselin, M. Jorge Cardoso", "title": "Estimating MRI Image Quality via Image Reconstruction Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality control (QC) in medical image analysis is time-consuming and\nlaborious, leading to increased interest in automated methods. However, what is\ndeemed suitable quality for algorithmic processing may be different from\nhuman-perceived measures of visual quality. In this work, we pose MR image\nquality assessment from an image reconstruction perspective. We train Bayesian\nCNNs using a heteroscedastic uncertainty model to recover clean images from\nnoisy data, providing measures of uncertainty over the predictions. This\nframework enables us to divide data corruption into learnable and non-learnable\ncomponents and leads us to interpret the predictive uncertainty as an\nestimation of the achievable recovery of an image. Thus, we argue that quality\ncontrol for visual assessment cannot be equated to quality control for\nalgorithmic processing. We validate this statement in a multi-task experiment\ncombining artefact recovery with uncertainty prediction and grey matter\nsegmentation. Recognising this distinction between visual and algorithmic\nquality has the impact that, depending on the downstream task, less data can be\nexcluded based on ``visual quality\" reasons alone.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 11:22:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shaw", "Richard", ""], ["Sudre", "Carole H.", ""], ["Ourselin", "Sebastien", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "2106.10996", "submitter": "Blerta Lindqvist", "authors": "Blerta Lindqvist", "title": "Delving into the pixels of adversarial samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite extensive research into adversarial attacks, we do not know how\nadversarial attacks affect image pixels. Knowing how image pixels are affected\nby adversarial attacks has the potential to lead us to better adversarial\ndefenses. Motivated by instances that we find where strong attacks do not\ntransfer, we delve into adversarial examples at pixel level to scrutinize how\nadversarial attacks affect image pixel values. We consider several ImageNet\narchitectures, InceptionV3, VGG19 and ResNet50, as well as several strong\nattacks. We find that attacks can have different effects at pixel level\ndepending on classifier architecture. In particular, input pre-processing plays\na previously overlooked role in the effect that attacks have on pixels. Based\non the insights of pixel-level examination, we find new ways to detect some of\nthe strongest current attacks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 11:28:06 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lindqvist", "Blerta", ""]]}, {"id": "2106.11013", "submitter": "Guoshun Nan Dr", "authors": "Guoshun Nan, Rui Qiao, Yao Xiao, Jun Liu, Sicong Leng, Hao Zhang, Wei\n  Lu", "title": "Interventional Video Grounding with Dual Contrastive Learning", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video grounding aims to localize a moment from an untrimmed video for a given\ntextual query. Existing approaches focus more on the alignment of visual and\nlanguage stimuli with various likelihood-based matching or regression\nstrategies, i.e., P(Y|X). Consequently, these models may suffer from spurious\ncorrelations between the language and video features due to the selection bias\nof the dataset. 1) To uncover the causality behind the model and data, we first\npropose a novel paradigm from the perspective of the causal inference, i.e.,\ninterventional video grounding (IVG) that leverages backdoor adjustment to\ndeconfound the selection bias based on structured causal model (SCM) and\ndo-calculus P(Y|do(X)). Then, we present a simple yet effective method to\napproximate the unobserved confounder as it cannot be directly sampled from the\ndataset. 2) Meanwhile, we introduce a dual contrastive learning approach (DCL)\nto better align the text and video by maximizing the mutual information (MI)\nbetween query and video clips, and the MI between start/end frames of a target\nmoment and the others within a video to learn more informative visual\nrepresentations. Experiments on three standard benchmarks show the\neffectiveness of our approaches. Our code is available on GitHub:\nhttps://github.com/nanguoshun/IVG.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 12:11:28 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 15:10:07 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Nan", "Guoshun", ""], ["Qiao", "Rui", ""], ["Xiao", "Yao", ""], ["Liu", "Jun", ""], ["Leng", "Sicong", ""], ["Zhang", "Hao", ""], ["Lu", "Wei", ""]]}, {"id": "2106.11037", "submitter": "Jiageng Mao", "authors": "Jiageng Mao, Minzhe Niu, Chenhan Jiang, Hanxue Liang, Xiaodan Liang,\n  Yamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Jie Yu, Hang Xu, Chunjing Xu", "title": "One Million Scenes for Autonomous Driving: ONCE Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current perception models in autonomous driving have become notorious for\ngreatly relying on a mass of annotated data to cover unseen cases and address\nthe long-tail problem. On the other hand, learning from unlabeled large-scale\ncollected data and incrementally self-training powerful recognition models have\nreceived increasing attention and may become the solutions of next-generation\nindustry-level powerful and robust perception models in autonomous driving.\nHowever, the research community generally suffered from data inadequacy of\nthose essential real-world scene data, which hampers the future exploration of\nfully/semi/self-supervised methods for 3D perception. In this paper, we\nintroduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the\nautonomous driving scenario. The ONCE dataset consists of 1 million LiDAR\nscenes and 7 million corresponding camera images. The data is selected from 144\ndriving hours, which is 20x longer than the largest 3D autonomous driving\ndataset available (e.g. nuScenes and Waymo), and it is collected across a range\nof different areas, periods and weather conditions. To facilitate future\nresearch on exploiting unlabeled data for 3D detection, we additionally provide\na benchmark in which we reproduce and evaluate a variety of self-supervised and\nsemi-supervised methods on the ONCE dataset. We conduct extensive analyses on\nthose methods and provide valuable observations on their performance related to\nthe scale of used data. Data, code, and more information are available at\nhttps://once-for-auto-driving.github.io/index.html.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 12:28:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mao", "Jiageng", ""], ["Niu", "Minzhe", ""], ["Jiang", "Chenhan", ""], ["Liang", "Hanxue", ""], ["Liang", "Xiaodan", ""], ["Li", "Yamin", ""], ["Ye", "Chaoqiang", ""], ["Zhang", "Wei", ""], ["Li", "Zhenguo", ""], ["Yu", "Jie", ""], ["Xu", "Hang", ""], ["Xu", "Chunjing", ""]]}, {"id": "2106.11048", "submitter": "Andr\\'es Marafioti PhD", "authors": "Andr\\'es Marafioti, Michel Hayoz, Mathias Gallardo, Pablo M\\'arquez\n  Neila, Sebastian Wolf, Martin Zinkernagel, and Raphael Sznitman", "title": "CataNet: Predicting remaining cataract surgery duration", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cataract surgery is a sight saving surgery that is performed over 10 million\ntimes each year around the world. With such a large demand, the ability to\norganize surgical wards and operating rooms efficiently is critical to delivery\nthis therapy in routine clinical care. In this context, estimating the\nremaining surgical duration (RSD) during procedures is one way to help\nstreamline patient throughput and workflows. To this end, we propose CataNet, a\nmethod for cataract surgeries that predicts in real time the RSD jointly with\ntwo influential elements: the surgeon's experience, and the current phase of\nthe surgery. We compare CataNet to state-of-the-art RSD estimation methods,\nshowing that it outperforms them even when phase and experience are not\nconsidered. We investigate this improvement and show that a significant\ncontributor is the way we integrate the elapsed time into CataNet's feature\nextractor.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 12:35:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Marafioti", "Andr\u00e9s", ""], ["Hayoz", "Michel", ""], ["Gallardo", "Mathias", ""], ["Neila", "Pablo M\u00e1rquez", ""], ["Wolf", "Sebastian", ""], ["Zinkernagel", "Martin", ""], ["Sznitman", "Raphael", ""]]}, {"id": "2106.11054", "submitter": "Witold Oleszkiewicz", "authors": "Witold Oleszkiewicz, Dominika Basaj, Igor Sieradzki, Micha{\\l}\n  G\\'orszczak, Barbara Rychalska, Koryna Lewandowska, Tomasz Trzci\\'nski,\n  Bartosz Zieli\\'nski", "title": "Visual Probing: Cognitive Framework for Explaining Self-Supervised Image\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently introduced self-supervised methods for image representation learning\nprovide on par or superior results to their fully supervised competitors, yet\nthe corresponding efforts to explain the self-supervised approaches lag behind.\nMotivated by this observation, we introduce a novel visual probing framework\nfor explaining the self-supervised models by leveraging probing tasks employed\npreviously in natural language processing. The probing tasks require knowledge\nabout semantic relationships between image parts. Hence, we propose a\nsystematic approach to obtain analogs of natural language in vision, such as\nvisual words, context, and taxonomy. Our proposal is grounded in Marr's\ncomputational theory of vision and concerns features like textures, shapes, and\nlines. We show the effectiveness and applicability of those analogs in the\ncontext of explaining self-supervised representations. Our key findings\nemphasize that relations between language and vision can serve as an effective\nyet intuitive tool for discovering how machine learning models work,\nindependently of data modality. Our work opens a plethora of research pathways\ntowards more explainable and transparent AI.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 12:40:31 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Oleszkiewicz", "Witold", ""], ["Basaj", "Dominika", ""], ["Sieradzki", "Igor", ""], ["G\u00f3rszczak", "Micha\u0142", ""], ["Rychalska", "Barbara", ""], ["Lewandowska", "Koryna", ""], ["Trzci\u0144ski", "Tomasz", ""], ["Zieli\u0144ski", "Bartosz", ""]]}, {"id": "2106.11056", "submitter": "Alessandro Sebastianelli", "authors": "Alessandro Sebastianelli, Maria Pia Del Rosso, Pierre Philippe\n  Mathieu, Silvia Liberata Ullo", "title": "Paradigm selection for Data Fusion of SAR and Multispectral Sentinel\n  data applied to Land-Cover Classification", "comments": "This work has been submitted to the IEEE Geoscience and Remote\n  Sensing Letters for possible publication. Copyright may be transferred\n  without notice, after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data fusion is a well-known technique, becoming more and more popular in the\nArtificial Intelligence for Earth Observation (AI4EO) domain mainly due to its\nability of reinforcing AI4EO applications by combining multiple data sources\nand thus bringing better results. On the other hand, like other methods for\nsatellite data analysis, data fusion itself is also benefiting and evolving\nthanks to the integration of Artificial Intelligence (AI). In this letter, four\ndata fusion paradigms, based on Convolutional Neural Networks (CNNs), are\nanalyzed and implemented. The goals are to provide a systematic procedure for\nchoosing the best data fusion framework, resulting in the best classification\nresults, once the basic structure for the CNN has been defined, and to help\ninterested researchers in their work when data fusion applied to remote sensing\nis involved. The procedure has been validated for land-cover classification but\nit can be transferred to other cases.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 11:36:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sebastianelli", "Alessandro", ""], ["Del Rosso", "Maria Pia", ""], ["Mathieu", "Pierre Philippe", ""], ["Ullo", "Silvia Liberata", ""]]}, {"id": "2106.11097", "submitter": "Pengfei Xiong", "authors": "Han Fang, Pengfei Xiong, Luhui Xu, Yu Chen", "title": "CLIP2Video: Mastering Video-Text Retrieval via Image CLIP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CLIP2Video network to transfer the image-language pre-training\nmodel to video-text retrieval in an end-to-end manner. Leading approaches in\nthe domain of video-and-language learning try to distill the spatio-temporal\nvideo features and multi-modal interaction between videos and languages from a\nlarge-scale video-text dataset. Different from them, we leverage pretrained\nimage-language model, simplify it as a two-stage framework with co-learning of\nimage-text and enhancing temporal relations between video frames and video-text\nrespectively, make it able to train on comparatively small datasets.\nSpecifically, based on the spatial semantics captured by Contrastive\nLanguage-Image Pretraining (CLIP) model, our model involves a Temporal\nDifference Block to capture motions at fine temporal video frames, and a\nTemporal Alignment Block to re-align the tokens of video clips and phrases and\nenhance the multi-modal correlation. We conduct thorough ablation studies, and\nachieve state-of-the-art performance on major text-to-video and video-to-text\nretrieval benchmarks, including new records of retrieval accuracy on MSR-VTT,\nMSVD and VATEX.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 13:30:33 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fang", "Han", ""], ["Xiong", "Pengfei", ""], ["Xu", "Luhui", ""], ["Chen", "Yu", ""]]}, {"id": "2106.11098", "submitter": "Jan Moros Esteban", "authors": "Jan Moros Esteban, Jaap van de Loosdrecht, Maya Aghaei", "title": "Obstacle Detection for BVLOS Drones", "comments": "7 pages, 7 figures, Supervisors: Maya Aghaei Gavari and Jaap van de\n  Loosdrecht", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of new regulations in the European Union, the future of\nBeyond Visual Line Of Sight (BVLOS) drones is set to bloom. This led to the\ncreation of the theBEAST project, which aims to create an autonomous security\ndrone, with focus on those regulations and on safety. This technical paper\ndescribes the first steps of a module within this project, which revolves\naround detecting obstacles so they can be avoided in a fail-safe landing. A\ndeep learning powered object detection method is the subject of our research,\nand various experiments are held to maximize its performance, such as comparing\nvarious data augmentation techniques or YOLOv3 and YOLOv5. According to the\nresults of the experiments, we conclude that although object detection is a\npromising approach to resolve this problem, more volume of data is required for\npotential usage in a real-life application.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 13:31:54 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 13:31:13 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Esteban", "Jan Moros", ""], ["van de Loosdrecht", "Jaap", ""], ["Aghaei", "Maya", ""]]}, {"id": "2106.11099", "submitter": "Jialin Shi", "authors": "Jialin Shi and Ji Wu", "title": "Distilling effective supervision for robust medical image segmentation\n  with noisy labels", "comments": "Accepted to MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of deep learning methods in medical image segmentation\ntasks, the human-level performance relies on massive training data with\nhigh-quality annotations, which are expensive and time-consuming to collect.\nThe fact is that there exist low-quality annotations with label noise, which\nleads to suboptimal performance of learned models. Two prominent directions for\nsegmentation learning with noisy labels include pixel-wise noise robust\ntraining and image-level noise robust training. In this work, we propose a\nnovel framework to address segmenting with noisy labels by distilling effective\nsupervision information from both pixel and image levels. In particular, we\nexplicitly estimate the uncertainty of every pixel as pixel-wise noise\nestimation, and propose pixel-wise robust learning by using both the original\nlabels and pseudo labels. Furthermore, we present an image-level robust\nlearning method to accommodate more information as the complements to\npixel-level learning. We conduct extensive experiments on both simulated and\nreal-world noisy datasets. The results demonstrate the advantageous performance\nof our method compared to state-of-the-art baselines for medical image\nsegmentation with noisy labels.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 13:33:38 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shi", "Jialin", ""], ["Wu", "Ji", ""]]}, {"id": "2106.11118", "submitter": "Jianhua Han", "authors": "Jianhua Han, Xiwen Liang, Hang Xu, Kai Chen, Lanqing Hong, Chaoqiang\n  Ye, Wei Zhang, Zhenguo Li, Xiaodan Liang, Chunjing Xu", "title": "SODA10M: Towards Large-Scale Object Detection Benchmark for Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Aiming at facilitating a real-world, ever-evolving and scalable autonomous\ndriving system, we present a large-scale benchmark for standardizing the\nevaluation of different self-supervised and semi-supervised approaches by\nlearning from raw data, which is the first and largest benchmark to date.\nExisting autonomous driving systems heavily rely on `perfect' visual perception\nmodels (e.g., detection) trained using extensive annotated data to ensure the\nsafety. However, it is unrealistic to elaborately label instances of all\nscenarios and circumstances (e.g., night, extreme weather, cities) when\ndeploying a robust autonomous driving system. Motivated by recent powerful\nadvances of self-supervised and semi-supervised learning, a promising direction\nis to learn a robust detection model by collaboratively exploiting large-scale\nunlabeled data and few labeled data. Existing dataset (e.g., KITTI, Waymo)\neither provides only a small amount of data or covers limited domains with full\nannotation, hindering the exploration of large-scale pre-trained models. Here,\nwe release a Large-Scale Object Detection benchmark for Autonomous driving,\nnamed as SODA10M, containing 10 million unlabeled images and 20K images labeled\nwith 6 representative object categories. To improve diversity, the images are\ncollected every ten seconds per frame within 32 different cities under\ndifferent weather conditions, periods and location scenes. We provide extensive\nexperiments and deep analyses of existing supervised state-of-the-art detection\nmodels, popular self-supervised and semi-supervised approaches, and some\ninsights about how to develop future models. The data and more up-to-date\ninformation have been released at https://soda-2d.github.io.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 13:55:57 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 01:27:44 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Han", "Jianhua", ""], ["Liang", "Xiwen", ""], ["Xu", "Hang", ""], ["Chen", "Kai", ""], ["Hong", "Lanqing", ""], ["Ye", "Chaoqiang", ""], ["Zhang", "Wei", ""], ["Li", "Zhenguo", ""], ["Liang", "Xiaodan", ""], ["Xu", "Chunjing", ""]]}, {"id": "2106.11119", "submitter": "Jack Dymond", "authors": "Jack Dymond", "title": "Graceful Degradation and Related Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When machine learning models encounter data which is out of the distribution\non which they were trained they have a tendency to behave poorly, most\nprominently over-confidence in erroneous predictions. Such behaviours will have\ndisastrous effects on real-world machine learning systems. In this field\ngraceful degradation refers to the optimisation of model performance as it\nencounters this out-of-distribution data. This work presents a definition and\ndiscussion of graceful degradation and where it can be applied in deployed\nvisual systems. Following this a survey of relevant areas is undertaken,\nnovelly splitting the graceful degradation problem into active and passive\napproaches. In passive approaches, graceful degradation is handled and achieved\nby the model in a self-contained manner, in active approaches the model is\nupdated upon encountering epistemic uncertainties. This work communicates the\nimportance of the problem and aims to prompt the development of machine\nlearning strategies that are aware of graceful degradation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 13:56:41 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 12:30:26 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Dymond", "Jack", ""]]}, {"id": "2106.11125", "submitter": "Omer Aydin", "authors": "Omer Aydin", "title": "Classification of Documents Extracted from Images with Optical Character\n  Recognition Methods", "comments": null, "journal-ref": "Computer Science , 6 (2) , 46-55 (2021). Retrieved from\n  https://dergipark.org.tr/tr/pub/bbd/issue/62530/864863", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decade, machine learning methods have given us driverless cars,\nvoice recognition, effective web search, and a much better understanding of the\nhuman genome. Machine learning is so common today that it is used dozens of\ntimes a day, possibly unknowingly. Trying to teach a machine some processes or\nsome situations can make them predict some results that are difficult to\npredict by the human brain. These methods also help us do some operations that\nare often impossible or difficult to do with human activities in a short time.\nFor these reasons, machine learning is so important today. In this study, two\ndifferent machine learning methods were combined. In order to solve a\nreal-world problem, the manuscript documents were first transferred to the\ncomputer and then classified. We used three basic methods to realize the whole\nprocess. Handwriting or printed documents have been digitalized by a scanner or\ndigital camera. These documents have been processed with two different Optical\nCharacter Recognition (OCR) operation. After that generated texts are\nclassified by using Naive Bayes algorithm. All project was programmed in\nMicrosoft Visual Studio 12 platform on Windows operating system. C# programming\nlanguage was used for all parts of the study. Also, some prepared codes and\nDLLs were used.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 15:56:00 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Aydin", "Omer", ""]]}, {"id": "2106.11145", "submitter": "Yiming Lin", "authors": "Yiming Lin, Jie Shen, Yujiang Wang, Maja Pantic", "title": "FP-Age: Leveraging Face Parsing Attention for Facial Age Estimation in\n  the Wild", "comments": "Code and data will be available on\n  https://github.com/hhj1897/age_estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-based age estimation aims to predict a person's age from facial images.\nIt is used in a variety of real-world applications. Although end-to-end deep\nmodels have achieved impressive results for age estimation on benchmark\ndatasets, their performance in-the-wild still leaves much room for improvement\ndue to the challenges caused by large variations in head pose, facial\nexpressions, and occlusions. To address this issue, we propose a simple yet\neffective method to explicitly incorporate facial semantics into age\nestimation, so that the model would learn to correctly focus on the most\ninformative facial components from unaligned facial images regardless of head\npose and non-rigid deformation. To this end, we design a face parsing-based\nnetwork to learn semantic information at different scales and a novel face\nparsing attention module to leverage these semantic features for age\nestimation. To evaluate our method on in-the-wild data, we also introduce a new\nchallenging large-scale benchmark called IMDB-Clean. This dataset is created by\nsemi-automatically cleaning the noisy IMDB-WIKI dataset using a constrained\nclustering method. Through comprehensive experiment on IMDB-Clean and other\nbenchmark datasets, under both intra-dataset and cross-dataset evaluation\nprotocols, we show that our method consistently outperforms all existing age\nestimation methods and achieves a new state-of-the-art performance. To the best\nof our knowledge, our work presents the first attempt of leveraging face\nparsing attention to achieve semantic-aware age estimation, which may be\ninspiring to other high level facial analysis tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 14:31:32 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lin", "Yiming", ""], ["Shen", "Jie", ""], ["Wang", "Yujiang", ""], ["Pantic", "Maja", ""]]}, {"id": "2106.11149", "submitter": "Xiang Wang", "authors": "Xiang Wang, Shiwei Zhang, Zhiwu Qing, Yuanjie Shao, Zhengrong Zuo,\n  Changxin Gao, Nong Sang", "title": "OadTR: Online Action Detection with Transformers", "comments": "Code is available at https://github.com/wangxiang1230/OadTR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent approaches for online action detection tend to apply Recurrent\nNeural Network (RNN) to capture long-range temporal structure. However, RNN\nsuffers from non-parallelism and gradient vanishing, hence it is hard to be\noptimized. In this paper, we propose a new encoder-decoder framework based on\nTransformers, named OadTR, to tackle these problems. The encoder attached with\na task token aims to capture the relationships and global interactions between\nhistorical observations. The decoder extracts auxiliary information by\naggregating anticipated future clip representations. Therefore, OadTR can\nrecognize current actions by encoding historical information and predicting\nfuture context simultaneously. We extensively evaluate the proposed OadTR on\nthree challenging datasets: HDD, TVSeries, and THUMOS14. The experimental\nresults show that OadTR achieves higher training and inference speeds than\ncurrent RNN based approaches, and significantly outperforms the\nstate-of-the-art methods in terms of both mAP and mcAP. Code is available at\nhttps://github.com/wangxiang1230/OadTR.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 14:39:35 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Xiang", ""], ["Zhang", "Shiwei", ""], ["Qing", "Zhiwu", ""], ["Shao", "Yuanjie", ""], ["Zuo", "Zhengrong", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""]]}, {"id": "2106.11154", "submitter": "Matthias K\\\"orschens", "authors": "Matthias K\\\"orschens, Paul Bodesheim, Christine R\\\"omermann, Solveig\n  Franziska Bucher, Mirco Migliavacca, Josephine Ulrich, Joachim Denzler", "title": "Automatic Plant Cover Estimation with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the responses of plants to environmental changes is essential for\nplant biodiversity research. This, however, is currently still being done\nmanually by botanists in the field. This work is very laborious, and the data\nobtained is, though following a standardized method to estimate plant coverage,\nusually subjective and has a coarse temporal resolution. To remedy these\ncaveats, we investigate approaches using convolutional neural networks (CNNs)\nto automatically extract the relevant data from images, focusing on plant\ncommunity composition and species coverages of 9 herbaceous plant species. To\nthis end, we investigate several standard CNN architectures and different\npretraining methods. We find that we outperform our previous approach at higher\nimage resolutions using a custom CNN with a mean absolute error of 5.16%. In\naddition to these investigations, we also conduct an error analysis based on\nthe temporal aspect of the plant cover images. This analysis gives insight into\nwhere problems for automatic approaches lie, like occlusion and likely\nmisclassifications caused by temporal changes.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 14:52:01 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 10:01:14 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 17:28:31 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["K\u00f6rschens", "Matthias", ""], ["Bodesheim", "Paul", ""], ["R\u00f6mermann", "Christine", ""], ["Bucher", "Solveig Franziska", ""], ["Migliavacca", "Mirco", ""], ["Ulrich", "Josephine", ""], ["Denzler", "Joachim", ""]]}, {"id": "2106.11166", "submitter": "Radu P Horaud", "authors": "Avinash Sharma, Radu Horaud and Diana Mateus", "title": "3D Shape Registration Using Spectral Graph Embedding and Probabilistic\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of 3D shape registration and we propose a novel\ntechnique based on spectral graph theory and probabilistic matching. The task\nof 3D shape analysis involves tracking, recognition, registration, etc.\nAnalyzing 3D data in a single framework is still a challenging task considering\nthe large variability of the data gathered with different acquisition devices.\n3D shape registration is one such challenging shape analysis task. The main\ncontribution of this chapter is to extend the spectral graph matching methods\nto very large graphs by combining spectral graph matching with Laplacian\nembedding. Since the embedded representation of a graph is obtained by\ndimensionality reduction we claim that the existing spectral-based methods are\nnot easily applicable. We discuss solutions for the exact and inexact graph\nisomorphism problems and recall the main spectral properties of the\ncombinatorial graph Laplacian; We provide a novel analysis of the commute-time\nembedding that allows us to interpret the latter in terms of the PCA of a\ngraph, and to select the appropriate dimension of the associated embedded\nmetric space; We derive a unit hyper-sphere normalization for the commute-time\nembedding that allows us to register two shapes with different samplings; We\npropose a novel method to find the eigenvalue-eigenvector ordering and the\neigenvector signs using the eigensignature (histogram) which is invariant to\nthe isometric shape deformations and fits well in the spectral graph matching\nframework, and we present a probabilistic shape matching formulation using an\nexpectation maximization point registration algorithm which alternates between\naligning the eigenbases and finding a vertex-to-vertex assignment.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:02:31 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sharma", "Avinash", ""], ["Horaud", "Radu", ""], ["Mateus", "Diana", ""]]}, {"id": "2106.11173", "submitter": "Andr\\'es Villa", "authors": "Andr\\'es Villa, Juan-Manuel Perez-Rua, Vladimir Araujo, Juan Carlos\n  Niebles, Victor Escorcia, Alvaro Soto", "title": "TNT: Text-Conditioned Network with Transductive Inference for Few-Shot\n  Video Classification", "comments": "10 pages including references, 7 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, few-shot learning has received increasing interest. Existing\nefforts have been focused on image classification, with very few attempts\ndedicated to the more challenging few-shot video classification problem. These\nfew attempts aim to effectively exploit the temporal dimension in videos for\nbetter learning in low data regimes. However, they have largely ignored a key\ncharacteristic of video which could be vital for few-shot recognition, that is,\nvideos are often accompanied by rich text descriptions. In this paper, for the\nfirst time, we propose to leverage these human-provided textual descriptions as\nprivileged information when training a few-shot video classification model.\nSpecifically, we formulate a text-based task conditioner to adapt video\nfeatures to the few-shot learning task. Our model follows a transductive\nsetting where query samples and support textual descriptions can be used to\nupdate the support set class prototype to further improve the task-adaptation\nability of the model. Our model obtains state-of-the-art performance on four\nchallenging benchmarks in few-shot video action classification.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:08:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Villa", "Andr\u00e9s", ""], ["Perez-Rua", "Juan-Manuel", ""], ["Araujo", "Vladimir", ""], ["Niebles", "Juan Carlos", ""], ["Escorcia", "Victor", ""], ["Soto", "Alvaro", ""]]}, {"id": "2106.11174", "submitter": "Steven Gutstein", "authors": "Steven Gutstein, Brent Lance and Sanjay Shakkottai", "title": "Does Optimal Source Task Performance Imply Optimal Pre-training for a\n  Target Task?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained deep nets are commonly used to improve accuracies and training\ntimes for neural nets. It is generally assumed that pre-training a net for\noptimal source task performance best prepares it to learn an arbitrary target\ntask. This is generally not true. Stopping source task training, prior to\noptimal performance, can create a pre-trained net better suited for learning a\nnew task.\n  We performed several experiments demonstrating this effect, as well as the\ninfluence of amount of training and of learning rate. Additionally, we show\nthat this reflects a general loss of learning ability that even extends to\nrelearning the source task\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:09:04 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gutstein", "Steven", ""], ["Lance", "Brent", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "2106.11193", "submitter": "Jie Xu", "authors": "Jie Xu, Huayi Tang, Yazhou Ren, Xiaofeng Zhu, Lifang He", "title": "Contrastive Multi-Modal Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal clustering, which explores complementary information from\nmultiple modalities or views, has attracted people's increasing attentions.\nHowever, existing works rarely focus on extracting high-level semantic\ninformation of multiple modalities for clustering. In this paper, we propose\nContrastive Multi-Modal Clustering (CMMC) which can mine high-level semantic\ninformation via contrastive learning. Concretely, our framework consists of\nthree parts. (1) Multiple autoencoders are optimized to maintain each\nmodality's diversity to learn complementary information. (2) A feature\ncontrastive module is proposed to learn common high-level semantic features\nfrom different modalities. (3) A label contrastive module aims to learn\nconsistent cluster assignments for all modalities. By the proposed multi-modal\ncontrastive learning, the mutual information of high-level features is\nmaximized, while the diversity of the low-level latent features is maintained.\nIn addition, to utilize the learned high-level semantic features, we further\ngenerate pseudo labels by solving a maximum matching problem to fine-tune the\ncluster assignments. Extensive experiments demonstrate that CMMC has good\nscalability and outperforms state-of-the-art multi-modal clustering methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:32:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xu", "Jie", ""], ["Tang", "Huayi", ""], ["Ren", "Yazhou", ""], ["Zhu", "Xiaofeng", ""], ["He", "Lifang", ""]]}, {"id": "2106.11208", "submitter": "Amin Sabet", "authors": "Amin Sabet, Jonathon Hare, Bashir Al-Hashimi, Geoff V. Merrett", "title": "Temporal Early Exits for Efficient Video Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring image-based object detectors to the domain of video remains\nchallenging under resource constraints. Previous efforts utilised optical flow\nto allow unchanged features to be propagated, however, the overhead is\nconsiderable when working with very slowly changing scenes from applications\nsuch as surveillance. In this paper, we propose temporal early exits to reduce\nthe computational complexity of per-frame video object detection. Multiple\ntemporal early exit modules with low computational overhead are inserted at\nearly layers of the backbone network to identify the semantic differences\nbetween consecutive frames. Full computation is only required if the frame is\nidentified as having a semantic change to previous frames; otherwise, detection\nresults from previous frames are reused. Experiments on CDnet show that our\nmethod significantly reduces the computational complexity and execution of\nper-frame video object detection up to $34 \\times$ compared to existing methods\nwith an acceptable reduction of 2.2\\% in mAP.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:49:46 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sabet", "Amin", ""], ["Hare", "Jonathon", ""], ["Al-Hashimi", "Bashir", ""], ["Merrett", "Geoff V.", ""]]}, {"id": "2106.11232", "submitter": "Jie Xu", "authors": "Jie Xu, Yazhou Ren, Huayi Tang, Xiaorong Pu, Xiaofeng Zhu, Ming Zeng,\n  Lifang He", "title": "Multi-VAE: Learning Disentangled View-common and View-peculiar Visual\n  Representations for Multi-view Clustering", "comments": "Because some important information about the authors hasn't been\n  confirmed, and our manuscript need to be improved and revised. The new\n  version may need a long time to modified, so we decide to withdrew it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering, a long-standing and important research problem,\nfocuses on mining complementary information from diverse views. However,\nexisting works often fuse multiple views' representations or handle clustering\nin a common feature space, which may result in their entanglement especially\nfor visual representations. To address this issue, we present a novel VAE-based\nmulti-view clustering framework (Multi-VAE) by learning disentangled visual\nrepresentations. Concretely, we define a view-common variable and multiple\nview-peculiar variables in the generative model. The prior of view-common\nvariable obeys approximately discrete Gumbel Softmax distribution, which is\nintroduced to extract the common cluster factor of multiple views. Meanwhile,\nthe prior of view-peculiar variable follows continuous Gaussian distribution,\nwhich is used to represent each view's peculiar visual factors. By controlling\nthe mutual information capacity to disentangle the view-common and\nview-peculiar representations, continuous visual information of multiple views\ncan be separated so that their common discrete cluster information can be\neffectively mined. Experimental results demonstrate that Multi-VAE enjoys the\ndisentangled and explainable visual representations, while obtaining superior\nclustering performance compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 16:23:28 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 14:29:15 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Xu", "Jie", ""], ["Ren", "Yazhou", ""], ["Tang", "Huayi", ""], ["Pu", "Xiaorong", ""], ["Zhu", "Xiaofeng", ""], ["Zeng", "Ming", ""], ["He", "Lifang", ""]]}, {"id": "2106.11236", "submitter": "Sara Beery", "authors": "Sara Beery, Elizabeth Bondi", "title": "Can poachers find animals from public camera trap images?", "comments": "CV4Animals Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To protect the location of camera trap data containing sensitive, high-target\nspecies, many ecologists randomly obfuscate the latitude and longitude of the\ncamera when publishing their data. For example, they may publish a random\nlocation within a 1km radius of the true camera location for each camera in\ntheir network. In this paper, we investigate the robustness of geo-obfuscation\nfor maintaining camera trap location privacy, and show via a case study that a\nfew simple, intuitive heuristics and publicly available satellite rasters can\nbe used to reduce the area likely to contain the camera by 87% (assuming random\nobfuscation within 1km), demonstrating that geo-obfuscation may be less\neffective than previously believed.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 16:31:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Beery", "Sara", ""], ["Bondi", "Elizabeth", ""]]}, {"id": "2106.11239", "submitter": "Dan Jia", "authors": "Dan Jia and Alexander Hermans and Bastian Leibe", "title": "Domain and Modality Gaps for LiDAR-based Person Detection on Mobile\n  Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person detection is a crucial task for mobile robots navigating in\nhuman-populated environments and LiDAR sensors are promising for this task,\ngiven their accurate depth measurements and large field of view. This paper\nstudies existing LiDAR-based person detectors with a particular focus on mobile\nrobot scenarios (e.g. service robot or social robot), where persons are\nobserved more frequently and in much closer ranges, compared to the driving\nscenarios. We conduct a series of experiments, using the recently released\nJackRabbot dataset and the state-of-the-art detectors based on 3D or 2D LiDAR\nsensors (CenterPoint and DR-SPAAM respectively). These experiments revolve\naround the domain gap between driving and mobile robot scenarios, as well as\nthe modality gap between 3D and 2D LiDAR sensors. For the domain gap, we aim to\nunderstand if detectors pretrained on driving datasets can achieve good\nperformance on the mobile robot scenarios, for which there are currently no\ntrained models readily available. For the modality gap, we compare detectors\nthat use 3D or 2D LiDAR, from various aspects, including performance, runtime,\nlocalization accuracy, robustness to range and crowdedness. The results from\nour experiments provide practical insights into LiDAR-based person detection\nand facilitate informed decisions for relevant mobile robot designs and\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 16:35:49 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Jia", "Dan", ""], ["Hermans", "Alexander", ""], ["Leibe", "Bastian", ""]]}, {"id": "2106.11240", "submitter": "John Howard", "authors": "John J. Howard, Yevgeniy B. Sirotin, Jerry L. Tipton, and Arun R.\n  Vemury", "title": "Reliability and Validity of Image-Based and Self-Reported Skin Phenotype\n  Metrics", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With increasing adoption of face recognition systems, it is important to\nensure adequate performance of these technologies across demographic groups.\nRecently, phenotypes such as skin-tone, have been proposed as superior\nalternatives to traditional race categories when exploring performance\ndifferentials. However, there is little consensus regarding how to\nappropriately measure skin-tone in evaluations of biometric performance or in\nAI more broadly. In this study, we explore the relationship between\nface-area-lightness-measures (FALMs) estimated from images and ground-truth\nskin readings collected using a device designed to measure human skin. FALMs\nestimated from different images of the same individual varied significantly\nrelative to ground-truth FALM. This variation was only reduced by greater\ncontrol of acquisition (camera, background, and environment). Next, we compare\nground-truth FALM to Fitzpatrick Skin Types (FST) categories obtained using the\nstandard, in-person, medical survey and show FST is poorly predictive of\nskin-tone. Finally, we show how noisy estimation of FALM leads to errors\nselecting explanatory factors for demographic differentials. These results\ndemonstrate that measures of skin-tone for biometric performance evaluations\nmust come from objective, characterized, and controlled sources. Further,\ndespite this being a currently practiced approach, estimating FST categories\nand FALMs from uncontrolled imagery does not provide an appropriate measure of\nskin-tone.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 16:12:24 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Howard", "John J.", ""], ["Sirotin", "Yevgeniy B.", ""], ["Tipton", "Jerry L.", ""], ["Vemury", "Arun R.", ""]]}, {"id": "2106.11250", "submitter": "Hao Tan", "authors": "Hao Tan, Jie Lei, Thomas Wolf, Mohit Bansal", "title": "VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive\n  Learning", "comments": "Under review, 23 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding relies on perceiving the global content and modeling its\ninternal connections (e.g., causality, movement, and spatio-temporal\ncorrespondence). To learn these interactions, we apply a mask-then-predict\npre-training task on discretized video tokens generated via VQ-VAE. Unlike\nlanguage, where the text tokens are more independent, neighboring video tokens\ntypically have strong correlations (e.g., consecutive video frames usually look\nvery similar), and hence uniformly masking individual tokens will make the task\ntoo trivial to learn useful representations. To deal with this issue, we\npropose a block-wise masking strategy where we mask neighboring video tokens in\nboth spatial and temporal domains. We also add an augmentation-free contrastive\nlearning method to further capture the global content by predicting whether the\nvideo clips are sampled from the same video. We pre-train our model on\nuncurated videos and show that our pre-trained model can reach state-of-the-art\nresults on several video understanding datasets (e.g., SSV2, Diving48). Lastly,\nwe provide detailed analyses on model scalability and pre-training method\ndesign. Code is released at https://github.com/airsplay/vimpac.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 16:48:19 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Tan", "Hao", ""], ["Lei", "Jie", ""], ["Wolf", "Thomas", ""], ["Bansal", "Mohit", ""]]}, {"id": "2106.11253", "submitter": "Na Li", "authors": "Na Li and Yao Liu", "title": "Applying VertexShuffle Toward 360-Degree Video Super-Resolution on\n  Focused-Icosahedral-Mesh", "comments": "This paper introduce a new mesh representation and a new upsampling\n  method on a mesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the emerging of 360-degree image/video, augmented reality (AR) and\nvirtual reality (VR), the demand for analysing and processing spherical signals\nget tremendous increase. However, plenty of effort paid on planar signals that\nprojected from spherical signals, which leading to some problems, e.g. waste of\npixels, distortion. Recent advances in spherical CNN have opened up the\npossibility of directly analysing spherical signals. However, they pay\nattention to the full mesh which makes it infeasible to deal with situations in\nreal-world application due to the extremely large bandwidth requirement. To\naddress the bandwidth waste problem associated with 360-degree video streaming\nand save computation, we exploit Focused Icosahedral Mesh to represent a small\narea and construct matrices to rotate spherical content to the focused mesh\narea. We also proposed a novel VertexShuffle operation that can significantly\nimprove both the performance and the efficiency compared to the original\nMeshConv Transpose operation introduced in UGSCNN. We further apply our\nproposed methods on super resolution model, which is the first to propose a\nspherical super-resolution model that directly operates on a mesh\nrepresentation of spherical pixels of 360-degree data. To evaluate our model,\nwe also collect a set of high-resolution 360-degree videos to generate a\nspherical image dataset. Our experiments indicate that our proposed spherical\nsuper-resolution model achieves significant benefits in terms of both\nperformance and inference time compared to the baseline spherical\nsuper-resolution model that uses the simple MeshConv Transpose operation. In\nsummary, our model achieves great super-resolution performance on 360-degree\ninputs, achieving 32.79 dB PSNR on average when super-resoluting 16x vertices\non the mesh.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 16:53:57 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Na", ""], ["Liu", "Yao", ""]]}, {"id": "2106.11272", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen, Hao Zhang", "title": "Neural Marching Cubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:18:52 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 00:13:22 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chen", "Zhiqin", ""], ["Zhang", "Hao", ""]]}, {"id": "2106.11277", "submitter": "Ce Zhang Mr.", "authors": "Ce Zhang, Azim Eskandarian, Xuelai Du", "title": "Attention-based Neural Network for Driving Environment Complexity\n  Perception", "comments": "Accepted by 2021 IEEE Intelligent Transportation Systems Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environment perception is crucial for autonomous vehicle (AV) safety. Most\nexisting AV perception algorithms have not studied the surrounding environment\ncomplexity and failed to include the environment complexity parameter. This\npaper proposes a novel attention-based neural network model to predict the\ncomplexity level of the surrounding driving environment. The proposed model\ntakes naturalistic driving videos and corresponding vehicle dynamics parameters\nas input. It consists of a Yolo-v3 object detection algorithm, a heat map\ngeneration algorithm, CNN-based feature extractors, and attention-based feature\nextractors for both video and time-series vehicle dynamics data inputs to\nextract features. The output from the proposed algorithm is a surrounding\nenvironment complexity parameter. The Berkeley DeepDrive dataset (BDD Dataset)\nand subjectively labeled surrounding environment complexity levels are used for\nmodel training and validation to evaluate the algorithm. The proposed\nattention-based network achieves 91.22% average classification accuracy to\nclassify the surrounding environment complexity. It proves that the environment\ncomplexity level can be accurately predicted and applied for future AVs'\nenvironment perception studies.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:27:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhang", "Ce", ""], ["Eskandarian", "Azim", ""], ["Du", "Xuelai", ""]]}, {"id": "2106.11280", "submitter": "Yapkan Choi", "authors": "Yapkan Choi, Yeshwanth Napolean, Jan C. van Gemert", "title": "The Arm-Swing Is Discriminative in Video Gait Recognition for Athlete\n  Re-Identification", "comments": "ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we evaluate running gait as an attribute for video person\nre-identification in a long-distance running event. We show that running gait\nrecognition achieves competitive performance compared to appearance-based\napproaches in the cross-camera retrieval task and that gait and appearance\nfeatures are complementary to each other. For gait, the arm swing during\nrunning is less distinguishable when using binary gait silhouettes, due to\nambiguity in the torso region. We propose to use human semantic parsing to\ncreate partial gait silhouettes where the torso is left out. Leaving out the\ntorso improves recognition results by allowing the arm swing to be more visible\nin the frontal and oblique viewing angles, which offers hints that arm swings\nare somewhat personal. Experiments show an increase of 3.2% mAP on the\nCampusRun and increased accuracy with 4.8% in the frontal and rear view on\nCASIA-B, compared to using the full body silhouettes.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:28:07 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Choi", "Yapkan", ""], ["Napolean", "Yeshwanth", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2106.11297", "submitter": "Michael S. Ryoo", "authors": "Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani,\n  Anelia Angelova", "title": "TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel visual representation learning which\nrelies on a handful of adaptively learned tokens, and which is applicable to\nboth image and video understanding tasks. Instead of relying on hand-designed\nsplitting strategies to obtain visual tokens and processing a large number of\ndensely sampled patches for attention, our approach learns to mine important\ntokens in visual data. This results in efficiently and effectively finding a\nfew important visual tokens and enables modeling of pairwise attention between\nsuch tokens, over a longer temporal horizon for videos, or the spatial content\nin images. Our experiments demonstrate strong performance on several\nchallenging benchmarks for both image and video recognition tasks. Importantly,\ndue to our tokens being adaptive, we accomplish competitive results at\nsignificantly reduced compute amount.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:55:59 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ryoo", "Michael S.", ""], ["Piergiovanni", "AJ", ""], ["Arnab", "Anurag", ""], ["Dehghani", "Mostafa", ""], ["Angelova", "Anelia", ""]]}, {"id": "2106.11303", "submitter": "Andreas Blattmann", "authors": "Andreas Blattmann, Timo Milbich, Michael Dorkenwald, Bj\\\"orn Ommer", "title": "Understanding Object Dynamics for Interactive Image-to-Video Synthesis", "comments": "CVPR 2021, project page available at https://bit.ly/3cxfA2L", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What would be the effect of locally poking a static scene? We present an\napproach that learns naturally-looking global articulations caused by a local\nmanipulation at a pixel level. Training requires only videos of moving objects\nbut no information of the underlying manipulation of the physical scene. Our\ngenerative model learns to infer natural object dynamics as a response to user\ninteraction and learns about the interrelations between different object body\nregions. Given a static image of an object and a local poking of a pixel, the\napproach then predicts how the object would deform over time. In contrast to\nexisting work on video prediction, we do not synthesize arbitrary realistic\nvideos but enable local interactive control of the deformation. Our model is\nnot restricted to particular object categories and can transfer dynamics onto\nnovel unseen object instances. Extensive experiments on diverse objects\ndemonstrate the effectiveness of our approach compared to common video\nprediction frameworks. Project page is available at https://bit.ly/3cxfA2L .\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:57:39 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Blattmann", "Andreas", ""], ["Milbich", "Timo", ""], ["Dorkenwald", "Michael", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2106.11304", "submitter": "Jindong Gu", "authors": "Jindong Gu, Wei Liu, Yonglong Tian", "title": "Simple Distillation Baselines for Improving Small Self-supervised Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While large self-supervised models have rivalled the performance of their\nsupervised counterparts, small models still struggle. In this report, we\nexplore simple baselines for improving small self-supervised models via\ndistillation, called SimDis. Specifically, we present an offline-distillation\nbaseline, which establishes a new state-of-the-art, and an online-distillation\nbaseline, which achieves similar performance with minimal computational\noverhead. We hope these baselines will provide useful experience for relevant\nfuture research. Code is available at: https://github.com/JindongGu/SimDis/\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:58:05 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gu", "Jindong", ""], ["Liu", "Wei", ""], ["Tian", "Yonglong", ""]]}, {"id": "2106.11308", "submitter": "Vladislav Golyanik", "authors": "Vladislav Golyanik and Soshi Shimada and Christian Theobalt", "title": "Fast Simultaneous Gravitational Alignment of Multiple Point Sets", "comments": "Project webpage: http://gvv.mpi-inf.mpg.de/projects/MBGA/", "journal-ref": "3DV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of simultaneous rigid alignment of multiple unordered point sets\nwhich is unbiased towards any of the inputs has recently attracted increasing\ninterest, and several reliable methods have been newly proposed. While being\nremarkably robust towards noise and clustered outliers, current approaches\nrequire sophisticated initialisation schemes and do not scale well to large\npoint sets. This paper proposes a new resilient technique for simultaneous\nregistration of multiple point sets by interpreting the latter as particle\nswarms rigidly moving in the mutually induced force fields. Thanks to the\nimproved simulation with altered physical laws and acceleration of globally\nmultiply-linked point interactions with a 2^D-tree (D is the space\ndimensionality), our Multi-Body Gravitational Approach (MBGA) is robust to\nnoise and missing data while supporting more massive point sets than previous\nmethods (with 10^5 points and more). In various experimental settings, MBGA is\nshown to outperform several baseline point set alignment approaches in terms of\naccuracy and runtime. We make our source code available for the community to\nfacilitate the reproducibility of the results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:59:40 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Golyanik", "Vladislav", ""], ["Shimada", "Soshi", ""], ["Theobalt", "Christian", ""]]}, {"id": "2106.11309", "submitter": "Zechun Liu", "authors": "Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang,\n  Kwang-Ting Cheng", "title": "How Do Adam and Training Strategies Help BNNs Optimization?", "comments": "ICML 2021. Code and models are available at\n  https://github.com/liuzechun/AdamBNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The best performing Binary Neural Networks (BNNs) are usually attained using\nAdam optimization and its multi-step training variants. However, to the best of\nour knowledge, few studies explore the fundamental reasons why Adam is superior\nto other optimizers like SGD for BNN optimization or provide analytical\nexplanations that support specific training strategies. To address this, in\nthis paper we first investigate the trajectories of gradients and weights in\nBNNs during the training process. We show the regularization effect of\nsecond-order momentum in Adam is crucial to revitalize the weights that are\ndead due to the activation saturation in BNNs. We find that Adam, through its\nadaptive learning rate strategy, is better equipped to handle the rugged loss\nsurface of BNNs and reaches a better optimum with higher generalization\nability. Furthermore, we inspect the intriguing role of the real-valued weights\nin binary networks, and reveal the effect of weight decay on the stability and\nsluggishness of BNN optimization. Through extensive experiments and analysis,\nwe derive a simple training scheme, building on existing Adam-based\noptimization, which achieves 70.5% top-1 accuracy on the ImageNet dataset using\nthe same architecture as the state-of-the-art ReActNet while achieving 1.1%\nhigher accuracy. Code and models are available at\nhttps://github.com/liuzechun/AdamBNN.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:59:51 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Liu", "Zechun", ""], ["Shen", "Zhiqiang", ""], ["Li", "Shichao", ""], ["Helwegen", "Koen", ""], ["Huang", "Dong", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "2106.11310", "submitter": "Chao-Yuan Wu", "authors": "Chao-Yuan Wu, Philipp Kr\\\"ahenb\\\"uhl", "title": "Towards Long-Form Video Understanding", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our world offers a never-ending stream of visual stimuli, yet today's vision\nsystems only accurately recognize patterns within a few seconds. These systems\nunderstand the present, but fail to contextualize it in past or future events.\nIn this paper, we study long-form video understanding. We introduce a framework\nfor modeling long-form videos and develop evaluation protocols on large-scale\ndatasets. We show that existing state-of-the-art short-term models are limited\nfor long-form tasks. A novel object-centric transformer-based video recognition\narchitecture performs significantly better on 7 diverse tasks. It also\noutperforms comparable state-of-the-art on the AVA dataset.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:59:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wu", "Chao-Yuan", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "2106.11322", "submitter": "J\\'er\\'emy Lebreton", "authors": "J\\'er\\'emy Lebreton, Roland Brochard, Matthieu Baudry, Gr\\'egory\n  Jonniaux, Adrien Hadj Salah, Keyvan Kanani, Matthieu Le Goff, Aurore Masson,\n  Nicolas Ollagnier, Paolo Panicucci, Amsha Proag, Cyril Robin", "title": "Image simulation for space applications with the SurRender software", "comments": "11th International ESA Conference on Guidance, Navigation & Control\n  Systems, 22 - 25 June 2021 16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP astro-ph.IM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Processing algorithms for vision-based navigation require reliable\nimage simulation capacities. In this paper we explain why traditional rendering\nengines may present limitations that are potentially critical for space\napplications. We introduce Airbus SurRender software v7 and provide details on\nfeatures that make it a very powerful space image simulator. We show how\nSurRender is at the heart of the development processes of our computer vision\nsolutions and we provide a series of illustrations of rendered images for\nvarious use cases ranging from Moon and Solar System exploration, to in orbit\nrendezvous and planetary robotics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:00:01 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Lebreton", "J\u00e9r\u00e9my", ""], ["Brochard", "Roland", ""], ["Baudry", "Matthieu", ""], ["Jonniaux", "Gr\u00e9gory", ""], ["Salah", "Adrien Hadj", ""], ["Kanani", "Keyvan", ""], ["Goff", "Matthieu Le", ""], ["Masson", "Aurore", ""], ["Ollagnier", "Nicolas", ""], ["Panicucci", "Paolo", ""], ["Proag", "Amsha", ""], ["Robin", "Cyril", ""]]}, {"id": "2106.11330", "submitter": "Liping Zhang", "authors": "Liping Zhang and Simon Chun-Ho Yu", "title": "Context-aware PolyUNet for Liver and Lesion Segmentation from Abdominal\n  CT Images", "comments": "7 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate liver and lesion segmentation from computed tomography (CT) images\nare highly demanded in clinical practice for assisting the diagnosis and\nassessment of hepatic tumor disease. However, automatic liver and lesion\nsegmentation from contrast-enhanced CT volumes is extremely challenging due to\nthe diversity in contrast, resolution, and quality of images. Previous methods\nbased on UNet for 2D slice-by-slice or 3D volume-by-volume segmentation either\nlack sufficient spatial contexts or suffer from high GPU computational cost,\nwhich limits the performance. To tackle these issues, we propose a novel\ncontext-aware PolyUNet for accurate liver and lesion segmentation. It jointly\nexplores structural diversity and consecutive t-adjacent slices to enrich\nfeature expressive power and spatial contextual information while avoiding the\noverload of GPU memory consumption. In addition, we utilize zoom out/in and\ntwo-stage refinement strategy to exclude the irrelevant contexts and focus on\nthe specific region for the fine-grained segmentation. Our method achieved very\ncompetitive performance at the MICCAI 2017 Liver Tumor Segmentation (LiTS)\nChallenge among all tasks with a single model and ranked the $3^{rd}$,\n$12^{th}$, $2^{nd}$, and $5^{th}$ places in the liver segmentation, lesion\nsegmentation, lesion detection, and tumor burden estimation, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:01:04 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zhang", "Liping", ""], ["Yu", "Simon Chun-Ho", ""]]}, {"id": "2106.11339", "submitter": "Freddie Bickford Smith", "authors": "Freddie Bickford Smith, Brett D Roads, Xiaoliang Luo, Bradley C Love", "title": "Understanding top-down attention using task-oriented ablation design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Top-down attention allows neural networks, both artificial and biological, to\nfocus on the information most relevant for a given task. This is known to\nenhance performance in visual perception. But it remains unclear how attention\nbrings about its perceptual boost, especially when it comes to naturalistic\nsettings like recognising an object in an everyday scene. What aspects of a\nvisual task does attention help to deal with? We aim to answer this with a\ncomputational experiment based on a general framework called task-oriented\nablation design. First we define a broad range of visual tasks and identify six\nfactors that underlie task variability. Then on each task we compare the\nperformance of two neural networks, one with top-down attention and one\nwithout. These comparisons reveal the task-dependence of attention's perceptual\nboost, giving a clearer idea of the role attention plays. Whereas many existing\ncognitive accounts link attention to stimulus-level variables, such as visual\nclutter and object scale, we find greater explanatory power in system-level\nvariables that capture the interaction between the model, the distribution of\ntraining data and the task format. This finding suggests a shift in how\nattention is studied could be fruitful. We make publicly available our code and\nresults, along with statistics relevant to ImageNet-based experiments beyond\nthis one. Our contribution serves to support the development of more human-like\nvision models and the design of more informative machine-learning experiments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 21:01:47 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Smith", "Freddie Bickford", ""], ["Roads", "Brett D", ""], ["Luo", "Xiaoliang", ""], ["Love", "Bradley C", ""]]}, {"id": "2106.11342", "submitter": "Aston Zhang", "authors": "Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola", "title": "Dive into Deep Learning", "comments": "(HTML) https://D2L.ai (GitHub) https://github.com/d2l-ai/d2l-en/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This open-source book represents our attempt to make deep learning\napproachable, teaching readers the concepts, the context, and the code. The\nentire book is drafted in Jupyter notebooks, seamlessly integrating exposition\nfigures, math, and interactive examples with self-contained code. Our goal is\nto offer a resource that could (i) be freely available for everyone; (ii) offer\nsufficient technical depth to provide a starting point on the path to actually\nbecoming an applied machine learning scientist; (iii) include runnable code,\nshowing readers how to solve problems in practice; (iv) allow for rapid\nupdates, both by us and also by the community at large; (v) be complemented by\na forum for interactive discussion of technical details and to answer\nquestions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:19:46 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 16:51:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Aston", ""], ["Lipton", "Zachary C.", ""], ["Li", "Mu", ""], ["Smola", "Alexander J.", ""]]}, {"id": "2106.11344", "submitter": "David Acuna", "authors": "David Acuna, Guojun Zhang, Marc T. Law, Sanja Fidler", "title": "f-Domain-Adversarial Learning: Theory and Algorithms", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation is used in many machine learning applications\nwhere, during training, a model has access to unlabeled data in the target\ndomain, and a related labeled dataset. In this paper, we introduce a novel and\ngeneral domain-adversarial framework. Specifically, we derive a novel\ngeneralization bound for domain adaptation that exploits a new measure of\ndiscrepancy between distributions based on a variational characterization of\nf-divergences. It recovers the theoretical results from Ben-David et al.\n(2010a) as a special case and supports divergences used in practice. Based on\nthis bound, we derive a new algorithmic framework that introduces a key\ncorrection in the original adversarial training method of Ganin et al. (2016).\nWe show that many regularizers and ad-hoc objectives introduced over the last\nyears in this framework are then not required to achieve performance comparable\nto (if not better than) state-of-the-art domain-adversarial methods.\nExperimental analysis conducted on real-world natural language and computer\nvision datasets show that our framework outperforms existing baselines, and\nobtains the best results for f-divergences that were not considered previously\nin domain-adversarial learning.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:21:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Acuna", "David", ""], ["Zhang", "Guojun", ""], ["Law", "Marc T.", ""], ["Fidler", "Sanja", ""]]}, {"id": "2106.11346", "submitter": "Xingyuan Bu", "authors": "Xingyuan Bu, Junran Peng, Junjie Yan, Tieniu Tan, Zhaoxiang Zhang", "title": "GAIA: A Transfer Learning System of Object Detection that Fits Your\n  Needs", "comments": "CVPR2021. The first two authors contribute equally. Code is released\n  at https://github.com/GAIA-vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning with pre-training on large-scale datasets has played an\nincreasingly significant role in computer vision and natural language\nprocessing recently. However, as there exist numerous application scenarios\nthat have distinctive demands such as certain latency constraints and\nspecialized data distributions, it is prohibitively expensive to take advantage\nof large-scale pre-training for per-task requirements. In this paper, we focus\non the area of object detection and present a transfer learning system named\nGAIA, which could automatically and efficiently give birth to customized\nsolutions according to heterogeneous downstream needs. GAIA is capable of\nproviding powerful pre-trained weights, selecting models that conform to\ndownstream demands such as latency constraints and specified data domains, and\ncollecting relevant data for practitioners who have very few datapoints for\ntheir tasks. With GAIA, we achieve promising results on COCO, Objects365, Open\nImages, Caltech, CityPersons, and UODB which is a collection of datasets\nincluding KITTI, VOC, WiderFace, DOTA, Clipart, Comic, and more. Taking COCO as\nan example, GAIA is able to efficiently produce models covering a wide range of\nlatency from 16ms to 53ms, and yields AP from 38.2 to 46.5 without whistles and\nbells. To benefit every practitioner in the community of object detection, GAIA\nis released at https://github.com/GAIA-vision.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:24:20 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Bu", "Xingyuan", ""], ["Peng", "Junran", ""], ["Yan", "Junjie", ""], ["Tan", "Tieniu", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "2106.11354", "submitter": "Amol Joshi", "authors": "Amol S. Joshi, Ali Dabouei, Jeremy Dawson, Nasser M. Nasrabadi", "title": "FDeblur-GAN: Fingerprint Deblurring using Generative Adversarial Network", "comments": "8 Pages, Accepted in IJCB Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While working with fingerprint images acquired from crime scenes, mobile\ncameras, or low-quality sensors, it becomes difficult for automated\nidentification systems to verify the identity due to image blur and distortion.\nWe propose a fingerprint deblurring model FDeblur-GAN, based on the conditional\nGenerative Adversarial Networks (cGANs) and multi-stage framework of the stack\nGAN. Additionally, we integrate two auxiliary sub-networks into the model for\nthe deblurring task. The first sub-network is a ridge extractor model. It is\nadded to generate ridge maps to ensure that fingerprint information and\nminutiae are preserved in the deblurring process and prevent the model from\ngenerating erroneous minutiae. The second sub-network is a verifier that helps\nthe generator to preserve the ID information during the generation process.\nUsing a database of blurred fingerprints and corresponding ridge maps, the deep\nnetwork learns to deblur from the input blurry samples. We evaluate the\nproposed method in combination with two different fingerprint matching\nalgorithms. We achieved an accuracy of 95.18% on our fingerprint database for\nthe task of matching deblurred and ground truth fingerprints.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:37:20 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Joshi", "Amol S.", ""], ["Dabouei", "Ali", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2106.11359", "submitter": "Trisha Singhal", "authors": "Trisha Singhal, Junhua Liu, Lucienne T. M. Blessing, Kwan Hui Lim", "title": "Photozilla: A Large-Scale Photography Dataset and Visual Embedding for\n  20 Photography Styles", "comments": "In the Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshops, 2021. (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of social media platforms has been a catalyst for the development\nof digital photography that engendered a boom in vision applications. With this\nmotivation, we introduce a large-scale dataset termed 'Photozilla', which\nincludes over 990k images belonging to 10 different photographic styles. The\ndataset is then used to train 3 classification models to automatically classify\nthe images into the relevant style which resulted in an accuracy of ~96%. With\nthe rapid evolution of digital photography, we have seen new types of\nphotography styles emerging at an exponential rate. On that account, we present\na novel Siamese-based network that uses the trained classification models as\nthe base architecture to adapt and classify unseen styles with only 25 training\nsamples. We report an accuracy of over 68% for identifying 10 other distinct\ntypes of photography styles. This dataset can be found at\nhttps://trisha025.github.io/Photozilla/\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:45:06 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Singhal", "Trisha", ""], ["Liu", "Junhua", ""], ["Blessing", "Lucienne T. M.", ""], ["Lim", "Kwan Hui", ""]]}, {"id": "2106.11379", "submitter": "Daniel Ruiz", "authors": "Daniel V. Ruiz, Eduardo Todt", "title": "BEyond observation: an approach for ObjectNav", "comments": "Presented at the 2th Embodied AI Workshop at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of automation, unmanned vehicles became a hot topic both as\ncommercial products and as a scientific research topic. It composes a\nmulti-disciplinary field of robotics that encompasses embedded systems, control\ntheory, path planning, Simultaneous Localization and Mapping (SLAM), scene\nreconstruction, and pattern recognition. In this work, we present our\nexploratory research of how sensor data fusion and state-of-the-art machine\nlearning algorithms can perform the Embodied Artificial Intelligence (E-AI)\ntask called Visual Semantic Navigation. This task, a.k.a Object-Goal Navigation\n(ObjectNav) consists of autonomous navigation using egocentric visual\nobservations to reach an object belonging to the target semantic class without\nprior knowledge of the environment. Our method reached fourth place on the\nHabitat Challenge 2021 ObjectNav on the Minival phase and the Test-Standard\nPhase.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:27:16 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ruiz", "Daniel V.", ""], ["Todt", "Eduardo", ""]]}, {"id": "2106.11395", "submitter": "Agatha Hennigen de Mattos", "authors": "Agatha C. H. de Mattos, Gavin McArdle, Michela Bertolotto", "title": "Mapping Slums with Medium Resolution Satellite Imagery: a Comparative\n  Analysis of Multi-Spectral Data and Grey-level Co-occurrence Matrix\n  Techniques", "comments": "Accepted at the 3rd Workshop on Artificial Intelligence for Social\n  Good (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The UN-Habitat estimates that over one billion people live in slums around\nthe world. However, state-of-the-art techniques to detect the location of slum\nareas employ high-resolution satellite imagery, which is costly to obtain and\nprocess. As a result, researchers have started to look at utilising free and\nopen-access medium resolution satellite imagery. Yet, there is no clear\nconsensus on which data preparation and machine learning approaches are the\nmost appropriate to use with such imagery data. In this paper, we evaluate two\ntechniques (multi-spectral data and grey-level co-occurrence matrix feature\nextraction) on an open-access dataset consisting of labelled Sentinel-2 images\nwith a spatial resolution of 10 meters. Both techniques were paired with a\ncanonical correlation forests classifier. The results show that the grey-level\nco-occurrence matrix performed better than multi-spectral data for all four\ncities. It had an average accuracy for the slum class of 97% and a mean\nintersection over union of 94%, while multi-spectral data had 75% and 64% for\nthe respective metrics. These results indicate that open-access satellite\nimagery with a resolution of at least 10 meters may be suitable for keeping\ntrack of development goals such as the detection of slums in cities.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 20:11:27 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["de Mattos", "Agatha C. H.", ""], ["McArdle", "Gavin", ""], ["Bertolotto", "Michela", ""]]}, {"id": "2106.11396", "submitter": "Feihu Huang", "authors": "Feihu Huang and Heng Huang", "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods", "comments": "66 pages, 2 tables. We add the detailed proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 20:16:40 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:44:03 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Huang", "Feihu", ""], ["Huang", "Heng", ""]]}, {"id": "2106.11401", "submitter": "Eslam Bakr Mohamed", "authors": "Eslam Mohamed and Ahmed El-Sallab", "title": "Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object\n  Detection and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Moving objects have special importance for Autonomous Driving tasks.\nDetecting moving objects can be posed as Moving Object Segmentation, by\nsegmenting the object pixels, or Moving Object Detection, by generating a\nbounding box for the moving targets. In this paper, we present a Multi-Task\nLearning architecture, based on Transformers, to jointly perform both tasks\nthrough one network. Due to the importance of the motion features to the task,\nthe whole setup is based on a Spatio-Temporal aggregation. We evaluate the\nperformance of the individual tasks architecture versus the MTL setup, both\nwith early shared encoders, and late shared encoder-decoder transformers. For\nthe latter, we present a novel joint tasks query decoder transformer, that\nenables us to have tasks dedicated heads out of the shared model. To evaluate\nour approach, we use the KITTI MOD [29] data set. Results show1.5% mAP\nimprovement for Moving Object Detection, and 2%IoU improvement for Moving\nObject Segmentation, over the individual tasks networks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 20:30:44 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mohamed", "Eslam", ""], ["El-Sallab", "Ahmed", ""]]}, {"id": "2106.11422", "submitter": "Eslam Bakr Mohamed", "authors": "Eslam Mohamed, Ahmad El-Sallab", "title": "MODETR: Moving Object Detection with Transformers", "comments": null, "journal-ref": "Machine Learning for Autonomous Driving Workshop at the 34th\n  Conference on Neural Information Processing Systems (NeurIPS 2020),\n  Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Moving Object Detection (MOD) is a crucial task for the Autonomous Driving\npipeline. MOD is usually handled via 2-stream convolutional architectures that\nincorporates both appearance and motion cues, without considering the\ninter-relations between the spatial or motion features. In this paper, we\ntackle this problem through multi-head attention mechanisms, both across the\nspatial and motion streams. We propose MODETR; a Moving Object DEtection\nTRansformer network, comprised of multi-stream transformer encoders for both\nspatial and motion modalities, and an object transformer decoder that produces\nthe moving objects bounding boxes using set predictions. The whole architecture\nis trained end-to-end using bi-partite loss. Several methods of incorporating\nmotion cues with the Transformer model are explored, including two-stream RGB\nand Optical Flow (OF) methods, and multi-stream architectures that take\nadvantage of sequence information. To incorporate the temporal information, we\npropose a new Temporal Positional Encoding (TPE) approach to extend the Spatial\nPositional Encoding(SPE) in DETR. We explore two architectural choices for\nthat, balancing between speed and time. To evaluate the our network, we perform\nthe MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of\nthe Transformer network for MOD over the state-of-the art methods. Moreover,\nthe proposed TPE encoding provides 10% mAP improvement over the SPE baseline.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 21:56:46 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mohamed", "Eslam", ""], ["El-Sallab", "Ahmad", ""]]}, {"id": "2106.11423", "submitter": "Liwen Hu", "authors": "Huiwen Luo, Koki Nagano, Han-Wei Kung, Mclean Goldwhite, Qingguo Xu,\n  Zejian Wang, Lingyu Wei, Liwen Hu, Hao Li", "title": "Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce a highly robust GAN-based framework for digitizing a normalized\n3D avatar of a person from a single unconstrained photo. While the input image\ncan be of a smiling person or taken in extreme lighting conditions, our method\ncan reliably produce a high-quality textured model of a person's face in\nneutral expression and skin textures under diffuse lighting condition.\nCutting-edge 3D face reconstruction methods use non-linear morphable face\nmodels combined with GAN-based decoders to capture the likeness and details of\na person but fail to produce neutral head models with unshaded albedo textures\nwhich is critical for creating relightable and animation-friendly avatars for\nintegration in virtual environments. The key challenges for existing methods to\nwork is the lack of training and ground truth data containing normalized 3D\nfaces. We propose a two-stage approach to address this problem. First, we adopt\na highly robust normalized 3D face generator by embedding a non-linear\nmorphable face model into a StyleGAN2 network. This allows us to generate\ndetailed but normalized facial assets. This inference is then followed by a\nperceptual refinement step that uses the generated assets as regularization to\ncope with the limited available training samples of normalized faces. We\nfurther introduce a Normalized Face Dataset, which consists of a combination\nphotogrammetry scans, carefully selected photographs, and generated fake people\nwith neutral expressions in diffuse lighting conditions. While our prepared\ndataset contains two orders of magnitude less subjects than cutting edge\nGAN-based 3D facial reconstruction methods, we show that it is possible to\nproduce high-quality normalized face models for very challenging unconstrained\ninput images, and demonstrate superior performance to the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 21:57:16 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Luo", "Huiwen", ""], ["Nagano", "Koki", ""], ["Kung", "Han-Wei", ""], ["Goldwhite", "Mclean", ""], ["Xu", "Qingguo", ""], ["Wang", "Zejian", ""], ["Wei", "Lingyu", ""], ["Hu", "Liwen", ""], ["Li", "Hao", ""]]}, {"id": "2106.11437", "submitter": "Justin Leo", "authors": "Justin Leo and Jugal Kalita", "title": "Incremental Deep Neural Network Learning using Classification Confidence\n  Thresholding", "comments": "Accepted to IEEE TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2021.3087104", "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern neural networks for classification fail to take into account the\nconcept of the unknown. Trained neural networks are usually tested in an\nunrealistic scenario with only examples from a closed set of known classes. In\nan attempt to develop a more realistic model, the concept of working in an open\nset environment has been introduced. This in turn leads to the concept of\nincremental learning where a model with its own architecture and initial\ntrained set of data can identify unknown classes during the testing phase and\nautonomously update itself if evidence of a new class is detected. Some\nproblems that arise in incremental learning are inefficient use of resources to\nretrain the classifier repeatedly and the decrease of classification accuracy\nas multiple classes are added over time. This process of instantiating new\nclasses is repeated as many times as necessary, accruing errors. To address\nthese problems, this paper proposes the Classification Confidence Threshold\napproach to prime neural networks for incremental learning to keep accuracies\nhigh by limiting forgetting. A lean method is also used to reduce resources\nused in the retraining of the neural network. The proposed method is based on\nthe idea that a network is able to incrementally learn a new class even when\nexposed to a limited number samples associated with the new class. This method\ncan be applied to most existing neural networks with minimal changes to network\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 22:46:28 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Leo", "Justin", ""], ["Kalita", "Jugal", ""]]}, {"id": "2106.11447", "submitter": "Jo\\~ao Louren\\c{c}o Silva", "authors": "Jo\\~ao Louren\\c{c}o Silva, Miguel Nobre Menezes, Tiago Rodrigues,\n  Beatriz Silva, Fausto J. Pinto, Arlindo L. Oliveira", "title": "Encoder-Decoder Architectures for Clinically Relevant Coronary Artery\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary X-ray angiography is a crucial clinical procedure for the diagnosis\nand treatment of coronary artery disease, which accounts for roughly 16% of\nglobal deaths every year. However, the images acquired in these procedures have\nlow resolution and poor contrast, making lesion detection and assessment\nchallenging. Accurate coronary artery segmentation not only helps mitigate\nthese problems, but also allows the extraction of relevant anatomical features\nfor further analysis by quantitative methods. Although automated segmentation\nof coronary arteries has been proposed before, previous approaches have used\nnon-optimal segmentation criteria, leading to less useful results. Most methods\neither segment only the major vessel, discarding important information from the\nremaining ones, or segment the whole coronary tree based mostly on contrast\ninformation, producing a noisy output that includes vessels that are not\nrelevant for diagnosis. We adopt a better-suited clinical criterion and segment\nvessels according to their clinical relevance. Additionally, we simultaneously\nperform catheter segmentation, which may be useful for diagnosis due to the\nscale factor provided by the catheter's known diameter, and is a task that has\nnot yet been performed with good results. To derive the optimal approach, we\nconducted an extensive comparative study of encoder-decoder architectures\ntrained on a combination of focal loss and a variant of generalized dice loss.\nBased on the EfficientNet and the UNet++ architectures, we propose a line of\nefficient and high-performance segmentation models using a new decoder\narchitecture, the EfficientUNet++, whose best-performing version achieved\naverage dice scores of 0.8904 and 0.7526 for the artery and catheter classes,\nrespectively, and an average generalized dice score of 0.9234.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 23:32:11 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Silva", "Jo\u00e3o Louren\u00e7o", ""], ["Menezes", "Miguel Nobre", ""], ["Rodrigues", "Tiago", ""], ["Silva", "Beatriz", ""], ["Pinto", "Fausto J.", ""], ["Oliveira", "Arlindo L.", ""]]}, {"id": "2106.11466", "submitter": "Chinh Tran Khac", "authors": "Khac Chinh Tran, Marc Daniel and Jean Meunier", "title": "Gait analysis with curvature maps: A simulation study", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait analysis is an important aspect of clinical investigation for detecting\nneurological and musculoskeletal disorders and assessing the global health of a\npatient. In this paper we propose to focus our attention on extracting relevant\ncurvature information from the body surface provided by a depth camera. We\nassumed that the 3D mesh was made available in a previous step and demonstrated\nhow curvature maps could be useful to assess asymmetric anomalies with two\nsimple simulated abnormal gaits compared with a normal one. This research set\nthe grounds for the future development of a curvature-based gait analysis\nsystem for healthcare professionals.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 00:59:17 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Tran", "Khac Chinh", ""], ["Daniel", "Marc", ""], ["Meunier", "Jean", ""]]}, {"id": "2106.11467", "submitter": "Jianyun Xu", "authors": "Jingni Yuan, Jianyun Xu, Yushi Zhu", "title": "Multimodal trajectory forecasting based on discrete heat map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Argoverse motion forecasting competition, the task is to predict the\nprobabilistic future trajectory distribution for the interested targets in the\ntraffic scene. We use vectorized lane map and 2 s targets' history trajectories\nas input. Then the model outputs 6 forecasted trajectories with probability for\neach target.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 01:02:52 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Yuan", "Jingni", ""], ["Xu", "Jianyun", ""], ["Zhu", "Yushi", ""]]}, {"id": "2106.11478", "submitter": "Chen Liu", "authors": "Chen Liu", "title": "Fourier Transform Approximation as an Auxiliary Task for Image\n  Classification", "comments": "Work in progress. It will be very much appreciated if you can give\n  suggestions on additional experiments and analyses that may improve this\n  manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image reconstruction is likely the most predominant auxiliary task for image\nclassification, but we would like to think twice about this convention. In this\npaper, we investigated \"approximating the Fourier Transform of the input image\"\nas a potential alternative, in the hope that it may further boost the\nperformances on the primary task or introduce novel constraints not well\ncovered by image reconstruction. We experimented with five popular\nclassification architectures on the CIFAR-10 dataset, and the empirical results\nindicated that our proposed auxiliary task generally improves the\nclassification accuracy. More notably, the results showed that in certain cases\nour proposed auxiliary task may enhance the classifiers' resistance to\nadversarial attacks generated using the fast gradient sign method.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 01:47:18 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 02:57:42 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 22:25:39 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Liu", "Chen", ""]]}, {"id": "2106.11480", "submitter": "Mengyang Zhao", "authors": "Mengyang Zhao, Quan Liu, Aadarsh Jha, Ruining Deng, Tianyuan Yao,\n  Anita Mahadevan-Jansen, Matthew J.Tyska, Bryan A. Millis, Yuankai Huo", "title": "VoxelEmbed: 3D Instance Segmentation and Tracking with Voxel Embedding\n  based Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in bioimaging have provided scientists a superior high\nspatial-temporal resolution to observe dynamics of living cells as 3D\nvolumetric videos. Unfortunately, the 3D biomedical video analysis is lagging,\nimpeded by resource insensitive human curation using off-the-shelf 3D analytic\ntools. Herein, biologists often need to discard a considerable amount of rich\n3D spatial information by compromising on 2D analysis via maximum intensity\nprojection. Recently, pixel embedding-based cell instance segmentation and\ntracking provided a neat and generalizable computing paradigm for understanding\ncellular dynamics. In this work, we propose a novel spatial-temporal\nvoxel-embedding (VoxelEmbed) based learning method to perform simultaneous cell\ninstance segmenting and tracking on 3D volumetric video sequences. Our\ncontribution is in four-fold: (1) The proposed voxel embedding generalizes the\npixel embedding with 3D context information; (2) Present a simple multi-stream\nlearning approach that allows effective spatial-temporal embedding; (3)\nAccomplished an end-to-end framework for one-stage 3D cell instance\nsegmentation and tracking without heavy parameter tuning; (4) The proposed 3D\nquantification is memory efficient via a single GPU with 12 GB memory. We\nevaluate our VoxelEmbed method on four 3D datasets (with different cell types)\nfrom the ISBI Cell Tracking Challenge. The proposed VoxelEmbed method achieved\nconsistent superior overall performance (OP) on two densely annotated datasets.\nThe performance is also competitive on two sparsely annotated cohorts with\n20.6% and 2% of data-set having segmentation annotations. The results\ndemonstrate that the VoxelEmbed method is a generalizable and memory-efficient\nsolution.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:03:26 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zhao", "Mengyang", ""], ["Liu", "Quan", ""], ["Jha", "Aadarsh", ""], ["Deng", "Ruining", ""], ["Yao", "Tianyuan", ""], ["Mahadevan-Jansen", "Anita", ""], ["Tyska", "Matthew J.", ""], ["Millis", "Bryan A.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2106.11481", "submitter": "Sourav Garg", "authors": "Sourav Garg and Michael Milford", "title": "SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for\n  Day-Night Place Recognition", "comments": "Accepted to CVPR 2021 Workshop on 3D Vision and Robotics (3DVR).\n  https://sites.google.com/view/cvpr2021-3d-vision-robotics/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place Recognition is a crucial capability for mobile robot localization and\nnavigation. Image-based or Visual Place Recognition (VPR) is a challenging\nproblem as scene appearance and camera viewpoint can change significantly when\nplaces are revisited. Recent VPR methods based on ``sequential\nrepresentations'' have shown promising results as compared to traditional\nsequence score aggregation or single image based techniques. In parallel to\nthese endeavors, 3D point clouds based place recognition is also being explored\nfollowing the advances in deep learning based point cloud processing. However,\na key question remains: is an explicit 3D structure based place representation\nalways superior to an implicit ``spatial'' representation based on sequence of\nRGB images which can inherently learn scene structure. In this extended\nabstract, we attempt to compare these two types of methods by considering a\nsimilar ``metric span'' to represent places. We compare a 3D point cloud based\nmethod (PointNetVLAD) with image sequence based methods (SeqNet and others) and\nshowcase that image sequence based techniques approach, and can even surpass,\nthe performance achieved by point cloud based methods for a given metric span.\nThese performance variations can be attributed to differences in data richness\nof input sensors as well as data accumulation strategies for a mobile robot.\nWhile a perfect apple-to-apple comparison may not be feasible for these two\ndifferent modalities, the presented comparison takes a step in the direction of\nanswering deeper questions regarding spatial representations, relevant to\nseveral applications like Autonomous Driving and Augmented/Virtual Reality.\nSource code available publicly https://github.com/oravus/seqNet.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:05:32 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Garg", "Sourav", ""], ["Milford", "Michael", ""]]}, {"id": "2106.11482", "submitter": "Ying Gao", "authors": "Ying Gao, Xiaohan Feng, Tiange Zhang, Eric Rigall, Huiyu Zhou, Lin Qi,\n  Junyu Dong", "title": "Wallpaper Texture Generation and Style Transfer Based on Multi-label\n  Semantics", "comments": "IEEE Transactions on Circuits and Systems for Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2021.3078560.", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textures contain a wealth of image information and are widely used in various\nfields such as computer graphics and computer vision. With the development of\nmachine learning, the texture synthesis and generation have been greatly\nimproved. As a very common element in everyday life, wallpapers contain a\nwealth of texture information, making it difficult to annotate with a simple\nsingle label. Moreover, wallpaper designers spend significant time to create\ndifferent styles of wallpaper. For this purpose, this paper proposes to\ndescribe wallpaper texture images by using multi-label semantics. Based on\nthese labels and generative adversarial networks, we present a framework for\nperception driven wallpaper texture generation and style transfer. In this\nframework, a perceptual model is trained to recognize whether the wallpapers\nproduced by the generator network are sufficiently realistic and have the\nattribute designated by given perceptual description; these multi-label\nsemantic attributes are treated as condition variables to generate wallpaper\nimages. The generated wallpaper images can be converted to those with\nwell-known artist styles using CycleGAN. Finally, using the aesthetic\nevaluation method, the generated wallpaper images are quantitatively measured.\nThe experimental results demonstrate that the proposed method can generate\nwallpaper textures conforming to human aesthetics and have artistic\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:09:25 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gao", "Ying", ""], ["Feng", "Xiaohan", ""], ["Zhang", "Tiange", ""], ["Rigall", "Eric", ""], ["Zhou", "Huiyu", ""], ["Qi", "Lin", ""], ["Dong", "Junyu", ""]]}, {"id": "2106.11485", "submitter": "Yutong He", "authors": "Yutong He, Dingjie Wang, Nicholas Lai, William Zhang, Chenlin Meng,\n  Marshall Burke, David B. Lobell, Stefano Ermon", "title": "Spatial-Temporal Super-Resolution of Satellite Imagery via Conditional\n  Pixel Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-resolution satellite imagery has proven useful for a broad range of\ntasks, including measurement of global human population, local economic\nlivelihoods, and biodiversity, among many others. Unfortunately,\nhigh-resolution imagery is both infrequently collected and expensive to\npurchase, making it hard to efficiently and effectively scale these downstream\ntasks over both time and space. We propose a new conditional pixel synthesis\nmodel that uses abundant, low-cost, low-resolution imagery to generate accurate\nhigh-resolution imagery at locations and times in which it is unavailable. We\nshow that our model attains photo-realistic sample quality and outperforms\ncompeting baselines on a key downstream task -- object counting -- particularly\nin geographic locations where conditions on the ground are changing rapidly.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:16:24 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["He", "Yutong", ""], ["Wang", "Dingjie", ""], ["Lai", "Nicholas", ""], ["Zhang", "William", ""], ["Meng", "Chenlin", ""], ["Burke", "Marshall", ""], ["Lobell", "David B.", ""], ["Ermon", "Stefano", ""]]}, {"id": "2106.11486", "submitter": "Dong Hoon Lee", "authors": "Dong Hoon Lee, Sae-Young Chung", "title": "Unsupervised Embedding Adaptation via Early-Stage Feature Reconstruction\n  for Few-Shot Classification", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose unsupervised embedding adaptation for the downstream few-shot\nclassification task. Based on findings that deep neural networks learn to\ngeneralize before memorizing, we develop Early-Stage Feature Reconstruction\n(ESFR) -- a novel adaptation scheme with feature reconstruction and\ndimensionality-driven early stopping that finds generalizable features.\nIncorporating ESFR consistently improves the performance of baseline methods on\nall standard settings, including the recently proposed transductive method.\nESFR used in conjunction with the transductive method further achieves\nstate-of-the-art performance on mini-ImageNet, tiered-ImageNet, and CUB;\nespecially with 1.2%~2.0% improvements in accuracy over the previous best\nperforming method on 1-shot setting.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:25:01 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Lee", "Dong Hoon", ""], ["Chung", "Sae-Young", ""]]}, {"id": "2106.11516", "submitter": "Lin Li", "authors": "Lin Li, Xin Kong, Xiangrui Zhao, Wanlong Li, Feng Wen, Hongbo Zhang\n  and Yong Liu", "title": "SA-LOAM: Semantic-aided LiDAR SLAM with Loop Closure", "comments": "8 pages. Accepted by ICRA-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR-based SLAM system is admittedly more accurate and stable than others,\nwhile its loop closure detection is still an open issue. With the development\nof 3D semantic segmentation for point cloud, semantic information can be\nobtained conveniently and steadily, essential for high-level intelligence and\nconductive to SLAM. In this paper, we present a novel semantic-aided LiDAR SLAM\nwith loop closure based on LOAM, named SA-LOAM, which leverages semantics in\nodometry as well as loop closure detection. Specifically, we propose a\nsemantic-assisted ICP, including semantically matching, downsampling and plane\nconstraint, and integrates a semantic graph-based place recognition method in\nour loop closure detection module. Benefitting from semantics, we can improve\nthe localization accuracy, detect loop closures effectively, and construct a\nglobal consistent semantic map even in large-scale scenes. Extensive\nexperiments on KITTI and Ford Campus dataset show that our system significantly\nimproves baseline performance, has generalization ability to unseen data and\nachieves competitive results compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 03:14:20 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 06:56:19 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Li", "Lin", ""], ["Kong", "Xin", ""], ["Zhao", "Xiangrui", ""], ["Li", "Wanlong", ""], ["Wen", "Feng", ""], ["Zhang", "Hongbo", ""], ["Liu", "Yong", ""]]}, {"id": "2106.11528", "submitter": "Gyeongho Kim", "authors": "Gyeongho Kim", "title": "Recent Deep Semi-supervised Learning Approaches and Related Works", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The author of this work proposes an overview of the recent semi-supervised\nlearning approaches and related works. Despite the remarkable success of neural\nnetworks in various applications, there exist few formidable constraints\nincluding the need for a large amount of labeled data. Therefore,\nsemi-supervised learning, which is a learning scheme in which the scarce labels\nand a larger amount of unlabeled data are utilized to train models (e.g., deep\nneural networks) is getting more important. Based on the key assumptions of\nsemi-supervised learning, which are the manifold assumption, cluster\nassumption, and continuity assumption, the work reviews the recent\nsemi-supervised learning approaches. In particular, the methods in regard to\nusing deep neural networks in a semi-supervised learning setting are primarily\ndiscussed. In addition, the existing works are first classified based on the\nunderlying idea and explained, and then the holistic approaches that unify the\naforementioned ideas are detailed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 03:44:03 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Kim", "Gyeongho", ""]]}, {"id": "2106.11536", "submitter": "Liguo Jiang", "authors": "Liguo Jiang, Miaopeng Li, Jianjie Zhang, Congyi Wang, Juntao Ye,\n  Xinguo Liu, Jinxiang Chai", "title": "Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies\n  from Single RGB Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach that accurately reconstructs 3D human poses and\ndetailed 3D full-body geometric models from single images in realtime. The key\nidea of our approach is a novel end-to-end multi-task deep learning framework\nthat uses single images to predict five outputs simultaneously: foreground\nsegmentation mask, 2D joints positions, semantic body partitions, 3D part\norientations and uv coordinates (uv map). The multi-task network architecture\nnot only generates more visual cues for reconstruction, but also makes each\nindividual prediction more accurate. The CNN regressor is further combined with\nan optimization based algorithm for accurate kinematic pose reconstruction and\nfull-body shape modeling. We show that the realtime reconstruction reaches\naccurate fitting that has not been seen before, especially for wild images. We\ndemonstrate the results of our realtime 3D pose and human body reconstruction\nsystem on various challenging in-the-wild videos. We show the system advances\nthe frontier of 3D human body and pose reconstruction from single images by\nquantitative evaluations and comparisons with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 04:26:11 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Jiang", "Liguo", ""], ["Li", "Miaopeng", ""], ["Zhang", "Jianjie", ""], ["Wang", "Congyi", ""], ["Ye", "Juntao", ""], ["Liu", "Xinguo", ""], ["Chai", "Jinxiang", ""]]}, {"id": "2106.11539", "submitter": "Srikar Appalaraju", "authors": "Srikar Appalaraju and Bhavan Jasani and Bhargava Urala Kota and\n  Yusheng Xie and R. Manmatha", "title": "DocFormer: End-to-End Transformer for Document Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present DocFormer -- a multi-modal transformer based architecture for the\ntask of Visual Document Understanding (VDU). VDU is a challenging problem which\naims to understand documents in their varied formats (forms, receipts etc.) and\nlayouts. In addition, DocFormer is pre-trained in an unsupervised fashion using\ncarefully designed tasks which encourage multi-modal interaction. DocFormer\nuses text, vision and spatial features and combines them using a novel\nmulti-modal self-attention layer. DocFormer also shares learned spatial\nembeddings across modalities which makes it easy for the model to correlate\ntext to visual tokens and vice versa. DocFormer is evaluated on 4 different\ndatasets each with strong baselines. DocFormer achieves state-of-the-art\nresults on all of them, sometimes beating models 4x its size (in no. of\nparameters).\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 04:28:07 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Appalaraju", "Srikar", ""], ["Jasani", "Bhavan", ""], ["Kota", "Bhargava Urala", ""], ["Xie", "Yusheng", ""], ["Manmatha", "R.", ""]]}, {"id": "2106.11541", "submitter": "Phong Tung Doan", "authors": "Tung Doan and Atsuhiro Takasu", "title": "Kernel Clustering with Sigmoid-based Regularization for Efficient\n  Segmentation of Sequential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernel segmentation aims at partitioning a data sequence into several\nnon-overlapping segments that may have nonlinear and complex structures. In\ngeneral, it is formulated as a discrete optimization problem with combinatorial\nconstraints. A popular algorithm for optimally solving this problem is dynamic\nprogramming (DP), which has quadratic computation and memory requirements.\nGiven that sequences in practice are too long, this algorithm is not a\npractical approach. Although many heuristic algorithms have been proposed to\napproximate the optimal segmentation, they have no guarantee on the quality of\ntheir solutions. In this paper, we take a differentiable approach to alleviate\nthe aforementioned issues. First, we introduce a novel sigmoid-based\nregularization to smoothly approximate the combinatorial constraints. Combining\nit with objective of the balanced kernel clustering, we formulate a\ndifferentiable model termed Kernel clustering with sigmoid-based regularization\n(KCSR), where the gradient-based algorithm can be exploited to obtain the\noptimal segmentation. Second, we develop a stochastic variant of the proposed\nmodel. By using the stochastic gradient descent algorithm, which has much lower\ntime and space complexities, for optimization, the second model can perform\nsegmentation on overlong data sequences. Finally, for simultaneously segmenting\nmultiple data sequences, we slightly modify the sigmoid-based regularization to\nfurther introduce an extended variant of the proposed model. Through extensive\nexperiments on various types of data sequences performances of our models are\nevaluated and compared with those of the existing methods. The experimental\nresults validate advantages of the proposed models. Our Matlab source code is\navailable on github.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 04:32:21 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Doan", "Tung", ""], ["Takasu", "Atsuhiro", ""]]}, {"id": "2106.11542", "submitter": "Miao Zhang", "authors": "Miao Zhang, Steven Su, Shirui Pan, Xiaojun Chang, Wei Huang,\n  Gholamreza Haffari", "title": "Differentiable Architecture Search Without Training Nor Labels: A\n  Pruning Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With leveraging the weight-sharing and continuous relaxation to enable\ngradient-descent to alternately optimize the supernet weights and the\narchitecture parameters through a bi-level optimization paradigm,\n\\textit{Differentiable ARchiTecture Search} (DARTS) has become the mainstream\nmethod in Neural Architecture Search (NAS) due to its simplicity and\nefficiency. However, more recent works found that the performance of the\nsearched architecture barely increases with the optimization proceeding in\nDARTS. In addition, several concurrent works show that the NAS could find more\ncompetitive architectures without labels. The above observations reveal that\nthe supervision signal in DARTS may be a poor indicator for architecture\noptimization, inspiring a foundational question: instead of using the\nsupervision signal to perform bi-level optimization, \\textit{can we find\nhigh-quality architectures \\textbf{without any training nor labels}}? We\nprovide an affirmative answer by customizing the NAS as a network pruning at\ninitialization problem. By leveraging recent techniques on the network pruning\nat initialization, we designed a FreeFlow proxy to score the importance of\ncandidate operations in NAS without any training nor labels, and proposed a\nnovel framework called \\textit{training and label free neural architecture\nsearch} (\\textbf{FreeNAS}) accordingly. We show that, without any training nor\nlabels, FreeNAS with the proposed FreeFlow proxy can outperform most NAS\nbaselines. More importantly, our framework is extremely efficient, which\ncompletes the architecture search within only \\textbf{3.6s} and \\textbf{79s} on\na single GPU for the NAS-Bench-201 and DARTS search space, respectively. We\nhope our work inspires more attempts in solving NAS from the perspective of\npruning at initialization.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 04:40:34 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zhang", "Miao", ""], ["Su", "Steven", ""], ["Pan", "Shirui", ""], ["Chang", "Xiaojun", ""], ["Huang", "Wei", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "2106.11549", "submitter": "Hyolim Kang", "authors": "Hyolim Kang, Jinwoo Kim, Kyungmin Kim, Taehyun Kim, Seon Joo Kim", "title": "Winning the CVPR'2021 Kinetics-GEBD Challenge: Contrastive Learning\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic Event Boundary Detection (GEBD) is a newly introduced task that aims\nto detect \"general\" event boundaries that correspond to natural human\nperception. In this paper, we introduce a novel contrastive learning based\napproach to deal with the GEBD. Our intuition is that the feature similarity of\nthe video snippet would significantly vary near the event boundaries, while\nremaining relatively the same in the remaining part of the video. In our model,\nTemporal Self-similarity Matrix (TSM) is utilized as an intermediate\nrepresentation which takes on a role as an information bottleneck. With our\nmodel, we achieved significant performance boost compared to the given\nbaselines. Our code is available at\nhttps://github.com/hello-jinwoo/LOVEU-CVPR2021.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 05:21:59 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Kang", "Hyolim", ""], ["Kim", "Jinwoo", ""], ["Kim", "Kyungmin", ""], ["Kim", "Taehyun", ""], ["Kim", "Seon Joo", ""]]}, {"id": "2106.11558", "submitter": "Mohana Singh", "authors": "Mohana Singh and Renu M. Rameshan", "title": "Learning-Based Practical Light Field Image Compression Using A\n  Disparity-Aware Model", "comments": "accepted to Picture Coding Symposium 2021, corrected typo in link to\n  source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Light field technology has increasingly attracted the attention of the\nresearch community with its many possible applications. The lenslet array in\ncommercial plenoptic cameras helps capture both the spatial and angular\ninformation of light rays in a single exposure. While the resulting high\ndimensionality of light field data enables its superior capabilities, it also\nimpedes its extensive adoption. Hence, there is a compelling need for efficient\ncompression of light field images. Existing solutions are commonly composed of\nseveral separate modules, some of which may not have been designed for the\nspecific structure and quality of light field data. This increases the\ncomplexity of the codec and results in impractical decoding runtimes. We\npropose a new learning-based, disparity-aided model for compression of 4D light\nfield images capable of parallel decoding. The model is end-to-end trainable,\neliminating the need for hand-tuning separate modules and allowing joint\nlearning of rate and distortion. The disparity-aided approach ensures the\nstructural integrity of the reconstructed light fields. Comparisons with the\nstate of the art show encouraging performance in terms of PSNR and MS-SSIM\nmetrics. Also, there is a notable gain in the encoding and decoding runtimes.\nSource code is available at https://moha23.github.io/LF-DAAE.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:30:25 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 04:45:09 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Singh", "Mohana", ""], ["Rameshan", "Renu M.", ""]]}, {"id": "2106.11559", "submitter": "Rohith Reddy Rachala", "authors": "Rachala Rohith Reddy and Mahesh Raveendranatha Panicker", "title": "Hand-Drawn Electrical Circuit Recognition using Object Detection and\n  Node Recognition", "comments": "11 pages, 15 figures, under review in springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the recent developments in neural networks, there has been a resurgence\nin algorithms for the automatic generation of simulation ready electronic\ncircuits from hand-drawn circuits. However, most of the approaches in\nliterature were confined to classify different types of electrical components\nand only a few of those methods have shown a way to rebuild the circuit\nschematic from the scanned image, which is extremely important for further\nautomation of netlist generation. This paper proposes a real-time algorithm for\nthe automatic recognition of hand-drawn electrical circuits based on object\ndetection and circuit node recognition. The proposed approach employs You Only\nLook Once version 5 (YOLOv5) for detection of circuit components and a novel\nHough transform based approach for node recognition. Using YOLOv5 object\ndetection algorithm, a mean average precision (mAP0.5) of 98.2% is achieved in\ndetecting the components. The proposed method is also able to rebuild the\ncircuit schematic with 80% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:30:50 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Reddy", "Rachala Rohith", ""], ["Panicker", "Mahesh Raveendranatha", ""]]}, {"id": "2106.11562", "submitter": "Sungmin Cha", "authors": "Sungmin Cha, Beomyoung Kim, Youngjoon Yoo and Taesup Moon", "title": "SSUL: Semantic Segmentation with Unknown Label for Exemplar-based\n  Class-Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class-incremental semantic segmentation (CISS) problem. While\nsome recently proposed algorithms utilized variants of knowledge distillation\n(KD) technique to tackle the problem, they only partially addressed the key\nadditional challenges in CISS that causes the catastrophic forgetting; i.e.,\nthe semantic drift of the background class and multi-label prediction issue. To\nbetter address these challenges, we propose a new method, dubbed as SSUL-M\n(Semantic Segmentation with Unknown Label with Memory), by carefully combining\nseveral techniques tailored for semantic segmentation. More specifically, we\nmake three main contributions; (1) modeling unknown class within the background\nclass to help learning future classes (help plasticity), (2) freezing backbone\nnetwork and past classifiers with binary cross-entropy loss and pseudo-labeling\nto overcome catastrophic forgetting (help stability), and (3) utilizing tiny\nexemplar memory for the first time in CISS to improve both plasticity and\nstability. As a result, we show our method achieves significantly better\nperformance than the recent state-of-the-art baselines on the standard\nbenchmark datasets. Furthermore, we justify our contributions with thorough and\nextensive ablation analyses and discuss different natures of the CISS problem\ncompared to the standard class-incremental learning for classification.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:40:26 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 01:39:56 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Cha", "Sungmin", ""], ["Kim", "Beomyoung", ""], ["Yoo", "Youngjoon", ""], ["Moon", "Taesup", ""]]}, {"id": "2106.11563", "submitter": "Bahram Sadeghi Bigham", "authors": "Kobra Nazari, Samaneh Mazaheri and Bahram Sadeghi Bigham", "title": "Creating A New Color Space utilizing PSO and FCM to Perform Skin\n  Detection by using Neural Network and ANFIS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Skin color detection is an essential required step in various applications\nrelated to computer vision. These applications will include face detection,\nfinding pornographic images in movies and photos, finding ethnicity, age,\ndiagnosis, and so on. Therefore, proposing a proper skin detection method can\nprovide solution to several problems. In this study, first a new color space is\ncreated using FCM and PSO algorithms. Then, skin classification has been\nperformed in the new color space utilizing linear and nonlinear modes.\nAdditionally, it has been done in RGB and LAB color spaces by using ANFIS and\nneural network. Skin detection in RBG color space has been performed using\nMahalanobis distance and Euclidean distance algorithms. In comparison, this\nmethod has 18.38% higher accuracy than the most accurate method on the same\ndatabase. Additionally, this method has achieved 90.05% in equal error rate\n(1-EER) in testing COMPAQ dataset and 92.93% accuracy in testing Pratheepan\ndataset, which compared to the previous method on COMPAQ database, 1-EER has\nincreased by %0.87.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:41:33 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Nazari", "Kobra", ""], ["Mazaheri", "Samaneh", ""], ["Bigham", "Bahram Sadeghi", ""]]}, {"id": "2106.11576", "submitter": "Boris Chidlovskii", "authors": "Chidlovskii Boris, Assem Sadek, Christian Wolf", "title": "Universal Domain Adaptation in Ordinal Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of universal domain adaptation (UDA) in ordinal\nregression (OR), which attempts to solve classification problems in which\nlabels are not independent, but follow a natural order. We show that the UDA\ntechniques developed for classification and based on the clustering assumption,\nunder-perform in OR settings. We propose a method that complements the OR\nclassifier with an auxiliary task of order learning, which plays the double\nrole of discriminating between common and private instances, and expanding\nclass labels to the private target images via ranking. Combined with\nadversarial domain discrimination, our model is able to address the closed set,\npartial and open set configurations. We evaluate our method on three face age\nestimation datasets, and show that it outperforms the baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 07:23:39 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Boris", "Chidlovskii", ""], ["Sadek", "Assem", ""], ["Wolf", "Christian", ""]]}, {"id": "2106.11582", "submitter": "Hechen Yang", "authors": "Hechen Yang, Chen Li, Jinghua Zhang, Peng Zhao, Ao Chen, Xin Zhao, Tao\n  Jiang, Marcin Grzegorzek", "title": "A Comparison for Patch-level Classification of Deep Learning Methods on\n  Transparent Environmental Microorganism Images: from Convolutional Neural\n  Networks to Visual Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, analysis of Transparent Environmental Microorganism Images (T-EM\nimages) in the field of computer vision has gradually become a new and\ninteresting spot. This paper compares different deep learning classification\nperformance for the problem that T-EM images are challenging to analyze. We\ncrop the T-EM images into 8 * 8 and 224 * 224 pixel patches in the same\nproportion and then divide the two different pixel patches into foreground and\nbackground according to ground truth. We also use four convolutional neural\nnetworks and a novel ViT network model to compare the foreground and background\nclassification experiments. We conclude that ViT performs the worst in\nclassifying 8 * 8 pixel patches, but it outperforms most convolutional neural\nnetworks in classifying 224 * 224 pixel patches.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 07:30:45 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 01:37:40 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Yang", "Hechen", ""], ["Li", "Chen", ""], ["Zhang", "Jinghua", ""], ["Zhao", "Peng", ""], ["Chen", "Ao", ""], ["Zhao", "Xin", ""], ["Jiang", "Tao", ""], ["Grzegorzek", "Marcin", ""]]}, {"id": "2106.11589", "submitter": "Hau Chu", "authors": "Hau Chu, Jia-Hong Lee, Yao-Chih Lee, Ching-Hsien Hsu, Jia-Da Li,\n  Chu-Song Chen", "title": "Part-Aware Measurement for Robust Multi-View Multi-Human 3D Pose\n  Estimation and Tracking", "comments": "12 pages with supplementary material; accepted to CVPR 2021 B-AMFG\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an approach for multi-human 3D pose estimation and\ntracking based on calibrated multi-view. The main challenge lies in finding the\ncross-view and temporal correspondences correctly even when several human pose\nestimations are noisy. Compare to previous solutions that construct 3D poses\nfrom multiple views, our approach takes advantage of temporal consistency to\nmatch the 2D poses estimated with previously constructed 3D skeletons in every\nview. Therefore cross-view and temporal associations are accomplished\nsimultaneously. Since the performance suffers from mistaken association and\nnoisy predictions, we design two strategies for aiming better correspondences\nand 3D reconstruction. Specifically, we propose a part-aware measurement for\n2D-3D association and a filter that can cope with 2D outliers during\nreconstruction. Our approach is efficient and effective comparing to\nstate-of-the-art methods; it achieves competitive results on two benchmarks:\n96.8% on Campus and 97.4% on Shelf. Moreover, we extends the length of Campus\nevaluation frames to be more challenging and our proposal also reach\nwell-performed result.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 07:50:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chu", "Hau", ""], ["Lee", "Jia-Hong", ""], ["Lee", "Yao-Chih", ""], ["Hsu", "Ching-Hsien", ""], ["Li", "Jia-Da", ""], ["Chen", "Chu-Song", ""]]}, {"id": "2106.11596", "submitter": "Jun Huang", "authors": "Xiwen Qu, Hao Che, Jun Huang, Linchuan Xu, Xiao Zheng", "title": "Multi-layered Semantic Representation Network for Multi-label Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label image classification (MLIC) is a fundamental and practical task,\nwhich aims to assign multiple possible labels to an image. In recent years,\nmany deep convolutional neural network (CNN) based approaches have been\nproposed which model label correlations to discover semantics of labels and\nlearn semantic representations of images. This paper advances this research\ndirection by improving both the modeling of label correlations and the learning\nof semantic representations. On the one hand, besides the local semantics of\neach label, we propose to further explore global semantics shared by multiple\nlabels. On the other hand, existing approaches mainly learn the semantic\nrepresentations at the last convolutional layer of a CNN. But it has been noted\nthat the image representations of different layers of CNN capture different\nlevels or scales of features and have different discriminative abilities. We\nthus propose to learn semantic representations at multiple convolutional\nlayers. To this end, this paper designs a Multi-layered Semantic Representation\nNetwork (MSRN) which discovers both local and global semantics of labels\nthrough modeling label correlations and utilizes the label semantics to guide\nthe semantic representations learning at multiple layers through an attention\nmechanism. Extensive experiments on four benchmark datasets including VOC 2007,\nCOCO, NUS-WIDE, and Apparel show a competitive performance of the proposed MSRN\nagainst state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 08:04:22 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Qu", "Xiwen", ""], ["Che", "Hao", ""], ["Huang", "Jun", ""], ["Xu", "Linchuan", ""], ["Zheng", "Xiao", ""]]}, {"id": "2106.11613", "submitter": "Chen Jingye", "authors": "Jingye Chen, Bin Li, Xiangyang Xue", "title": "Zero-Shot Chinese Character Recognition with Stroke-Level Decomposition", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chinese character recognition has attracted much research interest due to its\nwide applications. Although it has been studied for many years, some issues in\nthis field have not been completely resolved yet, e.g. the zero-shot problem.\nPrevious character-based and radical-based methods have not fundamentally\naddressed the zero-shot problem since some characters or radicals in test sets\nmay not appear in training sets under a data-hungry condition. Inspired by the\nfact that humans can generalize to know how to write characters unseen before\nif they have learned stroke orders of some characters, we propose a\nstroke-based method by decomposing each character into a sequence of strokes,\nwhich are the most basic units of Chinese characters. However, we observe that\nthere is a one-to-many relationship between stroke sequences and Chinese\ncharacters. To tackle this challenge, we employ a matching-based strategy to\ntransform the predicted stroke sequence to a specific character. We evaluate\nthe proposed method on handwritten characters, printed artistic characters, and\nscene characters. The experimental results validate that the proposed method\noutperforms existing methods on both character zero-shot and radical zero-shot\ntasks. Moreover, the proposed method can be easily generalized to other\nlanguages whose characters can be decomposed into strokes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 08:49:03 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chen", "Jingye", ""], ["Li", "Bin", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2106.11641", "submitter": "Jing Zhang", "authors": "Jiawei Liu and Jing Zhang and Nick Barnes", "title": "Confidence-Aware Learning for Camouflaged Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence-aware learning is proven as an effective solution to prevent\nnetworks becoming overconfident. We present a confidence-aware camouflaged\nobject detection framework using dynamic supervision to produce both accurate\ncamouflage map and meaningful \"confidence\" representing model awareness about\nthe current prediction. A camouflaged object detection network is designed to\nproduce our camouflage prediction. Then, we concatenate it with the input image\nand feed it to the confidence estimation network to produce an one channel\nconfidence map.We generate dynamic supervision for the confidence estimation\nnetwork, representing the agreement of camouflage prediction with the ground\ntruth camouflage map. With the produced confidence map, we introduce\nconfidence-aware learning with the confidence map as guidance to pay more\nattention to the hard/low-confidence pixels in the loss function. We claim\nthat, once trained, our confidence estimation network can evaluate pixel-wise\naccuracy of the prediction without relying on the ground truth camouflage map.\nExtensive results on four camouflaged object detection testing datasets\nillustrate the superior performance of the proposed model in explaining the\ncamouflage prediction.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 09:49:23 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Liu", "Jiawei", ""], ["Zhang", "Jing", ""], ["Barnes", "Nick", ""]]}, {"id": "2106.11644", "submitter": "Sungmin Cha", "authors": "Sungmin Cha, Naeun Ko, Youngjoon Yoo and Taesup Moon", "title": "Self-Supervised Iterative Contextual Smoothing for Efficient Adversarial\n  Defense against Gray- and Black-Box Attack", "comments": "Preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and effective input transformation based adversarial\ndefense method against gray- and black-box attack, which is computationally\nefficient and does not require any adversarial training or retraining of a\nclassification model. We first show that a very simple iterative Gaussian\nsmoothing can effectively wash out adversarial noise and achieve substantially\nhigh robust accuracy. Based on the observation, we propose Self-Supervised\nIterative Contextual Smoothing (SSICS), which aims to reconstruct the original\ndiscriminative features from the Gaussian-smoothed image in context-adaptive\nmanner, while still smoothing out the adversarial noise. From the experiments\non ImageNet, we show that our SSICS achieves both high standard accuracy and\nvery competitive robust accuracy for the gray- and black-box attacks; e.g.,\ntransfer-based PGD-attack and score-based attack. A note-worthy point to stress\nis that our defense is free of computationally expensive adversarial training,\nyet, can approach its robust accuracy via input transformation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 09:51:51 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Cha", "Sungmin", ""], ["Ko", "Naeun", ""], ["Yoo", "Youngjoon", ""], ["Moon", "Taesup", ""]]}, {"id": "2106.11650", "submitter": "Antonino Furnari", "authors": "Ronja M\\\"oller, Antonino Furnari, Sebastiano Battiato, Aki H\\\"arm\\\"a,\n  Giovanni Maria Farinella", "title": "A Survey on Human-aware Robot Navigation", "comments": "Robotics and Autonomous Systems, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent systems are increasingly part of our everyday lives and have been\nintegrated seamlessly to the point where it is difficult to imagine a world\nwithout them. Physical manifestations of those systems on the other hand, in\nthe form of embodied agents or robots, have so far been used only for specific\napplications and are often limited to functional roles (e.g. in the industry,\nentertainment and military fields). Given the current growth and innovation in\nthe research communities concerned with the topics of robot navigation,\nhuman-robot-interaction and human activity recognition, it seems like this\nmight soon change. Robots are increasingly easy to obtain and use and the\nacceptance of them in general is growing. However, the design of a socially\ncompliant robot that can function as a companion needs to take various areas of\nresearch into account. This paper is concerned with the navigation aspect of a\nsocially-compliant robot and provides a survey of existing solutions for the\nrelevant areas of research as well as an outlook on possible future directions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 10:09:25 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["M\u00f6ller", "Ronja", ""], ["Furnari", "Antonino", ""], ["Battiato", "Sebastiano", ""], ["H\u00e4rm\u00e4", "Aki", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "2106.11653", "submitter": "YuXi Wang", "authors": "Yuxi Wang, Jian Liang, Zhaoxiang Zhang", "title": "Give Me Your Trained Model: Domain Adaptive Semantic Segmentation\n  without Source Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Benefited from considerable pixel-level annotations collected from a specific\nsituation (source), the trained semantic segmentation model performs quite\nwell, but fails in a new situation (target) due to the large domain shift. To\nmitigate the domain gap, previous cross-domain semantic segmentation methods\nalways assume the co-existence of source data and target data during\ndistribution alignment. However, the access to source data in the real scenario\nmay raise privacy concerns and violate intellectual property. To tackle this\nproblem, we focus on an interesting and challenging cross-domain semantic\nsegmentation task where only the trained source model is provided to the target\ndomain, and further propose a unified framework called Domain Adaptive Semantic\nSegmentation without Source data (DAS$^3$ for short). Specifically, DAS$^3$\nconsists of three schemes, i.e., feature alignment, self-training, and\ninformation propagation. First, we mainly develop a focal entropic loss on the\nnetwork outputs to implicitly align the target features with unseen source\nfeatures via the provided source model. Second, besides positive pseudo labels\nin vanilla self-training, we first introduce negative pseudo labels to the\nfield and develop a bi-directional self-training strategy to enhance the\nrepresentation learning in the target domain. Finally, the information\npropagation scheme further reduces the intra-domain discrepancy within the\ntarget domain via pseudo semi-supervised learning. Extensive results on\nsynthesis-to-real and cross-city driving datasets validate DAS$^3$ yields\nstate-of-the-art performance, even on par with methods that need access to\nsource data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 10:21:39 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Yuxi", ""], ["Liang", "Jian", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "2106.11695", "submitter": "Tom\\'a\\v{s} \\v{S}ipka", "authors": "Tomas Sipka, Milan Sulc, Jiri Matas", "title": "The Hitchhiker's Guide to Prior-Shift Adaptation", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many computer vision classification tasks, class priors at test time often\ndiffer from priors on the training set. In the case of such prior shift,\nclassifiers must be adapted correspondingly to maintain close to optimal\nperformance. This paper analyzes methods for adaptation of probabilistic\nclassifiers to new priors and for estimating new priors on an unlabeled test\nset. We propose a novel method to address a known issue of prior estimation\nmethods based on confusion matrices, where inconsistent estimates of decision\nprobabilities and confusion matrices lead to negative values in the estimated\npriors. Experiments on fine-grained image classification datasets provide\ninsight into the best practice of prior shift estimation and classifier\nadaptation and show that the proposed method achieves state-of-the-art results\nin prior adaptation. Applying the best practice to two tasks with naturally\nimbalanced priors, learning from web-crawled images and plant species\nclassification, increased the recognition accuracy by 1.1% and 3.4%\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 11:55:51 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Sipka", "Tomas", ""], ["Sulc", "Milan", ""], ["Matas", "Jiri", ""]]}, {"id": "2106.11725", "submitter": "Jiayi Wang", "authors": "Jiayi Wang, Franziska Mueller, Florian Bernard, Suzanne Sorli,\n  Oleksandr Sotnychenko, Neng Qian, Miguel A. Otaduy, Dan Casas and Christian\n  Theobalt", "title": "RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB\n  Video", "comments": "SIGGRAPH Asia 2020", "journal-ref": "ACM Transactions on Graphics (TOG) 39 (6), 1-16, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tracking and reconstructing the 3D pose and geometry of two hands in\ninteraction is a challenging problem that has a high relevance for several\nhuman-computer interaction applications, including AR/VR, robotics, or sign\nlanguage recognition. Existing works are either limited to simpler tracking\nsettings (e.g., considering only a single hand or two spatially separated\nhands), or rely on less ubiquitous sensors, such as depth cameras. In contrast,\nin this work we present the first real-time method for motion capture of\nskeletal pose and 3D surface geometry of hands from a single RGB camera that\nexplicitly considers close interactions. In order to address the inherent depth\nambiguities in RGB data, we propose a novel multi-task CNN that regresses\nmultiple complementary pieces of information, including segmentation, dense\nmatchings to a 3D hand model, and 2D keypoint positions, together with newly\nproposed intra-hand relative depth and inter-hand distance maps. These\npredictions are subsequently used in a generative model fitting framework in\norder to estimate pose and shape parameters of a 3D hand model for both hands.\nWe experimentally verify the individual components of our RGB two-hand tracking\nand 3D reconstruction pipeline through an extensive ablation study. Moreover,\nwe demonstrate that our approach offers previously unseen two-hand tracking\nperformance from RGB, and quantitatively and qualitatively outperforms existing\nRGB-based methods that were not explicitly designed for two-hand interactions.\nMoreover, our method even performs on-par with depth-based real-time methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 12:53:56 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Jiayi", ""], ["Mueller", "Franziska", ""], ["Bernard", "Florian", ""], ["Sorli", "Suzanne", ""], ["Sotnychenko", "Oleksandr", ""], ["Qian", "Neng", ""], ["Otaduy", "Miguel A.", ""], ["Casas", "Dan", ""], ["Theobalt", "Christian", ""]]}, {"id": "2106.11731", "submitter": "Taro Langner", "authors": "Taro Langner, Andr\\'es Mart\\'inez Mora, Robin Strand, H{\\aa}kan\n  Ahlstr\\\"om, and Joel Kullberg", "title": "MIMIR: Deep Regression for Automated Analysis of UK Biobank Body MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  UK Biobank (UKB) is conducting a large-scale study of more than half a\nmillion volunteers, collecting health-related information on genetics,\nlifestyle, blood biochemistry, and more. Medical imaging furthermore targets\n100,000 subjects, with 70,000 follow-up sessions, enabling measurements of\norgans, muscle, and body composition. With up to 170,000 mounting MR images,\nvarious methodologies are accordingly engaged in large-scale image analysis.\nThis work presents an experimental inference engine that can automatically\npredict a comprehensive profile of subject metadata from UKB neck-to-knee body\nMRI. In cross-validation, it accurately inferred baseline characteristics such\nas age, height, weight, and sex, but also emulated measurements of body\ncomposition by DXA, organ volumes, and abstract properties like grip strength,\npulse rate, and type 2 diabetic status (AUC: 0.866). The proposed system can\nautomatically analyze thousands of subjects within hours and provide individual\nconfidence intervals. The underlying methodology is based on convolutional\nneural networks for image-based mean-variance regression on two-dimensional\nrepresentations of the MRI data. This work aims to make the proposed system\navailable for free to researchers, who can use it to obtain fast and\nfully-automated estimates of 72 different measurements immediately upon release\nof new UK Biobank image data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 13:09:40 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Langner", "Taro", ""], ["Mora", "Andr\u00e9s Mart\u00ednez", ""], ["Strand", "Robin", ""], ["Ahlstr\u00f6m", "H\u00e5kan", ""], ["Kullberg", "Joel", ""]]}, {"id": "2106.11756", "submitter": "C. V. Krishnakumar Iyer", "authors": "C.V.Krishnakumar Iyer, Feili Hou, Henry Wang, Yonghong Wang, Kay Oh,\n  Swetava Ganguli, Vipul Pandey", "title": "Trinity: A No-Code AI platform for complex spatial datasets", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a no-code Artificial Intelligence (AI) platform called Trinity\nwith the main design goal of enabling both machine learning researchers and\nnon-technical geospatial domain experts to experiment with domain-specific\nsignals and datasets for solving a variety of complex problems on their own.\nThis versatility to solve diverse problems is achieved by transforming complex\nSpatio-temporal datasets to make them consumable by standard deep learning\nmodels, in this case, Convolutional Neural Networks (CNNs), and giving the\nability to formulate disparate problems in a standard way, eg. semantic\nsegmentation. With an intuitive user interface, a feature store that hosts\nderivatives of complex feature engineering, a deep learning kernel, and a\nscalable data processing mechanism, Trinity provides a powerful platform for\ndomain experts to share the stage with scientists and engineers in solving\nbusiness-critical problems. It enables quick prototyping, rapid experimentation\nand reduces the time to production by standardizing model building and\ndeployment. In this paper, we present our motivation behind Trinity and its\ndesign along with showcasing sample applications to motivate the idea of\nlowering the bar to using AI.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 08:28:34 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 03:39:10 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 04:49:58 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2021 18:43:48 GMT"}, {"version": "v5", "created": "Thu, 1 Jul 2021 06:22:23 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Iyer", "C. V. Krishnakumar", ""], ["Hou", "Feili", ""], ["Wang", "Henry", ""], ["Wang", "Yonghong", ""], ["Oh", "Kay", ""], ["Ganguli", "Swetava", ""], ["Pandey", "Vipul", ""]]}, {"id": "2106.11759", "submitter": "Vikramjit Mitra", "authors": "Vikramjit Mitra, Zifang Huang, Colin Lea, Lauren Tooley, Sarah Wu,\n  Darren Botten, Ashwini Palekar, Shrinath Thelapurath, Panayiotis Georgiou,\n  Sachin Kajarekar, Jefferey Bigham", "title": "Analysis and Tuning of a Voice Assistant System for Dysfluent Speech", "comments": "5 pages, 1 page reference, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.CV cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dysfluencies and variations in speech pronunciation can severely degrade\nspeech recognition performance, and for many individuals with\nmoderate-to-severe speech disorders, voice operated systems do not work.\nCurrent speech recognition systems are trained primarily with data from fluent\nspeakers and as a consequence do not generalize well to speech with\ndysfluencies such as sound or word repetitions, sound prolongations, or audible\nblocks. The focus of this work is on quantitative analysis of a consumer speech\nrecognition system on individuals who stutter and production-oriented\napproaches for improving performance for common voice assistant tasks (i.e.,\n\"what is the weather?\"). At baseline, this system introduces a significant\nnumber of insertion and substitution errors resulting in intended speech Word\nError Rates (isWER) that are 13.64\\% worse (absolute) for individuals with\nfluency disorders. We show that by simply tuning the decoding parameters in an\nexisting hybrid speech recognition system one can improve isWER by 24\\%\n(relative) for individuals with fluency disorders. Tuning these parameters\ntranslates to 3.6\\% better domain recognition and 1.7\\% better intent\nrecognition relative to the default setup for the 18 study participants across\nall stuttering severities.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 20:58:34 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mitra", "Vikramjit", ""], ["Huang", "Zifang", ""], ["Lea", "Colin", ""], ["Tooley", "Lauren", ""], ["Wu", "Sarah", ""], ["Botten", "Darren", ""], ["Palekar", "Ashwini", ""], ["Thelapurath", "Shrinath", ""], ["Georgiou", "Panayiotis", ""], ["Kajarekar", "Sachin", ""], ["Bigham", "Jefferey", ""]]}, {"id": "2106.11760", "submitter": "GuanLin Li", "authors": "Li Guanlin, Guo Shangwei, Wang Run, Xu Guowen, Zhang Tianwei", "title": "A Stealthy and Robust Fingerprinting Scheme for Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel fingerprinting methodology for the Intellectual\nProperty protection of generative models. Prior solutions for discriminative\nmodels usually adopt adversarial examples as the fingerprints, which give\nanomalous inference behaviors and prediction results. Hence, these methods are\nnot stealthy and can be easily recognized by the adversary. Our approach\nleverages the invisible backdoor technique to overcome the above limitation.\nSpecifically, we design verification samples, whose model outputs look normal\nbut can trigger a backdoor classifier to make abnormal predictions. We propose\na new backdoor embedding approach with Unique-Triplet Loss and fine-grained\ncategorization to enhance the effectiveness of our fingerprints. Extensive\nevaluations show that this solution can outperform other strategies with higher\nrobustness, uniqueness and stealthiness for various GAN models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 06:25:10 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Guanlin", "Li", ""], ["Shangwei", "Guo", ""], ["Run", "Wang", ""], ["Guowen", "Xu", ""], ["Tianwei", "Zhang", ""]]}, {"id": "2106.11769", "submitter": "Haiyang Liu", "authors": "Haiyang Liu, Jihan Zhang", "title": "Improving Ultrasound Tongue Image Reconstruction from Lip Images Using\n  Self-supervised Learning and Attention Mechanism", "comments": "Accepted in KDD Workshop (BIOKDD 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech production is a dynamic procedure, which involved multi human organs\nincluding the tongue, jaw and lips. Modeling the dynamics of the vocal tract\ndeformation is a fundamental problem to understand the speech, which is the\nmost common way for human daily communication. Researchers employ several\nsensory streams to describe the process simultaneously, which are\nincontrovertibly statistically related to other streams. In this paper, we\naddress the following question: given an observable image sequences of lips,\ncan we picture the corresponding tongue motion. We formulated this problem as\nthe self-supervised learning problem, and employ the two-stream convolutional\nnetwork and long-short memory network for the learning task, with the attention\nmechanism. We evaluate the performance of the proposed method by leveraging the\nunlabeled lip videos to predict an upcoming ultrasound tongue image sequence.\nThe results show that our model is able to generate images that close to the\nreal ultrasound tongue images, and results in the matching between two imaging\nmodalities.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 10:51:23 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Liu", "Haiyang", ""], ["Zhang", "Jihan", ""]]}, {"id": "2106.11776", "submitter": "Ghalib Tahir", "authors": "Ghalib Tahir and Chu Kiong Loo", "title": "A Review of the Vision-based Approaches for Dietary Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Last ten years have witnessed the growth of many computer vision applications\nfor food recognition. Dietary studies showed that dietary-related problem such\nas obesity is associated with other chronic diseases like hypertension,\nirregular blood sugar levels, and increased risk of heart attacks. The primary\ncause of these problems is poor lifestyle choices and unhealthy dietary habits,\nwhich are manageable by using interactive mHealth apps that use automatic\nvisual-based methods to assess dietary intake. This review discusses the most\nperforming methodologies that have been developed so far for automatic food\nrecognition. First, we will present the rationale of visual-based methods for\nfood recognition. The core of the paper is the presentation, discussion and\nevaluation of these methods on popular food image databases. We also discussed\nthe mobile applications that are implementing these methods. The review ends\nwith a discussion of research gaps and future challenges in this area.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:30:06 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 18:12:06 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Tahir", "Ghalib", ""], ["Loo", "Chu Kiong", ""]]}, {"id": "2106.11795", "submitter": "Benoit Guillard", "authors": "Benoit Guillard, Edoardo Remelli, Artem Lukoianov, Stephan Richter,\n  Timur Bagautdinov, Pierre Baque and Pascal Fua", "title": "DeepMesh: Differentiable Iso-Surface Extraction", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.03997", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric Deep Learning has recently made striking progress with the advent\nof continuous Deep Implicit Fields. They allow for detailed modeling of\nwatertight surfaces of arbitrary topology while not relying on a 3D Euclidean\ngrid, resulting in a learnable parameterization that is unlimited in\nresolution. Unfortunately, these methods are often unsuitable for applications\nthat require an explicit mesh-based surface representation because converting\nan implicit field to such a representation relies on the Marching Cubes\nalgorithm, which cannot be differentiated with respect to the underlying\nimplicit field. In this work, we remove this limitation and introduce a\ndifferentiable way to produce explicit surface mesh representations from Deep\nImplicit Fields. Our key insight is that by reasoning on how implicit field\nperturbations impact local surface geometry, one can ultimately differentiate\nthe 3D location of surface samples with respect to the underlying deep implicit\nfield. We exploit this to define DeepMesh -- end-to-end differentiable mesh\nrepresentation that can vary its topology. We use two different applications to\nvalidate our theoretical insight: Single view 3D Reconstruction via\nDifferentiable Rendering and Physically-Driven Shape Optimization. In both\ncases our end-to-end differentiable parameterization gives us an edge over\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 20:12:41 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Guillard", "Benoit", ""], ["Remelli", "Edoardo", ""], ["Lukoianov", "Artem", ""], ["Richter", "Stephan", ""], ["Bagautdinov", "Timur", ""], ["Baque", "Pierre", ""], ["Fua", "Pascal", ""]]}, {"id": "2106.11797", "submitter": "Lorenzo Quir\\'os", "authors": "Lorenzo Quir\\'os and Enrique Vidal", "title": "Evaluation of a Region Proposal Architecture for Multi-task Document\n  Layout Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically recognizing the layout of handwritten documents is an important\nstep towards useful extraction of information from those documents. The most\ncommon application is to feed downstream applications such as automatic text\nrecognition and keyword spotting; however, the recognition of the layout also\nhelps to establish relationships between elements in the document which allows\nto enrich the information that can be extracted. Most of the modern document\nlayout analysis systems are designed to address only one part of the document\nlayout problem, namely: baseline detection or region segmentation. In contrast,\nwe evaluate the effectiveness of the Mask-RCNN architecture to address the\nproblem of baseline detection and region segmentation in an integrated manner.\nWe present experimental results on two handwritten text datasets and one\nhandwritten music dataset. The analyzed architecture yields promising results,\noutperforming state-of-the-art techniques in all three datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:07:27 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Quir\u00f3s", "Lorenzo", ""], ["Vidal", "Enrique", ""]]}, {"id": "2106.11810", "submitter": "Holger Caesar", "authors": "Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff,\n  Alex Lang, Luke Fletcher, Oscar Beijbom, Sammy Omari", "title": "NuPlan: A closed-loop ML-based planning benchmark for autonomous\n  vehicles", "comments": "Camera-ready for CVPR ADP3 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the world's first closed-loop ML-based planning\nbenchmark for autonomous driving. While there is a growing body of ML-based\nmotion planners, the lack of established datasets and metrics has limited the\nprogress in this area. Existing benchmarks for autonomous vehicle motion\nprediction have focused on short-term motion forecasting, rather than long-term\nplanning. This has led previous works to use open-loop evaluation with L2-based\nmetrics, which are not suitable for fairly evaluating long-term planning. Our\nbenchmark overcomes these limitations by introducing a large-scale driving\ndataset, lightweight closed-loop simulator, and motion-planning-specific\nmetrics. We provide a high-quality dataset with 1500h of human driving data\nfrom 4 cities across the US and Asia with widely varying traffic patterns\n(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop\nsimulation framework with reactive agents and provide a large set of both\ngeneral and scenario-specific planning metrics. We plan to release the dataset\nat NeurIPS 2021 and organize benchmark challenges starting in early 2022.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:24:55 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 06:35:54 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Caesar", "Holger", ""], ["Kabzan", "Juraj", ""], ["Tan", "Kok Seang", ""], ["Fong", "Whye Kit", ""], ["Wolff", "Eric", ""], ["Lang", "Alex", ""], ["Fletcher", "Luke", ""], ["Beijbom", "Oscar", ""], ["Omari", "Sammy", ""]]}, {"id": "2106.11811", "submitter": "Xiang Wang", "authors": "Xiang Wang, Zhiwu Qing, Ziyuan Huang, Yutong Feng, Shiwei Zhang,\n  Jianwen Jiang, Mingqian Tang, Yuanjie Shao, Nong Sang", "title": "Weakly-Supervised Temporal Action Localization Through Local-Global\n  Background Modeling", "comments": "CVPR-2021 HACS Challenge - Weakly-supervised Learning Track champion\n  solution (1st Place)", "journal-ref": "CVPRW-2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-Supervised Temporal Action Localization (WS-TAL) task aims to\nrecognize and localize temporal starts and ends of action instances in an\nuntrimmed video with only video-level label supervision. Due to lack of\nnegative samples of background category, it is difficult for the network to\nseparate foreground and background, resulting in poor detection performance. In\nthis report, we present our 2021 HACS Challenge - Weakly-supervised Learning\nTrack solution that based on BaSNet to address above problem. Specifically, we\nfirst adopt pre-trained CSN, Slowfast, TDN, and ViViT as feature extractors to\nget feature sequences. Then our proposed Local-Global Background Modeling\nNetwork (LGBM-Net) is trained to localize instances by using only video-level\nlabels based on Multi-Instance Learning (MIL). Finally, we ensemble multiple\nmodels to get the final detection results and reach 22.45% mAP on the test set\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 02:58:45 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Xiang", ""], ["Qing", "Zhiwu", ""], ["Huang", "Ziyuan", ""], ["Feng", "Yutong", ""], ["Zhang", "Shiwei", ""], ["Jiang", "Jianwen", ""], ["Tang", "Mingqian", ""], ["Shao", "Yuanjie", ""], ["Sang", "Nong", ""]]}, {"id": "2106.11812", "submitter": "Xiang Wang", "authors": "Xiang Wang, Zhiwu Qing, Ziyuan Huang, Yutong Feng, Shiwei Zhang,\n  Jianwen Jiang, Mingqian Tang, Changxin Gao, Nong Sang", "title": "Proposal Relation Network for Temporal Action Detection", "comments": "CVPR-2021 ActivityNet Temporal Action Localization Challenge champion\n  solution (1st Place)", "journal-ref": "CVPRW-2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report presents our solution for temporal action detection\ntask in AcitivityNet Challenge 2021. The purpose of this task is to locate and\nidentify actions of interest in long untrimmed videos. The crucial challenge of\nthe task comes from that the temporal duration of action varies dramatically,\nand the target actions are typically embedded in a background of irrelevant\nactivities. Our solution builds on BMN, and mainly contains three steps: 1)\naction classification and feature encoding by Slowfast, CSN and ViViT; 2)\nproposal generation. We improve BMN by embedding the proposed Proposal Relation\nNetwork (PRN), by which we can generate proposals of high quality; 3) action\ndetection. We calculate the detection results by assigning the proposals with\ncorresponding classification results. Finally, we ensemble the results under\ndifferent settings and achieve 44.7% on the test set, which improves the\nchampion result in ActivityNet 2020 by 1.9% in terms of average mAP.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 02:51:34 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Xiang", ""], ["Qing", "Zhiwu", ""], ["Huang", "Ziyuan", ""], ["Feng", "Yutong", ""], ["Zhang", "Shiwei", ""], ["Jiang", "Jianwen", ""], ["Tang", "Mingqian", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""]]}, {"id": "2106.11821", "submitter": "Niall McLaughlin", "authors": "Niall McLaughlin, Jesus Martinez del Rincon", "title": "Data Augmentation for Opcode Sequence Based Malware Detection", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data augmentation has been successfully used in many areas of deep-learning\nto significantly improve model performance. Typically data augmentation\nsimulates realistic variations in data in order to increase the apparent\ndiversity of the training-set. However, for opcode-based malware analysis,\nwhere deep learning methods are already achieving state of the art performance,\nit is not immediately clear how to apply data augmentation. In this paper we\nstudy different methods of data augmentation starting with basic methods using\nfixed transformations and moving to methods that adapt to the data. We propose\na novel data augmentation method based on using an opcode embedding layer\nwithin the network and its corresponding opcode embedding matrix to perform\nadaptive data augmentation during training. To the best of our knowledge this\nis the first paper to carry out a systematic study of different augmentation\nmethods applied to opcode sequence based malware classification.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:36:35 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["McLaughlin", "Niall", ""], ["del Rincon", "Jesus Martinez", ""]]}, {"id": "2106.11841", "submitter": "Zhipeng Wang", "authors": "Zhipeng Wang, Hao Wang, Jiexi Yan, Aming Wu, Cheng Deng", "title": "Domain-Smoothing Network for Zero-Shot Sketch-Based Image Retrieval", "comments": "Accepted to IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a novel cross-modal\nretrieval task, where abstract sketches are used as queries to retrieve natural\nimages under zero-shot scenario. Most existing methods regard ZS-SBIR as a\ntraditional classification problem and employ a cross-entropy or triplet-based\nloss to achieve retrieval, which neglect the problems of the domain gap between\nsketches and natural images and the large intra-class diversity in sketches.\nToward this end, we propose a novel Domain-Smoothing Network (DSN) for ZS-SBIR.\nSpecifically, a cross-modal contrastive method is proposed to learn generalized\nrepresentations to smooth the domain gap by mining relations with additional\naugmented samples. Furthermore, a category-specific memory bank with sketch\nfeatures is explored to reduce intra-class diversity in the sketch domain.\nExtensive experiments demonstrate that our approach notably outperforms the\nstate-of-the-art methods in both Sketchy and TU-Berlin datasets. Our source\ncode is publicly available at https://github.com/haowang1992/DSN.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:58:08 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Zhipeng", ""], ["Wang", "Hao", ""], ["Yan", "Jiexi", ""], ["Wu", "Aming", ""], ["Deng", "Cheng", ""]]}, {"id": "2106.11857", "submitter": "Otto Seiskari", "authors": "Otto Seiskari, Pekka Rantalankila, Juho Kannala, Jerry Ylilammi, Esa\n  Rahtu, Arno Solin", "title": "HybVIO: Pushing the Limits of Real-time Visual-inertial Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HybVIO, a novel hybrid approach for combining filtering-based\nvisual-inertial odometry (VIO) with optimization-based SLAM. The core of our\nmethod is highly robust, independent VIO with improved IMU bias modeling,\noutlier rejection, stationarity detection, and feature track selection, which\nis adjustable to run on embedded hardware. Long-term consistency is achieved\nwith a loosely-coupled SLAM module. In academic benchmarks, our solution yields\nexcellent performance in all categories, especially in the real-time use case,\nwhere we outperform the current state-of-the-art. We also demonstrate the\nfeasibility of VIO for vehicular tracking on consumer-grade hardware using a\ncustom dataset, and show good performance in comparison to current commercial\nVISLAM alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 15:21:33 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Seiskari", "Otto", ""], ["Rantalankila", "Pekka", ""], ["Kannala", "Juho", ""], ["Ylilammi", "Jerry", ""], ["Rahtu", "Esa", ""], ["Solin", "Arno", ""]]}, {"id": "2106.11858", "submitter": "Thomas Wollmann", "authors": "Deepthi Sreenivasaiah, Johannes Otterbach, Thomas Wollmann", "title": "MEAL: Manifold Embedding-based Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising patches\nextracted from full image, in each acquisition step. The problem is framed in\nan exploration-exploitation framework by combining an embedding based on\nUniform Manifold Approximation to model representativeness with entropy as\nuncertainty measure to model informativeness. We applied our proposed method to\nthe autonomous driving datasets CamVid and Cityscapes and performed a\nquantitative comparison with state-of-the-art baselines. We find that our\nactive learning method achieves better performance compared to previous\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 15:22:56 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 10:29:57 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Sreenivasaiah", "Deepthi", ""], ["Otterbach", "Johannes", ""], ["Wollmann", "Thomas", ""]]}, {"id": "2106.11895", "submitter": "Xu Yao", "authors": "Xu Yao, Alasdair Newson, Yann Gousseau, Pierre Hellier", "title": "A Latent Transformer for Disentangled and Identity-Preserving Face\n  Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality facial image editing is a challenging problem in the movie\npost-production industry, requiring a high degree of control and identity\npreservation. Previous works that attempt to tackle this problem may suffer\nfrom the entanglement of facial attributes and the loss of the person's\nidentity. Furthermore, many algorithms are limited to a certain task. To tackle\nthese limitations, we propose to edit facial attributes via the latent space of\na StyleGAN generator, by training a dedicated latent transformation network and\nincorporating explicit disentanglement and identity preservation terms in the\nloss function. We further introduce a pipeline to generalize our face editing\nto videos. Our model achieves a disentangled, controllable, and\nidentity-preserving facial attribute editing, even in the challenging case of\nreal (i.e., non-synthetic) images and videos. We conduct extensive experiments\non image and video datasets and show that our model outperforms other\nstate-of-the-art methods in visual quality and quantitative evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:04:30 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Yao", "Xu", ""], ["Newson", "Alasdair", ""], ["Gousseau", "Yann", ""], ["Hellier", "Pierre", ""]]}, {"id": "2106.11902", "submitter": "Mohammad Arif Ul Alam", "authors": "Mohammad Arif Ul Alam, Md Mahmudur Rahman, Jared Q Widberg", "title": "PALMAR: Towards Adaptive Multi-inhabitant Activity Recognition in\n  Point-Cloud Technology", "comments": "Accepted in IEEE International Conference on Computer Communications\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With the advancement of deep neural networks and computer vision-based Human\nActivity Recognition, employment of Point-Cloud Data technologies (LiDAR,\nmmWave) has seen a lot interests due to its privacy preserving nature. Given\nthe high promise of accurate PCD technologies, we develop, PALMAR, a\nmultiple-inhabitant activity recognition system by employing efficient signal\nprocessing and novel machine learning techniques to track individual person\ntowards developing an adaptive multi-inhabitant tracking and HAR system. More\nspecifically, we propose (i) a voxelized feature representation-based real-time\nPCD fine-tuning method, (ii) efficient clustering (DBSCAN and BIRCH), Adaptive\nOrder Hidden Markov Model based multi-person tracking and crossover ambiguity\nreduction techniques and (iii) novel adaptive deep learning-based domain\nadaptation technique to improve the accuracy of HAR in presence of data\nscarcity and diversity (device, location and population diversity). We\nexperimentally evaluate our framework and systems using (i) a real-time PCD\ncollected by three devices (3D LiDAR and 79 GHz mmWave) from 6 participants,\n(ii) one publicly available 3D LiDAR activity data (28 participants) and (iii)\nan embedded hardware prototype system which provided promising HAR performances\nin multi-inhabitants (96%) scenario with a 63% improvement of multi-person\ntracking than state-of-art framework without losing significant system\nperformances in the edge computing device.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:17:50 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Alam", "Mohammad Arif Ul", ""], ["Rahman", "Md Mahmudur", ""], ["Widberg", "Jared Q", ""]]}, {"id": "2106.11911", "submitter": "Yi Fang", "authors": "Hao Huang, Boulbaba Ben Amor, Xichan Lin, Fan Zhu, Yi Fang", "title": "Residual Networks as Flows of Velocity Fields for Diffeomorphic Time\n  Series Alignment", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Non-linear (large) time warping is a challenging source of nuisance in\ntime-series analysis. In this paper, we propose a novel diffeomorphic temporal\ntransformer network for both pairwise and joint time-series alignment. Our\nResNet-TW (Deep Residual Network for Time Warping) tackles the alignment\nproblem by compositing a flow of incremental diffeomorphic mappings. Governed\nby the flow equation, our Residual Network (ResNet) builds smooth, fluid and\nregular flows of velocity fields and consequently generates smooth and\ninvertible transformations (i.e. diffeomorphic warping functions). Inspired by\nthe elegant Large Deformation Diffeomorphic Metric Mapping (LDDMM) framework,\nthe final transformation is built by the flow of time-dependent vector fields\nwhich are none other than the building blocks of our Residual Network. The\nlatter is naturally viewed as an Eulerian discretization schema of the flow\nequation (an ODE). Once trained, our ResNet-TW aligns unseen data by a single\ninexpensive forward pass. As we show in experiments on both univariate (84\ndatasets from UCR archive) and multivariate time-series (MSR Action-3D,\nFlorence-3D and MSR Daily Activity), ResNet-TW achieves competitive performance\nin joint alignment and classification.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:38:48 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Huang", "Hao", ""], ["Amor", "Boulbaba Ben", ""], ["Lin", "Xichan", ""], ["Zhu", "Fan", ""], ["Fang", "Yi", ""]]}, {"id": "2106.11915", "submitter": "Youshan Zhang", "authors": "Youshan Zhang and Brian D. Davison", "title": "Enhanced Separable Disentanglement for Unsupervised Domain Adaptation", "comments": "ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Domain adaptation aims to mitigate the domain gap when transferring knowledge\nfrom an existing labeled domain to a new domain. However, existing\ndisentanglement-based methods do not fully consider separation between\ndomain-invariant and domain-specific features, which means the domain-invariant\nfeatures are not discriminative. The reconstructed features are also not\nsufficiently used during training. In this paper, we propose a novel enhanced\nseparable disentanglement (ESD) model. We first employ a disentangler to\ndistill domain-invariant and domain-specific features. Then, we apply feature\nseparation enhancement processes to minimize contamination between\ndomain-invariant and domain-specific features. Finally, our model reconstructs\ncomplete feature vectors, which are used for further disentanglement during the\ntraining phase. Extensive experiments from three benchmark datasets outperform\nstate-of-the-art methods, especially on challenging cross-domain tasks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:50:53 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zhang", "Youshan", ""], ["Davison", "Brian D.", ""]]}, {"id": "2106.11920", "submitter": "Yi Fang", "authors": "Hao Huang, Boulbaba Ben Amor, Xichan Lin, Fan Zhu, Yi Fang", "title": "G-VAE, a Geometric Convolutional VAE for ProteinStructure Generation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Analyzing the structure of proteins is a key part of understanding their\nfunctions and thus their role in biology at the molecular level. In addition,\ndesign new proteins in a methodical way is a major engineering challenge. In\nthis work, we introduce a joint geometric-neural networks approach for\ncomparing, deforming and generating 3D protein structures. Viewing protein\nstructures as 3D open curves, we adopt the Square Root Velocity Function (SRVF)\nrepresentation and leverage its suitable geometric properties along with Deep\nResidual Networks (ResNets) for a joint registration and comparison. Our\nResNets handle better large protein deformations while being more\ncomputationally efficient. On top of the mathematical framework, we further\ndesign a Geometric Variational Auto-Encoder (G-VAE), that once trained, maps\noriginal, previously unseen structures, into a low-dimensional (latent)\nhyper-sphere. Motivated by the spherical structure of the pre-shape space, we\nnaturally adopt the von Mises-Fisher (vMF) distribution to model our hidden\nvariables. We test the effectiveness of our models by generating novel protein\nstructures and predicting completions of corrupted protein structures.\nExperimental results show that our method is able to generate plausible\nstructures, different from the structures in the training data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:52:48 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Huang", "Hao", ""], ["Amor", "Boulbaba Ben", ""], ["Lin", "Xichan", ""], ["Zhu", "Fan", ""], ["Fang", "Yi", ""]]}, {"id": "2106.11921", "submitter": "Ismail Elezi", "authors": "Ismail Elezi, Zhiding Yu, Anima Anandkumar, Laura Leal-Taixe, Jose M.\n  Alvarez", "title": "Towards Reducing Labeling Cost in Deep Object Detection", "comments": "Includes supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have reached very high accuracy on object detection but\ntheir success hinges on large amounts of labeled data. To reduce the dependency\non labels, various active-learning strategies have been proposed, typically\nbased on the confidence of the detector. However, these methods are biased\ntowards best-performing classes and can lead to acquired datasets that are not\ngood representatives of the data in the testing set. In this work, we propose a\nunified framework for active learning, that considers both the uncertainty and\nthe robustness of the detector, ensuring that the network performs accurately\nin all classes. Furthermore, our method is able to pseudo-label the very\nconfident predictions, suppressing a potential distribution drift while further\nboosting the performance of the model. Experiments show that our method\ncomprehensively outperforms a wide range of active-learning methods on PASCAL\nVOC07+12 and MS-COCO, having up to a 7.7% relative improvement, or up to 82%\nreduction in labeling cost.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:53:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Elezi", "Ismail", ""], ["Yu", "Zhiding", ""], ["Anandkumar", "Anima", ""], ["Leal-Taixe", "Laura", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "2106.11930", "submitter": "Albin Soutif--Cormerais", "authors": "Albin Soutif--Cormerais, Marc Masana, Joost Van de Weijer,\n  Bart{\\l}omiej Twardowski", "title": "On the importance of cross-task features for class-incremental learning", "comments": "includes supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In class-incremental learning, an agent with limited resources needs to learn\na sequence of classification tasks, forming an ever growing classification\nproblem, with the constraint of not being able to access data from previous\ntasks. The main difference with task-incremental learning, where a task-ID is\navailable at inference time, is that the learner also needs to perform\ncross-task discrimination, i.e. distinguish between classes that have not been\nseen together. Approaches to tackle this problem are numerous and mostly make\nuse of an external memory (buffer) of non-negligible size. In this paper, we\nablate the learning of cross-task features and study its influence on the\nperformance of basic replay strategies used for class-IL. We also define a new\nforgetting measure for class-incremental learning, and see that forgetting is\nnot the principal cause of low performance. Our experimental results show that\nfuture algorithms for class-incremental learning should not only prevent\nforgetting, but also aim to improve the quality of the cross-task features.\nThis is especially important when the number of classes per task is small.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:03:15 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Soutif--Cormerais", "Albin", ""], ["Masana", "Marc", ""], ["Van de Weijer", "Joost", ""], ["Twardowski", "Bart\u0142omiej", ""]]}, {"id": "2106.11942", "submitter": "Abraham Smith", "authors": "Abraham George Smith, Jens Petersen, Cynthia Terrones-Campos, Anne\n  Kiil Berthelsen, Nora Jarrett Forbes, Sune Darkner, Lena Specht, and Ivan\n  Richter Vogelius", "title": "RootPainter3D: Interactive-machine-learning enables rapid and accurate\n  contouring for radiotherapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organ-at-risk contouring is still a bottleneck in radiotherapy, with many\ndeep learning methods falling short of promised results when evaluated on\nclinical data. We investigate the accuracy and time-savings resulting from the\nuse of an interactive-machine-learning method for an organ-at-risk contouring\ntask. We compare the method to the Eclipse contouring software and find strong\nagreement with manual delineations, with a dice score of 0.95. The annotations\ncreated using corrective-annotation also take less time to create as more\nimages are annotated, resulting in substantial time savings compared to manual\nmethods, with hearts that take 2 minutes and 2 seconds to delineate on average,\nafter 923 images have been delineated, compared to 7 minutes and 1 seconds when\ndelineating manually. Our experiment demonstrates that\ninteractive-machine-learning with corrective-annotation provides a fast and\naccessible way for non computer-scientists to train deep-learning models to\nsegment their own structures of interest as part of routine clinical workflows.\n  Source code is available at\n\\href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:26:58 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Smith", "Abraham George", ""], ["Petersen", "Jens", ""], ["Terrones-Campos", "Cynthia", ""], ["Berthelsen", "Anne Kiil", ""], ["Forbes", "Nora Jarrett", ""], ["Darkner", "Sune", ""], ["Specht", "Lena", ""], ["Vogelius", "Ivan Richter", ""]]}, {"id": "2106.11944", "submitter": "Shaofei Wang", "authors": "Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, Siyu Tang", "title": "MetaAvatar: Learning Animatable Clothed Human Models from Few Depth\n  Images", "comments": "17 pages, 9 figures. Project page:\n  https://neuralbodies.github.io/metavatar/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we aim to create generalizable and controllable neural signed\ndistance fields (SDFs) that represent clothed humans from monocular depth\nobservations. Recent advances in deep learning, especially neural implicit\nrepresentations, have enabled human shape reconstruction and controllable\navatar generation from different sensor inputs. However, to generate realistic\ncloth deformations from novel input poses, watertight meshes or dense full-body\nscans are usually needed as inputs. Furthermore, due to the difficulty of\neffectively modeling pose-dependent cloth deformations for diverse body shapes\nand cloth types, existing approaches resort to per-subject/cloth-type\noptimization from scratch, which is computationally expensive. In contrast, we\npropose an approach that can quickly generate realistic clothed human avatars,\nrepresented as controllable neural SDFs, given only monocular depth images. We\nachieve this by using meta-learning to learn an initialization of a\nhypernetwork that predicts the parameters of neural SDFs. The hypernetwork is\nconditioned on human poses and represents a clothed neural avatar that deforms\nnon-rigidly according to the input poses. Meanwhile, it is meta-learned to\neffectively incorporate priors of diverse body shapes and cloth types and thus\ncan be much faster to fine-tune, compared to models trained from scratch. We\nqualitatively and quantitatively show that our approach outperforms\nstate-of-the-art approaches that require complete meshes as inputs while our\napproach requires only depth frames as inputs and runs orders of magnitudes\nfaster. Furthermore, we demonstrate that our meta-learned hypernetwork is very\nrobust, being the first to generate avatars with realistic dynamic cloth\ndeformations given as few as 8 monocular depth frames.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:30:12 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Shaofei", ""], ["Mihajlovic", "Marko", ""], ["Ma", "Qianli", ""], ["Geiger", "Andreas", ""], ["Tang", "Siyu", ""]]}, {"id": "2106.11952", "submitter": "Jiahao Xie", "authors": "Jiahao Xie, Xiaohang Zhan, Ziwei Liu, Yew Soon Ong, Chen Change Loy", "title": "Unsupervised Object-Level Representation Learning from Scene Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive self-supervised learning has largely narrowed the gap to\nsupervised pre-training on ImageNet. However, its success highly relies on the\nobject-centric priors of ImageNet, i.e., different augmented views of the same\nimage correspond to the same object. Such a heavily curated constraint becomes\nimmediately infeasible when pre-trained on more complex scene images with many\nobjects. To overcome this limitation, we introduce Object-level Representation\nLearning (ORL), a new self-supervised learning framework towards scene images.\nOur key insight is to leverage image-level self-supervised pre-training as the\nprior to discover object-level semantic correspondence, thus realizing\nobject-level representation learning from scene images. Extensive experiments\non COCO show that ORL significantly improves the performance of self-supervised\nlearning on scene images, even surpassing supervised ImageNet pre-training on\nseveral downstream tasks. Furthermore, ORL improves the downstream performance\nwhen more unlabeled scene images are available, demonstrating its great\npotential of harnessing unlabeled data in the wild. We hope our approach can\nmotivate future research on more general-purpose unsupervised representation\nlearning from scene data. Project page: https://www.mmlab-ntu.com/project/orl/.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:51:24 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Xie", "Jiahao", ""], ["Zhan", "Xiaohang", ""], ["Liu", "Ziwei", ""], ["Ong", "Yew Soon", ""], ["Loy", "Chen Change", ""]]}, {"id": "2106.11958", "submitter": "Lei Ke", "authors": "Lei Ke, Xia Li, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang and\n  Fisher Yu", "title": "Prototypical Cross-Attention Networks for Multiple Object Tracking and\n  Segmentation", "comments": "Multiple object tracking and segmentation on large-scale datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple object tracking and segmentation requires detecting, tracking, and\nsegmenting objects belonging to a set of given classes. Most approaches only\nexploit the temporal dimension to address the association problem, while\nrelying on single frame predictions for the segmentation mask itself. We\npropose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich\nspatio-temporal information for online multiple object tracking and\nsegmentation. PCAN first distills a space-time memory into a set of prototypes\nand then employs cross-attention to retrieve rich information from the past\nframes. To segment each object, PCAN adopts a prototypical appearance module to\nlearn a set of contrastive foreground and background prototypes, which are then\npropagated over time. Extensive experiments demonstrate that PCAN outperforms\ncurrent video instance tracking and segmentation competition winners on both\nYoutube-VIS and BDD100K datasets, and shows efficacy to both one-stage and\ntwo-stage segmentation frameworks. Code will be available at\nhttp://vis.xyz/pub/pcan.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:57:24 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ke", "Lei", ""], ["Li", "Xia", ""], ["Danelljan", "Martin", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""], ["Yu", "Fisher", ""]]}, {"id": "2106.11963", "submitter": "Yuxin Fang", "authors": "Shusheng Yang, Yuxin Fang, Xinggang Wang, Yu Li, Ying Shan, Bin Feng,\n  Wenyu Liu", "title": "Tracking Instances as Queries", "comments": "Preprint. Work in progress", "journal-ref": "CVPR 2021 Workshop. 2nd Place Solution for YouTube-VOS Challenge\n  2021: Video Instance Segmentation", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, query based deep networks catch lots of attention owing to their\nend-to-end pipeline and competitive results on several fundamental computer\nvision tasks, such as object detection, semantic segmentation, and instance\nsegmentation. However, how to establish a query based video instance\nsegmentation (VIS) framework with elegant architecture and strong performance\nremains to be settled. In this paper, we present \\textbf{QueryTrack} (i.e.,\ntracking instances as queries), a unified query based VIS framework fully\nleveraging the intrinsic one-to-one correspondence between instances and\nqueries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on\nYouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS\nChallenge at CVPR 2021 \\textbf{with a single online end-to-end model, single\nscale testing \\& modest amount of training data}. We also provide\nQueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 val set as references\nfor the VIS community.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:59:12 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 15:02:24 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Yang", "Shusheng", ""], ["Fang", "Yuxin", ""], ["Wang", "Xinggang", ""], ["Li", "Yu", ""], ["Shan", "Ying", ""], ["Feng", "Bin", ""], ["Liu", "Wenyu", ""]]}, {"id": "2106.12011", "submitter": "Yun Liu", "authors": "Yu-Huan Wu, Yun Liu, Xin Zhan, Ming-Ming Cheng", "title": "P2T: Pyramid Pooling Transformer for Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper jointly resolves two problems in vision transformer: i) the\ncomputation of Multi-Head Self-Attention (MHSA) has high computational/space\ncomplexity; ii) recent vision transformer networks are overly tuned for image\nclassification, ignoring the difference between image classification (simple\nscenarios, more similar to NLP) and downstream scene understanding tasks\n(complicated scenarios, rich structural and contextual information). To this\nend, we note that pyramid pooling has been demonstrated to be effective in\nvarious vision tasks owing to its powerful ability in context abstraction, and\nits natural property of spatial invariance is also suitable to address the loss\nof structural information (problem ii)). Hence, we propose to adapt pyramid\npooling to MHSA for alleviating its high requirement on computational resources\n(problem i)). In this way, this pooling-based MHSA can well address the above\ntwo problems and is thus flexible and powerful for downstream scene\nunderstanding tasks. Plugged with our pooling-based MHSA, we build a\ndownstream-task-oriented transformer network, dubbed Pyramid Pooling\nTransformer (P2T). Extensive experiments demonstrate that, when applied P2T as\nthe backbone network, it shows substantial superiority in various downstream\nscene understanding tasks such as semantic segmentation, object detection,\ninstance segmentation, and visual saliency detection, compared to previous CNN-\nand transformer-based networks. The code will be released at\nhttps://github.com/yuhuan-wu/P2T.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:28:52 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 19:05:55 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 16:22:53 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wu", "Yu-Huan", ""], ["Liu", "Yun", ""], ["Zhan", "Xin", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2106.12014", "submitter": "Fernando P\\'erez-Garc\\'ia", "authors": "Fernando P\\'erez-Garc\\'ia, Catherine Scott, Rachel Sparks, Beate Diehl\n  and S\\'ebastien Ourselin", "title": "Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily\n  Long Videos of Seizures", "comments": "Accepted at the 24th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Detailed analysis of seizure semiology, the symptoms and signs which occur\nduring a seizure, is critical for management of epilepsy patients. Inter-rater\nreliability using qualitative visual analysis is often poor for semiological\nfeatures. Therefore, automatic and quantitative analysis of video-recorded\nseizures is needed for objective assessment.\n  We present GESTURES, a novel architecture combining convolutional neural\nnetworks (CNNs) and recurrent neural networks (RNNs) to learn deep\nrepresentations of arbitrarily long videos of epileptic seizures.\n  We use a spatiotemporal CNN (STCNN) pre-trained on large human action\nrecognition (HAR) datasets to extract features from short snippets (approx. 0.5\ns) sampled from seizure videos. We then train an RNN to learn seizure-level\nrepresentations from the sequence of features.\n  We curated a dataset of seizure videos from 68 patients and evaluated\nGESTURES on its ability to classify seizures into focal onset seizures (FOSs)\n(N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (N = 77),\nobtaining an accuracy of 98.9% using bidirectional long short-term memory\n(BLSTM) units.\n  We demonstrate that an STCNN trained on a HAR dataset can be used in\ncombination with an RNN to accurately represent arbitrarily long videos of\nseizures. GESTURES can provide accurate seizure classification by modeling\nsequences of semiologies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:40:31 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["P\u00e9rez-Garc\u00eda", "Fernando", ""], ["Scott", "Catherine", ""], ["Sparks", "Rachel", ""], ["Diehl", "Beate", ""], ["Ourselin", "S\u00e9bastien", ""]]}, {"id": "2106.12016", "submitter": "Keaton Hamm", "authors": "Reeshad Arian, Keaton Hamm", "title": "On Matrix Factorizations in Subspace Clustering", "comments": "13 pages plus 4 pages of tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article explores subspace clustering algorithms using CUR\ndecompositions, and examines the effect of various hyperparameters in these\nalgorithms on clustering performance on two real-world benchmark datasets, the\nHopkins155 motion segmentation dataset and the Yale face dataset. Extensive\nexperiments are done for a variety of sampling methods and oversampling\nparameters for these datasets, and some guidelines for parameter choices are\ngiven for practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:42:44 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Arian", "Reeshad", ""], ["Hamm", "Keaton", ""]]}, {"id": "2106.12023", "submitter": "Xianyuan Liu", "authors": "Xianyuan Liu, Raivo Koot, Shuo Zhou, Tao Lei, Haiping Lu", "title": "Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised\n  Domain Adaptation Challenge for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the technical details of our submission to the\nEPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action\nRecognition. The EPIC-Kitchens dataset is more difficult than other video\ndomain adaptation datasets due to multi-tasks with more modalities. Firstly, to\nparticipate in the challenge, we employ a transformer to capture the spatial\ninformation from each modality. Secondly, we employ a temporal attention module\nto model temporal-wise inter-dependency. Thirdly, we employ the adversarial\ndomain adaptation network to learn the general features between labeled source\nand unlabeled target domain. Finally, we incorporate multiple modalities to\nimprove the performance by a three-stream network with late fusion. Our network\nachieves the comparable performance with the state-of-the-art baseline T$A^3$N\nand outperforms the baseline on top-1 accuracy for verb class and top-5\naccuracies for all three tasks which are verb, noun and action. Under the team\nname xy9, our submission achieved 5th place in terms of top-1 accuracy for verb\nclass and all top-5 accuracies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 19:17:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liu", "Xianyuan", ""], ["Koot", "Raivo", ""], ["Zhou", "Shuo", ""], ["Lei", "Tao", ""], ["Lu", "Haiping", ""]]}, {"id": "2106.12026", "submitter": "R. Kenny Jones", "authors": "R. Kenny Jones and Rana Hanocka and Daniel Ritchie", "title": "The Neurally-Guided Shape Parser: A Monte Carlo Method for Hierarchical\n  Labeling of Over-segmented 3D Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many learning-based 3D shape semantic segmentation methods assign labels to\nshape atoms (e.g. points in a point cloud or faces in a mesh) with a\nsingle-pass approach trained in an end-to-end fashion. Such methods achieve\nimpressive performance but require large amounts of labeled training data. This\nparadigm entangles two separable subproblems: (1) decomposing a shape into\nregions and (2) assigning semantic labels to these regions. We claim that\ndisentangling these subproblems reduces the labeled data burden: (1) region\ndecomposition requires no semantic labels and could be performed in an\nunsupervised fashion, and (2) labeling shape regions instead of atoms results\nin a smaller search space and should be learnable with less labeled training\ndata. In this paper, we investigate this second claim by presenting the\nNeurally-Guided Shape Parser (NGSP), a method that learns how to assign\nsemantic labels to regions of an over-segmented 3D shape. We solve this problem\nvia MAP inference, modeling the posterior probability of a labeling assignment\nconditioned on an input shape. We employ a Monte Carlo importance sampling\napproach guided by a neural proposal network, a search-based approach made\nfeasible by assuming the input shape is decomposed into discrete regions. We\nevaluate NGSP on the task of hierarchical semantic segmentation on manufactured\n3D shapes from PartNet. We find that NGSP delivers significant performance\nimprovements over baselines that learn to label shape atoms and then aggregate\npredictions for each shape region, especially in low-data regimes. Finally, we\ndemonstrate that NGSP is robust to region granularity, as it maintains strong\nsegmentation performance even as the regions undergo significant corruption.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 19:26:01 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Jones", "R. Kenny", ""], ["Hanocka", "Rana", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2106.12037", "submitter": "Tomoyuki Shishido", "authors": "Tomoyuki Shishido, Fehmiju Fati, Daisuke Tokushige, and Yasuhiro Ono", "title": "Listen to Your Favorite Melodies with img2Mxml, Producing MusicXML from\n  Sheet Music Image by Measure-based Multimodal Deep Learning-driven Assembly", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has recently been applied to optical music recognition (OMR).\nHowever, currently OMR processing from various sheet music images still lacks\nprecision to be widely applicable. Here, we present an MMdA (Measure-based\nMultimodal deep learning (DL)-driven Assembly) method allowing for end-to-end\nOMR processing from various images including inclined photo images. Using this\nmethod, measures are extracted by a deep learning model, aligned, and resized\nto be used for inference of given musical symbol components by using multiple\ndeep learning models in sequence or in parallel. Use of each standardized\nmeasure enables efficient training of the models and accurate adjustment of\nfive staff lines in each measure. Multiple musical symbol component category\nmodels with a small number of feature types can represent a diverse set of\nnotes and other musical symbols including chords. This MMdA method provides a\nsolution to end-to-end OMR processing with precision.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 03:35:33 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shishido", "Tomoyuki", ""], ["Fati", "Fehmiju", ""], ["Tokushige", "Daisuke", ""], ["Ono", "Yasuhiro", ""]]}, {"id": "2106.12052", "submitter": "Lior Yariv", "authors": "Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman", "title": "Volume Rendering of Neural Implicit Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural volume rendering became increasingly popular recently due to its\nsuccess in synthesizing novel views of a scene from a sparse set of input\nimages. So far, the geometry learned by neural volume rendering techniques was\nmodeled using a generic density function. Furthermore, the geometry itself was\nextracted using an arbitrary level set of the density function leading to a\nnoisy, often low fidelity reconstruction. The goal of this paper is to improve\ngeometry representation and reconstruction in neural volume rendering. We\nachieve that by modeling the volume density as a function of the geometry. This\nis in contrast to previous work modeling the geometry as a function of the\nvolume density. In more detail, we define the volume density function as\nLaplace's cumulative distribution function (CDF) applied to a signed distance\nfunction (SDF) representation. This simple density representation has three\nbenefits: (i) it provides a useful inductive bias to the geometry learned in\nthe neural volume rendering process; (ii) it facilitates a bound on the opacity\napproximation error, leading to an accurate sampling of the viewing ray.\nAccurate sampling is important to provide a precise coupling of geometry and\nradiance; and (iii) it allows efficient unsupervised disentanglement of shape\nand appearance in volume rendering. Applying this new density representation to\nchallenging scene multiview datasets produced high quality geometry\nreconstructions, outperforming relevant baselines. Furthermore, switching shape\nand appearance between scenes is possible due to the disentanglement of the\ntwo.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 20:23:16 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Yariv", "Lior", ""], ["Gu", "Jiatao", ""], ["Kasten", "Yoni", ""], ["Lipman", "Yaron", ""]]}, {"id": "2106.12054", "submitter": "Youshan Zhang", "authors": "Youshan Zhang, Brian D. Davison, Vivien W. Talghader, Zhiyu Chen,\n  Zhiyong Xiao, Gary J. Kunkel", "title": "Automatic Head Overcoat Thickness Measure with NASNet-Large-Decoder Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Transmission electron microscopy (TEM) is one of the primary tools to show\nmicrostructural characterization of materials as well as film thickness.\nHowever, manual determination of film thickness from TEM images is\ntime-consuming as well as subjective, especially when the films in question are\nvery thin and the need for measurement precision is very high. Such is the case\nfor head overcoat (HOC) thickness measurements in the magnetic hard disk drive\nindustry. It is therefore necessary to develop software to automatically\nmeasure HOC thickness. In this paper, for the first time, we propose a HOC\nlayer segmentation method using NASNet-Large as an encoder and then followed by\na decoder architecture, which is one of the most commonly used architectures in\ndeep learning for image segmentation. To further improve segmentation results,\nwe are the first to propose a post-processing layer to remove irrelevant\nportions in the segmentation result. To measure the thickness of the segmented\nHOC layer, we propose a regressive convolutional neural network (RCNN) model as\nwell as orthogonal thickness calculation methods. Experimental results\ndemonstrate a higher dice score for our model which has lower mean squared\nerror and outperforms current state-of-the-art manual measurement.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 20:53:58 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Zhang", "Youshan", ""], ["Davison", "Brian D.", ""], ["Talghader", "Vivien W.", ""], ["Chen", "Zhiyu", ""], ["Xiao", "Zhiyong", ""], ["Kunkel", "Gary J.", ""]]}, {"id": "2106.12070", "submitter": "Navid Kardan", "authors": "Navid Kardan, Ankit Sharma and Kenneth O. Stanley", "title": "Towards Consistent Predictive Confidence through Fitted Ensembles", "comments": "IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are behind many of the recent successes in machine\nlearning applications. However, these models can produce overconfident\ndecisions while encountering out-of-distribution (OOD) examples or making a\nwrong prediction. This inconsistent predictive confidence limits the\nintegration of independently-trained learning models into a larger system. This\npaper introduces separable concept learning framework to realistically measure\nthe performance of classifiers in presence of OOD examples. In this setup,\nseveral instances of a classifier are trained on different parts of a partition\nof the set of classes. Later, the performance of the combination of these\nmodels is evaluated on a separate test set. Unlike current OOD detection\ntechniques, this framework does not require auxiliary OOD datasets and does not\nseparate classification from detection performance. Furthermore, we present a\nnew strong baseline for more consistent predictive confidence in deep models,\ncalled fitted ensembles, where overconfident predictions are rectified by\ntransformed versions of the original classification task. Fitted ensembles can\nnaturally detect OOD examples without requiring auxiliary data by observing\ncontradicting predictions among its components. Experiments on MNIST, SVHN,\nCIFAR-10/100, and ImageNet show fitted ensemble significantly outperform\nconventional ensembles on OOD examples and are possible to scale.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 21:32:31 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kardan", "Navid", ""], ["Sharma", "Ankit", ""], ["Stanley", "Kenneth O.", ""]]}, {"id": "2106.12074", "submitter": "Xiaodong Yang", "authors": "Xiaodong Yang, Tomoya Yamaguchi, Hoang-Dung Tran, Bardh Hoxha, Taylor\n  T Johnson, Danil Prokhorov", "title": "Reachability Analysis of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have been widely employed as an effective\ntechnique to handle complex and practical problems. However, one of the\nfundamental problems is the lack of formal methods to analyze their behavior.\nTo address this challenge, we propose an approach to compute the exact\nreachable sets of a network given an input domain, where the reachable set is\nrepresented by the face lattice structure. Besides the computation of reachable\nsets, our approach is also capable of backtracking to the input domain given an\noutput reachable set. Therefore, a full analysis of a network's behavior can be\nrealized. In addition, an approach for fast analysis is also introduced, which\nconducts fast computation of reachable sets by considering selected sensitive\nneurons in each layer. The exact pixel-level reachability analysis method is\nevaluated on a CNN for the CIFAR10 dataset and compared to related works. The\nfast analysis method is evaluated over a CNN CIFAR10 dataset and VGG16\narchitecture for the ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 21:42:00 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Yang", "Xiaodong", ""], ["Yamaguchi", "Tomoya", ""], ["Tran", "Hoang-Dung", ""], ["Hoxha", "Bardh", ""], ["Johnson", "Taylor T", ""], ["Prokhorov", "Danil", ""]]}, {"id": "2106.12102", "submitter": "Farid Yagubbayli", "authors": "Farid Yagubbayli, Alessio Tonioni, Federico Tombari", "title": "LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern deep learning-based multi-view 3D reconstruction techniques use\nRNNs or fusion modules to combine information from multiple images after\nencoding them. These two separate steps have loose connections and do not\nconsider all available information while encoding each view. We propose\nLegoFormer, a transformer-based model that unifies object reconstruction under\na single framework and parametrizes the reconstructed occupancy grid by its\ndecomposition factors. This reformulation allows the prediction of an object as\na set of independent structures then aggregated to obtain the final\nreconstruction. Experiments conducted on ShapeNet display the competitive\nperformance of our network with respect to the state-of-the-art methods. We\nalso demonstrate how the use of self-attention leads to increased\ninterpretability of the model output.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 00:15:08 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Yagubbayli", "Farid", ""], ["Tonioni", "Alessio", ""], ["Tombari", "Federico", ""]]}, {"id": "2106.12123", "submitter": "Xin Luo", "authors": "Xin Luo, Wei Chen, Yusong Tan, Chen Li, Yulin He, Xiaogang Jia", "title": "Exploiting Negative Learning for Implicit Pseudo Label Rectification in\n  Source-Free Domain Adaptive Semantic Segmentation", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is desirable to transfer the knowledge stored in a well-trained source\nmodel onto non-annotated target domain in the absence of source data. However,\nstate-of-the-art methods for source free domain adaptation (SFDA) are subject\nto strict limits: 1) access to internal specifications of source models is a\nmust; and 2) pseudo labels should be clean during self-training, making\ncritical tasks relying on semantic segmentation unreliable. Aiming at these\npitfalls, this study develops a domain adaptive solution to semantic\nsegmentation with pseudo label rectification (namely \\textit{PR-SFDA}), which\noperates in two phases: 1) \\textit{Confidence-regularized unsupervised\nlearning}: Maximum squares loss applies to regularize the target model to\nensure the confidence in prediction; and 2) \\textit{Noise-aware pseudo label\nlearning}: Negative learning enables tolerance to noisy pseudo labels in\ntraining, meanwhile positive learning achieves fast convergence. Extensive\nexperiments have been performed on domain adaptive semantic segmentation\nbenchmark, \\textit{GTA5 $\\to$ Cityscapes}. Overall, \\textit{PR-SFDA} achieves a\nperformance of 49.0 mIoU, which is very close to that of the state-of-the-art\ncounterparts. Note that the latter demand accesses to the source model's\ninternal specifications, whereas the \\textit{PR-SFDA} solution needs none as a\nsharp contrast.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 02:20:31 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Luo", "Xin", ""], ["Chen", "Wei", ""], ["Tan", "Yusong", ""], ["Li", "Chen", ""], ["He", "Yulin", ""], ["Jia", "Xiaogang", ""]]}, {"id": "2106.12139", "submitter": "Fangyuan Lei", "authors": "Fangyuan Lei, Da Huang, Jianjian Jiang, Ruijun Ma, Senhong Wang,\n  Jiangzhong Cao, Yusen Lin and Qingyun Dai", "title": "PatentNet: A Large-Scale Incomplete Multiview, Multimodal, Multilabel\n  Industrial Goods Image Database", "comments": "12 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In deep learning area, large-scale image datasets bring a breakthrough in the\nsuccess of object recognition and retrieval. Nowadays, as the embodiment of\ninnovation, the diversity of the industrial goods is significantly larger, in\nwhich the incomplete multiview, multimodal and multilabel are different from\nthe traditional dataset. In this paper, we introduce an industrial goods\ndataset, namely PatentNet, with numerous highly diverse, accurate and detailed\nannotations of industrial goods images, and corresponding texts. In PatentNet,\nthe images and texts are sourced from design patent. Within over 6M images and\ncorresponding texts of industrial goods labeled manually checked by\nprofessionals, PatentNet is the first ongoing industrial goods image database\nwhose varieties are wider than industrial goods datasets used previously for\nbenchmarking. PatentNet organizes millions of images into 32 classes and 219\nsubclasses based on the Locarno Classification Agreement. Through extensive\nexperiments on image classification, image retrieval and incomplete multiview\nclustering, we demonstrate that our PatentNet is much more diverse, complex,\nand challenging, enjoying higher potentials than existing industrial image\ndatasets. Furthermore, the characteristics of incomplete multiview, multimodal\nand multilabel in PatentNet are able to offer unparalleled opportunities in the\nartificial intelligence community and beyond.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 03:22:52 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Lei", "Fangyuan", ""], ["Huang", "Da", ""], ["Jiang", "Jianjian", ""], ["Ma", "Ruijun", ""], ["Wang", "Senhong", ""], ["Cao", "Jiangzhong", ""], ["Lin", "Yusen", ""], ["Dai", "Qingyun", ""]]}, {"id": "2106.12153", "submitter": "Wufeng Xue", "authors": "Zejian Chen, Wei Zhuo, Tianfu Wang, Wufeng Xue and Dong Ni", "title": "Bootstrap Representation Learning for Segmentation on Medical Volumes\n  and Sequences", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we propose a novel straightforward method for medical volume\nand sequence segmentation with limited annotations. To avert laborious\nannotating, the recent success of self-supervised learning(SSL) motivates the\npre-training on unlabeled data. Despite its success, it is still challenging to\nadapt typical SSL methods to volume/sequence segmentation, due to their lack of\nmining on local semantic discrimination and rare exploitation on volume and\nsequence structures. Based on the continuity between slices/frames and the\ncommon spatial layout of organs across volumes/sequences, we introduced a novel\nbootstrap self-supervised representation learning method by leveraging the\npredictable possibility of neighboring slices. At the core of our method is a\nsimple and straightforward dense self-supervision on the predictions of local\nrepresentations and a strategy of predicting locals based on global context,\nwhich enables stable and reliable supervision for both global and local\nrepresentation mining among volumes. Specifically, we first proposed an\nasymmetric network with an attention-guided predictor to enforce\ndistance-specific prediction and supervision on slices within and across\nvolumes/sequences. Secondly, we introduced a novel prototype-based\nforeground-background calibration module to enhance representation consistency.\nThe two parts are trained jointly on labeled and unlabeled data. When evaluated\non three benchmark datasets of medical volumes and sequences, our model\noutperforms existing methods with a large margin of 4.5\\% DSC on ACDC, 1.7\\% on\nProstate, and 2.3\\% on CAMUS. Intensive evaluations reveals the effectiveness\nand superiority of our method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 04:37:28 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chen", "Zejian", ""], ["Zhuo", "Wei", ""], ["Wang", "Tianfu", ""], ["Xue", "Wufeng", ""], ["Ni", "Dong", ""]]}, {"id": "2106.12154", "submitter": "Gilles Hacheme", "authors": "Gilles Hacheme, Noureini Sayouti", "title": "Neural Fashion Image Captioning : Accounting for Data Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Image captioning has increasingly large domains of application, and fashion\nis not an exception. Having automatic item descriptions is of great interest\nfor fashion web platforms hosting sometimes hundreds of thousands of images.\nThis paper is one of the first tackling image captioning for fashion images. To\ncontribute addressing dataset diversity issues, we introduced the InFashAIv1\ndataset containing almost 16.000 African fashion item images with their titles,\nprices and general descriptions. We also used the well known DeepFashion\ndataset in addition to InFashAIv1. Captions are generated using the Show and\nTell model made of CNN encoder and RNN Decoder. We showed that jointly training\nthe model on both datasets improves captions quality for African style fashion\nimages, suggesting a transfer learning from Western style data. The InFashAIv1\ndataset is released on Github to encourage works with more diversity inclusion.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 04:39:26 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 04:14:17 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Hacheme", "Gilles", ""], ["Sayouti", "Noureini", ""]]}, {"id": "2106.12157", "submitter": "Thangarajah Akilan Mr", "authors": "Thangarajah Akilan", "title": "CxSE: Chest X-ray Slow Encoding CNN forCOVID-19 Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The coronavirus continues to disrupt our everyday lives as it spreads at an\nexponential rate. It needs to be detected quickly in order to quarantine\npositive patients so as to avoid further spread. This work proposes a new\nconvolutional neural network (CNN) architecture called 'slow Encoding CNN. The\nproposed model's best performance wrt Sensitivity, Positive Predictive Value\n(PPV) found to be SP=0.67, PP=0.98, SN=0.96, and PN=0.52 on AI AGAINST COVID19\n- Screening X-ray images for COVID-19 Infections competition's test data\nsamples. SP and PP stand for the Sensitivity and PPV of the COVID-19 positive\nclass, while PN and SN stand for the Sensitivity and PPV of the COVID-19\nnegative class.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 04:57:39 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Akilan", "Thangarajah", ""]]}, {"id": "2106.12163", "submitter": "Yuehai Chen", "authors": "Yuehai Chen, Jing Yang, Dong Zhang, Kun Zhang, Badong Chen and Shaoyi\n  Du", "title": "Region-Aware Network: Model Human's Top-Down Visual Perception Mechanism\n  for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background noise and scale variation are common problems that have been long\nrecognized in crowd counting. Humans glance at a crowd image and instantly know\nthe approximate number of human and where they are through attention the crowd\nregions and the congestion degree of crowd regions with a global receptive\nfiled. Hence, in this paper, we propose a novel feedback network with\nRegion-Aware block called RANet by modeling human's Top-Down visual perception\nmechanism. Firstly, we introduce a feedback architecture to generate priority\nmaps that provide prior about candidate crowd regions in input images. The\nprior enables the RANet pay more attention to crowd regions. Then we design\nRegion-Aware block that could adaptively encode the contextual information into\ninput images through global receptive field. More specifically, we scan the\nwhole input images and its priority maps in the form of column vector to obtain\na relevance matrix estimating their similarity. The relevance matrix obtained\nwould be utilized to build global relationships between pixels. Our method\noutperforms state-of-the-art crowd counting methods on several public datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 05:11:58 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chen", "Yuehai", ""], ["Yang", "Jing", ""], ["Zhang", "Dong", ""], ["Zhang", "Kun", ""], ["Chen", "Badong", ""], ["Du", "Shaoyi", ""]]}, {"id": "2106.12169", "submitter": "Boyuan Feng", "authors": "Boyuan Feng, Yuke Wang, Tong Geng, Ang Li, Yufei Ding", "title": "APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU\n  Tensor Cores", "comments": "Accepted by SC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, accelerating neural networks with quantization has been\nwidely studied. Unfortunately, prior efforts with diverse precisions (e.g.,\n1-bit weights and 2-bit activations) are usually restricted by limited\nprecision support on GPUs (e.g., int1 and int4). To break such restrictions, we\nintroduce the first Arbitrary Precision Neural Network framework (APNN-TC) to\nfully exploit quantization benefits on Ampere GPU Tensor Cores. Specifically,\nAPNN-TC first incorporates a novel emulation algorithm to support arbitrary\nshort bit-width computation with int1 compute primitives and XOR/AND Boolean\noperations. Second, APNN-TC integrates arbitrary precision layer designs to\nefficiently map our emulation algorithm to Tensor Cores with novel batching\nstrategies and specialized memory organization. Third, APNN-TC embodies a novel\narbitrary precision NN design to minimize memory access across layers and\nfurther improve performance. Extensive evaluations show that APNN-TC can\nachieve significant speedup over CUTLASS kernels and various NN models, such as\nResNet and VGG.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 05:39:34 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Feng", "Boyuan", ""], ["Wang", "Yuke", ""], ["Geng", "Tong", ""], ["Li", "Ang", ""], ["Ding", "Yufei", ""]]}, {"id": "2106.12175", "submitter": "Junshen Xu", "authors": "Junshen Xu, Elfar Adalsteinsson", "title": "Deformed2Self: Self-Supervised Denoising for Dynamic Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is of great importance for medical imaging system, since it\ncan improve image quality for disease diagnosis and downstream image analyses.\nIn a variety of applications, dynamic imaging techniques are utilized to\ncapture the time-varying features of the subject, where multiple images are\nacquired for the same subject at different time points. Although\nsignal-to-noise ratio of each time frame is usually limited by the short\nacquisition time, the correlation among different time frames can be exploited\nto improve denoising results with shared information across time frames. With\nthe success of neural networks in computer vision, supervised deep learning\nmethods show prominent performance in single-image denoising, which rely on\nlarge datasets with clean-vs-noisy image pairs. Recently, several\nself-supervised deep denoising models have been proposed, achieving promising\nresults without needing the pairwise ground truth of clean images. In the field\nof multi-image denoising, however, very few works have been done on extracting\ncorrelated information from multiple slices for denoising using self-supervised\ndeep learning methods. In this work, we propose Deformed2Self, an end-to-end\nself-supervised deep learning framework for dynamic imaging denoising. It\ncombines single-image and multi-image denoising to improve image quality and\nuse a spatial transformer network to model motion between different slices.\nFurther, it only requires a single noisy image with a few auxiliary\nobservations at different time frames for training and inference. Evaluations\non phantom and in vivo data with different noise statistics show that our\nmethod has comparable performance to other state-of-the-art unsupervised or\nself-supervised denoising methods and outperforms under high noise levels.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 05:50:19 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Xu", "Junshen", ""], ["Adalsteinsson", "Elfar", ""]]}, {"id": "2106.12181", "submitter": "Aniket Shirke", "authors": "Aniket Shirke, Rebecca Golden, Mrinal Gautam, Angela Green-Miller,\n  Matthew Caesar, Ryan N. Dilger", "title": "Vision-based Behavioral Recognition of Novelty Preference in Pigs", "comments": "5 pages, 7 figures, Accepted at the CVPR 2021 CV4Animals workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioral scoring of research data is crucial for extracting domain-specific\nmetrics but is bottlenecked on the ability to analyze enormous volumes of\ninformation using human labor. Deep learning is widely viewed as a key\nadvancement to relieve this bottleneck. We identify one such domain, where deep\nlearning can be leveraged to alleviate the process of manual scoring. Novelty\npreference paradigms have been widely used to study recognition memory in pigs,\nbut analysis of these videos requires human intervention. We introduce a subset\nof such videos in the form of the 'Pig Novelty Preference Behavior' (PNPB)\ndataset that is fully annotated with pig actions and keypoints. In order to\ndemonstrate the application of state-of-the-art action recognition models on\nthis dataset, we compare LRCN, C3D, and TSM on the basis of various analytical\nmetrics and discuss common pitfalls of the models. Our methods achieve an\naccuracy of 93% and a mean Average Precision of 96% in estimating piglet\nbehavior.\n  We open-source our code and annotated dataset at\nhttps://github.com/AIFARMS/NOR-behavior-recognition\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 06:10:34 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shirke", "Aniket", ""], ["Golden", "Rebecca", ""], ["Gautam", "Mrinal", ""], ["Green-Miller", "Angela", ""], ["Caesar", "Matthew", ""], ["Dilger", "Ryan N.", ""]]}, {"id": "2106.12182", "submitter": "Ajil Jalal", "authors": "Ajil Jalal and Sushrut Karmalkar and Jessica Hoffmann and Alexandros\n  G. Dimakis and Eric Price", "title": "Fairness for Image Generation with Uncertain Sensitive Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work tackles the issue of fairness in the context of generative\nprocedures, such as image super-resolution, which entail different definitions\nfrom the standard classification setting. Moreover, while traditional group\nfairness definitions are typically defined with respect to specified protected\ngroups -- camouflaging the fact that these groupings are artificial and carry\nhistorical and political motivations -- we emphasize that there are no ground\ntruth identities. For instance, should South and East Asians be viewed as a\nsingle group or separate groups? Should we consider one race as a whole or\nfurther split by gender? Choosing which groups are valid and who belongs in\nthem is an impossible dilemma and being \"fair\" with respect to Asians may\nrequire being \"unfair\" with respect to South Asians. This motivates the\nintroduction of definitions that allow algorithms to be \\emph{oblivious} to the\nrelevant groupings.\n  We define several intuitive notions of group fairness and study their\nincompatibilities and trade-offs. We show that the natural extension of\ndemographic parity is strongly dependent on the grouping, and \\emph{impossible}\nto achieve obliviously. On the other hand, the conceptually new definition we\nintroduce, Conditional Proportional Representation, can be achieved obliviously\nthrough Posterior Sampling. Our experiments validate our theoretical results\nand achieve fair image reconstruction using state-of-the-art generative models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 06:17:17 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 10:23:09 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Jalal", "Ajil", ""], ["Karmalkar", "Sushrut", ""], ["Hoffmann", "Jessica", ""], ["Dimakis", "Alexandros G.", ""], ["Price", "Eric", ""]]}, {"id": "2106.12183", "submitter": "Nirmalya Thakur", "authors": "Nirmalya Thakur and Chia Y. Han", "title": "A Review of Assistive Technologies for Activities of Daily Living of\n  Elderly", "comments": null, "journal-ref": "Book chapter in: Assisted Living: Current Issues and Challenges,\n  ISBN: 978-1-53618-446-4, 2020 Nova Science Publishers, Pp. 61 - 84", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the distinct features of this century has been the population of older\nadults which has been on a constant rise. Elderly people have several needs and\nrequirements due to physical disabilities, cognitive issues, weakened memory\nand disorganized behavior, that they face with increasing age. The extent of\nthese limitations also differs according to the varying diversities in elderly,\nwhich include age, gender, background, experience, skills, knowledge and so on.\nThese varying needs and challenges with increasing age, limits abilities of\nolder adults to perform Activities of Daily Living (ADLs) in an independent\nmanner. To add to it, the shortage of caregivers creates a looming need for\ntechnology-based services for elderly people, to assist them in performing\ntheir daily routine tasks to sustain their independent living and active aging.\nTo address these needs, this work consists of making three major contributions\nin this field. First, it provides a rather comprehensive review of assisted\nliving technologies aimed at helping elderly people to perform ADLs. Second,\nthe work discusses the challenges identified through this review, that\ncurrently exist in the context of implementation of assisted living services\nfor elderly care in Smart Homes and Smart Cities. Finally, the work also\noutlines an approach for implementation, extension and integration of the\nexisting works in this field for development of a much-needed framework that\ncan provide personalized assistance and user-centered behavior interventions to\nelderly as per their varying and ever-changing needs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 06:17:49 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Thakur", "Nirmalya", ""], ["Han", "Chia Y.", ""]]}, {"id": "2106.12186", "submitter": "Kaiqi Chen", "authors": "Jialing Liu, Ruyu Liu, Kaiqi Chen, Jianhua Zhang, Dongyan Guo", "title": "Collaborative Visual Inertial SLAM for Multiple Smart Phones", "comments": "6 pages,4 figures,ICRA2021", "journal-ref": null, "doi": null, "report-no": "1052", "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency and accuracy of mapping are crucial in a large scene and\nlong-term AR applications. Multi-agent cooperative SLAM is the precondition of\nmulti-user AR interaction. The cooperation of multiple smart phones has the\npotential to improve efficiency and robustness of task completion and can\ncomplete tasks that a single agent cannot do. However, it depends on robust\ncommunication, efficient location detection, robust mapping, and efficient\ninformation sharing among agents. We propose a multi-intelligence collaborative\nmonocular visual-inertial SLAM deployed on multiple ios mobile devices with a\ncentralized architecture. Each agent can independently explore the environment,\nrun a visual-inertial odometry module online, and then send all the measurement\ninformation to a central server with higher computing resources. The server\nmanages all the information received, detects overlapping areas, merges and\noptimizes the map, and shares information with the agents when needed. We have\nverified the performance of the system in public datasets and real\nenvironments. The accuracy of mapping and fusion of the proposed system is\ncomparable to VINS-Mono which requires higher computing resources.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 06:24:04 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liu", "Jialing", ""], ["Liu", "Ruyu", ""], ["Chen", "Kaiqi", ""], ["Zhang", "Jianhua", ""], ["Guo", "Dongyan", ""]]}, {"id": "2106.12196", "submitter": "Kamil Deja", "authors": "Kamil Deja, Pawe{\\l} Wawrzy\\'nski, Daniel Marczak, Wojciech Masarczyk,\n  Tomasz Trzci\\'nski", "title": "Multiband VAE: Latent Space Partitioning for Knowledge Consolidation in\n  Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for unsupervised continual knowledge consolidation in\ngenerative models that relies on the partitioning of Variational Autoencoder's\nlatent space. Acquiring knowledge about new data samples without forgetting\nprevious ones is a critical problem of continual learning. Currently proposed\nmethods achieve this goal by extending the existing model while constraining\nits behavior not to degrade on the past data, which does not exploit the full\npotential of relations within the entire training dataset. In this work, we\nidentify this limitation and posit the goal of continual learning as a\nknowledge accumulation task. We solve it by continuously re-aligning latent\nspace partitions that we call bands which are representations of samples seen\nin different tasks, driven by the similarity of the information they contain.\nIn addition, we introduce a simple yet effective method for controlled\nforgetting of past data that improves the quality of reconstructions encoded in\nlatent bands and a latent space disentanglement technique that improves\nknowledge consolidation. On top of the standard continual learning evaluation\nbenchmarks, we evaluate our method on a new knowledge consolidation scenario\nand show that the proposed approach outperforms state-of-the-art by up to\ntwofold across all testing scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 06:58:40 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Deja", "Kamil", ""], ["Wawrzy\u0144ski", "Pawe\u0142", ""], ["Marczak", "Daniel", ""], ["Masarczyk", "Wojciech", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "2106.12204", "submitter": "Wentao Du", "authors": "Wentao Du, Zhiyu Xiang, Shuya Chen, Chengyu Qiao, Yiman Chen and\n  Tingming Bai", "title": "Real-time Instance Segmentation with Discriminative Orientation Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although instance segmentation has made considerable advancement over recent\nyears, it's still a challenge to design high accuracy algorithms with real-time\nperformance. In this paper, we propose a real-time instance segmentation\nframework termed OrienMask. Upon the one-stage object detector YOLOv3, a mask\nhead is added to predict some discriminative orientation maps, which are\nexplicitly defined as spatial offset vectors for both foreground and background\npixels. Thanks to the discrimination ability of orientation maps, masks can be\nrecovered without the need for extra foreground segmentation. All instances\nthat match with the same anchor size share a common orientation map. This\nspecial sharing strategy reduces the amortized memory utilization for mask\npredictions but without loss of mask granularity. Given the surviving box\npredictions after NMS, instance masks can be concurrently constructed from the\ncorresponding orientation maps with low complexity. Owing to the concise design\nfor mask representation and its effective integration with the anchor-based\nobject detector, our method is qualified under real-time conditions while\nmaintaining competitive accuracy. Experiments on COCO benchmark show that\nOrienMask achieves 34.8 mask AP at the speed of 42.7 fps evaluated with a\nsingle RTX 2080 Ti. The code is available at https://github.com/duwt/OrienMask.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 07:27:35 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Du", "Wentao", ""], ["Xiang", "Zhiyu", ""], ["Chen", "Shuya", ""], ["Qiao", "Chengyu", ""], ["Chen", "Yiman", ""], ["Bai", "Tingming", ""]]}, {"id": "2106.12212", "submitter": "Edoardo Lanzini", "authors": "Edoardo Lanzini and Sara Beery", "title": "Image-to-Image Translation of Synthetic Samples for Rare Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The natural world is long-tailed: rare classes are observed orders of\nmagnitudes less frequently than common ones, leading to highly-imbalanced data\nwhere rare classes can have only handfuls of examples. Learning from few\nexamples is a known challenge for deep learning based classification\nalgorithms, and is the focus of the field of low-shot learning. One potential\napproach to increase the training data for these rare classes is to augment the\nlimited real data with synthetic samples. This has been shown to help, but the\ndomain shift between real and synthetic hinders the approaches' efficacy when\ntested on real data.\n  We explore the use of image-to-image translation methods to close the domain\ngap between synthetic and real imagery for animal species classification in\ndata collected from camera traps: motion-activated static cameras used to\nmonitor wildlife. We use low-level feature alignment between source and target\ndomains to make synthetic data for a rare species generated using a graphics\nengine more \"realistic\". Compared against a system augmented with unaligned\nsynthetic data, our experiments show a considerable decrease in classification\nerror rates on a rare species.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 07:57:53 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Lanzini", "Edoardo", ""], ["Beery", "Sara", ""]]}, {"id": "2106.12226", "submitter": "Alessandro Sebastianelli", "authors": "Alessandro Sebastianelli, Artur Nowakowski, Erika Puglisi, Maria Pia\n  Del Rosso, Jamila Mifdal, Fiora Pirri, Pierre Philippe Mathieu and Silvia\n  Liberata Ullo", "title": "Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep\n  Hierarchical Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of clouds, located both spatially and temporally, often makes\nremote sensing (RS) applications with optical images difficult or even\nimpossible to perform. Traditional cloud removing techniques have been studied\nfor years, and recently, Machine Learning (ML)-based approaches have also been\nconsidered. In this manuscript, a novel method for the restoration of\nclouds-corrupted optical images is presented, able to generate the whole\noptical scene of interest, not only the cloudy pixels, and based on a Joint\nData Fusion paradigm, where three deep neural networks are hierarchically\ncombined. Spatio-temporal features are separately extracted by a conditional\nGenerative Adversarial Network (cGAN) and by a Convolutional Long Short-Term\nMemory (ConvLSTM), from Synthetic Aperture Radar (SAR) data and optical\ntime-series of data respectively, and then combined with a U-shaped network.\nThe use of time-series of data has been rarely explored in the state of the art\nfor this peculiar objective, and moreover existing models do not combine both\nspatio-temporal domains and SAR-optical imagery. Quantitative and qualitative\nresults have shown a good ability of the proposed method in producing\ncloud-free images, by also preserving the details and outperforming the cGAN\nand the ConvLSTM when individually used. Both the code and the dataset have\nbeen implemented from scratch and made available to interested researchers for\nfurther analysis and investigation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 08:15:01 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 09:35:50 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Sebastianelli", "Alessandro", ""], ["Nowakowski", "Artur", ""], ["Puglisi", "Erika", ""], ["Del Rosso", "Maria Pia", ""], ["Mifdal", "Jamila", ""], ["Pirri", "Fiora", ""], ["Mathieu", "Pierre Philippe", ""], ["Ullo", "Silvia Liberata", ""]]}, {"id": "2106.12252", "submitter": "Malik Boudiaf", "authors": "Malik Boudiaf, Ziko Imtiaz Masud, J\\'er\\^ome Rony, Jose Dolz, Ismail\n  Ben Ayed, Pablo Piantanida", "title": "Mutual-Information Based Few-Shot Classification", "comments": "Journal extension of arXiv:2008.11297. PyTorch implementation of\n  META-DATASET available at https://github.com/mboudiaf/pytorch-meta-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce Transductive Infomation Maximization (TIM) for few-shot\nlearning. Our method maximizes the mutual information between the query\nfeatures and their label predictions for a given few-shot task, in conjunction\nwith a supervision loss based on the support set. We motivate our transductive\nloss by deriving a formal relation between the classification accuracy and\nmutual-information maximization. Furthermore, we propose a new\nalternating-direction solver, which substantially speeds up transductive\ninference over gradient-based optimization, while yielding competitive\naccuracy. We also provide a convergence analysis of our solver based on\nZangwill's theory and bound-optimization arguments. TIM inference is modular:\nit can be used on top of any base-training feature extractor. Following\nstandard transductive few-shot settings, our comprehensive experiments\ndemonstrate that TIM outperforms state-of-the-art methods significantly across\nvarious datasets and networks, while used on top of a fixed feature extractor\ntrained with simple cross-entropy on the base classes, without resorting to\ncomplex meta-learning schemes. It consistently brings between 2 % and 5 %\nimprovement in accuracy over the best performing method, not only on all the\nwell-established few-shot benchmarks but also on more challenging scenarios,\nwith random tasks, domain shift and larger numbers of classes, as in the\nrecently introduced META-DATASET. Our code is publicly available at\nhttps://github.com/mboudiaf/TIM. We also publicly release a standalone PyTorch\nimplementation of META-DATASET, along with additional benchmarking results, at\nhttps://github.com/mboudiaf/pytorch-meta-dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 09:17:23 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Boudiaf", "Malik", ""], ["Masud", "Ziko Imtiaz", ""], ["Rony", "J\u00e9r\u00f4me", ""], ["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""], ["Piantanida", "Pablo", ""]]}, {"id": "2106.12265", "submitter": "Zeyu Gao", "authors": "Zeyu Gao, Bangyang Hong, Xianli Zhang, Yang Li, Chang Jia, Jialun Wu,\n  Chunbao Wang, Deyu Meng, Chen Li", "title": "Instance-based Vision Transformer for Subtyping of Papillary Renal Cell\n  Carcinoma in Histopathological Image", "comments": "Accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Histological subtype of papillary (p) renal cell carcinoma (RCC), type 1 vs.\ntype 2, is an essential prognostic factor. The two subtypes of pRCC have a\nsimilar pattern, i.e., the papillary architecture, yet some subtle differences,\nincluding cellular and cell-layer level patterns. However, the cellular and\ncell-layer level patterns almost cannot be captured by existing CNN-based\nmodels in large-size histopathological images, which brings obstacles to\ndirectly applying these models to such a fine-grained classification task. This\npaper proposes a novel instance-based Vision Transformer (i-ViT) to learn\nrobust representations of histopathological images for the pRCC subtyping task\nby extracting finer features from instance patches (by cropping around\nsegmented nuclei and assigning predicted grades). The proposed i-ViT takes\ntop-K instances as input and aggregates them for capturing both the cellular\nand cell-layer level patterns by a position-embedding layer, a grade-embedding\nlayer, and a multi-head multi-layer self-attention module. To evaluate the\nperformance of the proposed framework, experienced pathologists are invited to\nselected 1162 regions of interest from 171 whole slide images of type 1 and\ntype 2 pRCC. Experimental results show that the proposed method achieves better\nperformance than existing CNN-based models with a significant margin.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 09:42:49 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Gao", "Zeyu", ""], ["Hong", "Bangyang", ""], ["Zhang", "Xianli", ""], ["Li", "Yang", ""], ["Jia", "Chang", ""], ["Wu", "Jialun", ""], ["Wang", "Chunbao", ""], ["Meng", "Deyu", ""], ["Li", "Chen", ""]]}, {"id": "2106.12282", "submitter": "Meysam Madadi", "authors": "Meysam Madadi and Hugo Bertiche and Sergio Escalera", "title": "Deep unsupervised 3D human body reconstruction from a sparse set of\n  landmarks", "comments": null, "journal-ref": "IJCV (2021)", "doi": "10.1007/s11263-021-01488-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the first deep unsupervised approach in human body\nreconstruction to estimate body surface from a sparse set of landmarks, so\ncalled DeepMurf. We apply a denoising autoencoder to estimate missing\nlandmarks. Then we apply an attention model to estimate body joints from\nlandmarks. Finally, a cascading network is applied to regress parameters of a\nstatistical generative model that reconstructs body. Our set of proposed loss\nfunctions allows us to train the network in an unsupervised way. Results on\nfour public datasets show that our approach accurately reconstructs the human\nbody from real world mocap data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:02:58 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Madadi", "Meysam", ""], ["Bertiche", "Hugo", ""], ["Escalera", "Sergio", ""]]}, {"id": "2106.12284", "submitter": "Yanye Lu", "authors": "Mengdi Gao, Ximeng Feng, Mufeng Geng, Zhe Jiang, Lei Zhu, Xiangxi\n  Meng, Chuanqing Zhou, Qiushi Ren and Yanye Lu", "title": "A Label Management Mechanism for Retinal Fundus Image Classification of\n  Diabetic Retinopathy", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diabetic retinopathy (DR) remains the most prevalent cause of vision\nimpairment and irreversible blindness in the working-age adults. Due to the\nrenaissance of deep learning (DL), DL-based DR diagnosis has become a promising\ntool for the early screening and severity grading of DR. However, training deep\nneural networks (DNNs) requires an enormous amount of carefully labeled data.\nNoisy label data may be introduced when labeling plenty of data, degrading the\nperformance of models. In this work, we propose a novel label management\nmechanism (LMM) for the DNN to overcome overfitting on the noisy data. LMM\nutilizes maximum posteriori probability (MAP) in the Bayesian statistic and\ntime-weighted technique to selectively correct the labels of unclean data,\nwhich gradually purify the training data and improve classification\nperformance. Comprehensive experiments on both synthetic noise data (Messidor\n\\& our collected DR dataset) and real-world noise data (ANIMAL-10N)\ndemonstrated that LMM could boost performance of models and is superior to\nthree state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:05:47 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Gao", "Mengdi", ""], ["Feng", "Ximeng", ""], ["Geng", "Mufeng", ""], ["Jiang", "Zhe", ""], ["Zhu", "Lei", ""], ["Meng", "Xiangxi", ""], ["Zhou", "Chuanqing", ""], ["Ren", "Qiushi", ""], ["Lu", "Yanye", ""]]}, {"id": "2106.12300", "submitter": "Fanhua Shang", "authors": "Hua Huang, Fanhua Shang, Yuanyuan Liu, Hongying Liu", "title": "Behavior Mimics Distribution: Combining Individual and Group Behaviors\n  for Federated Learning", "comments": "This paper has been accepted by International Joint Conference on\n  Artificial Intelligence (IJCAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) has become an active and promising distributed\nmachine learning paradigm. As a result of statistical heterogeneity, recent\nstudies clearly show that the performance of popular FL methods (e.g., FedAvg)\ndeteriorates dramatically due to the client drift caused by local updates. This\npaper proposes a novel Federated Learning algorithm (called IGFL), which\nleverages both Individual and Group behaviors to mimic distribution, thereby\nimproving the ability to deal with heterogeneity. Unlike existing FL methods,\nour IGFL can be applied to both client and server optimization. As a\nby-product, we propose a new attention-based federated learning in the server\noptimization of IGFL. To the best of our knowledge, this is the first time to\nincorporate attention mechanisms into federated optimization. We conduct\nextensive experiments and show that IGFL can significantly improve the\nperformance of existing federated learning methods. Especially when the\ndistributions of data among individuals are diverse, IGFL can improve the\nclassification accuracy by about 13% compared with prior baselines.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:42:37 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Huang", "Hua", ""], ["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Liu", "Hongying", ""]]}, {"id": "2106.12302", "submitter": "Stylianos Ploumpis", "authors": "Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Triantafyllou,\n  Stefanos Zafeiriou", "title": "3D human tongue reconstruction from single \"in-the-wild\" images", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D face reconstruction from a single image is a task that has garnered\nincreased interest in the Computer Vision community, especially due to its\nbroad use in a number of applications such as realistic 3D avatar creation,\npose invariant face recognition and face hallucination. Since the introduction\nof the 3D Morphable Model in the late 90's, we witnessed an explosion of\nresearch aiming at particularly tackling this task. Nevertheless, despite the\nincreasing level of detail in the 3D face reconstructions from single images\nmainly attributed to deep learning advances, finer and highly deformable\ncomponents of the face such as the tongue are still absent from all 3D face\nmodels in the literature, although being very important for the realness of the\n3D avatar representations. In this work we present the first, to the best of\nour knowledge, end-to-end trainable pipeline that accurately reconstructs the\n3D face together with the tongue. Moreover, we make this pipeline robust in\n\"in-the-wild\" images by introducing a novel GAN method tailored for 3D tongue\nsurface generation. Finally, we make publicly available to the community the\nfirst diverse tongue dataset, consisting of 1,800 raw scans of 700 individuals\nvarying in gender, age, and ethnicity backgrounds. As we demonstrate in an\nextensive series of quantitative as well as qualitative experiments, our model\nproves to be robust and realistically captures the 3D tongue structure, even in\nadverse \"in-the-wild\" conditions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:49:34 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ploumpis", "Stylianos", ""], ["Moschoglou", "Stylianos", ""], ["Triantafyllou", "Vasileios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2106.12303", "submitter": "Kalun Ho", "authors": "Kalun Ho, Franz-Josef Pfreundt, Janis Keuper, Margret Keuper", "title": "Estimating the Robustness of Classification Models by the Structure of\n  the Learned Feature-Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last decade, the development of deep image classification networks\nhas mostly been driven by the search for the best performance in terms of\nclassification accuracy on standardized benchmarks like ImageNet. More\nrecently, this focus has been expanded by the notion of model robustness, i.e.\nthe generalization abilities of models towards previously unseen changes in the\ndata distribution. While new benchmarks, like ImageNet-C, have been introduced\nto measure robustness properties, we argue that fixed testsets are only able to\ncapture a small portion of possible data variations and are thus limited and\nprone to generate new overfitted solutions. To overcome these drawbacks, we\nsuggest to estimate the robustness of a model directly from the structure of\nits learned feature-space. We introduce robustness indicators which are\nobtained via unsupervised clustering of latent representations inside a trained\nclassifier and show very high correlations to the model performance on\ncorrupted test data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:52:29 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ho", "Kalun", ""], ["Pfreundt", "Franz-Josef", ""], ["Keuper", "Janis", ""], ["Keuper", "Margret", ""]]}, {"id": "2106.12313", "submitter": "ZhongLiang Li", "authors": "Zhongliang Li, Zhihao Jin, Xuechen Li, Linlin Shen", "title": "Learning from Pseudo Lesion: A Self-supervised Framework for COVID-19\n  Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Coronavirus disease 2019 (COVID-19) has rapidly spread all over the world\nsince its first report in December 2019 and thoracic computed tomography (CT)\nhas become one of the main tools for its diagnosis. In recent years, deep\nlearning-based approaches have shown impressive performance in myriad image\nrecognition tasks. However, they usually require a large number of annotated\ndata for training. Inspired by Ground Glass Opacity (GGO), a common finding in\nCOIVD-19 patient's CT scans, we proposed in this paper a novel self-supervised\npretraining method based on pseudo lesions generation and restoration for\nCOVID-19 diagnosis. We used Perlin noise, a gradient noise based mathematical\nmodel, to generate lesion-like patterns, which were then randomly pasted to the\nlung regions of normal CT images to generate pseudo COVID-19 images. The pairs\nof normal and pseudo COVID-19 images were then used to train an encoder-decoder\narchitecture based U-Net for image restoration, which does not require any\nlabelled data. The pretrained encoder was then fine-tuned using labelled data\nfor COVID-19 diagnosis task. Two public COVID-19 diagnosis datasets made up of\nCT images were employed for evaluation. Comprehensive experimental results\ndemonstrated that the proposed self-supervised learning approach could extract\nbetter feature representation for COVID-19 diagnosis and the accuracy of the\nproposed method outperformed the supervised model pretrained on large scale\nimages by 6.57% and 3.03% on SARS-CoV-2 dataset and Jinan COVID-19 dataset,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 11:21:30 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Li", "Zhongliang", ""], ["Jin", "Zhihao", ""], ["Li", "Xuechen", ""], ["Shen", "Linlin", ""]]}, {"id": "2106.12326", "submitter": "Vladislav Sovrasov", "authors": "Ilya Krylov, Sergei Nosov, Vladislav Sovrasov", "title": "Open Images V5 Text Annotation and Yet Another Mask Text Spotter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large scale human-labeled dataset plays an important role in creating high\nquality deep learning models. In this paper we present text annotation for Open\nImages V5 dataset. To our knowledge it is the largest among publicly available\nmanually created text annotations. Having this annotation we trained a simple\nMask-RCNN-based network, referred as Yet Another Mask Text Spotter (YAMTS),\nwhich achieves competitive performance or even outperforms current\nstate-of-the-art approaches in some cases on ICDAR2013, ICDAR2015 and\nTotal-Text datasets. Code for text spotting model available online at:\nhttps://github.com/openvinotoolkit/training_extensions. The model can be\nexported to OpenVINO-format and run on Intel CPUs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 11:46:56 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Krylov", "Ilya", ""], ["Nosov", "Sergei", ""], ["Sovrasov", "Vladislav", ""]]}, {"id": "2106.12362", "submitter": "Erkan Bostanci", "authors": "Talha Dilber, Mehmet Serdar Guzel, Erkan Bostanci", "title": "A new Video Synopsis Based Approach Using Stereo Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's world, the amount of data produced in every field has increased at\nan unexpected level. In the face of increasing data, the importance of data\nprocessing has increased remarkably. Our resource topic is on the processing of\nvideo data, which has an important place in increasing data, and the production\nof summary videos. Within the scope of this resource, a new method for anomaly\ndetection with object-based unsupervised learning has been developed while\ncreating a video summary. By using this method, the video data is processed as\npixels and the result is produced as a video segment. The process flow can be\nbriefly summarized as follows. Objects on the video are detected according to\ntheir type, and then they are tracked. Then, the tracking history data of the\nobjects are processed, and the classifier is trained with the object type.\nThanks to this classifier, anomaly behavior of objects is detected. Video\nsegments are determined by processing video moments containing anomaly\nbehaviors. The video summary is created by extracting the detected video\nsegments from the original video and combining them. The model we developed has\nbeen tested and verified separately for single camera and dual camera systems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 12:57:47 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Dilber", "Talha", ""], ["Guzel", "Mehmet Serdar", ""], ["Bostanci", "Erkan", ""]]}, {"id": "2106.12368", "submitter": "Qibin Hou", "authors": "Qibin Hou, Zihang Jiang, Li Yuan, Ming-Ming Cheng, Shuicheng Yan,\n  Jiashi Feng", "title": "Vision Permutator: A Permutable MLP-Like Architecture for Visual\n  Recognition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present Vision Permutator, a conceptually simple and data\nefficient MLP-like architecture for visual recognition. By realizing the\nimportance of the positional information carried by 2D feature representations,\nunlike recent MLP-like models that encode the spatial information along the\nflattened spatial dimensions, Vision Permutator separately encodes the feature\nrepresentations along the height and width dimensions with linear projections.\nThis allows Vision Permutator to capture long-range dependencies along one\nspatial direction and meanwhile preserve precise positional information along\nthe other direction. The resulting position-sensitive outputs are then\naggregated in a mutually complementing manner to form expressive\nrepresentations of the objects of interest. We show that our Vision Permutators\nare formidable competitors to convolutional neural networks (CNNs) and vision\ntransformers. Without the dependence on spatial convolutions or attention\nmechanisms, Vision Permutator achieves 81.5% top-1 accuracy on ImageNet without\nextra large-scale training data (e.g., ImageNet-22k) using only 25M learnable\nparameters, which is much better than most CNNs and vision transformers under\nthe same model size constraint. When scaling up to 88M, it attains 83.2% top-1\naccuracy. We hope this work could encourage research on rethinking the way of\nencoding spatial information and facilitate the development of MLP-like models.\nCode is available at https://github.com/Andrew-Qibin/VisionPermutator.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:05:23 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Hou", "Qibin", ""], ["Jiang", "Zihang", ""], ["Yuan", "Li", ""], ["Cheng", "Ming-Ming", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "2106.12378", "submitter": "Sucheng Ren", "authors": "Sucheng Ren, Zhengqi Gao, Tianyu Hua, Zihui Xue, Yonglong Tian,\n  Shengfeng He, Hang Zhao", "title": "Co-advise: Cross Inductive Bias Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers recently are adapted from the community of natural language\nprocessing as a promising substitute of convolution-based neural networks for\nvisual learning tasks. However, its supremacy degenerates given an insufficient\namount of training data (e.g., ImageNet). To make it into practical utility, we\npropose a novel distillation-based method to train vision transformers. Unlike\nprevious works, where merely heavy convolution-based teachers are provided, we\nintroduce lightweight teachers with different architectural inductive biases\n(e.g., convolution and involution) to co-advise the student transformer. The\nkey is that teachers with different inductive biases attain different knowledge\ndespite that they are trained on the same dataset, and such different knowledge\ncompounds and boosts the student's performance during distillation. Equipped\nwith this cross inductive bias distillation method, our vision transformers\n(termed as CivT) outperform all previous transformers of the same architecture\non ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:19:59 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ren", "Sucheng", ""], ["Gao", "Zhengqi", ""], ["Hua", "Tianyu", ""], ["Xue", "Zihui", ""], ["Tian", "Yonglong", ""], ["He", "Shengfeng", ""], ["Zhao", "Hang", ""]]}, {"id": "2106.12387", "submitter": "Esther Puyol-Anton Dr", "authors": "Esther Puyol-Anton, Bram Ruijsink, Stefan K. Piechnik, Stefan\n  Neubauer, Steffen E. Petersen, Reza Razavi, and Andrew P. King", "title": "Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to\n  Data Imbalance in Deep Learning Based Segmentation", "comments": "MICCAI 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The subject of \"fairness\" in artificial intelligence (AI) refers to assessing\nAI algorithms for potential bias based on demographic characteristics such as\nrace and gender, and the development of algorithms to address this bias. Most\napplications to date have been in computer vision, although some work in\nhealthcare has started to emerge. The use of deep learning (DL) in cardiac MR\nsegmentation has led to impressive results in recent years, and such techniques\nare starting to be translated into clinical practice. However, no work has yet\ninvestigated the fairness of such models. In this work, we perform such an\nanalysis for racial/gender groups, focusing on the problem of training data\nimbalance, using a nnU-Net model trained and evaluated on cine short axis\ncardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from\n6 different racial groups. We find statistically significant differences in\nDice performance between different racial groups. To reduce the racial bias, we\ninvestigated three strategies: (1) stratified batch sampling, in which batch\nsampling is stratified to ensure balance between racial groups; (2) fair\nmeta-learning for segmentation, in which a DL classifier is trained to classify\nrace and jointly optimized with the segmentation model; and (3) protected group\nmodels, in which a different segmentation model is trained for each racial\ngroup. We also compared the results to the scenario where we have a perfectly\nbalanced database. To assess fairness we used the standard deviation (SD) and\nskewed error ratio (SER) of the average Dice values. Our results demonstrate\nthat the racial bias results from the use of imbalanced training data, and that\nall proposed bias mitigation strategies improved fairness, with the best SD and\nSER resulting from the use of protected group models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:27:35 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 07:56:18 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Puyol-Anton", "Esther", ""], ["Ruijsink", "Bram", ""], ["Piechnik", "Stefan K.", ""], ["Neubauer", "Stefan", ""], ["Petersen", "Steffen E.", ""], ["Razavi", "Reza", ""], ["King", "Andrew P.", ""]]}, {"id": "2106.12407", "submitter": "Junshen Xu", "authors": "Junshen Xu, Esra Abaci Turk, P. Ellen Grant, Polina Golland, Elfar\n  Adalsteinsson", "title": "STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fetal motion is unpredictable and rapid on the scale of conventional MR scan\ntimes. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and\ndynamics of fetal function, is limited to fast imaging techniques with\ncompromises in image quality and resolution. Super-resolution for dynamic fetal\nMRI is still a challenge, especially when multi-oriented stacks of image slices\nfor oversampling are not available and high temporal resolution for recording\nthe dynamics of the fetus or placenta is desired. Further, fetal motion makes\nit difficult to acquire high-resolution images for supervised learning methods.\nTo address this problem, in this work, we propose STRESS (Spatio-Temporal\nResolution Enhancement with Simulated Scans), a self-supervised\nsuper-resolution framework for dynamic fetal MRI with interleaved slice\nacquisitions. Our proposed method simulates an interleaved slice acquisition\nalong the high-resolution axis on the originally acquired data to generate\npairs of low- and high-resolution images. Then, it trains a super-resolution\nnetwork by exploiting both spatial and temporal correlations in the MR time\nseries, which is used to enhance the resolution of the original data.\nEvaluations on both simulated and in utero data show that our proposed method\noutperforms other self-supervised super-resolution methods and improves image\nquality, which is beneficial to other downstream tasks and evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:52:11 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 16:46:00 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Xu", "Junshen", ""], ["Turk", "Esra Abaci", ""], ["Grant", "P. Ellen", ""], ["Golland", "Polina", ""], ["Adalsteinsson", "Elfar", ""]]}, {"id": "2106.12413", "submitter": "Libo Wang", "authors": "Libo Wang, Rui Li, Dongzhi Wang, Chenxi Duan, Teng Wang, Xiaoliang\n  Meng", "title": "Transformer Meets Convolution: A Bilateral Awareness Net-work for\n  Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation from very fine resolution (VFR) urban scene images\nplays a significant role in several application scenarios including autonomous\ndriving, land cover classification, and urban planning, etc. However, the\ntremendous details contained in the VFR image severely limit the potential of\nthe existing deep learning approaches. More seriously, the considerable\nvariations in scale and appearance of objects further deteriorate the\nrepresentational capacity of those se-mantic segmentation methods, leading to\nthe confusion of adjacent objects. Addressing such is-sues represents a\npromising research field in the remote sensing community, which paves the way\nfor scene-level landscape pattern analysis and decision making. In this\nmanuscript, we pro-pose a bilateral awareness network (BANet) which contains a\ndependency path and a texture path to fully capture the long-range\nrelationships and fine-grained details in VFR images. Specif-ically, the\ndependency path is conducted based on the ResT, a novel Transformer backbone\nwith memory-efficient multi-head self-attention, while the texture path is\nbuilt on the stacked convo-lution operation. Besides, using the linear\nattention mechanism, a feature aggregation module (FAM) is designed to\neffectively fuse the dependency features and texture features. Extensive\nexperiments conducted on the three large-scale urban scene image segmentation\ndatasets, i.e., ISPRS Vaihingen dataset, ISPRS Potsdam dataset, and UAVid\ndataset, demonstrate the effective-ness of our BANet. Specifically, a 64.6%\nmIoU is achieved on the UAVid dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:57:36 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Wang", "Libo", ""], ["Li", "Rui", ""], ["Wang", "Dongzhi", ""], ["Duan", "Chenxi", ""], ["Wang", "Teng", ""], ["Meng", "Xiaoliang", ""]]}, {"id": "2106.12423", "submitter": "Samuli Laine", "authors": "Tero Karras, Miika Aittala, Samuli Laine, Erik H\\\"ark\\\"onen, Janne\n  Hellsten, Jaakko Lehtinen, Timo Aila", "title": "Alias-Free Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe that despite their hierarchical convolutional nature, the\nsynthesis process of typical generative adversarial networks depends on\nabsolute pixel coordinates in an unhealthy manner. This manifests itself as,\ne.g., detail appearing to be glued to image coordinates instead of the surfaces\nof depicted objects. We trace the root cause to careless signal processing that\ncauses aliasing in the generator network. Interpreting all signals in the\nnetwork as continuous, we derive generally applicable, small architectural\nchanges that guarantee that unwanted information cannot leak into the\nhierarchical synthesis process. The resulting networks match the FID of\nStyleGAN2 but differ dramatically in their internal representations, and they\nare fully equivariant to translation and rotation even at subpixel scales. Our\nresults pave the way for generative models better suited for video and\nanimation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 14:20:01 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 14:43:18 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Karras", "Tero", ""], ["Aittala", "Miika", ""], ["Laine", "Samuli", ""], ["H\u00e4rk\u00f6nen", "Erik", ""], ["Hellsten", "Janne", ""], ["Lehtinen", "Jaakko", ""], ["Aila", "Timo", ""]]}, {"id": "2106.12442", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Daniel Olmeda Reino, Mario Fritz, Bernt Schiele", "title": "Euro-PVI: Pedestrian Vehicle Interactions in Dense Urban Centers", "comments": "To appear at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of pedestrian and bicyclist paths is integral to the\ndevelopment of reliable autonomous vehicles in dense urban environments. The\ninteractions between vehicle and pedestrian or bicyclist have a significant\nimpact on the trajectories of traffic participants e.g. stopping or turning to\navoid collisions. Although recent datasets and trajectory prediction approaches\nhave fostered the development of autonomous vehicles yet the amount of\nvehicle-pedestrian (bicyclist) interactions modeled are sparse. In this work,\nwe propose Euro-PVI, a dataset of pedestrian and bicyclist trajectories. In\nparticular, our dataset caters more diverse and complex interactions in dense\nurban scenarios compared to the existing datasets. To address the challenges in\npredicting future trajectories with dense interactions, we develop a joint\ninference model that learns an expressive multi-modal shared latent space\nacross agents in the urban scene. This enables our Joint-$\\beta$-cVAE approach\nto better model the distribution of future trajectories. We achieve state of\nthe art results on the nuScenes and Euro-PVI datasets demonstrating the\nimportance of capturing interactions between ego-vehicle and pedestrians\n(bicyclists) for accurate predictions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 15:40:21 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Reino", "Daniel Olmeda", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "2106.12445", "submitter": "Jihye Back", "authors": "Jihye Back", "title": "Fine-Tuning StyleGAN2 For Cartoon Face Generation", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies have shown remarkable success in the unsupervised image to\nimage (I2I) translation. However, due to the imbalance in the data, learning\njoint distribution for various domains is still very challenging. Although\nexisting models can generate realistic target images, it's difficult to\nmaintain the structure of the source image. In addition, training a generative\nmodel on large data in multiple domains requires a lot of time and computer\nresources. To address these limitations, we propose a novel image-to-image\ntranslation method that generates images of the target domain by finetuning a\nstylegan2 pretrained model. The stylegan2 model is suitable for unsupervised\nI2I translation on unbalanced datasets; it is highly stable, produces realistic\nimages, and even learns properly from limited data when applied with simple\nfine-tuning techniques. Thus, in this paper, we propose new methods to preserve\nthe structure of the source images and generate realistic images in the target\ndomain. The code and results are available at\nhttps://github.com/happy-jihye/Cartoon-StyleGan2\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:00:10 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Back", "Jihye", ""]]}, {"id": "2106.12447", "submitter": "Judy Borowski", "authors": "Roland S. Zimmermann, Judy Borowski, Robert Geirhos, Matthias Bethge,\n  Thomas S. A. Wallis, Wieland Brendel", "title": "How Well do Feature Visualizations Support Causal Understanding of CNN\n  Activations?", "comments": "ICML 2021 XAI workshop version. Joint first and last authors. Project\n  website at\n  https://brendel-group.github.io/causal-understanding-via-visualizations/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One widely used approach towards understanding the inner workings of deep\nconvolutional neural networks is to visualize unit responses via activation\nmaximization. Feature visualizations via activation maximization are thought to\nprovide humans with precise information about the image features that cause a\nunit to be activated. If this is indeed true, these synthetic images should\nenable humans to predict the effect of an intervention, such as whether\noccluding a certain patch of the image (say, a dog's head) changes a unit's\nactivation. Here, we test this hypothesis by asking humans to predict which of\ntwo square occlusions causes a larger change to a unit's activation. Both a\nlarge-scale crowdsourced experiment and measurements with experts show that on\naverage, the extremely activating feature visualizations by Olah et al. (2017)\nindeed help humans on this task ($67 \\pm 4\\%$ accuracy; baseline performance\nwithout any visualizations is $60 \\pm 3\\%$). However, they do not provide any\nsignificant advantage over other visualizations (such as e.g. dataset samples),\nwhich yield similar performance ($66 \\pm 3\\%$ to $67 \\pm 3\\%$ accuracy). Taken\ntogether, we propose an objective psychophysical task to quantify the benefit\nof unit-level interpretability methods for humans, and find no evidence that\nfeature visualizations provide humans with better \"causal understanding\" than\nsimple alternative visualizations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 14:52:23 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Zimmermann", "Roland S.", ""], ["Borowski", "Judy", ""], ["Geirhos", "Robert", ""], ["Bethge", "Matthias", ""], ["Wallis", "Thomas S. A.", ""], ["Brendel", "Wieland", ""]]}, {"id": "2106.12449", "submitter": "Dingfu Zhou", "authors": "Shaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Zhou Bin and Liangjun\n  Zhang", "title": "FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object\n  Detection", "comments": "Accepted by ITSC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Accurate detection of obstacles in 3D is an essential task for autonomous\ndriving and intelligent transportation. In this work, we propose a general\nmultimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D\npoint clouds at a semantic level for boosting the 3D object detection task.\nEspecially, the FusionPainting framework consists of three main modules: a\nmulti-modal semantic segmentation module, an adaptive attention-based semantic\nfusion module, and a 3D object detector. First, semantic information is\nobtained for 2D images and 3D Lidar point clouds based on 2D and 3D\nsegmentation approaches. Then the segmentation results from different sensors\nare adaptively fused based on the proposed attention-based semantic fusion\nmodule. Finally, the point clouds painted with the fused semantic label are\nsent to the 3D detector for obtaining the 3D objection results. The\neffectiveness of the proposed framework has been verified on the large-scale\nnuScenes detection benchmark by comparing it with three different baselines.\nThe experimental results show that the fusion strategy can significantly\nimprove the detection performance compared to the methods using only point\nclouds, and the methods using point clouds only painted with 2D segmentation\ninformation. Furthermore, the proposed approach outperforms other\nstate-of-the-art methods on the nuScenes testing benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 14:53:22 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Xu", "Shaoqing", ""], ["Zhou", "Dingfu", ""], ["Fang", "Jin", ""], ["Yin", "Junbo", ""], ["Bin", "Zhou", ""], ["Zhang", "Liangjun", ""]]}, {"id": "2106.12450", "submitter": "Jingyuan Yang", "authors": "Jingyuan Yang, Jie Li, Leida Li, Xiumei Wang, and Xinbo Gao", "title": "A Circular-Structured Representation for Visual Emotion Distribution\n  Learning", "comments": "accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Emotion Analysis (VEA) has attracted increasing attention recently\nwith the prevalence of sharing images on social networks. Since human emotions\nare ambiguous and subjective, it is more reasonable to address VEA in a label\ndistribution learning (LDL) paradigm rather than a single-label classification\ntask. Different from other LDL tasks, there exist intrinsic relationships\nbetween emotions and unique characteristics within them, as demonstrated in\npsychological theories. Inspired by this, we propose a well-grounded\ncircular-structured representation to utilize the prior knowledge for visual\nemotion distribution learning. To be specific, we first construct an Emotion\nCircle to unify any emotional state within it. On the proposed Emotion Circle,\neach emotion distribution is represented with an emotion vector, which is\ndefined with three attributes (i.e., emotion polarity, emotion type, emotion\nintensity) as well as two properties (i.e., similarity, additivity). Besides,\nwe design a novel Progressive Circular (PC) loss to penalize the\ndissimilarities between predicted emotion vector and labeled one in a\ncoarse-to-fine manner, which further boosts the learning process in an\nemotion-specific way. Extensive experiments and comparisons are conducted on\npublic visual emotion distribution datasets, and the results demonstrate that\nthe proposed method outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 14:53:27 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 01:36:35 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yang", "Jingyuan", ""], ["Li", "Jie", ""], ["Li", "Leida", ""], ["Wang", "Xiumei", ""], ["Gao", "Xinbo", ""]]}, {"id": "2106.12489", "submitter": "Sheng Liu", "authors": "Sheng Liu, Xiaozhen Xie, Wenfeng Kong, and Jifeng Ning", "title": "Multi-modal and frequency-weighted tensor nuclear norm for hyperspectral\n  image denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-rankness is important in the hyperspectral image (HSI) denoising tasks.\nThe tensor nuclear norm (TNN), defined based on the tensor singular value\ndecomposition, is a state-of-the-art method to describe the low-rankness of\nHSI. However, TNN ignores some of the physical meanings of HSI in tackling the\ndenoising tasks, leading to suboptimal denoising performance. In this paper, we\npropose the multi-modal and frequency-weighted tensor nuclear norm (MFWTNN) and\nthe non-convex MFWTNN for HSI denoising tasks. Firstly, we investigate the\nphysical meaning of frequency components and reconsider their weights to\nimprove the low-rank representation ability of TNN. Meanwhile, we also consider\nthe correlation among two spatial dimensions and the spectral dimension of HSI\nand combine the above improvements to TNN to propose MFWTNN. Secondly, we use\nnon-convex functions to approximate the rank function of the frequency tensor\nand propose the NonMFWTNN to relax the MFWTNN better. Besides, we adaptively\nchoose bigger weights for slices mainly containing noise information and\nsmaller weights for slices containing profile information. Finally, we develop\nthe efficient alternating direction method of multiplier (ADMM) based algorithm\nto solve the proposed models, and the effectiveness of our models are\nsubstantiated in simulated and real HSI datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:01:08 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liu", "Sheng", ""], ["Xie", "Xiaozhen", ""], ["Kong", "Wenfeng", ""], ["Ning", "Jifeng", ""]]}, {"id": "2106.12497", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Fangxu Xing, Chao Yang, Georges El Fakhri, Jonghye Woo", "title": "Adapting Off-the-Shelf Source Segmenter for Target Medical Image\n  Segmentation", "comments": "To appear in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to an unlabeled and unseen target domain, which is\nusually trained on data from both domains. Access to the source domain data at\nthe adaptation stage, however, is often limited, due to data storage or privacy\nissues. To alleviate this, in this work, we target source free UDA for\nsegmentation, and propose to adapt an ``off-the-shelf\" segmentation model\npre-trained in the source domain to the target domain, with an adaptive\nbatch-wise normalization statistics adaptation framework. Specifically, the\ndomain-specific low-order batch statistics, i.e., mean and variance, are\ngradually adapted with an exponential momentum decay scheme, while the\nconsistency of domain shareable high-order batch statistics, i.e., scaling and\nshifting parameters, is explicitly enforced by our optimization objective. The\ntransferability of each channel is adaptively measured first from which to\nbalance the contribution of each channel. Moreover, the proposed source free\nUDA framework is orthogonal to unsupervised learning methods, e.g.,\nself-entropy minimization, which can thus be simply added on top of our\nframework. Extensive experiments on the BraTS 2018 database show that our\nsource free UDA framework outperformed existing source-relaxed UDA methods for\nthe cross-subtype UDA segmentation task and yielded comparable results for the\ncross-modality UDA segmentation task, compared with a supervised UDA methods\nwith the source data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:16:55 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Xing", "Fangxu", ""], ["Yang", "Chao", ""], ["Fakhri", "Georges El", ""], ["Woo", "Jonghye", ""]]}, {"id": "2106.12499", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Fangxu Xing, Maureen Stone, Jiachen Zhuo, Reese Timothy,\n  Jerry L. Prince, Georges El Fakhri, Jonghye Woo", "title": "Generative Self-training for Cross-domain Unsupervised Tagged-to-Cine\n  MRI Synthesis", "comments": "MICCAI 2021 (early accept <13%)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-training based unsupervised domain adaptation (UDA) has shown great\npotential to address the problem of domain shift, when applying a trained deep\nlearning model in a source domain to unlabeled target domains. However, while\nthe self-training UDA has demonstrated its effectiveness on discriminative\ntasks, such as classification and segmentation, via the reliable pseudo-label\nselection based on the softmax discrete histogram, the self-training UDA for\ngenerative tasks, such as image synthesis, is not fully investigated. In this\nwork, we propose a novel generative self-training (GST) UDA framework with\ncontinuous value prediction and regression objective for cross-domain image\nsynthesis. Specifically, we propose to filter the pseudo-label with an\nuncertainty mask, and quantify the predictive confidence of generated images\nwith practical variational Bayes learning. The fast test-time adaptation is\nachieved by a round-based alternative optimization scheme. We validated our\nframework on the tagged-to-cine magnetic resonance imaging (MRI) synthesis\nproblem, where datasets in the source and target domains were acquired from\ndifferent scanners or centers. Extensive validations were carried out to verify\nour framework against popular adversarial training UDA methods. Results show\nthat our GST, with tagged MRI of test subjects in new target domains, improved\nthe synthesis quality by a large margin, compared with the adversarial training\nUDA methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:19:00 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Xing", "Fangxu", ""], ["Stone", "Maureen", ""], ["Zhuo", "Jiachen", ""], ["Timothy", "Reese", ""], ["Prince", "Jerry L.", ""], ["Fakhri", "Georges El", ""], ["Woo", "Jonghye", ""]]}, {"id": "2106.12511", "submitter": "David Ouyang", "authors": "Grant Duffy, Paul P Cheng, Neal Yuan, Bryan He, Alan C. Kwan, Matthew\n  J. Shun-Shin, Kevin M. Alexander, Joseph Ebinger, Matthew P. Lungren, Florian\n  Rader, David H. Liang, Ingela Schnittger, Euan A. Ashley, James Y. Zou,\n  Jignesh Patel, Ronald Witteles, Susan Cheng, David Ouyang", "title": "High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy\n  with Cardiovascular Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Left ventricular hypertrophy (LVH) results from chronic remodeling caused by\na broad range of systemic and cardiovascular disease including hypertension,\naortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early\ndetection and characterization of LVH can significantly impact patient care but\nis limited by under-recognition of hypertrophy, measurement error and\nvariability, and difficulty differentiating etiologies of LVH. To overcome this\nchallenge, we present EchoNet-LVH - a deep learning workflow that automatically\nquantifies ventricular hypertrophy with precision equal to human experts and\npredicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model\naccurately measures intraventricular wall thickness (mean absolute error [MAE]\n1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI\n2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and\nclassifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic\ncardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets\nfrom independent domestic and international healthcare systems, EchoNet-LVH\naccurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively)\nand detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy\n(AUC 0.89) on the domestic external validation site. Leveraging measurements\nacross multiple heart beats, our model can more accurately identify subtle\nchanges in LV geometry and its causal etiologies. Compared to human experts,\nEchoNet-LVH is fully automated, allowing for reproducible, precise\nmeasurements, and lays the foundation for precision diagnosis of cardiac\nhypertrophy. As a resource to promote further innovation, we also make publicly\navailable a large dataset of 23,212 annotated echocardiogram videos.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:28:40 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Duffy", "Grant", ""], ["Cheng", "Paul P", ""], ["Yuan", "Neal", ""], ["He", "Bryan", ""], ["Kwan", "Alan C.", ""], ["Shun-Shin", "Matthew J.", ""], ["Alexander", "Kevin M.", ""], ["Ebinger", "Joseph", ""], ["Lungren", "Matthew P.", ""], ["Rader", "Florian", ""], ["Liang", "David H.", ""], ["Schnittger", "Ingela", ""], ["Ashley", "Euan A.", ""], ["Zou", "James Y.", ""], ["Patel", "Jignesh", ""], ["Witteles", "Ronald", ""], ["Cheng", "Susan", ""], ["Ouyang", "David", ""]]}, {"id": "2106.12522", "submitter": "Saad Nadeem", "authors": "Shawn Mathew, Saad Nadeem, Arie Kaufman", "title": "FoldIt: Haustral Folds Detection and Segmentation in Colonoscopy Videos", "comments": "MICCAI 2021 (Early Accept), *Saad Nadeem and Shawn Mathew contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haustral folds are colon wall protrusions implicated for high polyp miss rate\nduring optical colonoscopy procedures. If segmented accurately, haustral folds\ncan allow for better estimation of missed surface and can also serve as\nvaluable landmarks for registering pre-treatment virtual (CT) and optical\ncolonoscopies, to guide navigation towards the anomalies found in pre-treatment\nscans. We present a novel generative adversarial network, FoldIt, for\nfeature-consistent image translation of optical colonoscopy videos to virtual\ncolonoscopy renderings with haustral fold overlays. A new transitive loss is\nintroduced in order to leverage ground truth information between haustral fold\nannotations and virtual colonoscopy renderings. We demonstrate the\neffectiveness of our model on real challenging optical colonoscopy videos as\nwell as on textured virtual colonoscopy videos with clinician-verified haustral\nfold annotations. All code and scripts to reproduce the experiments of this\npaper will be made available via our Computational Endoscopy Platform at\nhttps://github.com/nadeemlab/CEP.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:41:10 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Mathew", "Shawn", ""], ["Nadeem", "Saad", ""], ["Kaufman", "Arie", ""]]}, {"id": "2106.12534", "submitter": "Stephen James", "authors": "Stephen James, Kentaro Wada, Tristan Laidlow, Andrew J. Davison", "title": "Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic\n  Manipulation via Discretisation", "comments": "Videos and code found at\n  https://sites.google.com/view/c2f-q-attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflecting on the last few years, the biggest breakthroughs in deep\nreinforcement learning (RL) have been in the discrete action domain. Robotic\nmanipulation, however, is inherently a continuous control environment, but\nthese continuous control reinforcement learning algorithms often depend on\nactor-critic methods that are sample-inefficient and inherently difficult to\ntrain, due to the joint optimisation of the actor and critic. To that end, we\nexplore how we can bring the stability of discrete action RL algorithms to the\nrobot manipulation domain. We extend the recently released ARM algorithm, by\nreplacing the continuous next-best pose agent with a discrete next-best pose\nagent. Discretisation of rotation is trivial given its bounded nature, while\ntranslation is inherently unbounded, making discretisation difficult. We\nformulate the translation prediction as the voxel prediction problem by\ndiscretising the 3D space; however, voxelisation of a large workspace is memory\nintensive and would not work with a high density of voxels, crucial to\nobtaining the resolution needed for robotic manipulation. We therefore propose\nto apply this voxel prediction in a coarse-to-fine manner by gradually\nincreasing the resolution. In each step, we extract the highest valued voxel as\nthe predicted location, which is then used as the centre of the\nhigher-resolution voxelisation in the next step. This coarse-to-fine prediction\nis applied over several steps, giving a near-lossless prediction of the\ntranslation. We show that our new coarse-to-fine algorithm is able to\naccomplish RLBench tasks much more efficiently than the continuous control\nequivalent, and even train some real-world tasks, tabular rasa, in less than 7\nminutes, with only 3 demonstrations. Moreover, we show that by moving to a\nvoxel representation, we are able to easily incorporate observations from\nmultiple cameras.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:57:16 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["James", "Stephen", ""], ["Wada", "Kentaro", ""], ["Laidlow", "Tristan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2106.12545", "submitter": "Mouhammd Alkasassbeh Dr.", "authors": "Israa Odeh, Mouhammd Alkasassbeh, Mohammad Alauthman", "title": "Diabetic Retinopathy Detection using Ensemble Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diabetic Retinopathy (DR) is among the worlds leading vision loss causes in\ndiabetic patients. DR is a microvascular disease that affects the eye retina,\nwhich causes vessel blockage and therefore cuts the main source of nutrition\nfor the retina tissues. Treatment for this visual disorder is most effective\nwhen it is detected in its earliest stages, as severe DR can result in\nirreversible blindness. Nonetheless, DR identification requires the expertise\nof Ophthalmologists which is often expensive and time-consuming. Therefore,\nautomatic detection systems were introduced aiming to facilitate the\nidentification process, making it available globally in a time and\ncost-efficient manner. However, due to the limited reliable datasets and\nmedical records for this particular eye disease, the obtained predictions\naccuracies were relatively unsatisfying for eye specialists to rely on them as\ndiagnostic systems. Thus, we explored an ensemble-based learning strategy,\nmerging a substantial selection of well-known classification algorithms in one\nsophisticated diagnostic model. The proposed framework achieved the highest\naccuracy rates among all other common classification algorithms in the area. 4\nsubdatasets were generated to contain the top 5 and top 10 features of the\nMessidor dataset, selected by InfoGainEval. and WrapperSubsetEval., accuracies\nof 70.7% and 75.1% were achieved on the InfoGainEval. top 5 and original\ndataset respectively. The results imply the impressive performance of the\nsubdataset, which significantly conduces to a less complex classification\nprocess\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:36:08 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Odeh", "Israa", ""], ["Alkasassbeh", "Mouhammd", ""], ["Alauthman", "Mohammad", ""]]}, {"id": "2106.12548", "submitter": "Sai Sukruth Bezugam", "authors": "Sai Sukruth Bezugam", "title": "Multi-Class Classification of Blood Cells -- End to End Computer Vision\n  based diagnosis case study", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.CB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diagnosis of blood-based diseases often involves identifying and\ncharacterizing patient blood samples. Automated methods to detect and classify\nblood cell subtypes have important medical applications. Automated medical\nimage processing and analysis offers a powerful tool for medical diagnosis. In\nthis work we tackle the problem of white blood cell classification based on the\nmorphological characteristics of their outer contour, color. The work we would\nexplore a set of preprocessing and segmentation (Color-based segmentation,\nMorphological processing, contouring) algorithms along with a set of features\nextraction methods (Corner detection algorithms and Histogram of\nGradients(HOG)), dimensionality reduction algorithms (Principal Component\nAnalysis(PCA)) that are able to recognize and classify through various\nUnsupervised(k-nearest neighbors) and Supervised (Support Vector Machine,\nDecision Trees, Linear Discriminant Analysis, Quadratic Discriminant Analysis,\nNaive Bayes) algorithms different categories of white blood cells to\nEosinophil, Lymphocyte, Monocyte, and Neutrophil. We even take a step forwards\nto explore various Deep Convolutional Neural network architecture (Sqeezent,\nMobilenetV1,MobilenetV2, InceptionNet etc.) without preprocessing/segmentation\nand with preprocessing. We would like to explore many algorithms to identify\nthe robust algorithm with least time complexity and low resource requirement.\nThe outcome of this work can be a cue to selection of algorithms as per\nrequirement for automated blood cell classification.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 17:18:19 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Bezugam", "Sai Sukruth", ""]]}, {"id": "2106.12562", "submitter": "Tiago de Souza Farias", "authors": "Tiago de Souza Farias and Jonas Maziero", "title": "Feature Alignment for Approximated Reversibility in Neural Networks", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce feature alignment, a technique for obtaining approximate\nreversibility in artificial neural networks. By means of feature extraction, we\ncan train a neural network to learn an estimated map for its reverse process\nfrom outputs to inputs. Combined with variational autoencoders, we can generate\nnew samples from the same statistics as the training data. Improvements of the\nresults are obtained by using concepts from generative adversarial networks.\nFinally, we show that the technique can be modified for training neural\nnetworks locally, saving computational memory resources. Applying these\ntechniques, we report results for three vision generative tasks: MNIST,\nCIFAR-10, and celebA.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 17:42:47 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Farias", "Tiago de Souza", ""], ["Maziero", "Jonas", ""]]}, {"id": "2106.12569", "submitter": "Amy Widdicombe", "authors": "Amy Widdicombe, Simon J. Julier", "title": "Gradient-Based Interpretability Methods and Binarized Neural Networks", "comments": "Accepted at the ICML 2021 Workshop on Theoretic Foundation, Criticism\n  & Application Trend of Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarized Neural Networks (BNNs) have the potential to revolutionize the way\nthat deep learning is carried out in edge computing platforms. However, the\neffectiveness of interpretability methods on these networks has not been\nassessed.\n  In this paper, we compare the performance of several widely used saliency\nmap-based interpretabilty techniques (Gradient, SmoothGrad and GradCAM), when\napplied to Binarized or Full Precision Neural Networks (FPNNs). We found that\nthe basic Gradient method produces very similar-looking maps for both types of\nnetwork. However, SmoothGrad produces significantly noisier maps for BNNs.\nGradCAM also produces saliency maps which differ between network types, with\nsome of the BNNs having seemingly nonsensical explanations. We comment on\npossible reasons for these differences in explanations and present it as an\nexample of why interpretability techniques should be tested on a wider range of\nnetwork types.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 17:53:18 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Widdicombe", "Amy", ""], ["Julier", "Simon J.", ""]]}, {"id": "2106.12570", "submitter": "Thomas Joy", "authors": "Tom Joy, Yuge Shi, Philip H.S. Torr, Tom Rainforth, Sebastian M.\n  Schmon, N. Siddharth", "title": "Learning Multimodal VAEs through Mutual Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal VAEs seek to model the joint distribution over heterogeneous data\n(e.g.\\ vision, language), whilst also capturing a shared representation across\nsuch modalities. Prior work has typically combined information from the\nmodalities by reconciling idiosyncratic representations directly in the\nrecognition model through explicit products, mixtures, or other such\nfactorisations. Here we introduce a novel alternative, the MEME, that avoids\nsuch explicit combinations by repurposing semi-supervised VAEs to combine\ninformation between modalities implicitly through mutual supervision. This\nformulation naturally allows learning from partially-observed data where some\nmodalities can be entirely missing -- something that most existing approaches\neither cannot handle, or do so to a limited extent. We demonstrate that MEME\noutperforms baselines on standard metrics across both partial and complete\nobservation schemes on the MNIST-SVHN (image-image) and CUB (image-text)\ndatasets. We also contrast the quality of the representations learnt by mutual\nsupervision against standard approaches and observe interesting trends in its\nability to capture relatedness between data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 17:54:35 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 11:15:52 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Joy", "Tom", ""], ["Shi", "Yuge", ""], ["Torr", "Philip H. S.", ""], ["Rainforth", "Tom", ""], ["Schmon", "Sebastian M.", ""], ["Siddharth", "N.", ""]]}, {"id": "2106.12605", "submitter": "Samay Pashine", "authors": "Samay Pashine, Sagar Mandiya, Praveen Gupta, and Rashid Sheikh", "title": "Deep Fake Detection: Survey of Facial Manipulation Detection Solutions", "comments": "7 Pages, 14 figures, and 1 table", "journal-ref": "International Research Journal of Engineering and Technology\n  Volume 8, Issue 5, May 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning as a field has been successfully used to solve a plethora of\ncomplex problems, the likes of which we could not have imagined a few decades\nback. But as many benefits as it brings, there are still ways in which it can\nbe used to bring harm to our society. Deep fakes have been proven to be one\nsuch problem, and now more than ever, when any individual can create a fake\nimage or video simply using an application on the smartphone, there need to be\nsome countermeasures, with which we can detect if the image or video is a fake\nor real and dispose of the problem threatening the trustworthiness of online\ninformation. Although the Deep fakes created by neural networks, may seem to be\nas real as a real image or video, it still leaves behind spatial and temporal\ntraces or signatures after moderation, these signatures while being invisible\nto a human eye can be detected with the help of a neural network trained to\nspecialize in Deep fake detection. In this paper, we analyze several such\nstates of the art neural networks (MesoNet, ResNet-50, VGG-19, and Xception\nNet) and compare them against each other, to find an optimal solution for\nvarious scenarios like real-time deep fake detection to be deployed in online\nsocial media platforms where the classification should be made as fast as\npossible or for a small news agency where the classification need not be in\nreal-time but requires utmost accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 18:08:07 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Pashine", "Samay", ""], ["Mandiya", "Sagar", ""], ["Gupta", "Praveen", ""], ["Sheikh", "Rashid", ""]]}, {"id": "2106.12614", "submitter": "Samay Pashine", "authors": "Samay Pashine, Ritik Dixit, and Rishika Kushwah", "title": "Handwritten Digit Recognition using Machine and Deep Learning Algorithms", "comments": "6 Pages, 13 figures, and 1 table", "journal-ref": "International Journal of Computer Applications, Volume 176 -\n  Number 42, 2020", "doi": "10.5120/ijca2020920550", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The reliance of humans over machines has never been so high such that from\nobject classification in photographs to adding sound to silent movies\neverything can be performed with the help of deep learning and machine learning\nalgorithms. Likewise, Handwritten text recognition is one of the significant\nareas of research and development with a streaming number of possibilities that\ncould be attained. Handwriting recognition (HWR), also known as Handwritten\nText Recognition (HTR), is the ability of a computer to receive and interpret\nintelligible handwritten input from sources such as paper documents,\nphotographs, touch-screens and other devices [1]. Apparently, in this paper, we\nhave performed handwritten digit recognition with the help of MNIST datasets\nusing Support Vector Machines (SVM), Multi-Layer Perceptron (MLP) and\nConvolution Neural Network (CNN) models. Our main objective is to compare the\naccuracy of the models stated above along with their execution time to get the\nbest possible model for digit recognition.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 18:23:01 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Pashine", "Samay", ""], ["Dixit", "Ritik", ""], ["Kushwah", "Rishika", ""]]}, {"id": "2106.12620", "submitter": "Bowen Pan", "authors": "Bowen Pan, Yifan Jiang, Rameswar Panda, Zhangyang Wang, Rogerio Feris,\n  Aude Oliva", "title": "IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision\n  Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-attention-based model, transformer, is recently becoming the leading\nbackbone in the field of computer vision. In spite of the impressive success\nmade by transformers in a variety of vision tasks, it still suffers from heavy\ncomputation and intensive memory cost. To address this limitation, this paper\npresents an Interpretability-Aware REDundancy REDuction framework (IA-RED$^2$).\nWe start by observing a large amount of redundant computation, mainly spent on\nuncorrelated input patches, and then introduce an interpretable module to\ndynamically and gracefully drop these redundant patches. This novel framework\nis then extended to a hierarchical structure, where uncorrelated tokens at\ndifferent stages are gradually removed, resulting in a considerable shrinkage\nof computational cost. We include extensive experiments on both image and video\ntasks, where our method could deliver up to 1.4X speed-up for state-of-the-art\nmodels like DeiT and TimeSformer, by only sacrificing less than 0.7% accuracy.\nMore importantly, contrary to other acceleration approaches, our method is\ninherently interpretable with substantial visual evidence, making vision\ntransformer closer to a more human-understandable architecture while being\nlighter. We demonstrate that the interpretability that naturally emerged in our\nframework can outperform the raw attention learned by the original visual\ntransformer, as well as those generated by off-the-shelf interpretation\nmethods, with both qualitative and quantitative results. Project Page:\nhttp://people.csail.mit.edu/bpan/ia-red/.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 18:29:23 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Pan", "Bowen", ""], ["Jiang", "Yifan", ""], ["Panda", "Rameswar", ""], ["Wang", "Zhangyang", ""], ["Feris", "Rogerio", ""], ["Oliva", "Aude", ""]]}, {"id": "2106.12628", "submitter": "Jyoti Kini", "authors": "Crystal Gagne, Jyoti Kini, Daniel Smith, Mubarak Shah", "title": "Florida Wildlife Camera Trap Dataset", "comments": "IEEE Conference on Computer Vision and Pattern Recognition,\n  CV4Animals: Computer Vision for Animal Behavior Tracking and Modeling\n  Workshop, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Trail camera imagery has increasingly gained popularity amongst biologists\nfor conservation and ecological research. Minimal human interference required\nto operate camera traps allows capturing unbiased species activities. Several\nstudies - based on human and wildlife interactions, migratory patterns of\nvarious species, risk of extinction in endangered populations - are limited by\nthe lack of rich data and the time-consuming nature of manually annotating\ntrail camera imagery. We introduce a challenging wildlife camera trap\nclassification dataset collected from two different locations in Southwestern\nFlorida, consisting of 104,495 images featuring visually similar species,\nvarying illumination conditions, skewed class distribution, and including\nsamples of endangered species, i.e. Florida panthers. Experimental evaluations\nwith ResNet-50 architecture indicate that this image classification-based\ndataset can further push the advancements in wildlife statistical modeling. We\nwill make the dataset publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 18:53:15 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Gagne", "Crystal", ""], ["Kini", "Jyoti", ""], ["Smith", "Daniel", ""], ["Shah", "Mubarak", ""]]}, {"id": "2106.12666", "submitter": "Anna Nedorubova", "authors": "Anna Nedorubova, Alena Kadyrova, Aleksey Khlyupin", "title": "Human Activity Recognition using Continuous Wavelet Transform and\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quite a few people in the world have to stay under permanent surveillance for\nhealth reasons; they include diabetic people or people with some other chronic\nconditions, the elderly and the disabled.These groups may face heightened risk\nof having life-threatening falls or of being struck by a syncope. Due to\nlimited availability of resources a substantial part of people at risk can not\nreceive necessary monitoring and thus are exposed to excessive danger.\nNowadays, this problem is usually solved via applying Human Activity\nRecognition (HAR) methods. HAR is a perspective and fast-paced Data Science\nfield, which has a wide range of application areas such as healthcare, sport,\nsecurity etc. However, the currently techniques of recognition are markedly\nlacking in accuracy, hence, the present paper suggests a highly accurate method\nfor human activity classification. Wepropose a new workflow to address the HAR\nproblem and evaluate it on the UniMiB SHAR dataset, which consists of the\naccelerometer signals. The model we suggest is based on continuous wavelet\ntransform (CWT) and convolutional neural networks (CNNs). Wavelet transform\nlocalizes signal features both in time and frequency domains and after that a\nCNN extracts these features and recognizes activity. It is also worth noting\nthat CWT converts 1D accelerometer signal into 2D images and thus enables to\nobtain better results as 2D networks have a significantly higher predictive\ncapacity. In the course of the work we build a convolutional neural network and\nvary such model parameters as number of spatial axes, number of layers, number\nof neurons in each layer, image size, type of mother wavelet, the order of zero\nmoment of mother wavelet etc. Besides, we also apply models with residual\nblocks which resulted in significantly higher metric values. Finally, we\nsucceed to reach 99.26 % accuracy and it is a worthy performance for this\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 21:49:17 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 17:55:52 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Nedorubova", "Anna", ""], ["Kadyrova", "Alena", ""], ["Khlyupin", "Aleksey", ""]]}, {"id": "2106.12671", "submitter": "Stefan Schubert", "authors": "Stefan Schubert and Peer Neubert", "title": "What makes visual place recognition easy or hard?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition is a fundamental capability for the localization of\nmobile robots. It places image retrieval in the practical context of physical\nagents operating in a physical world. It is an active field of research and\nmany different approaches have been proposed and evaluated in many different\nexperiments. In the following, we argue that due to variations of this\npractical context and individual design decisions, place recognition\nexperiments are barely comparable across different papers and that there is a\nvariety of properties that can change from one experiment to another. We\nprovide an extensive list of such properties and give examples how they can be\nused to setup a place recognition experiment easier or harder. This might be\ninteresting for different involved parties: (1) people who just want to select\na place recognition approach that is suitable for the properties of their\nparticular task at hand, (2) researchers that look for open research questions\nand are interested in particularly difficult instances, (3) authors that want\nto create reproducible papers on this topic, and (4) also reviewers that have\nthe task to identify potential problems in papers under review.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 22:15:22 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Schubert", "Stefan", ""], ["Neubert", "Peer", ""]]}, {"id": "2106.12673", "submitter": "Tony C. W. Mok", "authors": "Tony C. W. Mok and Albert C. S. Chung", "title": "Conditional Deformable Image Registration with Convolutional Neural\n  Network", "comments": "Early accepted by MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning-based methods have shown promising results and runtime\nadvantages in deformable image registration. However, analyzing the effects of\nhyperparameters and searching for optimal regularization parameters prove to be\ntoo prohibitive in deep learning-based methods. This is because it involves\ntraining a substantial number of separate models with distinct hyperparameter\nvalues. In this paper, we propose a conditional image registration method and a\nnew self-supervised learning paradigm for deep deformable image registration.\nBy learning the conditional features that are correlated with the\nregularization hyperparameter, we demonstrate that optimal solutions with\narbitrary hyperparameters can be captured by a single deep convolutional neural\nnetwork. In addition, the smoothness of the resulting deformation field can be\nmanipulated with arbitrary strength of smoothness regularization during\ninference. Extensive experiments on a large-scale brain MRI dataset show that\nour proposed method enables the precise control of the smoothness of the\ndeformation field without sacrificing the runtime advantage or registration\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 22:25:28 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 12:21:32 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Mok", "Tony C. W.", ""], ["Chung", "Albert C. S.", ""]]}, {"id": "2106.12709", "submitter": "Ygor Sousa", "authors": "Ygor C. N. Sousa, Hansenclever F. Bassani", "title": "Topological Semantic Mapping by Consolidation of Deep Visual Features", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many works in the recent literature introduce semantic mapping methods that\nuse CNNs (Convolutional Neural Networks) to recognize semantic properties in\nimages. The types of properties (eg.: room size, place category, and objects)\nand their classes (eg.: kitchen and bathroom, for place category) are usually\npredefined and restricted to a specific task. Thus, all the visual data\nacquired and processed during the construction of the maps are lost and only\nthe recognized semantic properties remain on the maps. In contrast, this work\nintroduces a topological semantic mapping method that uses deep visual features\nextracted by a CNN, the GoogLeNet, from 2D images captured in multiple views of\nthe environment as the robot operates, to create consolidated representations\nof visual features acquired in the regions covered by each topological node.\nThese consolidated representations allow flexible recognition of semantic\nproperties of the regions and use in a range of visual tasks. The experiments,\nperformed using a real-world indoor dataset, showed that the method is able to\nconsolidate the visual features of regions and use them to recognize objects\nand place categories as semantic properties, and to indicate the topological\nlocation of images, with very promising results. The objects are classified\nusing the classification layer of GoogLeNet, without retraining, and the place\ncategories are recognized using a shallow Multilayer Perceptron.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 01:10:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Sousa", "Ygor C. N.", ""], ["Bassani", "Hansenclever F.", ""]]}, {"id": "2106.12720", "submitter": "Meng Cao", "authors": "Meng Cao, Can Zhang, Dongming Yang, Yuexian Zou", "title": "All You Need is a Second Look: Towards Arbitrary-Shaped Text Detection", "comments": "Accepted by T-CSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Arbitrary-shaped text detection is a challenging task since curved texts in\nthe wild are of the complex geometric layouts. Existing mainstream methods\nfollow the instance segmentation pipeline to obtain the text regions. However,\narbitraryshaped texts are difficult to be depicted through one single\nsegmentation network because of the varying scales. In this paper, we propose a\ntwo-stage segmentation-based detector, termed as NASK (Need A Second looK), for\narbitrary-shaped text detection. Compared to the traditional single-stage\nsegmentation network, our NASK conducts the detection in a coarse-to-fine\nmanner with the first stage segmentation spotting the rectangle text proposals\nand the second one retrieving compact representations. Specifically, NASK is\ncomposed of a Text Instance Segmentation (TIS) network (1st stage), a\nGeometry-aware Text RoI Alignment (GeoAlign) module, and a Fiducial pOint\neXpression (FOX) module (2nd stage). Firstly, TIS extracts the augmented\nfeatures with a novel Group Spatial and Channel Attention (GSCA) module and\nconducts instance segmentation to obtain rectangle proposals. Then, GeoAlign\nconverts these rectangles into the fixed size and encodes RoI-wise feature\nrepresentation. Finally, FOX disintegrates the text instance into serval\npivotal geometrical attributes to refine the detection results. Extensive\nexperimental results on three public benchmarks including Total-Text,\nSCUTCTW1500, and ICDAR 2015 verify that our NASK outperforms recent\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 01:44:10 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Cao", "Meng", ""], ["Zhang", "Can", ""], ["Yang", "Dongming", ""], ["Zou", "Yuexian", ""]]}, {"id": "2106.12728", "submitter": "Guanxiong Nie", "authors": "Guanxiong Nie, Yajian Zhou", "title": "ATP-Net: An Attention-based Ternary Projection Network For Compressed\n  Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compressed Sensing (CS) theory simultaneously realizes the signal sampling\nand compression process, and can use fewer observations to achieve accurate\nsignal recovery, providing a solution for better and faster transmission of\nmassive data. In this paper, a ternary sampling matrix-based method with\nattention mechanism is proposed with the purpose to solve the problem that the\nCS sampling matrices in most cases are random matrices, which are irrelative to\nthe sampled signal and need a large storage space. The proposed method consists\nof three components, i.e., ternary sampling, initial reconstruction and deep\nreconstruction, with the emphasis on the ternary sampling. The main idea of the\nternary method (-1, 0, +1) is to introduce the attention mechanism to evaluate\nthe importance of parameters at the sampling layer after the sampling matrix is\nbinarized (-1, +1), followed by pruning weight of parameters, whose importance\nis below a predefined threshold, to achieve ternarization. Furthermore, a\ncompressed sensing algorithm especially for image reconstruction is\nimplemented, on the basis of the ternary sampling matrix, which is called\nATP-Net, i.e., Attention-based ternary projection network. Experimental results\nshow that the quality of image reconstruction by means of ATP-Net maintains a\nsatisfactory level with the employment of the ternary sampling matrix, i.e.,\nthe average PSNR on Set11 is 30.4 when the sampling rate is 0.25, approximately\n6% improvement compared with that of DR2-Net.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 02:22:28 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Nie", "Guanxiong", ""], ["Zhou", "Yajian", ""]]}, {"id": "2106.12733", "submitter": "Ruibing Hou", "authors": "Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan and\n  Xilin Chen", "title": "Feature Completion for Occluded Person Re-Identification", "comments": "18 pages, 17 figures. The paper is accepted by TPAMI, and the code is\n  available at https://github.com/blue-blue272/OccludedReID-RFCnet", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI), 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Person re-identification (reID) plays an important role in computer vision.\nHowever, existing methods suffer from performance degradation in occluded\nscenes. In this work, we propose an occlusion-robust block, Region Feature\nCompletion (RFC), for occluded reID. Different from most previous works that\ndiscard the occluded regions, RFC block can recover the semantics of occluded\nregions in feature space. Firstly, a Spatial RFC (SRFC) module is developed.\nSRFC exploits the long-range spatial contexts from non-occluded regions to\npredict the features of occluded regions. The unit-wise prediction task leads\nto an encoder/decoder architecture, where the region-encoder models the\ncorrelation between non-occluded and occluded region, and the region-decoder\nutilizes the spatial correlation to recover occluded region features. Secondly,\nwe introduce Temporal RFC (TRFC) module which captures the long-term temporal\ncontexts to refine the prediction of SRFC. RFC block is lightweight, end-to-end\ntrainable and can be easily plugged into existing CNNs to form RFCnet.\nExtensive experiments are conducted on occluded and commonly holistic reID\nbenchmarks. Our method significantly outperforms existing methods on the\nocclusion datasets, while remains top even superior performance on holistic\ndatasets. The source code is available at\nhttps://github.com/blue-blue272/OccludedReID-RFCnet.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 02:40:40 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Hou", "Ruibing", ""], ["Ma", "Bingpeng", ""], ["Chang", "Hong", ""], ["Gu", "Xinqian", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2106.12735", "submitter": "Qiuyu Mao", "authors": "Yingjie Wang, Qiuyu Mao, Hanqi Zhu, Yu Zhang, Jianmin Ji, Yanyong\n  Zhang", "title": "Multi-Modal 3D Object Detection in Autonomous Driving: a Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past few years, we have witnessed rapid development of autonomous\ndriving. However, achieving full autonomy remains a daunting task due to the\ncomplex and dynamic driving environment. As a result, self-driving cars are\nequipped with a suite of sensors to conduct robust and accurate environment\nperception. As the number and type of sensors keep increasing, combining them\nfor better perception is becoming a natural trend. So far, there has been no\nindepth review that focuses on multi-sensor fusion based perception. To bridge\nthis gap and motivate future research, this survey devotes to review recent\nfusion-based 3D detection deep learning models that leverage multiple sensor\ndata sources, especially cameras and LiDARs. In this survey, we first introduce\nthe background of popular sensors for autonomous cars, including their common\ndata representations as well as object detection networks developed for each\ntype of sensor data. Next, we discuss some popular datasets for multi-modal 3D\nobject detection, with a special focus on the sensor data included in each\ndataset. Then we present in-depth reviews of recent multi-modal 3D detection\nnetworks by considering the following three aspects of the fusion: fusion\nlocation, fusion data representation, and fusion granularity. After a detailed\nreview, we discuss open challenges and point out possible solutions. We hope\nthat our detailed review can help researchers to embark investigations in the\narea of multi-modal 3D object detection.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 02:52:12 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 15:39:13 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Yingjie", ""], ["Mao", "Qiuyu", ""], ["Zhu", "Hanqi", ""], ["Zhang", "Yu", ""], ["Ji", "Jianmin", ""], ["Zhang", "Yanyong", ""]]}, {"id": "2106.12736", "submitter": "Zhiyuan Chen Dr", "authors": "Ee Fey Goh, ZhiYuan Chen and Wei Xiang Lim", "title": "Frequency Domain Convolutional Neural Network: Accelerated CNN for Large\n  Diabetic Retinopathy Image Classification", "comments": "This paper has been submitted to Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The conventional spatial convolution layers in the Convolutional Neural\nNetworks (CNNs) are computationally expensive at the point where the training\ntime could take days unless the number of layers, the number of training images\nor the size of the training images are reduced. The image size of 256x256\npixels is commonly used for most of the applications of CNN, but this image\nsize is too small for applications like Diabetic Retinopathy (DR)\nclassification where the image details are important for accurate\nclassification. This research proposed Frequency Domain Convolution (FDC) and\nFrequency Domain Pooling (FDP) layers which were built with RFFT, kernel\ninitialization strategy, convolution artifact removal and Channel Independent\nConvolution (CIC) to replace the conventional convolution and pooling layers.\nThe FDC and FDP layers are used to build a Frequency Domain Convolutional\nNeural Network (FDCNN) to accelerate the training of large images for DR\nclassification. The Full FDC layer is an extension of the FDC layer to allow\ndirect use in conventional CNNs, it is also used to modify the VGG16\narchitecture. FDCNN is shown to be at least 54.21% faster and 70.74% more\nmemory efficient compared to an equivalent CNN architecture. The modified VGG16\narchitecture with Full FDC layer is reported to achieve a shorter training time\nand a higher accuracy at 95.63% compared to the original VGG16 architecture for\nDR classification.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 02:52:54 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Goh", "Ee Fey", ""], ["Chen", "ZhiYuan", ""], ["Lim", "Wei Xiang", ""]]}, {"id": "2106.12738", "submitter": "Xue Wan", "authors": "Xue Wan, Yuanbin Shao, Shengyang Li", "title": "Planetary UAV localization based on Multi-modal Registration with\n  Pre-existing Digital Terrain Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The autonomous real-time optical navigation of planetary UAV is of the key\ntechnologies to ensure the success of the exploration. In such a GPS denied\nenvironment, vision-based localization is an optimal approach. In this paper,\nwe proposed a multi-modal registration based SLAM algorithm, which estimates\nthe location of a planet UAV using a nadir view camera on the UAV compared with\npre-existing digital terrain model. To overcome the scale and appearance\ndifference between on-board UAV images and pre-installed digital terrain model,\na theoretical model is proposed to prove that topographic features of UAV image\nand DEM can be correlated in frequency domain via cross power spectrum. To\nprovide the six-DOF of the UAV, we also developed an optimization approach\nwhich fuses the geo-referencing result into a SLAM system via LBA (Local Bundle\nAdjustment) to achieve robust and accurate vision-based navigation even in\nfeatureless planetary areas. To test the robustness and effectiveness of the\nproposed localization algorithm, a new cross-source drone-based localization\ndataset for planetary exploration is proposed. The proposed dataset includes\n40200 synthetic drone images taken from nine planetary scenes with related DEM\nquery images. Comparison experiments carried out demonstrate that over the\nflight distance of 33.8km, the proposed method achieved average localization\nerror of 0.45 meters, compared to 1.31 meters by ORB-SLAM, with the processing\nspeed of 12hz which will ensure a real-time performance. We will make our\ndatasets available to encourage further work on this promising topic.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 02:54:01 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Wan", "Xue", ""], ["Shao", "Yuanbin", ""], ["Li", "Shengyang", ""]]}, {"id": "2106.12746", "submitter": "Shuai Li", "authors": "Jian Yue, Yanbo Gao, Shuai Li, Hui Yuan, Fr\\'ed\\'eric Dufaux", "title": "A Global Appearance and Local Coding Distortion based Fusion Framework\n  for CNN based Filtering in Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-loop filtering is used in video coding to process the reconstructed frame\nin order to remove blocking artifacts. With the development of convolutional\nneural networks (CNNs), CNNs have been explored for in-loop filtering\nconsidering it can be treated as an image de-noising task. However, in addition\nto being a distorted image, the reconstructed frame is also obtained by a fixed\nline of block based encoding operations in video coding. It carries coding-unit\nbased coding distortion of some similar characteristics. Therefore, in this\npaper, we address the filtering problem from two aspects, global appearance\nrestoration for disrupted texture and local coding distortion restoration\ncaused by fixed pipeline of coding. Accordingly, a three-stream global\nappearance and local coding distortion based fusion network is developed with a\nhigh-level global feature stream, a high-level local feature stream and a\nlow-level local feature stream. Ablation study is conducted to validate the\nnecessity of different features, demonstrating that the global features and\nlocal features can complement each other in filtering and achieve better\nperformance when combined. To the best of our knowledge, we are the first one\nthat clearly characterizes the video filtering process from the above global\nappearance and local coding distortion restoration aspects with experimental\nverification, providing a clear pathway to developing filter techniques.\nExperimental results demonstrate that the proposed method significantly\noutperforms the existing single-frame based methods and achieves 13.5%, 11.3%,\n11.7% BD-Rate saving on average for AI, LDP and RA configurations,\nrespectively, compared with the HEVC reference software.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 03:08:44 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Yue", "Jian", ""], ["Gao", "Yanbo", ""], ["Li", "Shuai", ""], ["Yuan", "Hui", ""], ["Dufaux", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2106.12776", "submitter": "Anand Sahadevan S", "authors": "Rosly Boy Lyngdoh, Anand S Sahadevan, Touseef Ahmad, Pradyuman Singh\n  Rathore, Manoj Mishra, Praveen Kumar Gupta and Arundhati Misra", "title": "AVHYAS: A Free and Open Source QGIS Plugin for Advanced Hyperspectral\n  Image Analysis", "comments": "Accepted at IEEE International Conference on Emerging Techniques in\n  Computational Intelligence, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advanced Hyperspectral Data Analysis Software (AVHYAS) plugin is a python3\nbased quantum GIS (QGIS) plugin designed to process and analyse hyperspectral\n(Hx) images. It is developed to guarantee full usage of present and future Hx\nairborne or spaceborne sensors and provides access to advanced algorithms for\nHx data processing. The software is freely available and offers a range of\nbasic and advanced tools such as atmospheric correction (for airborne AVIRISNG\nimage), standard processing tools as well as powerful machine learning and Deep\nLearning interfaces for Hx data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 05:55:15 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lyngdoh", "Rosly Boy", ""], ["Sahadevan", "Anand S", ""], ["Ahmad", "Touseef", ""], ["Rathore", "Pradyuman Singh", ""], ["Mishra", "Manoj", ""], ["Gupta", "Praveen Kumar", ""], ["Misra", "Arundhati", ""]]}, {"id": "2106.12778", "submitter": "Guotao Meng", "authors": "Guotao Meng, Yue Wu, Sijin Li, Qifeng Chen", "title": "Video Super-Resolution with Long-Term Self-Exemplars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing video super-resolution methods often utilize a few neighboring\nframes to generate a higher-resolution image for each frame. However, the\nredundant information between distant frames has not been fully exploited in\nthese methods: corresponding patches of the same instance appear across distant\nframes at different scales. Based on this observation, we propose a video\nsuper-resolution method with long-term cross-scale aggregation that leverages\nsimilar patches (self-exemplars) across distant frames. Our model also consists\nof a multi-reference alignment module to fuse the features derived from similar\npatches: we fuse the features of distant references to perform high-quality\nsuper-resolution. We also propose a novel and practical training strategy for\nreferenced-based super-resolution. To evaluate the performance of our proposed\nmethod, we conduct extensive experiments on our collected CarCam dataset and\nthe Waymo Open dataset, and the results demonstrate our method outperforms\nstate-of-the-art methods. Our source code will be publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 06:07:13 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Meng", "Guotao", ""], ["Wu", "Yue", ""], ["Li", "Sijin", ""], ["Chen", "Qifeng", ""]]}, {"id": "2106.12790", "submitter": "Parul Kapoor", "authors": "Parul Kapoor, Rudrabha Mukhopadhyay, Sindhu B Hegde, Vinay Namboodiri,\n  C V Jawahar", "title": "Towards Automatic Speech to Sign Language Generation", "comments": "5 pages(including references), 5 figures, Accepted in Interspeech\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to solve the highly challenging task of generating continuous sign\nlanguage videos solely from speech segments for the first time. Recent efforts\nin this space have focused on generating such videos from human-annotated text\ntranscripts without considering other modalities. However, replacing speech\nwith sign language proves to be a practical solution while communicating with\npeople suffering from hearing loss. Therefore, we eliminate the need of using\ntext as input and design techniques that work for more natural, continuous,\nfreely uttered speech covering an extensive vocabulary. Since the current\ndatasets are inadequate for generating sign language directly from speech, we\ncollect and release the first Indian sign language dataset comprising\nspeech-level annotations, text transcripts, and the corresponding sign-language\nvideos. Next, we propose a multi-tasking transformer network trained to\ngenerate signer's poses from speech segments. With speech-to-text as an\nauxiliary task and an additional cross-modal discriminator, our model learns to\ngenerate continuous sign pose sequences in an end-to-end manner. Extensive\nexperiments and comparisons with other baselines demonstrate the effectiveness\nof our approach. We also conduct additional ablation studies to analyze the\neffect of different modules of our network. A demo video containing several\nresults is attached to the supplementary material.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 06:44:19 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Kapoor", "Parul", ""], ["Mukhopadhyay", "Rudrabha", ""], ["Hegde", "Sindhu B", ""], ["Namboodiri", "Vinay", ""], ["Jawahar", "C V", ""]]}, {"id": "2106.12802", "submitter": "Qiqi Hou", "authors": "Qiqi Hou, Zhan Li, Carl S Marshall, Selvakumar Panneer, Feng Liu", "title": "Fast Monte Carlo Rendering via Multi-Resolution Sampling", "comments": "Graphic Interface 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monte Carlo rendering algorithms are widely used to produce photorealistic\ncomputer graphics images. However, these algorithms need to sample a\nsubstantial amount of rays per pixel to enable proper global illumination and\nthus require an immense amount of computation. In this paper, we present a\nhybrid rendering method to speed up Monte Carlo rendering algorithms. Our\nmethod first generates two versions of a rendering: one at a low resolution\nwith a high sample rate (LRHS) and the other at a high resolution with a low\nsample rate (HRLS). We then develop a deep convolutional neural network to fuse\nthese two renderings into a high-quality image as if it were rendered at a high\nresolution with a high sample rate. Specifically, we formulate this fusion task\nas a super resolution problem that generates a high resolution rendering from a\nlow resolution input (LRHS), assisted with the HRLS rendering. The HRLS\nrendering provides critical high frequency details which are difficult to\nrecover from the LRHS for any super resolution methods. Our experiments show\nthat our hybrid rendering algorithm is significantly faster than the\nstate-of-the-art Monte Carlo denoising methods while rendering high-quality\nimages when tested on both our own BCR dataset and the Gharbi dataset.\n\\url{https://github.com/hqqxyy/msspl}\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 07:35:27 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Hou", "Qiqi", ""], ["Li", "Zhan", ""], ["Marshall", "Carl S", ""], ["Panneer", "Selvakumar", ""], ["Liu", "Feng", ""]]}, {"id": "2106.12832", "submitter": "Wei Lu", "authors": "Wei Lu, Lingyi Liu, Junwei Luo, Xianfeng Zhao, Yicong Zhou, Jiwu Huang", "title": "Detection of Deepfake Videos Using Long Distance Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid progress of deepfake techniques in recent years, facial video\nforgery can generate highly deceptive video contents and bring severe security\nthreats. And detection of such forgery videos is much more urgent and\nchallenging. Most existing detection methods treat the problem as a vanilla\nbinary classification problem. In this paper, the problem is treated as a\nspecial fine-grained classification problem since the differences between fake\nand real faces are very subtle. It is observed that most existing face forgery\nmethods left some common artifacts in the spatial domain and time domain,\nincluding generative defects in the spatial domain and inter-frame\ninconsistencies in the time domain. And a spatial-temporal model is proposed\nwhich has two components for capturing spatial and temporal forgery traces in\nglobal perspective respectively. The two components are designed using a novel\nlong distance attention mechanism. The one component of the spatial domain is\nused to capture artifacts in a single frame, and the other component of the\ntime domain is used to capture artifacts in consecutive frames. They generate\nattention maps in the form of patches. The attention method has a broader\nvision which contributes to better assembling global information and extracting\nlocal statistic information. Finally, the attention maps are used to guide the\nnetwork to focus on pivotal parts of the face, just like other fine-grained\nclassification methods. The experimental results on different public datasets\ndemonstrate that the proposed method achieves the state-of-the-art performance,\nand the proposed long distance attention method can effectively capture pivotal\nparts for face forgery.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:33:32 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lu", "Wei", ""], ["Liu", "Lingyi", ""], ["Luo", "Junwei", ""], ["Zhao", "Xianfeng", ""], ["Zhou", "Yicong", ""], ["Huang", "Jiwu", ""]]}, {"id": "2106.12859", "submitter": "Lang Nie", "authors": "Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao", "title": "Unsupervised Deep Image Stitching: Reconstructing Stitched Features to\n  Images", "comments": "Accepted by IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3092828", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional feature-based image stitching technologies rely heavily on\nfeature detection quality, often failing to stitch images with few features or\nlow resolution. The learning-based image stitching solutions are rarely studied\ndue to the lack of labeled data, making the supervised methods unreliable. To\naddress the above limitations, we propose an unsupervised deep image stitching\nframework consisting of two stages: unsupervised coarse image alignment and\nunsupervised image reconstruction. In the first stage, we design an\nablation-based loss to constrain an unsupervised homography network, which is\nmore suitable for large-baseline scenes. Moreover, a transformer layer is\nintroduced to warp the input images in the stitching-domain space. In the\nsecond stage, motivated by the insight that the misalignments in pixel-level\ncan be eliminated to a certain extent in feature-level, we design an\nunsupervised image reconstruction network to eliminate the artifacts from\nfeatures to pixels. Specifically, the reconstruction network can be implemented\nby a low-resolution deformation branch and a high-resolution refined branch,\nlearning the deformation rules of image stitching and enhancing the resolution\nsimultaneously. To establish an evaluation benchmark and train the learning\nframework, a comprehensive real-world image dataset for unsupervised deep image\nstitching is presented and released. Extensive experiments well demonstrate the\nsuperiority of our method over other state-of-the-art solutions. Even compared\nwith the supervised solutions, our image stitching quality is still preferred\nby users.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:45:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Nie", "Lang", ""], ["Lin", "Chunyu", ""], ["Liao", "Kang", ""], ["Liu", "Shuaicheng", ""], ["Zhao", "Yao", ""]]}, {"id": "2106.12864", "submitter": "Johann Li", "authors": "Johann Li, Guangming Zhu, Cong Hua, Mingtao Feng, BasheerBennamoun,\n  Ping Li, Xiaoyuan Lu, Juan Song, Peiyi Shen, Xu Xu, Lin Mei, Liang Zhang,\n  Syed Afaq Ali Shah, Mohammed Bennamoun", "title": "A Systematic Collection of Medical Image Datasets for Deep Learning", "comments": "This paper has been submitted to one journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The astounding success made by artificial intelligence (AI) in healthcare and\nother fields proves that AI can achieve human-like performance. However,\nsuccess always comes with challenges. Deep learning algorithms are\ndata-dependent and require large datasets for training. The lack of data in the\nmedical imaging field creates a bottleneck for the application of deep learning\nto medical image analysis. Medical image acquisition, annotation, and analysis\nare costly, and their usage is constrained by ethical restrictions. They also\nrequire many resources, such as human expertise and funding. That makes it\ndifficult for non-medical researchers to have access to useful and large\nmedical data. Thus, as comprehensive as possible, this paper provides a\ncollection of medical image datasets with their associated challenges for deep\nlearning research. We have collected information of around three hundred\ndatasets and challenges mainly reported between 2013 and 2020 and categorized\nthem into four categories: head & neck, chest & abdomen, pathology & blood, and\n``others''. Our paper has three purposes: 1) to provide a most up to date and\ncomplete list that can be used as a universal reference to easily find the\ndatasets for clinical image analysis, 2) to guide researchers on the\nmethodology to test and evaluate their methods' performance and robustness on\nrelevant datasets, 3) to provide a ``route'' to relevant algorithms for the\nrelevant medical topics, and challenge leaderboards.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 10:00:30 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Li", "Johann", ""], ["Zhu", "Guangming", ""], ["Hua", "Cong", ""], ["Feng", "Mingtao", ""], ["BasheerBennamoun", "", ""], ["Li", "Ping", ""], ["Lu", "Xiaoyuan", ""], ["Song", "Juan", ""], ["Shen", "Peiyi", ""], ["Xu", "Xu", ""], ["Mei", "Lin", ""], ["Zhang", "Liang", ""], ["Shah", "Syed Afaq Ali", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "2106.12871", "submitter": "Subhadip Maji", "authors": "Subhadip Maji, Swapna Sourav Rout and Sudeep Choudhary", "title": "DCoM: A Deep Column Mapper for Semantic Data Type Detection", "comments": "9 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detection of semantic data types is a very crucial task in data science for\nautomated data cleaning, schema matching, data discovery, semantic data type\nnormalization and sensitive data identification. Existing methods include\nregular expression-based or dictionary lookup-based methods that are not robust\nto dirty as well unseen data and are limited to a very less number of semantic\ndata types to predict. Existing Machine Learning methods extract large number\nof engineered features from data and build logistic regression, random forest\nor feedforward neural network for this purpose. In this paper, we introduce\nDCoM, a collection of multi-input NLP-based deep neural networks to detect\nsemantic data types where instead of extracting large number of features from\nthe data, we feed the raw values of columns (or instances) to the model as\ntexts. We train DCoM on 686,765 data columns extracted from VizNet corpus with\n78 different semantic data types. DCoM outperforms other contemporary results\nwith a quite significant margin on the same dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 10:12:35 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Maji", "Subhadip", ""], ["Rout", "Swapna Sourav", ""], ["Choudhary", "Sudeep", ""]]}, {"id": "2106.12900", "submitter": "Shuyu Zhao", "authors": "Fan Liu, Shuyu Zhao, Xuelong Dai, Bin Xiao", "title": "Long-term Cross Adversarial Training: A Robust Meta-learning Method for\n  Few-shot Classification Tasks", "comments": "Accepted by the ICML 2021 Workshop on A Blessing in Disguise: The\n  Prospects and Perils of Adversarial Machine\n  Learning(https://openreview.net/group?id=ICML.cc/2021/Workshop/AML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:31:16 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 12:13:14 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 09:41:16 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Liu", "Fan", ""], ["Zhao", "Shuyu", ""], ["Dai", "Xuelong", ""], ["Xiao", "Bin", ""]]}, {"id": "2106.12902", "submitter": "Fahim Faisal Niloy", "authors": "Fahim Faisal Niloy, M. Ashraful Amin, Amin Ahsan Ali, AKM Mahbubur\n  Rahman", "title": "Attention Toward Neighbors: A Context Aware Framework for High\n  Resolution Image Segmentation", "comments": "Accepted at ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-resolution image segmentation remains challenging and error-prone due to\nthe enormous size of intermediate feature maps. Conventional methods avoid this\nproblem by using patch based approaches where each patch is segmented\nindependently. However, independent patch segmentation induces errors,\nparticularly at the patch boundary due to the lack of contextual information in\nvery high-resolution images where the patch size is much smaller compared to\nthe full image. To overcome these limitations, in this paper, we propose a\nnovel framework to segment a particular patch by incorporating contextual\ninformation from its neighboring patches. This allows the segmentation network\nto see the target patch with a wider field of view without the need of larger\nfeature maps. Comparative analysis from a number of experiments shows that our\nproposed framework is able to segment high resolution images with significantly\nimproved mean Intersection over Union and overall accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 10:58:09 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Niloy", "Fahim Faisal", ""], ["Amin", "M. Ashraful", ""], ["Ali", "Amin Ahsan", ""], ["Rahman", "AKM Mahbubur", ""]]}, {"id": "2106.12917", "submitter": "Jens Petersen", "authors": "Jens Petersen and Fabian Isensee and Gregor K\\\"ohler and Paul F.\n  J\\\"ager and David Zimmerer and Ulf Neuberger and Wolfgang Wick and J\\\"urgen\n  Debus and Sabine Heiland and Martin Bendszus and Philipp Vollmuth and Klaus\n  H. Maier-Hein", "title": "Continuous-Time Deep Glioma Growth Models", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The ability to estimate how a tumor might evolve in the future could have\ntremendous clinical benefits, from improved treatment decisions to better dose\ndistribution in radiation therapy. Recent work has approached the glioma growth\nmodeling problem via deep learning and variational inference, thus learning\ngrowth dynamics entirely from a real patient data distribution. So far, this\napproach was constrained to predefined image acquisition intervals and\nsequences of fixed length, which limits its applicability in more realistic\nscenarios. We overcome these limitations by extending Neural Processes, a class\nof conditional generative models for stochastic time series, with a\nhierarchical multi-scale representation encoding including a spatio-temporal\nattention mechanism. The result is a learned growth model that can be\nconditioned on an arbitrary number of observations, and that can produce a\ndistribution of temporally consistent growth trajectories on a continuous time\naxis. On a dataset of 379 patients, the approach successfully captures both\nglobal and finer-grained variations in the images, exhibiting superior\nperformance compared to other learned growth models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 14:40:44 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 13:39:07 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Petersen", "Jens", ""], ["Isensee", "Fabian", ""], ["K\u00f6hler", "Gregor", ""], ["J\u00e4ger", "Paul F.", ""], ["Zimmerer", "David", ""], ["Neuberger", "Ulf", ""], ["Wick", "Wolfgang", ""], ["Debus", "J\u00fcrgen", ""], ["Heiland", "Sabine", ""], ["Bendszus", "Martin", ""], ["Vollmuth", "Philipp", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "2106.12930", "submitter": "Huy Hieu Pham", "authors": "Hieu T. Nguyen, Hieu H. Pham, Nghia T. Nguyen, Ha Q. Nguyen, Thang Q.\n  Huynh, Minh Dao, Van Vu", "title": "VinDr-SpineXR: A deep learning framework for spinal lesions detection\n  and classification from radiographs", "comments": "This is a preprint of our paper which was accepted for publication by\n  the International Conference on Medical Image Computing and Computer Assisted\n  Intervention (MICCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radiographs are used as the most important imaging tool for identifying spine\nanomalies in clinical practice. The evaluation of spinal bone lesions, however,\nis a challenging task for radiologists. This work aims at developing and\nevaluating a deep learning-based framework, named VinDr-SpineXR, for the\nclassification and localization of abnormalities from spine X-rays. First, we\nbuild a large dataset, comprising 10,468 spine X-ray images from 5,000 studies,\neach of which is manually annotated by an experienced radiologist with bounding\nboxes around abnormal findings in 13 categories. Using this dataset, we then\ntrain a deep learning classifier to determine whether a spine scan is abnormal\nand a detector to localize 7 crucial findings amongst the total 13. The\nVinDr-SpineXR is evaluated on a test set of 2,078 images from 1,000 studies,\nwhich is kept separate from the training set. It demonstrates an area under the\nreceiver operating characteristic curve (AUROC) of 88.61% (95% CI 87.19%,\n90.02%) for the image-level classification task and a mean average precision\n(mAP@0.5) of 33.56% for the lesion-level localization task. These results serve\nas a proof of concept and set a baseline for future research in this direction.\nTo encourage advances, the dataset, codes, and trained deep learning models are\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 11:45:44 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Nguyen", "Hieu T.", ""], ["Pham", "Hieu H.", ""], ["Nguyen", "Nghia T.", ""], ["Nguyen", "Ha Q.", ""], ["Huynh", "Thang Q.", ""], ["Dao", "Minh", ""], ["Vu", "Van", ""]]}, {"id": "2106.12940", "submitter": "Guozhi Tang", "authors": "Guozhi Tang, Lele Xie, Lianwen Jin, Jiapeng Wang, Jingdong Chen, Zhen\n  Xu, Qianying Wang, Yaqiang Wu, Hui Li", "title": "MatchVIE: Exploiting Match Relevancy between Entities for Visual\n  Information Extraction", "comments": "accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Information Extraction (VIE) task aims to extract key information from\nmultifarious document images (e.g., invoices and purchase receipts). Most\nprevious methods treat the VIE task simply as a sequence labeling problem or\nclassification problem, which requires models to carefully identify each kind\nof semantics by introducing multimodal features, such as font, color, layout.\nBut simply introducing multimodal features couldn't work well when faced with\nnumeric semantic categories or some ambiguous texts. To address this issue, in\nthis paper we propose a novel key-value matching model based on a graph neural\nnetwork for VIE (MatchVIE). Through key-value matching based on relevancy\nevaluation, the proposed MatchVIE can bypass the recognitions to various\nsemantics, and simply focuses on the strong relevancy between entities.\nBesides, we introduce a simple but effective operation, Num2Vec, to tackle the\ninstability of encoded values, which helps model converge more smoothly.\nComprehensive experiments demonstrate that the proposed MatchVIE can\nsignificantly outperform previous methods. Notably, to the best of our\nknowledge, MatchVIE may be the first attempt to tackle the VIE task by modeling\nthe relevancy between keys and values and it is a good complement to the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:06:29 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Tang", "Guozhi", ""], ["Xie", "Lele", ""], ["Jin", "Lianwen", ""], ["Wang", "Jiapeng", ""], ["Chen", "Jingdong", ""], ["Xu", "Zhen", ""], ["Wang", "Qianying", ""], ["Wu", "Yaqiang", ""], ["Li", "Hui", ""]]}, {"id": "2106.12942", "submitter": "Mahmoud Hossam", "authors": "Mahmoud Hossam", "title": "High Performance Hyperspectral Image Classification using Graphics\n  Processing Units", "comments": "Master Thesis, Ain Shams University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time remote sensing applications like search and rescue missions,\nmilitary target detection, environmental monitoring, hazard prevention and\nother time-critical applications require onboard real time processing\ncapabilities or autonomous decision making. Some unmanned remote systems like\nsatellites are physically remote from their operators, and all control of the\nspacecraft and data returned by the spacecraft must be transmitted over a\nwireless radio link. This link may not be available for extended periods when\nthe satellite is out of line of sight of its ground station. Therefore,\nlightweight, small size and low power consumption hardware is essential for\nonboard real time processing systems. With increasing dimensionality, size and\nresolution of recent hyperspectral imaging sensors, additional challenges are\nposed upon remote sensing processing systems and more capable computing\narchitectures are needed. Graphical Processing Units (GPUs) emerged as\npromising architecture for light weight high performance computing that can\naddress these computational requirements for onboard systems. The goal of this\nstudy is to build high performance methods for onboard hyperspectral analysis.\nWe propose accelerated methods for the well-known recursive hierarchical\nsegmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a\nGPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the\nNational Aeronautics and Space Administration (NASA), which is designed to\nprovide rich classification information with several output levels. The\nachieved speedups by parallel solutions compared to CPU sequential\nimplementations are 21x for parallel single GPU and 240x for hybrid multi-node\ncomputer clusters with 16 computing nodes. The energy consumption is reduced to\n74% using a single GPU compared to the equivalent parallel CPU cluster.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 09:26:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Hossam", "Mahmoud", ""]]}, {"id": "2106.12954", "submitter": "Chuanmin Jia", "authors": "Chuanmin Jia, Ziqing Ge, Shanshe Wang, Siwei Ma, Wen Gao", "title": "Rate Distortion Characteristic Modeling for Neural Image Compression", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  End-to-end optimization capability offers neural image compression (NIC)\nsuperior lossy compression performance. However, distinct models are required\nto be trained to reach different points in the rate-distortion (R-D) space. In\nthis paper, we consider the problem of R-D characteristic analysis and modeling\nfor NIC. We make efforts to formulate the essential mathematical functions to\ndescribe the R-D behavior of NIC using deep network and statistical modeling.\nThus continuous bit-rate points could be elegantly realized by leveraging such\nmodel via a single trained network. In this regard, we propose a plugin-in\nmodule to learn the relationship between the target bit-rate and the binary\nrepresentation for the latent variable of auto-encoder. Furthermore, we model\nthe rate and distortion characteristic of NIC as a function of the coding\nparameter $\\lambda$ respectively. Our experiments show our proposed method is\neasy to adopt and obtains competitive coding performance with fixed-rate coding\napproaches, which would benefit the practical deployment of NIC. In addition,\nthe proposed model could be applied to NIC rate control with limited bit-rate\nerror using a single network.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:23:05 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Jia", "Chuanmin", ""], ["Ge", "Ziqing", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2106.12955", "submitter": "Abdolrahman Khoshrou", "authors": "Abdolrahman Khoshrou, Eric J. Pauwels", "title": "Regularisation for PCA- and SVD-type matrix factorisations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Singular Value Decomposition (SVD) and its close relative, Principal\nComponent Analysis (PCA), are well-known linear matrix decomposition techniques\nthat are widely used in applications such as dimension reduction and\nclustering. However, an important limitation of SVD/PCA is its sensitivity to\nnoise in the input data. In this paper, we take another look at the problem of\nregularisation and show that different formulations of the minimisation problem\nlead to qualitatively different solutions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:25:12 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Khoshrou", "Abdolrahman", ""], ["Pauwels", "Eric J.", ""]]}, {"id": "2106.12958", "submitter": "Benjamin Keltjens", "authors": "Benjamin Keltjens and Tom van Dijk and Guido de Croon", "title": "Self-Supervised Monocular Depth Estimation of Untextured Indoor Rotated\n  Scenes", "comments": "Added references for related works section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised deep learning methods have leveraged stereo images for\ntraining monocular depth estimation. Although these methods show strong results\non outdoor datasets such as KITTI, they do not match performance of supervised\nmethods on indoor environments with camera rotation. Indoor, rotated scenes are\ncommon for less constrained applications and pose problems for two reasons:\nabundance of low texture regions and increased complexity of depth cues for\nimages under rotation. In an effort to extend self-supervised learning to more\ngeneralised environments we propose two additions. First, we propose a novel\nFilled Disparity Loss term that corrects for ambiguity of image reconstruction\nerror loss in textureless regions. Specifically, we interpolate disparity in\nuntextured regions, using the estimated disparity from surrounding textured\nareas, and use L1 loss to correct the original estimation. Our experiments show\nthat depth estimation is substantially improved on low-texture scenes, without\nany loss on textured scenes, when compared to Monodepth by Godard et al.\nSecondly, we show that training with an application's representative rotations,\nin both pitch and roll, is sufficient to significantly improve performance over\nthe entire range of expected rotation. We demonstrate that depth estimation is\nsuccessfully generalised as performance is not lost when evaluated on test sets\nwith no camera rotation. Together these developments enable a broader use of\nself-supervised learning of monocular depth estimation for complex\nenvironments.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:27:16 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 12:11:18 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Keltjens", "Benjamin", ""], ["van Dijk", "Tom", ""], ["de Croon", "Guido", ""]]}, {"id": "2106.12964", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Daniel Olmeda Reino, Nikolay Chumerin, Richard E.\n  Turner", "title": "Continual Novelty Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty Detection methods identify samples that are not representative of a\nmodel's training set thereby flagging misleading predictions and bringing a\ngreater flexibility and transparency at deployment time. However, research in\nthis area has only considered Novelty Detection in the offline setting.\nRecently, there has been a growing realization in the computer vision community\nthat applications demand a more flexible framework - Continual Learning - where\nnew batches of data representing new domains, new classes or new tasks become\navailable at different points in time. In this setting, Novelty Detection\nbecomes more important, interesting and challenging. This work identifies the\ncrucial link between the two problems and investigates the Novelty Detection\nproblem under the Continual Learning setting. We formulate the Continual\nNovelty Detection problem and present a benchmark, where we compare several\nNovelty Detection methods under different Continual Learning settings.\n  We show that Continual Learning affects the behaviour of novelty detection\nalgorithms, while novelty detection can pinpoint insights in the behaviour of a\ncontinual learner. We further propose baselines and discuss possible research\ndirections. We believe that the coupling of the two problems is a promising\ndirection to bring vision models into practice.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:30:41 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Reino", "Daniel Olmeda", ""], ["Chumerin", "Nikolay", ""], ["Turner", "Richard E.", ""]]}, {"id": "2106.12966", "submitter": "Zhuang He", "authors": "Zhuang He, Qi Li, Huajun Feng, Zhihai Xu", "title": "Class agnostic moving target detection by color and location prediction\n  of moving area", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Moving target detection plays an important role in computer vision. However,\ntraditional algorithms such as frame difference and optical flow usually suffer\nfrom low accuracy or heavy computation. Recent algorithms such as deep\nlearning-based convolutional neural networks have achieved high accuracy and\nreal-time performance, but they usually need to know the classes of targets in\nadvance, which limits the practical applications. Therefore, we proposed a\nmodel free moving target detection algorithm. This algorithm extracts the\nmoving area through the difference of image features. Then, the color and\nlocation probability map of the moving area will be calculated through maximum\na posteriori probability. And the target probability map can be obtained\nthrough the dot multiply between the two maps. Finally, the optimal moving\ntarget area can be solved by stochastic gradient descent on the target\nprobability map. Results show that the proposed algorithm achieves the highest\naccuracy compared with state-of-the-art algorithms, without needing to know the\nclasses of targets. Furthermore, as the existing datasets are not suitable for\nmoving target detection, we proposed a method for producing evaluation dataset.\nBesides, we also proved the proposed algorithm can be used to assist target\ntracking.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:34:58 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["He", "Zhuang", ""], ["Li", "Qi", ""], ["Feng", "Huajun", ""], ["Xu", "Zhihai", ""]]}, {"id": "2106.12991", "submitter": "Yulei Qin", "authors": "Yulei Qin, Yun Gu, Hanxiao Zhang, Jie Yang, Lihui Wang, Feng Yao,\n  Yue-Min Zhu", "title": "Relationship between pulmonary nodule malignancy and surrounding\n  pleurae, airways and vessels: a quantitative study using the public LIDC-IDRI\n  dataset", "comments": "33 pages, 3 figures, Submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.med-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To investigate whether the pleurae, airways and vessels surrounding a nodule\non non-contrast computed tomography (CT) can discriminate benign and malignant\npulmonary nodules. The LIDC-IDRI dataset, one of the largest publicly available\nCT database, was exploited for study. A total of 1556 nodules from 694 patients\nwere involved in statistical analysis, where nodules with average scorings <3\nand >3 were respectively denoted as benign and malignant. Besides, 339 nodules\nfrom 113 patients with diagnosis ground-truth were independently evaluated.\nComputer algorithms were developed to segment pulmonary structures and quantify\nthe distances to pleural surface, airways and vessels, as well as the counting\nnumber and normalized volume of airways and vessels near a nodule. Odds ratio\n(OR) and Chi-square (\\chi^2) testing were performed to demonstrate the\ncorrelation between features of surrounding structures and nodule malignancy. A\nnon-parametric receiver operating characteristic (ROC) analysis was conducted\nin logistic regression to evaluate discrimination ability of each structure.\nFor benign and malignant groups, the average distances from nodules to pleural\nsurface, airways and vessels are respectively (6.56, 5.19), (37.08, 26.43) and\n(1.42, 1.07) mm. The correlation between nodules and the counting number of\nairways and vessels that contact or project towards nodules are respectively\n(OR=22.96, \\chi^2=105.04) and (OR=7.06, \\chi^2=290.11). The correlation between\nnodules and the volume of airways and vessels are (OR=9.19, \\chi^2=159.02) and\n(OR=2.29, \\chi^2=55.89). The areas-under-curves (AUCs) for pleurae, airways and\nvessels are respectively 0.5202, 0.6943 and 0.6529. Our results show that\nmalignant nodules are often surrounded by more pulmonary structures compared\nwith benign ones, suggesting that features of these structures could be viewed\nas lung cancer biomarkers.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:05:51 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Qin", "Yulei", ""], ["Gu", "Yun", ""], ["Zhang", "Hanxiao", ""], ["Yang", "Jie", ""], ["Wang", "Lihui", ""], ["Yao", "Feng", ""], ["Zhu", "Yue-Min", ""]]}, {"id": "2106.12993", "submitter": "Pavol Bauer", "authors": "Indrani Sarkar, Indranil Maji, Charitha Omprakash, Sebastian Stober,\n  Sanja Mikulovic, Pavol Bauer", "title": "Evaluation of deep lift pose models for 3D rodent pose estimation based\n  on geometrically triangulated data", "comments": "5 pages, 6 figures, Accepted at the CVPR 2021 CV4Animals workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The assessment of laboratory animal behavior is of central interest in modern\nneuroscience research. Behavior is typically studied in terms of pose changes,\nwhich are ideally captured in three dimensions. This requires triangulation\nover a multi-camera system which view the animal from different angles.\nHowever, this is challenging in realistic laboratory setups due to occlusions\nand other technical constrains. Here we propose the usage of lift-pose models\nthat allow for robust 3D pose estimation of freely moving rodents from a single\nview camera view. To obtain high-quality training data for the pose-lifting, we\nfirst perform geometric calibration in a camera setup involving bottom as well\nas side views of the behaving animal. We then evaluate the performance of two\npreviously proposed model architectures under given inference perspectives and\nconclude that reliable 3D pose inference can be obtained using temporal\nconvolutions. With this work we would like to contribute to a more robust and\ndiverse behavior tracking of freely moving rodents for a wide range of\nexperiments and setups in the neuroscience community.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:08:33 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Sarkar", "Indrani", ""], ["Maji", "Indranil", ""], ["Omprakash", "Charitha", ""], ["Stober", "Sebastian", ""], ["Mikulovic", "Sanja", ""], ["Bauer", "Pavol", ""]]}, {"id": "2106.12994", "submitter": "Hengjie Lu", "authors": "Hengjie Lu, Shugong Xu, Shan Cao", "title": "SGTBN: Generating Dense Depth Maps from Single-Line LiDAR", "comments": null, "journal-ref": null, "doi": "10.1109/JSEN.2021.3088308", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion aims to generate a dense depth map from the sparse depth map\nand aligned RGB image. However, current depth completion methods use extremely\nexpensive 64-line LiDAR(about $100,000) to obtain sparse depth maps, which will\nlimit their application scenarios. Compared with the 64-line LiDAR, the\nsingle-line LiDAR is much less expensive and much more robust. Therefore, we\npropose a method to tackle the problem of single-line depth completion, in\nwhich we aim to generate a dense depth map from the single-line LiDAR info and\nthe aligned RGB image. A single-line depth completion dataset is proposed based\non the existing 64-line depth completion dataset(KITTI). A network called\nSemantic Guided Two-Branch Network(SGTBN) which contains global and local\nbranches to extract and fuse global and local info is proposed for this task. A\nSemantic guided depth upsampling module is used in our network to make full use\nof the semantic info in RGB images. Except for the usual MSE loss, we add the\nvirtual normal loss to increase the constraint of high-order 3D geometry in our\nnetwork. Our network outperforms the state-of-the-art in the single-line depth\ncompletion task. Besides, compared with the monocular depth estimation, our\nmethod also has significant advantages in precision and model size.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:08:35 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lu", "Hengjie", ""], ["Xu", "Shugong", ""], ["Cao", "Shan", ""]]}, {"id": "2106.13014", "submitter": "Zhiwu Qing", "authors": "Zhiwu Qing and Xiang Wang and Ziyuan Huang and Yutong Feng and Shiwei\n  Zhang and jianwen Jiang and Mingqian Tang and Changxin Gao and Nong Sang", "title": "Exploring Stronger Feature for Temporal Action Localization", "comments": "Rank 1st on the CVPR2021 HACS supervised Temporal Action Localization\n  Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization aims to localize starting and ending time with\naction category. Limited by GPU memory, mainstream methods pre-extract features\nfor each video. Therefore, feature quality determines the upper bound of\ndetection performance. In this technical report, we explored classic\nconvolution-based backbones and the recent surge of transformer-based\nbackbones. We found that the transformer-based methods can achieve better\nclassification performance than convolution-based, but they cannot generate\naccuracy action proposals. In addition, extracting features with larger frame\nresolution to reduce the loss of spatial information can also effectively\nimprove the performance of temporal action localization. Finally, we achieve\n42.42% in terms of mAP on validation set with a single SlowFast feature by a\nsimple combination: BMN+TCANet, which is 1.87% higher than the result of 2020's\nmulti-model ensemble. Finally, we achieve Rank 1st on the CVPR2021 HACS\nsupervised Temporal Action Localization Challenge.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:46:30 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Qing", "Zhiwu", ""], ["Wang", "Xiang", ""], ["Huang", "Ziyuan", ""], ["Feng", "Yutong", ""], ["Zhang", "Shiwei", ""], ["Jiang", "jianwen", ""], ["Tang", "Mingqian", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""]]}, {"id": "2106.13024", "submitter": "Hongyu Guo", "authors": "Sun Sun and Hongyu Guo", "title": "Symmetric Wasserstein Autoencoders", "comments": "Accepted by UAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging the framework of Optimal Transport, we introduce a new family of\ngenerative autoencoders with a learnable prior, called Symmetric Wasserstein\nAutoencoders (SWAEs). We propose to symmetrically match the joint distributions\nof the observed data and the latent representation induced by the encoder and\nthe decoder. The resulting algorithm jointly optimizes the modelling losses in\nboth the data and the latent spaces with the loss in the data space leading to\nthe denoising effect. With the symmetric treatment of the data and the latent\nrepresentation, the algorithm implicitly preserves the local structure of the\ndata in the latent space. To further improve the quality of the latent\nrepresentation, we incorporate a reconstruction loss into the objective, which\nsignificantly benefits both the generation and reconstruction. We empirically\nshow the superior performance of SWAEs over the state-of-the-art generative\nautoencoders in terms of classification, reconstruction, and generation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:56:02 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Sun", "Sun", ""], ["Guo", "Hongyu", ""]]}, {"id": "2106.13029", "submitter": "Yuxin Wang", "authors": "Yuxin Wang, Hongtao Xie, Shancheng Fang, Yadong Qu and Yongdong Zhang", "title": "A Simple and Strong Baseline: Progressively Region-based Scene Text\n  Removal Networks", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing scene text removal methods mainly train an elaborate network with\npaired images to realize the function of text localization and background\nreconstruction simultaneously, but there exists two problems: 1) lacking the\nexhaustive erasure of text region and 2) causing the excessive erasure to\ntext-free areas. To handle these issues, this paper provides a novel\nProgrEssively Region-based scene Text eraser (PERT), which introduces\nregion-based modification strategy to progressively erase the pixels in only\ntext region. Firstly, PERT decomposes the STR task to several erasing stages.\nAs each stage aims to take a further step toward the text-removed image rather\nthan directly regress to the final result, the decomposed operation reduces the\nlearning difficulty in each stage, and an exhaustive erasure result can be\nobtained by iterating over lightweight erasing blocks with shared parameters.\nThen, PERT introduces a region-based modification strategy to ensure the\nintegrity of text-free areas by decoupling text localization from erasure\nprocess to guide the removal. Benefiting from the simplicity architecture, PERT\nis a simple and strong baseline, and is easy to be followed and developed.\nExtensive experiments demonstrate that PERT obtains the state-of-the-art\nresults on both synthetic and real-world datasets. Code is available\nathttps://github.com/wangyuxin87/PERT.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 14:06:06 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Wang", "Yuxin", ""], ["Xie", "Hongtao", ""], ["Fang", "Shancheng", ""], ["Qu", "Yadong", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2106.13033", "submitter": "Kuan-Yu Chen", "authors": "Ke-Han Lu, Bo-Han Fang, Kuan-Yu Chen", "title": "A Transformer-based Cross-modal Fusion Model with Adversarial Training\n  for VQA Challenge 2021", "comments": "CVPR 2021 Workshop: Visual Question Answering (VQA) Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, inspired by the successes of visionlanguage pre-trained models\nand the benefits from training with adversarial attacks, we present a novel\ntransformerbased cross-modal fusion modeling by incorporating the both notions\nfor VQA challenge 2021. Specifically, the proposed model is on top of the\narchitecture of VinVL model [19], and the adversarial training strategy [4] is\napplied to make the model robust and generalized. Moreover, two implementation\ntricks are also used in our system to obtain better results. The experiments\ndemonstrate that the novel framework can achieve 76.72% on VQAv2 test-std set.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 14:09:57 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lu", "Ke-Han", ""], ["Fang", "Bo-Han", ""], ["Chen", "Kuan-Yu", ""]]}, {"id": "2106.13041", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko", "title": "Unsupervised Learning of Depth and Depth-of-Field Effect from Natural\n  Images with Aperture Rendering Generative Adversarial Networks", "comments": "Accepted to CVPR 2021 (Oral). Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/ar-gan/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the 3D world from 2D projected natural images is a fundamental\nchallenge in computer vision and graphics. Recently, an unsupervised learning\napproach has garnered considerable attention owing to its advantages in data\ncollection. However, to mitigate training limitations, typical methods need to\nimpose assumptions for viewpoint distribution (e.g., a dataset containing\nvarious viewpoint images) or object shape (e.g., symmetric objects). These\nassumptions often restrict applications; for instance, the application to\nnon-rigid objects or images captured from similar viewpoints (e.g., flower or\nbird images) remains a challenge. To complement these approaches, we propose\naperture rendering generative adversarial networks (AR-GANs), which equip\naperture rendering on top of GANs, and adopt focus cues to learn the depth and\ndepth-of-field (DoF) effect of unlabeled natural images. To address the\nambiguities triggered by unsupervised setting (i.e., ambiguities between smooth\ntexture and out-of-focus blurs, and between foreground and background blurs),\nwe develop DoF mixture learning, which enables the generator to learn real\nimage distribution while generating diverse DoF images. In addition, we devise\na center focus prior to guiding the learning direction. In the experiments, we\ndemonstrate the effectiveness of AR-GANs in various datasets, such as flower,\nbird, and face images, demonstrate their portability by incorporating them into\nother 3D representation learning GANs, and validate their applicability in\nshallow DoF rendering.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 14:15:50 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Kaneko", "Takuhiro", ""]]}, {"id": "2106.13043", "submitter": "Andrey Guzhov", "authors": "Andrey Guzhov, Federico Raue, J\\\"orn Hees, Andreas Dengel", "title": "AudioCLIP: Extending CLIP to Image, Text and Audio", "comments": "submitted to GCPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the past, the rapidly evolving field of sound classification greatly\nbenefited from the application of methods from other domains. Today, we observe\nthe trend to fuse domain-specific tasks and approaches together, which provides\nthe community with new outstanding models.\n  In this work, we present an extension of the CLIP model that handles audio in\naddition to text and images. Our proposed model incorporates the ESResNeXt\naudio-model into the CLIP framework using the AudioSet dataset. Such a\ncombination enables the proposed model to perform bimodal and unimodal\nclassification and querying, while keeping CLIP's ability to generalize to\nunseen datasets in a zero-shot inference fashion.\n  AudioCLIP achieves new state-of-the-art results in the Environmental Sound\nClassification (ESC) task, out-performing other approaches by reaching\naccuracies of 90.07% on the UrbanSound8K and 97.15% on the ESC-50 datasets.\nFurther it sets new baselines in the zero-shot ESC-task on the same datasets\n68.78% and 69.40%, respectively).\n  Finally, we also assess the cross-modal querying performance of the proposed\nmodel as well as the influence of full and partial training on the results. For\nthe sake of reproducibility, our code is published.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 14:16:38 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Guzhov", "Andrey", ""], ["Raue", "Federico", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "2106.13064", "submitter": "Ge Yang", "authors": "Tianjie Yang, Yaoru Luo, Wei Ji and Ge Yang", "title": "Advancing biological super-resolution microscopy through deep learning:\n  a brief review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Super-resolution microscopy overcomes the diffraction limit of conventional\nlight microscopy in spatial resolution. By providing novel spatial or\nspatio-temporal information on biological processes at nanometer resolution\nwith molecular specificity, it plays an increasingly important role in life\nsciences. However, its technical limitations require trade-offs to balance its\nspatial resolution, temporal resolution, and light exposure of samples.\nRecently, deep learning has achieved breakthrough performance in many image\nprocessing and computer vision tasks. It has also shown great promise in\npushing the performance envelope of super-resolution microscopy. In this brief\nReview, we survey recent advances in using deep learning to enhance performance\nof super-resolution microscopy. We focus primarily on how deep learning\nad-vances reconstruction of super-resolution images. Related key technical\nchallenges are discussed. Despite the challenges, deep learning is set to play\nan indispensable and transformative role in the development of super-resolution\nmicroscopy. We conclude with an outlook on how deep learning could shape the\nfuture of this new generation of light microscopy technology.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 14:44:23 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Yang", "Tianjie", ""], ["Luo", "Yaoru", ""], ["Ji", "Wei", ""], ["Yang", "Ge", ""]]}, {"id": "2106.13071", "submitter": "Meysam Madadi", "authors": "Sergio Escalera and Marti Soler and Stephane Ayache and Umut Guclu and\n  Jun Wan and Meysam Madadi and Xavier Baro and Hugo Jair Escalante and\n  Isabelle Guyon", "title": "ChaLearn Looking at People: Inpainting and Denoising challenges", "comments": null, "journal-ref": "Inpainting and Denoising Challenges. The Springer Series on\n  Challenges in Machine Learning. Springer, Cham. (2019)", "doi": "10.1007/978-3-030-25614-2_2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with incomplete information is a well studied problem in the context\nof machine learning and computational intelligence. However, in the context of\ncomputer vision, the problem has only been studied in specific scenarios (e.g.,\ncertain types of occlusions in specific types of images), although it is common\nto have incomplete information in visual data. This chapter describes the\ndesign of an academic competition focusing on inpainting of images and video\nsequences that was part of the competition program of WCCI2018 and had a\nsatellite event collocated with ECCV2018. The ChaLearn Looking at People\nInpainting Challenge aimed at advancing the state of the art on visual\ninpainting by promoting the development of methods for recovering missing and\noccluded information from images and video. Three tracks were proposed in which\nvisual inpainting might be helpful but still challenging: human body pose\nestimation, text overlays removal and fingerprint denoising. This chapter\ndescribes the design of the challenge, which includes the release of three\nnovel datasets, and the description of evaluation metrics, baselines and\nevaluation protocol. The results of the challenge are analyzed and discussed in\ndetail and conclusions derived from this event are outlined.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 14:57:21 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Escalera", "Sergio", ""], ["Soler", "Marti", ""], ["Ayache", "Stephane", ""], ["Guclu", "Umut", ""], ["Wan", "Jun", ""], ["Madadi", "Meysam", ""], ["Baro", "Xavier", ""], ["Escalante", "Hugo Jair", ""], ["Guyon", "Isabelle", ""]]}, {"id": "2106.13090", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Changgong Zhang, Wenbo Hu, Shijian Lu, Feiying Ma,\n  Xuansong Xie, Ling Shao", "title": "Sparse Needlets for Lighting Estimation with Spherical Transport Loss", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate lighting estimation is challenging yet critical to many computer\nvision and computer graphics tasks such as high-dynamic-range (HDR) relighting.\nExisting approaches model lighting in either frequency domain or spatial domain\nwhich is insufficient to represent the complex lighting conditions in scenes\nand tends to produce inaccurate estimation. This paper presents NeedleLight, a\nnew lighting estimation model that represents illumination with needlets and\nallows lighting estimation in both frequency domain and spatial domain jointly.\nAn optimal thresholding function is designed to achieve sparse needlets which\ntrims redundant lighting parameters and demonstrates superior localization\nproperties for illumination representation. In addition, a novel spherical\ntransport loss is designed based on optimal transport theory which guides to\nregress lighting representation parameters with consideration of the spatial\ninformation. Furthermore, we propose a new metric that is concise yet effective\nby directly evaluating the estimated illumination maps rather than rendered\nimages. Extensive experiments show that NeedleLight achieves superior lighting\nestimation consistently across multiple evaluation metrics as compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 15:19:42 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhan", "Fangneng", ""], ["Zhang", "Changgong", ""], ["Hu", "Wenbo", ""], ["Lu", "Shijian", ""], ["Ma", "Feiying", ""], ["Xie", "Xuansong", ""], ["Shao", "Ling", ""]]}, {"id": "2106.13112", "submitter": "Li Yuan", "authors": "Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, Shuicheng Yan", "title": "VOLO: Vision Outlooker for Visual Recognition", "comments": "code: https://github.com/sail-sg/volo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition has been dominated by convolutional neural networks (CNNs)\nfor years. Though recently the prevailing vision transformers (ViTs) have shown\ngreat potential of self-attention based models in ImageNet classification,\ntheir performance is still inferior to that of the latest SOTA CNNs if no extra\ndata are provided. In this work, we try to close the performance gap and\ndemonstrate that attention-based models are indeed able to outperform CNNs. We\nfind a major factor limiting the performance of ViTs for ImageNet\nclassification is their low efficacy in encoding fine-level features into the\ntoken representations. To resolve this, we introduce a novel outlook attention\nand present a simple and general architecture, termed Vision Outlooker (VOLO).\nUnlike self-attention that focuses on global dependency modeling at a coarse\nlevel, the outlook attention efficiently encodes finer-level features and\ncontexts into tokens, which is shown to be critically beneficial to recognition\nperformance but largely ignored by the self-attention. Experiments show that\nour VOLO achieves 87.1% top-1 accuracy on ImageNet-1K classification, which is\nthe first model exceeding 87% accuracy on this competitive benchmark, without\nusing any extra training data In addition, the pre-trained VOLO transfers well\nto downstream tasks, such as semantic segmentation. We achieve 84.3% mIoU score\non the cityscapes validation set and 54.3% on the ADE20K validation set. Code\nis available at \\url{https://github.com/sail-sg/volo}.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 15:46:54 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 14:40:33 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yuan", "Li", ""], ["Hou", "Qibin", ""], ["Jiang", "Zihang", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2106.13122", "submitter": "Benjamin Gilby", "authors": "Katelyn Morrison, Benjamin Gilby, Colton Lipchak, Adam Mattioli,\n  Adriana Kovashka", "title": "Exploring Corruption Robustness: Inductive Biases in Vision Transformers\n  and MLP-Mixers", "comments": "Under review at the Uncertainty and Robustness in Deep Learning\n  workshop at ICML 2021. Our appendix is attached to the last page of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, vision transformers and MLP-based models have been developed in\norder to address some of the prevalent weaknesses in convolutional neural\nnetworks. Due to the novelty of transformers being used in this domain along\nwith the self-attention mechanism, it remains unclear to what degree these\narchitectures are robust to corruptions. Despite some works proposing that data\naugmentation remains essential for a model to be robust against corruptions, we\npropose to explore the impact that the architecture has on corruption\nrobustness. We find that vision transformer architectures are inherently more\nrobust to corruptions than the ResNet-50 and MLP-Mixers. We also find that\nvision transformers with 5 times fewer parameters than a ResNet-50 have more\nshape bias. Our code is available to reproduce.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 15:57:01 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 19:35:22 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Morrison", "Katelyn", ""], ["Gilby", "Benjamin", ""], ["Lipchak", "Colton", ""], ["Mattioli", "Adam", ""], ["Kovashka", "Adriana", ""]]}, {"id": "2106.13139", "submitter": "Andre Rochow", "authors": "Andre Rochow, Max Schwarz, Michael Weinmann, Sven Behnke", "title": "FaDIV-Syn: Fast Depth-Independent View Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce FaDIV-Syn, a fast depth-independent view synthesis method. Our\nmulti-view approach addresses the problem that view synthesis methods are often\nlimited by their depth estimation stage, where incorrect depth predictions can\nlead to large projection errors. To avoid this issue, we efficiently warp\nmultiple input images into the target frame for a range of assumed depth\nplanes. The resulting tensor representation is fed into a U-Net-like CNN with\ngated convolutions, which directly produces the novel output view. We therefore\nside-step explicit depth estimation. This improves efficiency and performance\non transparent, reflective, and feature-less scene parts. FaDIV-Syn can handle\nboth interpolation and extrapolation tasks and outperforms state-of-the-art\nextrapolation methods on the large-scale RealEstate10k dataset. In contrast to\ncomparable methods, it is capable of real-time operation due to its lightweight\narchitecture. We further demonstrate data efficiency of FaDIV-Syn by training\nfrom fewer examples as well as its generalization to higher resolutions and\narbitrary depth ranges under severe depth discretization.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 16:14:01 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Rochow", "Andre", ""], ["Schwarz", "Max", ""], ["Weinmann", "Michael", ""], ["Behnke", "Sven", ""]]}, {"id": "2106.13150", "submitter": "Johannes Lotz", "authors": "Johannes Lotz, Nick Weiss, Jeroen van der Laak, StefanHeldmann", "title": "High-resolution Image Registration of Consecutive and Re-stained\n  Sections in Histopathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We compare variational image registration in consectutive and re-stained\nsections from histopathology. We present a fully-automatic algorithm for\nnon-parametric (nonlinear) image registration and apply it to a previously\nexisting dataset from the ANHIR challenge (230 slide pairs, consecutive\nsections) and a new dataset (hybrid re-stained and consecutive, 81 slide pairs,\nca. 3000 landmarks) which is made publicly available. Registration\nhyperparameters are obtained in the ANHIR dataset and applied to the new\ndataset without modification. In the new dataset, landmark errors after\nregistration range from 13.2 micrometers for consecutive sections to 1\nmicrometer for re-stained sections. We observe that non-parametric registration\nleads to lower landmark errors in both cases, even though the effect is smaller\nin re-stained sections. The nucleus-level alignment after non-parametric\nregistration of re-stained sections provides a valuable tool to generate\nautomatic ground-truth for machine learning applications in histopathology.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 16:25:28 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lotz", "Johannes", ""], ["Weiss", "Nick", ""], ["van der Laak", "Jeroen", ""], ["StefanHeldmann", "", ""]]}, {"id": "2106.13156", "submitter": "Jing Shi", "authors": "Jing Shi, Ning Xu, Yihang Xu, Trung Bui, Franck Dernoncourt, Chenliang\n  Xu", "title": "Learning by Planning: Language-Guided Global Image Editing", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, language-guided global image editing draws increasing attention\nwith growing application potentials. However, previous GAN-based methods are\nnot only confined to domain-specific, low-resolution data but also lacking in\ninterpretability. To overcome the collective difficulties, we develop a\ntext-to-operation model to map the vague editing language request into a series\nof editing operations, e.g., change contrast, brightness, and saturation. Each\noperation is interpretable and differentiable. Furthermore, the only\nsupervision in the task is the target image, which is insufficient for a stable\ntraining of sequential decisions. Hence, we propose a novel operation planning\nalgorithm to generate possible editing sequences from the target image as\npseudo ground truth. Comparison experiments on the newly collected MA5k-Req\ndataset and GIER dataset show the advantages of our methods. Code is available\nat https://jshi31.github.io/T2ONet.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 16:30:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Shi", "Jing", ""], ["Xu", "Ning", ""], ["Xu", "Yihang", ""], ["Bui", "Trung", ""], ["Dernoncourt", "Franck", ""], ["Xu", "Chenliang", ""]]}, {"id": "2106.13164", "submitter": "Sandareka Wickramanayake", "authors": "Sandareka Wickramanayake, Wynne Hsu, Mong Li Lee", "title": "Towards Fully Interpretable Deep Neural Networks: Are We There Yet?", "comments": "Presented at the ICML 2021 Workshop on Theoretic Foundation,\n  Criticism, and Application Trend of Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable performance, Deep Neural Networks (DNNs) behave as\nblack-boxes hindering user trust in Artificial Intelligence (AI) systems.\nResearch on opening black-box DNN can be broadly categorized into post-hoc\nmethods and inherently interpretable DNNs. While many surveys have been\nconducted on post-hoc interpretation methods, little effort is devoted to\ninherently interpretable DNNs. This paper provides a review of existing methods\nto develop DNNs with intrinsic interpretability, with a focus on Convolutional\nNeural Networks (CNNs). The aim is to understand the current progress towards\nfully interpretable DNNs that can cater to different interpretation\nrequirements. Finally, we identify gaps in current work and suggest potential\nresearch directions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 16:37:34 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Wickramanayake", "Sandareka", ""], ["Hsu", "Wynne", ""], ["Lee", "Mong Li", ""]]}, {"id": "2106.13178", "submitter": "Baaria Chaudhary", "authors": "Baaria Chaudhary, Poorya Aghdaie, Sobhan Soleymani, Jeremy Dawson,\n  Nasser M. Nasrabadi", "title": "Differential Morph Face Detection using Discriminative Wavelet Sub-bands", "comments": "CVPR Biometrics Workshop 2021", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops, 2021, pp. 1425-1434", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition systems are extremely vulnerable to morphing attacks, in\nwhich a morphed facial reference image can be successfully verified as two or\nmore distinct identities. In this paper, we propose a morph attack detection\nalgorithm that leverages an undecimated 2D Discrete Wavelet Transform (DWT) for\nidentifying morphed face images. The core of our framework is that artifacts\nresulting from the morphing process that are not discernible in the image\ndomain can be more easily identified in the spatial frequency domain. A\ndiscriminative wavelet sub-band can accentuate the disparity between a real and\na morphed image. To this end, multi-level DWT is applied to all images,\nyielding 48 mid and high-frequency sub-bands each. The entropy distributions\nfor each sub-band are calculated separately for both bona fide and morph\nimages. For some of the sub-bands, there is a marked difference between the\nentropy of the sub-band in a bona fide image and the identical sub-band's\nentropy in a morphed image. Consequently, we employ Kullback-Liebler Divergence\n(KLD) to exploit these differences and isolate the sub-bands that are the most\ndiscriminative. We measure how discriminative a sub-band is by its KLD value\nand the 22 sub-bands with the highest KLD values are chosen for network\ntraining. Then, we train a deep Siamese neural network using these 22 selected\nsub-bands for differential morph attack detection. We examine the efficacy of\ndiscriminative wavelet sub-bands for morph attack detection and show that a\ndeep neural network trained on these sub-bands can accurately identify morph\nimagery.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 16:55:34 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Chaudhary", "Baaria", ""], ["Aghdaie", "Poorya", ""], ["Soleymani", "Sobhan", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2106.13188", "submitter": "Mengwei Ren", "authors": "Mengwei Ren, Heejong Kim, Neel Dey, Guido Gerig", "title": "Q-space Conditioned Translation Networks for Directional Synthesis of\n  Diffusion Weighted Images from Multi-modal Structural MRI", "comments": "Accepted by MICCAI 2021. Project page:\n  https://heejongkim.com/dwi-synthesis; Code:\n  https://github.com/mengweiren/q-space-conditioned-dwi-synthesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning approaches for diffusion MRI modeling circumvent the\nneed for densely-sampled diffusion-weighted images (DWIs) by directly\npredicting microstructural indices from sparsely-sampled DWIs. However, they\nimplicitly make unrealistic assumptions of static $q$-space sampling during\ntraining and reconstruction. Further, such approaches can restrict downstream\nusage of variably sampled DWIs for usages including the estimation of\nmicrostructural indices or tractography. We propose a generative adversarial\ntranslation framework for high-quality DWI synthesis with arbitrary $q$-space\nsampling given commonly acquired structural images (e.g., B0, T1, T2). Our\ntranslation network linearly modulates its internal representations conditioned\non continuous $q$-space information, thus removing the need for fixed sampling\nschemes. Moreover, this approach enables downstream estimation of high-quality\nmicrostructural maps from arbitrarily subsampled DWIs, which may be\nparticularly important in cases with sparsely sampled DWIs. Across several\nrecent methodologies, the proposed approach yields improved DWI synthesis\naccuracy and fidelity with enhanced downstream utility as quantified by the\naccuracy of scalar microstructure indices estimated from the synthesized\nimages. Code is available at\nhttps://github.com/mengweiren/q-space-conditioned-dwi-synthesis.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:09:40 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Ren", "Mengwei", ""], ["Kim", "Heejong", ""], ["Dey", "Neel", ""], ["Gerig", "Guido", ""]]}, {"id": "2106.13195", "submitter": "Mohammad Babaeizadeh", "authors": "Mohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey\n  Levine, Chelsea Finn, Dumitru Erhan", "title": "FitVid: Overfitting in Pixel-Level Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An agent that is capable of predicting what happens next can perform a\nvariety of tasks through planning with no additional training. Furthermore,\nsuch an agent can internally represent the complex dynamics of the real-world\nand therefore can acquire a representation useful for a variety of visual\nperception tasks. This makes predicting the future frames of a video,\nconditioned on the observed past and potentially future actions, an interesting\ntask which remains exceptionally challenging despite many recent advances.\nExisting video prediction models have shown promising results on simple narrow\nbenchmarks but they generate low quality predictions on real-life datasets with\nmore complicated dynamics or broader domain. There is a growing body of\nevidence that underfitting on the training data is one of the primary causes\nfor the low quality predictions. In this paper, we argue that the inefficient\nuse of parameters in the current video models is the main reason for\nunderfitting. Therefore, we introduce a new architecture, named FitVid, which\nis capable of severe overfitting on the common benchmarks while having similar\nparameter count as the current state-of-the-art models. We analyze the\nconsequences of overfitting, illustrating how it can produce unexpected\noutcomes such as generating high quality output by repeating the training data,\nand how it can be mitigated using existing image augmentation techniques. As a\nresult, FitVid outperforms the current state-of-the-art models across four\ndifferent video prediction benchmarks on four different metrics.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:20:21 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Babaeizadeh", "Mohammad", ""], ["Saffar", "Mohammad Taghi", ""], ["Nair", "Suraj", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""], ["Erhan", "Dumitru", ""]]}, {"id": "2106.13201", "submitter": "Chengxi Li", "authors": "Chengxi Li, Stanley H. Chan, Yi-Ting Chen", "title": "Driver-centric Risk Object Identification", "comments": "Submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A massive number of traffic fatalities are due to driver errors. To reduce\nfatalities, developing intelligent driving systems assisting drivers to\nidentify potential risks is in urgent need. Risky situations are generally\ndefined based on collision prediction in existing research. However, collisions\nare only one type of risk in traffic scenarios. We believe a more generic\ndefinition is required. In this work, we propose a novel driver-centric\ndefinition of risk, i.e., risky objects influence driver behavior. Based on\nthis definition, a new task called risk object identification is introduced. We\nformulate the task as a cause-effect problem and present a novel two-stage risk\nobject identification framework, taking inspiration from models of situation\nawareness and causal inference. A driver-centric Risk Object Identification\n(ROI) dataset is curated to evaluate the proposed system. We demonstrate\nstate-of-the-art risk object identification performance compared with strong\nbaselines on the ROI dataset. In addition, we conduct extensive ablative\nstudies to justify our design choices.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:27:32 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Li", "Chengxi", ""], ["Chan", "Stanley H.", ""], ["Chen", "Yi-Ting", ""]]}, {"id": "2106.13203", "submitter": "Rakshit Naidu", "authors": "Rakshit Naidu, Aman Priyanshu, Aadith Kumar, Sasikanth Kotti, Haofan\n  Wang, Fatemehsadat Mireshghallah", "title": "When Differential Privacy Meets Interpretability: A Case Study", "comments": "4 pages, 7 figures; Extended abstract presented at RCV-CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the increase in the use of personal data for training Deep Neural\nNetworks (DNNs) in tasks such as medical imaging and diagnosis, differentially\nprivate training of DNNs is surging in importance and there is a large body of\nwork focusing on providing better privacy-utility trade-off. However, little\nattention is given to the interpretability of these models, and how the\napplication of DP affects the quality of interpretations. We propose an\nextensive study into the effects of DP training on DNNs, especially on medical\nimaging applications, on the APTOS dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:32:45 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 06:34:06 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Naidu", "Rakshit", ""], ["Priyanshu", "Aman", ""], ["Kumar", "Aadith", ""], ["Kotti", "Sasikanth", ""], ["Wang", "Haofan", ""], ["Mireshghallah", "Fatemehsadat", ""]]}, {"id": "2106.13208", "submitter": "Liangqiong Qu", "authors": "Liangqiong Qu, Niranjan Balachandar, Miao Zhang, Daniel Rubin", "title": "Handling Data Heterogeneity with Generative Replay in Collaborative\n  Learning for Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative learning, which enables collaborative and decentralized\ntraining of deep neural networks at multiple institutions in a\nprivacy-preserving manner, is rapidly emerging as a valuable technique in\nhealthcare applications. However, its distributed nature often leads to\nsignificant heterogeneity in data distributions across institutions. Existing\ncollaborative learning approaches generally do not account for the presence of\nheterogeneity in data among institutions, or only mildly skewed label\ndistributions are studied. In this paper, we present a novel generative replay\nstrategy to address the challenge of data heterogeneity in collaborative\nlearning methods. Instead of directly training a model for task performance, we\nleverage recent image synthesis techniques to develop a novel dual model\narchitecture: a primary model learns the desired task, and an auxiliary\n\"generative replay model\" either synthesizes images that closely resemble the\ninput images or helps extract latent variables. The generative replay strategy\nis flexible to use, can either be incorporated into existing collaborative\nlearning methods to improve their capability of handling data heterogeneity\nacross institutions, or be used as a novel and individual collaborative\nlearning framework (termed FedReplay) to reduce communication cost.\nExperimental results demonstrate the capability of the proposed method in\nhandling heterogeneous data across institutions. On highly heterogeneous data\npartitions, our model achieves ~4.88% improvement in the prediction accuracy on\na diabetic retinopathy classification dataset, and ~49.8% reduction of mean\nabsolution value on a Bone Age prediction dataset, respectively, compared to\nthe state-of-the art collaborative learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:39:55 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Qu", "Liangqiong", ""], ["Balachandar", "Niranjan", ""], ["Zhang", "Miao", ""], ["Rubin", "Daniel", ""]]}, {"id": "2106.13215", "submitter": "Youssef Alami Mejjati", "authors": "Youssef A.Mejjati and Isa Milefchik and Aaron Gokaslan and Oliver Wang\n  and Kwang In Kim and James Tompkin", "title": "GaussiGAN: Controllable Image Synthesis with 3D Gaussians from Unposed\n  Silhouettes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an algorithm that learns a coarse 3D representation of objects\nfrom unposed multi-view 2D mask supervision, then uses it to generate detailed\nmask and image texture. In contrast to existing voxel-based methods for unposed\nobject reconstruction, our approach learns to represent the generated shape and\npose with a set of self-supervised canonical 3D anisotropic Gaussians via a\nperspective camera, and a set of per-image transforms. We show that this\napproach can robustly estimate a 3D space for the camera and object, while\nrecent baselines sometimes struggle to reconstruct coherent 3D spaces in this\nsetting. We show results on synthetic datasets with realistic lighting, and\ndemonstrate object insertion with interactive posing. With our work, we help\nmove towards structured representations that handle more real-world variation\nin learning-based object reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:47:58 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Mejjati", "Youssef A.", ""], ["Milefchik", "Isa", ""], ["Gokaslan", "Aaron", ""], ["Wang", "Oliver", ""], ["Kim", "Kwang In", ""], ["Tompkin", "James", ""]]}, {"id": "2106.13217", "submitter": "Yuchao Dai Dr.", "authors": "Jing Zhang, Yunqiu Lv, Mochu Xiang, Aixuan Li, Yuchao Dai, Yiran Zhong", "title": "Depth-Guided Camouflaged Object Detection", "comments": "10 pages main content + 3 pages reference. The first work in RGB-D\n  Camouflaged object detection (COD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Camouflaged object detection (COD) aims to segment camouflaged objects hiding\nin the environment, which is challenging due to the similar appearance of\ncamouflaged objects and their surroundings. Research in biology suggests that\ndepth can provide useful object localization cues for camouflaged object\ndiscovery, as all the animals have 3D perception ability. However, the depth\ninformation has not been exploited for camouflaged object detection. To explore\nthe contribution of depth for camouflage detection, we present a depth-guided\ncamouflaged object detection network with pre-computed depth maps from existing\nmonocular depth estimation methods. Due to the domain gap between the depth\nestimation dataset and our camouflaged object detection dataset, the generated\ndepth may not be accurate enough to be directly used in our framework. We then\nintroduce a depth quality assessment module to evaluate the quality of depth\nbased on the model prediction from both RGB COD branch and RGB-D COD branch.\nDuring training, only high-quality depth is used to update the modal\ninteraction module for multi-modal learning. During testing, our depth quality\nassessment module can effectively determine the contribution of depth and\nselect the RGB branch or RGB-D branch for camouflage prediction. Extensive\nexperiments on various camouflaged object detection datasets prove the\neffectiveness of our solution in exploring the depth information for\ncamouflaged object detection. Our code and data is publicly available at:\n\\url{https://github.com/JingZhang617/RGBD-COD}.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:51:31 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 03:38:01 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhang", "Jing", ""], ["Lv", "Yunqiu", ""], ["Xiang", "Mochu", ""], ["Li", "Aixuan", ""], ["Dai", "Yuchao", ""], ["Zhong", "Yiran", ""]]}, {"id": "2106.13227", "submitter": "Xueqing Deng", "authors": "Xueqing Deng, Yi Zhu, Yuxin Tian, Shawn Newsam", "title": "AutoAdapt: Automated Segmentation Network Search for Unsupervised Domain\n  Adaptation", "comments": "short version has been accepted at 1st NAS workshop co-organized with\n  CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural network-based semantic segmentation has achieved remarkable results\nwhen large amounts of annotated data are available, that is, in the supervised\ncase. However, such data is expensive to collect and so methods have been\ndeveloped to adapt models trained on related, often synthetic data for which\nlabels are readily available. Current adaptation approaches do not consider the\ndependence of the generalization/transferability of these models on network\narchitecture. In this paper, we perform neural architecture search (NAS) to\nprovide architecture-level perspective and analysis for domain adaptation. We\nidentify the optimization gap that exists when searching architectures for\nunsupervised domain adaptation which makes this NAS problem uniquely difficult.\nWe propose bridging this gap by using maximum mean discrepancy and regional\nweighted entropy to estimate the accuracy metric. Experimental results on\nseveral widely adopted benchmarks show that our proposed AutoAdapt framework\nindeed discovers architectures that improve the performance of a number of\nexisting adaptation techniques.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:59:02 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Deng", "Xueqing", ""], ["Zhu", "Yi", ""], ["Tian", "Yuxin", ""], ["Newsam", "Shawn", ""]]}, {"id": "2106.13228", "submitter": "Keunhong Park", "authors": "Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien\n  Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz", "title": "HyperNeRF: A Higher-Dimensional Representation for Topologically Varying\n  Neural Radiance Fields", "comments": "Project page: https://hypernerf.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Radiance Fields (NeRF) are able to reconstruct scenes with\nunprecedented fidelity, and various recent works have extended NeRF to handle\ndynamic scenes. A common approach to reconstruct such non-rigid scenes is\nthrough the use of a learned deformation field mapping from coordinates in each\ninput image into a canonical template coordinate space. However, these\ndeformation-based approaches struggle to model changes in topology, as\ntopological changes require a discontinuity in the deformation field, but these\ndeformation fields are necessarily continuous. We address this limitation by\nlifting NeRFs into a higher dimensional space, and by representing the 5D\nradiance field corresponding to each individual input image as a slice through\nthis \"hyper-space\". Our method is inspired by level set methods, which model\nthe evolution of surfaces as slices through a higher dimensional surface. We\nevaluate our method on two tasks: (i) interpolating smoothly between \"moments\",\ni.e., configurations of the scene, seen in the input images while maintaining\nvisual plausibility, and (ii) novel-view synthesis at fixed moments. We show\nthat our method, which we dub HyperNeRF, outperforms existing methods on both\ntasks by significant margins. Compared to Nerfies, HyperNeRF reduces average\nerror rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as\nmeasured by LPIPS.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:59:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Park", "Keunhong", ""], ["Sinha", "Utkarsh", ""], ["Hedman", "Peter", ""], ["Barron", "Jonathan T.", ""], ["Bouaziz", "Sofien", ""], ["Goldman", "Dan B", ""], ["Martin-Brualla", "Ricardo", ""], ["Seitz", "Steven M.", ""]]}, {"id": "2106.13230", "submitter": "Yue Cao", "authors": "Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han\n  Hu", "title": "Video Swin Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vision community is witnessing a modeling shift from CNNs to\nTransformers, where pure Transformer architectures have attained top accuracy\non the major video recognition benchmarks. These video models are all built on\nTransformer layers that globally connect patches across the spatial and\ntemporal dimensions. In this paper, we instead advocate an inductive bias of\nlocality in video Transformers, which leads to a better speed-accuracy\ntrade-off compared to previous approaches which compute self-attention globally\neven with spatial-temporal factorization. The locality of the proposed video\narchitecture is realized by adapting the Swin Transformer designed for the\nimage domain, while continuing to leverage the power of pre-trained image\nmodels. Our approach achieves state-of-the-art accuracy on a broad range of\nvideo recognition benchmarks, including on action recognition (84.9 top-1\naccuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with ~20x less\npre-training data and ~3x smaller model size) and temporal modeling (69.6 top-1\naccuracy on Something-Something v2). The code and models will be made publicly\navailable at https://github.com/SwinTransformer/Video-Swin-Transformer.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:59:46 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Liu", "Ze", ""], ["Ning", "Jia", ""], ["Cao", "Yue", ""], ["Wei", "Yixuan", ""], ["Zhang", "Zheng", ""], ["Lin", "Stephen", ""], ["Hu", "Han", ""]]}, {"id": "2106.13239", "submitter": "Huazhu Fu", "authors": "Li Li, Huazhu Fu, Bo Han, Cheng-Zhong Xu, Ling Shao", "title": "Federated Noisy Client Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) collaboratively aggregates a shared global model\ndepending on multiple local clients, while keeping the training data\ndecentralized in order to preserve data privacy. However, standard FL methods\nignore the noisy client issue, which may harm the overall performance of the\naggregated model. In this paper, we first analyze the noisy client statement,\nand then model noisy clients with different noise distributions (e.g.,\nBernoulli and truncated Gaussian distributions). To learn with noisy clients,\nwe propose a simple yet effective FL framework, named Federated Noisy Client\nLearning (Fed-NCL), which is a plug-and-play algorithm and contains two main\ncomponents: a data quality measurement (DQM) to dynamically quantify the data\nquality of each participating client, and a noise robust aggregation (NRA) to\nadaptively aggregate the local models of each client by jointly considering the\namount of local training data and the data quality of each client. Our Fed-NCL\ncan be easily applied in any standard FL workflow to handle the noisy client\nissue. Experimental results on various datasets demonstrate that our algorithm\nboosts the performances of different state-of-the-art systems with noisy\nclients.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 11:09:17 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Li", "Li", ""], ["Fu", "Huazhu", ""], ["Han", "Bo", ""], ["Xu", "Cheng-Zhong", ""], ["Shao", "Ling", ""]]}, {"id": "2106.13266", "submitter": "Giorgos Kordopatis-Zilos", "authors": "Giorgos Kordopatis-Zilos, Christos Tzelepis, Symeon Papadopoulos,\n  Ioannis Kompatsiaris, Ioannis Patras", "title": "DnS: Distill-and-Select for Efficient and Accurate Video Indexing and\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, which we\ncall Distill-and-Select (DnS), that starting from a well-performing\nfine-grained Teacher Network learns: a) Student Networks at different retrieval\nperformance and computational efficiency trade-offs and b) a Selection Network\nthat at test time rapidly directs samples to the appropriate student to\nmaintain both high retrieval performance and high computational efficiency. We\ntrain several students with different architectures and arrive at different\ntrade-offs of performance and efficiency, i.e., speed and storage requirements,\nincluding fine-grained students that store index videos using binary\nrepresentations. Importantly, the proposed scheme allows Knowledge Distillation\nin large, unlabelled datasets -- this leads to good students. We evaluate DnS\non five public datasets on three different video retrieval tasks and\ndemonstrate a) that our students achieve state-of-the-art performance in\nseveral cases and b) that our DnS framework provides an excellent trade-off\nbetween retrieval performance, computational speed, and storage space. In\nspecific configurations, our method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. Our collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 18:34:24 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kordopatis-Zilos", "Giorgos", ""], ["Tzelepis", "Christos", ""], ["Papadopoulos", "Symeon", ""], ["Kompatsiaris", "Ioannis", ""], ["Patras", "Ioannis", ""]]}, {"id": "2106.13272", "submitter": "Anoop Cherian", "authors": "Anoop Cherian and Jue Wang", "title": "Generalized One-Class Learning Using Pairs of Complementary Classifiers", "comments": "Accepted at Trans. PAMI. arXiv admin note: text overlap with\n  arXiv:1908.05884", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One-class learning is the classic problem of fitting a model to the data for\nwhich annotations are available only for a single class. In this paper, we\nexplore novel objectives for one-class learning, which we collectively refer to\nas Generalized One-class Discriminative Subspaces (GODS). Our key idea is to\nlearn a pair of complementary classifiers to flexibly bound the one-class data\ndistribution, where the data belongs to the positive half-space of one of the\nclassifiers in the complementary pair and to the negative half-space of the\nother. To avoid redundancy while allowing non-linearity in the classifier\ndecision surfaces, we propose to design each classifier as an orthonormal frame\nand seek to learn these frames via jointly optimizing for two conflicting\nobjectives, namely: i) to minimize the distance between the two frames, and ii)\nto maximize the margin between the frames and the data. The learned orthonormal\nframes will thus characterize a piecewise linear decision surface that allows\nfor efficient inference, while our objectives seek to bound the data within a\nminimal volume that maximizes the decision margin, thereby robustly capturing\nthe data distribution. We explore several variants of our formulation under\ndifferent constraints on the constituent classifiers, including kernelized\nfeature maps. We demonstrate the empirical benefits of our approach via\nexperiments on data from several applications in computer vision, such as\nanomaly detection in video sequences, human poses, and human activities. We\nalso explore the generality and effectiveness of GODS for non-vision tasks via\nexperiments on several UCI datasets, demonstrating state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 18:52:05 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Cherian", "Anoop", ""], ["Wang", "Jue", ""]]}, {"id": "2106.13292", "submitter": "Xiao Liu", "authors": "Xiao Liu, Spyridon Thermos, Alison O'Neil, Sotirios A. Tsaftaris", "title": "Semi-supervised Meta-learning with Disentanglement for\n  Domain-generalised Medical Image Segmentation", "comments": "Accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalising deep models to new data from new centres (termed here domains)\nremains a challenge. This is largely attributed to shifts in data statistics\n(domain shifts) between source and unseen domains. Recently, gradient-based\nmeta-learning approaches where the training data are split into meta-train and\nmeta-test sets to simulate and handle the domain shifts during training have\nshown improved generalisation performance. However, the current fully\nsupervised meta-learning approaches are not scalable for medical image\nsegmentation, where large effort is required to create pixel-wise annotations.\nMeanwhile, in a low data regime, the simulated domain shifts may not\napproximate the true domain shifts well across source and unseen domains. To\naddress this problem, we propose a novel semi-supervised meta-learning\nframework with disentanglement. We explicitly model the representations related\nto domain shifts. Disentangling the representations and combining them to\nreconstruct the input image allows unlabeled data to be used to better\napproximate the true domain shifts for meta-learning. Hence, the model can\nachieve better generalisation performance, especially when there is a limited\namount of labeled data. Experiments show that the proposed method is robust on\ndifferent segmentation tasks and achieves state-of-the-art generalisation\nperformance on two public benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 19:50:07 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Liu", "Xiao", ""], ["Thermos", "Spyridon", ""], ["O'Neil", "Alison", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "2106.13299", "submitter": "George Drettakis", "authors": "Julien Philip and S\\'ebastien Morgenthaler and Micha\\\"el Gharbi and\n  George Drettakis", "title": "Free-viewpoint Indoor Neural Relighting from Multi-view Stereo", "comments": null, "journal-ref": "ACM Transactions on Graphics (2021)", "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a neural relighting algorithm for captured indoors scenes, that\nallows interactive free-viewpoint navigation. Our method allows illumination to\nbe changed synthetically, while coherently rendering cast shadows and complex\nglossy materials. We start with multiple images of the scene and a 3D mesh\nobtained by multi-view stereo (MVS) reconstruction. We assume that lighting is\nwell-explained as the sum of a view-independent diffuse component and a\nview-dependent glossy term concentrated around the mirror reflection direction.\nWe design a convolutional network around input feature maps that facilitate\nlearning of an implicit representation of scene materials and illumination,\nenabling both relighting and free-viewpoint navigation. We generate these input\nmaps by exploiting the best elements of both image-based and physically-based\nrendering. We sample the input views to estimate diffuse scene irradiance, and\ncompute the new illumination caused by user-specified light sources using path\ntracing. To facilitate the network's understanding of materials and synthesize\nplausible glossy reflections, we reproject the views and compute mirror images.\nWe train the network on a synthetic dataset where each scene is also\nreconstructed with MVS. We show results of our algorithm relighting real indoor\nscenes and performing free-viewpoint navigation with complex and realistic\nglossy reflections, which so far remained out of reach for view-synthesis\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 20:09:40 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Philip", "Julien", ""], ["Morgenthaler", "S\u00e9bastien", ""], ["Gharbi", "Micha\u00ebl", ""], ["Drettakis", "George", ""]]}, {"id": "2106.13301", "submitter": "Beatriz Moya", "authors": "Beatriz Moya, Alberto Badias, David Gonzalez, Francisco Chinesta,\n  Elias Cueto", "title": "Physics perception in sloshing scenes with guaranteed thermodynamic\n  consistency", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics perception very often faces the problem that only limited data or\npartial measurements on the scene are available. In this work, we propose a\nstrategy to learn the full state of sloshing liquids from measurements of the\nfree surface. Our approach is based on recurrent neural networks (RNN) that\nproject the limited information available to a reduced-order manifold so as to\nnot only reconstruct the unknown information, but also to be capable of\nperforming fluid reasoning about future scenarios in real time. To obtain\nphysically consistent predictions, we train deep neural networks on the\nreduced-order manifold that, through the employ of inductive biases, ensure the\nfulfillment of the principles of thermodynamics. RNNs learn from history the\nrequired hidden information to correlate the limited information with the\nlatent space where the simulation occurs. Finally, a decoder returns data back\nto the high-dimensional manifold, so as to provide the user with insightful\ninformation in the form of augmented reality. This algorithm is connected to a\ncomputer vision system to test the performance of the proposed methodology with\nreal information, resulting in a system capable of understanding and predicting\nfuture states of the observed fluid in real-time.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 20:13:56 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Moya", "Beatriz", ""], ["Badias", "Alberto", ""], ["Gonzalez", "David", ""], ["Chinesta", "Francisco", ""], ["Cueto", "Elias", ""]]}, {"id": "2106.13315", "submitter": "Angela Gao", "authors": "Angela F. Gao, Brandon Rasmussen, Peter Kulits, Eva L. Scheller,\n  Rebecca Greenberger, Bethany L. Ehlmann", "title": "Generalized Unsupervised Clustering of Hyperspectral Images of\n  Geological Targets in the Near Infrared", "comments": "10 pages, 4 figures. Accepted, CVPR PBVS Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of infrared hyperspectral imagery to geological problems is\nbecoming more popular as data become more accessible and cost-effective.\nClustering and classifying spectrally similar materials is often a first step\nin applications ranging from economic mineral exploration on Earth to planetary\nexploration on Mars. Semi-manual classification guided by expertly developed\nspectral parameters can be time consuming and biased, while supervised methods\nrequire abundant labeled data and can be difficult to generalize. Here we\ndevelop a fully unsupervised workflow for feature extraction and clustering\ninformed by both expert spectral geologist input and quantitative metrics. Our\npipeline uses a lightweight autoencoder followed by Gaussian mixture modeling\nto map the spectral diversity within any image. We validate the performance of\nour pipeline at submillimeter-scale with expert-labelled data from the Oman\nophiolite drill core and evaluate performance at meters-scale with partially\nclassified orbital data of Jezero Crater on Mars (the landing site for the\nPerseverance rover). We additionally examine the effects of various\npreprocessing techniques used in traditional analysis of hyperspectral imagery.\nThis pipeline provides a fast and accurate clustering map of similar geological\nmaterials and consistently identifies and separates major mineral classes in\nboth laboratory imagery and remote sensing imagery. We refer to our pipeline as\n\"Generalized Pipeline for Spectroscopic Unsupervised clustering of Minerals\n(GyPSUM).\"\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 21:05:10 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Gao", "Angela F.", ""], ["Rasmussen", "Brandon", ""], ["Kulits", "Peter", ""], ["Scheller", "Eva L.", ""], ["Greenberger", "Rebecca", ""], ["Ehlmann", "Bethany L.", ""]]}, {"id": "2106.13323", "submitter": "George Worrall", "authors": "George Worrall and Anand Rangarajan and Jasmeet Judge", "title": "Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth\n  Estimation", "comments": "7 pages, 7 tables, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced machine learning techniques have been used in remote sensing (RS)\napplications such as crop mapping and yield prediction, but remain\nunder-utilized for tracking crop progress. In this study, we demonstrate the\nuse of agronomic knowledge of crop growth drivers in a Long Short-Term\nMemory-based, Domain-guided neural network (DgNN) for in-season crop progress\nestimation. The DgNN uses a branched structure and attention to separate\nindependent crop growth drivers and capture their varying importance throughout\nthe growing season. The DgNN is implemented for corn, using RS data in Iowa for\nthe period 2003-2019, with USDA crop progress reports used as ground truth.\nState-wide DgNN performance shows significant improvement over sequential and\ndense-only NN structures, and a widely-used Hidden Markov Model method. The\nDgNN had a 3.5% higher Nash-Sutfliffe efficiency over all growth stages and 33%\nmore weeks with highest cosine similarity than the other NNs during test years.\nThe DgNN and Sequential NN were more robust during periods of abnormal crop\nprogress, though estimating the Silking-Grainfill transition was difficult for\nall methods. Finally, Uniform Manifold Approximation and Projection\nvisualizations of layer activations showed how LSTM-based NNs separate crop\ngrowth time-series differently from a dense-only structure. Results from this\nstudy exhibit both the viability of NNs in crop growth stage estimation (CGSE)\nand the benefits of using domain knowledge. The DgNN methodology presented here\ncan be extended to provide near-real time CGSE of other crops.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 21:21:35 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Worrall", "George", ""], ["Rangarajan", "Anand", ""], ["Judge", "Jasmeet", ""]]}, {"id": "2106.13328", "submitter": "Yize Jin", "authors": "Yize Jin, Anjul Patney, Richard Webb, Alan Bovik", "title": "FOVQA: Blind Foveated Video Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous blind or No Reference (NR) video quality assessment (VQA) models\nlargely rely on features drawn from natural scene statistics (NSS), but under\nthe assumption that the image statistics are stationary in the spatial domain.\nSeveral of these models are quite successful on standard pictures. However, in\nVirtual Reality (VR) applications, foveated video compression is regaining\nattention, and the concept of space-variant quality assessment is of interest,\ngiven the availability of increasingly high spatial and temporal resolution\ncontents and practical ways of measuring gaze direction. Distortions from\nfoveated video compression increase with increased eccentricity, implying that\nthe natural scene statistics are space-variant. Towards advancing the\ndevelopment of foveated compression / streaming algorithms, we have devised a\nno-reference (NR) foveated video quality assessment model, called FOVQA, which\nis based on new models of space-variant natural scene statistics (NSS) and\nnatural video statistics (NVS). Specifically, we deploy a space-variant\ngeneralized Gaussian distribution (SV-GGD) model and a space-variant\nasynchronous generalized Gaussian distribution (SV-AGGD) model of mean\nsubtracted contrast normalized (MSCN) coefficients and products of neighboring\nMSCN coefficients, respectively. We devise a foveated video quality predictor\nthat extracts radial basis features, and other features that capture\nperceptually annoying rapid quality fall-offs. We find that FOVQA achieves\nstate-of-the-art (SOTA) performance on the new 2D LIVE-FBT-FCVR database, as\ncompared with other leading FIQA / VQA models. we have made our implementation\nof FOVQA available at: http://live.ece.utexas.edu/research/Quality/FOVQA.zip.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 21:38:22 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Jin", "Yize", ""], ["Patney", "Anjul", ""], ["Webb", "Richard", ""], ["Bovik", "Alan", ""]]}, {"id": "2106.13364", "submitter": "Daniel McDuff", "authors": "Daniel McDuff, Yale Song, Jiyoung Lee, Vibhav Vineet, Sai Vemprala,\n  Nicholas Gyde, Hadi Salman, Shuang Ma, Kwanghoon Sohn and Ashish Kapoor", "title": "CausalCity: Complex Simulations with Agency for Causal Discovery and\n  Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perform causal and counterfactual reasoning are central\nproperties of human intelligence. Decision-making systems that can perform\nthese types of reasoning have the potential to be more generalizable and\ninterpretable. Simulations have helped advance the state-of-the-art in this\ndomain, by providing the ability to systematically vary parameters (e.g.,\nconfounders) and generate examples of the outcomes in the case of\ncounterfactual scenarios. However, simulating complex temporal causal events in\nmulti-agent scenarios, such as those that exist in driving and vehicle\nnavigation, is challenging. To help address this, we present a high-fidelity\nsimulation environment that is designed for developing algorithms for causal\ndiscovery and counterfactual reasoning in the safety-critical context. A core\ncomponent of our work is to introduce \\textit{agency}, such that it is simple\nto define and create complex scenarios using high-level definitions. The\nvehicles then operate with agency to complete these objectives, meaning\nlow-level behaviors need only be controlled if necessary. We perform\nexperiments with three state-of-the-art methods to create baselines and\nhighlight the affordances of this environment. Finally, we highlight challenges\nand opportunities for future work.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 00:21:41 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["McDuff", "Daniel", ""], ["Song", "Yale", ""], ["Lee", "Jiyoung", ""], ["Vineet", "Vibhav", ""], ["Vemprala", "Sai", ""], ["Gyde", "Nicholas", ""], ["Salman", "Hadi", ""], ["Ma", "Shuang", ""], ["Sohn", "Kwanghoon", ""], ["Kapoor", "Ashish", ""]]}, {"id": "2106.13365", "submitter": "Pei Sun", "authors": "Pei Sun, Weiyue Wang, Yuning Chai, Gamaleldin Elsayed, Alex Bewley,\n  Xiao Zhang, Cristian Sminchisescu, Dragomir Anguelov", "title": "RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The detection of 3D objects from LiDAR data is a critical component in most\nautonomous driving systems. Safe, high speed driving needs larger detection\nranges, which are enabled by new LiDARs. These larger detection ranges require\nmore efficient and accurate detection models. Towards this goal, we propose\nRange Sparse Net (RSN), a simple, efficient, and accurate 3D object detector in\norder to tackle real time 3D object detection in this extended detection\nregime. RSN predicts foreground points from range images and applies sparse\nconvolutions on the selected foreground points to detect objects. The\nlightweight 2D convolutions on dense range images results in significantly\nfewer selected foreground points, thus enabling the later sparse convolutions\nin RSN to efficiently operate. Combining features from the range image further\nenhance detection accuracy. RSN runs at more than 60 frames per second on a\n150m x 150m detection region on Waymo Open Dataset (WOD) while being more\naccurate than previously published detectors. As of 11/2020, RSN is ranked\nfirst in the WOD leaderboard based on the APH/LEVEL 1 metrics for LiDAR-based\npedestrian and vehicle detection, while being several times faster than\nalternatives.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 00:23:55 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Sun", "Pei", ""], ["Wang", "Weiyue", ""], ["Chai", "Yuning", ""], ["Elsayed", "Gamaleldin", ""], ["Bewley", "Alex", ""], ["Zhang", "Xiao", ""], ["Sminchisescu", "Cristian", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "2106.13381", "submitter": "Pei Sun", "authors": "Yuning Chai, Pei Sun, Jiquan Ngiam, Weiyue Wang, Benjamin Caine, Vijay\n  Vasudevan, Xiao Zhang, Dragomir Anguelov", "title": "To the Point: Efficient 3D Object Detection in the Range Image with\n  Graph Convolution Kernels", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  3D object detection is vital for many robotics applications. For tasks where\na 2D perspective range image exists, we propose to learn a 3D representation\ndirectly from this range image view. To this end, we designed a 2D\nconvolutional network architecture that carries the 3D spherical coordinates of\neach pixel throughout the network. Its layers can consume any arbitrary\nconvolution kernel in place of the default inner product kernel and exploit the\nunderlying local geometry around each pixel. We outline four such kernels: a\ndense kernel according to the bag-of-words paradigm, and three graph kernels\ninspired by recent graph neural network advances: the Transformer, the\nPointNet, and the Edge Convolution. We also explore cross-modality fusion with\nthe camera image, facilitated by operating in the perspective range image view.\nOur method performs competitively on the Waymo Open Dataset and improves the\nstate-of-the-art AP for pedestrian detection from 69.7% to 75.5%. It is also\nefficient in that our smallest model, which still outperforms the popular\nPointPillars in quality, requires 180 times fewer FLOPS and model parameters\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 01:27:26 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Chai", "Yuning", ""], ["Sun", "Pei", ""], ["Ngiam", "Jiquan", ""], ["Wang", "Weiyue", ""], ["Caine", "Benjamin", ""], ["Vasudevan", "Vijay", ""], ["Zhang", "Xiao", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "2106.13387", "submitter": "Qiang Ji", "authors": "Qiang Ji and Kang Wang", "title": "Bayesian Eye Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Model-based eye tracking has been a dominant approach for eye gaze tracking\nbecause of its ability to generalize to different subjects, without the need of\nany training data and eye gaze annotations. Model-based eye tracking, however,\nis susceptible to eye feature detection errors, in particular for eye tracking\nin the wild. To address this issue, we propose a Bayesian framework for\nmodel-based eye tracking. The proposed system consists of a cascade-Bayesian\nConvolutional Neural Network (c-BCNN) to capture the probabilistic\nrelationships between eye appearance and its landmarks, and a geometric eye\nmodel to estimate eye gaze from the eye landmarks. Given a testing eye image,\nthe Bayesian framework can generate, through Bayesian inference, the eye gaze\ndistribution without explicit landmark detection and model training, based on\nwhich it not only estimates the most likely eye gaze but also its uncertainty.\nFurthermore, with Bayesian inference instead of point-based inference, our\nmodel can not only generalize better to different sub-jects, head poses, and\nenvironments but also is robust to image noise and landmark detection errors.\nFinally, with the estimated gaze uncertainty, we can construct a cascade\narchitecture that allows us to progressively improve gaze estimation accuracy.\nCompared to state-of-the-art model-based and learning-based methods, the\nproposed Bayesian framework demonstrates significant improvement in\ngeneralization capability across several benchmark datasets and in accuracy and\nrobustness under challenging real-world conditions.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:08:03 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Ji", "Qiang", ""], ["Wang", "Kang", ""]]}, {"id": "2106.13389", "submitter": "Jing Zhang", "authors": "Jing Zhang and Jianwen Xie and Zilong Zheng and Nick Barnes", "title": "Energy-Based Generative Cooperative Saliency Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional saliency prediction models typically learn a deterministic\nmapping from images to the corresponding ground truth saliency maps. In this\npaper, we study the saliency prediction problem from the perspective of\ngenerative models by learning a conditional probability distribution over\nsaliency maps given an image, and treating the prediction as a sampling\nprocess. Specifically, we propose a generative cooperative saliency prediction\nframework based on the generative cooperative networks, where a conditional\nlatent variable model and a conditional energy-based model are jointly trained\nto predict saliency in a cooperative manner. We call our model the SalCoopNets.\nThe latent variable model serves as a fast but coarse predictor to efficiently\nproduce an initial prediction, which is then refined by the iterative Langevin\nrevision of the energy-based model that serves as a fine predictor. Such a\ncoarse-to-fine cooperative saliency prediction strategy offers the best of both\nworlds. Moreover, we generalize our framework to the scenario of weakly\nsupervised saliency prediction, where saliency annotation of training images is\npartially observed, by proposing a cooperative learning while recovering\nstrategy. Lastly, we show that the learned energy function can serve as a\nrefinement module that can refine the results of other pre-trained saliency\nprediction models. Experimental results show that our generative model can\nachieve state-of-the-art performance. Our code is publicly available at:\n\\url{https://github.com/JingZhang617/SalCoopNets}.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:11:50 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Zhang", "Jing", ""], ["Xie", "Jianwen", ""], ["Zheng", "Zilong", ""], ["Barnes", "Nick", ""]]}, {"id": "2106.13391", "submitter": "Jianbo Liu", "authors": "Jianbo Liu, Ying Wang, Shiming Xiang, Chunhong Pan", "title": "HAN: An Efficient Hierarchical Self-Attention Network for Skeleton-Based\n  Gesture Recognition", "comments": "Under peer review for TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous methods for skeleton-based gesture recognition mostly arrange the\nskeleton sequence into a pseudo picture or spatial-temporal graph and apply\ndeep Convolutional Neural Network (CNN) or Graph Convolutional Network (GCN)\nfor feature extraction. Although achieving superior results, these methods have\ninherent limitations in dynamically capturing local features of interactive\nhand parts, and the computing efficiency still remains a serious issue. In this\nwork, the self-attention mechanism is introduced to alleviate this problem.\nConsidering the hierarchical structure of hand joints, we propose an efficient\nhierarchical self-attention network (HAN) for skeleton-based gesture\nrecognition, which is based on pure self-attention without any CNN, RNN or GCN\noperators. Specifically, the joint self-attention module is used to capture\nspatial features of fingers, the finger self-attention module is designed to\naggregate features of the whole hand. In terms of temporal features, the\ntemporal self-attention module is utilized to capture the temporal dynamics of\nthe fingers and the entire hand. Finally, these features are fused by the\nfusion self-attention module for gesture classification. Experiments show that\nour method achieves competitive results on three gesture recognition datasets\nwith much lower computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:15:53 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Liu", "Jianbo", ""], ["Wang", "Ying", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "2106.13393", "submitter": "Xiaofeng Liu", "authors": "Wanqing Xie, Lizhong Liang, Yao Lu, Chen Wang, Jihong Shen, Hui Luo,\n  Xiaofeng Liu", "title": "Interpreting Depression From Question-wise Long-term Video Recording of\n  SDS Evaluation", "comments": "Published in IEEE Journal of Biomedical and Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-Rating Depression Scale (SDS) questionnaire has frequently been used for\nefficient depression preliminary screening. However, the uncontrollable\nself-administered measure can be easily affected by insouciantly or deceptively\nanswering, and producing the different results with the clinician-administered\nHamilton Depression Rating Scale (HDRS) and the final diagnosis. Clinically,\nfacial expression (FE) and actions play a vital role in clinician-administered\nevaluation, while FE and action are underexplored for self-administered\nevaluations. In this work, we collect a novel dataset of 200 subjects to\nevidence the validity of self-rating questionnaires with their corresponding\nquestion-wise video recording. To automatically interpret depression from the\nSDS evaluation and the paired video, we propose an end-to-end hierarchical\nframework for the long-term variable-length video, which is also conditioned on\nthe questionnaire results and the answering time. Specifically, we resort to a\nhierarchical model which utilizes a 3D CNN for local temporal pattern\nexploration and a redundancy-aware self-attention (RAS) scheme for\nquestion-wise global feature aggregation. Targeting for the redundant long-term\nFE video processing, our RAS is able to effectively exploit the correlations of\neach video clip within a question set to emphasize the discriminative\ninformation and eliminate the redundancy based on feature pair-wise affinity.\nThen, the question-wise video feature is concatenated with the questionnaire\nscores for final depression detection. Our thorough evaluations also show the\nvalidity of fusing SDS evaluation and its video recording, and the superiority\nof our framework to the conventional state-of-the-art temporal modeling\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:32:13 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Xie", "Wanqing", ""], ["Liang", "Lizhong", ""], ["Lu", "Yao", ""], ["Wang", "Chen", ""], ["Shen", "Jihong", ""], ["Luo", "Hui", ""], ["Liu", "Xiaofeng", ""]]}, {"id": "2106.13394", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Pan Gao", "title": "Countering Adversarial Examples: Combining Input Transformation and\n  Noisy Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies have shown that neural network (NN) based image classifiers\nare highly vulnerable to adversarial examples, which poses a threat to\nsecurity-sensitive image recognition task. Prior work has shown that JPEG\ncompression can combat the drop in classification accuracy on adversarial\nexamples to some extent. But, as the compression ratio increases, traditional\nJPEG compression is insufficient to defend those attacks but can cause an\nabrupt accuracy decline to the benign images. In this paper, with the aim of\nfully filtering the adversarial perturbations, we firstly make modifications to\ntraditional JPEG compression algorithm which becomes more favorable for NN.\nSpecifically, based on an analysis of the frequency coefficient, we design a\nNN-favored quantization table for compression. Considering compression as a\ndata augmentation strategy, we then combine our model-agnostic preprocess with\nnoisy training. We fine-tune the pre-trained model by training with images\nencoded at different compression levels, thus generating multiple classifiers.\nFinally, since lower (higher) compression ratio can remove both perturbations\nand original features slightly (aggressively), we use these trained multiple\nmodels for model ensemble. The majority vote of the ensemble of models is\nadopted as final predictions. Experiments results show our method can improve\ndefense efficiency while maintaining original accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:46:52 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Zhang", "Cheng", ""], ["Gao", "Pan", ""]]}, {"id": "2106.13409", "submitter": "Zhipeng Bao", "authors": "Zhipeng Bao, Martial Hebert, Yu-Xiong Wang", "title": "Generative Modeling for Multi-task Visual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling has recently shown great promise in computer vision, but\nit has mostly focused on synthesizing visually realistic images. In this paper,\nmotivated by multi-task learning of shareable feature representations, we\nconsider a novel problem of learning a shared generative model that is useful\nacross various visual perception tasks. Correspondingly, we propose a general\nmulti-task oriented generative modeling (MGM) framework, by coupling a\ndiscriminative multi-task network with a generative network. While it is\nchallenging to synthesize both RGB images and pixel-level annotations in\nmulti-task scenarios, our framework enables us to use synthesized images paired\nwith only weak annotations (i.e., image-level scene labels) to facilitate\nmultiple visual tasks. Experimental evaluation on challenging multi-task\nbenchmarks, including NYUv2 and Taskonomy, demonstrates that our MGM framework\nimproves the performance of all the tasks by large margins, consistently\noutperforming state-of-the-art multi-task approaches.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 03:42:59 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Bao", "Zhipeng", ""], ["Hebert", "Martial", ""], ["Wang", "Yu-Xiong", ""]]}, {"id": "2106.13415", "submitter": "Devendra Singh Chaplot", "authors": "Devendra Singh Chaplot", "title": "Building Intelligent Autonomous Navigation Agents", "comments": "CMU Ph.D. Thesis, March 2021. For more details see\n  http://devendrachaplot.github.io/", "journal-ref": null, "doi": null, "report-no": "CMU-ML-21-101", "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breakthroughs in machine learning in the last decade have led to `digital\nintelligence', i.e. machine learning models capable of learning from vast\namounts of labeled data to perform several digital tasks such as speech\nrecognition, face recognition, machine translation and so on. The goal of this\nthesis is to make progress towards designing algorithms capable of `physical\nintelligence', i.e. building intelligent autonomous navigation agents capable\nof learning to perform complex navigation tasks in the physical world involving\nvisual perception, natural language understanding, reasoning, planning, and\nsequential decision making. Despite several advances in classical navigation\nmethods in the last few decades, current navigation agents struggle at\nlong-term semantic navigation tasks. In the first part of the thesis, we\ndiscuss our work on short-term navigation using end-to-end reinforcement\nlearning to tackle challenges such as obstacle avoidance, semantic perception,\nlanguage grounding, and reasoning. In the second part, we present a new class\nof navigation methods based on modular learning and structured explicit map\nrepresentations, which leverage the strengths of both classical and end-to-end\nlearning methods, to tackle long-term navigation tasks. We show that these\nmethods are able to effectively tackle challenges such as localization,\nmapping, long-term planning, exploration and learning semantic priors. These\nmodular learning methods are capable of long-term spatial and semantic\nunderstanding and achieve state-of-the-art results on various navigation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 04:10:58 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Chaplot", "Devendra Singh", ""]]}, {"id": "2106.13416", "submitter": "Yuki Endo", "authors": "Yuki Endo, Yoshihiro Kanamori", "title": "Diversifying Semantic Image Synthesis and Editing via Class- and\n  Layer-wise VAEs", "comments": "Accepted to Pacific Graphics 2020, codes available at\n  https://github.com/endo-yuki-t/DiversifyingSMIS", "journal-ref": null, "doi": "10.1111/cgf.14164", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image synthesis is a process for generating photorealistic images\nfrom a single semantic mask. To enrich the diversity of multimodal image\nsynthesis, previous methods have controlled the global appearance of an output\nimage by learning a single latent space. However, a single latent code is often\ninsufficient for capturing various object styles because object appearance\ndepends on multiple factors. To handle individual factors that determine object\nstyles, we propose a class- and layer-wise extension to the variational\nautoencoder (VAE) framework that allows flexible control over each object class\nat the local to global levels by learning multiple latent spaces. Furthermore,\nwe demonstrate that our method generates images that are both plausible and\nmore diverse compared to state-of-the-art methods via extensive experiments\nwith real and synthetic datasets inthree different domains. We also show that\nour method enables a wide range of applications in image synthesis and editing\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 04:12:05 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 06:56:09 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Endo", "Yuki", ""], ["Kanamori", "Yoshihiro", ""]]}, {"id": "2106.13432", "submitter": "Long Dang", "authors": "Long Hoang Dang, Thao Minh Le, Vuong Le, Truyen Tran", "title": "Hierarchical Object-oriented Spatio-Temporal Reasoning for Video\n  Question Answering", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Question Answering (Video QA) is a powerful testbed to develop new AI\ncapabilities. This task necessitates learning to reason about objects,\nrelations, and events across visual and linguistic domains in space-time.\nHigh-level reasoning demands lifting from associative visual pattern\nrecognition to symbol-like manipulation over objects, their behavior and\ninteractions. Toward reaching this goal we propose an object-oriented reasoning\napproach in that video is abstracted as a dynamic stream of interacting\nobjects. At each stage of the video event flow, these objects interact with\neach other, and their interactions are reasoned about with respect to the query\nand under the overall context of a video. This mechanism is materialized into a\nfamily of general-purpose neural units and their multi-level architecture\ncalled Hierarchical Object-oriented Spatio-Temporal Reasoning (HOSTR) networks.\nThis neural model maintains the objects' consistent lifelines in the form of a\nhierarchically nested spatio-temporal graph. Within this graph, the dynamic\ninteractive object-oriented representations are built up along the video\nsequence, hierarchically abstracted in a bottom-up manner, and converge toward\nthe key information for the correct answer. The method is evaluated on multiple\nmajor Video QA datasets and establishes new state-of-the-arts in these tasks.\nAnalysis into the model's behavior indicates that object-oriented reasoning is\na reliable, interpretable and efficient approach to Video QA.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 05:12:42 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Dang", "Long Hoang", ""], ["Le", "Thao Minh", ""], ["Le", "Vuong", ""], ["Tran", "Truyen", ""]]}, {"id": "2106.13435", "submitter": "Xiaohui Zeng", "authors": "Xiaohui Zeng, Raquel Urtasun, Richard Zemel, Sanja Fidler, Renjie Liao", "title": "NP-DRAW: A Non-Parametric Structured Latent Variable Model for Image\n  Generation", "comments": "UAI2021, code at https://github.com/ZENGXH/NPDRAW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a non-parametric structured latent variable model\nfor image generation, called NP-DRAW, which sequentially draws on a latent\ncanvas in a part-by-part fashion and then decodes the image from the canvas.\nOur key contributions are as follows. 1) We propose a non-parametric prior\ndistribution over the appearance of image parts so that the latent variable\n``what-to-draw'' per step becomes a categorical random variable. This improves\nthe expressiveness and greatly eases the learning compared to Gaussians used in\nthe literature. 2) We model the sequential dependency structure of parts via a\nTransformer, which is more powerful and easier to train compared to RNNs used\nin the literature. 3) We propose an effective heuristic parsing algorithm to\npre-train the prior. Experiments on MNIST, Omniglot, CIFAR-10, and CelebA show\nthat our method significantly outperforms previous structured image models like\nDRAW and AIR and is competitive to other generic generative models. Moreover,\nwe show that our model's inherent compositionality and interpretability bring\nsignificant benefits in the low-data learning regime and latent space editing.\nCode is available at https://github.com/ZENGXH/NPDRAW.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 05:17:55 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 18:44:43 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zeng", "Xiaohui", ""], ["Urtasun", "Raquel", ""], ["Zemel", "Richard", ""], ["Fidler", "Sanja", ""], ["Liao", "Renjie", ""]]}, {"id": "2106.13445", "submitter": "Yusuke Hirota", "authors": "Yusuke Hirota, Noa Garcia, Mayu Otani, Chenhui Chu, Yuta Nakashima,\n  Ittetsu Taniguchi, Takao Onoye", "title": "A Picture May Be Worth a Hundred Words for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How far can we go with textual representations for understanding pictures? In\nimage understanding, it is essential to use concise but detailed image\nrepresentations. Deep visual features extracted by vision models, such as\nFaster R-CNN, are prevailing used in multiple tasks, and especially in visual\nquestion answering (VQA). However, conventional deep visual features may\nstruggle to convey all the details in an image as we humans do. Meanwhile, with\nrecent language models' progress, descriptive text may be an alternative to\nthis problem. This paper delves into the effectiveness of textual\nrepresentations for image understanding in the specific context of VQA. We\npropose to take description-question pairs as input, instead of deep visual\nfeatures, and fed them into a language-only Transformer model, simplifying the\nprocess and the computational cost. We also experiment with data augmentation\ntechniques to increase the diversity in the training set and avoid learning\nstatistical bias. Extensive evaluations have shown that textual representations\nrequire only about a hundred words to compete with deep visual features on both\nVQA 2.0 and VQA-CP v2.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 06:13:14 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Hirota", "Yusuke", ""], ["Garcia", "Noa", ""], ["Otani", "Mayu", ""], ["Chu", "Chenhui", ""], ["Nakashima", "Yuta", ""], ["Taniguchi", "Ittetsu", ""], ["Onoye", "Takao", ""]]}, {"id": "2106.13488", "submitter": "Hongwei Xue", "authors": "Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang\n  Li, Jiebo Luo", "title": "Probing Inter-modality: Visual Parsing with Self-Attention for\n  Vision-Language Pre-training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-Language Pre-training (VLP) aims to learn multi-modal representations\nfrom image-text pairs and serves for downstream vision-language tasks in a\nfine-tuning fashion. The dominant VLP models adopt a CNN-Transformer\narchitecture, which embeds images with a CNN, and then aligns images and text\nwith a Transformer. Visual relationship between visual contents plays an\nimportant role in image understanding and is the basic for inter-modal\nalignment learning. However, CNNs have limitations in visual relation learning\ndue to local receptive field's weakness in modeling long-range dependencies.\nThus the two objectives of learning visual relation and inter-modal alignment\nare encapsulated in the same Transformer network. Such design might restrict\nthe inter-modal alignment learning in the Transformer by ignoring the\nspecialized characteristic of each objective. To tackle this, we propose a\nfully Transformer visual embedding for VLP to better learn visual relation and\nfurther promote inter-modal alignment. Specifically, we propose a metric named\nInter-Modality Flow (IMF) to measure the interaction between vision and\nlanguage modalities (i.e., inter-modality). We also design a novel masking\noptimization mechanism named Masked Feature Regression (MFR) in Transformer to\nfurther promote the inter-modality learning. To the best of our knowledge, this\nis the first study to explore the benefit of Transformer for visual feature\nlearning in VLP. We verify our method on a wide range of vision-language tasks,\nincluding Image-Text Retrieval, Visual Question Answering (VQA), Visual\nEntailment and Visual Reasoning. Our approach not only outperforms the\nstate-of-the-art VLP performance, but also shows benefits on the IMF metric.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 08:04:25 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 04:42:48 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Xue", "Hongwei", ""], ["Huang", "Yupan", ""], ["Liu", "Bei", ""], ["Peng", "Houwen", ""], ["Fu", "Jianlong", ""], ["Li", "Houqiang", ""], ["Luo", "Jiebo", ""]]}, {"id": "2106.13497", "submitter": "Vignesh Srinivasan", "authors": "Vignesh Srinivasan, Nils Strodthoff, Jackie Ma, Alexander Binder,\n  Klaus-Robert M\\\"uller, Wojciech Samek", "title": "On the Robustness of Pretraining and Self-Supervision for a Deep\n  Learning-based Analysis of Diabetic Retinopathy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing number of medical use-cases where classification\nalgorithms based on deep neural networks reach performance levels that are\ncompetitive with human medical experts. To alleviate the challenges of small\ndataset sizes, these systems often rely on pretraining. In this work, we aim to\nassess the broader implications of these approaches. For diabetic retinopathy\ngrading as exemplary use case, we compare the impact of different training\nprocedures including recently established self-supervised pretraining methods\nbased on contrastive learning. To this end, we investigate different aspects\nsuch as quantitative performance, statistics of the learned feature\nrepresentations, interpretability and robustness to image distortions. Our\nresults indicate that models initialized from ImageNet pretraining report a\nsignificant increase in performance, generalization and robustness to image\ndistortions. In particular, self-supervised models show further benefits to\nsupervised models. Self-supervised models with initialization from ImageNet\npretraining not only report higher performance, they also reduce overfitting to\nlarge lesions along with improvements in taking into account minute lesions\nindicative of the progression of the disease. Understanding the effects of\npretraining in a broader sense that goes beyond simple performance comparisons\nis of crucial importance for the broader medical imaging community beyond the\nuse-case considered in this work.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 08:32:45 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Srinivasan", "Vignesh", ""], ["Strodthoff", "Nils", ""], ["Ma", "Jackie", ""], ["Binder", "Alexander", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "2106.13549", "submitter": "Damien Scieur", "authors": "Damien Scieur, Youngsung Kim", "title": "Connecting Sphere Manifolds Hierarchically for Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers classification problems with hierarchically organized\nclasses. We force the classifier (hyperplane) of each class to belong to a\nsphere manifold, whose center is the classifier of its super-class. Then,\nindividual sphere manifolds are connected based on their hierarchical\nrelations. Our technique replaces the last layer of a neural network by\ncombining a spherical fully-connected layer with a hierarchical layer. This\nregularization is shown to improve the performance of widely used deep neural\nnetwork architectures (ResNet and DenseNet) on publicly available datasets\n(CIFAR100, CUB200, Stanford dogs, Stanford cars, and Tiny-ImageNet).\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 10:51:36 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Scieur", "Damien", ""], ["Kim", "Youngsung", ""]]}, {"id": "2106.13551", "submitter": "Jos\\'e Gabriel Garc\\'ia Pardo", "authors": "Gabriel Garc\\'ia, Roc\\'io del Amor, Adri\\'an Colomer, Rafael\n  Verd\\'u-Monedero, Juan Morales-S\\'anchez and Valery Naranjo", "title": "Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using\n  Tailored Prototypical Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma is one of the leading causes of blindness worldwide and Optical\nCoherence Tomography (OCT) is the quintessential imaging technique for its\ndetection. Unlike most of the state-of-the-art studies focused on glaucoma\ndetection, in this paper, we propose, for the first time, a novel framework for\nglaucoma grading using raw circumpapillary B-scans. In particular, we set out a\nnew OCT-based hybrid network which combines hand-driven and deep learning\nalgorithms. An OCT-specific descriptor is proposed to extract hand-crafted\nfeatures related to the retinal nerve fibre layer (RNFL). In parallel, an\ninnovative CNN is developed using skip-connections to include tailored residual\nand attention modules to refine the automatic features of the latent space. The\nproposed architecture is used as a backbone to conduct a novel few-shot\nlearning based on static and dynamic prototypical networks. The k-shot paradigm\nis redefined giving rise to a supervised end-to-end system which provides\nsubstantial improvements discriminating between healthy, early and advanced\nglaucoma samples. The training and evaluation processes of the dynamic\nprototypical network are addressed from two fused databases acquired via\nHeidelberg Spectralis system. Validation and testing results reach a\ncategorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively.\nBesides, the high performance reported by the proposed model for glaucoma\ndetection deserves a special mention. The findings from the class activation\nmaps are directly in line with the clinicians' opinion since the heatmaps\npointed out the RNFL as the most relevant structure for glaucoma diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 10:53:01 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Garc\u00eda", "Gabriel", ""], ["del Amor", "Roc\u00edo", ""], ["Colomer", "Adri\u00e1n", ""], ["Verd\u00fa-Monedero", "Rafael", ""], ["Morales-S\u00e1nchez", "Juan", ""], ["Naranjo", "Valery", ""]]}, {"id": "2106.13552", "submitter": "Chen Xueying", "authors": "Xueying Chen, Rong Zhang, Yibing Zhan", "title": "Graph Pattern Loss based Diversified Attention Network for Cross-Modal\n  Retrieval", "comments": "5 pages, 3 figures", "journal-ref": "[1] Chen X , Zhang R , Zhan Y . Graph Pattern Loss Based\n  Diversified Attention Network For Cross-Modal Retrieval[C]// 2020 IEEE\n  International Conference on Image Processing (ICIP). IEEE, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-modal retrieval aims to enable flexible retrieval experience by\ncombining multimedia data such as image, video, text, and audio. One core of\nunsupervised approaches is to dig the correlations among different object\nrepresentations to complete satisfied retrieval performance without requiring\nexpensive labels. In this paper, we propose a Graph Pattern Loss based\nDiversified Attention Network(GPLDAN) for unsupervised cross-modal retrieval to\ndeeply analyze correlations among representations. First, we propose a\ndiversified attention feature projector by considering the interaction between\ndifferent representations to generate multiple representations of an instance.\nThen, we design a novel graph pattern loss to explore the correlations among\ndifferent representations, in this graph all possible distances between\ndifferent representations are considered. In addition, a modality classifier is\nadded to explicitly declare the corresponding modalities of features before\nfusion and guide the network to enhance discrimination ability. We test GPLDAN\non four public datasets. Compared with the state-of-the-art cross-modal\nretrieval methods, the experimental results demonstrate the performance and\ncompetitiveness of GPLDAN.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 10:53:07 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Chen", "Xueying", ""], ["Zhang", "Rong", ""], ["Zhan", "Yibing", ""]]}, {"id": "2106.13556", "submitter": "Yibao Sun", "authors": "Yibao Sun, Xingru Huang, Huiyu Zhou, Qianni Zhang", "title": "SRPN: similarity-based region proposal networks for nuclei and cells\n  detection in histology images", "comments": "Accepted by Medical Image Analysis for publication", "journal-ref": "Medical Image Analysis 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The detection of nuclei and cells in histology images is of great value in\nboth clinical practice and pathological studies. However, multiple reasons such\nas morphological variations of nuclei or cells make it a challenging task where\nconventional object detection methods cannot obtain satisfactory performance in\nmany cases. A detection task consists of two sub-tasks, classification and\nlocalization. Under the condition of dense object detection, classification is\na key to boost the detection performance. Considering this, we propose\nsimilarity based region proposal networks (SRPN) for nuclei and cells detection\nin histology images. In particular, a customized convolution layer termed as\nembedding layer is designed for network building. The embedding layer is added\ninto the region proposal networks, enabling the networks to learn\ndiscriminative features based on similarity learning. Features obtained by\nsimilarity learning can significantly boost the classification performance\ncompared to conventional methods. SRPN can be easily integrated into standard\nconvolutional neural networks architectures such as the Faster R-CNN and\nRetinaNet. We test the proposed approach on tasks of multi-organ nuclei\ndetection and signet ring cells detection in histological images. Experimental\nresults show that networks applying similarity learning achieved superior\nperformance on both tasks when compared to their counterparts. In particular,\nthe proposed SRPN achieve state-of-the-art performance on the MoNuSeg benchmark\nfor nuclei segmentation and detection while compared to previous methods, and\non the signet ring cell detection benchmark when compared with baselines. The\nsourcecode is publicly available at:\nhttps://github.com/sigma10010/nuclei_cells_det.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 10:56:54 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Sun", "Yibao", ""], ["Huang", "Xingru", ""], ["Zhou", "Huiyu", ""], ["Zhang", "Qianni", ""]]}, {"id": "2106.13559", "submitter": "Jos\\'e Gabriel Garc\\'ia Pardo", "authors": "Gabriel Garc\\'ia, Anna Esteve, Adri\\'an Colomer, David Ramos and\n  Valery Naranjo", "title": "A Novel Self-Learning Framework for Bladder Cancer Grading Using\n  Histopathological Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, bladder cancer has been significantly increased in terms of\nincidence and mortality. Currently, two subtypes are known based on tumour\ngrowth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC).\nIn this work, we focus on the MIBC subtype because it is of the worst prognosis\nand can spread to adjacent organs. We present a self-learning framework to\ngrade bladder cancer from histological images stained via immunohistochemical\ntechniques. Specifically, we propose a novel Deep Convolutional Embedded\nAttention Clustering (DCEAC) which allows classifying histological patches into\ndifferent severity levels of the disease, according to the patterns established\nin the literature. The proposed DCEAC model follows a two-step fully\nunsupervised learning methodology to discern between non-tumour, mild and\ninfiltrative patterns from high-resolution samples of 512x512 pixels. Our\nsystem outperforms previous clustering-based methods by including a\nconvolutional attention module, which allows refining the features of the\nlatent space before the classification stage. The proposed network exceeds\nstate-of-the-art approaches by 2-3% across different metrics, achieving a final\naverage accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported\nclass activation maps evidence that our model is able to learn by itself the\nsame patterns that clinicians consider relevant, without incurring prior\nannotation steps. This fact supposes a breakthrough in muscle-invasive bladder\ncancer grading which bridges the gap with respect to train the model on\nlabelled data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 11:04:04 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Garc\u00eda", "Gabriel", ""], ["Esteve", "Anna", ""], ["Colomer", "Adri\u00e1n", ""], ["Ramos", "David", ""], ["Naranjo", "Valery", ""]]}, {"id": "2106.13566", "submitter": "Sho Maeoki", "authors": "Sho Maeoki, Yusuke Mukuta, Tatsuya Harada", "title": "Video Moment Retrieval with Text Query Considering Many-to-Many\n  Correspondence Using Potentially Relevant Pair", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we undertake the task of text-based video moment retrieval from\na corpus of videos. To train the model, text-moment paired datasets were used\nto learn the correct correspondences. In typical training methods, ground-truth\ntext-moment pairs are used as positive pairs, whereas other pairs are regarded\nas negative pairs. However, aside from the ground-truth pairs, some text-moment\npairs should be regarded as positive. In this case, one text annotation can be\npositive for many video moments. Conversely, one video moment can be\ncorresponded to many text annotations. Thus, there are many-to-many\ncorrespondences between the text annotations and video moments. Based on these\ncorrespondences, we can form potentially relevant pairs, which are not given as\nground truth yet are not negative; effectively incorporating such relevant\npairs into training can improve the retrieval performance. The text query\nshould describe what is happening in a video moment. Hence, different video\nmoments annotated with similar texts, which contain a similar action, are\nlikely to hold the similar action, thus these pairs can be considered as\npotentially relevant pairs. In this paper, we propose a novel training method\nthat takes advantage of potentially relevant pairs, which are detected based on\nlinguistic analysis about text annotation. Experiments on two benchmark\ndatasets revealed that our method improves the retrieval performance both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 11:25:18 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Maeoki", "Sho", ""], ["Mukuta", "Yusuke", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2106.13574", "submitter": "Jaros{\\l}aw Samelak", "authors": "Jaros{\\l}aw Samelak, Marek Doma\\'nski", "title": "Multiview Video Compression Using Advanced HEVC Screen Content Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper presents a new approach to multiview video coding using Screen\nContent Coding. It is assumed that for a time instant the frames corresponding\nto all views are packed into a single frame, i.e. the frame-compatible approach\nto multiview coding is applied. For such coding scenario, the paper\ndemonstrates that Screen Content Coding can be efficiently used for multiview\nvideo coding. Two approaches are considered: the first using standard HEVC\nScreen Content Coding, and the second using Advanced Screen Content Coding. The\nlatter is the original proposal of the authors that exploits quarter-pel motion\nvectors and other nonstandard extensions of HEVC Screen Content Coding. The\nexperimental results demonstrate that multiview video coding even using\nstandard HEVC Screen Content Coding is much more efficient than simulcast HEVC\ncoding. The proposed Advanced Screen Content Coding provides virtually the same\ncoding efficiency as MV-HEVC, which is the state-of-the-art multiview video\ncompression technique. The authors suggest that Advanced Screen Content Coding\ncan be efficiently used within the new Versatile Video Coding (VVC) technology.\nNevertheless a reference multiview extension of VVC does not exist yet,\ntherefore, for VVC-based coding, the experimental comparisons are left for\nfuture work.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 11:53:48 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Samelak", "Jaros\u0142aw", ""], ["Doma\u0144ski", "Marek", ""]]}, {"id": "2106.13603", "submitter": "Lorenzo Berlincioni", "authors": "Francesco Bongini, Lorenzo Berlincioni, Marco Bertini, Alberto Del\n  Bimbo", "title": "Partially fake it till you make it: mixing real and fake thermal images\n  for improved object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper we propose a novel data augmentation approach for visual\ncontent domains that have scarce training datasets, compositing synthetic 3D\nobjects within real scenes. We show the performance of the proposed system in\nthe context of object detection in thermal videos, a domain where 1) training\ndatasets are very limited compared to visible spectrum datasets and 2) creating\nfull realistic synthetic scenes is extremely cumbersome and expensive due to\nthe difficulty in modeling the thermal properties of the materials of the\nscene. We compare different augmentation strategies, including state of the art\napproaches obtained through RL techniques, the injection of simulated data and\nthe employment of a generative model, and study how to best combine our\nproposed augmentation with these other techniques.Experimental results\ndemonstrate the effectiveness of our approach, and our single-modality detector\nachieves state-of-the-art results on the FLIR ADAS dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 12:56:09 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Bongini", "Francesco", ""], ["Berlincioni", "Lorenzo", ""], ["Bertini", "Marco", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2106.13629", "submitter": "Jianchuan Chen", "authors": "Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Huchuan\n  Lu", "title": "Animatable Neural Radiance Fields from Monocular RGB Video", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present animatable neural radiance fields for detailed human avatar\ncreation from monocular videos. Our approach extends neural radiance fields\n(NeRF) to the dynamic scenes with human movements via introducing explicit\npose-guided deformation while learning the scene representation network. In\nparticular, we estimate the human pose for each frame and learn a constant\ncanonical space for the detailed human template, which enables natural shape\ndeformation from the observation space to the canonical space under the\nexplicit control of the pose parameters. To compensate for inaccurate pose\nestimation, we introduce the pose refinement strategy that updates the initial\npose during the learning process, which not only helps to learn more accurate\nhuman reconstruction but also accelerates the convergence. In experiments we\nshow that the proposed approach achieves 1) implicit human geometry and\nappearance reconstruction with high-quality details, 2) photo-realistic\nrendering of the human from arbitrary views, and 3) animation of the human with\narbitrary poses.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 13:32:23 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Chen", "Jianchuan", ""], ["Zhang", "Ying", ""], ["Kang", "Di", ""], ["Zhe", "Xuefei", ""], ["Bao", "Linchao", ""], ["Lu", "Huchuan", ""]]}, {"id": "2106.13679", "submitter": "Giovanni Trappolini", "authors": "Giovanni Trappolini, Luca Cosmo, Luca Moschella, Riccardo Marin,\n  Simone Melzi, Emanuele Rodol\\`a", "title": "Shape registration in the time of transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a transformer-based procedure for the efficient\nregistration of non-rigid 3D point clouds. The proposed approach is data-driven\nand adopts for the first time the transformer architecture in the registration\ntask. Our method is general and applies to different settings. Given a fixed\ntemplate with some desired properties (e.g. skinning weights or other animation\ncues), we can register raw acquired data to it, thereby transferring all the\ntemplate properties to the input geometry. Alternatively, given a pair of\nshapes, our method can register the first onto the second (or vice-versa),\nobtaining a high-quality dense correspondence between the two. In both\ncontexts, the quality of our results enables us to target real applications\nsuch as texture transfer and shape interpolation. Furthermore, we also show\nthat including an estimation of the underlying density of the surface eases the\nlearning process. By exploiting the potential of this architecture, we can\ntrain our model requiring only a sparse set of ground truth correspondences\n($10\\sim20\\%$ of the total points). The proposed model and the analysis that we\nperform pave the way for future exploration of transformer-based architectures\nfor registration and matching applications. Qualitative and quantitative\nevaluations demonstrate that our pipeline outperforms state-of-the-art methods\nfor deformable and unordered 3D data registration on different datasets and\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:02:30 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 07:56:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Trappolini", "Giovanni", ""], ["Cosmo", "Luca", ""], ["Moschella", "Luca", ""], ["Marin", "Riccardo", ""], ["Melzi", "Simone", ""], ["Rodol\u00e0", "Emanuele", ""]]}, {"id": "2106.13689", "submitter": "Noorul Wahab", "authors": "Noorul Wahab, Islam M Miligy, Katherine Dodd, Harvir Sahota, Michael\n  Toss, Wenqi Lu, Mostafa Jahanifar, Mohsin Bilal, Simon Graham, Young Park,\n  Giorgos Hadjigeorghiou, Abhir Bhalerao, Ayat Lashen, Asmaa Ibrahim, Ayaka\n  Katayama, Henry O Ebili, Matthew Parkin, Tom Sorell, Shan E Ahmed Raza, Emily\n  Hero, Hesham Eldaly, Yee Wah Tsang, Kishore Gopalakrishnan, David Snead, Emad\n  Rakha, Nasir Rajpoot, Fayyaz Minhas", "title": "Semantic annotation for computational pathology: Multidisciplinary\n  experience and best practice recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in whole slide imaging (WSI) technology have led to the\ndevelopment of a myriad of computer vision and artificial intelligence (AI)\nbased diagnostic, prognostic, and predictive algorithms. Computational\nPathology (CPath) offers an integrated solution to utilize information embedded\nin pathology WSIs beyond what we obtain through visual assessment. For\nautomated analysis of WSIs and validation of machine learning (ML) models,\nannotations at the slide, tissue and cellular levels are required. The\nannotation of important visual constructs in pathology images is an important\ncomponent of CPath projects. Improper annotations can result in algorithms\nwhich are hard to interpret and can potentially produce inaccurate and\ninconsistent results. Despite the crucial role of annotations in CPath\nprojects, there are no well-defined guidelines or best practices on how\nannotations should be carried out. In this paper, we address this shortcoming\nby presenting the experience and best practices acquired during the execution\nof a large-scale annotation exercise involving a multidisciplinary team of\npathologists, ML experts and researchers as part of the Pathology image data\nLake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a\nreal-world case study along with examples of different types of annotations,\ndiagnostic algorithm, annotation data dictionary and annotation constructs. The\nanalyses reported in this work highlight best practice recommendations that can\nbe used as annotation guidelines over the lifecycle of a CPath project.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:15:17 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wahab", "Noorul", ""], ["Miligy", "Islam M", ""], ["Dodd", "Katherine", ""], ["Sahota", "Harvir", ""], ["Toss", "Michael", ""], ["Lu", "Wenqi", ""], ["Jahanifar", "Mostafa", ""], ["Bilal", "Mohsin", ""], ["Graham", "Simon", ""], ["Park", "Young", ""], ["Hadjigeorghiou", "Giorgos", ""], ["Bhalerao", "Abhir", ""], ["Lashen", "Ayat", ""], ["Ibrahim", "Asmaa", ""], ["Katayama", "Ayaka", ""], ["Ebili", "Henry O", ""], ["Parkin", "Matthew", ""], ["Sorell", "Tom", ""], ["Raza", "Shan E Ahmed", ""], ["Hero", "Emily", ""], ["Eldaly", "Hesham", ""], ["Tsang", "Yee Wah", ""], ["Gopalakrishnan", "Kishore", ""], ["Snead", "David", ""], ["Rakha", "Emad", ""], ["Rajpoot", "Nasir", ""], ["Minhas", "Fayyaz", ""]]}, {"id": "2106.13696", "submitter": "Robert Leer", "authors": "Robert Leer, Hessi Roma, James Amelia", "title": "Image-to-image Transformation with Auxiliary Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of image recognition like human pose detection, trained with\nsimulated images would usually get worse due to the divergence between real and\nsimulated data. To make the distribution of a simulated image close to that of\nreal one, there are several works applying GAN-based image-to-image\ntransformation methods, e.g., SimGAN and CycleGAN. However, these methods would\nnot be sensitive enough to the various change in pose and shape of subjects,\nespecially when the training data are imbalanced, e.g., some particular poses\nand shapes are minor in the training data. To overcome this problem, we propose\nto introduce the label information of subjects, e.g., pose and type of objects\nin the training of CycleGAN, and lead it to obtain label-wise transforamtion\nmodels. We evaluate our proposed method called Label-CycleGAN, through\nexperiments on the digit image transformation from SVHN to MNIST and the\nsurveillance camera image transformation from simulated to real images.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:33:11 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Leer", "Robert", ""], ["Roma", "Hessi", ""], ["Amelia", "James", ""]]}, {"id": "2106.13700", "submitter": "Shan You", "authors": "Xiu Su, Shan You, Jiyang Xie, Mingkai Zheng, Fei Wang, Chen Qian,\n  Changshui Zhang, Xiaogang Wang, Chang Xu", "title": "Vision Transformer Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, transformers have shown great superiority in solving computer\nvision tasks by modeling images as a sequence of manually-split patches with\nself-attention mechanism. However, current architectures of vision transformers\n(ViTs) are simply inherited from natural language processing (NLP) tasks and\nhave not been sufficiently investigated and optimized. In this paper, we make a\nfurther step by examining the intrinsic structure of transformers for vision\ntasks and propose an architecture search method, dubbed ViTAS, to search for\nthe optimal architecture with similar hardware budgets. Concretely, we design a\nnew effective yet efficient weight sharing paradigm for ViTs, such that\narchitectures with different token embedding, sequence size, number of heads,\nwidth, and depth can be derived from a single super-transformer. Moreover, to\ncater for the variance of distinct architectures, we introduce \\textit{private}\nclass token and self-attention maps in the super-transformer. In addition, to\nadapt the searching for different budgets, we propose to search the sampling\nprobability of identity operation. Experimental results show that our ViTAS\nattains excellent results compared to existing pure transformer architectures.\nFor example, with $1.3$G FLOPs budget, our searched architecture achieves\n$74.7\\%$ top-$1$ accuracy on ImageNet and is $2.5\\%$ superior than the current\nbaseline ViT architecture. Code is available at\n\\url{https://github.com/xiusu/ViTAS}.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:39:08 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Su", "Xiu", ""], ["You", "Shan", ""], ["Xie", "Jiyang", ""], ["Zheng", "Mingkai", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Zhang", "Changshui", ""], ["Wang", "Xiaogang", ""], ["Xu", "Chang", ""]]}, {"id": "2106.13734", "submitter": "Xianjing Liu", "authors": "Xianjing Liu, Bo Li, Esther Bron, Wiro Niessen, Eppo Wolvius and\n  Gennady Roshchupkin", "title": "Projection-wise Disentangling for Fair and Interpretable Representation\n  Learning: Application to 3D Facial Shape Analysis", "comments": "Accepted at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Confounding bias is a crucial problem when applying machine learning to\npractice, especially in clinical practice. We consider the problem of learning\nrepresentations independent to multiple biases. In literature, this is mostly\nsolved by purging the bias information from learned representations. We however\nexpect this strategy to harm the diversity of information in the\nrepresentation, and thus limiting its prospective usage (e.g., interpretation).\nTherefore, we propose to mitigate the bias while keeping almost all information\nin the latent representations, which enables us to observe and interpret them\nas well. To achieve this, we project latent features onto a learned vector\ndirection, and enforce the independence between biases and projected features\nrather than all learned features. To interpret the mapping between projected\nfeatures and input data, we propose projection-wise disentangling: a sampling\nand reconstruction along the learned vector direction. The proposed method was\nevaluated on the analysis of 3D facial shape and patient characteristics\n(N=5011). Experiments showed that this conceptually simple method achieved\nstate-of-the-art fair prediction performance and interpretability, showing its\ngreat potential for clinical applications.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:09:56 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 19:24:07 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Liu", "Xianjing", ""], ["Li", "Bo", ""], ["Bron", "Esther", ""], ["Niessen", "Wiro", ""], ["Wolvius", "Eppo", ""], ["Roshchupkin", "Gennady", ""]]}, {"id": "2106.13739", "submitter": "David Dehaene", "authors": "David Dehaene and R\\'emy Brossard", "title": "Re-parameterizing VAEs for stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a theoretical approach towards the training numerical stability of\nVariational AutoEncoders (VAE). Our work is motivated by recent studies\nempowering VAEs to reach state of the art generative results on complex image\ndatasets. These very deep VAE architectures, as well as VAEs using more complex\noutput distributions, highlight a tendency to haphazardly produce high training\ngradients as well as NaN losses. The empirical fixes proposed to train them\ndespite their limitations are neither fully theoretically grounded nor\ngenerally sufficient in practice. Building on this, we localize the source of\nthe problem at the interface between the model's neural networks and their\noutput probabilistic distributions. We explain a common source of instability\nstemming from an incautious formulation of the encoded Normal distribution's\nvariance, and apply the same approach on other, less obvious sources. We show\nthat by implementing small changes to the way we parameterize the Normal\ndistributions on which they rely, VAEs can securely be trained.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:19:09 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Dehaene", "David", ""], ["Brossard", "R\u00e9my", ""]]}, {"id": "2106.13765", "submitter": "Kaiyue Zhou", "authors": "Kaiyue Zhou, Ming Dong, Suzan Arslanturk", "title": "\"Zero Shot\" Point Cloud Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud upsampling using deep learning has been paid various efforts in\nthe past few years. Recent supervised deep learning methods are restricted to\nthe size of training data and is limited in terms of covering all shapes of\npoint clouds. Besides, the acquisition of such amount of data is unrealistic,\nand the network generally performs less powerful than expected on unseen\nrecords. In this paper, we present an unsupervised approach to upsample point\nclouds internally referred as \"Zero Shot\" Point Cloud Upsampling (ZSPU) at\nholistic level. Our approach is solely based on the internal information\nprovided by a particular point cloud without patching in both self-training and\ntesting phases. This single-stream design significantly reduces the training\ntime of the upsampling task, by learning the relation between low-resolution\n(LR) point clouds and their high (original) resolution (HR) counterparts. This\nassociation will provide super-resolution (SR) outputs when original point\nclouds are loaded as input. We demonstrate competitive performance on benchmark\npoint cloud datasets when compared to other upsampling methods. Furthermore,\nZSPU achieves superior qualitative results on shapes with complex local details\nor high curvatures.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 17:06:18 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Zhou", "Kaiyue", ""], ["Dong", "Ming", ""], ["Arslanturk", "Suzan", ""]]}, {"id": "2106.13787", "submitter": "Max Reimann", "authors": "Max Reimann and Benito Buchheim and Amir Semmo and J\\\"urgen D\\\"ollner\n  and Matthias Trapp", "title": "Interactive Multi-level Stroke Control for Neural Style Transfer", "comments": "International Conference on CyberWorlds (CW2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present StyleTune, a mobile app for interactive multi-level control of\nneural style transfers that facilitates creative adjustments of style elements\nand enables high output fidelity. In contrast to current mobile neural style\ntransfer apps, StyleTune supports users to adjust both the size and orientation\nof style elements, such as brushstrokes and texture patches, on a global as\nwell as local level. To this end, we propose a novel stroke-adaptive\nfeed-forward style transfer network, that enables control over stroke size and\nintensity and allows a larger range of edits than current approaches. For\nadditional level-of-control, we propose a network agnostic method for\nstroke-orientation adjustment by utilizing the rotation-variance of CNNs. To\nachieve high output fidelity, we further add a patch-based style transfer\nmethod that enables users to obtain output resolutions of more than 20\nMegapixel. Our approach empowers users to create many novel results that are\nnot possible with current mobile neural style transfer apps.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 17:43:00 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Reimann", "Max", ""], ["Buchheim", "Benito", ""], ["Semmo", "Amir", ""], ["D\u00f6llner", "J\u00fcrgen", ""], ["Trapp", "Matthias", ""]]}, {"id": "2106.13797", "submitter": "Wenhai Wang", "authors": "Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding\n  Liang, Tong Lu, Ping Luo, Ling Shao", "title": "PVTv2: Improved Baselines with Pyramid Vision Transformer", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer recently has shown encouraging progresses in computer vision. In\nthis work, we present new baselines by improving the original Pyramid Vision\nTransformer (abbreviated as PVTv1) by adding three designs, including (1)\noverlapping patch embedding, (2) convolutional feed-forward networks, and (3)\nlinear complexity attention layers.\n  With these modifications, our PVTv2 significantly improves PVTv1 on three\ntasks e.g., classification, detection, and segmentation. Moreover, PVTv2\nachieves comparable or better performances than recent works such as Swin\nTransformer. We hope this work will facilitate state-of-the-art Transformer\nresearches in computer vision. Code is available at\nhttps://github.com/whai362/PVT .\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 17:51:09 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 15:07:07 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 08:04:40 GMT"}, {"version": "v4", "created": "Sat, 17 Jul 2021 15:12:25 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Wenhai", ""], ["Xie", "Enze", ""], ["Li", "Xiang", ""], ["Fan", "Deng-Ping", ""], ["Song", "Kaitao", ""], ["Liang", "Ding", ""], ["Lu", "Tong", ""], ["Luo", "Ping", ""], ["Shao", "Ling", ""]]}, {"id": "2106.13802", "submitter": "Jaya Krishna Mandivarapu Mr", "authors": "Jaya Krishna Mandivarapu, Eric Bunch, Qian You, Glenn Fung", "title": "Efficient Document Image Classification Using Region-Based Graph Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Document image classification remains a popular research area because it can\nbe commercialized in many enterprise applications across different industries.\nRecent advancements in large pre-trained computer vision and language models\nand graph neural networks has lent document image classification many tools.\nHowever using large pre-trained models usually requires substantial computing\nresources which could defeat the cost-saving advantages of automatic document\nimage classification. In the paper we propose an efficient document image\nclassification framework that uses graph convolution neural networks and\nincorporates textual, visual and layout information of the document. We have\nrigorously benchmarked our proposed algorithm against several state-of-art\nvision and language models on both publicly available dataset and a real-life\ninsurance document classification dataset. Empirical results on both publicly\navailable and real-world data show that our methods achieve near SOTA\nperformance yet require much less computing resources and time for model\ntraining and inference. This results in solutions than offer better cost\nadvantages, especially in scalable deployment for enterprise applications. The\nresults showed that our algorithm can achieve classification performance quite\nclose to SOTA. We also provide comprehensive comparisons of computing\nresources, model sizes, train and inference time between our proposed methods\nand baselines. In addition we delineate the cost per image using our method and\nother baselines.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 17:57:04 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Mandivarapu", "Jaya Krishna", ""], ["Bunch", "Eric", ""], ["You", "Qian", ""], ["Fung", "Glenn", ""]]}, {"id": "2106.13804", "submitter": "Boyi Li", "authors": "Boyi Li and Yin Cui and Tsung-Yi Lin and Serge Belongie", "title": "Single Image Texture Translation for Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image synthesis enables one to translate images by\nlearning the mapping between a source domain and a target domain. Existing\nmethods tend to learn the distributions by training a model on a variety of\ndatasets, with results evaluated largely in a subjective manner. Relatively few\nworks in this area, however, study the potential use of semantic image\ntranslation methods for image recognition tasks. In this paper, we explore the\nuse of Single Image Texture Translation (SITT) for data augmentation. We first\npropose a lightweight model for translating texture to images based on a single\ninput of source texture, allowing for fast training and testing. Based on SITT,\nwe then explore the use of augmented data in long-tailed and few-shot image\nclassification tasks. We find the proposed method is capable of translating\ninput data into a target domain, leading to consistent improved image\nrecognition performance. Finally, we examine how SITT and related image\ntranslation methods can provide a basis for a data-efficient, augmentation\nengineering approach to model training.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 17:59:04 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Li", "Boyi", ""], ["Cui", "Yin", ""], ["Lin", "Tsung-Yi", ""], ["Belongie", "Serge", ""]]}, {"id": "2106.13842", "submitter": "Sumaiya Tabassum Nimi", "authors": "Sumaiya Tabassum Nimi, Md Adnan Arefeen, Md Yusuf Sarwar Uddin,\n  Yugyung Lee", "title": "EARLIN: Early Out-of-Distribution Detection for Resource-efficient\n  Collaborative Inference", "comments": "To Appear in the proceedings of ECML-PKDD'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative inference enables resource-constrained edge devices to make\ninferences by uploading inputs (e.g., images) to a server (i.e., cloud) where\nthe heavy deep learning models run. While this setup works cost-effectively for\nsuccessful inferences, it severely underperforms when the model faces input\nsamples on which the model was not trained (known as Out-of-Distribution (OOD)\nsamples). If the edge devices could, at least, detect that an input sample is\nan OOD, that could potentially save communication and computation resources by\nnot uploading those inputs to the server for inference workload. In this paper,\nwe propose a novel lightweight OOD detection approach that mines important\nfeatures from the shallow layers of a pretrained CNN model and detects an input\nsample as ID (In-Distribution) or OOD based on a distance function defined on\nthe reduced feature space. Our technique (a) works on pretrained models without\nany retraining of those models, and (b) does not expose itself to any OOD\ndataset (all detection parameters are obtained from the ID training dataset).\nTo this end, we develop EARLIN (EARLy OOD detection for Collaborative\nINference) that takes a pretrained model and partitions the model at the OOD\ndetection layer and deploys the considerably small OOD part on an edge device\nand the rest on the cloud. By experimenting using real datasets and a prototype\nimplementation, we show that our technique achieves better results than other\napproaches in terms of overall accuracy and cost when tested against popular\nOOD datasets on top of popular deep learning models pretrained on benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 18:43:23 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 01:27:49 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Nimi", "Sumaiya Tabassum", ""], ["Arefeen", "Md Adnan", ""], ["Uddin", "Md Yusuf Sarwar", ""], ["Lee", "Yugyung", ""]]}, {"id": "2106.13849", "submitter": "Abdullah Al-Battal", "authors": "Abdullah F. Al-Battal, Yan Gong, Lu Xu, Timothy Morton, Chen Du,\n  Yifeng Bu 1, Imanuel R Lerman, Radhika Madhavan, Truong Q. Nguyen", "title": "A CNN Segmentation-Based Approach to Object Detection and Tracking in\n  Ultrasound Scans with Application to the Vagus Nerve Detection", "comments": "7 pages , 4 figures, submitted to the IEEE EMBC 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Ultrasound scanning is essential in several medical diagnostic and\ntherapeutic applications. It is used to visualize and analyze anatomical\nfeatures and structures that influence treatment plans. However, it is both\nlabor intensive, and its effectiveness is operator dependent. Real-time\naccurate and robust automatic detection and tracking of anatomical structures\nwhile scanning would significantly impact diagnostic and therapeutic procedures\nto be consistent and efficient. In this paper, we propose a deep learning\nframework to automatically detect and track a specific anatomical target\nstructure in ultrasound scans. Our framework is designed to be accurate and\nrobust across subjects and imaging devices, to operate in real-time, and to not\nrequire a large training set. It maintains a localization precision and recall\nhigher than 90% when trained on training sets that are as small as 20% in size\nof the original training set. The framework backbone is a weakly trained\nsegmentation neural network based on U-Net. We tested the framework on two\ndifferent ultrasound datasets with the aim to detect and track the Vagus nerve,\nwhere it outperformed current state-of-the-art real-time object detection\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 19:12:46 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Al-Battal", "Abdullah F.", ""], ["Gong", "Yan", ""], ["Xu", "Lu", ""], ["Morton", "Timothy", ""], ["Du", "Chen", ""], ["1", "Yifeng Bu", ""], ["Lerman", "Imanuel R", ""], ["Madhavan", "Radhika", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "2106.13863", "submitter": "Pavlo Melnyk", "authors": "Pavlo Melnyk, Michael Felsberg, M{\\aa}rten Wadenb\\\"ack", "title": "Fully Steerable 3D Spherical Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging from low-level vision theory, steerable filters found their\ncounterpart in deep learning. Earlier works used the steering theorems and\npresented convolutional networks equivariant to rigid transformations. In our\nwork, we propose a steerable feed-forward learning-based approach that consists\nof spherical decision surfaces and operates on point clouds. Due to the\ninherent geometric 3D structure of our theory, we derive a 3D steerability\nconstraint for its atomic parts, the hypersphere neurons. Exploiting the\nrotational equivariance, we show how the model parameters are fully steerable\nat inference time. The proposed spherical filter banks enable to make\nequivariant and, after online optimization, invariant class predictions for\nknown synthetic point sets in unknown orientations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:30:02 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Melnyk", "Pavlo", ""], ["Felsberg", "Michael", ""], ["Wadenb\u00e4ck", "M\u00e5rten", ""]]}, {"id": "2106.13864", "submitter": "Hieu Thao Nguyen Dr.", "authors": "Nguyen Hieu Thao, Oleg Soloviev, Jacques Noom and Michel Verhaegen", "title": "Nonuniform Defocus Removal for Image Classification", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study the single-frame anisoplanatic deconvolution problem\nassociated with image classification using machine learning algorithms, named\nthe nonuniform defocus removal (NDR) problem. Mathematical analysis of the NDR\nproblem is done and the so-called defocus removal (DR) algorithm for solving it\nis proposed. Global convergence of the DR algorithm is established without\nimposing any unverifiable assumption. Numerical results on simulation data show\nsignificant features of DR including solvability, noise robustness,\nconvergence, model insensitivity and computational efficiency. Physical\nrelevance of the NDR problem and practicability of the DR algorithm are tested\non experimental data. Back to the application that originally motivated the\ninvestigation of the NDR problem, we show that the DR algorithm can improve the\naccuracy of classifying distorted images using convolutional neural networks.\nThe key difference of this paper compared to most existing works on\nsingle-frame anisoplanatic deconvolution is that the new method does not\nrequire the data image to be decomposable into isoplanatic subregions.\nTherefore, solution approaches partitioning the image into isoplanatic zones\nare not applicable to the NDR problem and those handling the entire image such\nas the DR algorithm need to be developed and analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:39:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Thao", "Nguyen Hieu", ""], ["Soloviev", "Oleg", ""], ["Noom", "Jacques", ""], ["Verhaegen", "Michel", ""]]}, {"id": "2106.13870", "submitter": "Stephanie Tsuei", "authors": "Stephanie Tsuei, Aditya Golatkar, Stefano Soatto", "title": "Scene Uncertainty and the Wellington Posterior of Deterministic Image\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCLA CS Report #210001", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to estimate the uncertainty of the outcome of an image\nclassifier on a given input datum. Deep neural networks commonly used for image\nclassification are deterministic maps from an input image to an output class.\nAs such, their outcome on a given datum involves no uncertainty, so we must\nspecify what variability we are referring to when defining, measuring and\ninterpreting \"confidence.\" To this end, we introduce the Wellington Posterior,\nwhich is the distribution of outcomes that would have been obtained in response\nto data that could have been generated by the same scene that produced the\ngiven image. Since there are infinitely many scenes that could have generated\nthe given image, the Wellington Posterior requires induction from scenes other\nthan the one portrayed. We explore alternate methods using data augmentation,\nensembling, and model linearization. Additional alternatives include generative\nadversarial networks, conditional prior networks, and supervised single-view\nreconstruction. We test these alternatives against the empirical posterior\nobtained by inferring the class of temporally adjacent frames in a video. These\ndevelopments are only a small step towards assessing the reliability of deep\nnetwork classifiers in a manner that is compatible with safety-critical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 20:10:00 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Tsuei", "Stephanie", ""], ["Golatkar", "Aditya", ""], ["Soatto", "Stefano", ""]]}, {"id": "2106.13880", "submitter": "Zhao Kang", "authors": "Zhao Kang, Hongfei Liu, Jiangxin Li, Xiaofeng Zhu, and Ling Tian", "title": "Self-paced Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) has been widely used for dimensionality\nreduction and feature extraction. Robust PCA (RPCA), under different robust\ndistance metrics, such as l1-norm and l2, p-norm, can deal with noise or\noutliers to some extent. However, real-world data may display structures that\ncan not be fully captured by these simple functions. In addition, existing\nmethods treat complex and simple samples equally. By contrast, a learning\npattern typically adopted by human beings is to learn from simple to complex\nand less to more. Based on this principle, we propose a novel method called\nSelf-paced PCA (SPCA) to further reduce the effect of noise and outliers.\nNotably, the complexity of each sample is calculated at the beginning of each\niteration in order to integrate samples from simple to more complex into\ntraining. Based on an alternating optimization, SPCA finds an optimal\nprojection matrix and filters out outliers iteratively. Theoretical analysis is\npresented to show the rationality of SPCA. Extensive experiments on popular\ndata sets demonstrate that the proposed method can improve the state of-the-art\nresults considerably.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 20:50:45 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kang", "Zhao", ""], ["Liu", "Hongfei", ""], ["Li", "Jiangxin", ""], ["Zhu", "Xiaofeng", ""], ["Tian", "Ling", ""]]}, {"id": "2106.13883", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi and Abdullah Abuolaim", "title": "Semi-Supervised Raw-to-Raw Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The raw-RGB colors of a camera sensor vary due to the spectral sensitivity\ndifferences across different sensor makes and models. This paper focuses on the\ntask of mapping between different sensor raw-RGB color spaces. Prior work\naddressed this problem using a pairwise calibration to achieve accurate color\nmapping. Although being accurate, this approach is less practical as it\nrequires: (1) capturing pair of images by both camera devices with a color\ncalibration object placed in each new scene; (2) accurate image alignment or\nmanual annotation of the color calibration object. This paper aims to tackle\ncolor mapping in the raw space through a more practical setup. Specifically, we\npresent a semi-supervised raw-to-raw mapping method trained on a small set of\npaired images alongside an unpaired set of images captured by each camera\ndevice. Through extensive experiments, we show that our method achieves better\nresults compared to other domain adaptation alternatives in addition to the\nsingle-calibration solution. We have generated a new dataset of raw images from\ntwo different smartphone cameras as part of this effort. Our dataset includes\nunpaired and paired sets for our semi-supervised training and evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 21:01:45 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 21:19:21 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Abuolaim", "Abdullah", ""]]}, {"id": "2106.13884", "submitter": "Jacob Menick", "authors": "Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol\n  Vinyals, Felix Hill", "title": "Multimodal Few-Shot Learning with Frozen Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 21:07:09 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 10:04:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tsimpoukelli", "Maria", ""], ["Menick", "Jacob", ""], ["Cabi", "Serkan", ""], ["Eslami", "S. M. Ali", ""], ["Vinyals", "Oriol", ""], ["Hill", "Felix", ""]]}, {"id": "2106.13899", "submitter": "Joao Monteiro", "authors": "Joao Monteiro, Xavier Gibert, Jianqiao Feng, Vincent Dumoulin,\n  Dar-Shyang Lee", "title": "Domain Conditional Predictors for Domain Adaptation", "comments": "Part of the pre-registration workshop at NeurIPS 2020:\n  https://preregister.science/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning guarantees often rely on assumptions of i.i.d. data, which will\nlikely be violated in practice once predictors are deployed to perform\nreal-world tasks. Domain adaptation approaches thus appeared as a useful\nframework yielding extra flexibility in that distinct train and test data\ndistributions are supported, provided that other assumptions are satisfied such\nas covariate shift, which expects the conditional distributions over labels to\nbe independent of the underlying data distribution. Several approaches were\nintroduced in order to induce generalization across varying train and test data\nsources, and those often rely on the general idea of domain-invariance, in such\na way that the data-generating distributions are to be disregarded by the\nprediction model. In this contribution, we tackle the problem of generalizing\nacross data sources by approaching it from the opposite direction: we consider\na conditional modeling approach in which predictions, in addition to being\ndependent on the input data, use information relative to the underlying\ndata-generating distribution. For instance, the model has an explicit mechanism\nto adapt to changing environments and/or new data sources. We argue that such\nan approach is more generally applicable than current domain adaptation methods\nsince it does not require extra assumptions such as covariate shift and further\nyields simpler training algorithms that avoid a common source of training\ninstabilities caused by minimax formulations, often employed in\ndomain-invariant methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 22:15:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Monteiro", "Joao", ""], ["Gibert", "Xavier", ""], ["Feng", "Jianqiao", ""], ["Dumoulin", "Vincent", ""], ["Lee", "Dar-Shyang", ""]]}, {"id": "2106.13913", "submitter": "Hongyu Guo", "authors": "Hongyu Guo", "title": "Midpoint Regularization: from High Uncertainty Training to Conservative\n  Classification", "comments": "Accepted to ECML-PKDD 2021. arXiv admin note: substantial text\n  overlap with arXiv:2012.01559", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label Smoothing (LS) improves model generalization through penalizing models\nfrom generating overconfident output distributions. For each training sample\nthe LS strategy smooths the one-hot encoded training signal by distributing its\ndistribution mass over the non-ground truth classes. We extend this technique\nby considering example pairs, coined PLS. PLS first creates midpoint samples by\naveraging random sample pairs and then learns a smoothing distribution during\ntraining for each of these midpoint samples, resulting in midpoints with high\nuncertainty labels for training. We empirically show that PLS significantly\noutperforms LS, achieving up to 30% of relative classification error reduction.\nWe also visualize that PLS produces very low winning softmax scores for both in\nand out of distribution samples.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 00:31:46 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Guo", "Hongyu", ""]]}, {"id": "2106.13920", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi, Abdullah Abuolaim, Mostafa Hussien, Marcus A. Brubaker,\n  Michael S. Brown", "title": "CAMS: Color-Aware Multi-Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image style transfer aims to manipulate the appearance of a source image, or\n\"content\" image, to share similar texture and colors of a target \"style\" image.\nIdeally, the style transfer manipulation should also preserve the semantic\ncontent of the source image. A commonly used approach to assist in transferring\nstyles is based on Gram matrix optimization. One problem of Gram matrix-based\noptimization is that it does not consider the correlation between colors and\ntheir styles. Specifically, certain textures or structures should be associated\nwith specific colors. This is particularly challenging when the target style\nimage exhibits multiple style types. In this work, we propose a color-aware\nmulti-style transfer method that generates aesthetically pleasing results while\npreserving the style-color correlation between style and generated images. We\nachieve this desired outcome by introducing a simple but efficient modification\nto classic Gram matrix-based style transfer optimization. A nice feature of our\nmethod is that it enables the users to manually select the color associations\nbetween the target style and content image for more transfer flexibility. We\nvalidated our method with several qualitative comparisons, including a user\nstudy conducted with 30 participants. In comparison with prior work, our method\nis simple, easy to implement, and achieves visually appealing results when\ntargeting images that have multiple styles. Source code is available at\nhttps://github.com/mahmoudnafifi/color-aware-style-transfer.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 01:15:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Abuolaim", "Abdullah", ""], ["Hussien", "Mostafa", ""], ["Brubaker", "Marcus A.", ""], ["Brown", "Michael S.", ""]]}, {"id": "2106.13929", "submitter": "Kaixiong Xu", "authors": "Huafeng Li, Kaixiong Xu, Jinxing Li, Guangming Lu, Yong Xu, Zhengtao\n  Yu, David Zhang", "title": "Dual-Stream Reciprocal Disentanglement Learning for Domain Adaption\n  Person Re-Identification", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since human-labeled samples are free for the target set, unsupervised person\nre-identification (Re-ID) has attracted much attention in recent years, by\nadditionally exploiting the source set. However, due to the differences on\ncamera styles, illumination and backgrounds, there exists a large gap between\nsource domain and target domain, introducing a great challenge on cross-domain\nmatching. To tackle this problem, in this paper we propose a novel method named\nDual-stream Reciprocal Disentanglement Learning (DRDL), which is quite\nefficient in learning domain-invariant features. In DRDL, two encoders are\nfirst constructed for id-related and id-unrelated feature extractions, which\nare respectively measured by their associated classifiers. Furthermore,\nfollowed by an adversarial learning strategy, both streams reciprocally and\npositively effect each other, so that the id-related features and id-unrelated\nfeatures are completely disentangled from a given image, allowing the encoder\nto be powerful enough to obtain the discriminative but domain-invariant\nfeatures. In contrast to existing approaches, our proposed method is free from\nimage generation, which not only reduces the computational complexity\nremarkably, but also removes redundant information from id-related features.\nExtensive experiments substantiate the superiority of our proposed method\ncompared with the state-of-the-arts. The source code has been released in\nhttps://github.com/lhf12278/DRDL.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 03:05:23 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Li", "Huafeng", ""], ["Xu", "Kaixiong", ""], ["Li", "Jinxing", ""], ["Lu", "Guangming", ""], ["Xu", "Yong", ""], ["Yu", "Zhengtao", ""], ["Zhang", "David", ""]]}, {"id": "2106.13933", "submitter": "Ang Cao", "authors": "Ang Cao, Justin Johnson", "title": "Inverting and Understanding Object Detectors", "comments": "Preprints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a core problem in computer vision, the performance of object detection has\nimproved drastically in the past few years. Despite their impressive\nperformance, object detectors suffer from a lack of interpretability.\nVisualization techniques have been developed and widely applied to introspect\nthe decisions made by other kinds of deep learning models; however, visualizing\nobject detectors has been underexplored. In this paper, we propose using\ninversion as a primary tool to understand modern object detectors and develop\nan optimization-based approach to layout inversion, allowing us to generate\nsynthetic images recognized by trained detectors as containing a desired\nconfiguration of objects. We reveal intriguing properties of detectors by\napplying our layout inversion technique to a variety of modern object\ndetectors, and further investigate them via validation experiments: they rely\non qualitatively different features for classification and regression; they\nlearn canonical motifs of commonly co-occurring objects; they use diff erent\nvisual cues to recognize objects of varying sizes. We hope our insights can\nhelp practitioners improve object detectors.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 03:31:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Cao", "Ang", ""], ["Johnson", "Justin", ""]]}, {"id": "2106.13939", "submitter": "Shizhao Zhang", "authors": "Shizhao Zhang, Hongya Tuo, Jian Hu, Zhongliang Jing", "title": "Domain Adaptive YOLO for One-Stage Cross-Domain Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain shift is a major challenge for object detectors to generalize well to\nreal world applications. Emerging techniques of domain adaptation for two-stage\ndetectors help to tackle this problem. However, two-stage detectors are not the\nfirst choice for industrial applications due to its long time consumption. In\nthis paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve\ncross-domain performance for one-stage detectors. Image level features\nalignment is used to strictly match for local features like texture, and\nloosely match for global features like illumination. Multi-scale instance level\nfeatures alignment is presented to reduce instance domain shift effectively ,\nsuch as variations in object appearance and viewpoint. A consensus\nregularization to these domain classifiers is employed to help the network\ngenerate domain-invariant detections. We evaluate our proposed method on\npopular datasets like Cityscapes, KITTI, SIM10K and etc.. The results\ndemonstrate significant improvement when tested under different cross-domain\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 04:17:42 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 02:34:06 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Shizhao", ""], ["Tuo", "Hongya", ""], ["Hu", "Jian", ""], ["Jing", "Zhongliang", ""]]}, {"id": "2106.13948", "submitter": "Xiaopeng Lu", "authors": "Jonathan Francis, Nariaki Kitamura, Felix Labelle, Xiaopeng Lu, Ingrid\n  Navarro, Jean Oh", "title": "Core Challenges in Embodied Vision-Language Planning", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 05:18:58 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 06:04:51 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Francis", "Jonathan", ""], ["Kitamura", "Nariaki", ""], ["Labelle", "Felix", ""], ["Lu", "Xiaopeng", ""], ["Navarro", "Ingrid", ""], ["Oh", "Jean", ""]]}, {"id": "2106.13952", "submitter": "Di Wang", "authors": "Di Wang, Bo Du, Liangpei Zhang", "title": "Spectral-Spatial Graph Reasoning Network for Hyperspectral Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a spectral-spatial graph reasoning network (SSGRN)\nfor hyperspectral image (HSI) classification. Concretely, this network contains\ntwo parts that separately named spatial graph reasoning subnetwork (SAGRN) and\nspectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral\ngraph contexts, respectively. Different from the previous approaches\nimplementing superpixel segmentation on the original image or attempting to\nobtain the category features under the guide of label image, we perform the\nsuperpixel segmentation on intermediate features of the network to adaptively\nproduce the homogeneous regions to get the effective descriptors. Then, we\nadopt a similar idea in spectral part that reasonably aggregating the channels\nto generate spectral descriptors for spectral graph contexts capturing. All\ngraph reasoning procedures in SAGRN and SEGRN are achieved through graph\nconvolution. To guarantee the global perception ability of the proposed\nmethods, all adjacent matrices in graph reasoning are obtained with the help of\nnon-local self-attention mechanism. At last, by combining the extracted spatial\nand spectral graph contexts, we obtain the SSGRN to achieve a high accuracy\nclassification. Extensive quantitative and qualitative experiments on three\npublic HSI benchmarks demonstrate the competitiveness of the proposed methods\ncompared with other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 06:24:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Di", ""], ["Du", "Bo", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2106.13953", "submitter": "Changho Jo", "authors": "Changho Jo, Woobin Im, Sung-Eui Yoon", "title": "In-N-Out: Towards Good Initialization for Inpainting and Outpainting", "comments": "13 pages (9 pages without references), 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In computer vision, recovering spatial information by filling in masked\nregions, e.g., inpainting, has been widely investigated for its usability and\nwide applicability to other various applications: image inpainting, image\nextrapolation, and environment map estimation. Most of them are studied\nseparately depending on the applications. Our focus, however, is on\naccommodating the opposite task, e.g., image outpainting, which would benefit\nthe target applications, e.g., image inpainting. Our self-supervision method,\nIn-N-Out, is summarized as a training approach that leverages the knowledge of\nthe opposite task into the target model. We empirically show that In-N-Out --\nwhich explores the complementary information -- effectively takes advantage\nover the traditional pipelines where only task-specific learning takes place in\ntraining. In experiments, we compare our method to the traditional procedure\nand analyze the effectiveness of our method on different applications: image\ninpainting, image extrapolation, and environment map estimation. For these\ntasks, we demonstrate that In-N-Out consistently improves the performance of\nthe recent works with In-N-Out self-supervision to their training procedure.\nAlso, we show that our approach achieves better results than an existing\ntraining approach for outpainting.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 06:26:15 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Jo", "Changho", ""], ["Im", "Woobin", ""], ["Yoon", "Sung-Eui", ""]]}, {"id": "2106.13959", "submitter": "Avishek Chatterjee", "authors": "Avishek Chatterjee, Satyaki Mazumder, Koel Das", "title": "Functional Classwise Principal Component Analysis: A Novel\n  Classification Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, functional data analysis (FDA) has been successfully applied\nin the field of high dimensional data classification. In this paper, we present\na novel classification framework using functional data and classwise Principal\nComponent Analysis (PCA). Our proposed method can be used in high dimensional\ntime series data which typically suffers from small sample size problem. Our\nmethod extracts a piece wise linear functional feature space and is\nparticularly suitable for hard classification problems.The proposed framework\nconverts time series data into functional data and uses classwise functional\nPCA for feature extraction followed by classification using a Bayesian linear\nclassifier. We demonstrate the efficacy of our proposed method by applying it\nto both synthetic data sets and real time series data from diverse fields\nincluding but not limited to neuroscience, food science, medical sciences and\nchemometrics.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 07:10:58 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chatterjee", "Avishek", ""], ["Mazumder", "Satyaki", ""], ["Das", "Koel", ""]]}, {"id": "2106.13963", "submitter": "P B Sujit Dr", "authors": "Anukriti Singh, Kartikeya Singh, and P.B. Sujit", "title": "OffRoadTranSeg: Semi-Supervised Segmentation using Transformers on\n  OffRoad environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present OffRoadTranSeg, the first end-to-end framework for semi-supervised\nsegmentation in unstructured outdoor environment using transformers and\nautomatic data selection for labelling. The offroad segmentation is a scene\nunderstanding approach that is widely used in autonomous driving. The popular\noffroad segmentation method is to use fully connected convolution layers and\nlarge labelled data, however, due to class imbalance, there will be several\nmismatches and also some classes may not be detected. Our approach is to do the\ntask of offroad segmentation in a semi-supervised manner. The aim is to provide\na model where self supervised vision transformer is used to fine-tune offroad\ndatasets with self-supervised data collection for labelling using depth\nestimation. The proposed method is validated on RELLIS-3D and RUGD offroad\ndatasets. The experiments show that OffRoadTranSeg outperformed other state of\nthe art models, and also solves the RELLIS-3D class imbalance problem.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 08:05:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Singh", "Anukriti", ""], ["Singh", "Kartikeya", ""], ["Sujit", "P. B.", ""]]}, {"id": "2106.13967", "submitter": "Vasiliki Vasileiou", "authors": "Vasiliki I. Vasileiou, Nikolaos Kardaris, Petros Maragos", "title": "Exploring Temporal Context and Human Movement Dynamics for Online Action\n  Detection in Videos", "comments": "EUSIPCO-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, the interaction between humans and robots is constantly expanding,\nrequiring more and more human motion recognition applications to operate in\nreal time. However, most works on temporal action detection and recognition\nperform these tasks in offline manner, i.e. temporally segmented videos are\nclassified as a whole. In this paper, based on the recently proposed framework\nof Temporal Recurrent Networks, we explore how temporal context and human\nmovement dynamics can be effectively employed for online action detection. Our\napproach uses various state-of-the-art architectures and appropriately combines\nthe extracted features in order to improve action detection. We evaluate our\nmethod on a challenging but widely used dataset for temporal action\nlocalization, THUMOS'14. Our experiments show significant improvement over the\nbaseline method, achieving state-of-the art results on THUMOS'14.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 08:34:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Vasileiou", "Vasiliki I.", ""], ["Kardaris", "Nikolaos", ""], ["Maragos", "Petros", ""]]}, {"id": "2106.13974", "submitter": "Tiago Cortinhal", "authors": "Tiago Cortinhal, Fatih Kurnaz, Eren Aksoy", "title": "Semantics-aware Multi-modal Domain Translation:From LiDAR Point Clouds\n  to Panoramic Color Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a simple yet effective framework to address the\ndomain translation problem between different sensor modalities with unique data\nformats. By relying only on the semantics of the scene, our modular generative\nframework can, for the first time, synthesize a panoramic color image from a\ngiven full 3D LiDAR point cloud. The framework starts with semantic\nsegmentation of the point cloud, which is initially projected onto a spherical\nsurface. The same semantic segmentation is applied to the corresponding camera\nimage. Next, our new conditional generative model adversarially learns to\ntranslate the predicted LiDAR segment maps to the camera image counterparts.\nFinally, generated image segments are processed to render the panoramic scene\nimages. We provide a thorough quantitative evaluation on the SemanticKitti\ndataset and show that our proposed framework outperforms other strong baseline\nmodels.\n  Our source code is available at\nhttps://github.com/halmstad-University/TITAN-NET\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 08:52:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Cortinhal", "Tiago", ""], ["Kurnaz", "Fatih", ""], ["Aksoy", "Eren", ""]]}, {"id": "2106.13982", "submitter": "Roger Trullo", "authors": "Arturo Mendoza, Roger Trullo, Yanneck Wielhorski", "title": "Descriptive Modeling of Textiles using FE Simulations and Deep Learning", "comments": "Preprint submitted to Composites Science and Technology. A. Mendoza\n  and R. Trullo contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we propose a novel and fully automated method for extracting the\nyarn geometrical features in woven composites so that a direct parametrization\nof the textile reinforcement is achieved (e.g., FE mesh). Thus, our aim is not\nonly to perform yarn segmentation from tomographic images but rather to provide\na complete descriptive modeling of the fabric. As such, this direct approach\nimproves on previous methods that use voxel-wise masks as intermediate\nrepresentations followed by re-meshing operations (yarn envelope estimation).\nThe proposed approach employs two deep neural network architectures (U-Net and\nMask RCNN). First, we train the U-Net to generate synthetic CT images from the\ncorresponding FE simulations. This allows to generate large quantities of\nannotated data without requiring costly manual annotations. This data is then\nused to train the Mask R-CNN, which is focused on predicting contour points\naround each of the yarns in the image. Experimental results show that our\nmethod is accurate and robust for performing yarn instance segmentation on CT\nimages, this is further validated by quantitative and qualitative analyses.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 09:32:24 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Mendoza", "Arturo", ""], ["Trullo", "Roger", ""], ["Wielhorski", "Yanneck", ""]]}, {"id": "2106.13984", "submitter": "Shuai Yang", "authors": "Shuai Yang, Kai Qiao", "title": "ShapeEditer: a StyleGAN Encoder for Face Swapping", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel encoder, called ShapeEditor, for\nhigh-resolution, realistic and high-fidelity face exchange. First of all, in\norder to ensure sufficient clarity and authenticity, our key idea is to use an\nadvanced pretrained high-quality random face image generator, i.e. StyleGAN, as\nbackbone. Secondly, we design ShapeEditor, a two-step encoder, to make the\nswapped face integrate the identity and attribute of the input faces. In the\nfirst step, we extract the identity vector of the source image and the\nattribute vector of the target image respectively; in the second step, we map\nthe concatenation of identity vector and attribute vector into the\n$\\mathcal{W+}$ potential space. In addition, for learning to map into the\nlatent space of StyleGAN, we propose a set of self-supervised loss functions\nwith which the training data do not need to be labeled manually. Extensive\nexperiments on the test dataset show that the results of our method not only\nhave a great advantage in clarity and authenticity than other state-of-the-art\nmethods, but also reflect the sufficient integration of identity and attribute.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 09:38:45 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yang", "Shuai", ""], ["Qiao", "Kai", ""]]}, {"id": "2106.13992", "submitter": "Chaabane Djeraba", "authors": "Chaabane Djeraba, J\\'er\\^ome Riedi", "title": "Mining atmospheric data", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper overviews two interdependent issues important for mining remote\nsensing data (e.g. images) obtained from atmospheric monitoring missions. The\nfirst issue relates the building new public datasets and benchmarks, which are\nhot priority of the remote sensing community. The second issue is the\ninvestigation of deep learning methodologies for atmospheric data\nclassification based on vast amount of data without annotations and with\nlocalized annotated data provided by sparse observing networks at the surface.\nThe targeted application is air quality assessment and prediction. Air quality\nis defined as the pollution level linked with several atmospheric constituents\nsuch as gases and aerosols. There are dependency relationships between the bad\nair quality, caused by air pollution, and the public health. The target\napplication is the development of a fast prediction model for local and\nregional air quality assessment and tracking. The results of mining data will\nhave significant implication for citizen and decision makers by providing a\nfast prediction and reliable air quality monitoring system able to cover the\nlocal and regional scale through intelligent extrapolation of sparse\nground-based in situ measurement networks.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 10:04:35 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Djeraba", "Chaabane", ""], ["Riedi", "J\u00e9r\u00f4me", ""]]}, {"id": "2106.14008", "submitter": "Wang Zhihua", "authors": "Zhihua Wang, Dingquan Li, Kede Ma", "title": "Semi-Supervised Deep Ensembles for Blind Image Quality Assessment", "comments": "6 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensemble methods are generally regarded to be better than a single model if\nthe base learners are deemed to be \"accurate\" and \"diverse.\" Here we\ninvestigate a semi-supervised ensemble learning strategy to produce\ngeneralizable blind image quality assessment models. We train a multi-head\nconvolutional network for quality prediction by maximizing the accuracy of the\nensemble (as well as the base learners) on labeled data, and the disagreement\n(i.e., diversity) among them on unlabeled data, both implemented by the\nfidelity loss. We conduct extensive experiments to demonstrate the advantages\nof employing unlabeled data for BIQA, especially in model generalization and\nfailure identification.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 11:59:46 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 19:36:30 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wang", "Zhihua", ""], ["Li", "Dingquan", ""], ["Ma", "Kede", ""]]}, {"id": "2106.14019", "submitter": "Hwanhee Lee", "authors": "Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Trung Bui, Kyomin\n  Jung", "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive\n  Learning", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of various text generation metrics such as BERTScore, it\nis still difficult to evaluate the image captions without enough reference\ncaptions due to the diversity of the descriptions. In this paper, we introduce\na new metric UMIC, an Unreferenced Metric for Image Captioning which does not\nrequire reference captions to evaluate image captions. Based on\nVision-and-Language BERT, we train UMIC to discriminate negative captions via\ncontrastive learning. Also, we observe critical problems of the previous\nbenchmark dataset (i.e., human annotations) on image captioning metric, and\nintroduce a new collection of human annotations on the generated captions. We\nvalidate UMIC on four datasets, including our new dataset, and show that UMIC\nhas a higher correlation than all previous metrics that require multiple\nreferences. We release the benchmark dataset and pre-trained models to compute\nthe UMIC.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 13:27:14 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lee", "Hwanhee", ""], ["Yoon", "Seunghyun", ""], ["Dernoncourt", "Franck", ""], ["Bui", "Trung", ""], ["Jung", "Kyomin", ""]]}, {"id": "2106.14033", "submitter": "Xinyi Wang", "authors": "Xinyi Wang, Tiange Xiang, Chaoyi Zhang, Yang Song, Dongnan Liu, Heng\n  Huang, Weidong Cai", "title": "BiX-NAS: Searching Efficient Bi-directional Architecture for Medical\n  Image Segmentation", "comments": "MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recurrent mechanism has recently been introduced into U-Net in various\nmedical image segmentation tasks. Existing studies have focused on promoting\nnetwork recursion via reusing building blocks. Although network parameters\ncould be greatly saved, computational costs still increase inevitably in\naccordance with the pre-set iteration time. In this work, we study a\nmulti-scale upgrade of a bi-directional skip connected network and then\nautomatically discover an efficient architecture by a novel two-phase Neural\nArchitecture Search (NAS) algorithm, namely BiX-NAS. Our proposed method\nreduces the network computational cost by sifting out ineffective multi-scale\nfeatures at different levels and iterations. We evaluate BiX-NAS on two\nsegmentation tasks using three different medical image datasets, and the\nexperimental results show that our BiX-NAS searched architecture achieves the\nstate-of-the-art performance with significantly lower computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 14:33:04 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 12:59:44 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 09:03:36 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Wang", "Xinyi", ""], ["Xiang", "Tiange", ""], ["Zhang", "Chaoyi", ""], ["Song", "Yang", ""], ["Liu", "Dongnan", ""], ["Huang", "Heng", ""], ["Cai", "Weidong", ""]]}, {"id": "2106.14049", "submitter": "Yue Lin", "authors": "Yue Lin, Ningchuan Xiao", "title": "Identifying High Accuracy Regions in Traffic Camera Images to Enhance\n  the Estimation of Road Traffic Metrics: A Quadtree Based Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The growing number of real-time camera feeds in urban areas has made it\npossible to provide high-quality traffic data for effective transportation\nplanning, operations, and management. However, deriving reliable traffic\nmetrics from these camera feeds has been a challenge due to the limitations of\ncurrent vehicle detection techniques, as well as the various camera conditions\nsuch as height and resolution. In this work, a quadtree based algorithm is\ndeveloped to continuously partition the image extent until only regions with\nhigh detection accuracy are remained. These regions are referred to as the\nhigh-accuracy identification regions (HAIR) in this paper. We demonstrate how\nthe use of the HAIR can improve the accuracy of traffic density estimates using\nimages from traffic cameras at different heights and resolutions in Central\nOhio. Our experiments show that the proposed algorithm can be used to derive\nrobust HAIR where vehicle detection accuracy is 41 percent higher than that in\nthe original image extent. The use of the HAIR also significantly improves the\ntraffic density estimation with an overall decrease of 49 percent in root mean\nsquared error.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 15:46:45 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 01:57:44 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Lin", "Yue", ""], ["Xiao", "Ningchuan", ""]]}, {"id": "2106.14060", "submitter": "Mohammed El Hassouni", "authors": "Zakariae Abbad, Ahmed Drissi El Maliani, Said Ouatik El Alaoui,\n  Mohammed El Hassouni", "title": "A Graph-based approach to derive the geodesic distance on Statistical\n  manifolds: Application to Multimedia Information Retrieval", "comments": null, "journal-ref": null, "doi": "10.1109/WINCOM50532.2020.9272434", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we leverage the properties of non-Euclidean Geometry to define\nthe Geodesic distance (GD) on the space of statistical manifolds. The Geodesic\ndistance is a real and intuitive similarity measure that is a good alternative\nto the purely statistical and extensively used Kullback-Leibler divergence\n(KLD). Despite the effectiveness of the GD, a closed-form does not exist for\nmany manifolds, since the geodesic equations are hard to solve. This explains\nthat the major studies have been content to use numerical approximations.\nNevertheless, most of those do not take account of the manifold properties,\nwhich leads to a loss of information and thus to low performances. We propose\nan approximation of the Geodesic distance through a graph-based method. This\nlatter permits to well represent the structure of the statistical manifold, and\nrespects its geometrical properties. Our main aim is to compare the graph-based\napproximation to the state of the art approximations. Thus, the proposed\napproach is evaluated for two statistical manifolds, namely the Weibull\nmanifold and the Gamma manifold, considering the Content-Based Texture\nRetrieval application on different databases.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 16:39:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Abbad", "Zakariae", ""], ["Maliani", "Ahmed Drissi El", ""], ["Alaoui", "Said Ouatik El", ""], ["Hassouni", "Mohammed El", ""]]}, {"id": "2106.14069", "submitter": "Ye Zhu", "authors": "Ye Zhu, Yu Wu, Yi Yang, Yan Yan", "title": "Saying the Unseen: Video Descriptions via Dialog Agents", "comments": "Accepted as a regular paper at TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current vision and language tasks usually take complete visual data (e.g.,\nraw images or videos) as input, however, practical scenarios may often consist\nthe situations where part of the visual information becomes inaccessible due to\nvarious reasons e.g., restricted view with fixed camera or intentional vision\nblock for security concerns. As a step towards the more practical application\nscenarios, we introduce a novel task that aims to describe a video using the\nnatural language dialog between two agents as a supplementary information\nsource given incomplete visual data. Different from most existing\nvision-language tasks where AI systems have full access to images or video\nclips, which may reveal sensitive information such as recognizable human faces\nor voices, we intentionally limit the visual input for AI systems and seek a\nmore secure and transparent information medium, i.e., the natural language\ndialog, to supplement the missing visual information. Specifically, one of the\nintelligent agents - Q-BOT - is given two semantic segmented frames from the\nbeginning and the end of the video, as well as a finite number of opportunities\nto ask relevant natural language questions before describing the unseen video.\nA-BOT, the other agent who has access to the entire video, assists Q-BOT to\naccomplish the goal by answering the asked questions. We introduce two\ndifferent experimental settings with either a generative (i.e., agents generate\nquestions and answers freely) or a discriminative (i.e., agents select the\nquestions and answers from candidates) internal dialog generation process. With\nthe proposed unified QA-Cooperative networks, we experimentally demonstrate the\nknowledge transfer process between the two dialog agents and the effectiveness\nof using the natural language dialog as a supplement for incomplete implicit\nvisions.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 17:36:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhu", "Ye", ""], ["Wu", "Yu", ""], ["Yang", "Yi", ""], ["Yan", "Yan", ""]]}, {"id": "2106.14070", "submitter": "Bowen Wen", "authors": "Andrew S. Morgan, Bowen Wen, Junchi Liang, Abdeslam Boularias, Aaron\n  M. Dollar, and Kostas Bekris", "title": "Vision-driven Compliant Manipulation for Reliable, High-Precision\n  Assembly Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Highly constrained manipulation tasks continue to be challenging for\nautonomous robots as they require high levels of precision, typically less than\n1mm, which is often incompatible with what can be achieved by traditional\nperception systems. This paper demonstrates that the combination of\nstate-of-the-art object tracking with passively adaptive mechanical hardware\ncan be leveraged to complete precision manipulation tasks with tight,\nindustrially-relevant tolerances (0.25mm). The proposed control method closes\nthe loop through vision by tracking the relative 6D pose of objects in the\nrelevant workspace. It adjusts the control reference of both the compliant\nmanipulator and the hand to complete object insertion tasks via within-hand\nmanipulation. Contrary to previous efforts for insertion, our method does not\nrequire expensive force sensors, precision manipulators, or time-consuming,\nonline learning, which is data hungry. Instead, this effort leverages\nmechanical compliance and utilizes an object agnostic manipulation model of the\nhand learned offline, off-the-shelf motion planning, and an RGBD-based object\ntracker trained solely with synthetic data. These features allow the proposed\nsystem to easily generalize and transfer to new tasks and environments. This\npaper describes in detail the system components and showcases its efficacy with\nextensive experiments involving tight tolerance peg-in-hole insertion tasks of\nvarious geometries as well as open-world constrained placement tasks.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 17:54:16 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Morgan", "Andrew S.", ""], ["Wen", "Bowen", ""], ["Liang", "Junchi", ""], ["Boularias", "Abdeslam", ""], ["Dollar", "Aaron M.", ""], ["Bekris", "Kostas", ""]]}, {"id": "2106.14073", "submitter": "Zhicheng Cai", "authors": "Zhicheng Cai", "title": "Interflow: Aggregating Multi-layer Feature Mappings with Attention\n  Mechanism", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, CNN models possess hierarchical structures and utilize the\nfeature mapping of the last layer to obtain the prediction output. However, it\ncan be difficulty to settle the optimal network depth and make the middle\nlayers learn distinguished features. This paper proposes the Interflow\nalgorithm specially for traditional CNN models. Interflow divides CNNs into\nseveral stages according to the depth and makes predictions by the feature\nmappings in each stage. Subsequently, we input these prediction branches into a\nwell-designed attention module, which learns the weights of these prediction\nbranches, aggregates them and obtains the final output. Interflow weights and\nfuses the features learned in both shallower and deeper layers, making the\nfeature information at each stage processed reasonably and effectively,\nenabling the middle layers to learn more distinguished features, and enhancing\nthe model representation ability. In addition, Interflow can alleviate gradient\nvanishing problem, lower the difficulty of network depth selection, and lighten\npossible over-fitting problem by introducing attention mechanism. Besides, it\ncan avoid network degradation as a byproduct. Compared with the original model,\nthe CNN model with Interflow achieves higher test accuracy on multiple\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 18:22:01 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 17:17:12 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 07:50:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Cai", "Zhicheng", ""]]}, {"id": "2106.14082", "submitter": "Nihar Shrikant Bendre", "authors": "Nihar Bendre, Kevin Desai and Peyman Najafirad", "title": "Generalized Zero-Shot Learning using Multimodal Variational Auto-Encoder\n  with Semantic Concepts", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-increasing amount of data, the central challenge in multimodal\nlearning involves limitations of labelled samples. For the task of\nclassification, techniques such as meta-learning, zero-shot learning, and\nfew-shot learning showcase the ability to learn information about novel classes\nbased on prior knowledge. Recent techniques try to learn a cross-modal mapping\nbetween the semantic space and the image space. However, they tend to ignore\nthe local and global semantic knowledge. To overcome this problem, we propose a\nMultimodal Variational Auto-Encoder (M-VAE) which can learn the shared latent\nspace of image features and the semantic space. In our approach we concatenate\nmultimodal data to a single embedding before passing it to the VAE for learning\nthe latent space. We propose the use of a multi-modal loss during the\nreconstruction of the feature embedding through the decoder. Our approach is\ncapable to correlating modalities and exploit the local and global semantic\nknowledge for novel sample predictions. Our experimental results using a MLP\nclassifier on four benchmark datasets show that our proposed model outperforms\nthe current state-of-the-art approaches for generalized zero-shot learning.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 20:08:37 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bendre", "Nihar", ""], ["Desai", "Kevin", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2106.14087", "submitter": "Felix Nobis", "authors": "Felix Nobis, Ehsan Shafiei, Phillip Karle, Johannes Betz and Markus\n  Lienkamp", "title": "Radar Voxel Fusion for 3D Object Detection", "comments": null, "journal-ref": null, "doi": "10.3390/app11125598", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automotive traffic scenes are complex due to the variety of possible\nscenarios, objects, and weather conditions that need to be handled. In contrast\nto more constrained environments, such as automated underground trains,\nautomotive perception systems cannot be tailored to a narrow field of specific\ntasks but must handle an ever-changing environment with unforeseen events. As\ncurrently no single sensor is able to reliably perceive all relevant activity\nin the surroundings, sensor data fusion is applied to perceive as much\ninformation as possible. Data fusion of different sensors and sensor modalities\non a low abstraction level enables the compensation of sensor weaknesses and\nmisdetections among the sensors before the information-rich sensor data are\ncompressed and thereby information is lost after a sensor-individual object\ndetection. This paper develops a low-level sensor fusion network for 3D object\ndetection, which fuses lidar, camera, and radar data. The fusion network is\ntrained and evaluated on the nuScenes data set. On the test set, fusion of\nradar data increases the resulting AP (Average Precision) detection score by\nabout 5.1% in comparison to the baseline lidar network. The radar sensor fusion\nproves especially beneficial in inclement conditions such as rain and night\nscenes. Fusing additional camera data contributes positively only in\nconjunction with the radar fusion, which shows that interdependencies of the\nsensors are important for the detection result. Additionally, the paper\nproposes a novel loss to handle the discontinuity of a simple yaw\nrepresentation for object detection. Our updated loss increases the detection\nand orientation estimation performance for all sensor input configurations. The\ncode for this research has been made available on GitHub.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 20:34:12 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nobis", "Felix", ""], ["Shafiei", "Ehsan", ""], ["Karle", "Phillip", ""], ["Betz", "Johannes", ""], ["Lienkamp", "Markus", ""]]}, {"id": "2106.14101", "submitter": "Youshaa Murhij", "authors": "Youshaa Murhij and Dmitry Yudin", "title": "Real-time 3D Object Detection using Feature Map Flow", "comments": "CVPR 2021 Workshop on autonomous driving (Waymo Real-time 3D\n  Detection)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a real-time 3D detection approach considering\ntime-spatial feature map aggregation from different time steps of deep neural\nmodel inference (named feature map flow, FMF). Proposed approach improves the\nquality of 3D detection center-based baseline and provides real-time\nperformance on the nuScenes and Waymo benchmark. Code is available at\nhttps://github.com/YoushaaMurhij/FMFNet\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 22:20:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Murhij", "Youshaa", ""], ["Yudin", "Dmitry", ""]]}, {"id": "2106.14102", "submitter": "Priyank Kalgaonkar", "authors": "Priyank Kalgaonkar, Mohamed El-Sharkawy", "title": "Image Classification with CondenseNeXt for ARM-Based Computing Platforms", "comments": "6 pages, 7 figures, conference, published IEEE Conference paper", "journal-ref": null, "doi": "10.1109/IEMTRONICS52119.2021.9422541", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate the implementation of our ultra-efficient deep\nconvolutional neural network architecture: CondenseNeXt on NXP BlueBox, an\nautonomous driving development platform developed for self-driving vehicles. We\nshow that CondenseNeXt is remarkably efficient in terms of FLOPs, designed for\nARM-based embedded computing platforms with limited computational resources and\ncan perform image classification without the need of a CUDA enabled GPU.\nCondenseNeXt utilizes the state-of-the-art depthwise separable convolution and\nmodel compression techniques to achieve a remarkable computational efficiency.\nExtensive analyses are conducted on CIFAR-10, CIFAR-100 and ImageNet datasets\nto verify the performance of CondenseNeXt Convolutional Neural Network (CNN)\narchitecture. It achieves state-of-the-art image classification performance on\nthree benchmark datasets including CIFAR-10 (4.79% top-1 error), CIFAR-100\n(21.98% top-1 error) and ImageNet (7.91% single model, single crop top-5\nerror). CondenseNeXt achieves final trained model size improvement of 2.9+ MB\nand up to 59.98% reduction in forward FLOPs compared to CondenseNet and can\nperform image classification on ARM-Based computing platforms without needing a\nCUDA enabled GPU support, with outstanding efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 22:22:03 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kalgaonkar", "Priyank", ""], ["El-Sharkawy", "Mohamed", ""]]}, {"id": "2106.14104", "submitter": "Quanfu Fan", "authors": "Quanfu Fan, Chun-Fu (Richard) Chen, Rameswar Panda", "title": "An Image Classifier Can Suffice For Video Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new perspective on video understanding by casting the video\nrecognition problem as an image recognition task. We show that an image\nclassifier alone can suffice for video understanding without temporal modeling.\nOur approach is simple and universal. It composes input frames into a super\nimage to train an image classifier to fulfill the task of action recognition,\nin exactly the same way as classifying an image. We prove the viability of such\nan idea by demonstrating strong and promising performance on four public\ndatasets including Kinetics400, Something-to-something (V2), MiT and Jester,\nusing a recently developed vision transformer. We also experiment with the\nprevalent ResNet image classifiers in computer vision to further validate our\nidea. The results on Kinetics400 are comparable to some of the best-performed\nCNN approaches based on spatio-temporal modeling. our code and models will be\nmade available at https://github.com/IBM/sifar-pytorch.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 22:28:30 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 20:12:32 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Fan", "Quanfu", "", "Richard"], ["Chun-Fu", "", "", "Richard"], ["Chen", "", ""], ["Panda", "Rameswar", ""]]}, {"id": "2106.14118", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Anurag Bagchi, Jazib Mahmood, Dolton Fernandes, Ravi Kiran\n  Sarvadevabhatla", "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (54.34 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data will be made available.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 00:49:02 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 06:44:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bagchi", "Anurag", ""], ["Mahmood", "Jazib", ""], ["Fernandes", "Dolton", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2106.14124", "submitter": "Changxing Ding", "authors": "Junyang Huang and Changxing Ding", "title": "Attention-guided Progressive Mapping for Profile Face Recognition", "comments": "Accepted by IJCB 2021. Code is available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed great progress in the domain of face\nrecognition thanks to advances in deep learning. However, cross pose face\nrecognition remains a significant challenge. It is difficult for many deep\nlearning algorithms to narrow the performance gap caused by pose variations;\nthe main reasons for this relate to the intra-class discrepancy between face\nimages in different poses and the pose imbalances of training datasets.\nLearning pose-robust features by traversing to the feature space of frontal\nfaces provides an effective and cheap way to alleviate this problem. In this\npaper, we present a method for progressively transforming profile face\nrepresentations to the canonical pose with an attentive pair-wise loss.\nFirstly, to reduce the difficulty of directly transforming the profile face\nfeatures into a frontal pose, we propose to learn the feature residual between\nthe source pose and its nearby pose in a block-byblock fashion, and thus\ntraversing to the feature space of a smaller pose by adding the learned\nresidual. Secondly, we propose an attentive pair-wise loss to guide the feature\ntransformation progressing in the most effective direction. Finally, our\nproposed progressive module and attentive pair-wise loss are light-weight and\neasy to implement, adding only about 7:5% extra parameters. Evaluations on the\nCFP and CPLFW datasets demonstrate the superiority of our proposed method. Code\nis available at https://github.com/hjy1312/AGPM.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 02:21:41 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 10:33:31 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Huang", "Junyang", ""], ["Ding", "Changxing", ""]]}, {"id": "2106.14127", "submitter": "Songwei Ge", "authors": "Songwei Ge and Devi Parikh", "title": "Visual Conceptual Blending with Large-scale Language and Vision Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We ask the question: to what extent can recent large-scale language and image\ngeneration models blend visual concepts? Given an arbitrary object, we identify\na relevant object and generate a single-sentence description of the blend of\nthe two using a language model. We then generate a visual depiction of the\nblend using a text-based image generation model. Quantitative and qualitative\nevaluations demonstrate the superiority of language models over classical\nmethods for conceptual blending, and of recent large-scale image generation\nmodels over prior models for the visual depiction.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 02:48:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ge", "Songwei", ""], ["Parikh", "Devi", ""]]}, {"id": "2106.14132", "submitter": "Yang-tian Sun", "authors": "Yang-tian Sun, Hao-zhi Huang, Xuan Wang, Yu-kun Lai, Wei Liu, Lin Gao", "title": "Robust Pose Transfer with Dynamic Details using Neural Video Rendering", "comments": "Video link: https://www.bilibili.com/video/BV1y64y1C7ge/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pose transfer of human videos aims to generate a high fidelity video of a\ntarget person imitating actions of a source person. A few studies have made\ngreat progress either through image translation with deep latent features or\nneural rendering with explicit 3D features. However, both of them rely on large\namounts of training data to generate realistic results, and the performance\ndegrades on more accessible internet videos due to insufficient training\nframes. In this paper, we demonstrate that the dynamic details can be preserved\neven trained from short monocular videos. Overall, we propose a neural video\nrendering framework coupled with an image-translation-based dynamic details\ngeneration network (D2G-Net), which fully utilizes both the stability of\nexplicit 3D features and the capacity of learning components. To be specific, a\nnovel texture representation is presented to encode both the static and\npose-varying appearance characteristics, which is then mapped to the image\nspace and rendered as a detail-rich frame in the neural rendering stage.\nMoreover, we introduce a concise temporal loss in the training stage to\nsuppress the detail flickering that is made more visible due to high-quality\ndynamic details generated by our method. Through extensive comparisons, we\ndemonstrate that our neural human video renderer is capable of achieving both\nclearer dynamic details and more robust performance even on accessible short\nvideos with only 2k - 4k frames.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 03:40:22 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 05:54:05 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Sun", "Yang-tian", ""], ["Huang", "Hao-zhi", ""], ["Wang", "Xuan", ""], ["Lai", "Yu-kun", ""], ["Liu", "Wei", ""], ["Gao", "Lin", ""]]}, {"id": "2106.14133", "submitter": "Xin Lai", "authors": "Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao, Liwei Wang,\n  Jiaya Jia", "title": "Semi-supervised Semantic Segmentation with Directional Context-aware\n  Consistency", "comments": "Accepted to CVPR 2021. Code is available at\n  https://github.com/dvlab-research/Context-Aware-Consistency", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has made tremendous progress in recent years. However,\nsatisfying performance highly depends on a large number of pixel-level\nannotations. Therefore, in this paper, we focus on the semi-supervised\nsegmentation problem where only a small set of labeled data is provided with a\nmuch larger collection of totally unlabeled images. Nevertheless, due to the\nlimited annotations, models may overly rely on the contexts available in the\ntraining data, which causes poor generalization to the scenes unseen before. A\npreferred high-level representation should capture the contextual information\nwhile not losing self-awareness. Therefore, we propose to maintain the\ncontext-aware consistency between features of the same identity but with\ndifferent contexts, making the representations robust to the varying\nenvironments. Moreover, we present the Directional Contrastive Loss (DC Loss)\nto accomplish the consistency in a pixel-to-pixel manner, only requiring the\nfeature with lower quality to be aligned towards its counterpart. In addition,\nto avoid the false-negative samples and filter the uncertain positive samples,\nwe put forward two sampling strategies. Extensive experiments show that our\nsimple yet effective method surpasses current state-of-the-art methods by a\nlarge margin and also generalizes well with extra image-level annotations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 03:42:40 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lai", "Xin", ""], ["Tian", "Zhuotao", ""], ["Jiang", "Li", ""], ["Liu", "Shu", ""], ["Zhao", "Hengshuang", ""], ["Wang", "Liwei", ""], ["Jia", "Jiaya", ""]]}, {"id": "2106.14137", "submitter": "Riko Suzuki", "authors": "Riko Suzuki and Hitomi Yanaka and Koji Mineshima and Daisuke Bekki", "title": "Building a Video-and-Language Dataset with Human Actions for Multimodal\n  Logical Inference", "comments": "Accepted to MMSR I", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new video-and-language dataset with human actions for\nmultimodal logical inference, which focuses on intentional and aspectual\nexpressions that describe dynamic human actions. The dataset consists of 200\nvideos, 5,554 action labels, and 1,942 action triplets of the form <subject,\npredicate, object> that can be translated into logical semantic\nrepresentations. The dataset is expected to be useful for evaluating multimodal\ninference systems between videos and semantically complicated sentences\nincluding negation and quantification.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 03:57:36 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Suzuki", "Riko", ""], ["Yanaka", "Hitomi", ""], ["Mineshima", "Koji", ""], ["Bekki", "Daisuke", ""]]}, {"id": "2106.14148", "submitter": "Amir Ivry", "authors": "Roger Alimi, Amir Ivry, Elad Fisher, Eyal Weiss", "title": "Machine Learning Detection Algorithm for Large Barkhausen Jumps in\n  Cluttered Environment", "comments": "Accepted to IEEE Magnetics Letters", "journal-ref": "pp. 1-5, vol. 10, year 2019", "doi": "10.1109/LMAG.2019.2938463", "report-no": null, "categories": "cs.LG cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern magnetic sensor arrays conventionally utilize state of the art low\npower magnetometers such as parallel and orthogonal fluxgates. Low power\nfluxgates tend to have large Barkhausen jumps that appear as a dc jump in the\nfluxgate output. This phenomenon deteriorates the signal fidelity and\neffectively increases the internal sensor noise. Even if sensors that are more\nprone to dc jumps can be screened during production, the conventional noise\nmeasurement does not always catch the dc jump because of its sparsity.\nMoreover, dc jumps persist in almost all the sensor cores although at a slower\nbut still intolerable rate. Even if dc jumps can be easily detected in a\nshielded environment, when deployed in presence of natural noise and clutter,\nit can be hard to positively detect them. This work fills this gap and presents\nalgorithms that distinguish dc jumps embedded in natural magnetic field data.\nTo improve robustness to noise, we developed two machine learning algorithms\nthat employ temporal and statistical physical-based features of a pre-acquired\nand well-known experimental data set. The first algorithm employs a support\nvector machine classifier, while the second is based on a neural network\narchitecture. We compare these new approaches to a more classical kernel-based\nmethod. To that purpose, the receiver operating characteristic curve is\ngenerated, which allows diagnosis ability of the different classifiers by\ncomparing their performances across various operation points. The accuracy of\nthe machine learning-based algorithms over the classic method is highly\nemphasized. In addition, high generalization and robustness of the neural\nnetwork can be concluded, based on the rapid convergence of the corresponding\nreceiver operating characteristic curves.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 05:37:12 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Alimi", "Roger", ""], ["Ivry", "Amir", ""], ["Fisher", "Elad", ""], ["Weiss", "Eyal", ""]]}, {"id": "2106.14150", "submitter": "Mojtaba Mahdavi", "authors": "Samira Hosseini, Mojtaba Mahdavi", "title": "Image content dependent semi-fragile watermarking with localized tamper\n  detection", "comments": "32 pages, 11 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-independent watermarks and block-wise independency can be considered\nas vulnerabilities in semi-fragile watermarking methods. In this paper to\nachieve the objectives of semi-fragile watermarking techniques, a method is\nproposed to not have the mentioned shortcomings. In the proposed method, the\nwatermark is generated by relying on image content and a key. Furthermore, the\nembedding scheme causes the watermarked blocks to become dependent on each\nother, using a key. In the embedding phase, the image is partitioned into\nnon-overlapping blocks. In order to detect and separate the different types of\nattacks more precisely, the proposed method embeds three copies of each\nwatermark bit into LWT coefficients of each 4x4 block. In the authentication\nphase, by voting between the extracted bits the error maps are created; these\nmaps indicate image authenticity and reveal the modified regions. Also, in\norder to automate the authentication, the images are classified into four\ncategories using seven features. Classification accuracy in the experiments is\n97.97 percent. It is noted that our experiments demonstrate that the proposed\nmethod is robust against JPEG compression and is competitive with a\nstate-of-the-art semi-fragile watermarking method, in terms of robustness and\nsemi-fragility.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 05:40:56 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hosseini", "Samira", ""], ["Mahdavi", "Mojtaba", ""]]}, {"id": "2106.14156", "submitter": "Zhenhua Liu", "authors": "Zhenhua Liu, Yunhe Wang, Kai Han, Siwei Ma and Wen Gao", "title": "Post-Training Quantization for Vision Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, transformer has achieved remarkable performance on a variety of\ncomputer vision applications. Compared with mainstream convolutional neural\nnetworks, vision transformers are often of sophisticated architectures for\nextracting powerful feature representations, which are more difficult to be\ndeveloped on mobile devices. In this paper, we present an effective\npost-training quantization algorithm for reducing the memory storage and\ncomputational costs of vision transformers. Basically, the quantization task\ncan be regarded as finding the optimal low-bit quantization intervals for\nweights and inputs, respectively. To preserve the functionality of the\nattention mechanism, we introduce a ranking loss into the conventional\nquantization objective that aims to keep the relative order of the\nself-attention results after quantization. Moreover, we thoroughly analyze the\nrelationship between quantization loss of different layers and the feature\ndiversity, and explore a mixed-precision quantization scheme by exploiting the\nnuclear norm of each attention map and output feature. The effectiveness of the\nproposed method is verified on several benchmark models and datasets, which\noutperforms the state-of-the-art post-training quantization algorithms. For\ninstance, we can obtain an 81.29\\% top-1 accuracy using DeiT-B model on\nImageNet dataset with about 8-bit quantization.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 06:27:22 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Zhenhua", ""], ["Wang", "Yunhe", ""], ["Han", "Kai", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2106.14160", "submitter": "Junru Gu", "authors": "Junru Gu, Qiao Sun, Hang Zhao", "title": "DenseTNT: Waymo Open Dataset Motion Prediction Challenge 1st Place\n  Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous driving, goal-based multi-trajectory prediction methods are\nproved to be effective recently, where they first score goal candidates, then\nselect a final set of goals, and finally complete trajectories based on the\nselected goals. However, these methods usually involve goal predictions based\non sparse predefined anchors. In this work, we propose an anchor-free model,\nnamed DenseTNT, which performs dense goal probability estimation for trajectory\nprediction. Our model achieves state-of-the-art performance, and ranks 1st on\nthe Waymo Open Dataset Motion Prediction Challenge.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 07:21:29 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gu", "Junru", ""], ["Sun", "Qiao", ""], ["Zhao", "Hang", ""]]}, {"id": "2106.14162", "submitter": "Bowen Yang", "authors": "Bowen Yang, Jing Zhang, Zhenfei Yin, Jing Shao", "title": "Few-Shot Domain Expansion for Face Anti-Spoofing", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face anti-spoofing (FAS) is an indispensable and widely used module in face\nrecognition systems. Although high accuracy has been achieved, a FAS system\nwill never be perfect due to the non-stationary applied environments and the\npotential emergence of new types of presentation attacks in real-world\napplications. In practice, given a handful of labeled samples from a new\ndeployment scenario (target domain) and abundant labeled face images in the\nexisting source domain, the FAS system is expected to perform well in the new\nscenario without sacrificing the performance on the original domain. To this\nend, we identify and address a more practical problem: Few-Shot Domain\nExpansion for Face Anti-Spoofing (FSDE-FAS). This problem is challenging since\nwith insufficient target domain training samples, the model may suffer from\nboth overfitting to the target domain and catastrophic forgetting of the source\ndomain. To address the problem, this paper proposes a Style transfer-based\nAugmentation for Semantic Alignment (SASA) framework. We propose to augment the\ntarget data by generating auxiliary samples based on photorealistic style\ntransfer. With the assistant of the augmented data, we further propose a\ncarefully designed mechanism to align different domains from both\ninstance-level and distribution-level, and then stabilize the performance on\nthe source domain with a less-forgetting constraint. Two benchmarks are\nproposed to simulate the FSDE-FAS scenarios, and the experimental results show\nthat the proposed SASA method outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 07:38:50 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yang", "Bowen", ""], ["Zhang", "Jing", ""], ["Yin", "Zhenfei", ""], ["Shao", "Jing", ""]]}, {"id": "2106.14166", "submitter": "Cheng Sun", "authors": "Cheng Sun, Chi-Wei Hsiao, Ning-Hsu Wang, Min Sun, Hwann-Tzong Chen", "title": "Indoor Panorama Planar 3D Reconstruction via Divide and Conquer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor panorama typically consists of human-made structures parallel or\nperpendicular to gravity. We leverage this phenomenon to approximate the scene\nin a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this\nend, we propose an effective divide-and-conquer strategy that divides pixels\nbased on their plane orientation estimation; then, the succeeding instance\nsegmentation module conquers the task of planes clustering more easily in each\nplane orientation group. Besides, parameters of V-planes depend on camera yaw\nrotation, but translation-invariant CNNs are less aware of the yaw change. We\nthus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We\ncreate a benchmark for indoor panorama planar reconstruction by extending\nexisting 360 depth datasets with ground truth H\\&V-planes (referred to as\nPanoH&V dataset) and adopt state-of-the-art planar reconstruction methods to\npredict H\\&V-planes as our baselines. Our method outperforms the baselines by a\nlarge margin on the proposed dataset.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 07:58:29 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sun", "Cheng", ""], ["Hsiao", "Chi-Wei", ""], ["Wang", "Ning-Hsu", ""], ["Sun", "Min", ""], ["Chen", "Hwann-Tzong", ""]]}, {"id": "2106.14178", "submitter": "Quanziang Wang", "authors": "Quanziang Wang, Renzhen Wang, Yuexiang Li, Kai Ma, Yefeng Zheng, Deyu\n  Meng", "title": "Residual Moment Loss for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Location information is proven to benefit the deep learning models on\ncapturing the manifold structure of target objects, and accordingly boosts the\naccuracy of medical image segmentation. However, most existing methods encode\nthe location information in an implicit way, e.g. the distance transform maps,\nwhich describe the relative distance from each pixel to the contour boundary,\nfor the network to learn. These implicit approaches do not fully exploit the\nposition information (i.e. absolute location) of targets. In this paper, we\npropose a novel loss function, namely residual moment (RM) loss, to explicitly\nembed the location information of segmentation targets during the training of\ndeep learning networks. Particularly, motivated by image moments, the\nsegmentation prediction map and ground-truth map are weighted by coordinate\ninformation. Then our RM loss encourages the networks to maintain the\nconsistency between the two weighted maps, which promotes the segmentation\nnetworks to easily locate the targets and extract manifold-structure-related\nfeatures. We validate the proposed RM loss by conducting extensive experiments\non two publicly available datasets, i.e., 2D optic cup and disk segmentation\nand 3D left atrial segmentation. The experimental results demonstrate the\neffectiveness of our RM loss, which significantly boosts the accuracy of\nsegmentation networks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 09:31:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Quanziang", ""], ["Wang", "Renzhen", ""], ["Li", "Yuexiang", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""], ["Meng", "Deyu", ""]]}, {"id": "2106.14183", "submitter": "Jun Bao", "authors": "Jun Bao, Buyu Liu, Jun Yu", "title": "The Story in Your Eyes: An Individual-difference-aware Model for\n  Cross-person Gaze Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel method on refining cross-person gaze prediction task with\neye/face images only by explicitly modelling the person-specific differences.\nSpecifically, we first assume that we can obtain some initial gaze prediction\nresults with existing method, which we refer to as InitNet, and then introduce\nthree modules, the Validity Module (VM), Self-Calibration (SC) and\nPerson-specific Transform (PT)) Module. By predicting the reliability of\ncurrent eye/face images, our VM is able to identify invalid samples, e.g. eye\nblinking images, and reduce their effects in our modelling process. Our SC and\nPT module then learn to compensate for the differences on valid samples only.\nThe former models the translation offsets by bridging the gap between initial\npredictions and dataset-wise distribution. And the later learns more general\nperson-specific transformation by incorporating the information from existing\ninitial predictions of the same person. We validate our ideas on three publicly\navailable datasets, EVE, XGaze and MPIIGaze and demonstrate that our proposed\nmethod outperforms the SOTA methods significantly on all of them, e.g.\nrespectively 21.7%, 36.0% and 32.9% relative performance improvements. We won\nthe GAZE 2021 Competition on the EVE dataset. Our code can be found here\nhttps://github.com/bjj9/EVE_SCPT.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 10:14:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bao", "Jun", ""], ["Liu", "Buyu", ""], ["Yu", "Jun", ""]]}, {"id": "2106.14184", "submitter": "Rwik Rana", "authors": "Praveen Venkatesh, Rwik Rana, Varun Jain", "title": "Memory Guided Road Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In self driving car applications, there is a requirement to predict the\nlocation of the lane given an input RGB front facing image. In this paper, we\npropose an architecture that allows us to increase the speed and robustness of\nroad detection without a large hit in accuracy by introducing an underlying\nshared feature space that is propagated over time, which serves as a flowing\ndynamic memory. By utilizing the gist of previous frames, we train the network\nto predict the current road with a greater accuracy and lesser deviation from\nprevious frames.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 10:17:02 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Venkatesh", "Praveen", ""], ["Rana", "Rwik", ""], ["Jain", "Varun", ""]]}, {"id": "2106.14186", "submitter": "Michele La Ferla", "authors": "Michele La Ferla, Matthew Montebello and Dylan Seychell", "title": "An XAI Approach to Deep Learning Models in the Detection of Ductal\n  Carcinoma in Situ", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the last decade or so, there has been an insurgence in the deep\nlearning community to solve health-related issues, particularly breast cancer.\nFollowing the Camelyon-16 challenge in 2016, several researchers have dedicated\ntheir time to build Convolutional Neural Networks (CNNs) to help radiologists\nand other clinicians diagnose breast cancer. In particular, there has been an\nemphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage\nbreast cancer. Large companies have given their fair share of research into\nthis subject, among these Google Deepmind who developed a model in 2020 that\nhas proven to be better than radiologists themselves to diagnose breast cancer\ncorrectly.\n  We found that among the issues which exist, there is a need for an\nexplanatory system that goes through the hidden layers of a CNN to highlight\nthose pixels that contributed to the classification of a mammogram. We then\nchose an open-source, reasonably successful project developed by Prof. Shen,\nusing the CBIS-DDSM image database to run our experiments on. It was later\nimproved using the Resnet-50 and VGG-16 patch-classifiers, analytically\ncomparing the outcome of both. The results showed that the Resnet-50 one\nconverged earlier in the experiments.\n  Following the research by Montavon and Binder, we used the DeepTaylor\nLayer-wise Relevance Propagation (LRP) model to highlight those pixels and\nregions within a mammogram which contribute most to its classification. This is\nrepresented as a map of those pixels in the original image, which contribute to\nthe diagnosis and the extent to which they contribute to the final\nclassification. The most significant advantage of this algorithm is that it\nperforms exceptionally well with the Resnet-50 patch classifier architecture.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 10:22:33 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["La Ferla", "Michele", ""], ["Montebello", "Matthew", ""], ["Seychell", "Dylan", ""]]}, {"id": "2106.14190", "submitter": "Stephen MacDonell", "authors": "Nidhi Gowdra, Roopak Sinha, Stephen MacDonell and Wei Qi Yan", "title": "Mitigating severe over-parameterization in deep convolutional neural\n  networks through forced feature abstraction and compression with an\n  entropy-based heuristic", "comments": "Journal paper, 14 pages, 3 tables, 3 figures", "journal-ref": "Pattern Recognition 119(2021), pp.108057", "doi": "10.1016/j.patcog.2021.108057.", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and\nResNeXt-56 are severely over-parameterized, necessitating a consequent increase\nin the computational resources required for model training which scales\nexponentially for increments in model depth. In this paper, we propose an\nEntropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust\nand simple, yet effective in resolving the problem of over-parameterization\nwith regards to network depth of CNN model. The EBCLE heuristic employs a\npriori knowledge of the entropic data distribution of input datasets to\ndetermine an upper bound for convolutional network depth, beyond which identity\ntransformations are prevalent offering insignificant contributions for\nenhancing model performance. Restricting depth redundancies by forcing feature\ncompression and abstraction restricts over-parameterization while decreasing\ntraining time by 24.99% - 78.59% without degradation in model performance. We\npresent empirical evidence to emphasize the relative effectiveness of broader,\nyet shallower models trained using the EBCLE heuristic, which maintains or\noutperforms baseline classification accuracies of narrower yet deeper models.\nThe EBCLE heuristic is architecturally agnostic and EBCLE based CNN models\nrestrict depth redundancies resulting in enhanced utilization of the available\ncomputational resources. The proposed EBCLE heuristic is a compelling technique\nfor researchers to analytically justify their HyperParameter (HP) choices for\nCNNs. Empirical validation of the EBCLE heuristic in training CNN models was\nestablished on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10,\nMNIST) and four network architectures (DenseNet, ResNet, ResNeXt and\nEfficientNet B0-B2) with appropriate statistical tests employed to infer any\nconclusive claims presented in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 10:34:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gowdra", "Nidhi", ""], ["Sinha", "Roopak", ""], ["MacDonell", "Stephen", ""], ["Yan", "Wei Qi", ""]]}, {"id": "2106.14192", "submitter": "Jianye Pang", "authors": "Kai Yi, Jianye Pang, Yungeng Zhang, Xiangrui Zeng, Min Xu", "title": "Disentangling semantic features of macromolecules in Cryo-Electron\n  Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cryo-electron tomography (Cryo-ET) is a 3D imaging technique that enables the\nsystemic study of shape, abundance, and distribution of macromolecular\nstructures in single cells in near-atomic resolution. However, the systematic\nand efficient $\\textit{de novo}$ recognition and recovery of macromolecular\nstructures captured by Cryo-ET are very challenging due to the structural\ncomplexity and imaging limits. Even macromolecules with identical structures\nhave various appearances due to different orientations and imaging limits, such\nas noise and the missing wedge effect. Explicitly disentangling the semantic\nfeatures of macromolecules is crucial for performing several downstream\nanalyses on the macromolecules. This paper has addressed the problem by\nproposing a 3D Spatial Variational Autoencoder that explicitly disentangle the\nstructure, orientation, and shift of macromolecules. Extensive experiments on\nboth synthesized and real cryo-ET datasets and cross-domain evaluations\ndemonstrate the efficacy of our method.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 10:41:26 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yi", "Kai", ""], ["Pang", "Jianye", ""], ["Zhang", "Yungeng", ""], ["Zeng", "Xiangrui", ""], ["Xu", "Min", ""]]}, {"id": "2106.14193", "submitter": "Haitao Lin", "authors": "Haitao Lin, Zichang Liu, Chilam Cheang, Lingwei Zhang, Yanwei Fu,\n  Xiangyang Xue", "title": "DONet: Learning Category-Level 6D Object Pose and Size Estimation from\n  Depth Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method of Category-level 6D Object Pose and Size Estimation\n(COPSE) from a single depth image, without external pose-annotated real-world\ntraining data. While previous works exploit visual cues in RGB(D) images, our\nmethod makes inferences based on the rich geometric information of the object\nin the depth channel alone. Essentially, our framework explores such geometric\ninformation by learning the unified 3D Orientation-Consistent Representations\n(3D-OCR) module, and further enforced by the property of Geometry-constrained\nReflection Symmetry (GeoReS) module. The magnitude information of object size\nand the center point is finally estimated by Mirror-Paired Dimensional\nEstimation (MPDE) module. Extensive experiments on the category-level NOCS\nbenchmark demonstrate that our framework competes with state-of-the-art\napproaches that require labeled real-world images. We also deploy our approach\nto a physical Baxter robot to perform manipulation tasks on unseen but\ncategory-known instances, and the results further validate the efficacy of our\nproposed model. Our videos are available in the supplementary material.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 10:41:50 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lin", "Haitao", ""], ["Liu", "Zichang", ""], ["Cheang", "Chilam", ""], ["Zhang", "Lingwei", ""], ["Fu", "Yanwei", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2106.14195", "submitter": "Jaroslav Macke", "authors": "J. Macke, J. Sedlar, M. Olsak, J. Urban, J. Sivic", "title": "Learning to solve geometric construction problems from images", "comments": "16 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a purely image-based method for finding geometric constructions\nwith a ruler and compass in the Euclidea geometric game. The method is based on\nadapting the Mask R-CNN state-of-the-art image processing neural architecture\nand adding a tree-based search procedure to it. In a supervised setting, the\nmethod learns to solve all 68 kinds of geometric construction problems from the\nfirst six level packs of Euclidea with an average 92% accuracy. When evaluated\non new kinds of problems, the method can solve 31 of the 68 kinds of Euclidea\nproblems. We believe that this is the first time that a purely image-based\nlearning has been trained to solve geometric construction problems of this\ndifficulty.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 10:47:41 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Macke", "J.", ""], ["Sedlar", "J.", ""], ["Olsak", "M.", ""], ["Urban", "J.", ""], ["Sivic", "J.", ""]]}, {"id": "2106.14207", "submitter": "Amith Khandakar Mr.", "authors": "Amith Khandakar, Muhammad E. H. Chowdhury, Mamun Bin Ibne Reaz, Sawal\n  Hamid Md Ali, Md Anwarul Hasan, Serkan Kiranyaz, Tawsifur Rahman, Rashad\n  Alfkey, Ahmad Ashrif A. Bakar, Rayaz A. Malik", "title": "A Machine Learning Model for Early Detection of Diabetic Foot using\n  Thermogram Images", "comments": "23 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetes foot ulceration (DFU) and amputation are a cause of significant\nmorbidity. The prevention of DFU may be achieved by the identification of\npatients at risk of DFU and the institution of preventative measures through\neducation and offloading. Several studies have reported that thermogram images\nmay help to detect an increase in plantar temperature prior to DFU. However,\nthe distribution of plantar temperature may be heterogeneous, making it\ndifficult to quantify and utilize to predict outcomes. We have compared a\nmachine learning-based scoring technique with feature selection and\noptimization techniques and learning classifiers to several state-of-the-art\nConvolutional Neural Networks (CNNs) on foot thermogram images and propose a\nrobust solution to identify the diabetic foot. A comparatively shallow CNN\nmodel, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram\nimage-based classification and the AdaBoost Classifier used 10 features and\nachieved an F1 score of 97 %. A comparison of the inference time for the\nbest-performing networks confirmed that the proposed algorithm can be deployed\nas a smartphone application to allow the user to monitor the progression of the\nDFU in a home setting.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 11:37:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Khandakar", "Amith", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Reaz", "Mamun Bin Ibne", ""], ["Ali", "Sawal Hamid Md", ""], ["Hasan", "Md Anwarul", ""], ["Kiranyaz", "Serkan", ""], ["Rahman", "Tawsifur", ""], ["Alfkey", "Rashad", ""], ["Bakar", "Ahmad Ashrif A.", ""], ["Malik", "Rayaz A.", ""]]}, {"id": "2106.14208", "submitter": "Mete Ahishali", "authors": "Mete Ahishali, Mehmet Yamac, Serkan Kiranyaz, Moncef Gabbouj", "title": "Representation Based Regression for Object Distance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a novel approach to predict the distances of the\ndetected objects in an observed scene. The proposed approach modifies the\nrecently proposed Convolutional Support Estimator Networks (CSENs). CSENs are\ndesigned to compute a direct mapping for the Support Estimation (SE) task in a\nrepresentation-based classification problem. We further propose and demonstrate\nthat representation-based methods (sparse or collaborative representation) can\nbe used in well-designed regression problems. To the best of our knowledge,\nthis is the first representation-based method proposed for performing a\nregression task by utilizing the modified CSENs; and hence, we name this novel\napproach as Representation-based Regression (RbR). The initial version of CSENs\nhas a proxy mapping stage (i.e., a coarse estimation for the support set) that\nis required for the input. In this study, we improve the CSEN model by\nproposing Compressive Learning CSEN (CL-CSEN) that has the ability to jointly\noptimize the so-called proxy mapping stage along with convolutional layers. The\nexperimental evaluations using the KITTI 3D Object Detection distance\nestimation dataset show that the proposed method can achieve a significantly\nimproved distance estimation performance over all competing methods. Finally,\nthe software implementations of the methods are publicly shared at\nhttps://github.com/meteahishali/CSENDistance.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 11:45:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ahishali", "Mete", ""], ["Yamac", "Mehmet", ""], ["Kiranyaz", "Serkan", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2106.14248", "submitter": "Chun-Mei Feng", "authors": "Chun-Mei Feng and Yunlu Yan and Geng Chen, Huazhu Fu and Yong Xu and\n  Ling Shao", "title": "Accelerated Multi-Modal MR Imaging with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accelerating multi-modal magnetic resonance (MR) imaging is a new and\neffective solution for fast MR imaging, providing superior performance in\nrestoring the target modality from its undersampled counterpart with guidance\nfrom an auxiliary modality. However, existing works simply introduce the\nauxiliary modality as prior information, lacking in-depth investigations on the\npotential mechanisms for fusing two modalities. Further, they usually rely on\nthe convolutional neural networks (CNNs), which focus on local information and\nprevent them from fully capturing the long-distance dependencies of global\nknowledge. To this end, we propose a multi-modal transformer (MTrans), which is\ncapable of transferring multi-scale features from the target modality to the\nauxiliary modality, for accelerated MR imaging. By restructuring the\ntransformer architecture, our MTrans gains a powerful ability to capture deep\nmulti-modal information. More specifically, the target modality and the\nauxiliary modality are first split into two branches and then fused using a\nmulti-modal transformer module. This module is based on an improved multi-head\nattention mechanism, named the cross attention module, which absorbs features\nfrom the auxiliary modality that contribute to the target modality. Our\nframework provides two appealing benefits: (i) MTrans is the first attempt at\nusing improved transformers for multi-modal MR imaging, affording more global\ninformation compared with CNN-based methods. (ii) A new cross attention module\nis proposed to exploit the useful information in each branch at different\nscales. It affords both distinct structural information and subtle pixel-level\ninformation, which supplement the target modality effectively.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 15:01:30 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 13:37:15 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Feng", "Chun-Mei", ""], ["Yan", "Yunlu", ""], ["Chen", "Geng", ""], ["Fu", "Huazhu", ""], ["Xu", "Yong", ""], ["Shao", "Ling", ""]]}, {"id": "2106.14256", "submitter": "Bojing Liu", "authors": "Boing Liu, Yinxi Wang, Philippe Weitz, Johan Lindberg, Lars Egevad,\n  Henrik Gr\\\"onberg, Martin Eklund, Mattias Rantalainen", "title": "Using deep learning to detect patients at risk for prostate cancer\n  despite benign biopsies", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background: Transrectal ultrasound guided systematic biopsies of the prostate\nis a routine procedure to establish a prostate cancer diagnosis. However, the\n10-12 prostate core biopsies only sample a relatively small volume of the\nprostate, and tumour lesions in regions between biopsy cores can be missed,\nleading to a well-known low sensitivity to detect clinically relevant cancer.\nAs a proof-of-principle, we developed and validated a deep convolutional neural\nnetwork model to distinguish between morphological patterns in benign prostate\nbiopsy whole slide images from men with and without established cancer.\nMethods: This study included 14,354 hematoxylin and eosin stained whole slide\nimages from benign prostate biopsies from 1,508 men in two groups: men without\nan established prostate cancer (PCa) diagnosis and men with at least one core\nbiopsy diagnosed with PCa. 80% of the participants were assigned as training\ndata and used for model optimization (1,211 men), and the remaining 20% (297\nmen) as a held-out test set used to evaluate model performance. An ensemble of\n10 deep convolutional neural network models was optimized for classification of\nbiopsies from men with and without established cancer. Hyperparameter\noptimization and model selection was performed by cross-validation in the\ntraining data . Results: Area under the receiver operating characteristic curve\n(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy\nlevel and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a\nspecificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:\nThe developed model has the ability to detect men with risk of missed PCa due\nto under-sampling of the prostate. The proposed model has the potential to\nreduce the number of false negative cases in routine systematic prostate\nbiopsies and to indicate men who could benefit from MRI-guided re-biopsy.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 15:21:33 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Boing", ""], ["Wang", "Yinxi", ""], ["Weitz", "Philippe", ""], ["Lindberg", "Johan", ""], ["Egevad", "Lars", ""], ["Gr\u00f6nberg", "Henrik", ""], ["Eklund", "Martin", ""], ["Rantalainen", "Mattias", ""]]}, {"id": "2106.14259", "submitter": "Hitoshi Nishimura", "authors": "Hitoshi Nishimura, Satoshi Komorita, Yasutomo Kawanishi, Hiroshi\n  Murase", "title": "SDOF-Tracker: Fast and Accurate Multiple Human Tracking by\n  Skipped-Detection and Optical-Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple human tracking is a fundamental problem for scene understanding.\nAlthough both accuracy and speed are required in real-world applications,\nrecent tracking methods based on deep learning have focused on accuracy and\nrequire substantial running time. This study aims to improve running speed by\nperforming human detection at a certain frame interval because it accounts for\nmost of the running time. The question is how to maintain accuracy while\nskipping human detection. In this paper, we propose a method that complements\nthe detection results with optical flow, based on the fact that someone's\nappearance does not change much between adjacent frames. To maintain the\ntracking accuracy, we introduce robust interest point selection within human\nregions and a tracking termination metric calculated by the distribution of the\ninterest points. On the MOT20 dataset in the MOTChallenge, the proposed\nSDOF-Tracker achieved the best performance in terms of the total running speed\nwhile maintaining the MOTA metric. Our code is available at\nhttps://anonymous.4open.science/r/sdof-tracker-75AE.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 15:35:35 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 04:58:45 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Nishimura", "Hitoshi", ""], ["Komorita", "Satoshi", ""], ["Kawanishi", "Yasutomo", ""], ["Murase", "Hiroshi", ""]]}, {"id": "2106.14269", "submitter": "Shuo Jiang", "authors": "Shuo Jiang, Jianxi Luo, Jie Hu, Christopher L. Magee", "title": "Deep Learning for Technical Document Classification", "comments": "34 pages, 7 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 16:12:47 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Jiang", "Shuo", ""], ["Luo", "Jianxi", ""], ["Hu", "Jie", ""], ["Magee", "Christopher L.", ""]]}, {"id": "2106.14274", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen, Andrea Tagliasacchi, Hao Zhang", "title": "Learning Mesh Representations via Binary Space Partitioning Tree\n  Networks", "comments": "Accepted to TPAMI. This is the extended journal version of BSP-Net\n  (arXiv:1911.06971) from CVPR 2020", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3093440", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polygonal meshes are ubiquitous, but have only played a relatively minor role\nin the deep learning revolution. State-of-the-art neural generative models for\n3D shapes learn implicit functions and generate meshes via expensive\niso-surfacing. We overcome these challenges by employing a classical spatial\ndata structure from computer graphics, Binary Space Partitioning (BSP), to\nfacilitate 3D learning. The core operation of BSP involves recursive\nsubdivision of 3D space to obtain convex sets. By exploiting this property, we\ndevise BSP-Net, a network that learns to represent a 3D shape via convex\ndecomposition without supervision. The network is trained to reconstruct a\nshape using a set of convexes obtained from a BSP-tree built over a set of\nplanes, where the planes and convexes are both defined by learned network\nweights. BSP-Net directly outputs polygonal meshes from the inferred convexes.\nThe generated meshes are watertight, compact (i.e., low-poly), and well suited\nto represent sharp geometry. We show that the reconstruction quality by BSP-Net\nis competitive with those from state-of-the-art methods while using much fewer\nprimitives. We also explore variations to BSP-Net including using a more\ngeneric decoder for reconstruction, more general primitives than planes, as\nwell as training a generative model with variational auto-encoders. Code is\navailable at https://github.com/czq142857/BSP-NET-original.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 16:37:54 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 00:24:22 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chen", "Zhiqin", ""], ["Tagliasacchi", "Andrea", ""], ["Zhang", "Hao", ""]]}, {"id": "2106.14275", "submitter": "Shafin Rahman", "authors": "Townim Chowdhury, Mahira Jalisha, Ali Cheraghian, Shafin Rahman", "title": "Learning without Forgetting for 3D Point Cloud Objects", "comments": "Accepted in International Work-Conference on Artificial and Natural\n  Neural Networks, (IWANN) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When we fine-tune a well-trained deep learning model for a new set of\nclasses, the network learns new concepts but gradually forgets the knowledge of\nold training. In some real-life applications, we may be interested in learning\nnew classes without forgetting the capability of previous experience. Such\nlearning without forgetting problem is often investigated using 2D image\nrecognition tasks. In this paper, considering the growth of depth camera\ntechnology, we address the same problem for the 3D point cloud object data.\nThis problem becomes more challenging in the 3D domain than 2D because of the\nunavailability of large datasets and powerful pretrained backbone models. We\ninvestigate knowledge distillation techniques on 3D data to reduce catastrophic\nforgetting of the previous training. Moreover, we improve the distillation\nprocess by using semantic word vectors of object classes. We observe that\nexploring the interrelation of old and new knowledge during training helps to\nlearn new concepts without forgetting old ones. Experimenting on three 3D point\ncloud recognition backbones (PointNet, DGCNN, and PointConv) and synthetic\n(ModelNet40, ModelNet10) and real scanned (ScanObjectNN) datasets, we establish\nnew baseline results on learning without forgetting for 3D data. This research\nwill instigate many future works in this area.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 16:39:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chowdhury", "Townim", ""], ["Jalisha", "Mahira", ""], ["Cheraghian", "Ali", ""], ["Rahman", "Shafin", ""]]}, {"id": "2106.14290", "submitter": "Aleksandr Petiushko", "authors": "Anton Razzhigaev, Klim Kireev, Igor Udovichenko, Aleksandr Petiushko", "title": "Darker than Black-Box: Face Reconstruction from Similarity Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods for inversion of face recognition models were recently\npresented, attempting to reconstruct a face from deep templates. Although some\nof these approaches work in a black-box setup using only face embeddings,\nusually, on the end-user side, only similarity scores are provided. Therefore,\nthese algorithms are inapplicable in such scenarios. We propose a novel\napproach that allows reconstructing the face querying only similarity scores of\nthe black-box model. While our algorithm operates in a more general setup,\nexperiments show that it is query efficient and outperforms the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 17:25:46 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 14:35:37 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Razzhigaev", "Anton", ""], ["Kireev", "Klim", ""], ["Udovichenko", "Igor", ""], ["Petiushko", "Aleksandr", ""]]}, {"id": "2106.14292", "submitter": "Rohit Jain", "authors": "Rohit Kumar Jain, Prasen Kumar Sharma, Sibaji Gaj, Arijit Sur and\n  Palash Ghosh", "title": "Knee Osteoarthritis Severity Prediction using an Attentive Multi-Scale\n  Deep Convolutional Neural Network", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knee Osteoarthritis (OA) is a destructive joint disease identified by joint\nstiffness, pain, and functional disability concerning millions of lives across\nthe globe. It is generally assessed by evaluating physical symptoms, medical\nhistory, and other joint screening tests like radiographs, Magnetic Resonance\nImaging (MRI), and Computed Tomography (CT) scans. Unfortunately, the\nconventional methods are very subjective, which forms a barrier in detecting\nthe disease progression at an early stage. This paper presents a deep\nlearning-based framework, namely OsteoHRNet, that automatically assesses the\nKnee OA severity in terms of Kellgren and Lawrence (KL) grade classification\nfrom X-rays. As a primary novelty, the proposed approach is built upon one of\nthe most recent deep models, called the High-Resolution Network (HRNet), to\ncapture the multi-scale features of knee X-rays. In addition, we have also\nincorporated an attention mechanism to filter out the counterproductive\nfeatures and boost the performance further. Our proposed model has achieved the\nbest multiclass accuracy of 71.74% and MAE of 0.311 on the baseline cohort of\nthe OAI dataset, which is a remarkable gain over the existing best-published\nworks. We have also employed the Gradient-based Class Activation Maps\n(Grad-CAMs) visualization to justify the proposed network learning.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 17:29:46 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Jain", "Rohit Kumar", ""], ["Sharma", "Prasen Kumar", ""], ["Gaj", "Sibaji", ""], ["Sur", "Arijit", ""], ["Ghosh", "Palash", ""]]}, {"id": "2106.14306", "submitter": "Rongjun Qin", "authors": "Rongjun Qin, Shuang Song, Xiao Ling, Mostafa Elhashash", "title": "3D Reconstruction through Fusion of Cross-View Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D recovery from multi-stereo and stereo images, as an important application\nof the image-based perspective geometry, serves many applications in computer\nvision, remote sensing and Geomatics. In this chapter, the authors utilize the\nimaging geometry and present approaches that perform 3D reconstruction from\ncross-view images that are drastically different in their viewpoints. We\nintroduce our framework that takes ground-view images and satellite images for\nfull 3D recovery, which includes necessary methods in satellite and\nground-based point cloud generation from images, 3D data co-registration,\nfusion and mesh generation. We demonstrate our proposed framework on a dataset\nconsisting of twelve satellite images and 150k video frames acquired through a\nvehicle-mounted Go-pro camera and demonstrate the reconstruction results. We\nhave also compared our results with results generated from an intuitive\nprocessing pipeline that involves typical geo-registration and meshing methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 18:31:08 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Qin", "Rongjun", ""], ["Song", "Shuang", ""], ["Ling", "Xiao", ""], ["Elhashash", "Mostafa", ""]]}, {"id": "2106.14307", "submitter": "Rongjun Qin", "authors": "Rongjun Qin, Xu Huang", "title": "Geometric Processing for Image-based 3D Object Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based 3D object modeling refers to the process of converting raw\noptical images to 3D digital representations of the objects. Very often, such\nmodels are desired to be dimensionally true, semantically labeled with\nphotorealistic appearance (reality-based modeling). Laser scanning was deemed\nas the standard (and direct) way to obtaining highly accurate 3D measurements\nof objects, while one would have to abide the high acquisition cost and its\nunavailability on some of the platforms. Nowadays the image-based methods\nbackboned by the recently developed advanced dense image matching algorithms\nand geo-referencing paradigms, are becoming the dominant approaches, due to its\nhigh flexibility, availability and low cost. The largely automated geometric\nprocessing of images in a 3D object reconstruction workflow, from\nordered/unordered raw imagery to textured meshes, is becoming a critical part\nof the reality-based 3D modeling. This article summarizes the overall geometric\nprocessing workflow, with focuses on introducing the state-of-the-art methods\nof three major components of geometric processing: 1) geo-referencing; 2) Image\ndense matching 3) texture mapping. Finally, we will draw conclusions and share\nour outlooks of the topics discussed in this article.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 18:33:30 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Qin", "Rongjun", ""], ["Huang", "Xu", ""]]}, {"id": "2106.14309", "submitter": "Rongjun Qin", "authors": "Rongjun Qin", "title": "Change Detection for Geodatabase Updating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geodatabase (vectorized data) nowadays becomes a rather standard digital\ncity infrastructure; however, updating geodatabase efficiently and economically\nremains a fundamental and practical issue in the geospatial industry. The cost\nof building a geodatabase is extremely high and labor intensive, and very often\nthe maps we use have several months and even years of latency. One solution is\nto develop more automated methods for (vectorized) geospatial data generation,\nwhich has been proven a difficult task in the past decades. An alternative\nsolution is to first detect the differences between the new data and the\nexisting geospatial data, and then only update the area identified as changes.\nThe second approach is becoming more favored due to its high practicality and\nflexibility. A highly relevant technique is change detection. This article aims\nto provide an overview the state-of-the-art change detection methods in the\nfield of Remote Sensing and Geomatics to support the task of updating\ngeodatabases. Data used for change detection are highly disparate, we therefore\nstructure our review intuitively based on the dimension of the data, being 1)\nchange detection with 2D data; 2) change detection with 3D data. Conclusions\nwill be drawn based on the reviewed efforts in the field, and we will share our\noutlooks of the topic of updating geodatabases.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 18:35:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Qin", "Rongjun", ""]]}, {"id": "2106.14324", "submitter": "Weimin Zhou", "authors": "Weimin Zhou, Sayantan Bhadra, Frank J. Brooks, Hua Li, Mark A.\n  Anastasio", "title": "Learning stochastic object models from medical imaging measurements by\n  use of advanced AmbientGANs", "comments": "Submitted to IEEE Transactions on Medical Imaging. arXiv admin note:\n  substantial text overlap with arXiv:2006.00033", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to objectively assess new medical imaging technologies via\ncomputer-simulations, it is important to account for all sources of variability\nthat contribute to image data. One important source of variability that can\nsignificantly limit observer performance is associated with the variability in\nthe ensemble of objects to-be-imaged. This source of variability can be\ndescribed by stochastic object models (SOMs), which are generative models that\ncan be employed to sample from a distribution of to-be-virtually-imaged\nobjects. It is generally desirable to establish SOMs from experimental imaging\nmeasurements acquired by use of a well-characterized imaging system, but this\ntask has remained challenging. Deep generative neural networks, such as\ngenerative adversarial networks (GANs) hold potential for such tasks. To\nestablish SOMs from imaging measurements, an AmbientGAN has been proposed that\naugments a GAN with a measurement operator. However, the original AmbientGAN\ncould not immediately benefit from modern training procedures and GAN\narchitectures, which limited its ability to be applied to realistically sized\nmedical image data. To circumvent this, in this work, a modified AmbientGAN\ntraining strategy is proposed that is suitable for modern progressive or\nmulti-resolution training approaches such as employed in the Progressive\nGrowing of GANs and Style-based GANs. AmbientGANs established by use of the\nproposed training procedure are systematically validated in a controlled way by\nuse of computer-simulated measurement data corresponding to a stylized imaging\nsystem. Finally, emulated single-coil experimental magnetic resonance imaging\ndata are employed to demonstrate the methods under less stylized conditions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 21:46:23 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhou", "Weimin", ""], ["Bhadra", "Sayantan", ""], ["Brooks", "Frank J.", ""], ["Li", "Hua", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "2106.14336", "submitter": "Dong Huo", "authors": "Dong Huo, Abbas Masoumzadeh, Yee-Hong Yang", "title": "Blind Non-Uniform Motion Deblurring using Atrous Spatial Pyramid\n  Deformable Convolution and Deblurring-Reblurring Consistency", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many deep learning based methods are designed to remove non-uniform\n(spatially variant) motion blur caused by object motion and camera shake\nwithout knowing the blur kernel. Some methods directly output the latent sharp\nimage in one stage, while others utilize a multi-stage strategy (\\eg\nmulti-scale, multi-patch, or multi-temporal) to gradually restore the sharp\nimage. However, these methods have the following two main issues: 1) The\ncomputational cost of multi-stage is high; 2) The same convolution kernel is\napplied in different regions, which is not an ideal choice for non-uniform\nblur. Hence, non-uniform motion deblurring is still a challenging and open\nproblem. In this paper, we propose a new architecture which consists of\nmultiple Atrous Spatial Pyramid Deformable Convolution (ASPDC) modules to\ndeblur an image end-to-end with more flexibility. Multiple ASPDC modules\nimplicitly learn the pixel-specific motion with different dilation rates in the\nsame layer to handle movements of different magnitude. To improve the training,\nwe also propose a reblurring network to map the deblurred output back to the\nblurred input, which constrains the solution space. Our experimental results\nshow that the proposed method outperforms state-of-the-art methods on the\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 23:14:52 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Huo", "Dong", ""], ["Masoumzadeh", "Abbas", ""], ["Yang", "Yee-Hong", ""]]}, {"id": "2106.14349", "submitter": "Jia Peng", "authors": "Peng Jia, Yongyang Sun, Qiang Liu", "title": "The Deep Neural Network based Photometry Framework for Wide Field Small\n  Aperture Telescopes", "comments": "Submitted to the MNRAS and welcome to any comments. Complete code and\n  data can be downloaded from https://zenodo.org/record/4784689#.YKxbe6gzaUk", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA astro-ph.SR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wide field small aperture telescopes (WFSATs) are mainly used to obtain\nscientific information of point--like and streak--like celestial objects.\nHowever, qualities of images obtained by WFSATs are seriously affected by the\nbackground noise and variable point spread functions. Developing high speed and\nhigh efficiency data processing method is of great importance for further\nscientific research. In recent years, deep neural networks have been proposed\nfor detection and classification of celestial objects and have shown better\nperformance than classical methods. In this paper, we further extend abilities\nof the deep neural network based astronomical target detection framework to\nmake it suitable for photometry and astrometry. We add new branches into the\ndeep neural network to obtain types, magnitudes and positions of different\ncelestial objects at the same time. Tested with simulated data, we find that\nour neural network has better performance in photometry than classical methods.\nBecause photometry and astrometry are regression algorithms, which would obtain\nhigh accuracy measurements instead of rough classification results, the\naccuracy of photometry and astrometry results would be affected by different\nobservation conditions. To solve this problem, we further propose to use\nreference stars to train our deep neural network with transfer learning\nstrategy when observation conditions change. The photometry framework proposed\nin this paper could be used as an end--to--end quick data processing framework\nfor WFSATs, which can further increase response speed and scientific outputs of\nWFSATs.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 00:34:15 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Jia", "Peng", ""], ["Sun", "Yongyang", ""], ["Liu", "Qiang", ""]]}, {"id": "2106.14350", "submitter": "Boris Kovalerchuk", "authors": "Boris Kovalerchuk, Divya Chandrika Kalla, Bedant Agarwal", "title": "Deep Learning Image Recognition for Non-images", "comments": "33 pages, 17 figures, 18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful deep learning algorithms open an opportunity for solving non-image\nMachine Learning (ML) problems by transforming these problems to into the image\nrecognition problems. The CPC-R algorithm presented in this chapter converts\nnon-image data into images by visualizing non-image data. Then deep learning\nCNN algorithms solve the learning problems on these images. The design of the\nCPC-R algorithm allows preserving all high-dimensional information in 2-D\nimages. The use of pair values mapping instead of single value mapping used in\nthe alternative approaches allows encoding each n-D point with 2 times fewer\nvisual elements. The attributes of an n-D point are divided into pairs of its\nvalues and each pair is visualized as 2-D points in the same 2-D Cartesian\ncoordinates. Next, grey scale or color intensity values are assigned to each\npair to encode the order of pairs. This is resulted in the heatmap image. The\ncomputational experiments with CPC-R are conducted for different CNN\narchitectures, and methods to optimize the CPC-R images showing that the\ncombined CPC-R and deep learning CNN algorithms are able to solve non-image ML\nproblems reaching high accuracy on the benchmark datasets. This chapter expands\nour prior work by adding more experiments to test accuracy of classification,\nexploring saliency and informativeness of discovered features to test their\ninterpretability, and generalizing the approach.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 00:36:36 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kovalerchuk", "Boris", ""], ["Kalla", "Divya Chandrika", ""], ["Agarwal", "Bedant", ""]]}, {"id": "2106.14366", "submitter": "Bingchen Zhao", "authors": "Zihao Zhang, Shaozuo Yu, Siwei Yang, Yu Zhou, Bingchen Zhao", "title": "Rail-5k: a Real-World Dataset for Rail Surface Defects Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Rail-5k dataset for benchmarking the performance of\nvisual algorithms in a real-world application scenario, namely the rail surface\ndefects detection task. We collected over 5k high-quality images from railways\nacross China, and annotated 1100 images with the help from railway experts to\nidentify the most common 13 types of rail defects. The dataset can be used for\ntwo settings both with unique challenges, the first is the fully-supervised\nsetting using the 1k+ labeled images for training, fine-grained nature and\nlong-tailed distribution of defect classes makes it hard for visual algorithms\nto tackle. The second is the semi-supervised learning setting facilitated by\nthe 4k unlabeled images, these 4k images are uncurated containing possible\nimage corruptions and domain shift with the labeled images, which can not be\neasily tackle by previous semi-supervised learning methods. We believe our\ndataset could be a valuable benchmark for evaluating robustness and reliability\nof visual algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 01:53:52 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhang", "Zihao", ""], ["Yu", "Shaozuo", ""], ["Yang", "Siwei", ""], ["Zhou", "Yu", ""], ["Zhao", "Bingchen", ""]]}, {"id": "2106.14385", "submitter": "Ji Yuanfeng", "authors": "Yuanfeng Ji, Ruimao Zhang, Huijie Wang, Zhen Li, Lingyun Wu, Shaoting\n  Zhang, and Ping Luo", "title": "Multi-Compound Transformer for Accurate Biomedical Image Segmentation", "comments": "Accepted by MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent vision transformer(i.e.for image classification) learns non-local\nattentive interaction of different patch tokens. However, prior arts miss\nlearning the cross-scale dependencies of different pixels, the semantic\ncorrespondence of different labels, and the consistency of the feature\nrepresentations and semantic embeddings, which are critical for biomedical\nsegmentation. In this paper, we tackle the above issues by proposing a unified\ntransformer network, termed Multi-Compound Transformer (MCTrans), which\nincorporates rich feature learning and semantic structure mining into a unified\nframework. Specifically, MCTrans embeds the multi-scale convolutional features\nas a sequence of tokens and performs intra- and inter-scale self-attention,\nrather than single-scale attention in previous works. In addition, a learnable\nproxy embedding is also introduced to model semantic relationship and feature\nenhancement by using self-attention and cross-attention, respectively. MCTrans\ncan be easily plugged into a UNet-like network and attains a significant\nimprovement over the state-of-the-art methods in biomedical image segmentation\nin six standard benchmarks. For example, MCTrans outperforms UNet by 3.64%,\n3.71%, 4.34%, 2.8%, 1.88%, 1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis,\nKavirs, ISIC2018 dataset, respectively. Code is available at\nhttps://github.com/JiYuanFeng/MCTrans.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 03:45:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ji", "Yuanfeng", ""], ["Zhang", "Ruimao", ""], ["Wang", "Huijie", ""], ["Li", "Zhen", ""], ["Wu", "Lingyun", ""], ["Zhang", "Shaoting", ""], ["Luo", "Ping", ""]]}, {"id": "2106.14386", "submitter": "Yulun Tian", "authors": "Yulun Tian, Yun Chang, Fernando Herrera Arias, Carlos Nieto-Granda,\n  Jonathan P. How, Luca Carlone", "title": "Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for\n  Multi-Robot Systems", "comments": "18 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Kimera-Multi, the first multi-robot system that (i) is\nrobust and capable of identifying and rejecting incorrect inter and intra-robot\nloop closures resulting from perceptual aliasing, (ii) is fully distributed and\nonly relies on local (peer-to-peer) communication to achieve distributed\nlocalization and mapping, and (iii) builds a globally consistent\nmetric-semantic 3D mesh model of the environment in real-time, where faces of\nthe mesh are annotated with semantic labels. Kimera-Multi is implemented by a\nteam of robots equipped with visual-inertial sensors. Each robot builds a local\ntrajectory estimate and a local mesh using Kimera. When communication is\navailable, robots initiate a distributed place recognition and robust pose\ngraph optimization protocol based on a novel distributed graduated\nnon-convexity algorithm. The proposed protocol allows the robots to improve\ntheir local trajectory estimates by leveraging inter-robot loop closures while\nbeing robust to outliers. Finally, each robot uses its improved trajectory\nestimate to correct the local mesh using mesh deformation techniques.\n  We demonstrate Kimera-Multi in photo-realistic simulations, SLAM benchmarking\ndatasets, and challenging outdoor datasets collected using ground robots. Both\nreal and simulated experiments involve long trajectories (e.g., up to 800\nmeters per robot). The experiments show that Kimera-Multi (i) outperforms the\nstate of the art in terms of robustness and accuracy, (ii) achieves estimation\nerrors comparable to a centralized SLAM system while being fully distributed,\n(iii) is parsimonious in terms of communication bandwidth, (iv) produces\naccurate metric-semantic 3D meshes, and (v) is modular and can be also used for\nstandard 3D reconstruction (i.e., without semantic labels) or for trajectory\nestimation (i.e., without reconstructing a 3D mesh).\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 03:56:40 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Tian", "Yulun", ""], ["Chang", "Yun", ""], ["Arias", "Fernando Herrera", ""], ["Nieto-Granda", "Carlos", ""], ["How", "Jonathan P.", ""], ["Carlone", "Luca", ""]]}, {"id": "2106.14403", "submitter": "Weijun Tan", "authors": "Weijun Tan, Jingfeng Liu", "title": "A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice\nimages. In this framework, the slice images of a CT-scan volume are first\nproprocessed using segmentation techniques to filter out images of closed lung,\nand to remove the useless background. Then a resampling method is used to\nselect one or multiple sets of a fixed number of slice images for training and\nvalidation. A 3D CNN network with BERT is used to classify this set of selected\nslice images. In this network, an embedding feature is also extracted. In cases\nwhere there are more than one set of slice images in a volume, the features of\nall sets are extracted and pooled into a global feature vector for the whole\nCT-scan volume. A simple multiple-layer perceptron (MLP) network is used to\nfurther classify the aggregated feature vector. The models are trained and\nevaluated on the provided training and validation datasets. On the validation\ndataset, the accuracy is 0.9278 and the F1 score is 0.9261.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 05:35:26 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Tan", "Weijun", ""], ["Liu", "Jingfeng", ""]]}, {"id": "2106.14412", "submitter": "Xi Li", "authors": "Hui Wang, Hanbin Zhao, and Xi Li", "title": "Progressive Class-based Expansion Learning For Image Classification", "comments": "Accepted to Journal of IEEE Signal Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel image process scheme called class-based\nexpansion learning for image classification, which aims at improving the\nsupervision-stimulation frequency for the samples of the confusing classes.\nClass-based expansion learning takes a bottom-up growing strategy in a\nclass-based expansion optimization fashion, which pays more attention to the\nquality of learning the fine-grained classification boundaries for the\npreferentially selected classes. Besides, we develop a class confusion\ncriterion to select the confusing class preferentially for training. In this\nway, the classification boundaries of the confusing classes are frequently\nstimulated, resulting in a fine-grained form. Experimental results demonstrate\nthe effectiveness of the proposed scheme on several benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 06:11:32 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Hui", ""], ["Zhao", "Hanbin", ""], ["Li", "Xi", ""]]}, {"id": "2106.14413", "submitter": "Hyuntak Cha", "authors": "Hyuntak Cha, Jaeho Lee, Jinwoo Shin", "title": "Co$^2$L: Contrastive Continual Learning", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in self-supervised learning show that such algorithms\nlearn visual representations that can be transferred better to unseen tasks\nthan joint-training methods relying on task-specific supervision. In this\npaper, we found that the similar holds in the continual learning con-text:\ncontrastively learned representations are more robust against the catastrophic\nforgetting than jointly trained representations. Based on this novel\nobservation, we propose a rehearsal-based continual learning algorithm that\nfocuses on continually learning and maintaining transferable representations.\nMore specifically, the proposed scheme (1) learns representations using the\ncontrastive learning objective, and (2) preserves learned representations using\na self-supervised distillation step. We conduct extensive experimental\nvalidations under popular benchmark image classification datasets, where our\nmethod sets the new state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 06:14:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Cha", "Hyuntak", ""], ["Lee", "Jaeho", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2106.14439", "submitter": "Yuhao Liu", "authors": "Yuhao Liu, Jiake Xie, Yu Qiao, Yong Tang and, Xin Yang", "title": "Prior-Induced Information Alignment for Image Matting", "comments": "IEEE TMM", "journal-ref": null, "doi": "10.1109/TMM.2021.3087007.", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Image matting is an ill-posed problem that aims to estimate the opacity of\nforeground pixels in an image. However, most existing deep learning-based\nmethods still suffer from the coarse-grained details. In general, these\nalgorithms are incapable of felicitously distinguishing the degree of\nexploration between deterministic domains (certain FG and BG pixels) and\nundetermined domains (uncertain in-between pixels), or inevitably lose\ninformation in the continuous sampling process, leading to a sub-optimal\nresult. In this paper, we propose a novel network named Prior-Induced\nInformation Alignment Matting Network (PIIAMatting), which can efficiently\nmodel the distinction of pixel-wise response maps and the correlation of\nlayer-wise feature maps. It mainly consists of a Dynamic Gaussian Modulation\nmechanism (DGM) and an Information Alignment strategy (IA). Specifically, the\nDGM can dynamically acquire a pixel-wise domain response map learned from the\nprior distribution. The response map can present the relationship between the\nopacity variation and the convergence process during training. On the other\nhand, the IA comprises an Information Match Module (IMM) and an Information\nAggregation Module (IAM), jointly scheduled to match and aggregate the adjacent\nlayer-wise features adaptively. Besides, we also develop a Multi-Scale\nRefinement (MSR) module to integrate multi-scale receptive field information at\nthe refinement stage to recover the fluctuating appearance details. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed\nPIIAMatting performs favourably against state-of-the-art image matting methods\non the Alphamatting.com, Composition-1K and Distinctions-646 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 07:46:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Yuhao", ""], ["Xie", "Jiake", ""], ["Qiao", "Yu", ""], ["and", "Yong Tang", ""], ["Yang", "Xin", ""]]}, {"id": "2106.14440", "submitter": "Ruihai Wu", "authors": "Ruihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian Wang, Tianhao Wu,\n  Qingnan Fan, Xuelin Chen, Leonidas Guibas, Hao Dong", "title": "VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating\n  3D ARTiculated Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in\nhuman environments is an important yet challenging task for future\nhome-assistant robots. The space of 3D articulated objects is exceptionally\nrich in their myriad semantic categories, diverse shape geometry, and\ncomplicated part functionality. Previous works mostly abstract kinematic\nstructure with estimated joint parameters and part poses as the visual\nrepresentations for manipulating 3D articulated objects. In this paper, we\npropose object-centric actionable visual priors as a novel\nperception-interaction handshaking point that the perception system outputs\nmore actionable guidance than kinematic structure estimation, by predicting\ndense geometry-aware, interaction-aware, and task-aware visual action\naffordance and trajectory proposals. We design an interaction-for-perception\nframework VAT-Mart to learn such actionable visual representations by\nsimultaneously training a curiosity-driven reinforcement learning policy\nexploring diverse interaction trajectories and a perception module summarizing\nand generalizing the explored knowledge for pointwise predictions among diverse\nshapes. Experiments prove the effectiveness of the proposed approach using the\nlarge-scale PartNet-Mobility dataset in SAPIEN environment and show promising\ngeneralization capabilities to novel test shapes, unseen object categories, and\nreal-world data. Project page: https://hyperplane-lab.github.io/vat-mart\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 07:47:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wu", "Ruihai", ""], ["Zhao", "Yan", ""], ["Mo", "Kaichun", ""], ["Guo", "Zizheng", ""], ["Wang", "Yian", ""], ["Wu", "Tianhao", ""], ["Fan", "Qingnan", ""], ["Chen", "Xuelin", ""], ["Guibas", "Leonidas", ""], ["Dong", "Hao", ""]]}, {"id": "2106.14447", "submitter": "Zhiyu Cheng", "authors": "Xin Zhou, Le Kang, Zhiyu Cheng, Bo He, Jingyu Xin", "title": "Feature Combination Meets Attention: Baidu Soccer Embeddings and\n  Transformer based Temporal Detection", "comments": "Tech Report. Authors Xin Zhou, Le Kang, and Zhiyu Cheng made equal\n  contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With rapidly evolving internet technologies and emerging tools, sports\nrelated videos generated online are increasing at an unprecedentedly fast pace.\nTo automate sports video editing/highlight generation process, a key task is to\nprecisely recognize and locate the events in the long untrimmed videos. In this\ntech report, we present a two-stage paradigm to detect what and when events\nhappen in soccer broadcast videos. Specifically, we fine-tune multiple action\nrecognition models on soccer data to extract high-level semantic features, and\ndesign a transformer based temporal detection module to locate the target\nevents. This approach achieved the state-of-the-art performance in both two\ntasks, i.e., action spotting and replay grounding, in the SoccerNet-v2\nChallenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features\nare released at https://github.com/baidu-research/vidpress-sports. By sharing\nthese features with the broader community, we hope to accelerate the research\ninto soccer video understanding.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:00:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhou", "Xin", ""], ["Kang", "Le", ""], ["Cheng", "Zhiyu", ""], ["He", "Bo", ""], ["Xin", "Jingyu", ""]]}, {"id": "2106.14459", "submitter": "Trung Ngo Tan", "authors": "Trung Tan Ngo, Hung Tuan Nguyen, Nam Tuan Ly, Masaki Nakagawa", "title": "Recurrent neural network transducer for Japanese and Chinese offline\n  handwritten text recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an RNN-Transducer model for recognizing Japanese\nand Chinese offline handwritten text line images. As far as we know, it is the\nfirst approach that adopts the RNN-Transducer model for offline handwritten\ntext recognition. The proposed model consists of three main components: a\nvisual feature encoder that extracts visual features from an input image by CNN\nand then encodes the visual features by BLSTM; a linguistic context encoder\nthat extracts and encodes linguistic features from the input image by embedded\nlayers and LSTM; and a joint decoder that combines and then decodes the visual\nfeatures and the linguistic features into the final label sequence by fully\nconnected and softmax layers. The proposed model takes advantage of both visual\nand linguistic information from the input image. In the experiments, we\nevaluated the performance of the proposed model on the two datasets: Kuzushiji\nand SCUT-EPT. Experimental results show that the proposed model achieves\nstate-of-the-art performance on all datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:16:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ngo", "Trung Tan", ""], ["Nguyen", "Hung Tuan", ""], ["Ly", "Nam Tuan", ""], ["Nakagawa", "Masaki", ""]]}, {"id": "2106.14465", "submitter": "Sk Imran Hossain", "authors": "Sk Imran Hossain (LIMOS), Jocelyn de Go\\\"er de Herve (INRAE), Md\n  Shahriar Hassan (LIMOS), Delphine Martineau, Evelina Petrosyan, Violaine\n  Corbain, Jean Beytout, Isabelle Lebert (INRAE), Elisabeth Baux (CHRU Nancy),\n  C\\'eline Cazorla (CHU de Saint-Etienne), Carole Eldin (IHU M\\'editerran\\'ee\n  Infection), Yves Hansmann, Solene Patrat-Delon, Thierry Prazuck (CHR), Alice\n  Raffetin (CHIV), Pierre Tattevin (CHU Rennes), Gwena\\\"el Vourc'H (INRAE),\n  Olivier Lesens, Engelbert Nguifo (LIMOS)", "title": "Benchmarking convolutional neural networks for diagnosing Lyme disease\n  from images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lyme disease is one of the most common infectious vector-borne diseases in\nthe world. In the early stage, the disease manifests itself in most cases with\nerythema migrans (EM) skin lesions. Better diagnosis of these early forms would\nallow improving the prognosis by preventing the transition to a severe late\nform thanks to appropriate antibiotic therapy. Recent studies show that\nconvolutional neural networks (CNNs) perform very well to identify skin lesions\nfrom the image but, there is not much work for Lyme disease prediction from EM\nlesion images. The main objective of this study is to extensively analyze the\neffectiveness of CNNs for diagnosing Lyme disease from images and to find out\nthe best CNN architecture for the purpose. There is no publicly available EM\nimage dataset for Lyme disease prediction mainly because of privacy concerns.\nIn this study, we utilized an EM dataset consisting of images collected from\nClermont-Ferrand University Hospital Center (CF-CHU) of France and the\ninternet. CF-CHU collected the images from several hospitals in France. This\ndataset was labeled by expert dermatologists and infectiologists from CF-CHU.\nFirst, we benchmarked this dataset for twenty-three well-known CNN\narchitectures in terms of predictive performance metrics, computational\ncomplexity metrics, and statistical significance tests. Second, to improve the\nperformance of the CNNs, we used transfer learning from ImageNet pre-trained\nmodels as well as pre-trained the CNNs with the skin lesion dataset \"Human\nAgainst Machine with 10000 training images (HAM1000)\". In that process, we\nsearched for the best performing number of layers to unfreeze during transfer\nlearning fine-tuning for each of the CNNs. Third, for model explainability, we\nutilized Gradient-weighted Class Activation Mapping to visualize the regions of\ninput that are significant to the CNNs for making predictions. Fourth, we\nprovided guidelines for model selection based on predictive performance and\ncomputational complexity. Our study confirmed the effectiveness and potential\nof even some lightweight CNNs to be used for Lyme disease pre-scanner mobile\napplications. We also made all the trained models publicly available at\nhttps://dappem.limos.fr/download.html, which can be used by others for transfer\nlearning and building pre-scanners for Lyme disease.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:28:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hossain", "Sk Imran", "", "LIMOS"], ["de Herve", "Jocelyn de Go\u00ebr", "", "INRAE"], ["Hassan", "Md Shahriar", "", "LIMOS"], ["Martineau", "Delphine", "", "INRAE"], ["Petrosyan", "Evelina", "", "INRAE"], ["Corbain", "Violaine", "", "INRAE"], ["Beytout", "Jean", "", "INRAE"], ["Lebert", "Isabelle", "", "INRAE"], ["Baux", "Elisabeth", "", "CHRU Nancy"], ["Cazorla", "C\u00e9line", "", "CHU de Saint-Etienne"], ["Eldin", "Carole", "", "IHU M\u00e9diterran\u00e9e\n  Infection"], ["Hansmann", "Yves", "", "CHR"], ["Patrat-Delon", "Solene", "", "CHR"], ["Prazuck", "Thierry", "", "CHR"], ["Raffetin", "Alice", "", "CHIV"], ["Tattevin", "Pierre", "", "CHU Rennes"], ["Vourc'H", "Gwena\u00ebl", "", "INRAE"], ["Lesens", "Olivier", "", "LIMOS"], ["Nguifo", "Engelbert", "", "LIMOS"]]}, {"id": "2106.14467", "submitter": "Sheng Huang", "authors": "Yi Zhang and Sheng Huang and Xi Peng and Dan Yang", "title": "Dizygotic Conditional Variational AutoEncoder for Multi-Modal and\n  Partial Modality Absent Few-Shot Learning", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data augmentation is a powerful technique for improving the performance of\nthe few-shot classification task. It generates more samples as supplements, and\nthen this task can be transformed into a common supervised learning issue for\nsolution. However, most mainstream data augmentation based approaches only\nconsider the single modality information, which leads to the low diversity and\nquality of generated features. In this paper, we present a novel multi-modal\ndata augmentation approach named Dizygotic Conditional Variational AutoEncoder\n(DCVAE) for addressing the aforementioned issue. DCVAE conducts feature\nsynthesis via pairing two Conditional Variational AutoEncoders (CVAEs) with the\nsame seed but different modality conditions in a dizygotic symbiosis manner.\nSubsequently, the generated features of two CVAEs are adaptively combined to\nyield the final feature, which can be converted back into its paired conditions\nwhile ensuring these conditions are consistent with the original conditions not\nonly in representation but also in function. DCVAE essentially provides a new\nidea of data augmentation in various multi-modal scenarios by exploiting the\ncomplement of different modality prior information. Extensive experimental\nresults demonstrate our work achieves state-of-the-art performances on\nminiImageNet, CIFAR-FS and CUB datasets, and is able to work well in the\npartial modality absence case.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:29:55 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhang", "Yi", ""], ["Huang", "Sheng", ""], ["Peng", "Xi", ""], ["Yang", "Dan", ""]]}, {"id": "2106.14474", "submitter": "Kira Maag", "authors": "Kira Maag", "title": "False Negative Reduction in Video Instance Segmentation using\n  Uncertainty Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation of images is an important tool for automated scene\nunderstanding. Neural networks are usually trained to optimize their overall\nperformance in terms of accuracy. Meanwhile, in applications such as automated\ndriving, an overlooked pedestrian seems more harmful than a falsely detected\none. In this work, we present a false negative detection method for image\nsequences based on inconsistencies in time series of tracked instances given\nthe availability of image sequences in online applications. As the number of\ninstances can be greatly increased by this algorithm, we apply a false positive\npruning using uncertainty estimates aggregated over instances. To this end,\ninstance-wise metrics are constructed which characterize uncertainty and\ngeometry of a given instance or are predicated on depth estimation. The\nproposed method serves as a post-processing step applicable to any neural\nnetwork that can also be trained on single frames only. In our tests, we obtain\nan improved trade-off between false negative and false positive instances by\nour fused detection approach in comparison to the use of an ordinary score\nvalue provided by the instance segmentation network during inference.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:38:55 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Maag", "Kira", ""]]}, {"id": "2106.14475", "submitter": "Wenchao Zhang", "authors": "Wenchao Zhang, Chong Fu, Xiangshi Chang, Tengfei Zhao, Xiang Li,\n  Chiu-Wing Sham", "title": "A More Compact Object Detector Head Network with Feature Enhancement and\n  Relational Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling implicit feature interaction patterns is of significant importance\nto object detection tasks. However, in the two-stage detectors, due to the\nexcessive use of hand-crafted components, it is very difficult to reason about\nthe implicit relationship of the instance features. To tackle this problem, we\nanalyze three different levels of feature interaction relationships, namely,\nthe dependency relationship between the cropped local features and global\nfeatures, the feature autocorrelation within the instance, and the\ncross-correlation relationship between the instances. To this end, we propose a\nmore compact object detector head network (CODH), which can not only preserve\nglobal context information and condense the information density, but also\nallows instance-wise feature enhancement and relational reasoning in a larger\nmatrix space. Without bells and whistles, our method can effectively improve\nthe detection performance while significantly reducing the parameters of the\nmodel, e.g., with our method, the parameters of the head network is 0.6 times\nsmaller than the state-of-the-art Cascade R-CNN, yet the performance boost is\n1.3% on COCO test-dev. Without losing generality, we can also build a more\nlighter head network for other multi-stage detectors by assembling our method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:38:57 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 16:08:12 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Wenchao", ""], ["Fu", "Chong", ""], ["Chang", "Xiangshi", ""], ["Zhao", "Tengfei", ""], ["Li", "Xiang", ""], ["Sham", "Chiu-Wing", ""]]}, {"id": "2106.14476", "submitter": "Daniel Reich", "authors": "Daniel Reich, Felix Putze, Tanja Schultz", "title": "Adventurer's Treasure Hunt: A Transparent System for Visually Grounded\n  Compositional Visual Question Answering based on Scene Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the expressed goal of improving system transparency and visual grounding\nin the reasoning process in VQA, we present a modular system for the task of\ncompositional VQA based on scene graphs. Our system is called \"Adventurer's\nTreasure Hunt\" (or ATH), named after an analogy we draw between our model's\nsearch procedure for an answer and an adventurer's search for treasure. We\ndeveloped ATH with three characteristic features in mind: 1. By design, ATH\nallows us to explicitly quantify the impact of each of the sub-components on\noverall VQA performance, as well as their performance on their individual\nsub-task. 2. By modeling the search task after a treasure hunt, ATH inherently\nproduces an explicit, visually grounded inference path for the processed\nquestion. 3. ATH is the first GQA-trained VQA system that dynamically extracts\nanswers by querying the visual knowledge base directly, instead of selecting\none from a specially learned classifier's output distribution over a pre-fixed\nanswer vocabulary. We report detailed results on all components and their\ncontributions to overall VQA performance on the GQA dataset and show that ATH\nachieves the highest visual grounding score among all examined systems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:39:34 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Reich", "Daniel", ""], ["Putze", "Felix", ""], ["Schultz", "Tanja", ""]]}, {"id": "2106.14483", "submitter": "Azmi Can \\\"Ozgen", "authors": "Azmi Can \\\"Ozgen, Mahiye Uluya\\u{g}mur \\\"Ozt\\\"urk, Umut Bayraktar", "title": "Cheating Detection Pipeline for Online Interviews and Exams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote examination and job interviews have gained popularity and become\nindispensable because of both pandemics and the advantage of remote working\ncircumstances. Most companies and academic institutions utilize these systems\nfor their recruitment processes and also for online exams. However, one of the\ncritical problems of the remote examination systems is conducting the exams in\na reliable environment. In this work, we present a cheating analysis pipeline\nfor online interviews and exams. The system only requires a video of the\ncandidate, which is recorded during the exam. Then cheating detection pipeline\nis employed to detect another person, electronic device usage, and candidate\nabsence status. The pipeline consists of face detection, face recognition,\nobject detection, and face tracking algorithms. To evaluate the performance of\nthe pipeline we collected a private video dataset. The video dataset includes\nboth cheating activities and clean videos. Ultimately, our pipeline presents an\nefficient and fast guideline to detect and analyze cheating activities in an\nonline interview and exam video.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:52:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["\u00d6zgen", "Azmi Can", ""], ["\u00d6zt\u00fcrk", "Mahiye Uluya\u011fmur", ""], ["Bayraktar", "Umut", ""]]}, {"id": "2106.14490", "submitter": "Li Niu", "authors": "Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, Liqing\n  Zhang", "title": "Making Images Real Again: A Comprehensive Survey on Deep Image\n  Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a common image editing operation, image composition aims to cut the\nforeground from one image and paste it on another image, resulting in a\ncomposite image. However, there are many issues that could make the composite\nimages unrealistic. These issues can be summarized as the inconsistency between\nforeground and background, which include appearance inconsistency (e.g.,\nincompatible color and illumination) and geometry inconsistency (e.g.,\nunreasonable size and location). Previous works on image composition target at\none or more issues. Since each individual issue is a complicated problem, there\nare some research directions (e.g., image harmonization, object placement)\nwhich focus on only one issue. By putting all the efforts together, we can\nacquire realistic composite images. Sometimes, we expect the composite images\nto be not only realistic but also aesthetic, in which case aesthetic evaluation\nneeds to be considered. In this survey, we summarize the datasets and methods\nfor the above research directions. We also discuss the limitations and\npotential directions to facilitate the future research for image composition.\nFinally, as a double-edged sword, image composition may also have negative\neffect on our lives (e.g., fake news) and thus it is imperative to develop\nalgorithms to fight against composite images. Datasets and codes for image\ncomposition are summarized at\nhttps://github.com/bcmi/Awesome-Image-Composition.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 09:09:14 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Niu", "Li", ""], ["Cong", "Wenyan", ""], ["Liu", "Liu", ""], ["Hong", "Yan", ""], ["Zhang", "Bo", ""], ["Liang", "Jing", ""], ["Zhang", "Liqing", ""]]}, {"id": "2106.14501", "submitter": "Hai Jiang", "authors": "Jiang Hai, Zhu Xuan, Ren Yang, Yutong Hao, Fengzhu Zou, Fang Lin and\n  Songchen Han", "title": "R2RNet: Low-light Image Enhancement via Real-low to Real-normal Network", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images captured in weak illumination conditions will seriously degrade the\nimage quality. Solving a series of degradation of low-light images can\neffectively improve the visual quality of the image and the performance of\nhigh-level visual tasks. In this paper, we propose a novel Real-low to\nReal-normal Network for low-light image enhancement, dubbed R2RNet, based on\nthe Retinex theory, which includes three subnets: a Decom-Net, a Denoise-Net,\nand a Relight-Net. These three subnets are used for decomposing, denoising, and\ncontrast enhancement, respectively. Unlike most previous methods trained on\nsynthetic images, we collect the first Large-Scale Real-World paired\nlow/normal-light images dataset (LSRW dataset) for training. Our method can\nproperly improve the contrast and suppress noise simultaneously. Extensive\nexperiments on publicly available datasets demonstrate that our method\noutperforms the existing state-of-the-art methods by a large margin both\nquantitatively and visually. And we also show that the performance of the\nhigh-level visual task (\\emph{i.e.} face detection) can be effectively improved\nby using the enhanced results obtained by our method in low-light conditions.\nOur codes and the LSRW dataset are available at:\nhttps://github.com/abcdef2000/R2RNet.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 09:33:13 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hai", "Jiang", ""], ["Xuan", "Zhu", ""], ["Yang", "Ren", ""], ["Hao", "Yutong", ""], ["Zou", "Fengzhu", ""], ["Lin", "Fang", ""], ["Han", "Songchen", ""]]}, {"id": "2106.14516", "submitter": "Alphin J Thottupattu", "authors": "Alphin J Thottupattu and Jayanthi Sivaswamy and Venkateswaran\n  P.Krishnan", "title": "A Diffeomorphic Aging Model for Adult Human Brain from Cross-Sectional\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Normative aging trends of the brain can serve as an important reference in\nthe assessment of neurological structural disorders. Such models are typically\ndeveloped from longitudinal brain image data -- follow-up data of the same\nsubject over different time points. In practice, obtaining such longitudinal\ndata is difficult. We propose a method to develop an aging model for a given\npopulation, in the absence of longitudinal data, by using images from different\nsubjects at different time points, the so-called cross-sectional data. We\ndefine an aging model as a diffeomorphic deformation on a structural template\nderived from the data and propose a method that develops topology preserving\naging model close to natural aging. The proposed model is successfully\nvalidated on two public cross-sectional datasets which provide templates\nconstructed from different sets of subjects at different age points.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:04:05 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Thottupattu", "Alphin J", ""], ["Sivaswamy", "Jayanthi", ""], ["Krishnan", "Venkateswaran P.", ""]]}, {"id": "2106.14556", "submitter": "Adam White Dr", "authors": "Adam White, Kwun Ho Ngan, James Phelan, Saman Sadeghi Afgeh, Kevin\n  Ryan, Constantino Carlos Reyes-Aldasoro, Artur d'Avila Garcez", "title": "Contrastive Counterfactual Visual Explanations With Overdetermination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A novel explainable AI method called CLEAR Image is introduced in this paper.\nCLEAR Image is based on the view that a satisfactory explanation should be\ncontrastive, counterfactual and measurable. CLEAR Image explains an image's\nclassification probability by contrasting the image with a corresponding image\ngenerated automatically via adversarial learning. This enables both salient\nsegmentation and perturbations that faithfully determine each segment's\nimportance. CLEAR Image was successfully applied to a medical imaging case\nstudy where it outperformed methods such as Grad-CAM and LIME by an average of\n27% using a novel pointing game metric. CLEAR Image excels in identifying cases\nof \"causal overdetermination\" where there are multiple patches in an image, any\none of which is sufficient by itself to cause the classification probability to\nbe close to one.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:24:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["White", "Adam", ""], ["Ngan", "Kwun Ho", ""], ["Phelan", "James", ""], ["Afgeh", "Saman Sadeghi", ""], ["Ryan", "Kevin", ""], ["Reyes-Aldasoro", "Constantino Carlos", ""], ["Garcez", "Artur d'Avila", ""]]}, {"id": "2106.14568", "submitter": "Shiwei Liu", "authors": "Shiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen, Ghada Sokar,\n  Elena Mocanu, Mykola Pechenizkiy, Zhangyang Wang, Decebal Constantin Mocanu", "title": "FreeTickets: Accurate, Robust and Efficient Deep Ensemble by Training\n  with Dynamic Sparsity", "comments": "preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on sparse neural networks have demonstrated that it is possible\nto train a sparse network in isolation to match the performance of the\ncorresponding dense networks with a fraction of parameters. However, the\nidentification of these performant sparse neural networks (winning tickets)\neither involves a costly iterative train-prune-retrain process (e.g., Lottery\nTicket Hypothesis) or an over-extended sparse training time (e.g., Training\nwith Dynamic Sparsity), both of which would raise financial and environmental\nconcerns. In this work, we attempt to address this cost-reducing problem by\nintroducing the FreeTickets concept, as the first solution which can boost the\nperformance of sparse convolutional neural networks over their dense network\nequivalents by a large margin, while using for complete training only a\nfraction of the computational resources required by the latter. Concretely, we\ninstantiate the FreeTickets concept, by proposing two novel efficient ensemble\nmethods with dynamic sparsity, which yield in one shot many diverse and\naccurate tickets \"for free\" during the sparse training process. The combination\nof these free tickets into an ensemble demonstrates a significant improvement\nin accuracy, uncertainty estimation, robustness, and efficiency over the\ncorresponding dense (ensemble) networks. Our results provide new insights into\nthe strength of sparse neural networks and suggest that the benefits of\nsparsity go way beyond the usual training/inference expected efficiency. We\nwill release all codes in https://github.com/Shiweiliuiiiiiii/FreeTickets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:48:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Shiwei", ""], ["Chen", "Tianlong", ""], ["Atashgahi", "Zahra", ""], ["Chen", "Xiaohan", ""], ["Sokar", "Ghada", ""], ["Mocanu", "Elena", ""], ["Pechenizkiy", "Mykola", ""], ["Wang", "Zhangyang", ""], ["Mocanu", "Decebal Constantin", ""]]}, {"id": "2106.14577", "submitter": "Yamin Sepehri", "authors": "Yamin Sepehri, Pedram Pad, Pascal Frossard, L. Andrea Dunbar", "title": "Privacy-Preserving Image Acquisition Using Trainable Optical Kernel", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving privacy is a growing concern in our society where sensors and\ncameras are ubiquitous. In this work, for the first time, we propose a\ntrainable image acquisition method that removes the sensitive identity\nrevealing information in the optical domain before it reaches the image sensor.\nThe method benefits from a trainable optical convolution kernel which transmits\nthe desired information while filters out the sensitive content. As the\nsensitive content is suppressed before it reaches the image sensor, it does not\nenter the digital domain therefore is unretrievable by any sort of privacy\nattack. This is in contrast with the current digital privacy-preserving methods\nthat are all vulnerable to direct access attack. Also, in contrast with the\nprevious optical privacy-preserving methods that cannot be trained, our method\nis data-driven and optimized for the specific application at hand. Moreover,\nthere is no additional computation, memory, or power burden on the acquisition\nsystem since this processing happens passively in the optical domain and can\neven be used together and on top of the fully digital privacy-preserving\nsystems. The proposed approach is adaptable to different digital neural\nnetworks and content. We demonstrate it for several scenarios such as smile\ndetection as the desired attribute while the gender is filtered out as the\nsensitive content. We trained the optical kernel in conjunction with two\nadversarial neural networks where the analysis network tries to detect the\ndesired attribute and the adversarial network tries to detect the sensitive\ncontent. We show that this method can reduce 65.1% of sensitive content when it\nis selected to be the gender and it only loses 7.3% of the desired content.\nMoreover, we reconstruct the original faces using the deep reconstruction\nmethod that confirms the ineffectiveness of reconstruction attacks to obtain\nthe sensitive content.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 11:08:14 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sepehri", "Yamin", ""], ["Pad", "Pedram", ""], ["Frossard", "Pascal", ""], ["Dunbar", "L. Andrea", ""]]}, {"id": "2106.14591", "submitter": "Yixin Wang", "authors": "Yixin Wang, Yang Zhang, Yang Liu, Zihao Lin, Jiang Tian, Cheng Zhong,\n  Zhongchao Shi, Jianping Fan, Zhiqiang He", "title": "ACN: Adversarial Co-training Network for Brain Tumor Segmentation with\n  Missing Modalities", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of brain tumors from magnetic resonance imaging (MRI)\nis clinically relevant in diagnoses, prognoses and surgery treatment, which\nrequires multiple modalities to provide complementary morphological and\nphysiopathologic information. However, missing modality commonly occurs due to\nimage corruption, artifacts, different acquisition protocols or allergies to\ncertain contrast agents in clinical practice. Though existing efforts\ndemonstrate the possibility of a unified model for all missing situations, most\nof them perform poorly when more than one modality is missing. In this paper,\nwe propose a novel Adversarial Co-training Network (ACN) to solve this issue,\nin which a series of independent yet related models are trained dedicated to\neach missing situation with significantly better results. Specifically, ACN\nadopts a novel co-training network, which enables a coupled learning process\nfor both full modality and missing modality to supplement each other's domain\nand feature representations, and more importantly, to recover the `missing'\ninformation of absent modalities. Then, two unsupervised modules, i.e., entropy\nand knowledge adversarial learning modules are proposed to minimize the domain\ngap while enhancing prediction reliability and encouraging the alignment of\nlatent representations, respectively. We also adapt modality-mutual information\nknowledge transfer learning to ACN to retain the rich mutual information among\nmodalities. Extensive experiments on BraTS2018 dataset show that our proposed\nmethod significantly outperforms all state-of-the-art methods under any missing\nsituation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 11:53:11 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 08:08:11 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Yixin", ""], ["Zhang", "Yang", ""], ["Liu", "Yang", ""], ["Lin", "Zihao", ""], ["Tian", "Jiang", ""], ["Zhong", "Cheng", ""], ["Shi", "Zhongchao", ""], ["Fan", "Jianping", ""], ["He", "Zhiqiang", ""]]}, {"id": "2106.14597", "submitter": "Roberto Fernandes", "authors": "Roberto Fernandes, Walber M. Rodrigues, Edna Barros", "title": "Dataset and Benchmarking of Real-Time Embedded Object Detection for\n  RoboCup SSL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When producing a model to object detection in a specific context, the first\nobstacle is to have a dataset labeling the desired classes. In RoboCup, some\nleagues already have more than one dataset to train and evaluate a model.\nHowever, in the Small Size League (SSL), there is not such dataset available\nyet. This paper presents an open-source dataset to be used as a benchmark for\nreal-time object detection in SSL. This work also presented a pipeline to\ntrain, deploy, and evaluate Convolutional Neural Networks (CNNs) models in a\nlow-power embedded system. This pipeline was used to evaluate the proposed\ndataset with state-of-art optimized models. In this dataset, the MobileNet SSD\nv1 achieves 44.88% AP (68.81% AP50) at 94 Frames Per Second (FPS) while running\non an SSL robot.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 12:09:36 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fernandes", "Roberto", ""], ["Rodrigues", "Walber M.", ""], ["Barros", "Edna", ""]]}, {"id": "2106.14694", "submitter": "Zhiqiang Deng", "authors": "Zhiqiang Deng, Huimin Yu and Yangqi Long", "title": "Fractal Pyramid Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new network architecture, the Fractal Pyramid Networks (PFNs)\nfor pixel-wise prediction tasks as an alternative to the widely used\nencoder-decoder structure. In the encoder-decoder structure, the input is\nprocessed by an encoding-decoding pipeline that tries to get a semantic\nlarge-channel feature. Different from that, our proposed PFNs hold multiple\ninformation processing pathways and encode the information to multiple separate\nsmall-channel features. On the task of self-supervised monocular depth\nestimation, even without ImageNet pretrained, our models can compete or\noutperform the state-of-the-art methods on the KITTI dataset with much fewer\nparameters. Moreover, the visual quality of the prediction is significantly\nimproved. The experiment of semantic segmentation provides evidence that the\nPFNs can be applied to other pixel-wise prediction tasks, and demonstrates that\nour models can catch more global structure information.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:15:30 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Deng", "Zhiqiang", ""], ["Yu", "Huimin", ""], ["Long", "Yangqi", ""]]}, {"id": "2106.14699", "submitter": "Johan \\\"Ofverstedt", "authors": "Johan \\\"Ofverstedt, Joakim Lindblad, Nata\\v{s}a Sladoje", "title": "Fast computation of mutual information in the frequency domain with\n  applications to global multimodal image alignment", "comments": "7 pages, 4 figures, 2 tables. The article is under consideration at\n  Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal image alignment is the process of finding spatial correspondences\nbetween images formed by different imaging techniques or under different\nconditions, to facilitate heterogeneous data fusion and correlative analysis.\nThe information-theoretic concept of mutual information (MI) is widely used as\na similarity measure to guide multimodal alignment processes, where most works\nhave focused on local maximization of MI that typically works well only for\nsmall displacements; this points to a need for global maximization of MI, which\nhas previously been computationally infeasible due to the high run-time\ncomplexity of existing algorithms. We propose an efficient algorithm for\ncomputing MI for all discrete displacements (formalized as the cross-mutual\ninformation function (CMIF)), which is based on cross-correlation computed in\nthe frequency domain. We show that the algorithm is equivalent to a direct\nmethod while asymptotically superior in terms of run-time. Furthermore, we\npropose a method for multimodal image alignment for transformation models with\nfew degrees of freedom (e.g. rigid) based on the proposed CMIF-algorithm. We\nevaluate the efficacy of the proposed method on three distinct benchmark\ndatasets, of aerial images, cytological images, and histological images, and we\nobserve excellent success-rates (in recovering known rigid transformations),\noverall outperforming alternative methods, including local optimization of MI\nas well as several recent deep learning-based approaches. We also evaluate the\nrun-times of a GPU implementation of the proposed algorithm and observe\nspeed-ups from 100 to more than 10,000 times for realistic image sizes compared\nto a GPU implementation of a direct method. Code is shared as open-source at\n\\url{github.com/MIDA-group/globalign}.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:27:05 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["\u00d6fverstedt", "Johan", ""], ["Lindblad", "Joakim", ""], ["Sladoje", "Nata\u0161a", ""]]}, {"id": "2106.14706", "submitter": "Guangming Wang", "authors": "Guangming Wang, Honghao Zeng, Ziliang Wang, Zhe Liu, Hesheng Wang", "title": "Motion Projection Consistency Based 3D Human Pose Estimation with\n  Virtual Bones from Monocular Videos", "comments": "10 pages, 7 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time 3D human pose estimation is crucial for human-computer interaction.\nIt is cheap and practical to estimate 3D human pose only from monocular video.\nHowever, recent bone splicing based 3D human pose estimation method brings\nabout the problem of cumulative error. In this paper, the concept of virtual\nbones is proposed to solve such a challenge. The virtual bones are imaginary\nbones between non-adjacent joints. They do not exist in reality, but they bring\nnew loop constraints for the estimation of 3D human joints. The proposed\nnetwork in this paper predicts real bones and virtual bones, simultaneously.\nThe final length of real bones is constrained and learned by the loop\nconstructed by the predicted real bones and virtual bones. Besides, the motion\nconstraints of joints in consecutive frames are considered. The consistency\nbetween the 2D projected position displacement predicted by the network and the\ncaptured real 2D displacement by the camera is proposed as a new projection\nconsistency loss for the learning of 3D human pose. The experiments on the\nHuman3.6M dataset demonstrate the good performance of the proposed method.\nAblation studies demonstrate the effectiveness of the proposed inter-frame\nprojection consistency constraints and intra-frame loop constraints.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:37:57 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Guangming", ""], ["Zeng", "Honghao", ""], ["Wang", "Ziliang", ""], ["Liu", "Zhe", ""], ["Wang", "Hesheng", ""]]}, {"id": "2106.14708", "submitter": "Lukas Hudec", "authors": "Peter Bokor, Lukas Hudec, Ondrej Fabian, Wanda Benesova", "title": "Weighted multi-level deep learning analysis and framework for processing\n  breast cancer WSIs", "comments": "9 pages, 12 images, 3 tables with results, We have an intention to\n  submit this paper to the current journal focused on computer methods/deep\n  learning in biomedicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prevention and early diagnosis of breast cancer (BC) is an essential\nprerequisite for the selection of proper treatment. The substantial pressure\ndue to the increase of demand for faster and more precise diagnostic results\ndrives for automatic solutions. In the past decade, deep learning techniques\nhave demonstrated their power over several domains, and Computer-Aided (CAD)\ndiagnostic became one of them. However, when it comes to the analysis of Whole\nSlide Images (WSI), most of the existing works compute predictions from levels\nindependently. This is, however, in contrast to the histopathologist expert\napproach who requires to see a global architecture of tissue structures\nimportant in BC classification.\n  We present a deep learning-based solution and framework for processing WSI\nbased on a novel approach utilizing the advantages of image levels. We apply\nthe weighing of information extracted from several levels into the final\nclassification of the malignancy. Our results demonstrate the profitability of\nglobal information with an increase of accuracy from 72.2% to 84.8%.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:38:11 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bokor", "Peter", ""], ["Hudec", "Lukas", ""], ["Fabian", "Ondrej", ""], ["Benesova", "Wanda", ""]]}, {"id": "2106.14724", "submitter": "Juan E Arco", "authors": "Juan E. Arco and Andr\\'es Ortiz and Javier Ram\\'irez and Juan M Gorriz", "title": "Tiled sparse coding in eigenspaces for the COVID-19 diagnosis in chest\n  X-ray images", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The ongoing crisis of the COVID-19 (Coronavirus disease 2019) pandemic has\nchanged the world. According to the World Health Organization (WHO), 4 million\npeople have died due to this disease, whereas there have been more than 180\nmillion confirmed cases of COVID-19. The collapse of the health system in many\ncountries has demonstrated the need of developing tools to automatize the\ndiagnosis of the disease from medical imaging. Previous studies have used deep\nlearning for this purpose. However, the performance of this alternative highly\ndepends on the size of the dataset employed for training the algorithm. In this\nwork, we propose a classification framework based on sparse coding in order to\nidentify the pneumonia patterns associated with different pathologies.\nSpecifically, each chest X-ray (CXR) image is partitioned into different tiles.\nThe most relevant features extracted from PCA are then used to build the\ndictionary within the sparse coding procedure. Once images are transformed and\nreconstructed from the elements of the dictionary, classification is performed\nfrom the reconstruction errors of individual patches associated with each\nimage. Performance is evaluated in a real scenario where simultaneously\ndifferentiation between four different pathologies: control vs bacterial\npneumonia vs viral pneumonia vs COVID-19. The accuracy when identifying the\npresence of pneumonia is 93.85%, whereas 88.11% is obtained in the 4-class\nclassification context. The excellent results and the pioneering use of sparse\ncoding in this scenario evidence the applicability of this approach as an aid\nfor clinicians in a real-world environment.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:50:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Arco", "Juan E.", ""], ["Ortiz", "Andr\u00e9s", ""], ["Ram\u00edrez", "Javier", ""], ["Gorriz", "Juan M", ""]]}, {"id": "2106.14729", "submitter": "Simon Bultmann", "authors": "Simon Bultmann and Sven Behnke", "title": "Real-Time Multi-View 3D Human Pose Estimation using Semantic Feedback to\n  Smart Edge Sensors", "comments": "Accepted for Robotics: Science and Systems (RSS), July 2021, 10\n  pages, 6 figures", "journal-ref": "Proceedings of Robotics: Science and Systems, July 2021", "doi": "10.15607/RSS.2021.XVII.040", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for estimation of 3D human poses from a\nmulti-camera setup, employing distributed smart edge sensors coupled with a\nbackend through a semantic feedback loop. 2D joint detection for each camera\nview is performed locally on a dedicated embedded inference processor. Only the\nsemantic skeleton representation is transmitted over the network and raw images\nremain on the sensor board. 3D poses are recovered from 2D joints on a central\nbackend, based on triangulation and a body model which incorporates prior\nknowledge of the human skeleton. A feedback channel from backend to individual\nsensors is implemented on a semantic level. The allocentric 3D pose is\nbackprojected into the sensor views where it is fused with 2D joint detections.\nThe local semantic model on each sensor can thus be improved by incorporating\nglobal context information. The whole pipeline is capable of real-time\noperation. We evaluate our method on three public datasets, where we achieve\nstate-of-the-art results and show the benefits of our feedback architecture, as\nwell as in our own setup for multi-person experiments. Using the feedback\nsignal improves the 2D joint detections and in turn the estimated 3D poses.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:00:00 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bultmann", "Simon", ""], ["Behnke", "Sven", ""]]}, {"id": "2106.14733", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni and Anelia Angelova and Michael S. Ryoo and Irfan Essa", "title": "Unsupervised Discovery of Actions in Instructional Videos", "comments": "Full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of automatically discovering atomic\nactions in unsupervised manner from instructional videos. Instructional videos\ncontain complex activities and are a rich source of information for intelligent\nagents, such as, autonomous robots or virtual assistants, which can, for\nexample, automatically `read' the steps from an instructional video and execute\nthem. However, videos are rarely annotated with atomic activities, their\nboundaries or duration. We present an unsupervised approach to learn atomic\nactions of structured human tasks from a variety of instructional videos. We\npropose a sequential stochastic autoregressive model for temporal segmentation\nof videos, which learns to represent and discover the sequential relationship\nbetween different atomic actions of the task, and which provides automatic and\nunsupervised self-labeling for videos. Our approach outperforms the\nstate-of-the-art unsupervised methods with large margins. We will open source\nthe code.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:05:01 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Angelova", "Anelia", ""], ["Ryoo", "Michael S.", ""], ["Essa", "Irfan", ""]]}, {"id": "2106.14736", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Rajmund Nagy, Patrik Jonell, Michael Neff, Hedvig\n  Kjellstr\\\"om, Gustav Eje Henter", "title": "Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for\n  Generating Representational Gestures from Speech", "comments": "Accepted for publication at the ACM International Conference on\n  Intelligent Virtual Agents (IVA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for gesture generation, aiming to allow\ndata-driven approaches to produce more semantically rich gestures. Our approach\nfirst predicts whether to gesture, followed by a prediction of the gesture\nproperties. Those properties are then used as conditioning for a modern\nprobabilistic gesture-generation model capable of high-quality output. This\nempowers the approach to generate gestures that are both diverse and\nrepresentational.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:07:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kucherenko", "Taras", ""], ["Nagy", "Rajmund", ""], ["Jonell", "Patrik", ""], ["Neff", "Michael", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Henter", "Gustav Eje", ""]]}, {"id": "2106.14739", "submitter": "Manuel Palermo", "authors": "Manuel Palermo, Sara Moccia, Lucia Migliorelli, Emanuele Frontoni,\n  Cristina P. Santos", "title": "Real-Time Human Pose Estimation on a Smart Walker using Convolutional\n  Neural Networks", "comments": "Accepted for publication in Expert Systems with Applications", "journal-ref": null, "doi": "10.1016/j.eswa.2021.115498", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Rehabilitation is important to improve quality of life for mobility-impaired\npatients. Smart walkers are a commonly used solution that should embed\nautomatic and objective tools for data-driven human-in-the-loop control and\nmonitoring. However, present solutions focus on extracting few specific metrics\nfrom dedicated sensors with no unified full-body approach. We investigate a\ngeneral, real-time, full-body pose estimation framework based on two RGB+D\ncamera streams with non-overlapping views mounted on a smart walker equipment\nused in rehabilitation. Human keypoint estimation is performed using a\ntwo-stage neural network framework. The 2D-Stage implements a detection module\nthat locates body keypoints in the 2D image frames. The 3D-Stage implements a\nregression module that lifts and relates the detected keypoints in both cameras\nto the 3D space relative to the walker. Model predictions are low-pass filtered\nto improve temporal consistency. A custom acquisition method was used to obtain\na dataset, with 14 healthy subjects, used for training and evaluating the\nproposed framework offline, which was then deployed on the real walker\nequipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage\nand 44.05mm for the 3D-Stage were reported, with an inference time of 26.6ms\nwhen deployed on the constrained hardware of the walker. We present a novel\napproach to patient monitoring and data-driven human-in-the-loop control in the\ncontext of smart walkers. It is able to extract a complete and compact body\nrepresentation in real-time and from inexpensive sensors, serving as a common\nbase for downstream metrics extraction solutions, and Human-Robot interaction\napplications. Despite promising results, more data should be collected on users\nwith impairments, to assess its performance as a rehabilitation tool in\nreal-world scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:11:48 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Palermo", "Manuel", ""], ["Moccia", "Sara", ""], ["Migliorelli", "Lucia", ""], ["Frontoni", "Emanuele", ""], ["Santos", "Cristina P.", ""]]}, {"id": "2106.14747", "submitter": "Hongchen Luo", "authors": "Hongchen Luo (1), Wei Zhai (1 and 3), Jing Zhang (2), Yang Cao (1) and\n  Dacheng Tao (3) ((1) University of Science and Technology of China, China,\n  (2) The University of Sydney, Australia, (3) JD Explore Academy, JD.com,\n  China)", "title": "One-Shot Affordance Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Affordance detection refers to identifying the potential action possibilities\nof objects in an image, which is an important ability for robot perception and\nmanipulation. To empower robots with this ability in unseen scenarios, we\nconsider the challenging one-shot affordance detection problem in this paper,\ni.e., given a support image that depicts the action purpose, all objects in a\nscene with the common affordance should be detected. To this end, we devise a\nOne-Shot Affordance Detection (OS-AD) network that firstly estimates the\npurpose and then transfers it to help detect the common affordance from all\ncandidate images. Through collaboration learning, OS-AD can capture the common\ncharacteristics between objects having the same underlying affordance and learn\na good adaptation capability for perceiving unseen affordances. Besides, we\nbuild a Purpose-driven Affordance Dataset (PAD) by collecting and labeling 4k\nimages from 31 affordance and 72 object categories. Experimental results\ndemonstrate the superiority of our model over previous representative ones in\nterms of both objective metrics and visual quality. The benchmark suite is at\nProjectPage.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:22:52 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Luo", "Hongchen", "", "1 and 3"], ["Zhai", "Wei", "", "1 and 3"], ["Zhang", "Jing", ""], ["Cao", "Yang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2106.14749", "submitter": "Pan Zhou", "authors": "Pan Zhou, Caiming Xiong, Xiao-Tong Yuan, Steven Hoi", "title": "A Theory-Driven Self-Labeling Refinement Method for Contrastive\n  Representation Learning", "comments": "under review. arXiv admin note: substantial text overlap with\n  arXiv:1903.11680 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG math.OC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For an image query, unsupervised contrastive learning labels crops of the\nsame image as positives, and other image crops as negatives. Although\nintuitive, such a native label assignment strategy cannot reveal the underlying\nsemantic similarity between a query and its positives and negatives, and\nimpairs performance, since some negatives are semantically similar to the query\nor even share the same semantic class as the query. In this work, we first\nprove that for contrastive learning, inaccurate label assignment heavily\nimpairs its generalization for semantic instance discrimination, while accurate\nlabels benefit its generalization. Inspired by this theory, we propose a novel\nself-labeling refinement approach for contrastive learning. It improves the\nlabel quality via two complementary modules: (i) self-labeling refinery (SLR)\nto generate accurate labels and (ii) momentum mixup (MM) to enhance similarity\nbetween query and its positive. SLR uses a positive of a query to estimate\nsemantic similarity between a query and its positive and negatives, and\ncombines estimated similarity with vanilla label assignment in contrastive\nlearning to iteratively generate more accurate and informative soft labels. We\ntheoretically show that our SLR can exactly recover the true semantic labels of\nlabel-corrupted data, and supervises networks to achieve zero prediction error\non classification tasks. MM randomly combines queries and positives to increase\nsemantic similarity between the generated virtual queries and their positives\nso as to improves label accuracy. Experimental results on CIFAR10, ImageNet,\nVOC and COCO show the effectiveness of our method. PyTorch code and model will\nbe released online.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:24:52 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhou", "Pan", ""], ["Xiong", "Caiming", ""], ["Yuan", "Xiao-Tong", ""], ["Hoi", "Steven", ""]]}, {"id": "2106.14804", "submitter": "Yunsong Zhao", "authors": "Yunsong Zhao, Yin Li, Zhihan Chen, Tianchong Qiu and Guojin Liu", "title": "Hyperspectral Remote Sensing Image Classification Based on Multi-scale\n  Cross Graphic Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The mining and utilization of features directly affect the classification\nperformance of models used in the classification and recognition of\nhyperspectral remote sensing images. Traditional models usually conduct feature\nmining from a single perspective, with the features mined being limited and the\ninternal relationships between them being ignored. Consequently, useful\nfeatures are lost and classification results are unsatisfactory. To fully mine\nand utilize image features, a new multi-scale feature-mining learning algorithm\n(MGRNet) is proposed. The model uses principal component analysis to reduce the\ndimensionality of the original hyperspectral image (HSI) to retain 99.99% of\nits semantic information and extract dimensionality reduction features. Using a\nmulti-scale convolution algorithm, the input dimensionality reduction features\nwere mined to obtain shallow features, which then served as inputs into a\nmulti-scale graph convolution algorithm to construct the internal relationships\nbetween eigenvalues at different scales. We then carried out cross fusion of\nmulti-scale information obtained by graph convolution, before inputting the new\ninformation obtained into the residual network algorithm for deep feature\nmining. Finally, a flexible maximum transfer function classifier was used to\npredict the final features and complete the classification. Experiments on\nthree common hyperspectral datasets showed the MGRNet algorithm proposed in\nthis paper to be superior to traditional methods in recognition accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 15:28:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhao", "Yunsong", ""], ["Li", "Yin", ""], ["Chen", "Zhihan", ""], ["Qiu", "Tianchong", ""], ["Liu", "Guojin", ""]]}, {"id": "2106.14829", "submitter": "Ekberjan Derman", "authors": "Ekberjan Derman", "title": "Dataset Bias Mitigation Through Analysis of CNN Training Scores", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training datasets are crucial for convolutional neural network-based\nalgorithms, which directly impact their overall performance. As such, using a\nwell-structured dataset that has minimum level of bias is always desirable. In\nthis paper, we proposed a novel, domain-independent approach, called\nscore-based resampling (SBR), to locate the under-represented samples of the\noriginal training dataset based on the model prediction scores obtained with\nthat training set. In our method, once trained, we use the same CNN model to\ninfer on its own training samples, obtain prediction scores, and based on the\ndistance between predicted and ground-truth, we identify samples that are far\naway from their ground-truth and augment them in the original training set. The\ntemperature term of the Sigmoid function is decreased to better differentiate\nscores. For experimental evaluation, we selected one Kaggle dataset for gender\nclassification. We first used a CNN-based classifier with relatively standard\nstructure, trained on the training images, and evaluated on the provided\nvalidation samples of the original dataset. Then, we assessed it on a totally\nnew test dataset consisting of light male, light female, dark male, and dark\nfemale groups. The obtained accuracies varied, revealing the existence of\ncategorical bias against certain groups in the original dataset. Subsequently,\nwe trained the model after resampling based on our proposed approach. We\ncompared our method with a previously proposed variational autoencoder (VAE)\nbased algorithm. The obtained results confirmed the validity of our proposed\nmethod regrading identifying under-represented samples among original dataset\nto decrease categorical bias of classifying certain groups. Although tested for\ngender classification, the proposed algorithm can be used for investigating\ndataset structure of any CNN-based tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 16:07:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Derman", "Ekberjan", ""]]}, {"id": "2106.14836", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Linjun Zhang, Zhun Deng", "title": "Understanding Dynamics of Nonlinear Representation Learning and Its\n  Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations of the world environment play a crucial role in machine\nintelligence. It is often inefficient to conduct reasoning and inference\ndirectly in the space of raw sensory representations, such as pixel values of\nimages. Representation learning allows us to automatically discover suitable\nrepresentations from raw sensory data. For example, given raw sensory data, a\nmultilayer perceptron learns nonlinear representations at its hidden layers,\nwhich are subsequently used for classification (or regression) at its output\nlayer. This happens implicitly during training through minimizing a supervised\nor unsupervised loss. In this paper, we study the dynamics of such implicit\nnonlinear representation learning. We identify a pair of a new assumption and a\nnovel condition, called the common model structure assumption and the\ndata-architecture alignment condition. Under the common model structure\nassumption, the data-architecture alignment condition is shown to be sufficient\nfor the global convergence and necessary for the global optimality. Our results\nprovide practical guidance for designing a model structure: e.g., the common\nmodel structure assumption can be used as a justification for using a\nparticular model structure instead of others. As an application, we then derive\na new training framework, which satisfies the data-architecture alignment\ncondition without assuming it by automatically modifying any given training\nalgorithm dependently on each data and architecture. Given a standard training\nalgorithm, the framework running its modified version is empirically shown to\nmaintain competitive (practical) test performances while providing global\nconvergence guarantees for ResNet-18 with convolutions, skip connections, and\nbatch normalization with standard benchmark datasets, including MNIST,\nCIFAR-10, CIFAR-100, Semeion, KMNIST and SVHN.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 16:31:30 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Zhang", "Linjun", ""], ["Deng", "Zhun", ""]]}, {"id": "2106.14843", "submitter": "Kevin Frans", "authors": "Kevin Frans, L.B. Soros, Olaf Witkowski", "title": "CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image\n  Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents CLIPDraw, an algorithm that synthesizes novel drawings\nbased on natural language input. CLIPDraw does not require any training; rather\na pre-trained CLIP language-image encoder is used as a metric for maximizing\nsimilarity between the given description and a generated drawing. Crucially,\nCLIPDraw operates over vector strokes rather than pixel images, a constraint\nthat biases drawings towards simpler human-recognizable shapes. Results compare\nbetween CLIPDraw and other synthesis-through-optimization methods, as well as\nhighlight various interesting behaviors of CLIPDraw, such as satisfying\nambiguous text in multiple ways, reliably producing drawings in diverse\nartistic styles, and scaling from simple to complex visual representations as\nstroke count is increased. Code for experimenting with the method is available\nat:\nhttps://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 16:43:26 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Frans", "Kevin", ""], ["Soros", "L. B.", ""], ["Witkowski", "Olaf", ""]]}, {"id": "2106.14844", "submitter": "Yucheng Lu", "authors": "Yucheng Lu and Seung-Won Jung", "title": "Progressive Joint Low-light Enhancement and Noise Removal for Raw Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light imaging on mobile devices is typically challenging due to\ninsufficient incident light coming through the relatively small aperture,\nresulting in a low signal-to-noise ratio. Most of the previous works on\nlow-light image processing focus either only on a single task such as\nillumination adjustment, color enhancement, or noise removal; or on a joint\nillumination adjustment and denoising task that heavily relies on short-long\nexposure image pairs collected from specific camera models, and thus these\napproaches are less practical and generalizable in real-world settings where\ncamera-specific joint enhancement and restoration is required. To tackle this\nproblem, in this paper, we propose a low-light image processing framework that\nperforms joint illumination adjustment, color enhancement, and denoising.\nConsidering the difficulty in model-specific data collection and the ultra-high\ndefinition of the captured images, we design two branches: a coefficient\nestimation branch as well as a joint enhancement and denoising branch. The\ncoefficient estimation branch works in a low-resolution space and predicts the\ncoefficients for enhancement via bilateral learning, whereas the joint\nenhancement and denoising branch works in a full-resolution space and\nprogressively performs joint enhancement and denoising. In contrast to existing\nmethods, our framework does not need to recollect massive data when being\nadapted to another camera model, which significantly reduces the efforts\nrequired to fine-tune our approach for practical usage. Through extensive\nexperiments, we demonstrate its great potential in real-world low-light imaging\napplications when compared with current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 16:43:52 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 06:47:48 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 08:38:46 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Lu", "Yucheng", ""], ["Jung", "Seung-Won", ""]]}, {"id": "2106.14845", "submitter": "Meiling Fang", "authors": "Meiling Fang, Naser Damer, Fadi Boutros, Florian Kirchbuchner, Arjan\n  Kuijper", "title": "Iris Presentation Attack Detection by Attention-based and Deep\n  Pixel-wise Binary Supervision Network", "comments": "To appear at the 2021 International Joint Conference on Biometrics\n  (IJCB 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris presentation attack detection (PAD) plays a vital role in iris\nrecognition systems. Most existing CNN-based iris PAD solutions 1) perform only\nbinary label supervision during the training of CNNs, serving global\ninformation learning but weakening the capture of local discriminative\nfeatures, 2) prefer the stacked deeper convolutions or expert-designed\nnetworks, raising the risk of overfitting, 3) fuse multiple PAD systems or\nvarious types of features, increasing difficulty for deployment on mobile\ndevices. Hence, we propose a novel attention-based deep pixel-wise binary\nsupervision (A-PBS) method. Pixel-wise supervision is first able to capture the\nfine-grained pixel/patch-level cues. Then, the attention mechanism guides the\nnetwork to automatically find regions that most contribute to an accurate PAD\ndecision. Extensive experiments are performed on LivDet-Iris 2017 and three\nother publicly available databases to show the effectiveness and robustness of\nproposed A-PBS methods. For instance, the A-PBS model achieves an HTER of 6.50%\non the IIITD-WVU database outperforming state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 16:47:08 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fang", "Meiling", ""], ["Damer", "Naser", ""], ["Boutros", "Fadi", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2106.14855", "submitter": "Wenwei Zhang", "authors": "Wenwei Zhang, Jiangmiao Pang, Kai Chen, Chen Change Loy", "title": "K-Net: Towards Unified Image Segmentation", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic, instance, and panoptic segmentations have been addressed using\ndifferent and specialized frameworks despite their underlying connections. This\npaper presents a unified, simple, and effective framework for these essentially\nsimilar tasks. The framework, named K-Net, segments both instances and semantic\ncategories consistently by a group of learnable kernels, where each kernel is\nresponsible for generating a mask for either a potential instance or a stuff\nclass. To remedy the difficulties of distinguishing various instances, we\npropose a kernel update strategy that enables each kernel dynamic and\nconditional on its meaningful group in the input image. K-Net can be trained in\nan end-to-end manner with bipartite matching, and its training and inference\nare naturally NMS-free and box-free. Without bells and whistles, K-Net\nsurpasses all previous state-of-the-art single-model results of panoptic\nsegmentation on MS COCO and semantic segmentation on ADE20K with 52.1% PQ and\n54.3% mIoU, respectively. Its instance segmentation performance is also on par\nwith Cascade Mask R-CNNon MS COCO with 60%-90% faster inference speeds. Code\nand models will be released at https://github.com/open-mmlab/mmdetection.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:18:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhang", "Wenwei", ""], ["Pang", "Jiangmiao", ""], ["Chen", "Kai", ""], ["Loy", "Chen Change", ""]]}, {"id": "2106.14879", "submitter": "Donglai Xiang", "authors": "Donglai Xiang, Fabian Andres Prada, Timur Bagautdinov, Weipeng Xu,\n  Yuan Dong, He Wen, Jessica Hodgins, Chenglei Wu", "title": "Explicit Clothing Modeling for an Animatable Full-Body Avatar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown great progress in building photorealistic animatable\nfull-body codec avatars, but these avatars still face difficulties in\ngenerating high-fidelity animation of clothing. To address the difficulties, we\npropose a method to build an animatable clothed body avatar with an explicit\nrepresentation of the clothing on the upper body from multi-view captured\nvideos. We use a two-layer mesh representation to separately register the 3D\nscans with templates. In order to improve the photometric correspondence across\ndifferent frames, texture alignment is then performed through inverse rendering\nof the clothing geometry and texture predicted by a variational autoencoder. We\nthen train a new two-layer codec avatar with separate modeling of the upper\nclothing and the inner body layer. To learn the interaction between the body\ndynamics and clothing states, we use a temporal convolution network to predict\nthe clothing latent code based on a sequence of input skeletal poses. We show\nphotorealistic animation output for three different actors, and demonstrate the\nadvantage of our clothed-body avatars over single-layer avatars in the previous\nwork. We also show the benefit of an explicit clothing model which allows the\nclothing texture to be edited in the animation output.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:58:40 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 19:51:00 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Xiang", "Donglai", ""], ["Prada", "Fabian Andres", ""], ["Bagautdinov", "Timur", ""], ["Xu", "Weipeng", ""], ["Dong", "Yuan", ""], ["Wen", "He", ""], ["Hodgins", "Jessica", ""], ["Wu", "Chenglei", ""]]}, {"id": "2106.14880", "submitter": "Yin Zhou", "authors": "Lu Mi, Hang Zhao, Charlie Nash, Xiaohan Jin, Jiyang Gao, Chen Sun,\n  Cordelia Schmid, Nir Shavit, Yuning Chai, Dragomir Anguelov", "title": "HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Definition (HD) maps are maps with precise definitions of road lanes\nwith rich semantics of the traffic rules. They are critical for several key\nstages in an autonomous driving system, including motion forecasting and\nplanning. However, there are only a small amount of real-world road topologies\nand geometries, which significantly limits our ability to test out the\nself-driving stack to generalize onto new unseen scenarios. To address this\nissue, we introduce a new challenging task to generate HD maps. In this work,\nwe explore several autoregressive models using different data representations,\nincluding sequence, plain graph, and hierarchical graph. We propose HDMapGen, a\nhierarchical graph generation model capable of producing high-quality and\ndiverse HD maps through a coarse-to-fine approach. Experiments on the Argoverse\ndataset and an in-house dataset show that HDMapGen significantly outperforms\nbaseline methods. Additionally, we demonstrate that HDMapGen achieves high\nscalability and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:59:30 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Mi", "Lu", ""], ["Zhao", "Hang", ""], ["Nash", "Charlie", ""], ["Jin", "Xiaohan", ""], ["Gao", "Jiyang", ""], ["Sun", "Chen", ""], ["Schmid", "Cordelia", ""], ["Shavit", "Nir", ""], ["Chai", "Yuning", ""], ["Anguelov", "Dragomir", ""]]}, {"id": "2106.14881", "submitter": "Tete Xiao", "authors": "Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll\\'ar,\n  Ross Girshick", "title": "Early Convolutions Help Transformers See Better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision transformer (ViT) models exhibit substandard optimizability. In\nparticular, they are sensitive to the choice of optimizer (AdamW vs. SGD),\noptimizer hyperparameters, and training schedule length. In comparison, modern\nconvolutional neural networks are far easier to optimize. Why is this the case?\nIn this work, we conjecture that the issue lies with the patchify stem of ViT\nmodels, which is implemented by a stride-p pxp convolution (p=16 by default)\napplied to the input image. This large-kernel plus large-stride convolution\nruns counter to typical design choices of convolutional layers in neural\nnetworks. To test whether this atypical design choice causes an issue, we\nanalyze the optimization behavior of ViT models with their original patchify\nstem versus a simple counterpart where we replace the ViT stem by a small\nnumber of stacked stride-two 3x3 convolutions. While the vast majority of\ncomputation in the two ViT designs is identical, we find that this small change\nin early visual processing results in markedly different training behavior in\nterms of the sensitivity to optimization settings as well as the final model\naccuracy. Using a convolutional stem in ViT dramatically increases optimization\nstability and also improves peak performance (by ~1-2% top-1 accuracy on\nImageNet-1k), while maintaining flops and runtime. The improvement can be\nobserved across the wide spectrum of model complexities (from 1G to 36G flops)\nand dataset scales (from ImageNet-1k to ImageNet-21k). These findings lead us\nto recommend using a standard, lightweight convolutional stem for ViT models as\na more robust architectural choice compared to the original ViT model design.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:59:33 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 16:49:06 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Xiao", "Tete", ""], ["Singh", "Mannat", ""], ["Mintun", "Eric", ""], ["Darrell", "Trevor", ""], ["Doll\u00e1r", "Piotr", ""], ["Girshick", "Ross", ""]]}, {"id": "2106.14882", "submitter": "Ping Li", "authors": "Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, Ping Li", "title": "Rethinking Token-Mixing MLP for MLP-based Vision Backbone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past decade, we have witnessed rapid progress in the machine vision\nbackbone. By introducing the inductive bias from the image processing,\nconvolution neural network (CNN) has achieved excellent performance in numerous\ncomputer vision tasks and has been established as \\emph{de facto} backbone. In\nrecent years, inspired by the great success achieved by Transformer in NLP\ntasks, vision Transformer models emerge. Using much less inductive bias, they\nhave achieved promising performance in computer vision tasks compared with\ntheir CNN counterparts. More recently, researchers investigate using the\npure-MLP architecture to build the vision backbone to further reduce the\ninductive bias, achieving good performance. The pure-MLP backbone is built upon\nchannel-mixing MLPs to fuse the channels and token-mixing MLPs for\ncommunications between patches. In this paper, we re-think the design of the\ntoken-mixing MLP. We discover that token-mixing MLPs in existing MLP-based\nbackbones are spatial-specific, and thus it is sensitive to spatial\ntranslation. Meanwhile, the channel-agnostic property of the existing\ntoken-mixing MLPs limits their capability in mixing tokens. To overcome those\nlimitations, we propose an improved structure termed as Circulant\nChannel-Specific (CCS) token-mixing MLP, which is spatial-invariant and\nchannel-specific. It takes fewer parameters but achieves higher classification\naccuracy on ImageNet1K benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:59:57 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yu", "Tan", ""], ["Li", "Xu", ""], ["Cai", "Yunfeng", ""], ["Sun", "Mingming", ""], ["Li", "Ping", ""]]}, {"id": "2106.14917", "submitter": "Junjiao Tian", "authors": "Junjiao Tian, Niluthpol Mithun, Zach Seymour, Han-Pang Chiu, Zsolt\n  Kira", "title": "Striking the Right Balance: Recall Loss for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Class imbalance is a fundamental problem in computer vision applications such\nas semantic segmentation. Specifically, uneven class distributions in a\ntraining dataset often result in unsatisfactory performance on\nunder-represented classes. Many works have proposed to weight the standard\ncross entropy loss function with pre-computed weights based on class\nstatistics, such as the number of samples and class margins. There are two\nmajor drawbacks to these methods: 1) constantly up-weighting minority classes\ncan introduce excessive false positives in semantic segmentation; 2) a minority\nclass is not necessarily a hard class. The consequence is low precision due to\nexcessive false positives. In this regard, we propose a hard-class mining loss\nby reshaping the vanilla cross entropy loss such that it weights the loss for\neach class dynamically based on instantaneous recall performance. We show that\nthe novel recall loss changes gradually between the standard cross entropy loss\nand the inverse frequency weighted loss. Recall loss also leads to improved\nmean accuracy while offering competitive mean Intersection over Union (IoU)\nperformance. On Synthia dataset, recall loss achieves 9% relative improvement\non mean accuracy with competitive mean IoU using DeepLab-ResNet18 compared to\nthe cross entropy loss. Code available at\nhttps://github.com/PotatoTian/recall-semseg.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 18:02:03 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Tian", "Junjiao", ""], ["Mithun", "Niluthpol", ""], ["Seymour", "Zach", ""], ["Chiu", "Han-Pang", ""], ["Kira", "Zsolt", ""]]}, {"id": "2106.14922", "submitter": "Chengyuan Xu", "authors": "Chengyuan Xu, Curtis McCully, Boning Dong, D. Andrew Howell, Pradeep\n  Sen", "title": "Cosmic-CoNN: A Cosmic Ray Detection Deep-Learning Framework, Dataset,\n  and Toolkit", "comments": "18 pages, 13 figures, 3 tables. Submitted to AAS Journals. See\n  https://github.com/cy-xu/cosmic-conn for the open-source software and\n  https://zenodo.org/record/5034763 for the dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejecting cosmic rays (CRs) is essential for scientific interpretation of\nCCD-captured data, but detecting CRs in single-exposure images has remained\nchallenging. Conventional CR-detection algorithms require tuning multiple\nparameters experimentally making it hard to automate across different\ninstruments or observation requests. Recent work using deep learning to train\nCR-detection models has demonstrated promising results. However,\ninstrument-specific models suffer from performance loss on images from\nground-based facilities not included in the training data. In this work, we\npresent Cosmic-CoNN, a deep-learning framework designed to produce generic\nCR-detection models. We build a large, diverse ground-based CR dataset\nleveraging thousands of images from the Las Cumbres Observatory global\ntelescope network to produce a generic CR-detection model which achieves a\n99.91% true-positive detection rate and maintains over 96.40% true-positive\nrates on unseen data from Gemini GMOS-N/S, with a false-positive rate of 0.01%.\nApart from the open-source framework and dataset, we also build a suite of\ntools including console commands, a web-based application, and Python APIs to\nmake automatic, robust CR detection widely accessible by the community of\nastronomers.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 18:04:56 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Xu", "Chengyuan", ""], ["McCully", "Curtis", ""], ["Dong", "Boning", ""], ["Howell", "D. Andrew", ""], ["Sen", "Pradeep", ""]]}, {"id": "2106.14942", "submitter": "Alexander Bergman", "authors": "Alexander W. Bergman and Petr Kellnhofer and Gordon Wetzstein", "title": "Fast Training of Neural Lumigraph Representations using Meta Learning", "comments": "Project website:\n  http://www.computationalimaging.org/publications/metanlr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel view synthesis is a long-standing problem in machine learning and\ncomputer vision. Significant progress has recently been made in developing\nneural scene representations and rendering techniques that synthesize\nphotorealistic images from arbitrary views. These representations, however, are\nextremely slow to train and often also slow to render. Inspired by neural\nvariants of image-based rendering, we develop a new neural rendering approach\nwith the goal of quickly learning a high-quality representation which can also\nbe rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a\nunique combination of a neural shape representation and 2D CNN-based image\nfeature extraction, aggregation, and re-projection. To push representation\nconvergence times down to minutes, we leverage meta learning to learn neural\nshape and image feature priors which accelerate training. The optimized shape\nand image features can then be extracted using traditional graphics techniques\nand rendered in real time. We show that MetaNLR++ achieves similar or better\nnovel view synthesis results in a fraction of the time that competing methods\nrequire.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 18:55:50 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Bergman", "Alexander W.", ""], ["Kellnhofer", "Petr", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2106.14943", "submitter": "Pu Zhao", "authors": "Pu Zhao, Wei Niu, Geng Yuan, Yuxuan Cai, Bin Ren, Yanzhi Wang, Xue Lin", "title": "Achieving Real-Time Object Detection on MobileDevices with Neural\n  Pruning Search", "comments": "Presented on the HiPEAC 2021 workshop (cogarch 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Object detection plays an important role in self-driving cars for security\ndevelopment. However, mobile systems on self-driving cars with limited\ncomputation resources lead to difficulties for object detection. To facilitate\nthis, we propose a compiler-aware neural pruning search framework to achieve\nhigh-speed inference on autonomous vehicles for 2D and 3D object detection. The\nframework automatically searches the pruning scheme and rate for each layer to\nfind a best-suited pruning for optimizing detection accuracy and speed\nperformance under compiler optimization. Our experiments demonstrate that for\nthe first time, the proposed method achieves (close-to) real-time, 55ms and\n99ms inference times for YOLOv4 based 2D object detection and PointPillars\nbased 3D detection, respectively, on an off-the-shelf mobile phone with minor\n(or no) accuracy loss.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 18:59:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhao", "Pu", ""], ["Niu", "Wei", ""], ["Yuan", "Geng", ""], ["Cai", "Yuxuan", ""], ["Ren", "Bin", ""], ["Wang", "Yanzhi", ""], ["Lin", "Xue", ""]]}, {"id": "2106.14947", "submitter": "Zalan Fabian", "authors": "Zalan Fabian, Reinhard Heckel, Mahdi Soltanolkotabi", "title": "Data augmentation for deep learning based accelerated MRI reconstruction\n  with limited data", "comments": "27 pages, 19 figures, to be published in ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have emerged as very successful tools for image\nrestoration and reconstruction tasks. These networks are often trained\nend-to-end to directly reconstruct an image from a noisy or corrupted\nmeasurement of that image. To achieve state-of-the-art performance, training on\nlarge and diverse sets of images is considered critical. However, it is often\ndifficult and/or expensive to collect large amounts of training images.\nInspired by the success of Data Augmentation (DA) for classification problems,\nin this paper, we propose a pipeline for data augmentation for accelerated MRI\nreconstruction and study its effectiveness at reducing the required training\ndata in a variety of settings. Our DA pipeline, MRAugment, is specifically\ndesigned to utilize the invariances present in medical imaging measurements as\nnaive DA strategies that neglect the physics of the problem fail. Through\nextensive studies on multiple datasets we demonstrate that in the low-data\nregime DA prevents overfitting and can match or even surpass the state of the\nart while using significantly fewer training data, whereas in the high-data\nregime it has diminishing returns. Furthermore, our findings show that DA can\nimprove the robustness of the model against various shifts in the test\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 19:08:46 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Fabian", "Zalan", ""], ["Heckel", "Reinhard", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "2106.14948", "submitter": "Zitong Yu", "authors": "Zitong Yu, Yunxiao Qin, Xiaobai Li, Chenxu Zhao, Zhen Lei, Guoying\n  Zhao", "title": "Deep Learning for Face Anti-Spoofing: A Survey", "comments": "submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing (FAS) has lately attracted increasing attention due to its\nvital role in securing face recognition systems from presentation attacks\n(PAs). As more and more realistic PAs with novel types spring up, traditional\nFAS methods based on handcrafted features become unreliable due to their\nlimited representation capacity. With the emergence of large-scale academic\ndatasets in the recent decade, deep learning based FAS achieves remarkable\nperformance and dominates this area. However, existing reviews in this field\nmainly focus on the handcrafted features, which are outdated and uninspiring\nfor the progress of FAS community. In this paper, to stimulate future research,\nwe present the first comprehensive review of recent advances in deep learning\nbased FAS. It covers several novel and insightful components: 1) besides\nsupervision with binary label (e.g., '0' for bonafide vs. '1' for PAs), we also\ninvestigate recent methods with pixel-wise supervision (e.g., pseudo depth\nmap); 2) in addition to traditional intra-dataset evaluation, we collect and\nanalyze the latest methods specially designed for domain generalization and\nopen-set FAS; and 3) besides commercial RGB camera, we summarize the deep\nlearning applications under multi-modal (e.g., depth and infrared) or\nspecialized (e.g., light field and flash) sensors. We conclude this survey by\nemphasizing current open issues and highlighting potential prospects.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 19:12:00 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yu", "Zitong", ""], ["Qin", "Yunxiao", ""], ["Li", "Xiaobai", ""], ["Zhao", "Chenxu", ""], ["Lei", "Zhen", ""], ["Zhao", "Guoying", ""]]}, {"id": "2106.14989", "submitter": "Yuli Wu", "authors": "Yuli Wu, Yucheng Hu, Suting Miao", "title": "Object Detection Based Handwriting Localization", "comments": "ICDAR 2021 Workshop: Industrial Applications of Document Analysis and\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an object detection based approach to localize handwritten regions\nfrom documents, which initially aims to enhance the anonymization during the\ndata transmission. The concatenated fusion of original and preprocessed images\ncontaining both printed texts and handwritten notes or signatures are fed into\nthe convolutional neural network, where the bounding boxes are learned to\ndetect the handwriting. Afterwards, the handwritten regions can be processed\n(e.g. replaced with redacted signatures) to conceal the personally identifiable\ninformation (PII). This processing pipeline based on the deep learning network\nCascade R-CNN works at 10 fps on a GPU during the inference, which ensures the\nenhanced anonymization with minimal computational overheads. Furthermore, the\nimpressive generalizability has been empirically showcased: the trained model\nbased on the English-dominant dataset works well on the fictitious unseen\ninvoices, even in Chinese. The proposed approach is also expected to facilitate\nother tasks such as handwriting recognition and signature verification.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 21:25:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wu", "Yuli", ""], ["Hu", "Yucheng", ""], ["Miao", "Suting", ""]]}, {"id": "2106.15004", "submitter": "Nachiket Deo", "authors": "Nachiket Deo, Eric M. Wolff and Oscar Beijbom", "title": "Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurately predicting the future motion of surrounding vehicles requires\nreasoning about the inherent uncertainty in goals and driving behavior. This\nuncertainty can be loosely decoupled into lateral (e.g., keeping lane, turning)\nand longitudinal (e.g., accelerating, braking). We present a novel method that\ncombines learned discrete policy rollouts with a focused decoder on subsets of\nthe lane graph. The policy rollouts explore different goals given our current\nobservations, ensuring that the model captures lateral variability. The\nlongitudinal variability is captured by our novel latent variable model decoder\nthat is conditioned on various subsets of the lane graph. Our model achieves\nstate-of-the-art performance on the nuScenes motion prediction dataset, and\nqualitatively demonstrates excellent scene compliance. Detailed ablations\nhighlight the importance of both the policy rollouts and the decoder\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:23:14 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Deo", "Nachiket", ""], ["Wolff", "Eric M.", ""], ["Beijbom", "Oscar", ""]]}, {"id": "2106.15007", "submitter": "William Beksi", "authors": "Zongyao Lyu, Nolan B. Gutierrez, William J. Beksi", "title": "An Uncertainty Estimation Framework for Probabilistic Object Detection", "comments": "To be published in the 2021 International Conference on Automation\n  Science and Engineering (CASE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a new technique that combines two popular methods\nto estimate uncertainty in object detection. Quantifying uncertainty is\ncritical in real-world robotic applications. Traditional detection models can\nbe ambiguous even when they provide a high-probability output. Robot actions\nbased on high-confidence, yet unreliable predictions, may result in serious\nrepercussions. Our framework employs deep ensembles and Monte Carlo dropout for\napproximating predictive uncertainty, and it improves upon the uncertainty\nestimation quality of the baseline method. The proposed approach is evaluated\non publicly available synthetic image datasets captured from sequences of\nvideo.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:29:59 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Lyu", "Zongyao", ""], ["Gutierrez", "Nolan B.", ""], ["Beksi", "William J.", ""]]}, {"id": "2106.15009", "submitter": "Ashish Jaiswal", "authors": "Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Fillia\n  Makedon, Glenn Wylie", "title": "Understanding Cognitive Fatigue from fMRI Scans with Self-supervised\n  Learning", "comments": "8 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) is a neuroimaging technique that\nrecords neural activations in the brain by capturing the blood oxygen level in\ndifferent regions based on the task performed by a subject. Given fMRI data,\nthe problem of predicting the state of cognitive fatigue in a person has not\nbeen investigated to its full extent. This paper proposes tackling this issue\nas a multi-class classification problem by dividing the state of cognitive\nfatigue into six different levels, ranging from no-fatigue to extreme fatigue\nconditions. We built a spatio-temporal model that uses convolutional neural\nnetworks (CNN) for spatial feature extraction and a long short-term memory\n(LSTM) network for temporal modeling of 4D fMRI scans. We also applied a\nself-supervised method called MoCo to pre-train our model on a public dataset\nBOLD5000 and fine-tuned it on our labeled dataset to classify cognitive\nfatigue. Our novel dataset contains fMRI scans from Traumatic Brain Injury\n(TBI) patients and healthy controls (HCs) while performing a series of\ncognitive tasks. This method establishes a state-of-the-art technique to\nanalyze cognitive fatigue from fMRI data and beats previous approaches to solve\nthis problem.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:38:51 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 17:09:18 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Jaiswal", "Ashish", ""], ["Babu", "Ashwin Ramesh", ""], ["Zadeh", "Mohammad Zaki", ""], ["Makedon", "Fillia", ""], ["Wylie", "Glenn", ""]]}, {"id": "2106.15011", "submitter": "Houssem Eddine Boulahbal", "authors": "Houssem-eddine Boulahbal, Adrian Voicila, Andrew Comport", "title": "Are conditional GANs explicitly conditional?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes two important contributions for conditional Generative\nAdversarial Networks (cGANs) to improve the wide variety of applications that\nexploit this architecture. The first main contribution is an analysis of cGANs\nto show that they are not explicitly conditional. In particular, it will be\nshown that the discriminator and subsequently the cGAN does not automatically\nlearn the conditionality between inputs. The second contribution is a new\nmethod, called acontrario, that explicitly models conditionality for both parts\nof the adversarial architecture via a novel acontrario loss that involves\ntraining the discriminator to learn unconditional (adverse) examples. This\nleads to a novel type of data augmentation approach for GANs (acontrario\nlearning) which allows to restrict the search space of the generator to\nconditional outputs using adverse examples. Extensive experimentation is\ncarried out to evaluate the conditionality of the discriminator by proposing a\nprobability distribution analysis. Comparisons with the cGAN architecture for\ndifferent applications show significant improvements in performance on well\nknown datasets including, semantic image synthesis, image segmentation and\nmonocular depth prediction using different metrics including Fr\\'echet\nInception Distance(FID), mean Intersection over Union (mIoU), Root Mean Square\nError log (RMSE log) and Number of statistically-Different Bins (NDB)\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:49:27 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Boulahbal", "Houssem-eddine", ""], ["Voicila", "Adrian", ""], ["Comport", "Andrew", ""]]}, {"id": "2106.15020", "submitter": "Sara Bj\\\"ork", "authors": "Sara Bj\\\"ork, Stian Normann Anfinsen, Erik N{\\ae}sset, Terje Gobakken\n  and Eliakimu Zahabu", "title": "Constructing Forest Biomass Prediction Maps from Radar Backscatter by\n  Sequential Regression with a Conditional Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies construction of above-ground biomass (AGB) prediction maps\nfrom synthetic aperture radar (SAR) intensity images. The purpose is to improve\ntraditional regression models based on SAR intensity, trained with a limited\namount of AGB in situ measurements. Although it is costly to collect, data from\nairborne laser scanning (ALS) sensors are highly correlated with AGB.\nTherefore, we propose using AGB predictions based on ALS data as surrogate\nresponse variables for SAR data in a sequential modelling fashion. This\nincreases the amount of training data dramatically. To model the regression\nfunction between SAR intensity and ALS-predicted AGB we propose to utilise a\nconditional generative adversarial network (cGAN), i.e. the Pix2Pix\nconvolutional neural network. This enables the recreation of existing ALS-based\nAGB prediction maps. The generated synthesised ALS-based AGB predictions are\nevaluated qualitatively and quantitatively against ALS-based AGB predictions\nretrieved from a traditional non-sequential regression model trained in the\nsame area. Results show that the proposed architecture manages to capture\ncharacteristics of the actual data. This suggests that the use of ALS-guided\ngenerative models is a promising avenue for AGB prediction from SAR intensity.\nFurther research on this area has the potential of providing both large-scale\nand low-cost predictions of AGB.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:05:35 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Bj\u00f6rk", "Sara", ""], ["Anfinsen", "Stian Normann", ""], ["N\u00e6sset", "Erik", ""], ["Gobakken", "Terje", ""], ["Zahabu", "Eliakimu", ""]]}, {"id": "2106.15021", "submitter": "Stylianos Venieris", "authors": "Stylianos I. Venieris and Ioannis Panopoulos and Ilias Leontiadis and\n  Iakovos S. Venieris", "title": "How to Reach Real-Time AI on Consumer Devices? Solutions for\n  Programmable and Custom Architectures", "comments": "Invited paper at the 32nd IEEE International Conference on\n  Application-Specific Systems, Architectures and Processors (ASAP), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented performance of deep neural networks (DNNs) has led to large\nstrides in various Artificial Intelligence (AI) inference tasks, such as object\nand speech recognition. Nevertheless, deploying such AI models across commodity\ndevices faces significant challenges: large computational cost, multiple\nperformance objectives, hardware heterogeneity and a common need for high\naccuracy, together pose critical problems to the deployment of DNNs across the\nvarious embedded and mobile devices in the wild. As such, we have yet to\nwitness the mainstream usage of state-of-the-art deep learning algorithms\nacross consumer devices. In this paper, we provide preliminary answers to this\npotentially game-changing question by presenting an array of design techniques\nfor efficient AI systems. We start by examining the major roadblocks when\ntargeting both programmable processors and custom accelerators. Then, we\npresent diverse methods for achieving real-time performance following a\ncross-stack approach. These span model-, system- and hardware-level techniques,\nand their combination. Our findings provide illustrative examples of AI systems\nthat do not overburden mobile hardware, while also indicating how they can\nimprove inference accuracy. Moreover, we showcase how custom ASIC- and\nFPGA-based accelerators can be an enabling factor for next-generation AI\napplications, such as multi-DNN systems. Collectively, these results highlight\nthe critical need for further exploration as to how the various cross-stack\nsolutions can be best combined in order to bring the latest advances in deep\nlearning close to users, in a robust and efficient manner.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 11:23:12 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Venieris", "Stylianos I.", ""], ["Panopoulos", "Ioannis", ""], ["Leontiadis", "Ilias", ""], ["Venieris", "Iakovos S.", ""]]}, {"id": "2106.15045", "submitter": "Nitin J. Sanket", "authors": "Nitin J. Sanket, Chahat Deep Singh, Chethan M. Parameshwara, Cornelia\n  Ferm\\\"uller, Guido C.H.E. de Croon, Yiannis Aloimonos", "title": "EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing\n  And Following", "comments": "11 pages, 10 figures, 6 tables. Accepted in Robotics: Science and\n  Systems (RSS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid rise of accessibility of unmanned aerial vehicles or drones pose a\nthreat to general security and confidentiality. Most of the commercially\navailable or custom-built drones are multi-rotors and are comprised of multiple\npropellers. Since these propellers rotate at a high-speed, they are generally\nthe fastest moving parts of an image and cannot be directly \"seen\" by a\nclassical camera without severe motion blur. We utilize a class of sensors that\nare particularly suitable for such scenarios called event cameras, which have a\nhigh temporal resolution, low-latency, and high dynamic range.\n  In this paper, we model the geometry of a propeller and use it to generate\nsimulated events which are used to train a deep neural network called EVPropNet\nto detect propellers from the data of an event camera. EVPropNet directly\ntransfers to the real world without any fine-tuning or retraining. We present\ntwo applications of our network: (a) tracking and following an unmarked drone\nand (b) landing on a near-hover drone. We successfully evaluate and demonstrate\nthe proposed approach in many real-world experiments with different propeller\nshapes and sizes. Our network can detect propellers at a rate of 85.1% even\nwhen 60% of the propeller is occluded and can run at upto 35Hz on a 2W power\nbudget. To our knowledge, this is the first deep learning-based solution for\ndetecting propellers (to detect drones). Finally, our applications also show an\nimpressive success rate of 92% and 90% for the tracking and landing tasks\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 01:16:01 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Sanket", "Nitin J.", ""], ["Singh", "Chahat Deep", ""], ["Parameshwara", "Chethan M.", ""], ["Ferm\u00fcller", "Cornelia", ""], ["de Croon", "Guido C. H. E.", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2106.15058", "submitter": "Zihao Xiao", "authors": "Zihao Xiao, Xianfeng Gao, Chilin Fu, Yinpeng Dong, Wei Gao, Xiaolu\n  Zhang, Jun Zhou, Jun Zhu", "title": "Improving Transferability of Adversarial Patches on Face Recognition\n  with Generative Models", "comments": "Accpeted by CVPR 2021. Based on the camera ready version, some typos\n  are fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is greatly improved by deep convolutional neural networks\n(CNNs). Recently, these face recognition models have been used for identity\nauthentication in security sensitive applications. However, deep CNNs are\nvulnerable to adversarial patches, which are physically realizable and\nstealthy, raising new security concerns on the real-world applications of these\nmodels. In this paper, we evaluate the robustness of face recognition models\nusing adversarial patches based on transferability, where the attacker has\nlimited accessibility to the target models. First, we extend the existing\ntransfer-based attack techniques to generate transferable adversarial patches.\nHowever, we observe that the transferability is sensitive to initialization and\ndegrades when the perturbation magnitude is large, indicating the overfitting\nto the substitute models. Second, we propose to regularize the adversarial\npatches on the low dimensional data manifold. The manifold is represented by\ngenerative models pre-trained on legitimate human face images. Using face-like\nfeatures as adversarial perturbations through optimization on the manifold, we\nshow that the gaps between the responses of substitute models and the target\nmodels dramatically decrease, exhibiting a better transferability. Extensive\ndigital world experiments are conducted to demonstrate the superiority of the\nproposed method in the black-box setting. We apply the proposed method in the\nphysical world as well.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 02:13:05 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Xiao", "Zihao", ""], ["Gao", "Xianfeng", ""], ["Fu", "Chilin", ""], ["Dong", "Yinpeng", ""], ["Gao", "Wei", ""], ["Zhang", "Xiaolu", ""], ["Zhou", "Jun", ""], ["Zhu", "Jun", ""]]}, {"id": "2106.15064", "submitter": "Yh.Peng Tu", "authors": "Peng Tu, Yawen Huang, Rongrong Ji, Feng Zheng, Ling Shao", "title": "GuidedMix-Net: Learning to Improve Pseudo Masks Using Labeled Images as\n  Reference", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning is a challenging problem which aims to construct a\nmodel by learning from a limited number of labeled examples. Numerous methods\nhave been proposed to tackle this problem, with most focusing on utilizing the\npredictions of unlabeled instances consistency alone to regularize networks.\nHowever, treating labeled and unlabeled data separately often leads to the\ndiscarding of mass prior knowledge learned from the labeled examples, and\nfailure to mine the feature interaction between the labeled and unlabeled image\npairs. In this paper, we propose a novel method for semi-supervised semantic\nsegmentation named GuidedMix-Net, by leveraging labeled information to guide\nthe learning of unlabeled instances. Specifically, we first introduce a feature\nalignment objective between labeled and unlabeled data to capture potentially\nsimilar image pairs and then generate mixed inputs from them. The proposed\nmutual information transfer (MITrans), based on the cluster assumption, is\nshown to be a powerful knowledge module for further progressive refining\nfeatures of unlabeled data in the mixed data space. To take advantage of the\nlabeled examples and guide unlabeled data learning, we further propose a mask\ngeneration module to generate high-quality pseudo masks for the unlabeled data.\nAlong with supervised learning for labeled data, the prediction of unlabeled\ndata is jointly learned with the generated pseudo masks from the mixed data.\nExtensive experiments on PASCAL VOC 2012, PASCAL-Context and Cityscapes\ndemonstrate the effectiveness of our GuidedMix-Net, which achieves competitive\nsegmentation accuracy and significantly improves the mIoU by +7$\\%$ compared to\nprevious state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 02:48:45 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 05:25:21 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tu", "Peng", ""], ["Huang", "Yawen", ""], ["Ji", "Rongrong", ""], ["Zheng", "Feng", ""], ["Shao", "Ling", ""]]}, {"id": "2106.15067", "submitter": "Xiang Ye", "authors": "Xiang Ye and Zihang He and Heng Wang and Yong Li", "title": "Towards Understanding the Effectiveness of Attention Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention Mechanism is a widely used method for improving the performance of\nconvolutional neural networks (CNNs) on computer vision tasks. Despite its\npervasiveness, we have a poor understanding of what its effectiveness stems\nfrom. It is popularly believed that its effectiveness stems from the visual\nattention explanation, advocating focusing on the important part of input data\nrather than ingesting the entire input. In this paper, we find that there is\nonly a weak consistency between the attention weights of features and their\nimportance. Instead, we verify the crucial role of feature map multiplication\nin attention mechanism and uncover a fundamental impact of feature map\nmultiplication on the learned landscapes of CNNs: with the high order\nnon-linearity brought by the feature map multiplication, it played a\nregularization role on CNNs, which made them learn smoother and more stable\nlandscapes near real samples compared to vanilla CNNs. This smoothness and\nstability induce a more predictive and stable behavior in-between real samples,\nand make CNNs generate better. Moreover, motivated by the proposed\neffectiveness of feature map multiplication, we design feature map\nmultiplication network (FMMNet) by simply replacing the feature map addition in\nResNet with feature map multiplication. FMMNet outperforms ResNet on various\ndatasets, and this indicates that feature map multiplication plays a vital role\nin improving the performance even without finely designed attention mechanism\nin existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 02:58:59 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ye", "Xiang", ""], ["He", "Zihang", ""], ["Wang", "Heng", ""], ["Li", "Yong", ""]]}, {"id": "2106.15069", "submitter": "Leyuan Wang", "authors": "Leyuan Wang, Kunbo Zhang, Yunlong Wang, Zhenan Sun", "title": "An End-to-End Autofocus Camera for Iris on the Move", "comments": "8 pages, 7 figures, International Joint Conference on Biometrics 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For distant iris recognition, a long focal length lens is generally used to\nensure the resolution ofiris images, which reduces the depth of field and leads\nto potential defocus blur. To accommodate users at different distances, it is\nnecessary to control focus quickly and accurately. While for users in motion,\nit is expected to maintain the correct focus on the iris area continuously. In\nthis paper, we introduced a novel rapid autofocus camera for active refocusing\nofthe iris area ofthe moving objects using a focus-tunable lens. Our end-to-end\ncomputational algorithm can predict the best focus position from one single\nblurred image and generate a lens diopter control signal automatically. This\nscene-based active manipulation method enables real-time focus tracking of the\niris area ofa moving object. We built a testing bench to collect real-world\nfocal stacks for evaluation of the autofocus methods. Our camera has reached an\nautofocus speed ofover 50 fps. The results demonstrate the advantages of our\nproposed camera for biometric perception in static and dynamic scenes. The code\nis available at https://github.com/Debatrix/AquulaCam.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 03:00:39 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Leyuan", ""], ["Zhang", "Kunbo", ""], ["Wang", "Yunlong", ""], ["Sun", "Zhenan", ""]]}, {"id": "2106.15083", "submitter": "Peter Kulits", "authors": "Peter Kulits and Jake Wall and Anka Bedetti and Michelle Henley and\n  Sara Beery", "title": "ElephantBook: A Semi-Automated Human-in-the-Loop System for Elephant\n  Re-Identification", "comments": null, "journal-ref": null, "doi": "10.1145/3460112.3471947", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  African elephants are vital to their ecosystems, but their populations are\nthreatened by a rise in human-elephant conflict and poaching. Monitoring\npopulation dynamics is essential in conservation efforts; however, tracking\nelephants is a difficult task, usually relying on the invasive and sometimes\ndangerous placement of GPS collars. Although there have been many recent\nsuccesses in the use of computer vision techniques for automated identification\nof other species, identification of elephants is extremely difficult and\ntypically requires expertise as well as familiarity with elephants in the\npopulation. We have built and deployed a web-based platform and database for\nhuman-in-the-loop re-identification of elephants combining manual attribute\nlabeling and state-of-the-art computer vision algorithms, known as\nElephantBook. Our system is currently in use at the Mara Elephant Project,\nhelping monitor the protected and at-risk population of elephants in the\nGreater Maasai Mara ecosystem. ElephantBook makes elephant re-identification\nusable by non-experts and scalable for use by multiple conservation NGOs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 04:18:22 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 01:46:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kulits", "Peter", ""], ["Wall", "Jake", ""], ["Bedetti", "Anka", ""], ["Henley", "Michelle", ""], ["Beery", "Sara", ""]]}, {"id": "2106.15087", "submitter": "Kaichun Mo", "authors": "Kaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, Leonidas Guibas", "title": "O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to the vast literature in modeling, perceiving, and understanding\nagent-object (e.g., human-object, hand-object, robot-object) interaction in\ncomputer vision and robotics, very few past works have studied the task of\nobject-object interaction, which also plays an important role in robotic\nmanipulation and planning tasks. There is a rich space of object-object\ninteraction scenarios in our daily life, such as placing an object on a messy\ntabletop, fitting an object inside a drawer, pushing an object using a tool,\netc. In this paper, we propose a unified affordance learning framework to learn\nobject-object interaction for various tasks. By constructing four object-object\ninteraction task environments using physical simulation (SAPIEN) and thousands\nof ShapeNet models with rich geometric diversity, we are able to conduct\nlarge-scale object-object affordance learning without the need for human\nannotations or demonstrations. At the core of technical contribution, we\npropose an object-kernel point convolution network to reason about detailed\ninteraction between two objects. Experiments on large-scale synthetic data and\nreal-world data prove the effectiveness of the proposed approach. Please refer\nto the project webpage for code, data, video, and more materials:\nhttps://cs.stanford.edu/~kaichun/o2oafford\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 04:38:12 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Mo", "Kaichun", ""], ["Qin", "Yuzhe", ""], ["Xiang", "Fanbo", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2106.15097", "submitter": "Qing Wu", "authors": "Qing Wu, Yuwei Li, Lan Xu, Ruiming Feng, Hongjiang Wei, Qing Yang,\n  Boliang Yu, Xiaozhao Liu, Jingyi Yu, and Yuyao Zhang", "title": "IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via\n  Implicit Neural Representation", "comments": "8 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For collecting high-quality high-resolution (HR) MR image, we propose a novel\nimage reconstruction network named IREM, which is trained on multiple\nlow-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR\nimage reconstruction. In this work, we suppose the desired HR image as an\nimplicit continuous function of the 3D image spatial coordinate and the\nthick-slice LR images as several sparse discrete samplings of this function.\nThen the super-resolution (SR) task is to learn the continuous volumetric\nfunction from a limited observations using an fully-connected neural network\ncombined with Fourier feature positional encoding. By simply minimizing the\nerror between the network prediction and the acquired LR image intensity across\neach imaging plane, IREM is trained to represent a continuous model of the\nobserved tissue anatomy. Experimental results indicate that IREM succeeds in\nrepresenting high frequency image feature, and in real scene data collection,\nIREM reduces scan time and achieves high-quality high-resolution MR imaging in\nterms of SNR and local image detail.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 05:25:43 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wu", "Qing", ""], ["Li", "Yuwei", ""], ["Xu", "Lan", ""], ["Feng", "Ruiming", ""], ["Wei", "Hongjiang", ""], ["Yang", "Qing", ""], ["Yu", "Boliang", ""], ["Liu", "Xiaozhao", ""], ["Yu", "Jingyi", ""], ["Zhang", "Yuyao", ""]]}, {"id": "2106.15113", "submitter": "Ziquan Wei", "authors": "Ziquan Wei, Shenghua Cheng, Xiuli Liu, Shaoqun Zeng", "title": "An Efficient Cervical Whole Slide Image Analysis Framework Based on\n  Multi-scale Semantic and Spatial Deep Features", "comments": "16 pages, 8 figures, already submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital gigapixel whole slide image (WSI) is widely used in clinical\ndiagnosis, and automated WSI analysis is key for computer-aided diagnosis.\nCurrently, analyzing the integrated descriptor of probabilities or feature maps\nfrom massive local patches encoded by ResNet classifier is the main manner for\nWSI-level prediction. Feature representations of the sparse and tiny lesion\ncells in cervical slides, however, are still challengeable for the\nunder-promoted upstream encoders, while the unused spatial representations of\ncervical cells are the available features to supply the semantics analysis. As\nwell as patches sampling with overlap and repetitive processing incur the\ninefficiency and the unpredictable side effect. This study designs a novel\ninline connection network (InCNet) by enriching the multi-scale connectivity to\nbuild the lightweight model named You Only Look Cytopathology Once (YOLCO) with\nthe additional supervision of spatial information. The proposed model allows\nthe input size enlarged to megapixel that can stitch the WSI without any\noverlap by the average repeats decreased from $10^3\\sim10^4$ to $10^1\\sim10^2$\nfor collecting features and predictions at two scales. Based on Transformer for\nclassifying the integrated multi-scale multi-task features, the experimental\nresults appear $0.872$ AUC score better and $2.51\\times$ faster than the best\nconventional method in WSI classification on multicohort datasets of 2,019\nslides from four scanning devices.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 06:24:55 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 02:48:15 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Wei", "Ziquan", ""], ["Cheng", "Shenghua", ""], ["Liu", "Xiuli", ""], ["Zeng", "Shaoqun", ""]]}, {"id": "2106.15117", "submitter": "Son Nguyen", "authors": "Son Nguyen Truong", "title": "SDL: New data generation tools for full-level annotated document layout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel data generation tool for document processing. The tool\nfocuses on providing a maximal level of visual information in a normal type\ndocument, ranging from character position to paragraph-level position. It also\nenables working with a large dataset on low-resource languages as well as\nproviding a mean of processing thorough full-level information of the\ndocumented text. The data generation tools come with a dataset of 320000\nVietnamese synthetic document images and an instruction to generate a dataset\nof similar size in other languages. The repository can be found at:\nhttps://github.com/tson1997/SDL-Document-Image-Generation\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 06:32:31 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Truong", "Son Nguyen", ""]]}, {"id": "2106.15121", "submitter": "Xingqun Qi", "authors": "Xingqun Qi, Muyi Sun, Weining Wang, Xiaoxiao Dong, Qi Li, Caifeng Shan", "title": "Face Sketch Synthesis via Semantic-Driven Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face sketch synthesis has made significant progress with the development of\ndeep neural networks in these years. The delicate depiction of sketch portraits\nfacilitates a wide range of applications like digital entertainment and law\nenforcement. However, accurate and realistic face sketch generation is still a\nchallenging task due to the illumination variations and complex backgrounds in\nthe real scenes. To tackle these challenges, we propose a novel Semantic-Driven\nGenerative Adversarial Network (SDGAN) which embeds global structure-level\nstyle injection and local class-level knowledge re-weighting. Specifically, we\nconduct facial saliency detection on the input face photos to provide overall\nfacial texture structure, which could be used as a global type of prior\ninformation. In addition, we exploit face parsing layouts as the semantic-level\nspatial prior to enforce globally structural style injection in the generator\nof SDGAN. Furthermore, to enhance the realistic effect of the details, we\npropose a novel Adaptive Re-weighting Loss (ARLoss) which dedicates to balance\nthe contributions of different semantic classes. Experimentally, our extensive\nexperiments on CUFS and CUFSF datasets show that our proposed algorithm\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 07:03:56 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Qi", "Xingqun", ""], ["Sun", "Muyi", ""], ["Wang", "Weining", ""], ["Dong", "Xiaoxiao", ""], ["Li", "Qi", ""], ["Shan", "Caifeng", ""]]}, {"id": "2106.15125", "submitter": "Yi-Fan Song", "authors": "Yi-Fan Song, Zhang Zhang, Caifeng Shan, Liang Wang", "title": "Constructing Stronger and Faster Baselines for Skeleton-based Action\n  Recognition", "comments": "15 pages, 12 tables, 10 figures, submitted to IEEE T-PAMI. arXiv\n  admin note: text overlap with arXiv:2010.09978", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One essential problem in skeleton-based action recognition is how to extract\ndiscriminative features over all skeleton joints. However, the complexity of\nthe recent State-Of-The-Art (SOTA) models for this task tends to be exceedingly\nsophisticated and over-parameterized. The low efficiency in model training and\ninference has increased the validation costs of model architectures in\nlarge-scale datasets. To address the above issue, recent advanced separable\nconvolutional layers are embedded into an early fused Multiple Input Branches\n(MIB) network, constructing an efficient Graph Convolutional Network (GCN)\nbaseline for skeleton-based action recognition. In addition, based on such the\nbaseline, we design a compound scaling strategy to expand the model's width and\ndepth synchronously, and eventually obtain a family of efficient GCN baselines\nwith high accuracies and small amounts of trainable parameters, termed\nEfficientGCN-Bx, where ''x'' denotes the scaling coefficient. On two\nlarge-scale datasets, i.e., NTU RGB+D 60 and 120, the proposed EfficientGCN-B4\nbaseline outperforms other SOTA methods, e.g., achieving 91.7% accuracy on the\ncross-subject benchmark of NTU 60 dataset, while being 3.15x smaller and 3.21x\nfaster than MS-G3D, which is one of the best SOTA methods. The source code in\nPyTorch version and the pretrained models are available at\nhttps://github.com/yfsong0709/EfficientGCNv1.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 07:09:11 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Song", "Yi-Fan", ""], ["Zhang", "Zhang", ""], ["Shan", "Caifeng", ""], ["Wang", "Liang", ""]]}, {"id": "2106.15130", "submitter": "Ehsan Nowroozi", "authors": "Mauro Conti, Simone Milani, Ehsan Nowroozi, Gabriele Orazi", "title": "Do Not Deceive Your Employer with a Virtual Background: A Video\n  Conferencing Manipulation-Detection System", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The last-generation video conferencing software allows users to utilize a\nvirtual background to conceal their personal environment due to privacy\nconcerns, especially in official meetings with other employers. On the other\nhand, users maybe want to fool people in the meeting by considering the virtual\nbackground to conceal where they are. In this case, developing tools to\nunderstand the virtual background utilize for fooling people in meeting plays\nan important role. Besides, such detectors must prove robust against different\nkinds of attacks since a malicious user can fool the detector by applying a set\nof adversarial editing steps on the video to conceal any revealing footprint.\nIn this paper, we study the feasibility of an efficient tool to detect whether\na videoconferencing user background is real. In particular, we provide the\nfirst tool which computes pixel co-occurrences matrices and uses them to search\nfor inconsistencies among spectral and spatial bands. Our experiments confirm\nthat cross co-occurrences matrices improve the robustness of the detector\nagainst different kinds of attacks. This work's performance is especially\nnoteworthy with regard to color SPAM features. Moreover, the performance\nespecially is significant with regard to robustness versus post-processing,\nlike geometric transformations, filtering, contrast enhancement, and JPEG\ncompression with different quality factors.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 07:31:21 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Conti", "Mauro", ""], ["Milani", "Simone", ""], ["Nowroozi", "Ehsan", ""], ["Orazi", "Gabriele", ""]]}, {"id": "2106.15171", "submitter": "David Varas", "authors": "Manuel Sarmiento Calder\\'o, David Varas, Elisenda Bou-Balust", "title": "Spatio-Temporal Context for Action Detection", "comments": "Computer Vision and Pattern Recognition Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research in action detection has grown in the recentyears, as it plays a key\nrole in video understanding. Modelling the interactions (either spatial or\ntemporal) between actors and their context has proven to be essential for this\ntask. While recent works use spatial features with aggregated temporal\ninformation, this work proposes to use non-aggregated temporal information.\nThis is done by adding an attention based method that leverages spatio-temporal\ninteractions between elements in the scene along the clip.The main contribution\nof this work is the introduction of two cross attention blocks to effectively\nmodel the spatial relations and capture short range temporal\ninteractions.Experiments on the AVA dataset show the advantages of the proposed\napproach that models spatio-temporal relations between relevant elements in the\nscene, outperforming other methods that model actor interactions with their\ncontext by +0.31 mAP.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 08:33:48 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Calder\u00f3", "Manuel Sarmiento", ""], ["Varas", "David", ""], ["Bou-Balust", "Elisenda", ""]]}, {"id": "2106.15176", "submitter": "Rita Pucci", "authors": "Rita Pucci, Niki Martinel", "title": "TUCaN: Progressively Teaching Colourisation to Capsules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image colourisation is the computer vision research path that\nstudies how to colourise greyscale images (for restoration). Deep learning\ntechniques improved image colourisation yielding astonishing results. These\ndiffer by various factors, such as structural differences, input types, user\nassistance, etc. Most of them, base the architectural structure on\nconvolutional layers with no emphasis on layers specialised in object features\nextraction. We introduce a novel downsampling upsampling architecture named\nTUCaN (Tiny UCapsNet) that exploits the collaboration of convolutional layers\nand capsule layers to obtain a neat colourisation of entities present in every\nsingle image. This is obtained by enforcing collaboration among such layers by\nskip and residual connections. We pose the problem as a per pixel colour\nclassification task that identifies colours as a bin in a quantized space. To\ntrain the network, in contrast with the standard end to end learning method, we\npropose the progressive learning scheme to extract the context of objects by\nonly manipulating the learning process without changing the model. In this\nscheme, the upsampling starts from the reconstruction of low resolution images\nand progressively grows to high resolution images throughout the training\nphase. Experimental results on three benchmark datasets show that our approach\nwith ImageNet10k dataset outperforms existing methods on standard quality\nmetrics and achieves state of the art performances on image colourisation. We\nperformed a user study to quantify the perceptual realism of the colourisation\nresults demonstrating: that progressive learning let the TUCaN achieve better\ncolours than the end to end scheme; and pointing out the limitations of the\nexisting evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 08:44:15 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Pucci", "Rita", ""], ["Martinel", "Niki", ""]]}, {"id": "2106.15179", "submitter": "Hendrik Richter", "authors": "Hendrik Richter", "title": "Wrong Colored Vermeer: Color-Symmetric Image Distortion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Color symmetry implies that the colors of geometrical objects are assigned\naccording to their symmetry properties. It is defined by associating the\nelements of the symmetry group with a color permutation. I use this concept for\ngenerative art and apply symmetry-consistent color distortions to images of\npaintings by Johannes Vermeer. The color permutations are realized as mappings\nof the HSV color space onto itself.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 08:51:23 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Richter", "Hendrik", ""]]}, {"id": "2106.15183", "submitter": "Arian Bakhtiarnia", "authors": "Arian Bakhtiarnia, Qi Zhang and Alexandros Iosifidis", "title": "Multi-Exit Vision Transformer for Dynamic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks can be converted to multi-exit architectures by\ninserting early exit branches after some of their intermediate layers. This\nallows their inference process to become dynamic, which is useful for time\ncritical IoT applications with stringent latency requirements, but with\ntime-variant communication and computation resources. In particular, in edge\ncomputing systems and IoT networks where the exact computation time budget is\nvariable and not known beforehand. Vision Transformer is a recently proposed\narchitecture which has since found many applications across various domains of\ncomputer vision. In this work, we propose seven different architectures for\nearly exit branches that can be used for dynamic inference in Vision\nTransformer backbones. Through extensive experiments involving both\nclassification and regression problems, we show that each one of our proposed\narchitectures could prove useful in the trade-off between accuracy and speed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 09:01:13 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 07:45:39 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bakhtiarnia", "Arian", ""], ["Zhang", "Qi", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2106.15202", "submitter": "Tao Bai", "authors": "Tao Bai, Jinqi Luo, Jun Zhao", "title": "Inconspicuous Adversarial Patches for Fooling Image Recognition Systems\n  on Mobile Devices", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.09774", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based image recognition systems have been widely deployed on\nmobile devices in today's world. In recent studies, however, deep learning\nmodels are shown vulnerable to adversarial examples. One variant of adversarial\nexamples, called adversarial patch, draws researchers' attention due to its\nstrong attack abilities. Though adversarial patches achieve high attack success\nrates, they are easily being detected because of the visual inconsistency\nbetween the patches and the original images. Besides, it usually requires a\nlarge amount of data for adversarial patch generation in the literature, which\nis computationally expensive and time-consuming. To tackle these challenges, we\npropose an approach to generate inconspicuous adversarial patches with one\nsingle image. In our approach, we first decide the patch locations basing on\nthe perceptual sensitivity of victim models, then produce adversarial patches\nin a coarse-to-fine way by utilizing multiple-scale generators and\ndiscriminators. The patches are encouraged to be consistent with the background\nimages with adversarial training while preserving strong attack abilities. Our\napproach shows the strong attack abilities in white-box settings and the\nexcellent transferability in black-box settings through extensive experiments\non various models with different architectures and training methods. Compared\nto other adversarial patches, our adversarial patches hold the most negligible\nrisks to be detected and can evade human observations, which is supported by\nthe illustrations of saliency maps and results of user evaluations. Lastly, we\nshow that our adversarial patches can be applied in the physical world.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 09:39:34 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Bai", "Tao", ""], ["Luo", "Jinqi", ""], ["Zhao", "Jun", ""]]}, {"id": "2106.15206", "submitter": "Kaiwen Yang", "authors": "Kaiwen Yang and Xinmei Tian", "title": "Domain-Class Correlation Decomposition for Generalizable Person\n  Re-Identification", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization in person re-identification is a highly important\nmeaningful and practical task in which a model trained with data from several\nsource domains is expected to generalize well to unseen target domains. Domain\nadversarial learning is a promising domain generalization method that aims to\nremove domain information in the latent representation through adversarial\ntraining. However, in person re-identification, the domain and class are\ncorrelated, and we theoretically show that domain adversarial learning will\nlose certain information about class due to this domain-class correlation.\nInspired by casual inference, we propose to perform interventions to the domain\nfactor $d$, aiming to decompose the domain-class correlation. To achieve this\ngoal, we proposed estimating the resulting representation $z^{*}$ caused by the\nintervention through first- and second-order statistical characteristic\nmatching. Specifically, we build a memory bank to restore the statistical\ncharacteristics of each domain. Then, we use the newly generated samples\n$\\{z^{*},y,d^{*}\\}$ to compute the loss function. These samples are\ndomain-class correlation decomposed; thus, we can learn a domain-invariant\nrepresentation that can capture more class-related features. Extensive\nexperiments show that our model outperforms the state-of-the-art methods on the\nlarge-scale domain generalization Re-ID benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 09:45:03 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yang", "Kaiwen", ""], ["Tian", "Xinmei", ""]]}, {"id": "2106.15232", "submitter": "Kaigen Tsuji", "authors": "Kaigen Tsuji, Seiichi Uchida, Brian Kenji Iwana", "title": "Using Robust Regression to Find Font Usage Trends", "comments": "16 pages with 10 figures. Accepted at ICDAR 2021 Workshop on Machine\n  Learning(ICDAR-WML2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fonts have had trends throughout their history, not only in when they were\ninvented but also in their usage and popularity. In this paper, we attempt to\nspecifically find the trends in font usage using robust regression on a large\ncollection of text images. We utilize movie posters as the source of fonts for\nthis task because movie posters can represent time periods by using their\nrelease date. In addition, movie posters are documents that are carefully\ndesigned and represent a wide range of fonts. To understand the relationship\nbetween the fonts of movie posters and time, we use a regression Convolutional\nNeural Network (CNN) to estimate the release year of a movie using an isolated\ntitle text image. Due to the difficulty of the task, we propose to use of a\nhybrid training regimen that uses a combination of Mean Squared Error (MSE) and\nTukey's biweight loss. Furthermore, we perform a thorough analysis on the\ntrends of fonts through time.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 10:29:00 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 09:21:50 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 13:01:44 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Tsuji", "Kaigen", ""], ["Uchida", "Seiichi", ""], ["Iwana", "Brian Kenji", ""]]}, {"id": "2106.15252", "submitter": "Kai Han", "authors": "Kai Han and Sylvestre-Alvise Rebuffi and S\\'ebastien Ehrhardt and\n  Andrea Vedaldi and Andrew Zisserman", "title": "AutoNovel: Automatically Discovering and Learning Novel Visual\n  Categories", "comments": "TPAMI 2021, code:\n  http://www.robots.ox.ac.uk/~vgg/research/auto_novel/. arXiv admin note:\n  substantial text overlap with arXiv:2002.05714", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3091944", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of discovering novel classes in an image collection\ngiven labelled examples of other classes. We present a new approach called\nAutoNovel to address this problem by combining three ideas: (1) we suggest that\nthe common approach of bootstrapping an image representation using the labelled\ndata only introduces an unwanted bias, and that this can be avoided by using\nself-supervised learning to train the representation from scratch on the union\nof labelled and unlabelled data; (2) we use ranking statistics to transfer the\nmodel's knowledge of the labelled classes to the problem of clustering the\nunlabelled images; and, (3) we train the data representation by optimizing a\njoint objective function on the labelled and unlabelled subsets of the data,\nimproving both the supervised classification of the labelled data, and the\nclustering of the unlabelled data. Moreover, we propose a method to estimate\nthe number of classes for the case where the number of new categories is not\nknown a priori. We evaluate AutoNovel on standard classification benchmarks and\nsubstantially outperform current methods for novel category discovery. In\naddition, we also show that AutoNovel can be used for fully unsupervised image\nclustering, achieving promising results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 11:12:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Han", "Kai", ""], ["Rebuffi", "Sylvestre-Alvise", ""], ["Ehrhardt", "S\u00e9bastien", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2106.15257", "submitter": "Mohammad  Amin Kashi", "authors": "Mohammad Amin Kashi", "title": "Predicting Depth from Semantic Segmentation using Game Engine Dataset", "comments": "79 pages, Master's thesis at K. N. Toosi University of Technology,\n  supervised by Professor Hamid D. Taghirad", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth perception is fundamental for robots to understand the surrounding\nenvironment. As the view of cognitive neuroscience, visual depth perception\nmethods are divided into three categories, namely binocular, active, and\npictorial. The first two categories have been studied for decades in detail.\nHowever, research for the exploration of the third category is still in its\ninfancy and has got momentum by the advent of deep learning methods in recent\nyears. In cognitive neuroscience, it is known that pictorial depth perception\nmechanisms are dependent on the perception of seen objects. Inspired by this\nfact, in this thesis, we investigated the relation of perception of objects and\ndepth estimation convolutional neural networks. For this purpose, we developed\nnew network structures based on a simple depth estimation network that only\nused a single image at its input. Our proposed structures use both an image and\na semantic label of the image as their input. We used semantic labels as the\noutput of object perception. The obtained results of performance comparison\nbetween the developed network and original network showed that our novel\nstructures can improve the performance of depth estimation by 52\\% of relative\nerror of distance in the examined cases. Most of the experimental studies were\ncarried out on synthetic datasets that were generated by game engines to\nisolate the performance comparison from the effect of inaccurate depth and\nsemantic labels of non-synthetic datasets. It is shown that particular\nsynthetic datasets may be used for training of depth networks in cases that an\nappropriate dataset is not available. Furthermore, we showed that in these\ncases, usage of semantic labels improves the robustness of the network against\ndomain shift from synthetic training data to non-synthetic test data.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 10:15:40 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kashi", "Mohammad Amin", ""]]}, {"id": "2106.15258", "submitter": "Can Zhang", "authors": "Ranyu Ning, Can Zhang, Yuexian Zou", "title": "SRF-Net: Selective Receptive Field Network for Anchor-Free Temporal\n  Action Detection", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action detection (TAD) is a challenging task which aims to\ntemporally localize and recognize the human action in untrimmed videos. Current\nmainstream one-stage TAD approaches localize and classify action proposals\nrelying on pre-defined anchors, where the location and scale for action\ninstances are set by designers. Obviously, such an anchor-based TAD method\nlimits its generalization capability and will lead to performance degradation\nwhen videos contain rich action variation. In this study, we explore to remove\nthe requirement of pre-defined anchors for TAD methods. A novel TAD model\ntermed as Selective Receptive Field Network (SRF-Net) is developed, in which\nthe location offsets and classification scores at each temporal location can be\ndirectly estimated in the feature map and SRF-Net is trained in an end-to-end\nmanner. Innovatively, a building block called Selective Receptive Field\nConvolution (SRFC) is dedicatedly designed which is able to adaptively adjust\nits receptive field size according to multiple scales of input information at\neach temporal location in the feature map. Extensive experiments are conducted\non the THUMOS14 dataset, and superior results are reported comparing to\nstate-of-the-art TAD approaches.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 11:29:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ning", "Ranyu", ""], ["Zhang", "Can", ""], ["Zou", "Yuexian", ""]]}, {"id": "2106.15268", "submitter": "Daniel Soares", "authors": "Daniel de Barros Soares (1), Fran\\c{c}ois Andrieux (1), Bastien Hell\n  (1), Julien Lenhardt (1 and 2), Jordi Badosa (3), Sylvain Gavoille (1),\n  St\\'ephane Gaiffas (1, 4 and 5), Emmanuel Bacry (1 and 6), ((1) namR, Paris,\n  France, (2) ENSTA Paris, France, (3) LMD, Ecole polytechnique, IP Paris,\n  Palaiseau, France, (4) LPSM, Universit\\'e de Paris, France, (5) DMA, Ecole\n  normale sup\\'erieure, Paris, France, (6) CEREMADE, Universit\\'e Paris\n  Dauphine, Paris, France)", "title": "Predicting the Solar Potential of Rooftops using Image Segmentation and\n  Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the amount of electricity that can be produced by rooftop\nphotovoltaic systems is a time-consuming process that requires on-site\nmeasurements, a difficult task to achieve on a large scale. In this paper, we\npresent an approach to estimate the solar potential of rooftops based on their\nlocation and architectural characteristics, as well as the amount of solar\nradiation they receive annually. Our technique uses computer vision to achieve\nsemantic segmentation of roof sections and roof objects on the one hand, and a\nmachine learning model based on structured building features to predict roof\npitch on the other hand. We then compute the azimuth and maximum number of\nsolar panels that can be installed on a rooftop with geometric approaches.\nFinally, we compute precise shading masks and combine them with solar\nirradiation data that enables us to estimate the yearly solar potential of a\nrooftop.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 15:49:13 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Soares", "Daniel de Barros", "", "1 and 2"], ["Andrieux", "Fran\u00e7ois", "", "1 and 2"], ["Hell", "Bastien", "", "1 and 2"], ["Lenhardt", "Julien", "", "1 and 2"], ["Badosa", "Jordi", "", "1, 4 and 5"], ["Gavoille", "Sylvain", "", "1, 4 and 5"], ["Gaiffas", "St\u00e9phane", "", "1, 4 and 5"], ["Bacry", "Emmanuel", "", "1 and 6"]]}, {"id": "2106.15274", "submitter": "Namig Aliyev", "authors": "Namig Aliyev, Oguzhan Sezer, Mehmet Turan Guzel", "title": "Autonomous Driving Implementation in an Experimental Environment", "comments": "8 pages, 21 figures.This is a bachelor's thesis research report and\n  was supported by the Scientific and Technological Research Council of Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous systems require identifying the environment and it has a long way\nto go before putting it safely into practice. In autonomous driving systems,\nthe detection of obstacles and traffic lights are of importance as well as lane\ntracking. In this study, an autonomous driving system is developed and tested\nin the experimental environment designed for this purpose. In this system, a\nmodel vehicle having a camera is used to trace the lanes and avoid obstacles to\nexperimentally study autonomous driving behavior. Convolutional Neural Network\nmodels were trained for Lane tracking. For the vehicle to avoid obstacles,\ncorner detection, optical flow, focus of expansion, time to collision, balance\ncalculation, and decision mechanism were created, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 11:14:09 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Aliyev", "Namig", ""], ["Sezer", "Oguzhan", ""], ["Guzel", "Mehmet Turan", ""]]}, {"id": "2106.15277", "submitter": "Mingkui Tan", "authors": "Zhuangwei Zhuang, Rong Li, Yuanqing Li, Kui Jia, Qicheng Wang, Mingkui\n  Tan", "title": "Perception-aware Multi-sensor Fusion for 3D LiDAR Semantic Segmentation", "comments": "11 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D LiDAR (light detection and ranging) based semantic segmentation is\nimportant in scene understanding for many applications, such as auto-driving\nand robotics. For example, for autonomous cars equipped with RGB cameras and\nLiDAR, it is crucial to fuse complementary information from different sensors\nfor robust and accurate segmentation. Existing fusion-based methods, however,\nmay not achieve promising performance due to the vast difference between two\nmodalities. In this work, we investigate a collaborative fusion scheme called\nperception-aware multi-sensor fusion (PMF) to exploit perceptual information\nfrom two modalities, namely, appearance information from RGB images and\nspatio-depth information from point clouds. To this end, we first project point\nclouds to the camera coordinates to provide spatio-depth information for RGB\nimages. Then, we propose a two-stream network to extract features from the two\nmodalities, separately, and fuse the features by effective residual-based\nfusion modules. Moreover, we propose additional perception-aware losses to\nmeasure the great perceptual difference between the two modalities. Extensive\nexperiments on two benchmark data sets show the superiority of our method. For\nexample, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8%\nin mIoU.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 10:47:26 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhuang", "Zhuangwei", ""], ["Li", "Rong", ""], ["Li", "Yuanqing", ""], ["Jia", "Kui", ""], ["Wang", "Qicheng", ""], ["Tan", "Mingkui", ""]]}, {"id": "2106.15278", "submitter": "Geeho Kim", "authors": "Geeho Kim and Bohyung Han", "title": "Open-Set Representation Learning through Combinatorial Embedding", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition tasks are often limited to dealing with a small subset of\nclasses simply because the labels for the remaining classes are unavailable. We\nare interested in identifying novel concepts in a dataset through\nrepresentation learning based on the examples in both labeled and unlabeled\nclasses, and extending the horizon of recognition to both known and novel\nclasses. To address this challenging task, we propose a combinatorial learning\napproach, which naturally clusters the examples in unseen classes using the\ncompositional knowledge given by multiple supervised meta-classifiers on\nheterogeneous label spaces. We also introduce a metric learning strategy to\nestimate pairwise pseudo-labels for improving representations of unlabeled\nexamples, which preserves semantic relations across known and novel classes\neffectively. The proposed algorithm discovers novel concepts via a joint\noptimization of enhancing the discrimitiveness of unseen classes as well as\nlearning the representations of known classes generalizable to novel ones. Our\nextensive experiments demonstrate remarkable performance gains by the proposed\napproach in multiple image retrieval and novel class discovery benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 11:51:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kim", "Geeho", ""], ["Han", "Bohyung", ""]]}, {"id": "2106.15280", "submitter": "Yiqin Zhao", "authors": "Yiqin Zhao and Tian Guo", "title": "Xihe: A 3D Vision-based Lighting Estimation Framework for Mobile\n  Augmented Reality", "comments": null, "journal-ref": null, "doi": "10.1145/3458864.3467886", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional lighting provides the foundation for achieving\nspatially-variant photorealistic 3D rendering, a desirable property for mobile\naugmented reality applications. However, in practice, estimating\nomnidirectional lighting can be challenging due to limitations such as partial\npanoramas of the rendering positions, and the inherent environment lighting and\nmobile user dynamics. A new opportunity arises recently with the advancements\nin mobile 3D vision, including built-in high-accuracy depth sensors and deep\nlearning-powered algorithms, which provide the means to better sense and\nunderstand the physical surroundings. Centering the key idea of 3D vision, in\nthis work, we design an edge-assisted framework called Xihe to provide mobile\nAR applications the ability to obtain accurate omnidirectional lighting\nestimation in real time. Specifically, we develop a novel sampling technique\nthat efficiently compresses the raw point cloud input generated at the mobile\ndevice. This technique is derived based on our empirical analysis of a recent\n3D indoor dataset and plays a key role in our 3D vision-based lighting\nestimator pipeline design. To achieve the real-time goal, we develop a tailored\nGPU pipeline for on-device point cloud processing and use an encoding technique\nthat reduces network transmitted bytes. Finally, we present an adaptive\ntriggering strategy that allows Xihe to skip unnecessary lighting estimations\nand a practical way to provide temporal coherent rendering integration with the\nmobile AR ecosystem. We evaluate both the lighting estimation accuracy and time\nof Xihe using a reference mobile application developed with Xihe's APIs. Our\nresults show that Xihe takes as fast as 20.67ms per lighting estimation and\nachieves 9.4% better estimation accuracy than a state-of-the-art neural\nnetwork.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:48:29 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhao", "Yiqin", ""], ["Guo", "Tian", ""]]}, {"id": "2106.15281", "submitter": "Alessandro Sebastianelli", "authors": "Maria Pia Del Rosso, Alessandro Sebastianelli, Dario Spiller, Pierre\n  Philippe Mathieu and Silvia Liberata Ullo", "title": "On Board Volcanic Eruption Detection through CNNs and Satellite\n  Multispectral Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the growth of Machine Learning (ML) algorithms has raised\nthe number of studies including their applicability in a variety of different\nscenarios. Among all, one of the hardest ones is the aerospace, due to its\npeculiar physical requirements. In this context, a feasibility study and a\nfirst prototype for an Artificial Intelligence (AI) model to be deployed on\nboard satellites are presented in this work. As a case study, the detection of\nvolcanic eruptions has been investigated as a method to swiftly produce alerts\nand allow immediate interventions. Two Convolutional Neural Networks (CNNs)\nhave been proposed and designed, showing how to efficiently implement them for\nidentifying the eruptions and at the same time adapting their complexity in\norder to fit on board requirements.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 11:52:43 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 10:20:57 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Del Rosso", "Maria Pia", ""], ["Sebastianelli", "Alessandro", ""], ["Spiller", "Dario", ""], ["Mathieu", "Pierre Philippe", ""], ["Ullo", "Silvia Liberata", ""]]}, {"id": "2106.15282", "submitter": "Jonathan Ho", "authors": "Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad\n  Norouzi, Tim Salimans", "title": "Cascaded Diffusion Models for High Fidelity Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation challenge, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep, and\nclassification accuracy scores of 63.02% (top-1) and 84.06% (top-5) at 256x256,\noutperforming VQ-VAE-2.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 17:14:52 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 19:43:38 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ho", "Jonathan", ""], ["Saharia", "Chitwan", ""], ["Chan", "William", ""], ["Fleet", "David J.", ""], ["Norouzi", "Mohammad", ""], ["Salimans", "Tim", ""]]}, {"id": "2106.15283", "submitter": "Chenglin Li", "authors": "Chenglin Li, Carrie Lu Tong, Di Niu, Bei Jiang, Xiao Zuo, Lei Cheng,\n  Jian Xiong and Jianming Yang", "title": "Similarity Embedding Networks for Robust Human Activity Recognition", "comments": null, "journal-ref": null, "doi": "10.1145/3448021", "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning models for human activity recognition (HAR) based on sensor\ndata have been heavily studied recently. However, the generalization ability of\ndeep models on complex real-world HAR data is limited by the availability of\nhigh-quality labeled activity data, which are hard to obtain. In this paper, we\ndesign a similarity embedding neural network that maps input sensor signals\nonto real vectors through carefully designed convolutional and LSTM layers. The\nembedding network is trained with a pairwise similarity loss, encouraging the\nclustering of samples from the same class in the embedded real space, and can\nbe effectively trained on a small dataset and even on a noisy dataset with\nmislabeled samples. Based on the learned embeddings, we further propose both\nnonparametric and parametric approaches for activity recognition. Extensive\nevaluation based on two public datasets has shown that the proposed similarity\nembedding network significantly outperforms state-of-the-art deep models on HAR\nclassification tasks, is robust to mislabeled samples in the training set, and\ncan also be used to effectively denoise a noisy dataset.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:52:32 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Li", "Chenglin", ""], ["Tong", "Carrie Lu", ""], ["Niu", "Di", ""], ["Jiang", "Bei", ""], ["Zuo", "Xiao", ""], ["Cheng", "Lei", ""], ["Xiong", "Jian", ""], ["Yang", "Jianming", ""]]}, {"id": "2106.15286", "submitter": "Lucas Nedel Kirsten Sr.", "authors": "Lucas N. Kirsten, Ricardo Piccoli and Ricardo Ribani", "title": "Evaluating Deep Neural Networks for Image Document Enhancement", "comments": "12 pages, 6 figures, 2 tables, CBDAR conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work evaluates six state-of-the-art deep neural network (DNN)\narchitectures applied to the problem of enhancing camera-captured document\nimages. The results from each network were evaluated both qualitatively and\nquantitatively using Image Quality Assessment (IQA) metrics, and also compared\nwith an existing approach based on traditional computer vision techniques. The\nbest performing architectures generally produced good enhancement compared to\nthe existing algorithm, showing that it is possible to use DNNs for document\nimage enhancement. Furthermore, the best performing architectures could work as\na baseline for future investigations on document enhancement using deep\nlearning techniques. The main contributions of this paper are: a baseline of\ndeep learning techniques that can be further improved to provide better\nresults, and a evaluation methodology using IQA metrics for quantitatively\ncomparing the produced images from the neural networks to a ground truth.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:48:28 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kirsten", "Lucas N.", ""], ["Piccoli", "Ricardo", ""], ["Ribani", "Ricardo", ""]]}, {"id": "2106.15287", "submitter": "Arthur Douillard", "authors": "Arthur Douillard, Yifu Chen, Arnaud Dapogny, Matthieu Cord", "title": "Tackling Catastrophic Forgetting and Background Shift in Continual\n  Semantic Segmentation", "comments": "Under review at IEEE TPAMI, journal extension of arXiv:2011.11390", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning approaches are nowadays ubiquitously used to tackle computer\nvision tasks such as semantic segmentation, requiring large datasets and\nsubstantial computational power. Continual learning for semantic segmentation\n(CSS) is an emerging trend that consists in updating an old model by\nsequentially adding new classes. However, continual learning methods are\nusually prone to catastrophic forgetting. This issue is further aggravated in\nCSS where, at each step, old classes from previous iterations are collapsed\ninto the background. In this paper, we propose Local POD, a multi-scale pooling\ndistillation scheme that preserves long- and short-range spatial relationships\nat feature level. Furthermore, we design an entropy-based pseudo-labelling of\nthe background w.r.t. classes predicted by the old model to deal with\nbackground shift and avoid catastrophic forgetting of the old classes. Finally,\nwe introduce a novel rehearsal method that is particularly suited for\nsegmentation. Our approach, called PLOP, significantly outperforms\nstate-of-the-art methods in existing CSS scenarios, as well as in newly\nproposed challenging benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 11:57:21 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Douillard", "Arthur", ""], ["Chen", "Yifu", ""], ["Dapogny", "Arnaud", ""], ["Cord", "Matthieu", ""]]}, {"id": "2106.15288", "submitter": "Fadi Boutros", "authors": "Fadi Boutros, Naser Damer, Jan Niklas Kolf, Kiran Raja, Florian\n  Kirchbuchner, Raghavendra Ramachandra, Arjan Kuijper, Pengcheng Fang, Chao\n  Zhang, Fei Wang, David Montero, Naiara Aginako, Basilio Sierra, Marcos Nieto,\n  Mustafa Ekrem Erakin, Ugur Demir, Hazim Kemal, Ekenel, Asaki Kataoka, Kohei\n  Ichikawa, Shizuma Kubo, Jie Zhang, Mingjie He, Dan Han, Shiguang Shan, Klemen\n  Grm, Vitomir \\v{S}truc, Sachith Seneviratne, Nuran Kasthuriarachchi, Sanka\n  Rasnayaka, Pedro C. Neto, Ana F. Sequeira, Joao Ribeiro Pinto, Mohsen\n  Saffari, and Jaime S. Cardoso", "title": "MFR 2021: Masked Face Recognition Competition", "comments": "Accepted at International Join Conference on Biometrics (IJCB 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a summary of the Masked Face Recognition Competitions\n(MFR) held within the 2021 International Joint Conference on Biometrics (IJCB\n2021). The competition attracted a total of 10 participating teams with valid\nsubmissions. The affiliations of these teams are diverse and associated with\nacademia and industry in nine different countries. These teams successfully\nsubmitted 18 valid solutions. The competition is designed to motivate solutions\naiming at enhancing the face recognition accuracy of masked faces. Moreover,\nthe competition considered the deployability of the proposed solutions by\ntaking the compactness of the face recognition models into account. A private\ndataset representing a collaborative, multi-session, real masked, capture\nscenario is used to evaluate the submitted solutions. In comparison to one of\nthe top-performing academic face recognition solutions, 10 out of the 18\nsubmitted solutions did score higher masked face verification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 11:59:56 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Boutros", "Fadi", ""], ["Damer", "Naser", ""], ["Kolf", "Jan Niklas", ""], ["Raja", "Kiran", ""], ["Kirchbuchner", "Florian", ""], ["Ramachandra", "Raghavendra", ""], ["Kuijper", "Arjan", ""], ["Fang", "Pengcheng", ""], ["Zhang", "Chao", ""], ["Wang", "Fei", ""], ["Montero", "David", ""], ["Aginako", "Naiara", ""], ["Sierra", "Basilio", ""], ["Nieto", "Marcos", ""], ["Erakin", "Mustafa Ekrem", ""], ["Demir", "Ugur", ""], ["Kemal", "Hazim", ""], ["Ekenel", "", ""], ["Kataoka", "Asaki", ""], ["Ichikawa", "Kohei", ""], ["Kubo", "Shizuma", ""], ["Zhang", "Jie", ""], ["He", "Mingjie", ""], ["Han", "Dan", ""], ["Shan", "Shiguang", ""], ["Grm", "Klemen", ""], ["\u0160truc", "Vitomir", ""], ["Seneviratne", "Sachith", ""], ["Kasthuriarachchi", "Nuran", ""], ["Rasnayaka", "Sanka", ""], ["Neto", "Pedro C.", ""], ["Sequeira", "Ana F.", ""], ["Pinto", "Joao Ribeiro", ""], ["Saffari", "Mohsen", ""], ["Cardoso", "Jaime S.", ""]]}, {"id": "2106.15292", "submitter": "Deep Patel", "authors": "Deep Patel and P.S. Sastry", "title": "Adaptive Sample Selection for Robust Learning under Label Noise", "comments": "19 pages, 7 figures, and 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:10:58 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 08:47:05 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Patel", "Deep", ""], ["Sastry", "P. S.", ""]]}, {"id": "2106.15294", "submitter": "Youry Khmelevsky", "authors": "Kenichi Sugihara, Martin Wallace, Kongwen (Frank) Zhang, Youry\n  Khmelevsky", "title": "Roof Damage Assessment from Automated 3D Building Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The 3D building modelling is important in urban planning and related domains\nthat draw upon the content of 3D models of urban scenes. Such 3D models can be\nused to visualize city images at multiple scales from individual buildings to\nentire cities prior to and after a change has occurred. This ability is of\ngreat importance in day-to-day work and special projects undertaken by\nplanners, geo-designers, and architects. In this research, we implemented a\nnovel approach to 3D building models for such matter, which included the\nintegration of geographic information systems (GIS) and 3D Computer Graphics\n(3DCG) components that generate 3D house models from building footprints\n(polygons), and the automated generation of simple and complex roof geometries\nfor rapid roof area damage reporting. These polygons (footprints) are usually\northogonal. A complicated orthogonal polygon can be partitioned into a set of\nrectangles. The proposed GIS and 3DCG integrated system partitions orthogonal\nbuilding polygons into a set of rectangles and places rectangular roofs and\nbox-shaped building bodies on these rectangles. Since technicians are drawing\nthese polygons manually with digitizers, depending on aerial photos, not all\nbuilding polygons are precisely orthogonal. But, when placing a set of boxes as\nbuilding bodies for creating the buildings, there may be gaps or overlaps\nbetween these boxes if building polygons are not precisely orthogonal. In our\nproposal, after approximately orthogonal building polygons are partitioned and\nrectified into a set of mutually orthogonal rectangles, each rectangle knows\nwhich rectangle is adjacent to and which edge of the rectangle is adjacent to,\nwhich will avoid unwanted intersection of windows and doors when building\nbodies combined.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 22:17:01 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Sugihara", "Kenichi", "", "Frank"], ["Wallace", "Martin", "", "Frank"], ["Kongwen", "", "", "Frank"], ["Zhang", "", ""], ["Khmelevsky", "Youry", ""]]}, {"id": "2106.15296", "submitter": "Deborah Pereg", "authors": "Deborah Pereg, Israel Cohen, and Anthony A. Vassiliou", "title": "Convolutional Sparse Coding Fast Approximation with Application to\n  Seismic Reflectivity Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse coding, we attempt to extract features of input vectors, assuming\nthat the data is inherently structured as a sparse superposition of basic\nbuilding blocks. Similarly, neural networks perform a given task by learning\nfeatures of the training data set. Recently both data-driven and model-driven\nfeature extracting methods have become extremely popular and have achieved\nremarkable results. Nevertheless, practical implementations are often too slow\nto be employed in real-life scenarios, especially for real-time applications.\nWe propose a speed-up upgraded version of the classic iterative thresholding\nalgorithm, that produces a good approximation of the convolutional sparse code\nwithin 2-5 iterations. The speed advantage is gained mostly from the\nobservation that most solvers are slowed down by inefficient global\nthresholding. The main idea is to normalize each data point by the local\nreceptive field energy, before applying a threshold. This way, the natural\ninclination towards strong feature expressions is suppressed, so that one can\nrely on a global threshold that can be easily approximated, or learned during\ntraining. The proposed algorithm can be employed with a known predetermined\ndictionary, or with a trained dictionary. The trained version is implemented as\na neural net designed as the unfolding of the proposed solver. The performance\nof the proposed solution is demonstrated via the seismic inversion problem in\nboth synthetic and real data scenarios. We also provide theoretical guarantees\nfor a stable support recovery. Namely, we prove that under certain conditions\nthe true support is perfectly recovered within the first iteration.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:19:07 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Pereg", "Deborah", ""], ["Cohen", "Israel", ""], ["Vassiliou", "Anthony A.", ""]]}, {"id": "2106.15299", "submitter": "Mostafa Jahanifar", "authors": "Neda Zamanitajeddin, Mostafa Jahanifar, and Nasir Rajpoot", "title": "Cells are Actors: Social Network Analysis with Classical ML for SOTA\n  Histology Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digitization of histology images and the advent of new computational methods,\nlike deep learning, have helped the automatic grading of colorectal\nadenocarcinoma cancer (CRA). Present automated CRA grading methods, however,\nusually use tiny image patches and thus fail to integrate the entire tissue\nmicro-architecture for grading purposes. To tackle these challenges, we propose\nto use a statistical network analysis method to describe the complex structure\nof the tissue micro-environment by modelling nuclei and their connections as a\nnetwork. We show that by analyzing only the interactions between the cells in a\nnetwork, we can extract highly discriminative statistical features for CRA\ngrading. Unlike other deep learning or convolutional graph-based approaches,\nour method is highly scalable (can be used for cell networks consist of\nmillions of nodes), completely explainable, and computationally inexpensive. We\ncreate cell networks on a broad CRC histology image dataset, experiment with\nour method, and report state-of-the-art performance for the prediction of\nthree-class CRA grading.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:22:10 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 11:57:37 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zamanitajeddin", "Neda", ""], ["Jahanifar", "Mostafa", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2106.15301", "submitter": "Rudrasis Chakraborty Dr.", "authors": "Monami Banerjee, Rudrasis Chakraborty, Jose Bouza and Baba C. Vemuri", "title": "VolterraNet: A higher order convolutional network with group\n  equivariance for homogeneous manifolds", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks have been highly successful in image-based\nlearning tasks due to their translation equivariance property. Recent work has\ngeneralized the traditional convolutional layer of a convolutional neural\nnetwork to non-Euclidean spaces and shown group equivariance of the generalized\nconvolution operation. In this paper, we present a novel higher order Volterra\nconvolutional neural network (VolterraNet) for data defined as samples of\nfunctions on Riemannian homogeneous spaces. Analagous to the result for\ntraditional convolutions, we prove that the Volterra functional convolutions\nare equivariant to the action of the isometry group admitted by the Riemannian\nhomogeneous spaces, and under some restrictions, any non-linear equivariant\nfunction can be expressed as our homogeneous space Volterra convolution,\ngeneralizing the non-linear shift equivariant characterization of Volterra\nexpansions in Euclidean space. We also prove that second order functional\nconvolution operations can be represented as cascaded convolutions which leads\nto an efficient implementation. Beyond this, we also propose a dilated\nVolterraNet model. These advances lead to large parameter reductions relative\nto baseline non-Euclidean CNNs. To demonstrate the efficacy of the VolterraNet\nperformance, we present several real data experiments involving classification\ntasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing\non diffusion MRI data. Performance comparisons to the state-of-the-art are also\npresented.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 19:28:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Banerjee", "Monami", ""], ["Chakraborty", "Rudrasis", ""], ["Bouza", "Jose", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "2106.15304", "submitter": "Xuan Shen", "authors": "Xuan Shen, Geng Yuan, Wei Niu, Xiaolong Ma, Jiexiong Guan, Zhengang\n  Li, Bin Ren and Yanzhi Wang", "title": "Towards Fast and Accurate Multi-Person Pose Estimation on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid development of autonomous driving, abnormal behavior detection, and\nbehavior recognition makes an increasing demand for multi-person pose\nestimation-based applications, especially on mobile platforms. However, to\nachieve high accuracy, state-of-the-art methods tend to have a large model size\nand complex post-processing algorithm, which costs intense computation and long\nend-to-end latency. To solve this problem, we propose an architecture\noptimization and weight pruning framework to accelerate inference of\nmulti-person pose estimation on mobile devices. With our optimization\nframework, we achieve up to 2.51x faster model inference speed with higher\naccuracy compared to representative lightweight multi-person pose estimator.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 22:39:40 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Shen", "Xuan", ""], ["Yuan", "Geng", ""], ["Niu", "Wei", ""], ["Ma", "Xiaolong", ""], ["Guan", "Jiexiong", ""], ["Li", "Zhengang", ""], ["Ren", "Bin", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2106.15305", "submitter": "Shaona Ghosh", "authors": "Mona Zehni, Shaona Ghosh, Krishna Sridhar, Sethu Raman", "title": "Joint Learning of Portrait Intrinsic Decomposition and Relighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse rendering is the problem of decomposing an image into its intrinsic\ncomponents, i.e. albedo, normal and lighting. To solve this ill-posed problem\nfrom single image, state-of-the-art methods in shape from shading mostly resort\nto supervised training on all the components on either synthetic or real\ndatasets. Here, we propose a new self-supervised training paradigm that 1)\nreduces the need for full supervision on the decomposition task and 2) takes\ninto account the relighting task. We introduce new self-supervised loss terms\nthat leverage the consistencies between multi-lit images (images of the same\nscene under different illuminations). Our approach is applicable to multi-lit\ndatasets. We apply our training approach in two settings: 1) train on a mixture\nof synthetic and real data, 2) train on real datasets with limited supervision.\nWe show-case the effectiveness of our training paradigm on both intrinsic\ndecomposition and relighting and demonstrate how the model struggles in both\ntasks without the self-supervised loss terms in limited supervision settings.\nWe provide results of comprehensive experiments on SfSNet, CelebA and Photoface\ndatasets and verify the performance of our approach on images in the wild.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 19:40:22 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zehni", "Mona", ""], ["Ghosh", "Shaona", ""], ["Sridhar", "Krishna", ""], ["Raman", "Sethu", ""]]}, {"id": "2106.15306", "submitter": "Daniel Ruijters", "authors": "Daniel Ruijters", "title": "Artificial Intelligence in Minimally Invasive Interventional Treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimally invasive image guided treatment procedures often employ advanced\nimage processing algorithms. The recent developments of artificial intelligence\nalgorithms harbor potential to further enhance this domain. In this article we\nexplore several application areas within the minimally invasive treatment space\nand discuss the deployment of artificial intelligence within these areas.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:57:25 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ruijters", "Daniel", ""]]}, {"id": "2106.15308", "submitter": "Daniel Ruijters", "authors": "Robert Homan, Ren\\'e van Rijsselt, Daniel Ruijters", "title": "Automatic 2D-3D Registration without Contrast Agent during Neurovascular\n  Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusing live fluoroscopy images with a 3D rotational reconstruction of the\nvasculature allows to navigate endovascular devices in minimally invasive\nneuro-vascular treatment, while reducing the usage of harmful iodine contrast\nmedium. The alignment of the fluoroscopy images and the 3D reconstruction is\ninitialized using the sensor information of the X-ray C-arm geometry. Patient\nmotion is then corrected by an image-based registration algorithm, based on a\ngradient difference similarity measure using digital reconstructed radiographs\nof the 3D reconstruction. This algorithm does not require the vessels in the\nfluoroscopy image to be filled with iodine contrast agent, but rather relies on\ngradients in the image (bone structures, sinuses) as landmark features. This\npaper investigates the accuracy, robustness and computation time aspects of the\nimage-based registration algorithm. Using phantom experiments 97% of the\nregistration attempts passed the success criterion of a residual registration\nerror of less than 1 mm translation and 3{\\deg} rotation. The paper establishes\na new method for validation of 2D-3D registration without requiring changes to\nthe clinical workflow, such as attaching fiducial markers. As a consequence,\nthis method can be retrospectively applied to pre-existing clinical data. For\nclinical data experiments, 87% of the registration attempts passed the\ncriterion of a residual translational error of < 1 mm, and 84% possessed a\nrotational error of < 3{\\deg}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 20:16:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Homan", "Robert", ""], ["van Rijsselt", "Ren\u00e9", ""], ["Ruijters", "Daniel", ""]]}, {"id": "2106.15309", "submitter": "Evin Pinar Ornek", "authors": "Ege \\\"Ozsoy, Evin P{\\i}nar \\\"Ornek, Ulrich Eck, Federico Tombari,\n  Nassir Navab", "title": "Multimodal Semantic Scene Graphs for Holistic Modeling of Surgical\n  Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  From a computer science viewpoint, a surgical domain model needs to be a\nconceptual one incorporating both behavior and data. It should therefore model\nactors, devices, tools, their complex interactions and data flow. To capture\nand model these, we take advantage of the latest computer vision methodologies\nfor generating 3D scene graphs from camera views. We then introduce the\nMultimodal Semantic Scene Graph (MSSG) which aims at providing a unified\nsymbolic, spatiotemporal and semantic representation of surgical procedures.\nThis methodology aims at modeling the relationship between different components\nin surgical domain including medical staff, imaging systems, and surgical\ndevices, opening the path towards holistic understanding and modeling of\nsurgical procedures. We then use MSSG to introduce a dynamically generated\ngraphical user interface tool for surgical procedure analysis which could be\nused for many applications including process optimization, OR design and\nautomatic report generation. We finally demonstrate that the proposed MSSGs\ncould also be used for synchronizing different complex surgical procedures.\nWhile the system still needs to be integrated into real operating rooms before\ngetting validated, this conference paper aims mainly at providing the community\nwith the basic principles of this novel concept through a first prototypal\npartial realization based on MVOR dataset.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:35:44 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["\u00d6zsoy", "Ege", ""], ["\u00d6rnek", "Evin P\u0131nar", ""], ["Eck", "Ulrich", ""], ["Tombari", "Federico", ""], ["Navab", "Nassir", ""]]}, {"id": "2106.15312", "submitter": "Chao Zeng", "authors": "Chao Zeng, Tiesong Zhao, Sam Kwong", "title": "Contrastive Semantic Similarity Learning for Image Captioning Evaluation\n  with Intrinsic Auto-encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically evaluating the quality of image captions can be very\nchallenging since human language is quite flexible that there can be various\nexpressions for the same meaning. Most of the current captioning metrics rely\non token level matching between candidate caption and the ground truth label\nsentences. It usually neglects the sentence-level information. Motivated by the\nauto-encoder mechanism and contrastive representation learning advances, we\npropose a learning-based metric for image captioning, which we call Intrinsic\nImage Captioning Evaluation($I^2CE$). We develop three progressive model\nstructures to learn the sentence level representations--single branch model,\ndual branches model, and triple branches model. Our empirical tests show that\n$I^2CE$ trained with dual branches structure achieves better consistency with\nhuman judgments to contemporary image captioning evaluation metrics.\nFurthermore, We select several state-of-the-art image captioning models and\ntest their performances on the MS COCO dataset concerning both contemporary\nmetrics and the proposed $I^2CE$. Experiment results show that our proposed\nmethod can align well with the scores generated from other contemporary\nmetrics. On this concern, the proposed metric could serve as a novel indicator\nof the intrinsic information between captions, which may be complementary to\nthe existing ones.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:27:05 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zeng", "Chao", ""], ["Zhao", "Tiesong", ""], ["Kwong", "Sam", ""]]}, {"id": "2106.15315", "submitter": "Neil Agarwal", "authors": "Neil Agarwal, Ravi Netravali", "title": "Boggart: Accelerating Retrospective Video Analytics via Model-Agnostic\n  Ingest Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delivering fast responses to retrospective queries on video datasets is\ndifficult due to the large number of frames to consider and the high costs of\nrunning convolutional neural networks (CNNs) on each one. A natural solution is\nto perform a subset of the necessary computations ahead of time, as video is\ningested. However, existing ingest-time systems require knowledge of the\nspecific CNN that will be used in future queries -- a challenging requisite\ngiven the evergrowing space of CNN architectures and training\ndatasets/methodologies.\n  This paper presents Boggart, a retrospective video analytics system that\ndelivers ingest-time speedups in a model-agnostic manner. Our underlying\ninsight is that traditional computer vision (CV) algorithms are capable of\nperforming computations that can be used to accelerate diverse queries with\nwide-ranging CNNs. Building on this, at ingest-time, Boggart carefully employs\na variety of motion tracking algorithms to identify potential objects and their\ntrajectories across frames. Then, at query-time, Boggart uses several novel\ntechniques to collect the smallest sample of CNN results required to meet the\ntarget accuracy: (1) a clustering strategy to efficiently unearth the\ninevitable discrepancies between CV- and CNN-generated outputs, and (2) a set\nof accuracy-preserving propagation techniques to safely extend sampled results\nalong each trajectory. Across many videos, CNNs, and queries Boggart\nconsistently meets accuracy targets while using CNNs sparingly (on 3-54% of\nframes).\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:21:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Agarwal", "Neil", ""], ["Netravali", "Ravi", ""]]}, {"id": "2106.15318", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Irene Kotsia and Elnar Hajiyev and Stefanos\n  Zafeiriou", "title": "Analysing Affective Behavior in the second ABAW2 Competition", "comments": "arXiv admin note: substantial text overlap with arXiv:2001.11409", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Affective Behavior Analysis in-the-wild (ABAW2) 2021 Competition is the\nsecond -- following the first very successful ABAW Competition held in\nconjunction with IEEE FG 2020- Competition that aims at automatically analyzing\naffect. ABAW2 is split into three Challenges, each one addressing one of the\nthree main behavior tasks of valence-arousal estimation, basic expression\nclassification and action unit detection. All three Challenges are based on a\ncommon benchmark database, Aff-Wild2, which is a large scale in-the-wild\ndatabase and the first one to be annotated for all these three tasks. In this\npaper, we describe this Competition, to be held in conjunction with ICCV 2021.\nWe present the three Challenges, with the utilized Competition corpora. We\noutline the evaluation metrics and present the baseline system with its\nresults. More information regarding the Competition is provided in the\nCompetition site: https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 11:30:19 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 19:33:22 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Kotsia", "Irene", ""], ["Hajiyev", "Elnar", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2106.15319", "submitter": "Fan Feng", "authors": "Jin Zhang, Fan Feng, Pere Marti-Puig, Cesar F. Caiafa, Zhe Sun, Feng\n  Duan, Jordi Sol\\'e-Casals", "title": "Serial-EMD: Fast Empirical Mode Decomposition Method for\n  Multi-dimensional Signals Based on Serialization", "comments": "19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical mode decomposition (EMD) has developed into a prominent tool for\nadaptive, scale-based signal analysis in various fields like robotics, security\nand biomedical engineering. Since the dramatic increase in amount of data puts\nforward higher requirements for the capability of real-time signal analysis, it\nis difficult for existing EMD and its variants to trade off the growth of data\ndimension and the speed of signal analysis. In order to decompose\nmulti-dimensional signals at a faster speed, we present a novel\nsignal-serialization method (serial-EMD), which concatenates multi-variate or\nmulti-dimensional signals into a one-dimensional signal and uses various\none-dimensional EMD algorithms to decompose it. To verify the effects of the\nproposed method, synthetic multi-variate time series, artificial 2D images with\nvarious textures and real-world facial images are tested. Compared with\nexisting multi-EMD algorithms, the decomposition time becomes significantly\nreduced. In addition, the results of facial recognition with Intrinsic Mode\nFunctions (IMFs) extracted using our method can achieve a higher accuracy than\nthose obtained by existing multi-EMD algorithms, which demonstrates the\nsuperior performance of our method in terms of the quality of IMFs.\nFurthermore, this method can provide a new perspective to optimize the existing\nEMD algorithms, that is, transforming the structure of the input signal rather\nthan being constrained by developing envelope computation techniques or signal\ndecomposition methods. In summary, the study suggests that the serial-EMD\ntechnique is a highly competitive and fast alternative for multi-dimensional\nsignal analysis.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 03:56:08 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhang", "Jin", ""], ["Feng", "Fan", ""], ["Marti-Puig", "Pere", ""], ["Caiafa", "Cesar F.", ""], ["Sun", "Zhe", ""], ["Duan", "Feng", ""], ["Sol\u00e9-Casals", "Jordi", ""]]}, {"id": "2106.15320", "submitter": "Sampanna Yashwant Kahu", "authors": "Sampanna Yashwant Kahu, William A. Ingram, Edward A. Fox, Jian Wu", "title": "ScanBank: A Benchmark Dataset for Figure Extraction from Scanned\n  Electronic Theses and Dissertations", "comments": "16 pages, 3 figures, submitted to ACM/IEEE Joint Conference on\n  Digital Libraries", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We focus on electronic theses and dissertations (ETDs), aiming to improve\naccess and expand their utility, since more than 6 million are publicly\navailable, and they constitute an important corpus to aid research and\neducation across disciplines. The corpus is growing as new born-digital\ndocuments are included, and since millions of older theses and dissertations\nhave been converted to digital form to be disseminated electronically in\ninstitutional repositories. In ETDs, as with other scholarly works, figures and\ntables can communicate a large amount of information in a concise way. Although\nmethods have been proposed for extracting figures and tables from born-digital\nPDFs, they do not work well with scanned ETDs. Considering this problem, our\nassessment of state-of-the-art figure extraction systems is that the reason\nthey do not function well on scanned PDFs is that they have only been trained\non born-digital documents. To address this limitation, we present ScanBank, a\nnew dataset containing 10 thousand scanned page images, manually labeled by\nhumans as to the presence of the 3.3 thousand figures or tables found therein.\nWe use this dataset to train a deep neural network model based on YOLOv5 to\naccurately extract figures and tables from scanned ETDs. We pose and answer\nimportant research questions aimed at finding better methods for figure\nextraction from scanned documents. One of those concerns the value for\ntraining, of data augmentation techniques applied to born-digital documents\nwhich are used to train models better suited for figure extraction from scanned\ndocuments. To the best of our knowledge, ScanBank is the first manually\nannotated dataset for figure and table extraction for scanned ETDs. A\nYOLOv5-based model, trained on ScanBank, outperforms existing comparable\nopen-source and freely available baseline methods by a considerable margin.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 04:43:56 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kahu", "Sampanna Yashwant", ""], ["Ingram", "William A.", ""], ["Fox", "Edward A.", ""], ["Wu", "Jian", ""]]}, {"id": "2106.15321", "submitter": "Nicolas Saunier", "authors": "Laurent Boucaud, Daniel Aloise and Nicolas Saunier", "title": "Soft Attention: Does it Actually Help to Learn Social Interactions in\n  Pedestrian Trajectory Prediction?", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of predicting the future path of a pedestrian using\nits motion history and the motion history of the surrounding pedestrians,\ncalled social information. Since the seminal paper on Social-LSTM,\ndeep-learning has become the main tool used to model the impact of social\ninteractions on a pedestrian's motion. The demonstration that these models can\nlearn social interactions relies on an ablative study of these models. The\nmodels are compared with and without their social interactions module on two\nstandard metrics, the Average Displacement Error and Final Displacement Error.\nYet, these complex models were recently outperformed by a simple\nconstant-velocity approach. This questions if they actually allow to model\nsocial interactions as well as the validity of the proof. In this paper, we\nfocus on the deep-learning models with a soft-attention mechanism for social\ninteraction modeling and study whether they use social information at\nprediction time. We conduct two experiments across four state-of-the-art\napproaches on the ETH and UCY datasets, which were also used in previous work.\nFirst, the models are trained by replacing the social information with random\nnoise and compared to model trained with actual social information. Second, we\nuse a gating mechanism along with a $L_0$ penalty, allowing models to shut down\ntheir inner components. The models consistently learn to prune their\nsoft-attention mechanism. For both experiments, neither the course of the\nconvergence nor the prediction performance were altered. This demonstrates that\nthe soft-attention mechanism and therefore the social information are ignored\nby the models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:39:35 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Boucaud", "Laurent", ""], ["Aloise", "Daniel", ""], ["Saunier", "Nicolas", ""]]}, {"id": "2106.15322", "submitter": "Abdelrahman Abdallah", "authors": "Abdelrahman Abdallah, Alexander Berendeyev, Islam Nuradin, Daniyar\n  Nurseitov", "title": "TNCR: Table Net Detection and Classification Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present TNCR, a new table dataset with varying image quality collected\nfrom free websites. The TNCR dataset can be used for table detection in scanned\ndocument images and their classification into 5 different classes. TNCR\ncontains 9428 high-quality labeled images. In this paper, we have implemented\nstate-of-the-art deep learning-based methods for table detection to create\nseveral strong baselines. Cascade Mask R-CNN with ResNeXt-101-64x4d Backbone\nNetwork achieves the highest performance compared to other methods with a\nprecision of 79.7%, recall of 89.8%, and f1 score of 84.4% on the TNCR dataset.\nWe have made TNCR open source in the hope of encouraging more deep learning\napproaches to table detection, classification, and structure recognition. The\ndataset and trained model checkpoints are available at\nhttps://github.com/abdoelsayed2016/TNCR_Dataset.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 10:48:58 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Abdallah", "Abdelrahman", ""], ["Berendeyev", "Alexander", ""], ["Nuradin", "Islam", ""], ["Nurseitov", "Daniyar", ""]]}, {"id": "2106.15323", "submitter": "Geraldine Jeckeln", "authors": "G\\'eraldine Jeckeln, Ying Hu, Jacqueline G. Cavazos, Amy N. Yates,\n  Carina A. Hahn, Larry Tang, P. Jonathon Phillips, Alice J. O'Toole", "title": "Face Identification Proficiency Test Designed Using Item Response Theory", "comments": "17 pages (including references), 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures of face identification proficiency are essential to ensure accurate\nand consistent performance by professional forensic face examiners and others\nwho perform face identification tasks in applied scenarios. Current proficiency\ntests rely on static sets of stimulus items, and so, cannot be administered\nvalidly to the same individual multiple times. To create a proficiency test, a\nlarge number of items of \"known\" difficulty must be assembled. Multiple tests\nof equal difficulty can be constructed then using subsets of items. Here, we\nintroduce a proficiency test, the Triad Identity Matching (TIM) test, based on\nstimulus difficulty measures based on Item Response Theory (IRT). Participants\nview face-image \"triads\" (N=225) (two images of one identity and one image of a\ndifferent identity) and select the different identity. In Experiment 1,\nuniversity students (N=197) showed wide-ranging accuracy on the TIM test.\nFurthermore, IRT modeling demonstrated that the TIM test produces items of\nvarious difficulty levels. In Experiment 2, IRT-based item difficulty measures\nwere used to partition the TIM test into three equally \"easy\" and three equally\n\"difficult\" subsets. Simulation results indicated that the full set, as well as\ncurated subsets, of the TIM items yielded reliable estimates of subject\nability. In summary, the TIM test can provide a starting point for developing a\nframework that is flexible, calibrated, and adaptive to measure proficiency\nacross various ability levels (e.g., professionals or populations with face\nprocessing deficits)\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 22:37:32 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 16:52:11 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Jeckeln", "G\u00e9raldine", ""], ["Hu", "Ying", ""], ["Cavazos", "Jacqueline G.", ""], ["Yates", "Amy N.", ""], ["Hahn", "Carina A.", ""], ["Tang", "Larry", ""], ["Phillips", "P. Jonathon", ""], ["O'Toole", "Alice J.", ""]]}, {"id": "2106.15324", "submitter": "Nathan Beck", "authors": "Nathan Beck, Durga Sivasubramanian, Apurva Dani, Ganesh Ramakrishnan,\n  Rishabh Iyer", "title": "Effective Evaluation of Deep Active Learning on Image Classification\n  Tasks", "comments": "9 pages in main paper, 6 figures in main paper, 3 tables in main\n  paper. 23 pages in total, 15 figures in total, 14 tables in total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the goal of making deep learning more label-efficient, a growing number\nof papers have been studying active learning (AL) for deep models. However,\nthere are a number of issues in the prevalent experimental settings, mainly\nstemming from a lack of unified implementation and benchmarking. Issues in the\ncurrent literature include sometimes contradictory observations on the\nperformance of different AL algorithms, unintended exclusion of important\ngeneralization approaches such as data augmentation and SGD for optimization, a\nlack of study of evaluation facets like the labeling efficiency of AL, and\nlittle or no clarity on the scenarios in which AL outperforms random sampling\n(RS). In this work, we present a unified re-implementation of state-of-the-art\nAL algorithms in the context of image classification, and we carefully study\nthese issues as facets of effective evaluation. On the positive side, we show\nthat AL techniques are 2x to 4x more label-efficient compared to RS with the\nuse of data augmentation. Surprisingly, when data augmentation is included,\nthere is no longer a consistent gain in using BADGE, a state-of-the-art\napproach, over simple uncertainty sampling. We then do a careful analysis of\nhow existing approaches perform with varying amounts of redundancy and number\nof examples per class. Finally, we provide several insights for AL\npractitioners to consider in future work, such as the effect of the AL batch\nsize, the effect of initialization, the importance of retraining a new model at\nevery round, and other insights.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 23:29:39 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 04:49:40 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Beck", "Nathan", ""], ["Sivasubramanian", "Durga", ""], ["Dani", "Apurva", ""], ["Ramakrishnan", "Ganesh", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2106.15325", "submitter": "Abdul Mueed Hafiz Dr.", "authors": "Abdul Mueed Hafiz, Rouf Ul Alam Bhat, Shabir Ahmad Parah, M.\n  Hassaballah", "title": "SE-MD: A Single-encoder multiple-decoder deep network for point cloud\n  generation from 2D images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D model generation from single 2D RGB images is a challenging and actively\nresearched computer vision task. Various techniques using conventional network\narchitectures have been proposed for the same. However, the body of research\nwork is limited and there are various issues like using inefficient 3D\nrepresentation formats, weak 3D model generation backbones, inability to\ngenerate dense point clouds, dependence of post-processing for generation of\ndense point clouds, and dependence on silhouettes in RGB images. In this paper,\na novel 2D RGB image to point cloud conversion technique is proposed, which\nimproves the state of art in the field due to its efficient, robust and simple\nmodel by using the concept of parallelization in network architecture. It not\nonly uses the efficient and rich 3D representation of point clouds, but also\nuses a novel and robust point cloud generation backbone in order to address the\nprevalent issues. This involves using a single-encoder multiple-decoder deep\nnetwork architecture wherein each decoder generates certain fixed viewpoints.\nThis is followed by fusing all the viewpoints to generate a dense point cloud.\nVarious experiments are conducted on the technique and its performance is\ncompared with those of other state of the art techniques and impressive gains\nin performance are demonstrated. Code is available at\nhttps://github.com/mueedhafiz1982/\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:48:46 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hafiz", "Abdul Mueed", ""], ["Bhat", "Rouf Ul Alam", ""], ["Parah", "Shabir Ahmad", ""], ["Hassaballah", "M.", ""]]}, {"id": "2106.15326", "submitter": "Mingkui Tan", "authors": "Zhen Qiu, Yifan Zhang, Hongbin Lin, Shuaicheng Niu, Yanxia Liu, Qing\n  Du, Mingkui Tan", "title": "Source-free Domain Adaptation via Avatar Prototype Generation and\n  Adaptation", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a practical domain adaptation task, called source-free unsupervised\ndomain adaptation (UDA) problem, in which we cannot access source domain data\ndue to data privacy issues but only a pre-trained source model and unlabeled\ntarget data are available. This task, however, is very difficult due to one key\nchallenge: the lack of source data and target domain labels makes model\nadaptation very challenging. To address this, we propose to mine the hidden\nknowledge in the source model and exploit it to generate source avatar\nprototypes (i.e., representative features for each source class) as well as\ntarget pseudo labels for domain alignment. To this end, we propose a\nContrastive Prototype Generation and Adaptation (CPGA) method. Specifically,\nCPGA consists of two stages: (1) prototype generation: by exploring the\nclassification boundary information of the source model, we train a prototype\ngenerator to generate avatar prototypes via contrastive learning. (2) prototype\nadaptation: based on the generated source prototypes and target pseudo labels,\nwe develop a new robust contrastive prototype adaptation strategy to align each\npseudo-labeled target data to the corresponding source prototypes. Extensive\nexperiments on three UDA benchmark datasets demonstrate the effectiveness and\nsuperiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:30:54 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Qiu", "Zhen", ""], ["Zhang", "Yifan", ""], ["Lin", "Hongbin", ""], ["Niu", "Shuaicheng", ""], ["Liu", "Yanxia", ""], ["Du", "Qing", ""], ["Tan", "Mingkui", ""]]}, {"id": "2106.15327", "submitter": "Yoann Altmann", "authors": "Dan Yao and Stephen McLaughlin and Yoann Altmann", "title": "Patch-Based Image Restoration using Expectation Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a new Expectation Propagation (EP) framework for image\nrestoration using patch-based prior distributions. While Monte Carlo techniques\nare classically used to sample from intractable posterior distributions, they\ncan suffer from scalability issues in high-dimensional inference problems such\nas image restoration. To address this issue, EP is used here to approximate the\nposterior distributions using products of multivariate Gaussian densities.\nMoreover, imposing structural constraints on the covariance matrices of these\ndensities allows for greater scalability and distributed computation. While the\nmethod is naturally suited to handle additive Gaussian observation noise, it\ncan also be extended to non-Gaussian noise. Experiments conducted for\ndenoising, inpainting and deconvolution problems with Gaussian and Poisson\nnoise illustrate the potential benefits of such flexible approximate Bayesian\nmethod for uncertainty quantification in imaging problems, at a reduced\ncomputational cost compared to sampling techniques.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:45:15 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yao", "Dan", ""], ["McLaughlin", "Stephen", ""], ["Altmann", "Yoann", ""]]}, {"id": "2106.15328", "submitter": "Qingtian Zhu", "authors": "Qingtian Zhu, Chen Min, Zizhuang Wei, Yisong Chen and Guoping Wang", "title": "Deep Learning for Multi-View Stereo via Plane Sweep: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D reconstruction has lately attracted increasing attention due to its wide\napplication in many areas, such as autonomous driving, robotics and virtual\nreality. As a dominant technique in artificial intelligence, deep learning has\nbeen successfully adopted to solve various computer vision problems. However,\ndeep learning for 3D reconstruction is still at its infancy due to its unique\nchallenges and varying pipelines. To stimulate future research, this paper\npresents a review of recent progress in deep learning methods for Multi-view\nStereo (MVS), which is considered as a crucial task of image-based 3D\nreconstruction. It also presents comparative results on several publicly\navailable datasets, with insightful observations and inspiring future research\ndirections.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:10:44 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 06:44:44 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhu", "Qingtian", ""], ["Min", "Chen", ""], ["Wei", "Zizhuang", ""], ["Chen", "Yisong", ""], ["Wang", "Guoping", ""]]}, {"id": "2106.15329", "submitter": "Ashiq Anjum Prof", "authors": "Muhammad Usman Yaseen, Ashiq Anjum, Giancarlo Fortino, Antonio Liotta,\n  Amir Hussain", "title": "Cloud based Scalable Object Recognition from Video Streams using\n  Orientation Fusion and Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Object recognition from live video streams comes with numerous challenges\nsuch as the variation in illumination conditions and poses. Convolutional\nneural networks (CNNs) have been widely used to perform intelligent visual\nobject recognition. Yet, CNNs still suffer from severe accuracy degradation,\nparticularly on illumination-variant datasets. To address this problem, we\npropose a new CNN method based on orientation fusion for visual object\nrecognition. The proposed cloud-based video analytics system pioneers the use\nof bi-dimensional empirical mode decomposition to split a video frame into\nintrinsic mode functions (IMFs). We further propose these IMFs to endure Reisz\ntransform to produce monogenic object components, which are in turn used for\nthe training of CNNs. Past works have demonstrated how the object orientation\ncomponent may be used to pursue accuracy levels as high as 93\\%. Herein we\ndemonstrate how a feature-fusion strategy of the orientation components leads\nto further improving visual recognition accuracy to 97\\%. We also assess the\nscalability of our method, looking at both the number and the size of the video\nstreams under scrutiny. We carry out extensive experimentation on the publicly\navailable Yale dataset, including also a self generated video datasets, finding\nsignificant improvements (both in accuracy and scale), in comparison to\nAlexNet, LeNet and SE-ResNeXt, which are the three most commonly used deep\nlearning models for visual object recognition and classification.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 07:15:15 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yaseen", "Muhammad Usman", ""], ["Anjum", "Ashiq", ""], ["Fortino", "Giancarlo", ""], ["Liotta", "Antonio", ""], ["Hussain", "Amir", ""]]}, {"id": "2106.15331", "submitter": "Juan Liu", "authors": "Juan Liu", "title": "Improved Padding in CNNs for Quantitative Susceptibility Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, deep learning methods have been proposed for quantitative\nsusceptibility mapping (QSM) data processing: background field removal,\nfield-to-source inversion, and single-step QSM reconstruction. However, the\nconventional padding mechanism used in convolutional neural networks (CNNs) can\nintroduce spatial artifacts, especially in QSM background field removal and\nsingle-step QSM which requires inference from total fields with extreme large\nvalues at the edge boundaries of volume of interest. To address this issue, we\npropose an improved padding technique which utilizes the neighboring valid\nvoxels to estimate the invalid voxels of feature maps at volume boundaries in\nthe neural networks. Studies using simulated and in-vivo data show that the\nproposed padding greatly improves estimation accuracy and reduces artifacts in\nthe results in the tasks of background field removal, field-to-source\ninversion, and single-step QSM reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 01:35:00 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Liu", "Juan", ""]]}, {"id": "2106.15332", "submitter": "Jun Wang", "authors": "Yixuan Qiao, Hao Chen, Jun Wang, Yihao Chen, Xianbin Ye, Ziliang Li,\n  Xianbiao Qi, Peng Gao, Guotong Xie", "title": "Winner Team Mia at TextVQA Challenge 2021: Vision-and-Language\n  Representation Learning with Pre-trained Sequence-to-Sequence Model", "comments": "Winner of TextVQA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  TextVQA requires models to read and reason about text in images to answer\nquestions about them. Specifically, models need to incorporate a new modality\nof text present in the images and reason over it to answer TextVQA questions.\nIn this challenge, we use generative model T5 for TextVQA task. Based on\npre-trained checkpoint T5-3B from HuggingFace repository, two other\npre-training tasks including masked language modeling(MLM) and relative\nposition prediction(RPP) are designed to better align object feature and scene\ntext. In the stage of pre-training, encoder is dedicate to handle the fusion\namong multiple modalities: question text, object text labels, scene text\nlabels, object visual features, scene visual features. After that decoder\ngenerates the text sequence step-by-step, cross entropy loss is required by\ndefault. We use a large-scale scene text dataset in pre-training and then\nfine-tune the T5-3B with the TextVQA dataset only.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 06:39:37 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Qiao", "Yixuan", ""], ["Chen", "Hao", ""], ["Wang", "Jun", ""], ["Chen", "Yihao", ""], ["Ye", "Xianbin", ""], ["Li", "Ziliang", ""], ["Qi", "Xianbiao", ""], ["Gao", "Peng", ""], ["Xie", "Guotong", ""]]}, {"id": "2106.15338", "submitter": "Prasad Gabbur", "authors": "Prasad Gabbur and Manjot Bilkhu and Javier Movellan", "title": "Probabilistic Attention for Interactive Segmentation", "comments": "Updated with link to GitHub, 17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a probabilistic interpretation of attention and show that the\nstandard dot-product attention in transformers is a special case of Maximum A\nPosteriori (MAP) inference. The proposed approach suggests the use of\nExpectation Maximization algorithms for online adaptation of key and value\nmodel parameters. This approach is useful for cases in which external agents,\ne.g., annotators, provide inference-time information about the correct values\nof some tokens, e.g, the semantic category of some pixels, and we need for this\nnew information to propagate to other tokens in a principled manner. We\nillustrate the approach on an interactive semantic segmentation task in which\nannotators and models collaborate online to improve annotation efficiency.\nUsing standard benchmarks, we observe that key adaptation boosts model\nperformance ($\\sim10\\%$ mIoU) in the low feedback regime and value propagation\nimproves model responsiveness in the high feedback regime. A PyTorch layer\nimplementation of our probabilistic attention model will be made publicly\navailable here: https://github.com/apple/ml-probabilistic-attention.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 00:19:43 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 20:42:33 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Gabbur", "Prasad", ""], ["Bilkhu", "Manjot", ""], ["Movellan", "Javier", ""]]}, {"id": "2106.15341", "submitter": "Magda Friedjungov\\'a", "authors": "Daniel Va\\v{s}ata, Tom\\'a\\v{s} Halama, Magda Friedjungov\\'a", "title": "Image Inpainting Using Wasserstein Generative Adversarial Imputation\n  Network", "comments": "To be published in conference proceedings of ICANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image inpainting is one of the important tasks in computer vision which\nfocuses on the reconstruction of missing regions in an image. The aim of this\npaper is to introduce an image inpainting model based on Wasserstein Generative\nAdversarial Imputation Network. The generator network of the model uses\nbuilding blocks of convolutional layers with different dilation rates, together\nwith skip connections that help the model reproduce fine details of the output.\nThis combination yields a universal imputation model that is able to handle\nvarious scenarios of missingness with sufficient quality. To show this\nexperimentally, the model is simultaneously trained to deal with three\nscenarios given by missing pixels at random, missing various smaller square\nregions, and one missing square placed in the center of the image. It turns out\nthat our model achieves high-quality inpainting results on all scenarios.\nPerformance is evaluated using peak signal-to-noise ratio and structural\nsimilarity index on two real-world benchmark datasets, CelebA faces and Paris\nStreetView. The results of our model are compared to biharmonic imputation and\nto some of the other state-of-the-art image inpainting methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 05:55:07 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Va\u0161ata", "Daniel", ""], ["Halama", "Tom\u00e1\u0161", ""], ["Friedjungov\u00e1", "Magda", ""]]}, {"id": "2106.15345", "submitter": "Yuanqi Du", "authors": "Yuanqi Du, Quan Quan, Hu Han, S. Kevin Zhou", "title": "Where is the disease? Semi-supervised pseudo-normality synthesis from an\n  abnormal image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pseudo-normality synthesis, which computationally generates a pseudo-normal\nimage from an abnormal one (e.g., with lesions), is critical in many\nperspectives, from lesion detection, data augmentation to clinical surgery\nsuggestion. However, it is challenging to generate high-quality pseudo-normal\nimages in the absence of the lesion information. Thus, expensive lesion\nsegmentation data have been introduced to provide lesion information for the\ngenerative models and improve the quality of the synthetic images. In this\npaper, we aim to alleviate the need of a large amount of lesion segmentation\ndata when generating pseudo-normal images. We propose a Semi-supervised Medical\nImage generative LEarning network (SMILE) which not only utilizes limited\nmedical images with segmentation masks, but also leverages massive medical\nimages without segmentation masks to generate realistic pseudo-normal images.\nExtensive experiments show that our model outperforms the best state-of-the-art\nmodel by up to 6% for data augmentation task and 3% in generating high-quality\nimages. Moreover, the proposed semi-supervised learning achieves comparable\nmedical image synthesis quality with supervised learning model, using only 50\nof segmentation data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 05:56:41 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Du", "Yuanqi", ""], ["Quan", "Quan", ""], ["Han", "Hu", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2106.15350", "submitter": "Radu Dogaru", "authors": "Radu Dogaru, Ioana Dogaru", "title": "LB-CNN: An Open Source Framework for Fast Training of Light Binary\n  Convolutional Neural Networks using Chainer and Cupy", "comments": "6 pages, includes reference to code (Jupyter - Python notebook)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Light binary convolutional neural networks (LB-CNN) are particularly useful\nwhen implemented in low-energy computing platforms as required in many\nindustrial applications. Herein, a framework for optimizing compact LB-CNN is\nintroduced and its effectiveness is evaluated. The framework is freely\navailable and may run on free-access cloud platforms, thus requiring no major\ninvestments. The optimized model is saved in the standardized .h5 format and\ncan be used as input to specialized tools for further deployment into specific\ntechnologies, thus enabling the rapid development of various intelligent image\nsensors. The main ingredient in accelerating the optimization of our model,\nparticularly the selection of binary convolution kernels, is the Chainer/Cupy\nmachine learning library offering significant speed-ups for training the output\nlayer as an extreme-learning machine. Additional training of the output layer\nusing Keras/Tensorflow is included, as it allows an increase in accuracy.\nResults for widely used datasets including MNIST, GTSRB, ORL, VGG show very\ngood compromise between accuracy and complexity. Particularly, for face\nrecognition problems a carefully optimized LB-CNN model provides up to 100%\naccuracies. Such TinyML solutions are well suited for industrial applications\nrequiring image recognition with low energy consumption.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:40:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Dogaru", "Radu", ""], ["Dogaru", "Ioana", ""]]}, {"id": "2106.15357", "submitter": "Xiaosen Wang", "authors": "Xiaosen Wang, Chuanbiao Song, Liwei Wang, Kun He", "title": "Multi-stage Optimization based Adversarial Training", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of adversarial robustness, there is a common practice that\nadopts the single-step adversarial training for quickly developing\nadversarially robust models. However, the single-step adversarial training is\nmost likely to cause catastrophic overfitting, as after a few training epochs\nit will be hard to generate strong adversarial examples to continuously boost\nthe adversarial robustness. In this work, we aim to avoid the catastrophic\noverfitting by introducing multi-step adversarial examples during the\nsingle-step adversarial training. Then, to balance the large training overhead\nof generating multi-step adversarial examples, we propose a Multi-stage\nOptimization based Adversarial Training (MOAT) method that periodically trains\nthe model on mixed benign examples, single-step adversarial examples, and\nmulti-step adversarial examples stage by stage. In this way, the overall\ntraining overhead is reduced significantly, meanwhile, the model could avoid\ncatastrophic overfitting. Extensive experiments on CIFAR-10 and CIFAR-100\ndatasets demonstrate that under similar amount of training overhead, the\nproposed MOAT exhibits better robustness than either single-step or multi-step\nadversarial training methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 07:59:52 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Xiaosen", ""], ["Song", "Chuanbiao", ""], ["Wang", "Liwei", ""], ["He", "Kun", ""]]}, {"id": "2106.15361", "submitter": "Yusuke Kumakoshi", "authors": "Yusuke Kumakoshi, Shigeaki Onoda, Tetsuya Takahashi, Yuji Yoshimura", "title": "Quantifying urban streetscapes with deep learning: focus on aesthetic\n  evaluation", "comments": "4pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disorder of urban streetscapes would negatively affect people's\nperception of their aesthetic quality. The presence of billboards on building\nfacades has been regarded as an important factor of the disorder, but its\nquantification methodology has not yet been developed in a scalable manner. To\nfill the gap, this paper reports the performance of our deep learning model on\na unique data set prepared in Tokyo to recognize the areas covered by facades\nand billboards in streetscapes, respectively. The model achieved 63.17 % of\naccuracy, measured by Intersection-over-Union (IoU), thus enabling researchers\nand practitioners to obtain insights on urban streetscape design by combining\ndata of people's preferences.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:51:00 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kumakoshi", "Yusuke", ""], ["Onoda", "Shigeaki", ""], ["Takahashi", "Tetsuya", ""], ["Yoshimura", "Yuji", ""]]}, {"id": "2106.15366", "submitter": "Cong Ma", "authors": "Cong Ma", "title": "TANet++: Triple Attention Network with Filtered Pointcloud on 3D\n  Detection", "comments": "3pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  TANet is one of state-of-the-art 3D object detection method on KITTI and JRDB\nbenchmark, the network contains a Triple Attention module and Coarse-to-Fine\nRegression module to improve the robustness and accuracy of 3D Detection.\nHowever, since the original input data (point clouds) contains a lot of noise\nduring collecting the data, which will further affect the training of the\nmodel. For example, the object is far from the robot, the sensor is difficult\nto obtain enough pointcloud. If the objects only contains few point clouds, and\nthe samples are fed into model with the normal samples together during\ntraining, the detector will be difficult to distinguish the individual with few\npointcloud belong to object or background. In this paper, we propose TANet++ to\nimprove the performance on 3D Detection, which adopt a novel training strategy\non training the TANet. In order to reduce the negative impact by the weak\nsamples, the training strategy previously filtered the training data, and then\nthe TANet++ is trained by the rest of data. The experimental results shows that\nAP score of TANet++ is 8.98 higher than TANet on JRDB benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 16:56:35 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 15:42:17 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ma", "Cong", ""]]}, {"id": "2106.15368", "submitter": "Jianqi Ma", "authors": "Jianqi Ma, Shi Guo, Lei Zhang", "title": "Text Prior Guided Scene Text Image Super-resolution", "comments": "Code has been released on https://github.com/mjq11302010044/TPGSR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text image super-resolution (STISR) aims to improve the resolution and\nvisual quality of low-resolution (LR) scene text images, and consequently boost\nthe performance of text recognition. However, most of existing STISR methods\nregard text images as natural scene images, ignoring the categorical\ninformation of text. In this paper, we make an inspiring attempt to embed\ncategorical text prior into STISR model training. Specifically, we adopt the\ncharacter probability sequence as the text prior, which can be obtained\nconveniently from a text recognition model. The text prior provides categorical\nguidance to recover high-resolution (HR) text images. On the other hand, the\nreconstructed HR image can refine the text prior in return. Finally, we present\na multi-stage text prior guided super-resolution (TPGSR) framework for STISR.\nOur experiments on the benchmark TextZoom dataset show that TPGSR can not only\neffectively improve the visual quality of scene text images, but also\nsignificantly improve the text recognition accuracy over existing STISR\nmethods. Our model trained on TextZoom also demonstrates certain generalization\ncapability to the LR images in other datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:52:33 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:14:56 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ma", "Jianqi", ""], ["Guo", "Shi", ""], ["Zhang", "Lei", ""]]}, {"id": "2106.15379", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "title": "Unified Framework for Spectral Dimensionality Reduction, Maximum\n  Variance Unfolding, and Kernel Learning By Semidefinite Programming: Tutorial\n  and Survey", "comments": "To appear as a part of an upcoming textbook on dimensionality\n  reduction and manifold learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a tutorial and survey paper on unification of spectral dimensionality\nreduction methods, kernel learning by Semidefinite Programming (SDP), Maximum\nVariance Unfolding (MVU) or Semidefinite Embedding (SDE), and its variants. We\nfirst explain how the spectral dimensionality reduction methods can be unified\nas kernel Principal Component Analysis (PCA) with different kernels. This\nunification can be interpreted as eigenfunction learning or representation of\nkernel in terms of distance matrix. Then, since the spectral methods are\nunified as kernel PCA, we say let us learn the best kernel for unfolding the\nmanifold of data to its maximum variance. We first briefly introduce kernel\nlearning by SDP for the transduction task. Then, we explain MVU in detail.\nVarious versions of supervised MVU using nearest neighbors graph, by class-wise\nunfolding, by Fisher criterion, and by colored MVU are explained. We also\nexplain out-of-sample extension of MVU using eigenfunctions and kernel mapping.\nFinally, we introduce other variants of MVU including action respecting\nembedding, relaxed MVU, and landmark MVU for big data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 13:09:40 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2106.15382", "submitter": "Quanxue Gao", "authors": "Tianyu Jiang, Quanxue Gao", "title": "Multiple Graph Learning for Scalable Multi-view Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based multi-view clustering has become an active topic due to the\nefficiency in characterizing both the complex structure and relationship\nbetween multimedia data. However, existing methods have the following\nshortcomings: (1) They are inefficient or even fail for graph learning in large\nscale due to the graph construction and eigen-decomposition. (2) They cannot\nwell exploit both the complementary information and spatial structure embedded\nin graphs of different views. To well exploit complementary information and\ntackle the scalability issue plaguing graph-based multi-view clustering, we\npropose an efficient multiple graph learning model via a small number of anchor\npoints and tensor Schatten p-norm minimization. Specifically, we construct a\nhidden and tractable large graph by anchor graph for each view and well exploit\ncomplementary information embedded in anchor graphs of different views by\ntensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,\nwhich scales linearly with the data size, to solve our proposed model.\nExtensive experimental results on several datasets indicate that our proposed\nmethod outperforms some state-of-the-art multi-view clustering algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 13:10:56 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Jiang", "Tianyu", ""], ["Gao", "Quanxue", ""]]}, {"id": "2106.15395", "submitter": "Zhiyang Lu", "authors": "Zhiyang Lu, Zheng Li, Jun Wang, Jun shi, Dinggang Shen", "title": "Two-Stage Self-Supervised Cycle-Consistency Network for Reconstruction\n  of Thin-Slice MR Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The thick-slice magnetic resonance (MR) images are often structurally blurred\nin coronal and sagittal views, which causes harm to diagnosis and image\npost-processing. Deep learning (DL) has shown great potential to re-construct\nthe high-resolution (HR) thin-slice MR images from those low-resolution (LR)\ncases, which we refer to as the slice interpolation task in this work. However,\nsince it is generally difficult to sample abundant paired LR-HR MR images, the\nclassical fully supervised DL-based models cannot be effectively trained to get\nrobust performance. To this end, we propose a novel Two-stage Self-supervised\nCycle-consistency Network (TSCNet) for MR slice interpolation, in which a\ntwo-stage self-supervised learning (SSL) strategy is developed for unsupervised\nDL network training. The paired LR-HR images are synthesized along the sagittal\nand coronal directions of input LR images for network pretraining in the\nfirst-stage SSL, and then a cyclic in-terpolation procedure based on triplet\naxial slices is designed in the second-stage SSL for further refinement. More\ntraining samples with rich contexts along all directions are exploited as\nguidance to guarantee the improved in-terpolation performance. Moreover, a new\ncycle-consistency constraint is proposed to supervise this cyclic procedure,\nwhich encourages the network to reconstruct more realistic HR images. The\nexperimental results on a real MRI dataset indicate that TSCNet achieves\nsuperior performance over the conventional and other SSL-based algorithms, and\nobtains competitive quali-tative and quantitative results compared with the\nfully supervised algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 13:29:18 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Lu", "Zhiyang", ""], ["Li", "Zheng", ""], ["Wang", "Jun", ""], ["shi", "Jun", ""], ["Shen", "Dinggang", ""]]}, {"id": "2106.15402", "submitter": "Kunchi Liu", "authors": "Wei Zhuo, Kunchi Liu, Taofeng Xue, Beihong Jin, Beibei Li, Xinzhou\n  Dong, He Chen, Wenhai Pan, Xuejian Zhang, Shuo Zhou", "title": "A Behavior-aware Graph Convolution Network Model for Video\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions between users and videos are the major data source of performing\nvideo recommendation. Despite lots of existing recommendation methods, user\nbehaviors on videos, which imply the complex relations between users and\nvideos, are still far from being fully explored. In the paper, we present a\nmodel named Sagittarius. Sagittarius adopts a graph convolutional neural\nnetwork to capture the influence between users and videos. In particular,\nSagittarius differentiates between different user behaviors by weighting and\nfuses the semantics of user behaviors into the embeddings of users and videos.\nMoreover, Sagittarius combines multiple optimization objectives to learn user\nand video embeddings and then achieves the video recommendation by the learned\nuser and video embeddings. The experimental results on multiple datasets show\nthat Sagittarius outperforms several state-of-the-art models in terms of\nrecall, unique recall and NDCG.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 08:24:45 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhuo", "Wei", ""], ["Liu", "Kunchi", ""], ["Xue", "Taofeng", ""], ["Jin", "Beihong", ""], ["Li", "Beibei", ""], ["Dong", "Xinzhou", ""], ["Chen", "He", ""], ["Pan", "Wenhai", ""], ["Zhang", "Xuejian", ""], ["Zhou", "Shuo", ""]]}, {"id": "2106.15409", "submitter": "Charalampos Symeonidis", "authors": "C. Symeonidis, P. Nousi, P. Tosidis, K. Tsampazis, N. Passalis, A.\n  Tefas, N. Nikolaidis", "title": "Efficient Realistic Data Generation Framework leveraging Deep\n  Learning-based Human Digitization", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-80568-5", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of supervised deep learning algorithms depends significantly\non the scale, quality and diversity of the data used for their training.\nCollecting and manually annotating large amount of data can be both\ntime-consuming and costly tasks to perform. In the case of tasks related to\nvisual human-centric perception, the collection and distribution of such data\nmay also face restrictions due to legislation regarding privacy. In addition,\nthe design and testing of complex systems, e.g., robots, which often employ\ndeep learning-based perception models, may face severe difficulties as even\nstate-of-the-art methods trained on real and large-scale datasets cannot always\nperform adequately due to not having been adapted to the visual differences\nbetween the virtual and the real world data. As an attempt to tackle and\nmitigate the effect of these issues, we present a method that automatically\ngenerates realistic synthetic data with annotations for a) person detection, b)\nface recognition, and c) human pose estimation. The proposed method takes as\ninput real background images and populates them with human figures in various\nposes. Instead of using hand-made 3D human models, we propose the use of models\ngenerated through deep learning methods, further reducing the dataset creation\ncosts, while maintaining a high level of realism. In addition, we provide\nopen-source and easy to use tools that implement the proposed pipeline,\nallowing for generating highly-realistic synthetic datasets for a variety of\ntasks. A benchmarking and evaluation in the corresponding tasks shows that\nsynthetic data can be effectively used as a supplement to real data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:07:31 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 20:05:23 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Symeonidis", "C.", ""], ["Nousi", "P.", ""], ["Tosidis", "P.", ""], ["Tsampazis", "K.", ""], ["Passalis", "N.", ""], ["Tefas", "A.", ""], ["Nikolaidis", "N.", ""]]}, {"id": "2106.15413", "submitter": "Jie Li", "authors": "Jie Li, Laiyan Ding and Rui Huang", "title": "IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation\n  through Iterative Mutual Enhancement", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D semantic scene completion and 2D semantic segmentation are two tightly\ncorrelated tasks that are both essential for indoor scene understanding,\nbecause they predict the same semantic classes, using positively correlated\nhigh-level features. Current methods use 2D features extracted from early-fused\nRGB-D images for 2D segmentation to improve 3D scene completion. We argue that\nthis sequential scheme does not ensure these two tasks fully benefit each\nother, and present an Iterative Mutual Enhancement Network (IMENet) to solve\nthem jointly, which interactively refines the two tasks at the late prediction\nstage. Specifically, two refinement modules are developed under a unified\nframework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP)\nmodule, which receives the projection from the current 3D predictions to refine\nthe 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is\nproposed to leverage the reprojected results from 2D predictions to update the\ncoarse 3D predictions. This iterative fusion happens to the stable high-level\nfeatures of both tasks at a late stage. Extensive experiments on NYU and NYUCAD\ndatasets verify the effectiveness of the proposed iterative late fusion scheme,\nand our approach outperforms the state of the art on both 3D semantic scene\ncompletion and 2D semantic segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 13:34:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Li", "Jie", ""], ["Ding", "Laiyan", ""], ["Huang", "Rui", ""]]}, {"id": "2106.15420", "submitter": "Vineet Kotariya", "authors": "Vineet Kotariya, Udayan Ganguly", "title": "Spiking-GAN: A Spiking Generative Adversarial Network Using\n  Time-To-First-Spike Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have shown great potential in solving deep\nlearning problems in an energy-efficient manner. However, they are still\nlimited to simple classification tasks. In this paper, we propose Spiking-GAN,\nthe first spike-based Generative Adversarial Network (GAN). It employs a kind\nof temporal coding scheme called time-to-first-spike coding. We train it using\napproximate backpropagation in the temporal domain. We use simple\nintegrate-and-fire (IF) neurons with very high refractory period for our\nnetwork which ensures a maximum of one spike per neuron. This makes the model\nmuch sparser than a spike rate-based system. Our modified temporal loss\nfunction called 'Aggressive TTFS' improves the inference time of the network by\nover 33% and reduces the number of spikes in the network by more than 11%\ncompared to previous works. Our experiments show that on training the network\non the MNIST dataset using this approach, we can generate high quality samples.\nThereby demonstrating the potential of this framework for solving such problems\nin the spiking domain.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 13:43:07 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kotariya", "Vineet", ""], ["Ganguly", "Udayan", ""]]}, {"id": "2106.15448", "submitter": "Caleb Robinson", "authors": "Caleb Robinson, Anthony Ortiz, Lacey Hughey, Jared A. Stabach, Juan M.\n  Lavista Ferres", "title": "Detecting Cattle and Elk in the Wild from Space", "comments": "Presented at the KDD 2021 Fragile Earth Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Localizing and counting large ungulates -- hoofed mammals like cows and elk\n-- in very high-resolution satellite imagery is an important task for\nsupporting ecological studies. Prior work has shown that this is feasible with\ndeep learning based methods and sub-meter multi-spectral satellite imagery. We\nextend this line of work by proposing a baseline method, CowNet, that\nsimultaneously estimates the number of animals in an image (counts), as well as\npredicts their location at a pixel level (localizes). We also propose an\nmethodology for evaluating such models on counting and localization tasks\nacross large scenes that takes the uncertainty of noisy labels and the\ninformation needed by stakeholders in ecological monitoring tasks into account.\nFinally, we benchmark our baseline method with state of the art vision methods\nfor counting objects in scenes. We specifically test the temporal\ngeneralization of the resulting models over a large landscape in Point Reyes\nSeashore, CA. We find that the LC-FCN model performs the best and achieves an\naverage precision between 0.56 and 0.61 and an average recall between 0.78 and\n0.92 over three held out test scenes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 14:35:23 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Robinson", "Caleb", ""], ["Ortiz", "Anthony", ""], ["Hughey", "Lacey", ""], ["Stabach", "Jared A.", ""], ["Ferres", "Juan M. Lavista", ""]]}, {"id": "2106.15453", "submitter": "Varsha Suresh", "authors": "Yan San Kong, Varsha Suresh, Jonathan Soh, Desmond C. Ong", "title": "A Systematic Evaluation of Domain Adaptation in Facial Expression\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Facial Expression Recognition is a commercially important application, but\none common limitation is that applications often require making predictions on\nout-of-sample distributions, where target images may have very different\nproperties from the images that the model was trained on. How well, or badly,\ndo these models do on unseen target domains? In this paper, we provide a\nsystematic evaluation of domain adaptation in facial expression recognition.\nUsing state-of-the-art transfer learning techniques and six commonly-used\nfacial expression datasets (three collected in the lab and three\n\"in-the-wild\"), we conduct extensive round-robin experiments to examine the\nclassification accuracies for a state-of-the-art CNN model. We also perform\nmulti-source experiments where we examine a model's ability to transfer from\nmultiple source datasets, including (i) within-setting (e.g., lab to lab), (ii)\ncross-setting (e.g., in-the-wild to lab), (iii) mixed-setting (e.g., lab and\nwild to lab) transfer learning experiments. We find sobering results that the\naccuracy of transfer learning is not high, and varies idiosyncratically with\nthe target dataset, and to a lesser extent the source dataset. Generally, the\nbest settings for transfer include fine-tuning the weights of a pre-trained\nmodel, and we find that training with more datasets, regardless of setting,\nimproves transfer performance. We end with a discussion of the need for more --\nand regular -- systematic investigations into the generalizability of FER\nmodels, especially for deployed applications.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 14:41:19 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kong", "Yan San", ""], ["Suresh", "Varsha", ""], ["Soh", "Jonathan", ""], ["Ong", "Desmond C.", ""]]}, {"id": "2106.15475", "submitter": "Bidur Khanal", "authors": "Bidur Khanal and Christopher Kanan", "title": "How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Incorrectly labeled examples, or label noise, is common in real-world\ncomputer vision datasets. While the impact of label noise on learning in deep\nneural networks has been studied in prior work, these studies have exclusively\nfocused on homogeneous label noise, i.e., the degree of label noise is the same\nacross all categories. However, in the real-world, label noise is often\nheterogeneous, with some categories being affected to a greater extent than\nothers. Here, we address this gap in the literature. We hypothesized that\nheterogeneous label noise would only affect the classes that had label noise\nunless there was transfer from those classes to the classes without label\nnoise. To test this hypothesis, we designed a series of computer vision studies\nusing MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous\nlabel noise during the training of multi-class, multi-task, and multi-label\nsystems. Our results provide evidence in support of our hypothesis: label noise\nonly affects the class affected by it unless there is transfer.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 14:58:46 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Khanal", "Bidur", ""], ["Kanan", "Christopher", ""]]}, {"id": "2106.15507", "submitter": "Amit Chatterjee", "authors": "Amit Chatterjee, Jitendra Dhanotiya, Vimal Bhatia and Shashi Prakash", "title": "Study of visual processing techniques for dynamic speckles: a\n  comparative analysis", "comments": null, "journal-ref": "OSI ICLLT-2016, Tezpur University", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Main visual techniques used to obtain information from speckle patterns are\nFujii method, generalized difference, weighted generalized difference, mean\nwindowed difference, structural function (SF), modified SF, etc. In this work,\na comparative analysis of major visual techniques for natural gum sample is\ncarried out. Obtained results conclusively establish SF based method as an\noptimum tool for visual inspection of dynamic speckle data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:15:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chatterjee", "Amit", ""], ["Dhanotiya", "Jitendra", ""], ["Bhatia", "Vimal", ""], ["Prakash", "Shashi", ""]]}, {"id": "2106.15510", "submitter": "Kai Li", "authors": "Kai Li, Bo Wang, Yingjie Tian, and Zhiquan Qi", "title": "Fast and Accurate Road Crack Detection Based on Adaptive Cost-Sensitive\n  Loss Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous detection problems in computer vision, including road crack\ndetection, suffer from exceedingly foreground-background imbalance.\nFortunately, modification of loss function appears to solve this puzzle once\nand for all. In this paper, we propose a pixel-based adaptive weighted\ncross-entropy loss in conjunction with Jaccard distance to facilitate\nhigh-quality pixel-level road crack detection. Our work profoundly demonstrates\nthe influence of loss functions on detection outcomes, and sheds light on the\nsophisticated consecutive improvements in the realm of crack detection.\nSpecifically, to verify the effectiveness of the proposed loss, we conduct\nextensive experiments on four public databases, i.e., CrackForest, AigleRN,\nCrack360, and BJN260. Compared with the vanilla weighted cross-entropy, the\nproposed loss significantly speeds up the training process while retaining the\ntest accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 15:39:37 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Li", "Kai", ""], ["Wang", "Bo", ""], ["Tian", "Yingjie", ""], ["Qi", "Zhiquan", ""]]}, {"id": "2106.15537", "submitter": "Narinder Punn", "authors": "Gaurav Rajput, Narinder Singh punn, Sanjay Kumar Sonbhadra, Sonali\n  Agarwal", "title": "Hate speech detection using static BERT embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With increasing popularity of social media platforms hate speech is emerging\nas a major concern, where it expresses abusive speech that targets specific\ngroup characteristics, such as gender, religion or ethnicity to spread\nviolence. Earlier people use to verbally deliver hate speeches but now with the\nexpansion of technology, some people are deliberately using social media\nplatforms to spread hate by posting, sharing, commenting, etc. Whether it is\nChristchurch mosque shootings or hate crimes against Asians in west, it has\nbeen observed that the convicts are very much influenced from hate text present\nonline. Even though AI systems are in place to flag such text but one of the\nkey challenges is to reduce the false positive rate (marking non hate as hate),\nso that these systems can detect hate speech without undermining the freedom of\nexpression. In this paper, we use ETHOS hate speech detection dataset and\nanalyze the performance of hate speech detection classifier by replacing or\nintegrating the word embeddings (fastText (FT), GloVe (GV) or FT + GV) with\nstatic BERT embeddings (BE). With the extensive experimental trails it is\nobserved that the neural network performed better with static BE compared to\nusing FT, GV or FT + GV as word embeddings. In comparison to fine-tuned BERT,\none metric that significantly improved is specificity.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:17:10 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Rajput", "Gaurav", ""], ["punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""]]}, {"id": "2106.15542", "submitter": "Uddeshya Upadhyay", "authors": "Uddeshya Upadhyay, Yanbei Chen, Tobias Hepp, Sergios Gatidis, Zeynep\n  Akata", "title": "Uncertainty-Guided Progressive GANs for Medical Image Translation", "comments": "accepted at MICCAI 2021, code is released here:\n  https://github.com/ExplainableML/UncerGuidedI2I", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-to-image translation plays a vital role in tackling various medical\nimaging tasks such as attenuation correction, motion correction, undersampled\nreconstruction, and denoising. Generative adversarial networks have been shown\nto achieve the state-of-the-art in generating high fidelity images for these\ntasks. However, the state-of-the-art GAN-based frameworks do not estimate the\nuncertainty in the predictions made by the network that is essential for making\ninformed medical decisions and subsequent revision by medical experts and has\nrecently been shown to improve the performance and interpretability of the\nmodel. In this work, we propose an uncertainty-guided progressive learning\nscheme for image-to-image translation. By incorporating aleatoric uncertainty\nas attention maps for GANs trained in a progressive manner, we generate images\nof increasing fidelity progressively. We demonstrate the efficacy of our model\non three challenging medical image translation tasks, including PET to CT\ntranslation, undersampled MRI reconstruction, and MRI motion artefact\ncorrection. Our model generalizes well in three different tasks and improves\nperformance over state of the art under full-supervision and weak-supervision\nwith limited data. Code is released here:\nhttps://github.com/ExplainableML/UncerGuidedI2I\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:26:12 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 05:13:09 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Upadhyay", "Uddeshya", ""], ["Chen", "Yanbei", ""], ["Hepp", "Tobias", ""], ["Gatidis", "Sergios", ""], ["Akata", "Zeynep", ""]]}, {"id": "2106.15550", "submitter": "Shoya Matsumori", "authors": "Shoya Matsumori, Kosuke Shingyouchi, Yuki Abe, Yosuke Fukuchi, Komei\n  Sugiura, and Michita Imai", "title": "Unified Questioner Transformer for Descriptive Question Generation in\n  Goal-Oriented Visual Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building an interactive artificial intelligence that can ask questions about\nthe real world is one of the biggest challenges for vision and language\nproblems. In particular, goal-oriented visual dialogue, where the aim of the\nagent is to seek information by asking questions during a turn-taking dialogue,\nhas been gaining scholarly attention recently. While several existing models\nbased on the GuessWhat?! dataset have been proposed, the Questioner typically\nasks simple category-based questions or absolute spatial questions. This might\nbe problematic for complex scenes where the objects share attributes or in\ncases where descriptive questions are required to distinguish objects. In this\npaper, we propose a novel Questioner architecture, called Unified Questioner\nTransformer (UniQer), for descriptive question generation with referring\nexpressions. In addition, we build a goal-oriented visual dialogue task called\nCLEVR Ask. It synthesizes complex scenes that require the Questioner to\ngenerate descriptive questions. We train our model with two variants of CLEVR\nAsk datasets. The results of the quantitative and qualitative evaluations show\nthat UniQer outperforms the baseline.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:36:34 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Matsumori", "Shoya", ""], ["Shingyouchi", "Kosuke", ""], ["Abe", "Yuki", ""], ["Fukuchi", "Yosuke", ""], ["Sugiura", "Komei", ""], ["Imai", "Michita", ""]]}, {"id": "2106.15553", "submitter": "Anett Hoppe", "authors": "Anett Hoppe and David Morris and Ralph Ewerth", "title": "Evaluation of Automated Image Descriptions for Visually Impaired\n  Students", "comments": "6 pages, 12 references. Accepted for publication at the 22nd\n  International Conference on Artificial Intelligence in Education (AIED 2021),\n  June 14-16 2021, Utrecht, The Netherlands", "journal-ref": "Hoppe A., Morris D., Ewerth R. (2021) Evaluation of Automated\n  Image Descriptions for Visually Impaired Students. In: Roll I., McNamara D.,\n  Sosnovsky S., Luckin R., Dimitrova V. (eds) AIED 2021. LNCS vol 12749.\n  Springer, Cham", "doi": "10.1007/978-3-030-78270-2_35", "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illustrations are widely used in education, and sometimes, alternatives are\nnot available for visually impaired students. Therefore, those students would\nbenefit greatly from an automatic illustration description system, but only if\nthose descriptions were complete, correct, and easily understandable using a\nscreenreader. In this paper, we report on a study for the assessment of\nautomated image descriptions. We interviewed experts to establish evaluation\ncriteria, which we then used to create an evaluation questionnaire for sighted\nnon-expert raters, and description templates. We used this questionnaire to\nevaluate the quality of descriptions which could be generated with a\ntemplate-based automatic image describer. We present evidence that these\ntemplates have the potential to generate useful descriptions, and that the\nquestionnaire identifies problems with description templates.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:40:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hoppe", "Anett", ""], ["Morris", "David", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2106.15575", "submitter": "Uddeshya Upadhyay", "authors": "Uddeshya Upadhyay, Suyash Awate", "title": "A Mixed-Supervision Multilevel GAN Framework for Image Quality\n  Enhancement", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks for image quality enhancement typically need large\nquantities of highly-curated training data comprising pairs of low-quality\nimages and their corresponding high-quality images. While high-quality image\nacquisition is typically expensive and time-consuming, medium-quality images\nare faster to acquire, at lower equipment costs, and available in larger\nquantities. Thus, we propose a novel generative adversarial network (GAN) that\ncan leverage training data at multiple levels of quality (e.g., high and medium\nquality) to improve performance while limiting costs of data curation. We apply\nour mixed-supervision GAN to (i) super-resolve histopathology images and (ii)\nenhance laparoscopy images by combining super-resolution and surgical smoke\nremoval. Results on large clinical and pre-clinical datasets show the benefits\nof our mixed-supervision GAN over the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:10:41 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Upadhyay", "Uddeshya", ""], ["Awate", "Suyash", ""]]}, {"id": "2106.15597", "submitter": "Dewen Zeng", "authors": "Dewen Zeng, Mingqi Li, Yukun Ding, Xiaowei Xu, Qiu Xie, Ruixue Xu,\n  Hongwen Fei, Meiping Huang, Jian Zhuang and Yiyu Shi", "title": "Segmentation with Multiple Acceptable Annotations: A Case Study of\n  Myocardial Segmentation in Contrast Echocardiography", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing deep learning-based frameworks for image segmentation assume\nthat a unique ground truth is known and can be used for performance evaluation.\nThis is true for many applications, but not all. Myocardial segmentation of\nMyocardial Contrast Echocardiography (MCE), a critical task in automatic\nmyocardial perfusion analysis, is an example. Due to the low resolution and\nserious artifacts in MCE data, annotations from different cardiologists can\nvary significantly, and it is hard to tell which one is the best. In this case,\nhow can we find a good way to evaluate segmentation performance and how do we\ntrain the neural network? In this paper, we address the first problem by\nproposing a new extended Dice to effectively evaluate the segmentation\nperformance when multiple accepted ground truth is available. Then based on our\nproposed metric, we solve the second problem by further incorporating the new\nmetric into a loss function that enables neural networks to flexibly learn\ngeneral features of myocardium. Experiment results on our clinical MCE data set\ndemonstrate that the neural network trained with the proposed loss function\noutperforms those existing ones that try to obtain a unique ground truth from\nmultiple annotations, both quantitatively and qualitatively. Finally, our\ngrading study shows that using extended Dice as an evaluation metric can better\nidentify segmentation results that need manual correction compared with using\nDice.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:32:24 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zeng", "Dewen", ""], ["Li", "Mingqi", ""], ["Ding", "Yukun", ""], ["Xu", "Xiaowei", ""], ["Xie", "Qiu", ""], ["Xu", "Ruixue", ""], ["Fei", "Hongwen", ""], ["Huang", "Meiping", ""], ["Zhuang", "Jian", ""], ["Shi", "Yiyu", ""]]}, {"id": "2106.15599", "submitter": "Nirmalya Thakur", "authors": "Nirmalya Thakur and Chia Y. Han", "title": "Framework for an Intelligent Affect Aware Smart Home Environment for\n  Elderly People", "comments": null, "journal-ref": "International Journal of Recent Trends in Human Computer\n  Interaction (IJHCI), Volume - 9, Issue 1, 2019, pp. 23-43", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.ET cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The population of elderly people has been increasing at a rapid rate over the\nlast few decades and their population is expected to further increase in the\nupcoming future. Their increasing population is associated with their\nincreasing needs due to problems like physical disabilities, cognitive issues,\nweakened memory and disorganized behavior, that elderly people face with\nincreasing age. To reduce their financial burden on the world economy and to\nenhance their quality of life, it is essential to develop technology-based\nsolutions that are adaptive, assistive and intelligent in nature. Intelligent\nAffect Aware Systems that can not only analyze but also predict the behavior of\nelderly people in the context of their day to day interactions with technology\nin an IoT-based environment, holds immense potential for serving as a long-term\nsolution for improving the user experience of elderly in smart homes. This work\ntherefore proposes the framework for an Intelligent Affect Aware environment\nfor elderly people that can not only analyze the affective components of their\ninteractions but also predict their likely user experience even before they\nstart engaging in any activity in the given smart home environment. This\nforecasting of user experience would provide scope for enhancing the same,\nthereby increasing the assistive and adaptive nature of such intelligent\nsystems. To uphold the efficacy of this proposed framework for improving the\nquality of life of elderly people in smart homes, it has been tested on three\ndatasets and the results are presented and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:34:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Thakur", "Nirmalya", ""], ["Han", "Chia Y.", ""]]}, {"id": "2106.15610", "submitter": "Aviv Gabbay", "authors": "Aviv Gabbay, Niv Cohen, Yedid Hoshen", "title": "An Image is Worth More Than a Thousand Words: Towards Disentanglement in\n  the Wild", "comments": "Project page: http://www.vision.huji.ac.il/zerodim", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised disentanglement has been shown to be theoretically impossible\nwithout inductive biases on the models and the data. As an alternative\napproach, recent methods rely on limited supervision to disentangle the factors\nof variation and allow their identifiability. While annotating the true\ngenerative factors is only required for a limited number of observations, we\nargue that it is infeasible to enumerate all the factors of variation that\ndescribe a real-world image distribution. To this end, we propose a method for\ndisentangling a set of factors which are only partially labeled, as well as\nseparating the complementary set of residual factors that are never explicitly\nspecified. Our success in this challenging setting, demonstrated on synthetic\nbenchmarks, gives rise to leveraging off-the-shelf image descriptors to\npartially annotate a subset of attributes in real image domains (e.g. of human\nfaces) with minimal manual effort. Specifically, we use a recent language-image\nembedding model (CLIP) to annotate a set of attributes of interest in a\nzero-shot manner and demonstrate state-of-the-art disentangled image\nmanipulation results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:54:24 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gabbay", "Aviv", ""], ["Cohen", "Niv", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2106.15648", "submitter": "Georgios Georgakis", "authors": "Georgios Georgakis, Bernadette Bucher, Karl Schmeckpeper, Siddharth\n  Singh, Kostas Daniilidis", "title": "Learning to Map for Active Semantic Goal Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of object goal navigation in unseen environments. In\nour view, solving this problem requires learning of contextual semantic priors,\na challenging endeavour given the spatial and semantic variability of indoor\nenvironments. Current methods learn to implicitly encode these priors through\ngoal-oriented navigation policy functions operating on spatial representations\nthat are limited to the agent's observable areas. In this work, we propose a\nnovel framework that actively learns to generate semantic maps outside the\nfield of view of the agent and leverages the uncertainty over the semantic\nclasses in the unobserved areas to decide on long term goals. We demonstrate\nthat through this spatial prediction strategy, we are able to learn semantic\npriors in scenes that can be leveraged in unknown environments. Additionally,\nwe show how different objectives can be defined by balancing exploration with\nexploitation during searching for semantic targets. Our method is validated in\nthe visually realistic environments offered by the Matterport3D dataset and\nshow state of the art results on the object goal navigation task.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 18:01:30 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Georgakis", "Georgios", ""], ["Bucher", "Bernadette", ""], ["Schmeckpeper", "Karl", ""], ["Singh", "Siddharth", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "2106.15681", "submitter": "Jordan Malof", "authors": "Yang Xu, Bohao Huang, Xiong Luo, Kyle Bradbury, and Jordan M. Malof", "title": "SIMPL: Generating Synthetic Overhead Imagery to Address Zero-shot and\n  Few-Shot Detection Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep neural networks (DNNs) have achieved tremendous success for\nobject detection in overhead (e.g., satellite) imagery. One ongoing challenge\nhowever is the acquisition of training data, due to high costs of obtaining\nsatellite imagery and annotating objects in it. In this work we present a\nsimple approach - termed Synthetic object IMPLantation (SIMPL) - to easily and\nrapidly generate large quantities of synthetic overhead training data for\ncustom target objects. We demonstrate the effectiveness of using SIMPL\nsynthetic imagery for training DNNs in zero-shot scenarios where no real\nimagery is available; and few-shot learning scenarios, where limited real-world\nimagery is available. We also conduct experiments to study the sensitivity of\nSIMPL's effectiveness to some key design parameters, providing users for\ninsights when designing synthetic imagery for custom objects. We release a\nsoftware implementation of our SIMPL approach so that others can build upon it,\nor use it for their own custom problems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 19:06:05 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Xu", "Yang", ""], ["Huang", "Bohao", ""], ["Luo", "Xiong", ""], ["Bradbury", "Kyle", ""], ["Malof", "Jordan M.", ""]]}, {"id": "2106.15686", "submitter": "Poorya Aghdaie", "authors": "Poorya Aghdaie, Baaria Chaudhary, Sobhan Soleymani, Jeremy Dawson,\n  Nasser M. Nasrabadi", "title": "Attention Aware Wavelet-based Detection of Morphed Face Images", "comments": "IJCB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphed images have exploited loopholes in the face recognition checkpoints,\ne.g., Credential Authentication Technology (CAT), used by Transportation\nSecurity Administration (TSA), which is a non-trivial security concern. To\novercome the risks incurred due to morphed presentations, we propose a\nwavelet-based morph detection methodology which adopts an end-to-end trainable\nsoft attention mechanism . Our attention-based deep neural network (DNN)\nfocuses on the salient Regions of Interest (ROI) which have the most spatial\nsupport for morph detector decision function, i.e, morph class binary softmax\noutput. A retrospective of morph synthesizing procedure aids us to speculate\nthe ROI as regions around facial landmarks , particularly for the case of\nlandmark-based morphing techniques. Moreover, our attention-based DNN is\nadapted to the wavelet space, where inputs of the network are coarse-to-fine\nspectral representations, 48 stacked wavelet sub-bands to be exact. We evaluate\nperformance of the proposed framework using three datasets, VISAPP17, LMA, and\nMorGAN. In addition, as attention maps can be a robust indicator whether a\nprobe image under investigation is genuine or counterfeit, we analyze the\nestimated attention maps for both a bona fide image and its corresponding\nmorphed image. Finally, we present an ablation study on the efficacy of\nutilizing attention mechanism for the sake of morph detection.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 19:29:19 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 19:46:56 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Aghdaie", "Poorya", ""], ["Chaudhary", "Baaria", ""], ["Soleymani", "Sobhan", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2106.15693", "submitter": "Teofilo de Campos", "authors": "Tiago de C. G. Pereira, Teofilo E. de Campos", "title": "Domain adaptation for person re-identification on new unlabeled data\n  using AlignedReID++", "comments": "9 pages; 4 figues; built upon work published in VISAPP 2020 (best\n  student paper award)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the world where big data reigns and there is plenty of hardware prepared\nto gather a huge amount of non structured data, data acquisition is no longer a\nproblem. Surveillance cameras are ubiquitous and they capture huge numbers of\npeople walking across different scenes. However, extracting value from this\ndata is challenging, specially for tasks that involve human images, such as\nface recognition and person re-identification. Annotation of this kind of data\nis a challenging and expensive task. In this work we propose a domain\nadaptation workflow to allow CNNs that were trained in one domain to be applied\nto another domain without the need for new annotation of the target data. Our\nmethod uses AlignedReID++ as the baseline, trained using a Triplet loss with\nbatch hard. Domain adaptation is done by using pseudo-labels generated using an\nunsupervised learning strategy. Our results show that domain adaptation\ntechniques really improve the performance of the CNN when applied in the target\ndomain.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 19:58:04 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Pereira", "Tiago de C. G.", ""], ["de Campos", "Teofilo E.", ""]]}, {"id": "2106.15707", "submitter": "Guang Yang A", "authors": "Yinzhe Wu, Zeyu Tang, Binghuan Li, David Firmin, Guang Yang", "title": "Recent Advances in Fibrosis and Scar Segmentation from Cardiac MRI: A\n  State-of-the-Art Review and Future Perspectives", "comments": "3 figure, 8 tables, 46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Segmentation of cardiac fibrosis and scar are essential for clinical\ndiagnosis and can provide invaluable guidance for the treatment of cardiac\ndiseases. Late Gadolinium enhancement (LGE) cardiovascular magnetic resonance\n(CMR) has been successful for its efficacy in guiding the clinical diagnosis\nand treatment reliably. For LGE CMR, many methods have demonstrated success in\naccurately segmenting scarring regions. Co-registration with other\nnon-contrast-agent (non-CA) modalities, balanced steady-state free precession\n(bSSFP) and cine magnetic resonance imaging (MRI) for example, can further\nenhance the efficacy of automated segmentation of cardiac anatomies. Many\nconventional methods have been proposed to provide automated or semi-automated\nsegmentation of scars. With the development of deep learning in recent years,\nwe can also see more advanced methods that are more efficient in providing more\naccurate segmentations. This paper conducts a state-of-the-art review of\nconventional and current state-of-the-art approaches utilising different\nmodalities for accurate cardiac fibrosis and scar segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 11:30:35 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wu", "Yinzhe", ""], ["Tang", "Zeyu", ""], ["Li", "Binghuan", ""], ["Firmin", "David", ""], ["Yang", "Guang", ""]]}, {"id": "2106.15711", "submitter": "Christopher Xie", "authors": "Christopher Xie, Arsalan Mousavian, Yu Xiang, Dieter Fox", "title": "RICE: Refining Instance Masks in Cluttered Environments with Graph\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmenting unseen object instances in cluttered environments is an important\ncapability that robots need when functioning in unstructured environments.\nWhile previous methods have exhibited promising results, they still tend to\nprovide incorrect results in highly cluttered scenes. We postulate that a\nnetwork architecture that encodes relations between objects at a high-level can\nbe beneficial. Thus, in this work, we propose a novel framework that refines\nthe output of such methods by utilizing a graph-based representation of\ninstance masks. We train deep networks capable of sampling smart perturbations\nto the segmentations, and a graph neural network, which can encode relations\nbetween objects, to evaluate the perturbed segmentations. Our proposed method\nis orthogonal to previous works and achieves state-of-the-art performance when\ncombined with them. We demonstrate an application that uses uncertainty\nestimates generated by our method to guide a manipulator, leading to efficient\nunderstanding of cluttered scenes. Code, models, and video can be found at\nhttps://github.com/chrisdxie/rice .\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 20:29:29 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Xie", "Christopher", ""], ["Mousavian", "Arsalan", ""], ["Xiang", "Yu", ""], ["Fox", "Dieter", ""]]}, {"id": "2106.15716", "submitter": "C.B. Scott", "authors": "Cory Braker Scott, Eric Mjolsness, Diane Oyen, Chie Kodera, David\n  Bouchez, and Magalie Uyttewaal", "title": "Diff2Dist: Learning Spectrally Distinct Edge Functions, with\n  Applications to Cell Morphology Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.MG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for learning \"spectrally descriptive\" edge weights for\ngraphs. We generalize a previously known distance measure on graphs (Graph\nDiffusion Distance), thereby allowing it to be tuned to minimize an arbitrary\nloss function. Because all steps involved in calculating this modified GDD are\ndifferentiable, we demonstrate that it is possible for a small neural network\nmodel to learn edge weights which minimize loss. GDD alone does not effectively\ndiscriminate between graphs constructed from shoot apical meristem images of\nwild-type vs. mutant \\emph{Arabidopsis thaliana} specimens. However, training\nedge weights and kernel parameters with contrastive loss produces a learned\ndistance metric with large margins between these graph categories. We\ndemonstrate this by showing improved performance of a simple\nk-nearest-neighbors classifier on the learned distance matrix. We also\ndemonstrate a further application of this method to biological image analysis:\nonce trained, we use our model to compute the distance between the biological\ngraphs and a set of graphs output by a cell division simulator. This allows us\nto identify simulation parameter regimes which are similar to each class of\ngraph in our original dataset.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 20:40:22 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Scott", "Cory Braker", ""], ["Mjolsness", "Eric", ""], ["Oyen", "Diane", ""], ["Kodera", "Chie", ""], ["Bouchez", "David", ""], ["Uyttewaal", "Magalie", ""]]}, {"id": "2106.15753", "submitter": "Liming Wu", "authors": "Liming Wu, Shuo Han, Alain Chen, Paul Salama, Kenneth W. Dunn, Edward\n  J. Delp", "title": "RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid\n  Detection in Three-Dimensional Fluorescence Microscopy Images", "comments": "Accepted by CVPR Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and accurate nuclei centroid detection is important for the\nunderstanding of biological structures in fluorescence microscopy images.\nExisting automated nuclei localization methods face three main challenges: (1)\nMost of object detection methods work only on 2D images and are difficult to\nextend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes\nbut it is computational expensive for large microscopy volumes and they have\ndifficulty distinguishing different instances of objects; (3) Hand annotated\nground truth is limited for 3D microscopy volumes. To address these issues, we\npresent a scalable approach for nuclei centroid detection of 3D microscopy\nvolumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each\nslice of the volume from different directions and 3D agglomerative hierarchical\nclustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.\nThe model was trained with the synthetic microscopy data generated using\nSpatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and\ntested on different types of real 3D microscopy data. Extensive experimental\nresults demonstrate that our proposed method can accurately count and detect\nthe nuclei centroids in a 3D microscopy volume.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 23:38:29 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wu", "Liming", ""], ["Han", "Shuo", ""], ["Chen", "Alain", ""], ["Salama", "Paul", ""], ["Dunn", "Kenneth W.", ""], ["Delp", "Edward J.", ""]]}, {"id": "2106.15754", "submitter": "Lei Ding", "authors": "Lei Ding, Dong Lin, Shaofu Lin, Jing Zhang, Xiaojie Cui, Yuebin Wang,\n  Hao Tang and Lorenzo Bruzzone", "title": "Looking Outside the Window: Wide-Context Transformer for the Semantic\n  Segmentation of High-Resolution Remote Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-range context information is crucial for the semantic segmentation of\nHigh-Resolution (HR) Remote Sensing Images (RSIs). The image cropping\noperations, commonly used for training neural networks, limit the perception of\nlong-range context information in large RSIs. To break this limitation, we\npropose a Wide-Context Network (WiCoNet) for the semantic segmentation of HR\nRSIs. In the WiCoNet, apart from a conventional feature extraction network that\naggregates the local information, an extra context branch is designed to\nexplicitly model the spatial information in a larger image area. The\ninformation between the two branches is communicated through a Context\nTransformer, which is a novel design derived from the Vision Transformer to\nmodel the long-range context correlations. Ablation studies and comparative\nexperiments conducted on several benchmark datasets prove the effectiveness of\nthe proposed method. In addition, we present a new Beijing Land-Use (BLU)\ndataset. This is a large-scale HR satellite dataset provided with high-quality\nand fine-grained reference labels, which can boost future studies in this\nfield.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 23:41:54 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 01:06:19 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 14:24:31 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ding", "Lei", ""], ["Lin", "Dong", ""], ["Lin", "Shaofu", ""], ["Zhang", "Jing", ""], ["Cui", "Xiaojie", ""], ["Wang", "Yuebin", ""], ["Tang", "Hao", ""], ["Bruzzone", "Lorenzo", ""]]}, {"id": "2106.15765", "submitter": "Zhihong Zhang", "authors": "Zhihong Zhang, Chao Deng, Yang Liu, Xin Yuan, Jinli Suo, Qionghai Dai", "title": "10-mega pixel snapshot compressive imaging with a hybrid coded aperture", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution images are widely used in our daily life, whereas high-speed\nvideo capture is challenging due to the low frame rate of cameras working at\nthe high resolution mode. Digging deeper, the main bottleneck lies in the low\nthroughput of existing imaging systems. Towards this end, snapshot compressive\nimaging (SCI) was proposed as a promising solution to improve the throughput of\nimaging systems by compressive sampling and computational reconstruction.\nDuring acquisition, multiple high-speed images are encoded and collapsed to a\nsingle measurement. After this, algorithms are employed to retrieve the video\nframes from the coded snapshot. Recently developed Plug-and-Play (PnP)\nalgorithms make it possible for SCI reconstruction in large-scale problems.\nHowever, the lack of high-resolution encoding systems still precludes SCI's\nwide application. In this paper, we build a novel hybrid coded aperture\nsnapshot compressive imaging (HCA-SCI) system by incorporating a dynamic liquid\ncrystal on silicon and a high-resolution lithography mask. We further implement\na PnP reconstruction algorithm with cascaded denoisers for high quality\nreconstruction. Based on the proposed HCA-SCI system and algorithm, we achieve\na 10-mega pixel SCI system to capture high-speed scenes, leading to a high\nthroughput of 4.6G voxels per second. Both simulation and real data experiments\nverify the feasibility and performance of our proposed HCA-SCI scheme.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 01:09:24 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhang", "Zhihong", ""], ["Deng", "Chao", ""], ["Liu", "Yang", ""], ["Yuan", "Xin", ""], ["Suo", "Jinli", ""], ["Dai", "Qionghai", ""]]}, {"id": "2106.15778", "submitter": "Wenming Tang", "authors": "Wenming Tang Guoping Qiu", "title": "Dense Graph Convolutional Neural Networks on 3D Meshes for 3D Object\n  Segmentation and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents new designs of graph convolutional neural networks (GCNs)\non 3D meshes for 3D object segmentation and classification. We use the faces of\nthe mesh as basic processing units and represent a 3D mesh as a graph where\neach node corresponds to a face. To enhance the descriptive power of the graph,\nwe introduce a 1-ring face neighbourhood structure to derive novel\nmulti-dimensional spatial and structure features to represent the graph nodes.\nBased on this new graph representation, we then design a densely connected\ngraph convolutional block which aggregates local and regional features as the\nkey construction component to build effective and efficient practical GCN\nmodels for 3D object classification and segmentation. We will present\nexperimental results to show that our new technique outperforms state of the\nart where our models are shown to have the smallest number of parameters and\nconsietently achieve the highest accuracies across a number of benchmark\ndatasets. We will also present ablation studies to demonstrate the soundness of\nour design principles and the effectiveness of our practical models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 02:17:16 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Qiu", "Wenming Tang Guoping", ""]]}, {"id": "2106.15787", "submitter": "Can Zhang", "authors": "Liyu Wu, Yuexian Zou, Can Zhang", "title": "Long-Short Temporal Modeling for Efficient Action Recognition", "comments": "Accepted by ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient long-short temporal modeling is key for enhancing the performance\nof action recognition task. In this paper, we propose a new two-stream action\nrecognition network, termed as MENet, consisting of a Motion Enhancement (ME)\nmodule and a Video-level Aggregation (VLA) module to achieve long-short\ntemporal modeling. Specifically, motion representations have been proved\neffective in capturing short-term and high-frequency action. However, current\nmotion representations are calculated from adjacent frames, which may have poor\ninterpretation and bring useless information (noisy or blank). Thus, for\nshort-term motions, we design an efficient ME module to enhance the short-term\nmotions by mingling the motion saliency among neighboring segments. As for\nlong-term aggregations, VLA is adopted at the top of the appearance branch to\nintegrate the long-term dependencies across all segments. The two components of\nMENet are complementary in temporal modeling. Extensive experiments are\nconducted on UCF101 and HMDB51 benchmarks, which verify the effectiveness and\nefficiency of our proposed MENet.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 02:54:13 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wu", "Liyu", ""], ["Zou", "Yuexian", ""], ["Zhang", "Can", ""]]}, {"id": "2106.15788", "submitter": "Di Wu", "authors": "Di Wu, Siyuan Li, Zelin Zang, Kai Wang, Lei Shang, Baigui Sun, Hao Li,\n  Stan Z. Li", "title": "Align Yourself: Self-supervised Pre-training for Fine-grained\n  Recognition via Saliency Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised contrastive learning has demonstrated great potential in\nlearning visual representations. Despite their success on various downstream\ntasks such as image classification and object detection, self-supervised\npre-training for fine-grained scenarios is not fully explored. In this paper,\nwe first point out that current contrastive methods are prone to memorizing\nbackground/foreground texture and therefore have a limitation in localizing the\nforeground object. Analysis suggests that learning to extract discriminative\ntexture information and localization are equally crucial for self-supervised\npre-training under fine-grained scenarios. Based on our findings, we introduce\nCross-view Saliency Alignment (CVSA), a contrastive learning framework that\nfirst crops and swaps saliency regions of images as a novel view generation and\nthen guides the model to localize on the foreground object via a cross-view\nalignment loss. Extensive experiments on four popular fine-grained\nclassification benchmarks show that CVSA significantly improves the learned\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 02:56:26 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wu", "Di", ""], ["Li", "Siyuan", ""], ["Zang", "Zelin", ""], ["Wang", "Kai", ""], ["Shang", "Lei", ""], ["Sun", "Baigui", ""], ["Li", "Hao", ""], ["Li", "Stan Z.", ""]]}, {"id": "2106.15793", "submitter": "Sicheng Zhao", "authors": "Xingxu Yao, Sicheng Zhao, Pengfei Xu, Jufeng Yang", "title": "Multi-Source Domain Adaptation for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce annotation labor associated with object detection, an increasing\nnumber of studies focus on transferring the learned knowledge from a labeled\nsource domain to another unlabeled target domain. However, existing methods\nassume that the labeled data are sampled from a single source domain, which\nignores a more generalized scenario, where labeled data are from multiple\nsource domains. For the more challenging task, we propose a unified Faster\nR-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which\ncan simultaneously enhance domain invariance and preserve discriminative power.\nSpecifically, the framework contains multiple source subnets and a pseudo\ntarget subnet. First, we propose a hierarchical feature alignment strategy to\nconduct strong and weak alignments for low- and high-level features,\nrespectively, considering their different effects for object detection. Second,\nwe develop a novel pseudo subnet learning algorithm to approximate optimal\nparameters of pseudo target subset by weighted combination of parameters in\ndifferent source subnets. Finally, a consistency regularization for region\nproposal network is proposed to facilitate each subnet to learn more abstract\ninvariances. Extensive experiments on different adaptation scenarios\ndemonstrate the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 03:17:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Yao", "Xingxu", ""], ["Zhao", "Sicheng", ""], ["Xu", "Pengfei", ""], ["Yang", "Jufeng", ""]]}, {"id": "2106.15796", "submitter": "Yunsong Zhou", "authors": "Yunsong Zhou, Yuan He, Hongzi Zhu, Cheng Wang, Hongyang Li, Qinhong\n  Jiang", "title": "Monocular 3D Object Detection: An Extrinsic Parameter Free Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D object detection is an important task in autonomous driving. It\ncan be easily intractable where there exists ego-car pose change w.r.t. ground\nplane. This is common due to the slight fluctuation of road smoothness and\nslope. Due to the lack of insight in industrial application, existing methods\non open datasets neglect the camera pose information, which inevitably results\nin the detector being susceptible to camera extrinsic parameters. The\nperturbation of objects is very popular in most autonomous driving cases for\nindustrial products. To this end, we propose a novel method to capture camera\npose to formulate the detector free from extrinsic perturbation. Specifically,\nthe proposed framework predicts camera extrinsic parameters by detecting\nvanishing point and horizon change. A converter is designed to rectify\nperturbative features in the latent space. By doing so, our 3D detector works\nindependent of the extrinsic parameter variations and produces accurate results\nin realistic cases, e.g., potholed and uneven roads, where almost all existing\nmonocular detectors fail to handle. Experiments demonstrate our method yields\nthe best performance compared with the other state-of-the-arts by a large\nmargin on both KITTI 3D and nuScenes datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 03:35:51 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhou", "Yunsong", ""], ["He", "Yuan", ""], ["Zhu", "Hongzi", ""], ["Wang", "Cheng", ""], ["Li", "Hongyang", ""], ["Jiang", "Qinhong", ""]]}, {"id": "2106.15797", "submitter": "Yong Guo", "authors": "Yong Guo, Yaofo Chen, Mingkui Tan, Kui Jia, Jian Chen, Jingdong Wang", "title": "Content-Aware Convolutional Neural Networks", "comments": "Accepted by Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have achieved great success due to the\npowerful feature learning ability of convolution layers. Specifically, the\nstandard convolution traverses the input images/features using a sliding window\nscheme to extract features. However, not all the windows contribute equally to\nthe prediction results of CNNs. In practice, the convolutional operation on\nsome of the windows (e.g., smooth windows that contain very similar pixels) can\nbe very redundant and may introduce noises into the computation. Such\nredundancy may not only deteriorate the performance but also incur the\nunnecessary computational cost. Thus, it is important to reduce the\ncomputational redundancy of convolution to improve the performance. To this\nend, we propose a Content-aware Convolution (CAC) that automatically detects\nthe smooth windows and applies a 1x1 convolutional kernel to replace the\noriginal large kernel. In this sense, we are able to effectively avoid the\nredundant computation on similar pixels. By replacing the standard convolution\nin CNNs with our CAC, the resultant models yield significantly better\nperformance and lower computational cost than the baseline models with the\nstandard convolution. More critically, we are able to dynamically allocate\nsuitable computation resources according to the data smoothness of different\nimages, making it possible for content-aware computation. Extensive experiments\non various computer vision tasks demonstrate the superiority of our method over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 03:54:35 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 07:32:54 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Guo", "Yong", ""], ["Chen", "Yaofo", ""], ["Tan", "Mingkui", ""], ["Jia", "Kui", ""], ["Chen", "Jian", ""], ["Wang", "Jingdong", ""]]}, {"id": "2106.15827", "submitter": "Xi Li", "authors": "Hanbin Zhao, Xin Qin, Shihao Su, Zibo Lin, Xi Li", "title": "When Video Classification Meets Incremental Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of social media, tremendous videos with new\nclasses are generated daily, which raise an urgent demand for video\nclassification methods that can continuously update new classes while\nmaintaining the knowledge of old videos with limited storage and computing\nresources. In this paper, we summarize this task as \\textit{Class-Incremental\nVideo Classification (CIVC)} and propose a novel framework to address it. As a\nsubarea of incremental learning tasks, the challenge of \\textit{catastrophic\nforgetting} is unavoidable in CIVC. To better alleviate it, we utilize some\ncharacteristics of videos. First, we decompose the spatio-temporal knowledge\nbefore distillation rather than treating it as a whole in the knowledge\ntransfer process; trajectory is also used to refine the decomposition. Second,\nwe propose a dual granularity exemplar selection method to select and store\nrepresentative video instances of old classes and key-frames inside videos\nunder a tight storage budget. We benchmark our method and previous SOTA\nclass-incremental learning methods on Something-Something V2 and Kinetics\ndatasets, and our method outperforms previous methods significantly.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 06:12:33 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhao", "Hanbin", ""], ["Qin", "Xin", ""], ["Su", "Shihao", ""], ["Lin", "Zibo", ""], ["Li", "Xi", ""]]}, {"id": "2106.15828", "submitter": "Juan Tapia Dr.", "authors": "Juan Tapia, Enrique Lopez Droguett, Andres Valenzuela, Daniel\n  Benalcazar, Leonardo Causa, Christoph Busch", "title": "Semantic Segmentation of Periocular Near-Infra-Red Eye Images Under\n  Alcohol Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper proposes a new framework to detect, segment, and estimate the\nlocalization of the eyes from a periocular Near-Infra-Red iris image under\nalcohol consumption. The purpose of the system is to measure the fitness for\nduty. Fitness systems allow us to determine whether a person is physically or\npsychologically able to perform their tasks. Our framework is based on an\nobject detector trained from scratch to detect both eyes from a single image.\nThen, two efficient networks were used for semantic segmentation; a Criss-Cross\nattention network and DenseNet10, with only 122,514 and 210,732 parameters,\nrespectively. These networks can find the pupil, iris, and sclera. In the end,\nthe binary output eye mask is used for pupil and iris diameter estimation with\nhigh precision. Five state-of-the-art algorithms were used for this purpose. A\nmixed proposal reached the best results. A second contribution is establishing\nan alcohol behavior curve to detect the alcohol presence utilizing a stream of\nimages captured from an iris instance. Also, a manually labeled database with\nmore than 20k images was created. Our best method obtains a mean\nIntersection-over-Union of 94.54% with DenseNet10 with only 210,732 parameters\nand an error of only 1-pixel on average.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 06:15:17 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Tapia", "Juan", ""], ["Droguett", "Enrique Lopez", ""], ["Valenzuela", "Andres", ""], ["Benalcazar", "Daniel", ""], ["Causa", "Leonardo", ""], ["Busch", "Christoph", ""]]}, {"id": "2106.15831", "submitter": "Anders Johan Andreassen", "authors": "Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, Rebecca Roelofs", "title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning", "comments": "27 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although machine learning models typically experience a drop in performance\non out-of-distribution data, accuracies on in- versus out-of-distribution data\nare widely observed to follow a single linear trend when evaluated across a\ntestbed of models. Models that are more accurate on the out-of-distribution\ndata relative to this baseline exhibit \"effective robustness\" and are\nexceedingly rare. Identifying such models, and understanding their properties,\nis key to improving out-of-distribution performance. We conduct a thorough\nempirical investigation of effective robustness during fine-tuning and\nsurprisingly find that models pre-trained on larger datasets exhibit effective\nrobustness during training that vanishes at convergence. We study how\nproperties of the data influence effective robustness, and we show that it\nincreases with the larger size, more diversity, and higher example difficulty\nof the dataset. We also find that models that display effective robustness are\nable to correctly classify 10% of the examples that no other current testbed\nmodel gets correct. Finally, we discuss several strategies for scaling\neffective robustness to the high-accuracy regime to improve the\nout-of-distribution accuracy of state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 06:21:42 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Andreassen", "Anders", ""], ["Bahri", "Yasaman", ""], ["Neyshabur", "Behnam", ""], ["Roelofs", "Rebecca", ""]]}, {"id": "2106.15889", "submitter": "Christian Berger", "authors": "Christian Berger", "title": "A Structured Analysis of the Video Degradation Effects on the\n  Performance of a Machine Learning-enabled Pedestrian Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ML-enabled software systems have been incorporated in many public\ndemonstrations for automated driving (AD) systems. Such solutions have also\nbeen considered as a crucial approach to aim at SAE Level 5 systems, where the\npassengers in such vehicles do not have to interact with the system at all\nanymore. Already in 2016, Nvidia demonstrated a complete end-to-end approach\nfor training the complete software stack covering perception, planning and\ndecision making, and the actual vehicle control. While such approaches show the\ngreat potential of such ML-enabled systems, there have also been demonstrations\nwhere already changes to single pixels in a video frame can potentially lead to\ncompletely different decisions with dangerous consequences. In this paper, a\nstructured analysis has been conducted to explore video degradation effects on\nthe performance of an ML-enabled pedestrian detector. Firstly, a baseline of\napplying YOLO to 1,026 frames with pedestrian annotations in the KITTI Vision\nBenchmark Suite has been established. Next, video degradation candidates for\neach of these frames were generated using the leading video codecs libx264,\nlibx265, Nvidia HEVC, and AV1: 52 frames for the various compression presets\nfor color and gray-scale frames resulting in 104 degradation candidates per\noriginal KITTI frame and 426,816 images in total. YOLO was applied to each\nimage to compute the intersection-over-union (IoU) metric to compare the\nperformance with the original baseline. While aggressively lossy compression\nsettings result in significant performance drops as expected, it was also\nobserved that some configurations actually result in slightly better IoU\nresults compared to the baseline. The findings show that carefully chosen lossy\nvideo configurations preserve a decent performance of particular ML-enabled\nsystems while allowing for substantial savings when storing or transmitting\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:30:12 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Berger", "Christian", ""]]}, {"id": "2106.15893", "submitter": "Frauke Wilm", "authors": "Frauke Wilm, Michaela Benz, Volker Bruns, Serop Baghdadlian, Jakob\n  Dexl, David Hartmann, Petr Kuritcyn, Martin Weidenfeller, Thomas Wittenberg,\n  Susanne Merkel, Arndt Hartmann, Markus Eckstein, Carol I. Geppert", "title": "Fast whole-slide cartography in colon cancer histology using superpixels\n  and CNN classification", "comments": "29 pages, 21 figures, 6 tables, submitted to Elsevier Medical Image\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole-slide-image cartography is the process of automatically detecting and\noutlining different tissue types in digitized histological specimen. This\nsemantic segmentation provides a basis for many follow-up analyses and can\npotentially guide subsequent medical decisions. Due to their large size,\nwhole-slide-images typically have to be divided into smaller patches which are\nthen analyzed individually using machine learning-based approaches. Thereby,\nlocal dependencies of image regions get lost and since a whole-slide-image\ncomprises many thousands of such patches this process is inherently slow. We\npropose to subdivide the image into coherent regions prior to classification by\ngrouping visually similar adjacent image pixels into larger segments, i.e.\nsuperpixels. Afterwards, only a random subset of patches per superpixel is\nclassified and patch labels are combined into a single superpixel label. The\nalgorithm has been developed and validated on a dataset of 159 hand-annotated\nwhole-slide-images of colon resections and its performance has been compared to\na standard patch-based approach. The algorithm shows an average speed-up of 41%\non the test data and the overall accuracy is increased from 93.8% to 95.7%. We\nadditionally propose a metric for identifying superpixels with an uncertain\nclassification so they can be excluded from further analysis. Finally, we\nevaluate two potential medical applications, namely tumor area estimation\nincluding tumor invasive margin generation and tumor composition analysis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:34:06 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wilm", "Frauke", ""], ["Benz", "Michaela", ""], ["Bruns", "Volker", ""], ["Baghdadlian", "Serop", ""], ["Dexl", "Jakob", ""], ["Hartmann", "David", ""], ["Kuritcyn", "Petr", ""], ["Weidenfeller", "Martin", ""], ["Wittenberg", "Thomas", ""], ["Merkel", "Susanne", ""], ["Hartmann", "Arndt", ""], ["Eckstein", "Markus", ""], ["Geppert", "Carol I.", ""]]}, {"id": "2106.15918", "submitter": "Zipei Zhao", "authors": "Zipei Zhao, Fengqian Pang, Zhiwen Liu, Chuyang Ye", "title": "Positive-unlabeled Learning for Cell Detection in Histopathology Images\n  with Incomplete Annotations", "comments": "Accepted by MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell detection in histopathology images is of great value in clinical\npractice. \\textit{Convolutional neural networks} (CNNs) have been applied to\ncell detection to improve the detection accuracy, where cell annotations are\nrequired for network training. However, due to the variety and large number of\ncells, complete annotations that include every cell of interest in the training\nimages can be challenging. Usually, incomplete annotations can be achieved,\nwhere positive labeling results are carefully examined to ensure their\nreliability but there can be other positive instances, i.e., cells of interest,\nthat are not included in the annotations. This annotation strategy leads to a\nlack of knowledge about true negative samples. Most existing methods simply\ntreat instances that are not labeled as positive as truly negative during\nnetwork training, which can adversely affect the network performance. In this\nwork, to address the problem of incomplete annotations, we formulate the\ntraining of detection networks as a positive-unlabeled learning problem.\nSpecifically, the classification loss in network training is revised to take\ninto account incomplete annotations, where the terms corresponding to negative\nsamples are approximated with the true positive samples and the other samples\nof which the labels are unknown. To evaluate the proposed method, experiments\nwere performed on a publicly available dataset for mitosis detection in breast\ncancer cells, and the experimental results show that our method improves the\nperformance of cell detection given incomplete annotations for training.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:20:25 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhao", "Zipei", ""], ["Pang", "Fengqian", ""], ["Liu", "Zhiwen", ""], ["Ye", "Chuyang", ""]]}, {"id": "2106.15941", "submitter": "Yehui Tang", "authors": "Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng, Chao Xu, Yunhe\n  Wang", "title": "Augmented Shortcuts for Vision Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer models have achieved great progress on computer vision tasks\nrecently. The rapid development of vision transformers is mainly contributed by\ntheir high representation ability for extracting informative features from\ninput images. However, the mainstream transformer models are designed with deep\narchitectures, and the feature diversity will be continuously reduced as the\ndepth increases, i.e., feature collapse. In this paper, we theoretically\nanalyze the feature collapse phenomenon and study the relationship between\nshortcuts and feature diversity in these transformer models. Then, we present\nan augmented shortcut scheme, which inserts additional paths with learnable\nparameters in parallel on the original shortcuts. To save the computational\ncosts, we further explore an efficient approach that uses the block-circulant\nprojection to implement augmented shortcuts. Extensive experiments conducted on\nbenchmark datasets demonstrate the effectiveness of the proposed method, which\nbrings about 1% accuracy increase of the state-of-the-art visual transformers\nwithout obviously increasing their parameters and FLOPs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:48:30 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Tang", "Yehui", ""], ["Han", "Kai", ""], ["Xu", "Chang", ""], ["Xiao", "An", ""], ["Deng", "Yiping", ""], ["Xu", "Chao", ""], ["Wang", "Yunhe", ""]]}, {"id": "2106.15944", "submitter": "Wenqi Ren", "authors": "Jingang Zhang and Runmu Su and Wenqi Ren and Qiang Fu and Yunfeng Nie", "title": "Learnable Reconstruction Methods from RGB Images to Hyperspectral\n  Imaging: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging enables versatile applications due to its competence in\ncapturing abundant spatial and spectral information, which are crucial for\nidentifying substances. However, the devices for acquiring hyperspectral images\nare expensive and complicated. Therefore, many alternative spectral imaging\nmethods have been proposed by directly reconstructing the hyperspectral\ninformation from lower-cost, more available RGB images. We present a thorough\ninvestigation of these state-of-the-art spectral reconstruction methods from\nthe widespread RGB images. A systematic study and comparison of more than 25\nmethods has revealed that most of the data-driven deep learning methods are\nsuperior to prior-based methods in terms of reconstruction accuracy and quality\ndespite lower speeds. This comprehensive review can serve as a fruitful\nreference source for peer researchers, thus further inspiring future\ndevelopment directions in related domains.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:52:41 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhang", "Jingang", ""], ["Su", "Runmu", ""], ["Ren", "Wenqi", ""], ["Fu", "Qiang", ""], ["Nie", "Yunfeng", ""]]}, {"id": "2106.15947", "submitter": "Chunhua Shen", "authors": "Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, Lei Li", "title": "SOLO: A Simple Framework for Instance Segmentation", "comments": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:1912.04488, arXiv:2003.10152", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Compared to many other dense prediction tasks, e.g., semantic segmentation,\nit is the arbitrary number of instances that has made instance segmentation\nmuch more challenging. In order to predict a mask for each instance, mainstream\napproaches either follow the 'detect-then-segment' strategy (e.g., Mask R-CNN),\nor predict embedding vectors first then cluster pixels into individual\ninstances. In this paper, we view the task of instance segmentation from a\ncompletely new perspective by introducing the notion of \"instance categories\",\nwhich assigns categories to each pixel within an instance according to the\ninstance's location. With this notion, we propose segmenting objects by\nlocations (SOLO), a simple, direct, and fast framework for instance\nsegmentation with strong performance. We derive a few SOLO variants (e.g.,\nVanilla SOLO, Decoupled SOLO, Dynamic SOLO) following the basic principle. Our\nmethod directly maps a raw input image to the desired object categories and\ninstance masks, eliminating the need for the grouping post-processing or the\nbounding box detection. Our approach achieves state-of-the-art results for\ninstance segmentation in terms of both speed and accuracy, while being\nconsiderably simpler than the existing methods. Besides instance segmentation,\nour method yields state-of-the-art results in object detection (from our mask\nbyproduct) and panoptic segmentation. We further demonstrate the flexibility\nand high-quality segmentation of SOLO by extending it to perform one-stage\ninstance-level image matting. Code is available at: https://git.io/AdelaiDet\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:56:54 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wang", "Xinlong", ""], ["Zhang", "Rufeng", ""], ["Shen", "Chunhua", ""], ["Kong", "Tao", ""], ["Li", "Lei", ""]]}, {"id": "2106.15953", "submitter": "Xinxu Wei", "authors": "Xinxu Wei, Xianshi Zhang, Shisen Wang, Cheng Cheng, Yanlin Huang,\n  Kaifu Yang, and Yongjie Li", "title": "BLNet: A Fast Deep Learning Framework for Low-Light Image Enhancement\n  with Noise Removal and Color Restoration", "comments": "13 pages, 12 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Images obtained in real-world low-light conditions are not only low in\nbrightness, but they also suffer from many other types of degradation, such as\ncolor bias, unknown noise, detail loss and halo artifacts. In this paper, we\npropose a very fast deep learning framework called Bringing the Lightness\n(denoted as BLNet) that consists of two U-Nets with a series of well-designed\nloss functions to tackle all of the above degradations. Based on Retinex\nTheory, the decomposition net in our model can decompose low-light images into\nreflectance and illumination and remove noise in the reflectance during the\ndecomposition phase. We propose a Noise and Color Bias Control module (NCBC\nModule) that contains a convolutional neural network and two loss functions\n(noise loss and color loss). This module is only used to calculate the loss\nfunctions during the training phase, so our method is very fast during the test\nphase. This module can smooth the reflectance to achieve the purpose of noise\nremoval while preserving details and edge information and controlling color\nbias. We propose a network that can be trained to learn the mapping between\nlow-light and normal-light illumination and enhance the brightness of images\ntaken in low-light illumination. We train and evaluate the performance of our\nproposed model over the real-world Low-Light (LOL) dataset), and we also test\nour model over several other frequently used datasets (LIME, DICM and MEF\ndatasets). We conduct extensive experiments to demonstrate that our approach\nachieves a promising effect with good rubustness and generalization and\noutperforms many other state-of-the-art methods qualitatively and\nquantitatively. Our method achieves high speed because we use loss functions\ninstead of introducing additional denoisers for noise removal and color\ncorrection. The code and model are available at\nhttps://github.com/weixinxu666/BLNet.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 10:06:16 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wei", "Xinxu", ""], ["Zhang", "Xianshi", ""], ["Wang", "Shisen", ""], ["Cheng", "Cheng", ""], ["Huang", "Yanlin", ""], ["Yang", "Kaifu", ""], ["Li", "Yongjie", ""]]}, {"id": "2106.15989", "submitter": "Katsufumi Inoue", "authors": "Mizuki Maruyama, Shuvozit Ghose, Katsufumi Inoue, Partha Pratim Roy,\n  Masakazu Iwamura, Michifumi Yoshioka", "title": "Word-level Sign Language Recognition with Multi-stream Neural Networks\n  Focusing on Local Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Word-level Sign Language Recognition (WSLR) research has\ngained popularity in the computer vision community, and thus various approaches\nhave been proposed. Among these approaches, the method using I3D network\nachieves the highest recognition accuracy on large public datasets for WSLR.\nHowever, the method with I3D only utilizes appearance information of the upper\nbody of the signers to recognize sign language words. On the other hand, in\nWSLR, the information of local regions, such as the hand shape and facial\nexpression, and the positional relationship among the body and both hands are\nimportant. Thus in this work, we utilized local region images of both hands and\nface, along with skeletal information to capture local information and the\npositions of both hands relative to the body, respectively. In other words, we\npropose a novel multi-stream WSLR framework, in which a stream with local\nregion images and a stream with skeletal information are introduced by\nextending I3D network to improve the recognition accuracy of WSLR. From the\nexperimental results on WLASL dataset, it is evident that the proposed method\nhas achieved about 15% improvement in the Top-1 accuracy than the existing\nconventional methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:30:06 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Maruyama", "Mizuki", ""], ["Ghose", "Shuvozit", ""], ["Inoue", "Katsufumi", ""], ["Roy", "Partha Pratim", ""], ["Iwamura", "Masakazu", ""], ["Yoshioka", "Michifumi", ""]]}, {"id": "2106.15991", "submitter": "Stefan Zernetsch", "authors": "Stefan Zernetsch and Oliver Trupp and Viktor Kress and Konrad Doll and\n  Bernhard Sick", "title": "Cyclist Trajectory Forecasts by Incorporation of Multi-View Video\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This article presents a novel approach to incorporate visual cues from\nvideo-data from a wide-angle stereo camera system mounted at an urban\nintersection into the forecast of cyclist trajectories. We extract features\nfrom image and optical flow (OF) sequences using 3D convolutional neural\nnetworks (3D-ConvNet) and combine them with features extracted from the\ncyclist's past trajectory to forecast future cyclist positions. By the use of\nadditional information, we are able to improve positional accuracy by about 7.5\n% for our test dataset and by up to 22 % for specific motion types compared to\na method solely based on past trajectories. Furthermore, we compare the use of\nimage sequences to the use of OF sequences as additional information, showing\nthat OF alone leads to significant improvements in positional accuracy. By\ntraining and testing our methods using a real-world dataset recorded at a\nheavily frequented public intersection and evaluating the methods' runtimes, we\ndemonstrate the applicability in real traffic scenarios. Our code and parts of\nour dataset are made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:34:43 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zernetsch", "Stefan", ""], ["Trupp", "Oliver", ""], ["Kress", "Viktor", ""], ["Doll", "Konrad", ""], ["Sick", "Bernhard", ""]]}, {"id": "2106.15998", "submitter": "Daniel Wiens", "authors": "Daniel Wiens and Barbara Hammer", "title": "Single-Step Adversarial Training for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Even though deep neural networks succeed on many different tasks including\nsemantic segmentation, they lack on robustness against adversarial examples. To\ncounteract this exploit, often adversarial training is used. However, it is\nknown that adversarial training with weak adversarial attacks (e.g. using the\nFast Gradient Method) does not improve the robustness against stronger attacks.\nRecent research shows that it is possible to increase the robustness of such\nsingle-step methods by choosing an appropriate step size during the training.\nFinding such a step size, without increasing the computational effort of\nsingle-step adversarial training, is still an open challenge. In this work we\naddress the computationally particularly demanding task of semantic\nsegmentation and propose a new step size control algorithm that increases the\nrobustness of single-step adversarial training. The proposed algorithm does not\nincrease the computational effort of single-step adversarial training\nconsiderably and also simplifies training, because it is free of\nmeta-parameter. We show that the robustness of our approach can compete with\nmulti-step adversarial training on two popular benchmarks for semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:41:09 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wiens", "Daniel", ""], ["Hammer", "Barbara", ""]]}, {"id": "2106.16000", "submitter": "Yuexiang Li", "authors": "Jiawei Chen and Yuexiang Li and Kai Ma and Yefeng Zheng", "title": "Mutual-GAN: Towards Unsupervised Cross-Weather Adaptation with Mutual\n  Information Constraint", "comments": "An extension of our MICCAI paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural network (CNN) have proven its success for semantic\nsegmentation, which is a core task of emerging industrial applications such as\nautonomous driving. However, most progress in semantic segmentation of urban\nscenes is reported on standard scenarios, i.e., daytime scenes with favorable\nillumination conditions. In practical applications, the outdoor weather and\nillumination are changeable, e.g., cloudy and nighttime, which results in a\nsignificant drop of semantic segmentation accuracy of CNN only trained with\ndaytime data. In this paper, we propose a novel generative adversarial network\n(namely Mutual-GAN) to alleviate the accuracy decline when daytime-trained\nneural network is applied to videos captured under adverse weather conditions.\nThe proposed Mutual-GAN adopts mutual information constraint to preserve\nimage-objects during cross-weather adaptation, which is an unsolved problem for\nmost unsupervised image-to-image translation approaches (e.g., CycleGAN). The\nproposed Mutual-GAN is evaluated on two publicly available driving video\ndatasets (i.e., CamVid and SYNTHIA). The experimental results demonstrate that\nour Mutual-GAN can yield visually plausible translated images and significantly\nimprove the semantic segmentation accuracy of daytime-trained deep learning\nnetwork while processing videos under challenging weathers.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:44:22 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chen", "Jiawei", ""], ["Li", "Yuexiang", ""], ["Ma", "Kai", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2106.16006", "submitter": "Hamid Tabani", "authors": "Hamid Tabani, Ajay Balasubramaniam, Shabbir Marzban, Elahe Arani,\n  Bahram Zonooz", "title": "Improving the Efficiency of Transformers for Resource-Constrained\n  Devices", "comments": "This paper is accepted as a full paper at 24th Euromicro Conference\n  on Digital System Design (DSD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformers provide promising accuracy and have become popular and used in\nvarious domains such as natural language processing and computer vision.\nHowever, due to their massive number of model parameters, memory and\ncomputation requirements, they are not suitable for resource-constrained\nlow-power devices. Even with high-performance and specialized devices, the\nmemory bandwidth can become a performance-limiting bottleneck. In this paper,\nwe present a performance analysis of state-of-the-art vision transformers on\nseveral devices. We propose to reduce the overall memory footprint and memory\ntransfers by clustering the model parameters. We show that by using only 64\nclusters to represent model parameters, it is possible to reduce the data\ntransfer from the main memory by more than 4x, achieve up to 22% speedup and\n39% energy savings on mobile devices with less than 0.1% accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 12:10:48 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Tabani", "Hamid", ""], ["Balasubramaniam", "Ajay", ""], ["Marzban", "Shabbir", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2106.16009", "submitter": "Stefan Becker", "authors": "Stefan Becker and Ronny Hug and Wolfgang H\\\"ubner and Michael Arens\n  and Brendan T. Morris", "title": "MissFormer: (In-)attention-based handling of missing observations for\n  trajectory filtering and prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications such as object tracking, time-series data inevitably carry\nmissing observations. Following the success of deep learning-based models for\nvarious sequence learning tasks, these models increasingly replace classic\napproaches in object tracking applications for inferring the objects' motion\nstates. While traditional tracking approaches can deal with missing\nobservations, most of their deep counterparts are, by default, not suited for\nthis.\n  Towards this end, this paper introduces a transformer-based approach for\nhandling missing observations in variable input length trajectory data. The\nmodel is formed indirectly by successively increasing the complexity of the\ndemanded inference tasks. Starting from reproducing noise-free trajectories,\nthe model then learns to infer trajectories from noisy inputs. By providing\nmissing tokens, binary-encoded missing events, the model learns to in-attend to\nmissing data and infers a complete trajectory conditioned on the remaining\ninputs. In the case of a sequence of successive missing events, the model then\nacts as a pure prediction model. The abilities of the approach are demonstrated\non synthetic data and real-world data reflecting prototypical object tracking\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 12:12:52 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 09:09:34 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 11:44:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Becker", "Stefan", ""], ["Hug", "Ronny", ""], ["H\u00fcbner", "Wolfgang", ""], ["Arens", "Michael", ""], ["Morris", "Brendan T.", ""]]}, {"id": "2106.16028", "submitter": "Zhihang Zhong", "authors": "Zhihang Zhong, Ye Gao, Yinqiang Zheng, Bo Zheng, and Imari Sato", "title": "Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring", "comments": "Extended journal version of the ECCV2020 paper (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time video deblurring still remains a challenging task due to the\ncomplexity of spatially and temporally varying blur itself and the requirement\nof low computational cost. To improve the network efficiency, we adopt residual\ndense blocks into RNN cells, so as to efficiently extract the spatial features\nof the current frame. Furthermore, a global spatio-temporal attention module is\nproposed to fuse the effective hierarchical features from past and future\nframes to help better deblur the current frame. Another issue needs to be\naddressed urgently is the lack of a real-world benchmark dataset. Thus, we\ncontribute a novel dataset (BSD) to the community, by collecting paired\nblurry/sharp video clips using a co-axis beam splitter acquisition system.\nExperimental results show that the proposed method (ESTRNN) can achieve better\ndeblurring performance both quantitatively and qualitatively with less\ncomputational cost against state-of-the-art video deblurring methods. In\naddition, cross-validation experiments between datasets illustrate the high\ngenerality of BSD over the synthetic datasets. The code and dataset are\nreleased at https://github.com/zzh-tech/ESTRNN.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 12:53:02 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhong", "Zhihang", ""], ["Gao", "Ye", ""], ["Zheng", "Yinqiang", ""], ["Zheng", "Bo", ""], ["Sato", "Imari", ""]]}, {"id": "2106.16031", "submitter": "Onat Dalmaz", "authors": "Onat Dalmaz, Mahmut Yurt, Tolga \\c{C}ukur", "title": "ResViT: Residual vision transformers for multi-modal medical image\n  synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal imaging is a key healthcare technology in the diagnosis and\nmanagement of disease, but it is often underutilized due to costs associated\nwith multiple separate scans. This limitation yields the need for synthesis of\nunacquired modalities from the subset of available modalities. In recent years,\ngenerative adversarial network (GAN) models with superior depiction of\nstructural details have been established as state-of-the-art in numerous\nmedical image synthesis tasks. However, GANs are characteristically based on\nconvolutional neural network (CNN) backbones that perform local processing with\ncompact filters. This inductive bias, in turn, compromises learning of\nlong-range spatial dependencies. While attention maps incorporated in GANs can\nmultiplicatively modulate CNN features to emphasize critical image regions,\ntheir capture of global context is mostly implicit. Here, we propose a novel\ngenerative adversarial approach for medical image synthesis, ResViT, to combine\nlocal precision of convolution operators with contextual sensitivity of vision\ntransformers. Based on an encoder-decoder architecture, ResViT employs a\ncentral bottleneck comprising novel aggregated residual transformer (ART)\nblocks that synergistically combine convolutional and transformer modules.\nComprehensive demonstrations are performed for synthesizing missing sequences\nin multi-contrast MRI and CT images from MRI. Our results indicate the\nsuperiority of ResViT against competing methods in terms of qualitative\nobservations and quantitative metrics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 12:57:37 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Dalmaz", "Onat", ""], ["Yurt", "Mahmut", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "2106.16056", "submitter": "arXiv Admin", "authors": "William Roy, Glen Kelly, Robert Leer, Frederick Ricardo", "title": "A Survey on Adversarial Image Synthesis", "comments": "arXiv admin note: submission has been withdrawn by arXiv\n  administrators due to inappropriate text overlap with external source", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have been extremely successful in\nvarious application domains. Adversarial image synthesis has drawn increasing\nattention and made tremendous progress in recent years because of its wide\nrange of applications in many computer vision and image processing problems.\nAmong the many applications of GAN, image synthesis is the most well-studied\none, and research in this area has already demonstrated the great potential of\nusing GAN in image synthesis. In this paper, we provide a taxonomy of methods\nused in image synthesis, review different models for text-to-image synthesis\nand image-to-image translation, and discuss some evaluation metrics as well as\npossible future research directions in image synthesis with GAN.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:31:48 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 13:38:47 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Roy", "William", ""], ["Kelly", "Glen", ""], ["Leer", "Robert", ""], ["Ricardo", "Frederick", ""]]}, {"id": "2106.16060", "submitter": "Emanuele Sansone", "authors": "Emanuele Sansone", "title": "Leveraging Hidden Structure in Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work considers the problem of learning structured representations from\nraw images using self-supervised learning. We propose a principled framework\nbased on a mutual information objective, which integrates self-supervised and\nstructure learning. Furthermore, we devise a post-hoc procedure to interpret\nthe meaning of the learnt representations. Preliminary experiments on CIFAR-10\nshow that the proposed framework achieves higher generalization performance in\ndownstream classification tasks and provides more interpretable representations\ncompared to the ones learnt through traditional self-supervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:35:36 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Sansone", "Emanuele", ""]]}, {"id": "2106.16091", "submitter": "Felix Leeb", "authors": "Felix Leeb, Stefan Bauer, Bernhard Sch\\\"olkopf", "title": "Interventional Assays for the Latent Space of Autoencoders", "comments": "Under review for NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The encoders and decoders of autoencoders effectively project the input onto\nlearned manifolds in the latent space and data space respectively. We propose a\nframework, called latent responses, for probing the learned data manifold using\ninterventions in the latent space. Using this framework, we investigate \"holes\"\nin the representation to quantitatively ascertain to what extent the latent\nspace of a trained VAE is consistent with the chosen prior. Furthermore, we use\nthe identified structure to improve interpolation between latent vectors. We\nevaluate how our analyses improve the quality of the generated samples using\nthe VAE on a variety of benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 14:31:08 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Leeb", "Felix", ""], ["Bauer", "Stefan", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2106.16093", "submitter": "Florent Couzini\\'e-Devy", "authors": "Marin Scalbert, Maria Vakalopoulou, Florent Couzini\\'e-Devy", "title": "Multi-Source domain adaptation via supervised contrastive learning and\n  confident consistency regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-Source Unsupervised Domain Adaptation (multi-source UDA) aims to learn\na model from several labeled source domains while performing well on a\ndifferent target domain where only unlabeled data are available at training\ntime. To align source and target features distributions, several recent works\nuse source and target explicit statistics matching such as features moments or\nclass centroids. Yet, these approaches do not guarantee class conditional\ndistributions alignment across domains. In this work, we propose a new\nframework called Contrastive Multi-Source Domain Adaptation (CMSDA) for\nmulti-source UDA that addresses this limitation. Discriminative features are\nlearned from interpolated source examples via cross entropy minimization and\nfrom target examples via consistency regularization and hard pseudo-labeling.\nSimultaneously, interpolated source examples are leveraged to align source\nclass conditional distributions through an interpolated version of the\nsupervised contrastive loss. This alignment leads to more general and\ntransferable features which further improve the generalization on the target\ndomain. Extensive experiments have been carried out on three standard\nmulti-source UDA datasets where our method reports state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 14:39:15 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 14:24:33 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Scalbert", "Marin", ""], ["Vakalopoulou", "Maria", ""], ["Couzini\u00e9-Devy", "Florent", ""]]}, {"id": "2106.16100", "submitter": "Yuchi Liu", "authors": "Yuchi Liu, Zhongdao Wang, Xiangxin Zhou and Liang Zheng", "title": "Synthetic Data Are as Good as the Real for Association Knowledge\n  Learning in Multi-object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association, aiming to link bounding boxes of the same identity in a video\nsequence, is a central component in multi-object tracking (MOT). To train\nassociation modules, e.g., parametric networks, real video data are usually\nused. However, annotating person tracks in consecutive video frames is\nexpensive, and such real data, due to its inflexibility, offer us limited\nopportunities to evaluate the system performance w.r.t changing tracking\nscenarios. In this paper, we study whether 3D synthetic data can replace\nreal-world videos for association training. Specifically, we introduce a\nlarge-scale synthetic data engine named MOTX, where the motion characteristics\nof cameras and objects are manually configured to be similar to those in\nreal-world datasets. We show that compared with real data, association\nknowledge obtained from synthetic data can achieve very similar performance on\nreal-world test sets without domain adaption techniques. Our intriguing\nobservation is credited to two factors. First and foremost, 3D engines can well\nsimulate motion factors such as camera movement, camera view and object\nmovement, so that the simulated videos can provide association modules with\neffective motion features. Second, experimental results show that the\nappearance domain gap hardly harms the learning of association knowledge. In\naddition, the strong customization ability of MOTX allows us to quantitatively\nassess the impact of motion factors on MOT, which brings new insights to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 14:46:36 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:36:52 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Liu", "Yuchi", ""], ["Wang", "Zhongdao", ""], ["Zhou", "Xiangxin", ""], ["Zheng", "Liang", ""]]}, {"id": "2106.16108", "submitter": "Shayan Kousha", "authors": "Shayan Kousha, Marcus A. Brubaker", "title": "Zero-shot Learning with Class Description Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of generative Zero-shot learning (ZSL) is to learning from seen\nclasses, transfer the learned knowledge, and create samples of unseen classes\nfrom the description of these unseen categories. To achieve better ZSL\naccuracies, models need to better understand the descriptions of unseen\nclasses. We introduce a novel form of regularization that encourages generative\nZSL models to pay more attention to the description of each category. Our\nempirical results demonstrate improvements over the performance of multiple\nstate-of-the-art models on the task of generalized zero-shot recognition and\nclassification when trained on textual description-based datasets like CUB and\nNABirds and attribute-based datasets like AWA2, aPY and SUN.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 14:56:15 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kousha", "Shayan", ""], ["Brubaker", "Marcus A.", ""]]}, {"id": "2106.16118", "submitter": "Michael Laskey", "authors": "Thomas Kollar, Michael Laskey, Kevin Stone, Brijen Thananjeyan, Mark\n  Tjersland", "title": "SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic\n  Data via Stereo", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robot manipulation of unknown objects in unstructured environments is a\nchallenging problem due to the variety of shapes, materials, arrangements and\nlighting conditions. Even with large-scale real-world data collection, robust\nperception and manipulation of transparent and reflective objects across\nvarious lighting conditions remain challenging. To address these challenges we\npropose an approach to performing sim-to-real transfer of robotic perception.\nThe underlying model, SimNet, is trained as a single multi-headed neural\nnetwork using simulated stereo data as input and simulated object segmentation\nmasks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as\noutput. A key component of SimNet is the incorporation of a learned stereo\nsub-network that predicts disparity. SimNet is evaluated on 2D car detection,\nunknown object detection, and deformable object keypoint detection and\nsignificantly outperforms a baseline that uses a structured light RGB-D sensor.\nBy inferring grasp positions using the OBB and keypoint predictions, SimNet can\nbe used to perform end-to-end manipulation of unknown objects in both easy and\nhard scenarios using our fleet of Toyota HSR robots in four home environments.\nIn unknown object grasping experiments, the predictions from the baseline RGB-D\nnetwork and SimNet enable successful grasps of most of the easy objects.\nHowever, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)\nobjects, while SimNet grasps 95%, suggesting that SimNet can enable robust\nmanipulation of unknown objects, including transparent objects, in unknown\nenvironments.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:18:14 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kollar", "Thomas", ""], ["Laskey", "Michael", ""], ["Stone", "Kevin", ""], ["Thananjeyan", "Brijen", ""], ["Tjersland", "Mark", ""]]}, {"id": "2106.16125", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Xingxu Yao, Jufeng Yang, Guoli Jia, Guiguang Ding,\n  Tat-Seng Chua, Bj\\\"orn W. Schuller, Kurt Keutzer", "title": "Affective Image Content Analysis: Two Decades Review and New\n  Perspectives", "comments": "Accepted by IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images can convey rich semantics and induce various emotions in viewers.\nRecently, with the rapid advancement of emotional intelligence and the\nexplosive growth of visual data, extensive research efforts have been dedicated\nto affective image content analysis (AICA). In this survey, we will\ncomprehensively review the development of AICA in the recent two decades,\nespecially focusing on the state-of-the-art methods with respect to three main\nchallenges -- the affective gap, perception subjectivity, and label noise and\nabsence. We begin with an introduction to the key emotion representation models\nthat have been widely employed in AICA and description of available datasets\nfor performing evaluation with quantitative comparison of label noise and\ndataset bias. We then summarize and compare the representative approaches on\n(1) emotion feature extraction, including both handcrafted and deep features,\n(2) learning methods on dominant emotion recognition, personalized emotion\nprediction, emotion distribution learning, and learning from noisy data or few\nlabels, and (3) AICA based applications. Finally, we discuss some challenges\nand promising research directions in the future, such as image content and\ncontext understanding, group emotion clustering, and viewer-image interaction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:20:56 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zhao", "Sicheng", ""], ["Yao", "Xingxu", ""], ["Yang", "Jufeng", ""], ["Jia", "Guoli", ""], ["Ding", "Guiguang", ""], ["Chua", "Tat-Seng", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2106.16126", "submitter": "Rauf Momin", "authors": "Rauf Momin, Ali Shan Momin, Khalid Rasheed, Muhammad Saqib", "title": "Recognizing Facial Expressions in the Wild using Multi-Architectural\n  Representations based Ensemble Learning with Distillation", "comments": "5 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions are the most common universal forms of body language. In\nthe past few years, automatic facial expression recognition (FER) has been an\nactive field of research. However, it is still a challenging task due to\ndifferent uncertainties and complications. Nevertheless, efficiency and\nperformance are yet essential aspects for building robust systems. In this\nwork, we propose two models named EmoXNet and EmoXNetLite. EmoXNet is an\nensemble learning technique for learning convoluted facial representations,\nwhereas EmoXNetLite is a distillation technique for transferring the knowledge\nfrom our ensemble model to an efficient deep neural network using\nlabel-smoothen soft labels to detect expressions effectively in real-time. Both\nmodels attained better accuracy level in comparison to the models reported to\ndate. The ensemble model (EmoXNet) attained 85.07% test accuracy on FER-2013\nwith FER+ annotations and 86.25% test accuracy on Real-world Affective Faces\nDatabase (RAF-DB). Whereas, the distilled model (EmoXNetLite) attained 82.07%\ntest accuracy on FER-2013 with FER+ annotations and 81.78% test accuracy on\nRAF-DB. Results show that our models seem to generalize well on new data and\nare learned to focus on relevant facial representations for expressions\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:22:24 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 18:41:39 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Momin", "Rauf", ""], ["Momin", "Ali Shan", ""], ["Rasheed", "Khalid", ""], ["Saqib", "Muhammad", ""]]}, {"id": "2106.16128", "submitter": "Shubao Liu", "authors": "Shubao Liu, Ke-Yue Zhang, Taiping Yao, Kekai Sheng, Shouhong Ding,\n  Ying Tai, Jilin Li, Yuan Xie, Lizhuang Ma", "title": "Dual Reweighting Domain Generalization for Face Presentation Attack\n  Detection", "comments": "accepted on IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing approaches based on domain generalization (DG) have drawn\ngrowing attention due to their robustness for unseen scenarios. Previous\nmethods treat each sample from multiple domains indiscriminately during the\ntraining process, and endeavor to extract a common feature space to improve the\ngeneralization. However, due to complex and biased data distribution, directly\ntreating them equally will corrupt the generalization ability. To settle the\nissue, we propose a novel Dual Reweighting Domain Generalization (DRDG)\nframework which iteratively reweights the relative importance between samples\nto further improve the generalization. Concretely, Sample Reweighting Module is\nfirst proposed to identify samples with relatively large domain bias, and\nreduce their impact on the overall optimization. Afterwards, Feature\nReweighting Module is introduced to focus on these samples and extract more\ndomain-irrelevant features via a self-distilling mechanism. Combined with the\ndomain discriminator, the iteration of the two modules promotes the extraction\nof generalized features. Extensive experiments and visualizations are presented\nto demonstrate the effectiveness and interpretability of our method against the\nstate-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:24:34 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Liu", "Shubao", ""], ["Zhang", "Ke-Yue", ""], ["Yao", "Taiping", ""], ["Sheng", "Kekai", ""], ["Ding", "Shouhong", ""], ["Tai", "Ying", ""], ["Li", "Jilin", ""], ["Xie", "Yuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2106.16129", "submitter": "Tommaso Cavallari", "authors": "Mihaela C\\u{a}t\\u{a}lina Stoian, Tommaso Cavallari", "title": "Recurrently Estimating Reflective Symmetry Planes from Partial\n  Pointclouds", "comments": "Presented at the CVPR 2021 Workshop on 3D Vision and Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many man-made objects are characterised by a shape that is symmetric along\none or more planar directions. Estimating the location and orientation of such\nsymmetry planes can aid many tasks such as estimating the overall orientation\nof an object of interest or performing shape completion, where a partial scan\nof an object is reflected across the estimated symmetry plane in order to\nobtain a more detailed shape. Many methods processing 3D data rely on expensive\n3D convolutions. In this paper we present an alternative novel encoding that\ninstead slices the data along the height dimension and passes it sequentially\nto a 2D convolutional recurrent regression scheme. The method also comprises a\ndifferentiable least squares step, allowing for end-to-end accurate and fast\nprocessing of both full and partial scans of symmetric objects. We use this\napproach to efficiently handle 3D inputs to design a method to estimate planar\nreflective symmetries. We show that our approach has an accuracy comparable to\nstate-of-the-art techniques on the task of planar reflective symmetry\nestimation on full synthetic objects. Additionally, we show that it can be\ndeployed on partial scans of objects in a real-world pipeline to improve the\noutputs of a 3D object detector.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:26:15 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Stoian", "Mihaela C\u0103t\u0103lina", ""], ["Cavallari", "Tommaso", ""]]}, {"id": "2106.16136", "submitter": "Jiajun Deng", "authors": "Yuechen Wang, Jiajun Deng, Wengang Zhou, and Houqiang Li", "title": "Weakly Supervised Temporal Adjacent Network for Language Grounding", "comments": "Accepted by IEEE Transactions on Multimedia, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal language grounding (TLG) is a fundamental and challenging problem\nfor vision and language understanding. Existing methods mainly focus on fully\nsupervised setting with temporal boundary labels for training, which, however,\nsuffers expensive cost of annotation. In this work, we are dedicated to weakly\nsupervised TLG, where multiple description sentences are given to an untrimmed\nvideo without temporal boundary labels. In this task, it is critical to learn a\nstrong cross-modal semantic alignment between sentence semantics and visual\ncontent. To this end, we introduce a novel weakly supervised temporal adjacent\nnetwork (WSTAN) for temporal language grounding. Specifically, WSTAN learns\ncross-modal semantic alignment by exploiting temporal adjacent network in a\nmultiple instance learning (MIL) paradigm, with a whole description paragraph\nas input. Moreover, we integrate a complementary branch into the framework,\nwhich explicitly refines the predictions with pseudo supervision from the MIL\nstage. An additional self-discriminating loss is devised on both the MIL branch\nand the complementary branch, aiming to enhance semantic discrimination by\nself-supervising. Extensive experiments are conducted on three widely used\nbenchmark datasets, \\emph{i.e.}, ActivityNet-Captions, Charades-STA, and\nDiDeMo, and the results demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:42:08 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wang", "Yuechen", ""], ["Deng", "Jiajun", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""]]}, {"id": "2106.16139", "submitter": "Huseyin Uvet", "authors": "Abdurrahim Yilmaz, Rahmetullah Varol, Fatih Goktay, Gulsum Gencoglan,\n  Ali Anil Demircali, Berk Dilsizoglu, Huseyin Uvet", "title": "Automated Onychomycosis Detection Using Deep Neural Networks", "comments": "It was noticed that the data collection procedure was not performed\n  in accordance with the high standards we normally follow in our studies. As\n  such, the dataset that was described in Section II-A will be created from\n  scratch. We may need to make changes in the methodology and reevaluate the\n  results. Also, significance of the study need to be changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Clinical dermatology, still relies heavily on manual introspection of fungi\nwithin a Potassium Hydroxide (KOH) solution using a brightfield microscope.\nHowever, this method takes a long time, is based on the experience of the\nclinician, and has a low accuracy. With the increase of neural network\napplications in the field of clinical microscopy it is now possible to automate\nsuch manual processes increasing both efficiency and accuracy. This study\npresents a deep neural network structure that enables the rapid solutions for\nthese problems and can perform automatic fungi detection in grayscale images\nwithout colorants. Microscopic images of 81 fungi and 235 ceratine were\ncollected. Then, smaller patches were extracted containing 2062 fungi and 2142\nceratine. In order to detect fungus and ceratine, two models were created one\nof which was a custom neural network and the other was based on the VGG16\narchitecture. The developed custom model had 99.84% accuracy, and an area under\nthe curve (AUC) value of 1.00, while the VGG16 model had 98.89% accuracy and an\nAUC value of 0.99. However, average accuracy and AUC value of clinicians is\n72.8% and 0.87 respectively. This deep learning model allows the development of\nan automated system that can detect fungi within microscopic images.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:45:47 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 07:06:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Yilmaz", "Abdurrahim", ""], ["Varol", "Rahmetullah", ""], ["Goktay", "Fatih", ""], ["Gencoglan", "Gulsum", ""], ["Demircali", "Ali Anil", ""], ["Dilsizoglu", "Berk", ""], ["Uvet", "Huseyin", ""]]}, {"id": "2106.16162", "submitter": "Anuja Vats", "authors": "Anuja Vats, Marius Pedersen, Ahmed Mohammed,{\\O}istein Hovde", "title": "Learning More for Free - A Multi Task Learning Approach for Improved\n  Pathology Classification in Capsule Endoscopy", "comments": "MICCAI 2021 (Provisional accept)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The progress in Computer Aided Diagnosis (CADx) of Wireless Capsule Endoscopy\n(WCE) is thwarted by the lack of data. The inadequacy in richly representative\nhealthy and abnormal conditions results in isolated analyses of pathologies,\nthat can not handle realistic multi-pathology scenarios. In this work, we\nexplore how to learn more for free, from limited data through solving a WCE\nmulticentric, multi-pathology classification problem. Learning more implies to\nlearning more than full supervision would allow with the same data. This is\ndone by combining self supervision with full supervision, under multi task\nlearning. Additionally, we draw inspiration from the Human Visual System (HVS)\nin designing self supervision tasks and investigate if seemingly ineffectual\nsignals within the data itself can be exploited to gain performance, if so,\nwhich signals would be better than others. Further, we present our analysis of\nthe high level features as a stepping stone towards more robust multi-pathology\nCADx in WCE.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:55:17 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Vats", "Anuja", ""], ["Pedersen", "Marius", ""], ["Mohammed", "Ahmed", ""], ["Hovde", "\u00d8istein", ""]]}, {"id": "2106.16174", "submitter": "Pingjun Chen", "authors": "Pingjun Chen, Muhammad Aminu, Siba El Hussein, Joseph Khoury, Jia Wu", "title": "Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in\n  Lymphoid Neoplasms", "comments": "Accepted by MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cells and their spatial patterns in the tumor microenvironment (TME) play\na key role in tumor evolution, and yet remains an understudied topic in\ncomputational pathology. This study, to the best of our knowledge, is among the\nfirst to hybrid local and global graph methods to profile orchestration and\ninteraction of cellular components. To address the challenge in hematolymphoid\ncancers where the cell classes in TME are unclear, we first implemented cell\nlevel unsupervised learning and identified two new cell subtypes. Local cell\ngraphs or supercells were built for each image by considering the individual\ncell's geospatial location and classes. Then, we applied supercell level\nclustering and identified two new cell communities. In the end, we built global\ngraphs to abstract spatial interaction patterns and extract features for\ndisease diagnosis. We evaluate the proposed algorithm on H\\&E slides of 60\nhematolymphoid neoplasm patients and further compared it with three cell level\ngraph-based algorithms, including the global cell graph, cluster cell graph,\nand FLocK. The proposed algorithm achieves a mean diagnosis accuracy of 0.703\nwith the repeated 5-fold cross-validation scheme. In conclusion, our algorithm\nshows superior performance over the existing methods and can be potentially\napplied to other cancer types.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:09:32 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chen", "Pingjun", ""], ["Aminu", "Muhammad", ""], ["Hussein", "Siba El", ""], ["Khoury", "Joseph", ""], ["Wu", "Jia", ""]]}, {"id": "2106.16198", "submitter": "Spandan Madan", "authors": "Spandan Madan, Tomotake Sasaki, Tzu-Mao Li, Xavier Boix, Hanspeter\n  Pfister", "title": "Small in-distribution changes in 3D perspective and lighting fool both\n  CNNs and Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks are susceptible to small transformations including 2D\nrotations and shifts, image crops, and even changes in object colors. This is\noften attributed to biases in the training dataset, and the lack of 2D\nshift-invariance due to not respecting the sampling theorem. In this paper, we\nchallenge this hypothesis by training and testing on unbiased datasets, and\nshowing that networks are brittle to both small 3D perspective changes and\nlighting variations which cannot be explained by dataset bias or lack of\nshift-invariance. To find these in-distribution errors, we introduce an\nevolution strategies (ES) based approach, which we call CMA-Search. Despite\ntraining with a large-scale (0.5 million images), unbiased dataset of camera\nand light variations, in over 71% cases CMA-Search can find camera parameters\nin the vicinity of a correctly classified image which lead to in-distribution\nmisclassifications with < 3.6% change in parameters. With lighting changes,\nCMA-Search finds misclassifications in 33% cases with < 11.6% change in\nparameters. Finally, we extend this method to find misclassifications in the\nvicinity of ImageNet images for both ResNet and OpenAI's CLIP model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:49:19 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Madan", "Spandan", ""], ["Sasaki", "Tomotake", ""], ["Li", "Tzu-Mao", ""], ["Boix", "Xavier", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2106.16209", "submitter": "Lars Schmarje", "authors": "Lars Schmarje and Monty Santarossa and Simon-Martin Schr\\\"oder and\n  Claudius Zelenka and Rainer Kiko and Jenny Stracke and Nina Volkmann and\n  Reinhard Koch", "title": "S2C2 - An orthogonal method for Semi-Supervised Learning on fuzzy labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Supervised Learning (SSL) can decrease the amount of required labeled\nimage data and thus the cost for deep learning. Most SSL methods only consider\na clear distinction between classes but in many real-world datasets, this clear\ndistinction is not given due to intra- or interobserver variability. This\nvariability can lead to different annotations per image. Thus many images have\nambiguous annotations and their label needs to be considered \"fuzzy\". This\nfuzziness of labels must be addressed as it will limit the performance of\nSemi-Supervised Learning (SSL) and deep learning in general. We propose\nSemi-Supervised Classification & Clustering (S2C2) which can extend many deep\nSSL algorithms. S2C2 can estimate the fuzziness of a label and applies SSL as a\nclassification to certainly labeled data while creating distinct clusters for\nimages with similar but fuzzy labels. We show that S2C2 results in median 7.4%\nbetter F1-score for classifications and 5.4% lower inner distance of clusters\nacross multiple SSL algorithms and datasets while being more interpretable due\nto the fuzziness estimation of our method. Overall, a combination of\nSemi-Supervised Learning with our method S2C2 leads to better handling of the\nfuzziness of labels and thus real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 17:00:47 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Schmarje", "Lars", ""], ["Santarossa", "Monty", ""], ["Schr\u00f6der", "Simon-Martin", ""], ["Zelenka", "Claudius", ""], ["Kiko", "Rainer", ""], ["Stracke", "Jenny", ""], ["Volkmann", "Nina", ""], ["Koch", "Reinhard", ""]]}, {"id": "2106.16237", "submitter": "Saurabh Mishra", "authors": "Himanshu Arora, Saurabh Mishra, Shichong Peng, Ke Li, Ali\n  Mahdavi-Amiri", "title": "Multimodal Shape Completion via IMLE", "comments": "Project Website:\n  https://sites.google.com/site/alimahdaviamiri/projects/shape-completion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shape completion is the problem of completing partial input shapes such as\npartial scans. This problem finds important applications in computer vision and\nrobotics due to issues such as occlusion or sparsity in real-world data.\nHowever, most of the existing research related to shape completion has been\nfocused on completing shapes by learning a one-to-one mapping which limits the\ndiversity and creativity of the produced results. We propose a novel multimodal\nshape completion technique that is effectively able to learn a one-to-many\nmapping and generates diverse complete shapes. Our approach is based on the\nconditional Implicit MaximumLikelihood Estimation (IMLE) technique wherein we\ncondition our inputs on partial 3D point clouds. We extensively evaluate our\napproach by comparing it to various baselines both quantitatively and\nqualitatively. We show that our method is superior to alternatives in terms of\ncompleteness and diversity of shapes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 17:45:10 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 17:41:55 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Arora", "Himanshu", ""], ["Mishra", "Saurabh", ""], ["Peng", "Shichong", ""], ["Li", "Ke", ""], ["Mahdavi-Amiri", "Ali", ""]]}, {"id": "2106.16245", "submitter": "Wei-Lun Chao", "authors": "Han-Jia Ye, Wei-Lun Chao", "title": "How to Train Your MAML to Excel in Few-Shot Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Model-agnostic meta-learning (MAML) is arguably the most popular\nmeta-learning algorithm nowadays, given its flexibility to incorporate various\nmodel architectures and to be applied to different problems. Nevertheless, its\nperformance on few-shot classification is far behind many recent algorithms\ndedicated to the problem. In this paper, we point out several key facets of how\nto train MAML to excel in few-shot classification. First, we find that a large\nnumber of gradient steps are needed for the inner loop update, which\ncontradicts the common usage of MAML for few-shot classification. Second, we\nfind that MAML is sensitive to the permutation of class assignments in\nmeta-testing: for a few-shot task of $N$ classes, there are exponentially many\nways to assign the learned initialization of the $N$-way classifier to the $N$\nclasses, leading to an unavoidably huge variance. Third, we investigate several\nways for permutation invariance and find that learning a shared classifier\ninitialization for all the classes performs the best. On benchmark datasets\nsuch as MiniImageNet and TieredImageNet, our approach, which we name\nUNICORN-MAML, performs on a par with or even outperforms state-of-the-art\nalgorithms, while keeping the simplicity of MAML without adding any extra\nsub-networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 17:56:15 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ye", "Han-Jia", ""], ["Chao", "Wei-Lun", ""]]}]