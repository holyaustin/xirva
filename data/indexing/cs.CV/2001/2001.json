[{"id": "2001.00030", "submitter": "Sirui Lu", "authors": "Sirui Lu, Lu-Ming Duan, Dong-Ling Deng", "title": "Quantum Adversarial Machine Learning", "comments": "22 pages, 16 figures, 5 tables", "journal-ref": "Phys. Rev. Research 2, 033212 (2020)", "doi": "10.1103/PhysRevResearch.2.033212", "report-no": null, "categories": "quant-ph cond-mat.dis-nn cond-mat.str-el cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial machine learning is an emerging field that focuses on studying\nvulnerabilities of machine learning approaches in adversarial settings and\ndeveloping techniques accordingly to make learning robust to adversarial\nmanipulations. It plays a vital role in various machine learning applications\nand has attracted tremendous attention across different communities recently.\nIn this paper, we explore different adversarial scenarios in the context of\nquantum machine learning. We find that, similar to traditional classifiers\nbased on classical neural networks, quantum learning systems are likewise\nvulnerable to crafted adversarial examples, independent of whether the input\ndata is classical or quantum. In particular, we find that a quantum classifier\nthat achieves nearly the state-of-the-art accuracy can be conclusively deceived\nby adversarial examples obtained via adding imperceptible perturbations to the\noriginal legitimate samples. This is explicitly demonstrated with quantum\nadversarial learning in different scenarios, including classifying real-life\nimages (e.g., handwritten digit images in the dataset MNIST), learning phases\nof matter (such as, ferromagnetic/paramagnetic orders and symmetry protected\ntopological phases), and classifying quantum data. Furthermore, we show that\nbased on the information of the adversarial examples at hand, practical defense\nstrategies can be designed to fight against a number of different attacks. Our\nresults uncover the notable vulnerability of quantum machine learning systems\nto adversarial perturbations, which not only reveals a novel perspective in\nbridging machine learning and quantum physics in theory but also provides\nvaluable guidance for practical applications of quantum classifiers based on\nboth near-term and future quantum technologies.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 19:00:12 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Lu", "Sirui", ""], ["Duan", "Lu-Ming", ""], ["Deng", "Dong-Ling", ""]]}, {"id": "2001.00035", "submitter": "Shuo Huang", "authors": "Shuo Huang, Ke wu, Xiaolin Meng and Cheng Li", "title": "Non-rigid Registration Method between 3D CT Liver Data and 2D Ultrasonic\n  Images based on Demons Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-rigid registration between CT data and ultrasonic images of liver can\nfacilitate the diagnosis and treatment, which has been widely studied in recent\nyears. To improve the registration accuracy of the Demons model on the\nnon-rigid registration between 3D CT liver data and 2D ultrasonic images, a\nnovel boundary extraction and enhancement method based on radial directional\nlocal intuitionistic fuzzy entropy in the polar coordinates has been put\nforward, and a new registration workflow has been provided. Experiments show\nthat our method can acquire high-accuracy registration results. Experiments\nalso show that the accuracy of the results of our method is higher than that of\nthe original Demons method and the Demons method using simulated ultrasonic\nimage by Field II. The operation time of our registration workflow is about 30\nseconds, and it can be used in the surgery.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 19:01:04 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Huang", "Shuo", ""], ["wu", "Ke", ""], ["Meng", "Xiaolin", ""], ["Li", "Cheng", ""]]}, {"id": "2001.00057", "submitter": "Bhairav Chidambaram", "authors": "Bhairav Chidambaram, Mason McGill, Pietro Perona", "title": "HMM-guided frame querying for bandwidth-constrained video search", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design an agent to search for frames of interest in video stored on a\nremote server, under bandwidth constraints. Using a convolutional neural\nnetwork to score individual frames and a hidden Markov model to propagate\npredictions across frames, our agent accurately identifies temporal regions of\ninterest based on sparse, strategically sampled frames. On a subset of the\nImageNet-VID dataset, we demonstrate that using a hidden Markov model to\ninterpolate between frame scores allows requests of 98% of frames to be\nomitted, without compromising frame-of-interest classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 19:54:35 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Chidambaram", "Bhairav", ""], ["McGill", "Mason", ""], ["Perona", "Pietro", ""]]}, {"id": "2001.00060", "submitter": "Issam Hammad", "authors": "Issam Hammad, Kamal El-Sankary, and Jason Gu", "title": "Deep Learning Training with Simulated Approximate Multipliers", "comments": "Presented at: IEEE International Conference on Robotics and\n  Biomimetics (ROBIO) 2019, Dali, China, December 2019. WINNER OF THE MOZI BEST\n  PAPER IN AI AWARD", "journal-ref": "2019 IEEE International Conference on Robotics and Biomimetics\n  (ROBIO)", "doi": "10.1109/ROBIO49542.2019.8961780", "report-no": null, "categories": "cs.LG cs.CV cs.PF eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents by simulation how approximate multipliers can be utilized\nto enhance the training performance of convolutional neural networks (CNNs).\nApproximate multipliers have significantly better performance in terms of\nspeed, power, and area compared to exact multipliers. However, approximate\nmultipliers have an inaccuracy which is defined in terms of the Mean Relative\nError (MRE). To assess the applicability of approximate multipliers in\nenhancing CNN training performance, a simulation for the impact of approximate\nmultipliers error on CNN training is presented. The paper demonstrates that\nusing approximate multipliers for CNN training can significantly enhance the\nperformance in terms of speed, power, and area at the cost of a small negative\nimpact on the achieved accuracy. Additionally, the paper proposes a hybrid\ntraining method which mitigates this negative impact on the accuracy. Using the\nproposed hybrid method, the training can start using approximate multipliers\nthen switches to exact multipliers for the last few epochs. Using this method,\nthe performance benefits of approximate multipliers in terms of speed, power,\nand area can be attained for a large portion of the training stage. On the\nother hand, the negative impact on the accuracy is diminished by using the\nexact multipliers for the last epochs of training.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 12:50:06 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 13:22:32 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hammad", "Issam", ""], ["El-Sankary", "Kamal", ""], ["Gu", "Jason", ""]]}, {"id": "2001.00071", "submitter": "Sumit Mukherjee", "authors": "Sumit Mukherjee, Yixi Xu, Anusua Trivedi, Juan Lavista Ferres", "title": "privGAN: Protecting GANs from membership inference attacks at low cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have made releasing of synthetic\nimages a viable approach to share data without releasing the original dataset.\nIt has been shown that such synthetic data can be used for a variety of\ndownstream tasks such as training classifiers that would otherwise require the\noriginal dataset to be shared. However, recent work has shown that the GAN\nmodels and their synthetically generated data can be used to infer the training\nset membership by an adversary who has access to the entire dataset and some\nauxiliary information. Current approaches to mitigate this problem (such as\nDPGAN) lead to dramatically poorer generated sample quality than the original\nnon--private GANs. Here we develop a new GAN architecture (privGAN), where the\ngenerator is trained not only to cheat the discriminator but also to defend\nmembership inference attacks. The new mechanism provides protection against\nthis mode of attack while leading to negligible loss in downstream\nperformances. In addition, our algorithm has been shown to explicitly prevent\noverfitting to the training set, which explains why our protection is so\neffective. The main contributions of this paper are: i) we propose a novel GAN\narchitecture that can generate synthetic data in a privacy preserving manner\nwithout additional hyperparameter tuning and architecture selection, ii) we\nprovide a theoretical understanding of the optimal solution of the privGAN loss\nfunction, iii) we demonstrate the effectiveness of our model against several\nwhite and black--box attacks on several benchmark datasets, iv) we demonstrate\non three common benchmark datasets that synthetic images generated by privGAN\nlead to negligible loss in downstream performance when compared against\nnon--private GANs.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 20:47:21 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 17:44:07 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 06:53:47 GMT"}, {"version": "v4", "created": "Sun, 13 Dec 2020 18:27:26 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Mukherjee", "Sumit", ""], ["Xu", "Yixi", ""], ["Trivedi", "Anusua", ""], ["Ferres", "Juan Lavista", ""]]}, {"id": "2001.00116", "submitter": "Qiang Zeng", "authors": "Fei Zuo, Qiang Zeng", "title": "Exploiting the Sensitivity of $L_2$ Adversarial Examples to\n  Erase-and-Restore", "comments": "Accepted to AsiaCCS'21 on 10/24/2020; 12 pages; the code, datasets,\n  and models will be made publicly available when the paper is presented", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By adding carefully crafted perturbations to input images, adversarial\nexamples (AEs) can be generated to mislead neural-network-based image\nclassifiers. $L_2$ adversarial perturbations by Carlini and Wagner (CW) are\namong the most effective but difficult-to-detect attacks. While many\ncountermeasures against AEs have been proposed, detection of adaptive CW-$L_2$\nAEs is still an open question. We find that, by randomly erasing some pixels in\nan $L_2$ AE and then restoring it with an inpainting technique, the AE, before\nand after the steps, tends to have different classification results, while a\nbenign sample does not show this symptom. We thus propose a novel AE detection\ntechnique, Erase-and-Restore (E&R), that exploits the intriguing sensitivity of\n$L_2$ attacks. Experiments conducted on two popular image datasets, CIFAR-10\nand ImageNet, show that the proposed technique is able to detect over 98% of\n$L_2$ AEs and has a very low false positive rate on benign images. The\ndetection technique exhibits high transferability: a detection system trained\nusing CW-$L_2$ AEs can accurately detect AEs generated using another $L_2$\nattack method. More importantly, our approach demonstrates strong resilience to\nadaptive $L_2$ attacks, filling a critical gap in AE detection. Finally, we\ninterpret the detection technique through both visualization and\nquantification.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 00:15:07 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 23:48:02 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zuo", "Fei", ""], ["Zeng", "Qiang", ""]]}, {"id": "2001.00138", "submitter": "Xiaolong Ma", "authors": "Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin,\n  Yanzhi Wang, Bin Ren", "title": "PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with\n  Pattern-based Weight Pruning", "comments": "To be published in the Proceedings of Twenty-Fifth International\n  Conference on Architectural Support for Programming Languages and Operating\n  Systems (ASPLOS 20)", "journal-ref": null, "doi": "10.1145/3373376.3378534", "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of a spectrum of high-end mobile devices, many\napplications that formerly required desktop-level computation capability are\nbeing transferred to these devices. However, executing the inference of Deep\nNeural Networks (DNNs) is still challenging considering high computation and\nstorage demands, specifically, if real-time performance with high accuracy is\nneeded. Weight pruning of DNNs is proposed, but existing schemes represent two\nextremes in the design space: non-structured pruning is fine-grained, accurate,\nbut not hardware friendly; structured pruning is coarse-grained,\nhardware-efficient, but with higher accuracy loss. In this paper, we introduce\na new dimension, fine-grained pruning patterns inside the coarse-grained\nstructures, revealing a previously unknown point in design space. With the\nhigher accuracy enabled by fine-grained pruning patterns, the unique insight is\nto use the compiler to re-gain and guarantee high hardware efficiency. In other\nwords, our method achieves the best of both worlds, and is desirable across\ntheory/algorithm, compiler, and hardware levels. The proposed PatDNN is an\nend-to-end framework to efficiently execute DNN on mobile devices with the help\nof a novel model compression technique (pattern-based pruning based on extended\nADMM solution framework) and a set of thorough architecture-aware compiler- and\ncode generation-based optimizations (filter kernel reordering, compressed\nweight storage, register load redundancy elimination, and parameter\nauto-tuning). Evaluation results demonstrate that PatDNN outperforms three\nstate-of-the-art end-to-end DNN frameworks, TensorFlow Lite, TVM, and Alibaba\nMobile Neural Network with speedup up to 44.5x, 11.4x, and 7.1x, respectively,\nwith no accuracy compromise. Real-time inference of representative large-scale\nDNNs (e.g., VGG-16, ResNet-50) can be achieved using mobile devices.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 04:52:07 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 00:27:57 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 04:32:38 GMT"}, {"version": "v4", "created": "Wed, 22 Jan 2020 04:13:06 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Niu", "Wei", ""], ["Ma", "Xiaolong", ""], ["Lin", "Sheng", ""], ["Wang", "Shihao", ""], ["Qian", "Xuehai", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""], ["Ren", "Bin", ""]]}, {"id": "2001.00139", "submitter": "Rizwan Ahmed Khan", "authors": "Jamshed Memon, Maira Sami, Rizwan Ahmed Khan", "title": "Handwritten Optical Character Recognition (OCR): A Comprehensive\n  Systematic Literature Review (SLR)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the ubiquity of handwritten documents in human transactions, Optical\nCharacter Recognition (OCR) of documents have invaluable practical worth.\nOptical character recognition is a science that enables to translate various\ntypes of documents or images into analyzable, editable and searchable data.\nDuring last decade, researchers have used artificial intelligence / machine\nlearning tools to automatically analyze handwritten and printed documents in\norder to convert them into electronic format. The objective of this review\npaper is to summarize research that has been conducted on character recognition\nof handwritten documents and to provide research directions. In this Systematic\nLiterature Review (SLR) we collected, synthesized and analyzed research\narticles on the topic of handwritten OCR (and closely related topics) which\nwere published between year 2000 to 2018. We followed widely used electronic\ndatabases by following pre-defined review protocol. Articles were searched\nusing keywords, forward reference searching and backward reference searching in\norder to search all the articles related to the topic. After carefully\nfollowing study selection process 142 articles were selected for this SLR. This\nreview article serves the purpose of presenting state of the art results and\ntechniques on OCR and also provide research directions by highlighting research\ngaps.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 04:55:04 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Memon", "Jamshed", ""], ["Sami", "Maira", ""], ["Khan", "Rizwan Ahmed", ""]]}, {"id": "2001.00149", "submitter": "Shuo Huang", "authors": "Ping Zhou, Shuo Huang, Qiang Chen, Siyuan He, Guochao Cai", "title": "Simulation of Skin Stretching around the Forehead Wrinkles in\n  Rhytidectomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Skin stretching around the forehead wrinkles is an important\nmethod in rhytidectomy. Proper parameters are required to evaluate the surgical\neffect. In this paper, a simulation method was proposed to obtain the\nparameters. Methods: Three-dimensional point cloud data with a resolution of 50\n{\\mu}m were employed. First, a smooth supporting contour under the wrinkled\nforehead was generated via b-spline interpolation and extrapolation to\nconstrain the deformation of the wrinkled zone. Then, based on the vector\nformed intrinsic finite element (VFIFE) algorithm, the simulation was\nimplemented in Matlab for the deformation of wrinkled forehead skin in the\nstretching process. Finally, the stress distribution and the residual wrinkles\nof forehead skin were employed to evaluate the surgical effect. Results:\nAlthough the residual wrinkles are similar when forehead wrinkles are finitely\nstretched, their stress distribution changes greatly. This indicates that the\nstress distribution in the skin is effective to evaluate the surgical effect,\nand the forehead wrinkles are easily to be overstretched, which may lead to\npotential skin injuries. Conclusion: The simulation method can predict stress\ndistribution and residual wrinkles after forehead wrinkle stretching surgery,\nwhich can be potentially used to control the surgical process and further\nreduce risks of skin injury.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 06:06:56 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Zhou", "Ping", ""], ["Huang", "Shuo", ""], ["Chen", "Qiang", ""], ["He", "Siyuan", ""], ["Cai", "Guochao", ""]]}, {"id": "2001.00150", "submitter": "Shuo Huang", "authors": "Shuo Huang, Suiren Wan", "title": "A Total Variation Denoising Method Based on Median Filter and Phase\n  Consistency", "comments": null, "journal-ref": "Sens Imaging 21, 19 (2020)", "doi": "10.1007/s11220-020-00281-8", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total variation method is widely used in image noise suppression.\nHowever, this method is easy to cause the loss of image details, and it is also\nsensitive to parameters such as iteration time. In this work, the total\nvariation method has been modified using a diffusion rate adjuster based on the\nphase congruency and a fusion filter of median filter and phase consistency\nboundary, which is called the MPC-TV method. Experimental results indicate that\nMPC-TV method is effective in noise suppression, especially for the removing of\nspeckle noise, and it can also improve the robustness of iteration time of TV\nmethod on noise with different variance.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 06:15:42 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Huang", "Shuo", ""], ["Wan", "Suiren", ""]]}, {"id": "2001.00170", "submitter": "Chunli Qin", "authors": "Chunli Qin, Demin Yao, Han Zhuang, Hui Wang, Yonghong Shi, and Zhijian\n  Song", "title": "Residual Block-based Multi-Label Classification and Localization Network\n  with Integral Regression for Vertebrae Labeling", "comments": "10 pages with 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate identification and localization of the vertebrae in CT scans is a\ncritical and standard preprocessing step for clinical spinal diagnosis and\ntreatment. Existing methods are mainly based on the integration of multiple\nneural networks, and most of them use the Gaussian heat map to locate the\nvertebrae's centroid. However, the process of obtaining the vertebrae's\ncentroid coordinates using heat maps is non-differentiable, so it is impossible\nto train the network to label the vertebrae directly. Therefore, for end-to-end\ndifferential training of vertebra coordinates on CT scans, a robust and\naccurate automatic vertebral labeling algorithm is proposed in this study.\nFirstly, a novel residual-based multi-label classification and localization\nnetwork is developed, which can capture multi-scale features, but also utilize\nthe residual module and skip connection to fuse the multi-level features.\nSecondly, to solve the problem that the process of finding coordinates is\nnon-differentiable and the spatial structure is not destructible, integral\nregression module is used in the localization network. It combines the\nadvantages of heat map representation and direct regression coordinates to\nachieve end-to-end training, and can be compatible with any key point detection\nmethods of medical image based on heat map. Finally, multi-label classification\nof vertebrae is carried out, which use bidirectional long short term memory\n(Bi-LSTM) to enhance the learning of long contextual information to improve the\nclassification performance. The proposed method is evaluated on a challenging\ndataset and the results are significantly better than the state-of-the-art\nmethods (mean localization error <3mm).\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 09:16:10 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Qin", "Chunli", ""], ["Yao", "Demin", ""], ["Zhuang", "Han", ""], ["Wang", "Hui", ""], ["Shi", "Yonghong", ""], ["Song", "Zhijian", ""]]}, {"id": "2001.00179", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales,\n  Javier Ortega-Garcia", "title": "DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection", "comments": null, "journal-ref": "Information Fusion, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The free access to large-scale public databases, together with the fast\nprogress of deep learning techniques, in particular Generative Adversarial\nNetworks, have led to the generation of very realistic fake content with its\ncorresponding implications towards society in this era of fake news. This\nsurvey provides a thorough review of techniques for manipulating face images\nincluding DeepFake methods, and methods to detect such manipulations. In\nparticular, four types of facial manipulation are reviewed: i) entire face\nsynthesis, ii) identity swap (DeepFakes), iii) attribute manipulation, and iv)\nexpression swap. For each manipulation group, we provide details regarding\nmanipulation techniques, existing public databases, and key benchmarks for\ntechnology evaluation of fake detection methods, including a summary of results\nfrom those evaluations. Among all the aspects discussed in the survey, we pay\nspecial attention to the latest generation of DeepFakes, highlighting its\nimprovements and challenges for fake detection.\n  In addition to the survey information, we also discuss open issues and future\ntrends that should be considered to advance in the field.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 09:54:34 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 07:22:46 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 18:17:43 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""], ["Morales", "Aythami", ""], ["Ortega-Garcia", "Javier", ""]]}, {"id": "2001.00187", "submitter": "Yihua Cheng", "authors": "Yihua Cheng, Shiyao Huang, Fei Wang, Chen Qian, Feng Lu", "title": "A Coarse-to-Fine Adaptive Network for Appearance-Based Gaze Estimation", "comments": "9 pages, 7figures, AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human gaze is essential for various appealing applications. Aiming at more\naccurate gaze estimation, a series of recent works propose to utilize face and\neye images simultaneously. Nevertheless, face and eye images only serve as\nindependent or parallel feature sources in those works, the intrinsic\ncorrelation between their features is overlooked. In this paper we make the\nfollowing contributions: 1) We propose a coarse-to-fine strategy which\nestimates a basic gaze direction from face image and refines it with\ncorresponding residual predicted from eye images. 2) Guided by the proposed\nstrategy, we design a framework which introduces a bi-gram model to bridge gaze\nresidual and basic gaze direction, and an attention component to adaptively\nacquire suitable fine-grained feature. 3) Integrating the above innovations, we\nconstruct a coarse-to-fine adaptive network named CA-Net and achieve\nstate-of-the-art performances on MPIIGaze and EyeDiap.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 10:39:03 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Cheng", "Yihua", ""], ["Huang", "Shiyao", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Lu", "Feng", ""]]}, {"id": "2001.00208", "submitter": "Xi Fang", "authors": "Xi Fang, Pingkun Yan", "title": "Multi-organ Segmentation over Partially Labeled Datasets with\n  Multi-scale Feature Abstraction", "comments": "Accepted for publication at IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shortage of fully annotated datasets has been a limiting factor in developing\ndeep learning based image segmentation algorithms and the problem becomes more\npronounced in multi-organ segmentation. In this paper, we propose a unified\ntraining strategy that enables a novel multi-scale deep neural network to be\ntrained on multiple partially labeled datasets for multi-organ segmentation. In\naddition, a new network architecture for multi-scale feature abstraction is\nproposed to integrate pyramid input and feature analysis into a U-shape pyramid\nstructure. To bridge the semantic gap caused by directly merging features from\ndifferent scales, an equal convolutional depth mechanism is introduced.\nFurthermore, we employ a deep supervision mechanism to refine the outputs in\ndifferent scales. To fully leverage the segmentation features from all the\nscales, we design an adaptive weighting layer to fuse the outputs in an\nautomatic fashion. All these mechanisms together are integrated into a Pyramid\nInput Pyramid Output Feature Abstraction Network (PIPO-FAN). Our proposed\nmethod was evaluated on four publicly available datasets, including BTCV, LiTS,\nKiTS and Spleen, where very promising performance has been achieved. The source\ncode of this work is publicly shared at https://github.com/DIAL-RPI/PIPO-FAN\nfor others to easily reproduce the work and build their own models with the\nintroduced mechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 13:51:11 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 14:44:25 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Fang", "Xi", ""], ["Yan", "Pingkun", ""]]}, {"id": "2001.00215", "submitter": "Joshua Peeples", "authors": "Joshua Peeples, Weihuang Xu, and Alina Zare", "title": "Histogram Layers for Texture Analysis", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a histogram layer for artificial neural networks (ANNs). An\nessential aspect of texture analysis is the extraction of features that\ndescribe the distribution of values in local spatial regions. The proposed\nhistogram layer directly computes the spatial distribution of features for\ntexture analysis and parameters for the layer are estimated during\nbackpropagation. We compare our method with state-of-the-art texture encoding\nmethods such as the Deep Encoding Network Pooling (DEP), Deep Texture Encoding\nNetwork (DeepTEN), Fisher Vector convolutional neural network (FV-CNN), and\nMulti-level Texture Encoding and Representation (MuLTER) on three\nmaterial/texture datasets: (1) the Describable Texture Dataset (DTD); (2) an\nextension of the ground terrain in outdoor scenes (GTOS-mobile); (3) and a\nsubset of the Materials in Context (MINC-2500) dataset. Results indicate that\nthe inclusion of the proposed histogram layer improves performance. The source\ncode for the histogram layer is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 14:41:54 GMT"}, {"version": "v10", "created": "Thu, 22 Apr 2021 21:24:34 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 02:05:44 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 19:59:16 GMT"}, {"version": "v4", "created": "Wed, 25 Mar 2020 00:09:51 GMT"}, {"version": "v5", "created": "Fri, 27 Mar 2020 16:56:22 GMT"}, {"version": "v6", "created": "Mon, 30 Mar 2020 17:03:11 GMT"}, {"version": "v7", "created": "Fri, 17 Apr 2020 14:20:41 GMT"}, {"version": "v8", "created": "Wed, 22 Apr 2020 15:45:35 GMT"}, {"version": "v9", "created": "Wed, 6 Jan 2021 01:40:47 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Peeples", "Joshua", ""], ["Xu", "Weihuang", ""], ["Zare", "Alina", ""]]}, {"id": "2001.00236", "submitter": "Shubham Goswami", "authors": "Donghoon Chang (1), Vinjohn Chirakkal (2), Shubham Goswami (3),\n  Munawar Hasan (1), Taekwon Jung (2), Jinkeon Kang (1,3), Seok-Cheol Kee (4),\n  Dongkyu Lee (5), Ajit Pratap Singh (1) ((1) Department of Computer Science,\n  IIIT-Delhi, India, (2) Springcloud Inc., Korea, (3) Center for Information\n  Security Technologies (CIST), Korea University, Korea, (4) Smart Car Research\n  Center, Chungbuk National University, Korea, (5) Department of Smart Car\n  Engineering, Chungbuk National University, Korea)", "title": "Multi-lane Detection Using Instance Segmentation and Attentive Voting", "comments": "Accepted in ICCAS 2019 - The 19th International Conference on\n  Control, Automation and Systems, Corresponding Author: Shubham Goswami", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Autonomous driving is becoming one of the leading industrial research areas.\nTherefore many automobile companies are coming up with semi to fully autonomous\ndriving solutions. Among these solutions, lane detection is one of the vital\ndriver-assist features that play a crucial role in the decision-making process\nof the autonomous vehicle. A variety of solutions have been proposed to detect\nlanes on the road, which ranges from using hand-crafted features to the\nstate-of-the-art end-to-end trainable deep learning architectures. Most of\nthese architectures are trained in a traffic constrained environment. In this\npaper, we propose a novel solution to multi-lane detection, which outperforms\nstate of the art methods in terms of both accuracy and speed. To achieve this,\nwe also offer a dataset with a more intuitive labeling scheme as compared to\nother benchmark datasets. Using our approach, we are able to obtain a lane\nsegmentation accuracy of 99.87% running at 54.53 fps (average).\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 16:48:42 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Chang", "Donghoon", ""], ["Chirakkal", "Vinjohn", ""], ["Goswami", "Shubham", ""], ["Hasan", "Munawar", ""], ["Jung", "Taekwon", ""], ["Kang", "Jinkeon", ""], ["Kee", "Seok-Cheol", ""], ["Lee", "Dongkyu", ""], ["Singh", "Ajit Pratap", ""]]}, {"id": "2001.00238", "submitter": "Jurandy Almeida", "authors": "Jurandy Almeida, Cristiano Saltori, Paolo Rota, and Nicu Sebe", "title": "Low-Budget Label Query through Domain Alignment Enforcement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning revolution happened thanks to the availability of a massive\namount of labelled data which have contributed to the development of models\nwith extraordinary inference capabilities. Despite the public availability of a\nlarge quantity of datasets, to address specific requirements it is often\nnecessary to generate a new set of labelled data. Quite often, the production\nof labels is costly and sometimes it requires specific know-how to be\nfulfilled. In this work, we tackle a new problem named low-budget label query\nthat consists in suggesting to the user a small (low budget) set of samples to\nbe labelled, from a completely unlabelled dataset, with the final goal of\nmaximizing the classification accuracy on that dataset. In this work we first\nimprove an Unsupervised Domain Adaptation (UDA) method to better align source\nand target domains using consistency constraints, reaching the state of the art\non a few UDA tasks. Finally, using the previously trained model as reference,\nwe propose a simple yet effective selection method based on uniform sampling of\nthe prediction consistency distribution, which is deterministic and steadily\noutperforms other baselines as well as competing models on a large variety of\npublicly available datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 16:52:44 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 11:43:05 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Almeida", "Jurandy", ""], ["Saltori", "Cristiano", ""], ["Rota", "Paolo", ""], ["Sebe", "Nicu", ""]]}, {"id": "2001.00258", "submitter": "Avinash Kori", "authors": "Mahendra Khened, Avinash Kori, Haran Rajkumar, Balaji Srinivasan,\n  Ganapathy Krishnamurthi", "title": "A Generalized Deep Learning Framework for Whole-Slide Image Segmentation\n  and Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.TO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Histopathology tissue analysis is considered the gold standard in cancer\ndiagnosis and prognosis. Given the large size of these images and the increase\nin the number of potential cancer cases, an automated solution as an aid to\nhistopathologists is highly desirable. In the recent past, deep learning-based\ntechniques have provided state of the art results in a wide variety of image\nanalysis tasks, including analysis of digitized slides. However, the size of\nimages and variability in histopathology tasks makes it a challenge to develop\nan integrated framework for histopathology image analysis. We propose a deep\nlearning-based framework for histopathology tissue analysis. We demonstrate the\ngeneralizability of our framework, including training and inference, on several\nopen-source datasets, which include CAMELYON (breast cancer metastases),\nDigestPath (colon cancer), and PAIP (liver cancer) datasets. We discuss\nmultiple types of uncertainties pertaining to data and model, namely aleatoric\nand epistemic, respectively. Simultaneously, we demonstrate our model\ngeneralization across different data distribution by evaluating some samples on\nTCGA data. On CAMELYON16 test data (n=139) for the task of lesion detection,\nthe FROC score achieved was 0.86 and in the CAMELYON17 test-data (n=500) for\nthe task of pN-staging the Cohen's kappa score achieved was 0.9090 (third in\nthe open leaderboard). On DigestPath test data (n=212) for the task of tumor\nsegmentation, a Dice score of 0.782 was achieved (fourth in the challenge). On\nPAIP test data (n=40) for the task of viable tumor segmentation, a Jaccard\nIndex of 0.75 (third in the challenge) was achieved, and for viable tumor\nburden, a score of 0.633 was achieved (second in the challenge). Our entire\nframework and related documentation are freely available at GitHub and PyPi.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 18:05:44 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 08:29:35 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Khened", "Mahendra", ""], ["Kori", "Avinash", ""], ["Rajkumar", "Haran", ""], ["Srinivasan", "Balaji", ""], ["Krishnamurthi", "Ganapathy", ""]]}, {"id": "2001.00281", "submitter": "Amir Gholami", "authors": "Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W. Mahoney,\n  Kurt Keutzer", "title": "ZeroQ: A Novel Zero Shot Quantization Framework", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization is a promising approach for reducing the inference time and\nmemory footprint of neural networks. However, most existing quantization\nmethods require access to the original training dataset for retraining during\nquantization. This is often not possible for applications with sensitive or\nproprietary data, e.g., due to privacy and security concerns. Existing\nzero-shot quantization methods use different heuristics to address this, but\nthey result in poor performance, especially when quantizing to ultra-low\nprecision. Here, we propose ZeroQ , a novel zero-shot quantization framework to\naddress this. ZeroQ enables mixed-precision quantization without any access to\nthe training or validation data. This is achieved by optimizing for a Distilled\nDataset, which is engineered to match the statistics of batch normalization\nacross different layers of the network. ZeroQ supports both uniform and\nmixed-precision quantization. For the latter, we introduce a novel Pareto\nfrontier based method to automatically determine the mixed-precision bit\nsetting for all layers, with no manual search involved. We extensively test our\nproposed method on a diverse set of models, including ResNet18/50/152,\nMobileNetV2, ShuffleNet, SqueezeNext, and InceptionV3 on ImageNet, as well as\nRetinaNet-ResNet50 on the Microsoft COCO dataset. In particular, we show that\nZeroQ can achieve 1.71\\% higher accuracy on MobileNetV2, as compared to the\nrecently proposed DFQ method. Importantly, ZeroQ has a very low computational\noverhead, and it can finish the entire quantization process in less than 30s\n(0.5\\% of one epoch training time of ResNet50 on ImageNet). We have\nopen-sourced the ZeroQ\nframework\\footnote{https://github.com/amirgholami/ZeroQ}.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 23:58:26 GMT"}], "update_date": "2020-03-29", "authors_parsed": [["Cai", "Yaohui", ""], ["Yao", "Zhewei", ""], ["Dong", "Zhen", ""], ["Gholami", "Amir", ""], ["Mahoney", "Michael W.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2001.00292", "submitter": "Jin Chen", "authors": "Jin Chen, Huihui Song, Kaihua Zhang, Bo Liu, Qingshan Liu", "title": "Video Saliency Prediction Using Enhanced Spatiotemporal Alignment\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to a variety of motions across different frames, it is highly challenging\nto learn an effective spatiotemporal representation for accurate video saliency\nprediction (VSP). To address this issue, we develop an effective spatiotemporal\nfeature alignment network tailored to VSP, mainly including two key\nsub-networks: a multi-scale deformable convolutional alignment network (MDAN)\nand a bidirectional convolutional Long Short-Term Memory (Bi-ConvLSTM) network.\nThe MDAN learns to align the features of the neighboring frames to the\nreference one in a coarse-to-fine manner, which can well handle various\nmotions. Specifically, the MDAN owns a pyramidal feature hierarchy structure\nthat first leverages deformable convolution (Dconv) to align the\nlower-resolution features across frames, and then aggregates the aligned\nfeatures to align the higher-resolution features, progressively enhancing the\nfeatures from top to bottom. The output of MDAN is then fed into the\nBi-ConvLSTM for further enhancement, which captures the useful long-time\ntemporal information along forward and backward timing directions to\neffectively guide attention orientation shift prediction under complex scene\ntransformation. Finally, the enhanced features are decoded to generate the\npredicted saliency map. The proposed model is trained end-to-end without any\nintricate post processing. Extensive evaluations on four VSP benchmark datasets\ndemonstrate that the proposed method achieves favorable performance against\nstate-of-the-art methods. The source codes and all the results will be\nreleased.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 02:05:35 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Chen", "Jin", ""], ["Song", "Huihui", ""], ["Zhang", "Kaihua", ""], ["Liu", "Bo", ""], ["Liu", "Qingshan", ""]]}, {"id": "2001.00294", "submitter": "Dezhao Luo", "authors": "Dezhao Luo, Chang Liu, Yu Zhou, Dongbao Yang, Can Ma, Qixiang Ye,\n  Weiping Wang", "title": "Video Cloze Procedure for Self-Supervised Spatio-Temporal Learning", "comments": "AAAI2020(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel self-supervised method, referred to as Video Cloze\nProcedure (VCP), to learn rich spatial-temporal representations. VCP first\ngenerates \"blanks\" by withholding video clips and then creates \"options\" by\napplying spatio-temporal operations on the withheld clips. Finally, it fills\nthe blanks with \"options\" and learns representations by predicting the\ncategories of operations applied on the clips. VCP can act as either a proxy\ntask or a target task in self-supervised learning. As a proxy task, it converts\nrich self-supervised representations into video clip operations (options),\nwhich enhances the flexibility and reduces the complexity of representation\nlearning. As a target task, it can assess learned representation models in a\nuniform and interpretable manner. With VCP, we train spatial-temporal\nrepresentation models (3D-CNNs) and apply such models on action recognition and\nvideo retrieval tasks. Experiments on commonly used benchmarks show that the\ntrained models outperform the state-of-the-art self-supervised models with\nsignificant margins.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 02:15:24 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Luo", "Dezhao", ""], ["Liu", "Chang", ""], ["Zhou", "Yu", ""], ["Yang", "Dongbao", ""], ["Ma", "Can", ""], ["Ye", "Qixiang", ""], ["Wang", "Weiping", ""]]}, {"id": "2001.00309", "submitter": "Chunhua Shen", "authors": "Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang,\n  Youliang Yan", "title": "BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation", "comments": "Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  2020. Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Instance segmentation is one of the fundamental vision tasks. Recently, fully\nconvolutional instance segmentation methods have drawn much attention as they\nare often simpler and more efficient than two-stage approaches like Mask R-CNN.\nTo date, almost all such approaches fall behind the two-stage Mask R-CNN method\nin mask precision when models have similar computation complexity, leaving\ngreat room for improvement.\n  In this work, we achieve improved mask prediction by effectively combining\ninstance-level information with semantic information with lower-level\nfine-granularity. Our main contribution is a blender module which draws\ninspiration from both top-down and bottom-up instance segmentation approaches.\nThe proposed BlendMask can effectively predict dense per-pixel\nposition-sensitive instance features with very few channels, and learn\nattention maps for each instance with merely one convolution layer, thus being\nfast in inference. BlendMask can be easily incorporated with the\nstate-of-the-art one-stage detection frameworks and outperforms Mask R-CNN\nunder the same training schedule while being 20% faster. A light-weight version\nof BlendMask achieves $ 34.2% $ mAP at 25 FPS evaluated on a single 1080Ti GPU\ncard. Because of its simplicity and efficacy, we hope that our BlendMask could\nserve as a simple yet strong baseline for a wide range of instance-wise\nprediction tasks.\n  Code is available at https://git.io/AdelaiDet\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 03:30:17 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 08:45:23 GMT"}, {"version": "v3", "created": "Sun, 26 Apr 2020 10:27:02 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chen", "Hao", ""], ["Sun", "Kunyang", ""], ["Tian", "Zhi", ""], ["Shen", "Chunhua", ""], ["Huang", "Yongming", ""], ["Yan", "Youliang", ""]]}, {"id": "2001.00326", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong and Yi Yang", "title": "NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture\n  Search", "comments": "Published at ICLR 2020 as a spotlight paper; 16 pages; 10 figures; 7\n  tables; Code is available at https://github.com/D-X-Y/AutoDL-Projects", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has achieved breakthrough success in a great\nnumber of applications in the past few years. It could be time to take a step\nback and analyze the good and bad aspects in the field of NAS. A variety of\nalgorithms search architectures under different search space. These searched\narchitectures are trained using different setups, e.g., hyper-parameters, data\naugmentation, regularization. This raises a comparability problem when\ncomparing the performance of various NAS algorithms. NAS-Bench-101 has shown\nsuccess to alleviate this problem. In this work, we propose an extension to\nNAS-Bench-101: NAS-Bench-201 with a different search space, results on multiple\ndatasets, and more diagnostic information. NAS-Bench-201 has a fixed search\nspace and provides a unified benchmark for almost any up-to-date NAS\nalgorithms. The design of our search space is inspired from the one used in the\nmost popular cell-based searching algorithms, where a cell is represented as a\nDAG. Each edge here is associated with an operation selected from a predefined\noperation set. For it to be applicable for all NAS algorithms, the search space\ndefined in NAS-Bench-201 includes all possible architectures generated by 4\nnodes and 5 associated operation options, which results in 15,625 candidates in\ntotal. The training log and the performance for each architecture candidate are\nprovided for three datasets. This allows researchers to avoid unnecessary\nrepetitive training for selected candidate and focus solely on the search\nalgorithm itself. The training time saved for every candidate also largely\nimproves the efficiency of many methods. We provide additional diagnostic\ninformation such as fine-grained loss and accuracy, which can give inspirations\nto new designs of NAS algorithms. In further support, we have analyzed it from\nmany aspects and benchmarked 10 recent NAS algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 05:28:26 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 12:38:55 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Dong", "Xuanyi", ""], ["Yang", "Yi", ""]]}, {"id": "2001.00335", "submitter": "Yi Lu", "authors": "Yi Lu, Yaran Chen, Dongbin Zhao, Jianxin Chen", "title": "Graph-FCN for image semantic segmentation", "comments": null, "journal-ref": "Advances in Neural Networks, ISNN 2019. Lecture Notes in Computer\n  Science, vol 11554, pp. 97-105, Springer, Cham", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation with deep learning has achieved great progress in\nclassifying the pixels in the image. However, the local location information is\nusually ignored in the high-level feature extraction by the deep learning,\nwhich is important for image semantic segmentation. To avoid this problem, we\npropose a graph model initialized by a fully convolutional network (FCN) named\nGraph-FCN for image semantic segmentation. Firstly, the image grid data is\nextended to graph structure data by a convolutional network, which transforms\nthe semantic segmentation problem into a graph node classification problem.\nThen we apply graph convolutional network to solve this graph node\nclassification problem. As far as we know, it is the first time that we apply\nthe graph convolutional network in image semantic segmentation. Our method\nachieves competitive performance in mean intersection over union (mIOU) on the\nVOC dataset(about 1.34% improvement), compared to the original FCN model.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 06:05:29 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Lu", "Yi", ""], ["Chen", "Yaran", ""], ["Zhao", "Dongbin", ""], ["Chen", "Jianxin", ""]]}, {"id": "2001.00339", "submitter": "Yuanyuan Lyu", "authors": "Yuanyuan Lyu, Haofu Liao, Heqin Zhu, S. Kevin Zhou", "title": "A$^3$DSegNet: Anatomy-aware artifact disentanglement and segmentation\n  network for unpaired segmentation, artifact reduction, and modality\n  translation", "comments": "Accepted by IPMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spinal surgery planning necessitates automatic segmentation of vertebrae in\ncone-beam computed tomography (CBCT), an intraoperative imaging modality that\nis widely used in intervention. However, CBCT images are of low-quality and\nartifact-laden due to noise, poor tissue contrast, and the presence of metallic\nobjects, causing vertebra segmentation, even manually, a demanding task. In\ncontrast, there exists a wealth of artifact-free, high quality CT images with\nvertebra annotations. This motivates us to build a CBCT vertebra segmentation\nmodel using unpaired CT images with annotations. To overcome the domain and\nartifact gaps between CBCT and CT, it is a must to address the three\nheterogeneous tasks of vertebra segmentation, artifact reduction and modality\ntranslation all together. To this, we propose a novel anatomy-aware artifact\ndisentanglement and segmentation network (A$^3$DSegNet) that intensively\nleverages knowledge sharing of these three tasks to promote learning.\nSpecifically, it takes a random pair of CBCT and CT images as the input and\nmanipulates the synthesis and segmentation via different decoding combinations\nfrom the disentangled latent layers. Then, by proposing various forms of\nconsistency among the synthesized images and among segmented vertebrae, the\nlearning is achieved without paired (i.e., anatomically identical) data.\nFinally, we stack 2D slices together and build 3D networks on top to obtain\nfinal 3D segmentation result. Extensive experiments on a large number of\nclinical CBCT (21,364) and CT (17,089) images show that the proposed\nA$^3$DSegNet performs significantly better than state-of-the-art competing\nmethods trained independently for each task and, remarkably, it achieves an\naverage Dice coefficient of 0.926 for unpaired 3D CBCT vertebra segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 06:37:09 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 14:13:11 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 12:49:56 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Lyu", "Yuanyuan", ""], ["Liao", "Haofu", ""], ["Zhu", "Heqin", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2001.00340", "submitter": "Yuanyuan Lyu", "authors": "Yuanyuan Lyu, Wei-An Lin, Haofu Liao, Jingjing Lu, S. Kevin Zhou", "title": "Encoding Metal Mask Projection for Metal Artifact Reduction in Computed\n  Tomography", "comments": "accepted by MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59713-9_15", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metal artifact reduction (MAR) in computed tomography (CT) is a notoriously\nchallenging task because the artifacts are structured and non-local in the\nimage domain. However, they are inherently local in the sinogram domain. Thus,\none possible approach to MAR is to exploit the latter characteristic by\nlearning to reduce artifacts in the sinogram. However, if we directly treat the\nmetal-affected regions in sinogram as missing and replace them with the\nsurrogate data generated by a neural network, the artifact-reduced CT images\ntend to be over-smoothed and distorted since fine-grained details within the\nmetal-affected regions are completely ignored. In this work, we provide\nanalytical investigation to the issue and propose to address the problem by (1)\nretaining the metal-affected regions in sinogram and (2) replacing the\nbinarized metal trace with the metal mask projection such that the geometry\ninformation of metal implants is encoded. Extensive experiments on simulated\ndatasets and expert evaluations on clinical images demonstrate that our novel\nnetwork yields anatomically more precise artifact-reduced images than the\nstate-of-the-art approaches, especially when metallic objects are large.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 06:39:06 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 14:10:04 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 14:31:56 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lyu", "Yuanyuan", ""], ["Lin", "Wei-An", ""], ["Liao", "Haofu", ""], ["Lu", "Jingjing", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2001.00346", "submitter": "Ce Wang", "authors": "Ce Wang, S. Kevin Zhou, Zhiwei Cheng", "title": "First image then video: A two-stage network for spatiotemporal video\n  denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video denoising is to remove noise from noise-corrupted data, thus recovering\ntrue signals via spatiotemporal processing. Existing approaches for\nspatiotemporal video denoising tend to suffer from motion blur artifacts, that\nis, the boundary of a moving object tends to appear blurry especially when the\nobject undergoes a fast motion, causing optical flow calculation to break down.\nIn this paper, we address this challenge by designing a first-image-then-video\ntwo-stage denoising neural network, consisting of an image denoising module for\nspatially reducing intra-frame noise followed by a regular spatiotemporal video\ndenoising module. The intuition is simple yet powerful and effective: the first\nstage of image denoising effectively reduces the noise level and, therefore,\nallows the second stage of spatiotemporal denoising for better modeling and\nlearning everywhere, including along the moving object boundaries. This\ntwo-stage network, when trained in an end-to-end fashion, yields the\nstate-of-the-art performances on the video denoising benchmark Vimeo90K dataset\nin terms of both denoising quality and computation. It also enables an\nunsupervised approach that achieves comparable performance to existing\nsupervised approaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 07:21:39 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 03:36:15 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Wang", "Ce", ""], ["Zhou", "S. Kevin", ""], ["Cheng", "Zhiwei", ""]]}, {"id": "2001.00360", "submitter": "Cong Chen", "authors": "Cong Chen, Kim Batselier, Wenjian Yu, Ngai Wong", "title": "Kernelized Support Tensor Train Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor, a multi-dimensional data structure, has been exploited recently in\nthe machine learning community. Traditional machine learning approaches are\nvector- or matrix-based, and cannot handle tensorial data directly. In this\npaper, we propose a tensor train (TT)-based kernel technique for the first\ntime, and apply it to the conventional support vector machine (SVM) for image\nclassification. Specifically, we propose a kernelized support tensor train\nmachine that accepts tensorial input and preserves the intrinsic kernel\nproperty. The main contributions are threefold. First, we propose a TT-based\nfeature mapping procedure that maintains the TT structure in the feature space.\nSecond, we demonstrate two ways to construct the TT-based kernel function while\nconsidering consistency with the TT inner product and preservation of\ninformation. Third, we show that it is possible to apply different kernel\nfunctions on different data modes. In principle, our method tensorizes the\nstandard SVM on its input structure and kernel mapping scheme. Extensive\nexperiments are performed on real-world tensor data, which demonstrates the\nsuperiority of the proposed scheme under few-sample high-dimensional inputs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 08:40:15 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Chen", "Cong", ""], ["Batselier", "Kim", ""], ["Yu", "Wenjian", ""], ["Wong", "Ngai", ""]]}, {"id": "2001.00361", "submitter": "Shangxi Wu", "authors": "Bohan Liang and Shangxi Wu and Kaiyuan Xu and Jingyu Hao", "title": "Butterfly Detection and Classification Based on Integrated YOLO\n  Algorithm", "comments": "13th ICGEC 2019: Qingdao, China", "journal-ref": "Genetic and Evolutionary Computing. ICGEC 2019. Advances in\n  Intelligent Systems and Computing, vol 1107. Springer, Singapore", "doi": "10.1007/978-981-15-3308-2_55", "report-no": "500-512", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insects are abundant species on the earth, and the task of identification and\nidentification of insects is complex and arduous. How to apply artificial\nintelligence technology and digital image processing methods to automatic\nidentification of insect species is a hot issue in current research. In this\npaper, the problem of automatic detection and classification recognition of\nbutterfly photographs is studied, and a method of bio-labeling suitable for\nbutterfly classification is proposed. On the basis of YOLO algorithm, by\nsynthesizing the results of YOLO models with different training mechanisms, a\nbutterfly automatic detection and classification recognition algorithm based on\nYOLO algorithm is proposed. It greatly improves the generalization ability of\nYOLO algorithm and makes it have better ability to solve small sample problems.\nThe experimental results show that the proposed annotation method and\nintegrated YOLO algorithm have high accuracy and recognition rate in butterfly\nautomatic detection and recognition.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 08:52:18 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 02:50:28 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Liang", "Bohan", ""], ["Wu", "Shangxi", ""], ["Xu", "Kaiyuan", ""], ["Hao", "Jingyu", ""]]}, {"id": "2001.00396", "submitter": "Leon Sixt", "authors": "Karl Schulz, Leon Sixt, Federico Tombari, Tim Landgraf", "title": "Restricting the Flow: Information Bottlenecks for Attribution", "comments": "18 pages, 12 figures, accepted at ICLR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribution methods provide insights into the decision-making of machine\nlearning models like artificial neural networks. For a given input sample, they\nassign a relevance score to each individual input variable, such as the pixels\nof an image. In this work we adapt the information bottleneck concept for\nattribution. By adding noise to intermediate feature maps we restrict the flow\nof information and can quantify (in bits) how much information image regions\nprovide. We compare our method against ten baselines using three different\nmetrics on VGG-16 and ResNet-50, and find that our methods outperform all\nbaselines in five out of six settings. The method's information-theoretic\nfoundation provides an absolute frame of reference for attribution values\n(bits) and a guarantee that regions scored close to zero are not necessary for\nthe network's decision. For reviews: https://openreview.net/forum?id=S1xWh1rYwB\nFor code: https://github.com/BioroboticsLab/IBA\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 11:24:35 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 18:37:23 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 17:31:52 GMT"}, {"version": "v4", "created": "Mon, 25 May 2020 14:21:37 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Schulz", "Karl", ""], ["Sixt", "Leon", ""], ["Tombari", "Federico", ""], ["Landgraf", "Tim", ""]]}, {"id": "2001.00425", "submitter": "Ricardo Borsoi", "authors": "Ricardo Augusto Borsoi, Tales Imbiriba, Pau Closas, Jos\\'e Carlos\n  Moreira Bermudez, C\\'edric Richard", "title": "Kalman Filtering and Expectation Maximization for Multitemporal Spectral\n  Unmixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent evolution of hyperspectral imaging technology and the\nproliferation of new emerging applications presses for the processing of\nmultiple temporal hyperspectral images. In this work, we propose a novel\nspectral unmixing (SU) strategy using physically motivated parametric endmember\nrepresentations to account for temporal spectral variability. By representing\nthe multitemporal mixing process using a state-space formulation, we are able\nto exploit the Bayesian filtering machinery to estimate the endmember\nvariability coefficients. Moreover, by assuming that the temporal variability\nof the abundances is small over short intervals, an efficient implementation of\nthe expectation maximization (EM) algorithm is employed to estimate the\nabundances and the other model parameters. Simulation results indicate that the\nproposed strategy outperforms state-of-the-art multitemporal SU algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 13:12:46 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 21:52:20 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Borsoi", "Ricardo Augusto", ""], ["Imbiriba", "Tales", ""], ["Closas", "Pau", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "2001.00487", "submitter": "Lionel Dominjon", "authors": "Pierre-Olivier Pigny and Lionel Dominjon", "title": "Using CNNs For Users Segmentation In Video See-Through Augmented\n  Virtuality", "comments": "6 pages, 6 figures. Published in the 2nd International Conference on\n  Artificial Intelligence & Virtual Reality (IEEE AIVR 2019)", "journal-ref": "Proceedings of 2019 IEEE International Conference on Artificial\n  Intelligence and Virtual Reality (AIVR), pp. 229-234, San Diego, US, December\n  2019", "doi": "10.1109/AIVR46125.2019.00048", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present preliminary results on the use of deep learning\ntechniques to integrate the users self-body and other participants into a\nhead-mounted video see-through augmented virtuality scenario. It has been\npreviously shown that seeing users bodies in such simulations may improve the\nfeeling of both self and social presence in the virtual environment, as well as\nuser performance. We propose to use a convolutional neural network for real\ntime semantic segmentation of users bodies in the stereoscopic RGB video\nstreams acquired from the perspective of the user. We describe design issues as\nwell as implementation details of the system and demonstrate the feasibility of\nusing such neural networks for merging users bodies in an augmented virtuality\nsimulation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 15:22:36 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Pigny", "Pierre-Olivier", ""], ["Dominjon", "Lionel", ""]]}, {"id": "2001.00526", "submitter": "Fahimeh Fooladgar", "authors": "Fahimeh Fooladgar and Shohreh Kasaei", "title": "Lightweight Residual Densely Connected Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": "10.1007/s11042-020-09223-8", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extremely efficient convolutional neural network architectures are one of the\nmost important requirements for limited-resource devices (such as embedded and\nmobile devices). The computing power and memory size are two important\nconstraints of these devices. Recently, some architectures have been proposed\nto overcome these limitations by considering specific hardware-software\nequipment. In this paper, the lightweight residual densely connected blocks are\nproposed to guaranty the deep supervision, efficient gradient flow, and feature\nreuse abilities of convolutional neural network. The proposed method decreases\nthe cost of training and inference processes without using any special\nhardware-software equipment by just reducing the number of parameters and\ncomputational operations while achieving a feasible accuracy. Extensive\nexperimental results demonstrate that the proposed architecture is more\nefficient than the AlexNet and VGGNet in terms of model size, required\nparameters, and even accuracy. The proposed model has been evaluated on the\nImageNet, MNIST, Fashion MNIST, SVHN, CIFAR-10, and CIFAR-100. It achieves\nstate-of-the-art results on Fashion MNIST dataset and reasonable results on the\nothers. The obtained results show the superiority of the proposed method to\nefficient models such as the SqueezNet. It is also comparable with\nstate-of-the-art efficient models such as CondenseNet and ShuffleNet.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 17:15:32 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 14:18:58 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Fooladgar", "Fahimeh", ""], ["Kasaei", "Shohreh", ""]]}, {"id": "2001.00558", "submitter": "Yi-Tun Lin", "authors": "Yi-Tun Lin, Graham D. Finlayson", "title": "Physically Plausible Spectral Reconstruction from RGB Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Convolutional Neural Networks (CNN) have been used to reconstruct\nhyperspectral information from RGB images. Moreover, this spectral\nreconstruction problem (SR) can often be solved with good (low) error. However,\nthese methods are not physically plausible: that is when the recovered spectra\nare reintegrated with the underlying camera sensitivities, the resulting\npredicted RGB is not the same as the actual RGB, and sometimes this discrepancy\ncan be large. The problem is further compounded by exposure change. Indeed,\nmost learning-based SR models train for a fixed exposure setting and we show\nthat this can result in poor performance when exposure varies.\n  In this paper we show how CNN learning can be extended so that physical\nplausibility is enforced and the problem resulting from changing exposures is\nmitigated. Our SR solution improves the state-of-the-art spectral recovery\nperformance under varying exposure conditions while simultaneously ensuring\nphysical plausibility (the recovered spectra reintegrate to the input RGBs\nexactly).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 18:46:26 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Lin", "Yi-Tun", ""], ["Finlayson", "Graham D.", ""]]}, {"id": "2001.00561", "submitter": "Vahid Mirjalili Dr", "authors": "Vahid Mirjalili, Sebastian Raschka, Arun Ross", "title": "PrivacyNet: Semi-Adversarial Networks for Multi-attribute Face Privacy", "comments": "13 pages, 9 figures", "journal-ref": "IEEE Transactions on Image Processing, 2020", "doi": "10.1109/TIP.2020.3024026", "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has established the possibility of deducing soft-biometric\nattributes such as age, gender and race from an individual's face image with\nhigh accuracy. However, this raises privacy concerns, especially when face\nimages collected for biometric recognition purposes are used for attribute\nanalysis without the person's consent. To address this problem, we develop a\ntechnique for imparting soft biometric privacy to face images via an image\nperturbation methodology. The image perturbation is undertaken using a\nGAN-based Semi-Adversarial Network (SAN) - referred to as PrivacyNet - that\nmodifies an input face image such that it can be used by a face matcher for\nmatching purposes but cannot be reliably used by an attribute classifier.\nFurther, PrivacyNet allows a person to choose specific attributes that have to\nbe obfuscated in the input face images (e.g., age and race), while allowing for\nother types of attributes to be extracted (e.g., gender). Extensive experiments\nusing multiple face matchers, multiple age/gender/race classifiers, and\nmultiple face datasets demonstrate the generalizability of the proposed\nmulti-attribute privacy enhancing method across multiple face and attribute\nclassifiers.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 18:53:31 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 00:10:11 GMT"}, {"version": "v3", "created": "Sun, 14 Mar 2021 00:13:02 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mirjalili", "Vahid", ""], ["Raschka", "Sebastian", ""], ["Ross", "Arun", ""]]}, {"id": "2001.00630", "submitter": "Leo Furkan Isikdogan", "authors": "Masayoshi Asama, Leo F. Isikdogan, Sushma Rao, Bhavin V. Nayak, Gilad\n  Michael", "title": "A Machine Learning Imaging Core using Separable FIR-IIR Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose fixed-function neural network hardware that is designed to perform\npixel-to-pixel image transformations in a highly efficient way. We use a fully\ntrainable, fixed-topology neural network to build a model that can perform a\nwide variety of image processing tasks. Our model uses compressed skip lines\nand hybrid FIR-IIR blocks to reduce the latency and hardware footprint. Our\nproposed Machine Learning Imaging Core, dubbed MagIC, uses a silicon area of\n~3mm^2 (in TSMC 16nm), which is orders of magnitude smaller than a comparable\npixel-wise dense prediction model. MagIC requires no DDR bandwidth, no SRAM,\nand practically no external memory. Each MagIC core consumes 56mW (215 mW max\npower) at 500MHz and achieves an energy-efficient throughput of 23TOPS/W/mm^2.\nMagIC can be used as a multi-purpose image processing block in an imaging\npipeline, approximating compute-heavy image processing applications, such as\nimage deblurring, denoising, and colorization, within the power and silicon\narea limits of mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 21:24:26 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Asama", "Masayoshi", ""], ["Isikdogan", "Leo F.", ""], ["Rao", "Sushma", ""], ["Nayak", "Bhavin V.", ""], ["Michael", "Gilad", ""]]}, {"id": "2001.00645", "submitter": "Hamed Alqahtani Mr", "authors": "Hamed Alqahtani", "title": "PI-GAN: Learning Pose Independent representations for multiple profile\n  face synthesis", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a pose-invariant representation capable of synthesizing multiple\nface pose views from a single pose is still a difficult problem. The solution\nis demanded in various areas like multimedia security, computer vision,\nrobotics, etc. Generative adversarial networks (GANs) have encoder-decoder\nstructures possessing the capability to learn pose-independent representation\nincorporated with discriminator network for realistic face synthesis. We\npresent PIGAN, a cyclic shared encoder-decoder framework, in an attempt to\nsolve the problem. As compared to traditional GAN, it consists of secondary\nencoder-decoder framework sharing weights from the primary structure and\nreconstructs the face with the original pose. The primary framework focuses on\ncreating disentangle representation, and secondary framework aims to restore\nthe original face. We use CFP high-resolution, realistic dataset to check the\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 02:51:24 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Alqahtani", "Hamed", ""]]}, {"id": "2001.00657", "submitter": "Jesse Scott", "authors": "Jesse Scott, Christopher Funk, Bharadwaj Ravichandran, John H.\n  Challis, Robert T. Collins, Yanxi Liu", "title": "From Kinematics To Dynamics: Estimating Center of Pressure and Base of\n  Support from Video Frames of Human Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To gain an understanding of the relation between a given human pose image and\nthe corresponding physical foot pressure of the human subject, we propose and\nvalidate two end-to-end deep learning architectures, PressNet and\nPressNet-Simple, to regress foot pressure heatmaps (dynamics) from 2D human\npose (kinematics) derived from a video frame. A unique video and foot pressure\ndata set of 813,050 synchronized pairs, composed of 5-minute long choreographed\nTaiji movement sequences of 6 subjects, is collected and used for\nleaving-one-subject-out cross validation. Our initial experimental results\ndemonstrate reliable and repeatable foot pressure prediction from a single\nimage, setting the first baseline for such a complex cross modality mapping\nproblem in computer vision. Furthermore, we compute and quantitatively validate\nthe Center of Pressure (CoP) and Base of Support (BoS) from predicted foot\npressure distribution, obtaining key components in pose stability analysis from\nimages with potential applications in kinesiology, medicine, sports and\nrobotics.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 22:41:00 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Scott", "Jesse", ""], ["Funk", "Christopher", ""], ["Ravichandran", "Bharadwaj", ""], ["Challis", "John H.", ""], ["Collins", "Robert T.", ""], ["Liu", "Yanxi", ""]]}, {"id": "2001.00666", "submitter": "Nil Stolt Ans\\'o", "authors": "Nil Stolt Ans\\'o", "title": "Synthetic vascular structure generation for unsupervised pre-training in\n  CTA segmentation tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large enough computed tomography (CT) data sets to train supervised deep\nmodels are often hard to come by. One contributing issue is the amount of\nmanual labor that goes into creating ground truth labels, specially for\nvolumetric data. In this research, we train a U-net architecture at a vessel\nsegmentation task that can be used to provide insights when treating stroke\npatients. We create a computational model that generates synthetic vascular\nstructures which can be blended into unlabeled CT scans of the head. This\nunsupervised approached to labelling is used to pre-train deep segmentation\nmodels, which are later fine-tuned on real examples to achieve an increase in\naccuracy compared to models trained exclusively on a hand-labeled data set.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 23:21:22 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Ans\u00f3", "Nil Stolt", ""]]}, {"id": "2001.00667", "submitter": "Adrian Shajkofci", "authors": "Adrian Shajkofci, Michael Liebling", "title": "DeepFocus: a Few-Shot Microscope Slide Auto-Focus using a Sample\n  Invariant CNN-based Sharpness Function", "comments": "Submitted to IEEE ISBI 2020", "journal-ref": "2020 IEEE 17th International Symposium on Biomedical Imaging\n  (ISBI)", "doi": "10.1109/ISBI45749.2020.9098331", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autofocus (AF) methods are extensively used in biomicroscopy, for example to\nacquire timelapses, where the imaged objects tend to drift out of focus. AD\nalgorithms determine an optimal distance by which to move the sample back into\nthe focal plane. Current hardware-based methods require modifying the\nmicroscope and image-based algorithms either rely on many images to converge to\nthe sharpest position or need training data and models specific to each\ninstrument and imaging configuration. Here we propose DeepFocus, an AF method\nwe implemented as a Micro-Manager plugin, and characterize its Convolutional\nneural network-based sharpness function, which we observed to be depth\nco-variant and sample-invariant. Sample invariance allows our AF algorithm to\nconverge to an optimal axial position within as few as three iterations using a\nmodel trained once for use with a wide range of optical microscopes and a\nsingle instrument-dependent calibration stack acquisition of a flat (but\narbitrary) textured object. From experiments carried out both on synthetic and\nexperimental data, we observed an average precision, given 3 measured images,\nof 0.30 +- 0.16 micrometers with a 10x, NA 0.3 objective. We foresee that this\nperformance and low image number will help limit photodamage during\nacquisitions with light-sensitive samples.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 23:29:11 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Shajkofci", "Adrian", ""], ["Liebling", "Michael", ""]]}, {"id": "2001.00677", "submitter": "Huan Song", "authors": "Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, Liu Ren", "title": "Improve Unsupervised Domain Adaptation with Mixup Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation studies the problem of utilizing a relevant\nsource domain with abundant labels to build predictive modeling for an\nunannotated target domain. Recent work observe that the popular adversarial\napproach of learning domain-invariant features is insufficient to achieve\ndesirable target domain performance and thus introduce additional training\nconstraints, e.g. cluster assumption. However, these approaches impose the\nconstraints on source and target domains individually, ignoring the important\ninterplay between them. In this work, we propose to enforce training\nconstraints across domains using mixup formulation to directly address the\ngeneralization performance for target data. In order to tackle potentially huge\ndomain discrepancy, we further propose a feature-level consistency regularizer\nto facilitate the inter-domain constraint. When adding intra-domain mixup and\ndomain adversarial learning, our general framework significantly improves\nstate-of-the-art performance on several important tasks from both image\nclassification and human activity recognition.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 01:21:27 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Yan", "Shen", ""], ["Song", "Huan", ""], ["Li", "Nanxiang", ""], ["Zou", "Lincan", ""], ["Ren", "Liu", ""]]}, {"id": "2001.00686", "submitter": "Jacky Chow", "authors": "Jacky C.K. Chow, Steven K. Boyd, Derek D. Lichti and Janet L. Ronsky", "title": "Robust Self-Supervised Learning of Deterministic Errors in Single-Plane\n  (Monoplanar) and Dual-Plane (Biplanar) X-ray Fluoroscopy", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2019.2963446", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluoroscopic imaging that captures X-ray images at video framerates is\nadvantageous for guiding catheter insertions by vascular surgeons and\ninterventional radiologists. Visualizing the dynamical movements non-invasively\nallows complex surgical procedures to be performed with less trauma to the\npatient. To improve surgical precision, endovascular procedures can benefit\nfrom more accurate fluoroscopy data via calibration. This paper presents a\nrobust self-calibration algorithm suitable for single-plane and dual-plane\nfluoroscopy. A three-dimensional (3D) target field was imaged by the\nfluoroscope in a strong geometric network configuration. The unknown 3D\npositions of targets and the fluoroscope pose were estimated simultaneously by\nmaximizing the likelihood of the Student-t probability distribution function. A\nsmoothed k-nearest neighbour (kNN) regression is then used to model the\ndeterministic component of the image reprojection error of the robust bundle\nadjustment. The Maximum Likelihood Estimation step and the kNN regression step\nare then repeated iteratively until convergence. Four different error modeling\nschemes were compared while varying the quantity of training images. It was\nfound that using a smoothed kNN regression can automatically model the\nsystematic errors in fluoroscopy with similar accuracy as a human expert using\na small training dataset. When all training images were used, the 3D mapping\nerror was reduced from 0.61-0.83 mm to 0.04 mm post-calibration (94.2-95.7%\nimprovement), and the 2D reprojection error was reduced from 1.17-1.31 to\n0.20-0.21 pixels (83.2-83.8% improvement). When using biplanar fluoroscopy, the\n3D measurement accuracy of the system improved from 0.60 mm to 0.32 mm (47.2%\nimprovement).\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 01:56:21 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Chow", "Jacky C. K.", ""], ["Boyd", "Steven K.", ""], ["Lichti", "Derek D.", ""], ["Ronsky", "Janet L.", ""]]}, {"id": "2001.00692", "submitter": "Geng Xiebo", "authors": "Xiebo Geng (1 and 4), Sibo Liua (1 and 4), Wei Han (1), Xu Li (1),\n  Jiabo Ma (1), Jingya Yu (1), Xiuli Liu (1), Sahoqun Zeng (1), Li Chen (2 and\n  3), Shenghua Cheng (1 and 3) ((1) Britton Chance Center for Biomedical\n  Photonics, Wuhan National Laboratory for Optoelectronics-Huazhong University\n  of Science and Technology, China,(2) Department of Clinical Laboratory,\n  Tongji Hospital, Huazhong University of Science and Technology, China, (3)\n  Corresponding author, (4) Equal contribution to this work)", "title": "FFusionCGAN: An end-to-end fusion method for few-focus images using\n  conditional GAN in cytopathological digital slides", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-focus image fusion technologies compress different focus depth images\ninto an image in which most objects are in focus. However, although existing\nimage fusion techniques, including traditional algorithms and deep\nlearning-based algorithms, can generate high-quality fused images, they need\nmultiple images with different focus depths in the same field of view. This\ncriterion may not be met in some cases where time efficiency is required or the\nhardware is insufficient. The problem is especially prominent in large-size\nwhole slide images. This paper focused on the multi-focus image fusion of\ncytopathological digital slide images, and proposed a novel method for\ngenerating fused images from single-focus or few-focus images based on\nconditional generative adversarial network (GAN). Through the adversarial\nlearning of the generator and discriminator, the method is capable of\ngenerating fused images with clear textures and large depth of field. Combined\nwith the characteristics of cytopathological images, this paper designs a new\ngenerator architecture combining U-Net and DenseBlock, which can effectively\nimprove the network's receptive field and comprehensively encode image\nfeatures. Meanwhile, this paper develops a semantic segmentation network that\nidentifies the blurred regions in cytopathological images. By integrating the\nnetwork into the generative model, the quality of the generated fused images is\neffectively improved. Our method can generate fused images from only\nsingle-focus or few-focus images, thereby avoiding the problem of collecting\nmultiple images of different focus depths with increased time and hardware\ncosts. Furthermore, our model is designed to learn the direct mapping of input\nsource images to fused images without the need to manually design complex\nactivity level measurements and fusion rules as in traditional methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 02:13:47 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Geng", "Xiebo", "", "1 and 4"], ["Liua", "Sibo", "", "1 and 4"], ["Han", "Wei", "", "2 and\n  3"], ["Li", "Xu", "", "2 and\n  3"], ["Ma", "Jiabo", "", "2 and\n  3"], ["Yu", "Jingya", "", "2 and\n  3"], ["Liu", "Xiuli", "", "2 and\n  3"], ["Zeng", "Sahoqun", "", "2 and\n  3"], ["Chen", "Li", "", "2 and\n  3"], ["Cheng", "Shenghua", "", "1 and 3"]]}, {"id": "2001.00702", "submitter": "Haichao Zhu", "authors": "Zhaohui Zhang and Shipeng Xie and Mingxiu Chen and Haichao Zhu", "title": "HandAugment: A Simple Data Augmentation Method for Depth-Based 3D Hand\n  Pose Estimation", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hand pose estimation from 3D depth images, has been explored widely using\nvarious kinds of techniques in the field of computer vision. Though, deep\nlearning based method improve the performance greatly recently, however, this\nproblem still remains unsolved due to lack of large datasets, like ImageNet or\neffective data synthesis methods. In this paper, we propose HandAugment, a\nmethod to synthesize image data to augment the training process of the neural\nnetworks. Our method has two main parts: First, We propose a scheme of\ntwo-stage neural networks. This scheme can make the neural networks focus on\nthe hand regions and thus to improve the performance. Second, we introduce a\nsimple and effective method to synthesize data by combining real and synthetic\nimage together in the image space. Finally, we show that our method achieves\nthe first place in the task of depth-based 3D hand pose estimation in HANDS\n2019 challenge.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 03:00:40 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 06:07:38 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Zhang", "Zhaohui", ""], ["Xie", "Shipeng", ""], ["Chen", "Mingxiu", ""], ["Zhu", "Haichao", ""]]}, {"id": "2001.00714", "submitter": "Yipu Zhao", "authors": "Yipu Zhao, Patricio A. Vela", "title": "Good Feature Matching: Towards Accurate, Robust VO/VSLAM with Low\n  Latency", "comments": "Accepted as a Regular Paper to the IEEE Transactions on Robotics\n  Journal", "journal-ref": null, "doi": "10.1109/TRO.2020.2964138", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of state-of-the-art VO/VSLAM system exposes a gap in balancing\nperformance (accuracy & robustness) and efficiency (latency). Feature-based\nsystems exhibit good performance, yet have higher latency due to explicit data\nassociation; direct & semidirect systems have lower latency, but are\ninapplicable in some target scenarios or exhibit lower accuracy than\nfeature-based ones. This paper aims to fill the performance-efficiency gap with\nan enhancement applied to feature-based VSLAM. We present good feature\nmatching, an active map-to-frame feature matching method. Feature matching\neffort is tied to submatrix selection, which has combinatorial time complexity\nand requires choosing a scoring metric. Via simulation, the Max-logDet matrix\nrevealing metric is shown to perform best. For real-time applicability, the\ncombination of deterministic selection and randomized acceleration is studied.\nThe proposed algorithm is integrated into monocular & stereo feature-based\nVSLAM systems. Extensive evaluations on multiple benchmarks and compute\nhardware quantify the latency reduction and the accuracy & robustness\npreservation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 03:50:54 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Zhao", "Yipu", ""], ["Vela", "Patricio A.", ""]]}, {"id": "2001.00722", "submitter": "Pei Xu", "authors": "Pei Xu, Shan Huang, Hongzhen Wang, Hao Song, Shen Huang, Qi Ju", "title": "A Multi-oriented Chinese Keyword Spotter Guided by Text Line Detection", "comments": "Accepted by ICDAR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese keyword spotting is a challenging task as there is no visual blank\nfor Chinese words. Different from English words which are split naturally by\nvisual blanks, Chinese words are generally split only by semantic information.\nIn this paper, we propose a new Chinese keyword spotter for natural images,\nwhich is inspired by Mask R-CNN. We propose to predict the keyword masks guided\nby text line detection. Firstly, proposals of text lines are generated by\nFaster R-CNN;Then, text line masks and keyword masks are predicted by\nsegmentation in the proposals. In this way, the text lines and keywords are\npredicted in parallel. We create two Chinese keyword datasets based on RCTW-17\nand ICPR MTWI2018 to verify the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 05:01:00 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 11:27:24 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Xu", "Pei", ""], ["Huang", "Shan", ""], ["Wang", "Hongzhen", ""], ["Song", "Hao", ""], ["Huang", "Shen", ""], ["Ju", "Qi", ""]]}, {"id": "2001.00735", "submitter": "Nachiket Deo", "authors": "Nachiket Deo and Mohan M. Trivedi", "title": "Trajectory Forecasts in Unknown Environments Conditioned on Grid-Based\n  Plans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of forecasting pedestrian and vehicle trajectories in\nunknown environments, conditioned on their past motion and scene structure.\nTrajectory forecasting is a challenging problem due to the large variation in\nscene structure and the multimodal distribution of future trajectories. Unlike\nprior approaches that directly learn one-to-many mappings from observed context\nto multiple future trajectories, we propose to condition trajectory forecasts\non plans sampled from a grid based policy learned using maximum entropy inverse\nreinforcement learning (MaxEnt IRL). We reformulate MaxEnt IRL to allow the\npolicy to jointly infer plausible agent goals, and paths to those goals on a\ncoarse 2-D grid defined over the scene. We propose an attention based\ntrajectory generator that generates continuous valued future trajectories\nconditioned on state sequences sampled from the MaxEnt policy. Quantitative and\nqualitative evaluation on the publicly available Stanford drone and NuScenes\ndatasets shows that our model generates trajectories that are diverse,\nrepresenting the multimodal predictive distribution, and precise, conforming to\nthe underlying scene structure over long prediction horizons.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 06:12:26 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 17:35:21 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Deo", "Nachiket", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "2001.00762", "submitter": "Andreas B\\\"uhler", "authors": "Andreas B\\\"uhler, Niclas V\\\"odisch, Mathias B\\\"urki, Lukas Schaupp", "title": "Deep Unsupervised Common Representation Learning for LiDAR and Camera\n  Data using Double Siamese Networks", "comments": "8 pages, CoRL 2019 template used", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain gaps of sensor modalities pose a challenge for the design of\nautonomous robots. Taking a step towards closing this gap, we propose two\nunsupervised training frameworks for finding a common representation of LiDAR\nand camera data. The first method utilizes a double Siamese training structure\nto ensure consistency in the results. The second method uses a Canny edge image\nguiding the networks towards a desired representation. All networks are trained\nin an unsupervised manner, leaving room for scalability. The results are\nevaluated using common computer vision applications, and the limitations of the\nproposed approaches are outlined.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 08:53:27 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["B\u00fchler", "Andreas", ""], ["V\u00f6disch", "Niclas", ""], ["B\u00fcrki", "Mathias", ""], ["Schaupp", "Lukas", ""]]}, {"id": "2001.00769", "submitter": "Gang Xu", "authors": "Gang Xu, Yandong Gao, Jinwei Li and Mengdao Xing", "title": "InSAR Phase Denoising: A Review of Current Technologies and Future\n  Directions", "comments": "G. Xu, Y. Gao, J. Li and M. Xing, \"InSAR Phase Denoising: A Review of\n  Current Technologies and Future Directions,\" IEEE Geoscience and Remote\n  Sensing Magazine, vol. 8, no. 2, pp. 64-82, June 2020. DOI\n  10.1109/MGRS.2019.2955120", "journal-ref": null, "doi": "10.1109/MGRS.2019.2955120", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, interferometric synthetic aperture radar (InSAR) has been a\npowerful tool in remote sensing by enhancing the information acquisition.\nDuring the InSAR processing, phase denoising of interferogram is a mandatory\nstep for topography mapping and deformation monitoring. Over the last three\ndecades, a large number of effective algorithms have been developed to do\nefforts on this topic. In this paper, we give a comprehensive overview of InSAR\nphase denoising methods, classifying the established and emerging algorithms\ninto four main categories. The first two parts refer to the categories of\ntraditional local filters and transformed-domain filters, respectively. The\nthird part focuses on the category of nonlocal (NL) filters, considering their\noutstanding performances. Latter, some advanced methods based on new concept of\nsignal processing are also introduced to show their potentials in this field.\nMoreover, several popular phase denoising methods are illustrated and compared\nby performing the numerical experiments using both simulated and measured data.\nThe purpose of this paper is intended to provide necessary guideline and\ninspiration to related researchers by promoting the architecture development of\nInSAR signal processing.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 09:36:36 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 08:58:10 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Xu", "Gang", ""], ["Gao", "Yandong", ""], ["Li", "Jinwei", ""], ["Xing", "Mengdao", ""]]}, {"id": "2001.00817", "submitter": "Marc Walton", "authors": "Lindsay Oakley, Stephanie Zaleski, Billie Males, Ollie Cossairt, Marc\n  Walton", "title": "Improved Spectral Imaging Microscopy for Cultural Heritage through\n  Oblique Illumination", "comments": null, "journal-ref": null, "doi": "10.1186/s40494-020-00369-0", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.optics", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work presents the development of a flexible microscopic chemical imaging\nplatform for cultural heritage that utilizes wavelength-tunable oblique\nillumination from a point source to obtain per-pixel reflectance spectra in the\nVIS-NIR range. The microscope light source can be adjusted on two axes allowing\nfor a hemisphere of possible illumination directions. The synthesis of multiple\nillumination angles allows for the calculation of surface normal vectors,\nsimilar to phase gradients, and axial optical sectioning. The extraction of\nspectral reflectance images with high spatial resolutions from these data is\ndemonstrated through the analysis of a replica cross-section, created from\nknown painting reference materials, as well as a sample extracted from a\npainting by Pablo Picasso entitled La Mis\\'ereuse accroupie (1902). These case\nstudies show the rich microscale molecular information that may be obtained\nusing this microscope and how the instrument overcomes challenges for spectral\nanalysis commonly encountered on works of art with complex matrices composed of\nboth inorganic minerals and organic lakes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 17:00:49 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Oakley", "Lindsay", ""], ["Zaleski", "Stephanie", ""], ["Males", "Billie", ""], ["Cossairt", "Ollie", ""], ["Walton", "Marc", ""]]}, {"id": "2001.00986", "submitter": "Kevin Karsch", "authors": "Kevin Karsch", "title": "Inverse Rendering Techniques for Physically Grounded Image Editing", "comments": "PhD thesis, Computer Science, University of Illinois at\n  Urbana-Champaign, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a single picture of a scene, people can typically grasp the spatial\nlayout immediately and even make good guesses at materials properties and where\nlight is coming from to illuminate the scene. For example, we can reliably tell\nwhich objects occlude others, what an object is made of and its rough shape,\nregions that are illuminated or in shadow, and so on. It is interesting how\nlittle is known about our ability to make these determinations; as such, we are\nstill not able to robustly \"teach\" computers to make the same high-level\nobservations as people. This document presents algorithms for understanding\nintrinsic scene properties from single images. The goal of these inverse\nrendering techniques is to estimate the configurations of scene elements\n(geometry, materials, luminaires, camera parameters, etc) using only\ninformation visible in an image. Such algorithms have applications in robotics\nand computer graphics. One such application is in physically grounded image\nediting: photo editing made easier by leveraging knowledge of the physical\nspace. These applications allow sophisticated editing operations to be\nperformed in a matter of seconds, enabling seamless addition, removal, or\nrelocation of objects in images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 04:01:34 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Karsch", "Kevin", ""]]}, {"id": "2001.00987", "submitter": "Kevin Karsch", "authors": "Kevin Karsch, Ce Liu, Sing Bing Kang", "title": "DepthTransfer: Depth Extraction from Video Using Non-parametric Sampling", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  Volume: 36 Issue: 11 pgs 2144-2158 (2014)", "doi": "10.1109/TPAMI.2014.2316835", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a technique that automatically generates plausible depth maps\nfrom videos using non-parametric depth sampling. We demonstrate our technique\nin cases where past methods fail (non-translating cameras and dynamic scenes).\nOur technique is applicable to single images as well as videos. For videos, we\nuse local motion cues to improve the inferred depth maps, while optical flow is\nused to ensure temporal depth consistency. For training and evaluation, we use\na Kinect-based system to collect a large dataset containing stereoscopic videos\nwith known depths. We show that our depth estimation technique outperforms the\nstate-of-the-art on benchmark databases. Our technique can be used to\nautomatically convert a monoscopic video into stereo for 3D visualization, and\nwe demonstrate this through a variety of visually pleasing results for indoor\nand outdoor scenes, including results from the feature film Charade.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 23:46:16 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Karsch", "Kevin", ""], ["Liu", "Ce", ""], ["Kang", "Sing Bing", ""]]}, {"id": "2001.00989", "submitter": "Ajay Kumar", "authors": "Kuo Wang, Ajay Kumar", "title": "Segmentation-Aware and Adaptive Iris Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris recognition has emerged as one of the most accurate and convenient\nbiometric for the human identification and has been increasingly employed in a\nwide range of e-security applications. The quality of iris images acquired\nat-a-distance or under less constrained imaging environments is known to\ndegrade the iris matching accuracy. The periocular information is inherently\nembedded in such iris images and can be exploited to assist in the iris\nrecognition under such non-ideal scenarios. Our analysis of such iris templates\nalso indicates significant degradation and reduction in the region of interest,\nwhere the iris recognition can benefit from a similarity distance that can\nconsider importance of different binary bits, instead of the direct use of\nHamming distance in the literature. Periocular information can be dynamically\nreinforced, by incorporating the differences in the effective area of available\niris regions, for more accurate iris recognition. This paper presents such a\nsegmentation-assisted adaptive framework for more accurate less-constrained\niris recognition. The effectiveness of this framework is evaluated on three\npublicly available iris databases using within-dataset and cross-dataset\nperformance evaluation and validates the merit of the proposed iris recognition\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 04:31:37 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Wang", "Kuo", ""], ["Kumar", "Ajay", ""]]}, {"id": "2001.00994", "submitter": "Guruprasad Nayak", "authors": "Guruprasad Nayak, Rahul Ghosh, Xiaowei Jia, Varun Mithal, Vipin Kumar", "title": "Semi-supervised Classification using Attention-based Regularization on\n  Coarse-resolution Data", "comments": "To appear in the proceedings of the SIAM International Conference on\n  Data Mining (SDM20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world phenomena are observed at multiple resolutions. Predictive\nmodels designed to predict these phenomena typically consider different\nresolutions separately. This approach might be limiting in applications where\npredictions are desired at fine resolutions but available training data is\nscarce. In this paper, we propose classification algorithms that leverage\nsupervision from coarser resolutions to help train models on finer resolutions.\nThe different resolutions are modeled as different views of the data in a\nmulti-view framework that exploits the complementarity of features across\ndifferent views to improve models on both views. Unlike traditional multi-view\nlearning problems, the key challenge in our case is that there is no one-to-one\ncorrespondence between instances across different views in our case, which\nrequires explicit modeling of the correspondence of instances across\nresolutions. We propose to use the features of instances at different\nresolutions to learn the correspondence between instances across resolutions\nusing an attention mechanism.Experiments on the real-world application of\nmapping urban areas using satellite observations and sentiment classification\non text data show the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 21:29:26 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Nayak", "Guruprasad", ""], ["Ghosh", "Rahul", ""], ["Jia", "Xiaowei", ""], ["Mithal", "Varun", ""], ["Kumar", "Vipin", ""]]}, {"id": "2001.01005", "submitter": "Kivanc Kose", "authors": "Kivanc Kose, Alican Bozkurt, Christi Alessi-Fox, Melissa Gill,\n  Caterina Longo, Giovanni Pellacani, Jennifer Dy, Dana H. Brooks, Milind\n  Rajadhyaksha", "title": "Segmentation of Cellular Patterns in Confocal Images of Melanocytic\n  Lesions in vivo via a Multiscale Encoder-Decoder Network (MED-Net)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-vivo optical microscopy is advancing into routine clinical practice for\nnon-invasively guiding diagnosis and treatment of cancer and other diseases,\nand thus beginning to reduce the need for traditional biopsy. However, reading\nand analysis of the optical microscopic images are generally still qualitative,\nrelying mainly on visual examination. Here we present an automated semantic\nsegmentation method called \"Multiscale Encoder-Decoder Network (MED-Net)\" that\nprovides pixel-wise labeling into classes of patterns in a quantitative manner.\nThe novelty in our approach is the modeling of textural patterns at multiple\nscales. This mimics the procedure for examining pathology images, which\nroutinely starts with low magnification (low resolution, large field of view)\nfollowed by closer inspection of suspicious areas with higher magnification\n(higher resolution, smaller fields of view). We trained and tested our model on\nnon-overlapping partitions of 117 reflectance confocal microscopy (RCM) mosaics\nof melanocytic lesions, an extensive dataset for this application, collected at\nfour clinics in the US, and two in Italy. With patient-wise cross-validation,\nwe achieved pixel-wise mean sensitivity and specificity of $70\\pm11\\%$ and\n$95\\pm2\\%$, respectively, with $0.71\\pm0.09$ Dice coefficient over six classes.\nIn the scenario, we partitioned the data clinic-wise and tested the\ngeneralizability of the model over multiple clinics. In this setting, we\nachieved pixel-wise mean sensitivity and specificity of $74\\%$ and $95\\%$,\nrespectively, with $0.75$ Dice coefficient. We compared MED-Net against the\nstate-of-the-art semantic segmentation models and achieved better quantitative\nsegmentation performance. Our results also suggest that, due to its nested\nmultiscale architecture, the MED-Net model annotated RCM mosaics more\ncoherently, avoiding unrealistic-fragmented annotations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 22:34:52 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Kose", "Kivanc", ""], ["Bozkurt", "Alican", ""], ["Alessi-Fox", "Christi", ""], ["Gill", "Melissa", ""], ["Longo", "Caterina", ""], ["Pellacani", "Giovanni", ""], ["Dy", "Jennifer", ""], ["Brooks", "Dana H.", ""], ["Rajadhyaksha", "Milind", ""]]}, {"id": "2001.01017", "submitter": "Waheed Bajwa", "authors": "Haroon Raja and Waheed U. Bajwa", "title": "Distributed Stochastic Algorithms for High-rate Streaming Principal\n  Component Analysis", "comments": "37 pages, 11 figures; preprint of a journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating the principal eigenvector of a\ncovariance matrix from independent and identically distributed data samples in\nstreaming settings. The streaming rate of data in many contemporary\napplications can be high enough that a single processor cannot finish an\niteration of existing methods for eigenvector estimation before a new sample\narrives. This paper formulates and analyzes a distributed variant of the\nclassical Krasulina's method (D-Krasulina) that can keep up with the high\nstreaming rate of data by distributing the computational load across multiple\nprocessing nodes. The analysis shows that---under appropriate\nconditions---D-Krasulina converges to the principal eigenvector in an\norder-wise optimal manner; i.e., after receiving $M$ samples across all nodes,\nits estimation error can be $O(1/M)$. In order to reduce the network\ncommunication overhead, the paper also develops and analyzes a mini-batch\nextension of D-Krasulina, which is termed DM-Krasulina. The analysis of\nDM-Krasulina shows that it can also achieve order-optimal estimation error\nrates under appropriate conditions, even when some samples have to be discarded\nwithin the network due to communication latency. Finally, experiments are\nperformed over synthetic and real-world data to validate the convergence\nbehaviors of D-Krasulina and DM-Krasulina in high-rate streaming settings.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 00:46:47 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Raja", "Haroon", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "2001.01026", "submitter": "Amy Zhao", "authors": "Amy Zhao, Guha Balakrishnan, Kathleen M. Lewis, Fr\\'edo Durand, John\n  V. Guttag, Adrian V. Dalca", "title": "Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings", "comments": "10 pages, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new video synthesis task: synthesizing time lapse videos\ndepicting how a given painting might have been created. Artists paint using\nunique combinations of brushes, strokes, and colors. There are often many\npossible ways to create a given painting. Our goal is to learn to capture this\nrich range of possibilities.\n  Creating distributions of long-term videos is a challenge for learning-based\nvideo synthesis methods. We present a probabilistic model that, given a single\nimage of a completed painting, recurrently synthesizes steps of the painting\nprocess. We implement this model as a convolutional neural network, and\nintroduce a novel training scheme to enable learning from a limited dataset of\npainting time lapses. We demonstrate that this model can be used to sample many\ntime steps, enabling long-term stochastic video synthesis. We evaluate our\nmethod on digital and watercolor paintings collected from video websites, and\nshow that human raters find our synthetic videos to be similar to time lapse\nvideos produced by real artists. Our code is available at\nhttps://xamyzhao.github.io/timecraft.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 03:12:38 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 22:20:46 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhao", "Amy", ""], ["Balakrishnan", "Guha", ""], ["Lewis", "Kathleen M.", ""], ["Durand", "Fr\u00e9do", ""], ["Guttag", "John V.", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "2001.01028", "submitter": "Zirui Zhao", "authors": "Zirui Zhao, Yijun Mao, Yan Ding, Pengju Ren, Nanning Zheng", "title": "Visual Semantic SLAM with Landmarks for Large-Scale Outdoor Environment", "comments": "Accepted by 2019 China Symposium on Cognitive Computing and Hybrid\n  Intelligence(CCHI'19)", "journal-ref": null, "doi": "10.1109/CCHI.2019.8901910", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic SLAM is an important field in autonomous driving and intelligent\nagents, which can enable robots to achieve high-level navigation tasks, obtain\nsimple cognition or reasoning ability and achieve language-based\nhuman-robot-interaction. In this paper, we built a system to creat a semantic\n3D map by combining 3D point cloud from ORB SLAM with semantic segmentation\ninformation from Convolutional Neural Network model PSPNet-101 for large-scale\nenvironments. Besides, a new dataset for KITTI sequences has been built, which\ncontains the GPS information and labels of landmarks from Google Map in related\nstreets of the sequences. Moreover, we find a way to associate the real-world\nlandmark with point cloud map and built a topological map based on semantic\nmap.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 03:34:23 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zhao", "Zirui", ""], ["Mao", "Yijun", ""], ["Ding", "Yan", ""], ["Ren", "Pengju", ""], ["Zheng", "Nanning", ""]]}, {"id": "2001.01033", "submitter": "Xiaochen Liu", "authors": "Xiaochen Liu, Yurong Jiang, Kyu-Han Kim, Ramesh Govindan", "title": "Grab: Fast and Accurate Sensor Processing for Cashier-Free Shopping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cashier-free shopping systems like Amazon Go improve shopping experience, but\ncan require significant store redesign. In this paper, we propose Grab, a\npractical system that leverages existing infrastructure and devices to enable\ncashier-free shopping. Grab needs to accurately identify and track customers,\nand associate each shopper with items he or she retrieves from shelves. To do\nthis, it uses a keypoint-based pose tracker as a building block for\nidentification and tracking, develops robust feature-based face trackers, and\nalgorithms for associating and tracking arm movements. It also uses a\nprobabilistic framework to fuse readings from camera, weight and RFID sensors\nin order to accurately assess which shopper picks up which item. In experiments\nfrom a pilot deployment in a retail store, Grab can achieve over 90% precision\nand recall even when 40% of shopping actions are designed to confuse the\nsystem. Moreover, Grab has optimizations that help reduce investment in\ncomputing infrastructure four-fold.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 04:12:06 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Liu", "Xiaochen", ""], ["Jiang", "Yurong", ""], ["Kim", "Kyu-Han", ""], ["Govindan", "Ramesh", ""]]}, {"id": "2001.01034", "submitter": "Yiming Sun", "authors": "Yifei Li and Zheng Wang and Xiaoyu Lu and Kuangyan Song and Yiming Sun", "title": "FrequentNet : A New Interpretable Deep Learning Baseline for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has proposed a new baseline deep learning model of more benefits\nfor image classification. Different from the convolutional neural network(CNN)\npractice where filters are trained by back propagation to represent different\npatterns of an image, we are inspired by a method called \"PCANet\" in \"PCANet: A\nSimple Deep Learning Baseline for Image Classification?\" to choose filter\nvectors from basis vectors in frequency domain like Fourier coefficients or\nwavelets without back propagation. Researchers have demonstrated that those\nbasis in frequency domain can usually provide physical insights, which adds to\nthe interpretability of the model by analyzing the frequencies selected.\nBesides, the training process will also be more time efficient, mathematically\nclear and interpretable compared with the \"black-box\" training process of CNN.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 04:31:32 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 07:21:28 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Li", "Yifei", ""], ["Wang", "Zheng", ""], ["Lu", "Xiaoyu", ""], ["Song", "Kuangyan", ""], ["Sun", "Yiming", ""]]}, {"id": "2001.01037", "submitter": "Jiamei Sun", "authors": "Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Alexander Binder", "title": "Understanding Image Captioning Models beyond Visualizing Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper interprets the predictions of image captioning models with\nattention mechanisms beyond visualizing the attention itself. In this paper, we\ndevelop variants of layer-wise relevance propagation (LRP) and gradient-based\nexplanation methods, tailored to image captioning models with attention\nmechanisms. We compare the interpretability of attention heatmaps\nsystematically against the explanations computed with explanation methods such\nas LRP, Grad-CAM, and Guided Grad-CAM. We show that explanation methods provide\nsimultaneously pixel-wise image explanation (supporting and opposing pixels of\nthe input image) and linguistic explanation (supporting and opposing words of\nthe preceding sequence) for each word in the predicted captions. We demonstrate\nwith extensive experiments that explanation methods can 1) reveal more related\nevidence used by the model to make decisions than attention; 2) correlate to\nobject locations with high precision; 3) is helpful to `debug' the model such\nas analyzing the reasons for hallucinated object words. With the observed\nproperties of explanations, we further design an LRP-inference fine-tuning\nstrategy that can alleviate the object hallucination of image captioning\nmodels, meanwhile, maintain the sentence fluency. We conduct experiments with\ntwo widely used attention mechanisms: the adaptive attention mechanism\ncalculated with the additive attention and the multi-head attention calculated\nwith the scaled dot product.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 05:15:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 02:09:36 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 02:41:41 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2020 01:41:04 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Sun", "Jiamei", ""], ["Lapuschkin", "Sebastian", ""], ["Samek", "Wojciech", ""], ["Binder", "Alexander", ""]]}, {"id": "2001.01046", "submitter": "Minghao Chen", "authors": "Minghao Chen, Shuai Zhao, Haifeng Liu, Deng Cai", "title": "Adversarial-Learned Loss for Domain Adaptation", "comments": "Published in 34th AAAI Conference on Artificial Intelligence, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, remarkable progress has been made in learning transferable\nrepresentation across domains. Previous works in domain adaptation are majorly\nbased on two techniques: domain-adversarial learning and self-training.\nHowever, domain-adversarial learning only aligns feature distributions between\ndomains but does not consider whether the target features are discriminative.\nOn the other hand, self-training utilizes the model predictions to enhance the\ndiscrimination of target features, but it is unable to explicitly align domain\ndistributions. In order to combine the strengths of these two methods, we\npropose a novel method called Adversarial-Learned Loss for Domain Adaptation\n(ALDA). We first analyze the pseudo-label method, a typical self-training\nmethod. Nevertheless, there is a gap between pseudo-labels and the ground\ntruth, which can cause incorrect training. Thus we introduce the confusion\nmatrix, which is learned through an adversarial manner in ALDA, to reduce the\ngap and align the feature distributions. Finally, a new loss function is\nauto-constructed from the learned confusion matrix, which serves as the loss\nfor unlabeled target samples. Our ALDA outperforms state-of-the-art approaches\nin four standard domain adaptation datasets. Our code is available at\nhttps://github.com/ZJULearning/ALDA.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 06:24:44 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Chen", "Minghao", ""], ["Zhao", "Shuai", ""], ["Liu", "Haifeng", ""], ["Cai", "Deng", ""]]}, {"id": "2001.01050", "submitter": "Mingkui Tan", "authors": "Jing Liu, Bohan Zhuang, Zhuangwei Zhuang, Yong Guo, Junzhou Huang,\n  Jinhui Zhu, Mingkui Tan", "title": "Discrimination-aware Network Pruning for Deep Model Compression", "comments": "14 pages. Extended version of the NeurIPS paper arXiv:1810.11809", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3066410", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study network pruning which aims to remove redundant channels/kernels and\nhence speed up the inference of deep networks. Existing pruning methods either\ntrain from scratch with sparsity constraints or minimize the reconstruction\nerror between the feature maps of the pre-trained models and the compressed\nones. Both strategies suffer from some limitations: the former kind is\ncomputationally expensive and difficult to converge, while the latter kind\noptimizes the reconstruction error but ignores the discriminative power of\nchannels. In this paper, we propose a simple-yet-effective method called\ndiscrimination-aware channel pruning (DCP) to choose the channels that actually\ncontribute to the discriminative power. Note that a channel often consists of a\nset of kernels. Besides the redundancy in channels, some kernels in a channel\nmay also be redundant and fail to contribute to the discriminative power of the\nnetwork, resulting in kernel level redundancy. To solve this, we propose a\ndiscrimination-aware kernel pruning (DKP) method to further compress deep\nnetworks by removing redundant kernels. To prevent DCP/DKP from selecting\nredundant channels/kernels, we propose a new adaptive stopping condition, which\nhelps to automatically determine the number of selected channels/kernels and\noften results in more compact models with better performance. Extensive\nexperiments on both image classification and face recognition demonstrate the\neffectiveness of our methods. For example, on ILSVRC-12, the resultant\nResNet-50 model with 30% reduction of channels even outperforms the baseline\nmodel by 0.36% in terms of Top-1 accuracy. The pruned MobileNetV1 and\nMobileNetV2 achieve 1.93x and 1.42x inference acceleration on a mobile device,\nrespectively, with negligible performance degradation. The source code and the\npre-trained models are available at https://github.com/SCUT-AILab/DCP.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 07:07:41 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:52:18 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Jing", ""], ["Zhuang", "Bohan", ""], ["Zhuang", "Zhuangwei", ""], ["Guo", "Yong", ""], ["Huang", "Junzhou", ""], ["Zhu", "Jinhui", ""], ["Tan", "Mingkui", ""]]}, {"id": "2001.01055", "submitter": "Shuo Huang", "authors": "Huang Shuo, Zhou Ping, Shi Hao, Sun Yu, Wan Suiren", "title": "Image Speckle Noise Denoising by a Multi-Layer Fusion Enhancement Method\n  based on Block Matching and 3D Filtering", "comments": null, "journal-ref": "The Imaging Science Journal, 67(4), 224-235 (2019)", "doi": "10.1080/13682199.2019.1612589", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve speckle noise denoising of block matching 3d filtering\n(BM3D) method, an image frequency-domain multi-layer fusion enhancement method\n(MLFE-BM3D) based on nonsubsampled contourlet transform (NSCT) has been\nproposed. The method designs a NSCT hard threshold denoising enhancement to\npreprocess the image, then uses fusion enhancement in NSCT domain to fuse the\npreliminary estimation results of images before and after the NSCT hard\nthreshold denoising, finally, BM3D denoising is carried out with the fused\nimage to obtain the final denoising result. Experiments on natural images and\nmedical ultrasound images show that MLFE-BM3D method can achieve better visual\neffects than BM3D method, the peak signal to noise ratio (PSNR) of the denoised\nimage is increased by 0.5dB. The MLFE-BM3D method can improve the denoising\neffect of speckle noise in the texture region, and still maintain a good\ndenoising effect in the smooth region of the image.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 08:17:52 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Shuo", "Huang", ""], ["Ping", "Zhou", ""], ["Hao", "Shi", ""], ["Yu", "Sun", ""], ["Suiren", "Wan", ""]]}, {"id": "2001.01057", "submitter": "Qian Li", "authors": "Qian Li, Nan Guo, Xiaochun Ye, Dongrui Fan, and Zhimin Tang", "title": "Pixel-Semantic Revise of Position Learning A One-Stage Object Detector\n  with A Shared Encoder-Decoder", "comments": "Accepted by ICONIP2020(International Conference on Neural Information\n  Processing)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many methods have been proposed for object detection. They cannot\ndetect objects by semantic features, adaptively. In this work, according to\nchannel and spatial attention mechanisms, we mainly analyze that different\nmethods detect objects adaptively. Some state-of-the-art detectors combine\ndifferent feature pyramids with many mechanisms to enhance multi-level semantic\ninformation. However, they require more cost. This work addresses that by an\nanchor-free detector with shared encoder-decoder with attention mechanism,\nextracting shared features. We consider features of different levels from\nbackbone (e.g., ResNet-50) as the basis features. Then, we feed the features\ninto a simple module, followed by a detector header to detect objects.\nMeantime, we use the semantic features to revise geometric locations, and the\ndetector is a pixel-semantic revising of position. More importantly, this work\nanalyzes the impact of different pooling strategies (e.g., mean, maximum or\nminimum) on multi-scale objects, and finds the minimum pooling improve\ndetection performance on small objects better. Compared with state-of-the-art\nMNC based on ResNet-101 for the standard MSCOCO 2014 baseline, our method\nimproves detection AP of 3.8%.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 08:55:00 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 02:28:34 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Li", "Qian", ""], ["Guo", "Nan", ""], ["Ye", "Xiaochun", ""], ["Fan", "Dongrui", ""], ["Tang", "Zhimin", ""]]}, {"id": "2001.01083", "submitter": "Naina Dhingra", "authors": "Naina Dhingra and Andreas Kunz", "title": "Res3ATN -- Deep 3D Residual Attention Network for Hand Gesture\n  Recognition in Videos", "comments": "10 pages, 4 figures, International Conference on 3D Vision (3DV\n  2019), Quebec City, Canada, September 16-19, 2019", "journal-ref": "2019 International Conference on 3D Vision (3DV), 491--501, 2019", "doi": "10.1109/3DV.2019.00061", "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gesture recognition is a strenuous task to solve in videos. In this\npaper, we use a 3D residual attention network which is trained end to end for\nhand gesture recognition. Based on the stacked multiple attention blocks, we\nbuild a 3D network which generates different features at each attention block.\nOur 3D attention based residual network (Res3ATN) can be built and extended to\nvery deep layers. Using this network, an extensive analysis is performed on\nother 3D networks based on three publicly available datasets. The Res3ATN\nnetwork performance is compared to C3D, ResNet-10, and ResNext-101 networks. We\nalso study and evaluate our baseline network with different number of attention\nblocks. The comparison shows that the 3D residual attention network with 3\nattention blocks is robust in attention learning and is able to classify the\ngestures with better accuracy, thus outperforming existing networks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 14:36:36 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Dhingra", "Naina", ""], ["Kunz", "Andreas", ""]]}, {"id": "2001.01091", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Luca Benini", "title": "RPR: Random Partition Relaxation for Training; Binary and Ternary Weight\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Random Partition Relaxation (RPR), a method for strong\nquantization of neural networks weight to binary (+1/-1) and ternary (+1/0/-1)\nvalues. Starting from a pre-trained model, we quantize the weights and then\nrelax random partitions of them to their continuous values for retraining\nbefore re-quantizing them and switching to another weight partition for further\nadaptation. We demonstrate binary and ternary-weight networks with accuracies\nbeyond the state-of-the-art for GoogLeNet and competitive performance for\nResNet-18 and ResNet-50 using an SGD-based training method that can easily be\nintegrated into existing frameworks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 15:56:10 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "2001.01096", "submitter": "Weiya Ren", "authors": "Weiya Ren", "title": "Represented Value Function Approach for Large Scale Multi Agent\n  Reinforcement Learning", "comments": "9 pages the code is published and the result is reproducible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of large scale multi agent\nreinforcement learning. Firstly, we studied the representation problem of the\npairwise value function to reduce the complexity of the interactions among\nagents. Secondly, we adopt a l2-norm trick to ensure the trivial term of the\napproximated value function is bounded. Thirdly, experimental results on battle\ngame demonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 16:29:13 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 01:57:34 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Ren", "Weiya", ""]]}, {"id": "2001.01100", "submitter": "Jalil Ahmed", "authors": "Jalil Ahmed, Sulaiman Vesal, Felix Durlak, Rainer Kaergel, Nishant\n  Ravikumar, Martine Remy-Jardin, Andreas Maier", "title": "COPD Classification in CT Images Using a 3D Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronic obstructive pulmonary disease (COPD) is a lung disease that is not\nfully reversible and one of the leading causes of morbidity and mortality in\nthe world. Early detection and diagnosis of COPD can increase the survival rate\nand reduce the risk of COPD progression in patients. Currently, the primary\nexamination tool to diagnose COPD is spirometry. However, computed tomography\n(CT) is used for detecting symptoms and sub-type classification of COPD. Using\ndifferent imaging modalities is a difficult and tedious task even for\nphysicians and is subjective to inter-and intra-observer variations. Hence,\ndeveloping meth-ods that can automatically classify COPD versus healthy\npatients is of great interest. In this paper, we propose a 3D deep learning\napproach to classify COPD and emphysema using volume-wise annotations only. We\nalso demonstrate the impact of transfer learning on the classification of\nemphysema using knowledge transfer from a pre-trained COPD classification\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 16:58:45 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Ahmed", "Jalil", ""], ["Vesal", "Sulaiman", ""], ["Durlak", "Felix", ""], ["Kaergel", "Rainer", ""], ["Ravikumar", "Nishant", ""], ["Remy-Jardin", "Martine", ""], ["Maier", "Andreas", ""]]}, {"id": "2001.01121", "submitter": "Takashi Shinozaki", "authors": "Takashi Shinozaki", "title": "Biologically-Motivated Deep Learning Method using Hierarchical\n  Competitive Learning", "comments": "Appeared at NIPS 2019 Workshop: Shared Visual Representations in\n  Human and Machine Intelligence (SVRHM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a novel biologically-motivated learning method for deep\nconvolutional neural networks (CNNs). The combination of CNNs and back\npropagation (BP) learning is the most powerful method in recent machine\nlearning regimes. However, it requires large labeled data for training, and\nthis requirement can occasionally become a barrier for real world applications.\nTo address this problem and utilize unlabeled data, I propose to introduce\nunsupervised competitive learning which only requires forward propagating\nsignals as a pre-training method for CNNs. The method was evaluated by image\ndiscrimination tasks using MNIST, CIFAR-10, and ImageNet datasets, and it\nachieved a state-of-the-art performance as a biologically-motivated method in\nthe ImageNet experiment. The results suggested that the method enables\nhigher-level learning representations solely from forward propagating signals\nwithout a backward error signal for the learning of convolutional layers. The\nproposed method could be useful for a variety of poorly labeled data, for\nexample, time series or medical data.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 20:07:36 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Shinozaki", "Takashi", ""]]}, {"id": "2001.01129", "submitter": "Jiju Peethambaran Poovvancheri", "authors": "Aby Thomas, Adarsh Sunilkumar, Shankar Shylesh, Aby Abahai T.,\n  Subhasree Methirumangalath, Dong Chen and Jiju Peethambaran", "title": "TCM-ICP: Transformation Compatibility Measure for Registering Multiple\n  LIDAR Scans", "comments": "9 pages, 8 figures, submitted to IEEE GRSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Rigid registration of multi-view and multi-platform LiDAR scans is a\nfundamental problem in 3D mapping, robotic navigation, and large-scale urban\nmodeling applications. Data acquisition with LiDAR sensors involves scanning\nmultiple areas from different points of view, thus generating partially\noverlapping point clouds of the real world scenes. Traditionally, ICP\n(Iterative Closest Point) algorithm is used to register the acquired point\nclouds together to form a unique point cloud that captures the scanned real\nworld scene. Conventional ICP faces local minima issues and often needs a\ncoarse initial alignment to converge to the optimum. In this work, we present\nan algorithm for registering multiple, overlapping LiDAR scans. We introduce a\ngeometric metric called Transformation Compatibility Measure (TCM) which aids\nin choosing the most similar point clouds for registration in each iteration of\nthe algorithm. The LiDAR scan most similar to the reference LiDAR scan is then\ntransformed using simplex technique. An optimization of the transformation\nusing gradient descent and simulated annealing techniques are then applied to\nimprove the resulting registration. We evaluate the proposed algorithm on four\ndifferent real world scenes and experimental results shows that the\nregistration performance of the proposed method is comparable or superior to\nthe traditionally used registration methods. Further, the algorithm achieves\nsuperior registration results even when dealing with outliers.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 21:05:27 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 17:20:47 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Thomas", "Aby", ""], ["Sunilkumar", "Adarsh", ""], ["Shylesh", "Shankar", ""], ["T.", "Aby Abahai", ""], ["Methirumangalath", "Subhasree", ""], ["Chen", "Dong", ""], ["Peethambaran", "Jiju", ""]]}, {"id": "2001.01162", "submitter": "Xiaohong Liu", "authors": "Xiaohong Liu, Lingshi Kong, Yang Zhou, Jiying Zhao, Jun Chen", "title": "End-To-End Trainable Video Super-Resolution Based on a New Mechanism for\n  Implicit Motion Estimation and Compensation", "comments": "10 pages, accepted in WACV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution aims at generating a high-resolution video from its\nlow-resolution counterpart. With the rapid rise of deep learning, many recently\nproposed video super-resolution methods use convolutional neural networks in\nconjunction with explicit motion compensation to capitalize on statistical\ndependencies within and across low-resolution frames. Two common issues of such\nmethods are noteworthy. Firstly, the quality of the final reconstructed HR\nvideo is often very sensitive to the accuracy of motion estimation. Secondly,\nthe warp grid needed for motion compensation, which is specified by the two\nflow maps delineating pixel displacements in horizontal and vertical\ndirections, tends to introduce additional errors and jeopardize the temporal\nconsistency across video frames. To address these issues, we propose a novel\ndynamic local filter network to perform implicit motion estimation and\ncompensation by employing, via locally connected layers, sample-specific and\nposition-specific dynamic local filters that are tailored to the target pixels.\nWe also propose a global refinement network based on ResBlock and autoencoder\nstructures to exploit non-local correlations and enhance the spatial\nconsistency of super-resolved frames. The experimental results demonstrate that\nthe proposed method outperforms the state-of-the-art, and validate its strength\nin terms of local transformation handling, temporal consistency as well as edge\nsharpness.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 03:47:24 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Liu", "Xiaohong", ""], ["Kong", "Lingshi", ""], ["Zhou", "Yang", ""], ["Zhao", "Jiying", ""], ["Chen", "Jun", ""]]}, {"id": "2001.01168", "submitter": "Zhiwen Shao", "authors": "Zhiwen Shao, Lixin Zou, Jianfei Cai, Yunsheng Wu, Lizhuang Ma", "title": "Spatio-Temporal Relation and Attention Learning for Facial Action Unit\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal relations among facial action units (AUs) convey significant\ninformation for AU detection yet have not been thoroughly exploited. The main\nreasons are the limited capability of current AU detection works in\nsimultaneously learning spatial and temporal relations, and the lack of precise\nlocalization information for AU feature learning. To tackle these limitations,\nwe propose a novel spatio-temporal relation and attention learning framework\nfor AU detection. Specifically, we introduce a spatio-temporal graph\nconvolutional network to capture both spatial and temporal relations from\ndynamic AUs, in which the AU relations are formulated as a spatio-temporal\ngraph with adaptively learned instead of predefined edge weights. Moreover, the\nlearning of spatio-temporal relations among AUs requires individual AU\nfeatures. Considering the dynamism and shape irregularity of AUs, we propose an\nattention regularization method to adaptively learn regional attentions that\ncapture highly relevant regions and suppress irrelevant regions so as to\nextract a complete feature for each AU. Extensive experiments show that our\napproach achieves substantial improvements over the state-of-the-art AU\ndetection methods on BP4D and especially DISFA benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 05:14:03 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Shao", "Zhiwen", ""], ["Zou", "Lixin", ""], ["Cai", "Jianfei", ""], ["Wu", "Yunsheng", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2001.01172", "submitter": "Yaoshiang Ho", "authors": "Yaoshiang Ho, Samuel Wookey", "title": "The Human Visual System and Adversarial AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies theories about the Human Visual System to make Adversarial\nAI more effective. To date, Adversarial AI has modeled perceptual distances\nbetween clean and adversarial examples of images using Lp norms. These norms\nhave the benefit of simple mathematical description and reasonable\neffectiveness in approximating perceptual distance. However, in prior decades,\nother areas of image processing have moved beyond simpler models like Mean\nSquared Error (MSE) towards more complex models that better approximate the\nHuman Visual System (HVS). We demonstrate a proof of concept of incorporating\nHVS models into Adversarial AI.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 05:47:48 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 21:15:45 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Ho", "Yaoshiang", ""], ["Wookey", "Samuel", ""]]}, {"id": "2001.01173", "submitter": "Jie Cao", "authors": "Jie Cao, Huaibo Huang, Yi Li, Ran He, Zhenan Sun", "title": "Informative Sample Mining Network for Multi-Domain Image-to-Image\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of multi-domain image-to-image translation has been\nsignificantly improved by recent progress in deep generative models. Existing\napproaches can use a unified model to achieve translations between all the\nvisual domains. However, their outcomes are far from satisfying when there are\nlarge domain variations. In this paper, we reveal that improving the sample\nselection strategy is an effective solution. To select informative samples, we\ndynamically estimate sample importance during the training of Generative\nAdversarial Networks, presenting Informative Sample Mining Network. We\ntheoretically analyze the relationship between the sample importance and the\nprediction of the global optimal discriminator. Then a practical importance\nestimation function for general conditions is derived. Furthermore, we propose\na novel multi-stage sample training scheme to reduce sample hardness while\npreserving sample informativeness. Extensive experiments on a wide range of\nspecific image-to-image translation tasks are conducted, and the results\ndemonstrate our superiority over current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 05:48:02 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 11:15:30 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 06:38:56 GMT"}, {"version": "v4", "created": "Sun, 20 Sep 2020 09:56:53 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Cao", "Jie", ""], ["Huang", "Huaibo", ""], ["Li", "Yi", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "2001.01211", "submitter": "Lizhao Gao", "authors": "Lizhao Gao, Haihua Xu, Chong Sun, Junling Liu, Yu-Wing Tai", "title": "Spatial-Scale Aligned Network for Fine-Grained Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for fine-grained visual recognition focus on learning\nmarginal region-based representations while neglecting the spatial and scale\nmisalignments, leading to inferior performance. In this paper, we propose the\nspatial-scale aligned network (SSANET) and implicitly address misalignments\nduring the recognition process. Especially, SSANET consists of 1) a\nself-supervised proposal mining formula with Morphological Alignment\nConstraints; 2) a discriminative scale mining (DSM) module, which exploits the\nfeature pyramid via a circulant matrix, and provides the Fourier solver for\nfast scale alignments; 3) an oriented pooling (OP) module, that performs the\npooling operation in several pre-defined orientations. Each orientation defines\none kind of spatial alignment, and the network automatically determines which\nis the optimal alignments through learning. With the proposed two modules, our\nalgorithm can automatically determine the accurate local proposal regions and\ngenerate more robust target representations being invariant to various\nappearance variances. Extensive experiments verify that SSANET is competent at\nlearning better spatial-scale invariant target representations, yielding\nsuperior performance on the fine-grained recognition task on several\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 11:12:08 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Gao", "Lizhao", ""], ["Xu", "Haihua", ""], ["Sun", "Chong", ""], ["Liu", "Junling", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "2001.01233", "submitter": "Wenwei Zhang", "authors": "Dongzhan Zhou, Xinchi Zhou, Wenwei Zhang, Chen Change Loy, Shuai Yi,\n  Xuesen Zhang, Wanli Ouyang", "title": "EcoNAS: Finding Proxies for Economical Neural Architecture Search", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) achieves significant progress in many\ncomputer vision tasks. While many methods have been proposed to improve the\nefficiency of NAS, the search progress is still laborious because training and\nevaluating plausible architectures over large search space is time-consuming.\nAssessing network candidates under a proxy (i.e., computationally reduced\nsetting) thus becomes inevitable. In this paper, we observe that most existing\nproxies exhibit different behaviors in maintaining the rank consistency among\nnetwork candidates. In particular, some proxies can be more reliable -- the\nrank of candidates does not differ much comparing their reduced setting\nperformance and final performance. In this paper, we systematically investigate\nsome widely adopted reduction factors and report our observations. Inspired by\nthese observations, we present a reliable proxy and further formulate a\nhierarchical proxy strategy. The strategy spends more computations on candidate\nnetworks that are potentially more accurate, while discards unpromising ones in\nearly stage with a fast proxy. This leads to an economical evolutionary-based\nNAS (EcoNAS), which achieves an impressive 400x search time reduction in\ncomparison to the evolutionary-based state of the art (8 vs. 3150 GPU days).\nSome new proxies led by our observations can also be applied to accelerate\nother NAS methods while still able to discover good candidate networks with\nperformance matching those found by previous proxy strategies.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 13:29:02 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 02:42:45 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Zhou", "Dongzhan", ""], ["Zhou", "Xinchi", ""], ["Zhang", "Wenwei", ""], ["Loy", "Chen Change", ""], ["Yi", "Shuai", ""], ["Zhang", "Xuesen", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2001.01240", "submitter": "Pravendra Singh", "authors": "Pravendra Singh, Munender Varshney, Vinay P. Namboodiri", "title": "Cooperative Initialization based Deep Neural Network Training", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have proposed various activation functions. These activation\nfunctions help the deep network to learn non-linear behavior with a significant\neffect on training dynamics and task performance. The performance of these\nactivations also depends on the initial state of the weight parameters, i.e.,\ndifferent initial state leads to a difference in the performance of a network.\nIn this paper, we have proposed a cooperative initialization for training the\ndeep network using ReLU activation function to improve the network performance.\nOur approach uses multiple activation functions in the initial few epochs for\nthe update of all sets of weight parameters while training the network. These\nactivation functions cooperate to overcome their drawbacks in the update of\nweight parameters, which in effect learn better \"feature representation\" and\nboost the network performance later. Cooperative initialization based training\nalso helps in reducing the overfitting problem and does not increase the number\nof parameters, inference (test) time in the final model while improving the\nperformance. Experiments show that our approach outperforms various baselines\nand, at the same time, performs well over various tasks such as classification\nand detection. The Top-1 classification accuracy of the model trained using our\napproach improves by 2.8% for VGG-16 and 2.1% for ResNet-56 on CIFAR-100\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 14:08:46 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Singh", "Pravendra", ""], ["Varshney", "Munender", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2001.01248", "submitter": "Marco Monforte", "authors": "Marco Monforte, Ander Arriandiaga, Arren Glover and Chiara Bartolozzi", "title": "Exploiting Event Cameras for Spatio-Temporal Prediction of Fast-Changing\n  Trajectories", "comments": "5 pages, 5 figures, 1 table, paper accepted for presentation at the\n  2nd IEEE International Conference on Artificial Intelligence Circuits and\n  Systems (AICAS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates trajectory prediction for robotics, to improve the\ninteraction of robots with moving targets, such as catching a bouncing ball.\nUnexpected, highly-non-linear trajectories cannot easily be predicted with\nregression-based fitting procedures, therefore we apply state of the art\nmachine learning, specifically based on Long-Short Term Memory (LSTM)\narchitectures. In addition, fast moving targets are better sensed using event\ncameras, which produce an asynchronous output triggered by spatial change,\nrather than at fixed temporal intervals as with traditional cameras. We\ninvestigate how LSTM models can be adapted for event camera data, and in\nparticular look at the benefit of using asynchronously sampled data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 14:37:28 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 13:13:02 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Monforte", "Marco", ""], ["Arriandiaga", "Ander", ""], ["Glover", "Arren", ""], ["Bartolozzi", "Chiara", ""]]}, {"id": "2001.01258", "submitter": "Vegard Antun", "authors": "Nina M. Gottschling, Vegard Antun, Ben Adcock and Anders C. Hansen", "title": "The troublesome kernel: why deep learning for inverse problems is\n  typically unstable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is overwhelming empirical evidence that Deep Learning (DL) leads to\nunstable methods in applications ranging from image classification and computer\nvision to voice recognition and automated diagnosis in medicine. Recently, a\nsimilar instability phenomenon has been discovered when DL is used to solve\ncertain problems in computational science, namely, inverse problems in imaging.\nIn this paper we present a comprehensive mathematical analysis explaining the\nmany facets of the instability phenomenon in DL for inverse problems. Our main\nresults not only explain why this phenomenon occurs, they also shed light as to\nwhy finding a cure for instabilities is so difficult in practice. Additionally,\nthese theorems show that instabilities are typically not rare events - rather,\nthey can occur even when the measurements are subject to completely random\nnoise - and consequently how easy it can be to destablise certain trained\nneural networks. We also examine the delicate balance between reconstruction\nperformance and stability, and in particular, how DL methods may outperform\nstate-of-the-art sparse regularization methods, but at the cost of instability.\nFinally, we demonstrate a counterintuitive phenomenon: training a neural\nnetwork may generically not yield an optimal reconstruction method for an\ninverse problem.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 15:30:23 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Gottschling", "Nina M.", ""], ["Antun", "Vegard", ""], ["Adcock", "Ben", ""], ["Hansen", "Anders C.", ""]]}, {"id": "2001.01259", "submitter": "Arnab Karmakar", "authors": "Arnab Karmakar, Deepak Mishra", "title": "A Robust Pose Transformational GAN for Pose Guided Person Image\n  Synthesis", "comments": "Accepted in 7th National Conference on Computer Vision, Pattern\n  Recognition, Image Processing and Graphics (NCVPRIPG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generating photorealistic images of human subjects in any unseen pose have\ncrucial applications in generating a complete appearance model of the subject.\nHowever, from a computer vision perspective, this task becomes significantly\nchallenging due to the inability of modelling the data distribution conditioned\non pose. Existing works use a complicated pose transformation model with\nvarious additional features such as foreground segmentation, human body parsing\netc. to achieve robustness that leads to computational overhead. In this work,\nwe propose a simple yet effective pose transformation GAN by utilizing the\nResidual Learning method without any additional feature learning to generate a\ngiven human image in any arbitrary pose. Using effective data augmentation\ntechniques and cleverly tuning the model, we achieve robustness in terms of\nillumination, occlusion, distortion and scale. We present a detailed study,\nboth qualitative and quantitative, to demonstrate the superiority of our model\nover the existing methods on two large datasets.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 15:32:35 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Karmakar", "Arnab", ""], ["Mishra", "Deepak", ""]]}, {"id": "2001.01265", "submitter": "Hyeonseong Jeon", "authors": "Hyeonseong Jeon, Youngoh Bang, Simon S. Woo", "title": "FDFtNet: Facing Off Fake Images using Fake Detection Fine-tuning Network", "comments": "IFIP-Sec 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating fake images and videos such as \"Deepfake\" has become much easier\nthese days due to the advancement in Generative Adversarial Networks (GANs).\nMoreover, recent research such as the few-shot learning can create highly\nrealistic personalized fake images with only a few images. Therefore, the\nthreat of Deepfake to be used for a variety of malicious intents such as\npropagating fake images and videos becomes prevalent. And detecting these\nmachine-generated fake images has been quite challenging than ever. In this\nwork, we propose a light-weight robust fine-tuning neural network-based\nclassifier architecture called Fake Detection Fine-tuning Network (FDFtNet),\nwhich is capable of detecting many of the new fake face image generation\nmodels, and can be easily combined with existing image classification networks\nand finetuned on a few datasets. In contrast to many existing methods, our\napproach aims to reuse popular pre-trained models with only a few images for\nfine-tuning to effectively detect fake images. The core of our approach is to\nintroduce an image-based self-attention module called Fine-Tune Transformer\nthat uses only the attention module and the down-sampling layer. This module is\nadded to the pre-trained model and fine-tuned on a few data to search for new\nsets of feature space to detect fake images. We experiment with our FDFtNet on\nthe GANsbased dataset (Progressive Growing GAN) and Deepfake-based dataset\n(Deepfake and Face2Face) with a small input image resolution of 64x64 that\ncomplicates detection. Our FDFtNet achieves an overall accuracy of 90.29% in\ndetecting fake images generated from the GANs-based dataset, outperforming the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 16:04:17 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 06:08:29 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Jeon", "Hyeonseong", ""], ["Bang", "Youngoh", ""], ["Woo", "Simon S.", ""]]}, {"id": "2001.01275", "submitter": "Wenchi Ma", "authors": "Ziming Zhang, Wenchi Ma, Yuanwei Wu, Guanghui Wang", "title": "Self-Orthogonality Module: A Network Architecture Plug-in for Learning\n  Orthogonal Filters", "comments": "This version fixed the controversial expression in Section 2.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the empirical impact of orthogonality\nregularization (OR) in deep learning, either solo or collaboratively. Recent\nworks on OR showed some promising results on the accuracy. In our ablation\nstudy, however, we do not observe such significant improvement from existing OR\ntechniques compared with the conventional training based on weight decay,\ndropout, and batch normalization. To identify the real gain from OR, inspired\nby the locality sensitive hashing (LSH) in angle estimation, we propose to\nintroduce an implicit self-regularization into OR to push the mean and variance\nof filter angles in a network towards 90 and 0 simultaneously to achieve (near)\northogonality among the filters, without using any other explicit\nregularization. Our regularization can be implemented as an architectural\nplug-in and integrated with an arbitrary network. We reveal that OR helps\nstabilize the training process and leads to faster convergence and better\ngeneralization.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 17:31:07 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 18:10:58 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Zhang", "Ziming", ""], ["Ma", "Wenchi", ""], ["Wu", "Yuanwei", ""], ["Wang", "Guanghui", ""]]}, {"id": "2001.01277", "submitter": "Sanket Badhe", "authors": "Sanket Badhe, Varun Singh, Joy Li and Paras Lakhani", "title": "Automated Segmentation of Vertebrae on Lateral Chest Radiography Using\n  Deep Learning", "comments": "10 pages, Accepted Poster presentation at Conference on Machine\n  Intelligence in Medical Imaging 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to develop an automated algorithm for thoracic\nvertebral segmentation on chest radiography using deep learning. 124\nde-identified lateral chest radiographs on unique patients were obtained.\nSegmentations of visible vertebrae were manually performed by a medical student\nand verified by a board-certified radiologist. 74 images were used for\ntraining, 10 for validation, and 40 were held out for testing. A U-Net deep\nconvolutional neural network was employed for segmentation, using the sum of\ndice coefficient and binary cross-entropy as the loss function. On the test\nset, the algorithm demonstrated an average dice coefficient value of 90.5 and\nan average intersection-over-union (IoU) of 81.75. Deep learning demonstrates\npromise in the segmentation of vertebrae on lateral chest radiography.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 17:35:04 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Badhe", "Sanket", ""], ["Singh", "Varun", ""], ["Li", "Joy", ""], ["Lakhani", "Paras", ""]]}, {"id": "2001.01279", "submitter": "Xie Tingli", "authors": "Xufeng Huang, Qiang Lei, Tingli Xie, Yahui Zhang, Zhen Hu, Qi Zhou", "title": "Deep Transfer Convolutional Neural Network and Extreme Learning Machine\n  for Lung Nodule Diagnosis on CT images", "comments": "Some content of the article needs to be kept secret", "journal-ref": "Knowledge-Based Systems (2020) 106230", "doi": "10.1016/j.knosys.2020.106230.", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some content of the article needs to be kept secret\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 17:49:38 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 10:36:35 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Huang", "Xufeng", ""], ["Lei", "Qiang", ""], ["Xie", "Tingli", ""], ["Zhang", "Yahui", ""], ["Hu", "Zhen", ""], ["Zhou", "Qi", ""]]}, {"id": "2001.01284", "submitter": "Zhiyong Dou", "authors": "Zhiyong Dou, Haotian Cui, Lin Zhang, Bo Wang", "title": "Learning Global and Local Consistent Representations for Unsupervised\n  Image Retrieval via Deep Graph Diffusion Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion has shown great success in improving accuracy of unsupervised image\nretrieval systems by utilizing high-order structures of image manifold.\nHowever, existing diffusion methods suffer from three major limitations: 1)\nthey usually rely on local structures without considering global manifold\ninformation; 2) they focus on improving pair-wise similarities within existing\nimages input output transductively while lacking flexibility to learn\nrepresentations for novel unseen instances inductively; 3) they fail to scale\nto large datasets due to prohibitive memory consumption and computational\nburden due to intrinsic high-order operations on the whole graph. In this\npaper, to address these limitations, we propose a novel method, Graph Diffusion\nNetworks (GRAD-Net), that adopts graph neural networks (GNNs), a novel variant\nof deep learning algorithms on irregular graphs. GRAD-Net learns semantic\nrepresentations by exploiting both local and global structures of image\nmanifold in an unsupervised fashion. By utilizing sparse coding techniques,\nGRAD-Net not only preserves global information on the image manifold, but also\nenables scalable training and efficient querying. Experiments on several large\nbenchmark datasets demonstrate effectiveness of our method over\nstate-of-the-art diffusion algorithms for unsupervised image retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 18:24:28 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 19:53:53 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Dou", "Zhiyong", ""], ["Cui", "Haotian", ""], ["Zhang", "Lin", ""], ["Wang", "Bo", ""]]}, {"id": "2001.01290", "submitter": "Brian Chen", "authors": "Brian Chen, Bo Wu, Alireza Zareian, Hanwang Zhang, Shih-Fu Chang", "title": "General Partial Label Learning via Dual Bipartite Graph Autoencoder", "comments": "8 pages", "journal-ref": "AAAI, vol. 34, no. 07, pp. 10502-10509, Apr. 2020", "doi": "10.1609/aaai.v34i07.6621", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate a practical yet challenging problem: General Partial Label\nLearning (GPLL). Compared to the traditional Partial Label Learning (PLL)\nproblem, GPLL relaxes the supervision assumption from instance-level --- a\nlabel set partially labels an instance --- to group-level: 1) a label set\npartially labels a group of instances, where the within-group instance-label\nlink annotations are missing, and 2) cross-group links are allowed ---\ninstances in a group may be partially linked to the label set from another\ngroup. Such ambiguous group-level supervision is more practical in real-world\nscenarios as additional annotation on the instance-level is no longer required,\ne.g., face-naming in videos where the group consists of faces in a frame,\nlabeled by a name set in the corresponding caption. In this paper, we propose a\nnovel graph convolutional network (GCN) called Dual Bipartite Graph Autoencoder\n(DB-GAE) to tackle the label ambiguity challenge of GPLL. First, we exploit the\ncross-group correlations to represent the instance groups as dual bipartite\ngraphs: within-group and cross-group, which reciprocally complements each other\nto resolve the linking ambiguities. Second, we design a GCN autoencoder to\nencode and decode them, where the decodings are considered as the refined\nresults. It is worth noting that DB-GAE is self-supervised and transductive, as\nit only uses the group-level supervision without a separate offline training\nstage. Extensive experiments on two real-world datasets demonstrate that DB-GAE\nsignificantly outperforms the best baseline over absolute 0.159 F1-score and\n24.8% accuracy. We further offer analysis on various levels of label\nambiguities.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 19:00:41 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Chen", "Brian", ""], ["Wu", "Bo", ""], ["Zareian", "Alireza", ""], ["Zhang", "Hanwang", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2001.01293", "submitter": "Samet Akcay", "authors": "Samet Akcay and Toby Breckon", "title": "Towards Automatic Threat Detection: A Survey of Advances of Deep\n  Learning within X-ray Security Imaging", "comments": "Deep Learning in X-ray Security Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray security screening is widely used to maintain aviation/transport\nsecurity, and its significance poses a particular interest in automated\nscreening systems. This paper aims to review computerised X-ray security\nimaging algorithms by taxonomising the field into conventional machine learning\nand contemporary deep learning applications. The first part briefly discusses\nthe classical machine learning approaches utilised within X-ray security\nimaging, while the latter part thoroughly investigates the use of modern deep\nlearning algorithms. The proposed taxonomy sub-categorises the use of deep\nlearning approaches into supervised, semi-supervised and unsupervised learning,\nwith a particular focus on object classification, detection, segmentation and\nanomaly detection tasks. The paper further explores well-established X-ray\ndatasets and provides a performance benchmark. Based on the current and future\ntrends in deep learning, the paper finally presents a discussion and future\ndirections for X-ray security imagery.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 19:17:32 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Akcay", "Samet", ""], ["Breckon", "Toby", ""]]}, {"id": "2001.01306", "submitter": "Honghui Shi", "authors": "Mang Tik Chiu, Xingqian Xu, Yunchao Wei, Zilong Huang, Alexander\n  Schwing, Robert Brunner, Hrant Khachatrian, Hovnatan Karapetyan, Ivan Dozier,\n  Greg Rose, David Wilson, Adrian Tudor, Naira Hovakimyan, Thomas S. Huang,\n  Honghui Shi", "title": "Agriculture-Vision: A Large Aerial Image Database for Agricultural\n  Pattern Analysis", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning in visual recognition tasks has driven\nadvancements in multiple fields of research. Particularly, increasing attention\nhas been drawn towards its application in agriculture. Nevertheless, while\nvisual pattern recognition on farmlands carries enormous economic values,\nlittle progress has been made to merge computer vision and crop sciences due to\nthe lack of suitable agricultural image datasets. Meanwhile, problems in\nagriculture also pose new challenges in computer vision. For example, semantic\nsegmentation of aerial farmland images requires inference over extremely\nlarge-size images with extreme annotation sparsity. These challenges are not\npresent in most of the common object datasets, and we show that they are more\nchallenging than many other aerial image datasets. To encourage research in\ncomputer vision for agriculture, we present Agriculture-Vision: a large-scale\naerial farmland image dataset for semantic segmentation of agricultural\npatterns. We collected 94,986 high-quality aerial images from 3,432 farmlands\nacross the US, where each image consists of RGB and Near-infrared (NIR)\nchannels with resolution as high as 10 cm per pixel. We annotate nine types of\nfield anomaly patterns that are most important to farmers. As a pilot study of\naerial agricultural semantic segmentation, we perform comprehensive experiments\nusing popular semantic segmentation models; we also propose an effective model\ndesigned for aerial agricultural pattern recognition. Our experiments\ndemonstrate several challenges Agriculture-Vision poses to both the computer\nvision and agriculture communities. Future versions of this dataset will\ninclude even more aerial images, anomaly patterns and image channels. More\ninformation at https://www.agriculture-vision.com.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 20:19:33 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 04:13:08 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Chiu", "Mang Tik", ""], ["Xu", "Xingqian", ""], ["Wei", "Yunchao", ""], ["Huang", "Zilong", ""], ["Schwing", "Alexander", ""], ["Brunner", "Robert", ""], ["Khachatrian", "Hrant", ""], ["Karapetyan", "Hovnatan", ""], ["Dozier", "Ivan", ""], ["Rose", "Greg", ""], ["Wilson", "David", ""], ["Tudor", "Adrian", ""], ["Hovakimyan", "Naira", ""], ["Huang", "Thomas S.", ""], ["Shi", "Honghui", ""]]}, {"id": "2001.01330", "submitter": "Radu Tudor Ionescu", "authors": "Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Nicolae Verga", "title": "Convolutional Neural Networks with Intermediate Loss for 3D\n  Super-Resolution of CT and MRI Scans", "comments": "Accepted in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2020.2980266", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  CT scanners that are commonly-used in hospitals nowadays produce\nlow-resolution images, up to 512 pixels in size. One pixel in the image\ncorresponds to a one millimeter piece of tissue. In order to accurately segment\ntumors and make treatment plans, doctors need CT scans of higher resolution.\nThe same problem appears in MRI. In this paper, we propose an approach for the\nsingle-image super-resolution of 3D CT or MRI scans. Our method is based on\ndeep convolutional neural networks (CNNs) composed of 10 convolutional layers\nand an intermediate upscaling layer that is placed after the first 6\nconvolutional layers. Our first CNN, which increases the resolution on two axes\n(width and height), is followed by a second CNN, which increases the resolution\non the third axis (depth). Different from other methods, we compute the loss\nwith respect to the ground-truth high-resolution output right after the\nupscaling layer, in addition to computing the loss after the last convolutional\nlayer. The intermediate loss forces our network to produce a better output,\ncloser to the ground-truth. A widely-used approach to obtain sharp results is\nto add Gaussian blur using a fixed standard deviation. In order to avoid\noverfitting to a fixed standard deviation, we apply Gaussian smoothing with\nvarious standard deviations, unlike other approaches. We evaluate our method in\nthe context of 2D and 3D super-resolution of CT and MRI scans from two\ndatabases, comparing it to relevant related works from the literature and\nbaselines based on various interpolation schemes, using 2x and 4x scaling\nfactors. The empirical results show that our approach attains superior results\nto all other methods. Moreover, our human annotation study reveals that both\ndoctors and regular annotators chose our method in favor of Lanczos\ninterpolation in 97.55% cases for 2x upscaling factor and in 96.69% cases for\n4x upscaling factor.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 23:21:40 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 18:13:14 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 14:19:24 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Georgescu", "Mariana-Iuliana", ""], ["Ionescu", "Radu Tudor", ""], ["Verga", "Nicolae", ""]]}, {"id": "2001.01349", "submitter": "Tong He", "authors": "Tong He and Dong Gong and Zhi Tian and Chunhua Shen", "title": "Learning and Memorizing Representative Prototypes for 3D Point Cloud\n  Semantic and Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud semantic and instance segmentation is crucial and fundamental\nfor 3D scene understanding. Due to the complex structure, point sets are\ndistributed off balance and diversely, which appears as both category imbalance\nand pattern imbalance. As a result, deep networks can easily forget the\nnon-dominant cases during the learning process, resulting in unsatisfactory\nperformance. Although re-weighting can reduce the influence of the\nwell-classified examples, they cannot handle the non-dominant patterns during\nthe dynamic training. In this paper, we propose a memory-augmented network to\nlearn and memorize the representative prototypes that cover diverse samples\nuniversally. Specifically, a memory module is introduced to alleviate the\nforgetting issue by recording the patterns seen in mini-batch training. The\nlearned memory items consistently reflect the interpretable and meaningful\ninformation for both dominant and non-dominant categories and cases. The\ndistorted observations and rare cases can thus be augmented by retrieving the\nstored prototypes, leading to better performances and generalization.\nExhaustive experiments on the benchmarks, i.e. S3DIS and ScanNetV2, reflect the\nsuperiority of our method on both effectiveness and efficiency. Not only the\noverall accuracy but also nondominant classes have improved substantially.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 01:07:46 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["He", "Tong", ""], ["Gong", "Dong", ""], ["Tian", "Zhi", ""], ["Shen", "Chunhua", ""]]}, {"id": "2001.01354", "submitter": "Deyu Yin", "authors": "Deyu Yin, Qian Zhang, Jingbin Liu, Xinlian Liang, Yunsheng Wang, Jyri\n  Maanp\\\"a\\\"a, Hao Ma, Juha Hyypp\\\"a, and Ruizhi Chen", "title": "CAE-LO: LiDAR Odometry Leveraging Fully Unsupervised Convolutional\n  Auto-Encoder for Interest Point Detection and Feature Description", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important technology in 3D mapping, autonomous driving, and robot\nnavigation, LiDAR odometry is still a challenging task. Appropriate data\nstructure and unsupervised deep learning are the keys to achieve an easy\nadjusted LiDAR odometry solution with high performance. Utilizing compact 2D\nstructured spherical ring projection model and voxel model which preserves the\noriginal shape of input data, we propose a fully unsupervised Convolutional\nAuto-Encoder based LiDAR Odometry (CAE-LO) that detects interest points from\nspherical ring data using 2D CAE and extracts features from multi-resolution\nvoxel model using 3D CAE. We make several key contributions: 1) experiments\nbased on KITTI dataset show that our interest points can capture more local\ndetails to improve the matching success rate on unstructured scenarios and our\nfeatures outperform state-of-the-art by more than 50% in matching inlier ratio;\n2) besides, we also propose a keyframe selection method based on matching pairs\ntransferring, an odometry refinement method for keyframes based on extended\ninterest points from spherical rings, and a backward pose update method. The\nodometry refinement experiments verify the proposed ideas' feasibility and\neffectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 01:26:28 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 07:32:36 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 01:22:44 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yin", "Deyu", ""], ["Zhang", "Qian", ""], ["Liu", "Jingbin", ""], ["Liang", "Xinlian", ""], ["Wang", "Yunsheng", ""], ["Maanp\u00e4\u00e4", "Jyri", ""], ["Ma", "Hao", ""], ["Hyypp\u00e4", "Juha", ""], ["Chen", "Ruizhi", ""]]}, {"id": "2001.01385", "submitter": "Wei-Lun Chao", "authors": "Han-Jia Ye, Hong-You Chen, De-Chuan Zhan, Wei-Lun Chao", "title": "Identifying and Compensating for Feature Deviation in Imbalanced Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate learning a ConvNet classifier with class-imbalanced data. We\nfound that a ConvNet significantly over-fits the minor classes that do not have\nsufficient training instances, which is quite opposite to a traditional machine\nlearning model like logistic regression that often under-fits minor classes. We\nconduct a series of analysis and argue that feature deviation between the\ntraining and test instances serves as the main cause. We propose to incorporate\nclass-dependent temperatures (CDT) in learning a ConvNet: CDT forces the\nminor-class instances to have larger decision values in the training phase, so\nas to compensate for the effect of feature deviation in the test data. We\nvalidate our approach on several benchmark datasets and achieve promising\nperformance. We hope that our insights can inspire new ways of thinking in\nresolving class-imbalanced deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 03:52:11 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 05:10:52 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 00:13:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ye", "Han-Jia", ""], ["Chen", "Hong-You", ""], ["Zhan", "De-Chuan", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2001.01425", "submitter": "Zhongling Huang", "authors": "Zhongling Huang, Corneliu Octavian Dumitru, Zongxu Pan, Bin Lei, Mihai\n  Datcu", "title": "Classification of Large-Scale High-Resolution SAR Images with Deep\n  Transfer Learning", "comments": null, "journal-ref": "IEEE Geoscience and Remote Sensing Letters 2020", "doi": "10.1109/LGRS.2020.2965558", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The classification of large-scale high-resolution SAR land cover images\nacquired by satellites is a challenging task, facing several difficulties such\nas semantic annotation with expertise, changing data characteristics due to\nvarying imaging parameters or regional target area differences, and complex\nscattering mechanisms being different from optical imaging. Given a large-scale\nSAR land cover dataset collected from TerraSAR-X images with a hierarchical\nthree-level annotation of 150 categories and comprising more than 100,000\npatches, three main challenges in automatically interpreting SAR images of\nhighly imbalanced classes, geographic diversity, and label noise are addressed.\nIn this letter, a deep transfer learning method is proposed based on a\nsimilarly annotated optical land cover dataset (NWPU-RESISC45). Besides, a\ntop-2 smooth loss function with cost-sensitive parameters was introduced to\ntackle the label noise and imbalanced classes' problems. The proposed method\nshows high efficiency in transferring information from a similarly annotated\nremote sensing dataset, a robust performance on highly imbalanced classes, and\nis alleviating the over-fitting problem caused by label noise. What's more, the\nlearned deep model has a good generalization for other SAR-specific tasks, such\nas MSTAR target recognition with a state-of-the-art classification accuracy of\n99.46%.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 07:22:28 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Huang", "Zhongling", ""], ["Dumitru", "Corneliu Octavian", ""], ["Pan", "Zongxu", ""], ["Lei", "Bin", ""], ["Datcu", "Mihai", ""]]}, {"id": "2001.01431", "submitter": "Yuge Zhang", "authors": "Yuge Zhang, Zejun Lin, Junyang Jiang, Quanlu Zhang, Yujing Wang, Hui\n  Xue, Chen Zhang, Yaming Yang", "title": "Deeper Insights into Weight Sharing in Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of deep neural networks, Neural Architecture Search (NAS) as\na way of automatic model design has attracted wide attention. As training every\nchild model from scratch is very time-consuming, recent works leverage\nweight-sharing to speed up the model evaluation procedure. These approaches\ngreatly reduce computation by maintaining a single copy of weights on the\nsuper-net and share the weights among every child model. However,\nweight-sharing has no theoretical guarantee and its impact has not been well\nstudied before. In this paper, we conduct comprehensive experiments to reveal\nthe impact of weight-sharing: (1) The best-performing models from different\nruns or even from consecutive epochs within the same run have significant\nvariance; (2) Even with high variance, we can extract valuable information from\ntraining the super-net with shared weights; (3) The interference between child\nmodels is a main factor that induces high variance; (4) Properly reducing the\ndegree of weight sharing could effectively reduce variance and improve\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 07:50:08 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zhang", "Yuge", ""], ["Lin", "Zejun", ""], ["Jiang", "Junyang", ""], ["Zhang", "Quanlu", ""], ["Wang", "Yujing", ""], ["Xue", "Hui", ""], ["Zhang", "Chen", ""], ["Yang", "Yaming", ""]]}, {"id": "2001.01456", "submitter": "Faisal Ghaffar", "authors": "Faisal Ghaffar", "title": "Facial Emotions Recognition using Convolutional Neural Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings displays their emotions using facial expressions. For human it\nis very easy to recognize those emotions but for computer it is very\nchallenging. Facial expressions vary from person to person. Brightness,\ncontrast and resolution of every random image is different. This is why\nrecognizing facial expression is very difficult. The facial expression\nrecognition is an active research area. In this project, we worked on\nrecognition of seven basic human emotions. These emotions are angry, disgust,\nfear, happy, sad, surprise and neutral. Every image was first passed through\nface detection algorithm to include it in train dataset. As CNN requires large\namount of data so we duplicated our data using various filter on each image.\nThe system is trained using CNN architecture. Preprocessed images of size\n80*100 is passed as input to the first layer of CNN. Three convolutional layers\nwere used, each of which was followed by a pooling layer and then three dense\nlayers. The dropout rate for dense layer was 20%. The model was trained by\ncombination of two publicly available datasets JAFFED and KDEF. 90% of the data\nwas used for training while 10% was used for testing. We achieved maximum\naccuracy of 78% using combined dataset.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 09:43:06 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Ghaffar", "Faisal", ""]]}, {"id": "2001.01458", "submitter": "Yingshi Chen", "authors": "Yingshi Chen", "title": "Express Wavenet -- a low parameter optical neural network with random\n  shift wavelet pattern", "comments": "5 pages,4 figures", "journal-ref": null, "doi": "10.1016/j.optcom.2020.126709", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Express Wavenet is an improved optical diffractive neural network. At each\nlayer, it uses wavelet-like pattern to modulate the phase of optical waves. For\ninput image with n2 pixels, express wavenet reduce parameter number from O(n2)\nto O(n). Only need one percent of the parameters, and the accuracy is still\nvery high. In the MNIST dataset, it only needs 1229 parameters to get accuracy\nof 92%, while the standard optical network needs 125440 parameters. The random\nshift wavelets show the characteristics of optical network more vividly.\nEspecially the vanishing gradient phenomenon in the training process. We\npresent a modified expressway structure for this problem. Experiments verified\nthe effect of random shift wavelet and expressway structure. Our work shows\noptical diffractive network would use much fewer parameters than other neural\nnetworks. The source codes are available at\nhttps://github.com/closest-git/ONNet.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 09:45:20 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chen", "Yingshi", ""]]}, {"id": "2001.01469", "submitter": "Vishwanath D", "authors": "Shubham Paliwal, Vishwanath D, Rohit Rahul, Monika Sharma, Lovekesh\n  Vig", "title": "TableNet: Deep Learning model for end-to-end Table detection and Tabular\n  data extraction from Scanned Document Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of mobile phones and scanners to photograph and\nupload documents, the need for extracting the information trapped in\nunstructured document images such as retail receipts, insurance claim forms and\nfinancial invoices is becoming more acute. A major hurdle to this objective is\nthat these images often contain information in the form of tables and\nextracting data from tabular sub-images presents a unique set of challenges.\nThis includes accurate detection of the tabular region within an image, and\nsubsequently detecting and extracting information from the rows and columns of\nthe detected table. While some progress has been made in table detection,\nextracting the table contents is still a challenge since this involves more\nfine grained table structure(rows & columns) recognition. Prior approaches have\nattempted to solve the table detection and structure recognition problems\nindependently using two separate models. In this paper, we propose TableNet: a\nnovel end-to-end deep learning model for both table detection and structure\nrecognition. The model exploits the interdependence between the twin tasks of\ntable detection and table structure recognition to segment out the table and\ncolumn regions. This is followed by semantic rule-based row extraction from the\nidentified tabular sub-regions. The proposed model and extraction approach was\nevaluated on the publicly available ICDAR 2013 and Marmot Table datasets\nobtaining state of the art results. Additionally, we demonstrate that feeding\nadditional semantic features further improves model performance and that the\nmodel exhibits transfer learning across datasets. Another contribution of this\npaper is to provide additional table structure annotations for the Marmot data,\nwhich currently only has annotations for table detection.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 10:25:32 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Paliwal", "Shubham", ""], ["D", "Vishwanath", ""], ["Rahul", "Rohit", ""], ["Sharma", "Monika", ""], ["Vig", "Lovekesh", ""]]}, {"id": "2001.01506", "submitter": "Lin Wang", "authors": "Lin Wang, Wonjune Cho, and Kuk-Jin Yoon", "title": "Deceiving Image-to-Image Translation Networks for Autonomous Driving\n  with Adversarial Perturbations", "comments": "8pages, Accepted on IEEE Robotics and Automation Letters (RAL)", "journal-ref": null, "doi": "10.1109/LRA.2020.2967289", "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNNs) have achieved impressive performance on handling\ncomputer vision problems, however, it has been found that DNNs are vulnerable\nto adversarial examples. For such reason, adversarial perturbations have been\nrecently studied in several respects. However, most previous works have focused\non image classification tasks, and it has never been studied regarding\nadversarial perturbations on Image-to-image (Im2Im) translation tasks, showing\ngreat success in handling paired and/or unpaired mapping problems in the field\nof autonomous driving and robotics. This paper examines different types of\nadversarial perturbations that can fool Im2Im frameworks for autonomous driving\npurpose. We propose both quasi-physical and digital adversarial perturbations\nthat can make Im2Im models yield unexpected results. We then empirically\nanalyze these perturbations and show that they generalize well under both\npaired for image synthesis and unpaired settings for style transfer. We also\nvalidate that there exist some perturbation thresholds over which the Im2Im\nmapping is disrupted or impossible. The existence of these perturbations\nreveals that there exist crucial weaknesses in Im2Im models. Lastly, we show\nthat our methods illustrate how perturbations affect the quality of outputs,\npioneering the improvement of the robustness of current SOTA networks for\nautonomous driving.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 11:51:04 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Lin", ""], ["Cho", "Wonjune", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "2001.01526", "submitter": "Yixiao Ge", "authors": "Yixiao Ge, Dapeng Chen, Hongsheng Li", "title": "Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain\n  Adaptation on Person Re-identification", "comments": "Accepted in International Conference on Learning Representations\n  (ICLR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) aims at identifying the same persons' images\nacross different cameras. However, domain diversities between different\ndatasets pose an evident challenge for adapting the re-ID model trained on one\ndataset to another one. State-of-the-art unsupervised domain adaptation methods\nfor person re-ID transferred the learned knowledge from the source domain by\noptimizing with pseudo labels created by clustering algorithms on the target\ndomain. Although they achieved state-of-the-art performances, the inevitable\nlabel noise caused by the clustering procedure was ignored. Such noisy pseudo\nlabels substantially hinders the model's capability on further improving\nfeature representations on the target domain. In order to mitigate the effects\nof noisy pseudo labels, we propose to softly refine the pseudo labels in the\ntarget domain by proposing an unsupervised framework, Mutual Mean-Teaching\n(MMT), to learn better features from the target domain via off-line refined\nhard pseudo labels and on-line refined soft pseudo labels in an alternative\ntraining manner. In addition, the common practice is to adopt both the\nclassification loss and the triplet loss jointly for achieving optimal\nperformances in person re-ID models. However, conventional triplet loss cannot\nwork with softly refined labels. To solve this problem, a novel soft\nsoftmax-triplet loss is proposed to support learning with soft pseudo triplet\nlabels for achieving the optimal domain adaptation performance. The proposed\nMMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and\n16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT\nunsupervised domain adaptation tasks. Code is available at\nhttps://github.com/yxgeee/MMT.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 12:42:58 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 06:37:43 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Ge", "Yixiao", ""], ["Chen", "Dapeng", ""], ["Li", "Hongsheng", ""]]}, {"id": "2001.01536", "submitter": "Liuyu Xiang", "authors": "Liuyu Xiang, Guiguang Ding and Jungong Han", "title": "Learning From Multiple Experts: Self-paced Knowledge Distillation for\n  Long-tailed Classification", "comments": "ECCV 2020 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world scenarios, data tends to exhibit a long-tailed distribution,\nwhich increases the difficulty of training deep networks. In this paper, we\npropose a novel self-paced knowledge distillation framework, termed Learning\nFrom Multiple Experts (LFME). Our method is inspired by the observation that\nnetworks trained on less imbalanced subsets of the distribution often yield\nbetter performances than their jointly-trained counterparts. We refer to these\nmodels as 'Experts', and the proposed LFME framework aggregates the knowledge\nfrom multiple 'Experts' to learn a unified student model. Specifically, the\nproposed framework involves two levels of adaptive learning schedules:\nSelf-paced Expert Selection and Curriculum Instance Selection, so that the\nknowledge is adaptively transferred to the 'Student'. We conduct extensive\nexperiments and demonstrate that our method is able to achieve superior\nperformances compared to state-of-the-art methods. We also show that our method\ncan be easily plugged into state-of-the-art long-tailed classification\nalgorithms for further improvements.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 12:57:36 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 05:21:56 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 02:44:16 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Xiang", "Liuyu", ""], ["Ding", "Guiguang", ""], ["Han", "Jungong", ""]]}, {"id": "2001.01547", "submitter": "Wei He", "authors": "Wei He, Yong Chen, Naoto Yokoya, Chao Li, Qibin Zhao", "title": "Hyperspectral Super-Resolution via Coupled Tensor Ring Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral super-resolution (HSR) fuses a low-resolution hyperspectral\nimage (HSI) and a high-resolution multispectral image (MSI) to obtain a\nhigh-resolution HSI (HR-HSI). In this paper, we propose a new model, named\ncoupled tensor ring factorization (CTRF), for HSR. The proposed CTRF approach\nsimultaneously learns high spectral resolution core tensor from the HSI and\nhigh spatial resolution core tensors from the MSI, and reconstructs the HR-HSI\nvia tensor ring (TR) representation (Figure~\\ref{fig:framework}). The CTRF\nmodel can separately exploit the low-rank property of each class (Section\n\\ref{sec:analysis}), which has been never explored in the previous coupled\ntensor model. Meanwhile, it inherits the simple representation of coupled\nmatrix/CP factorization and flexible low-rank exploration of coupled Tucker\nfactorization.\n  Guided by Theorem~\\ref{th:1}, we further propose a spectral nuclear norm\nregularization to explore the global spectral low-rank property.\n  The experiments have demonstrated the advantage of the proposed nuclear norm\nregularized CTRF (NCTRF) as compared to previous matrix/tensor and deep\nlearning methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 13:19:59 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["He", "Wei", ""], ["Chen", "Yong", ""], ["Yokoya", "Naoto", ""], ["Li", "Chao", ""], ["Zhao", "Qibin", ""]]}, {"id": "2001.01550", "submitter": "Shenda Hong", "authors": "Shenda Hong, Yuxi Zhou, Junyuan Shang, Cao Xiao, Jimeng Sun", "title": "Opportunities and Challenges of Deep Learning Methods for\n  Electrocardiogram Data: A Systematic Review", "comments": "Accepted by Computers in Biology and Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background:The electrocardiogram (ECG) is one of the most commonly used\ndiagnostic tools in medicine and healthcare. Deep learning methods have\nachieved promising results on predictive healthcare tasks using ECG signals.\nObjective:This paper presents a systematic review of deep learning methods for\nECG data from both modeling and application perspectives. Methods:We extracted\npapers that applied deep learning (deep neural network) models to ECG data that\nwere published between Jan. 1st of 2010 and Feb. 29th of 2020 from Google\nScholar, PubMed, and the DBLP. We then analyzed each article according to three\nfactors: tasks, models, and data. Finally, we discuss open challenges and\nunsolved problems in this area. Results: The total number of papers extracted\nwas 191. Among these papers, 108 were published after 2019. Different deep\nlearning architectures have been used in various ECG analytics tasks, such as\ndisease detection/classification, annotation/localization, sleep staging,\nbiometric human identification, and denoising. Conclusion: The number of works\non deep learning for ECG data has grown explosively in recent years. Such works\nhave achieved accuracy comparable to that of traditional feature-based\napproaches and ensembles of multiple approaches can achieve even better\nresults. Specifically, we found that a hybrid architecture of a convolutional\nneural network and recurrent neural network ensemble using expert features\nyields the best results. However, there are some new challenges and problems\nrelated to interpretability, scalability, and efficiency that must be\naddressed. Furthermore, it is also worth investigating new applications from\nthe perspectives of datasets and methods. Significance: This paper summarizes\nexisting deep learning research using ECG data from multiple perspectives and\nhighlights existing challenges and problems to identify potential future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 02:44:29 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 19:01:55 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 18:42:49 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Hong", "Shenda", ""], ["Zhou", "Yuxi", ""], ["Shang", "Junyuan", ""], ["Xiao", "Cao", ""], ["Sun", "Jimeng", ""]]}, {"id": "2001.01599", "submitter": "Noriaki Hashimoto", "authors": "Noriaki Hashimoto, Daisuke Fukushima, Ryoichi Koga, Yusuke Takagi,\n  Kaho Ko, Kei Kohno, Masato Nakaguro, Shigeo Nakamura, Hidekata Hontani and\n  Ichiro Takeuchi", "title": "Multi-scale Domain-adversarial Multiple-instance CNN for Cancer Subtype\n  Classification with Unannotated Histopathological Images", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for cancer subtype classification from\nhistopathological images, which can automatically detect tumor-specific\nfeatures in a given whole slide image (WSI). The cancer subtype should be\nclassified by referring to a WSI, i.e., a large-sized image (typically\n40,000x40,000 pixels) of an entire pathological tissue slide, which consists of\ncancer and non-cancer portions. One difficulty arises from the high cost\nassociated with annotating tumor regions in WSIs. Furthermore, both global and\nlocal image features must be extracted from the WSI by changing the\nmagnifications of the image. In addition, the image features should be stably\ndetected against the differences of staining conditions among the\nhospitals/specimens. In this paper, we develop a new CNN-based cancer subtype\nclassification method by effectively combining multiple-instance, domain\nadversarial, and multi-scale learning frameworks in order to overcome these\npractical difficulties. When the proposed method was applied to malignant\nlymphoma subtype classifications of 196 cases collected from multiple\nhospitals, the classification performance was significantly better than the\nstandard CNN or other conventional methods, and the accuracy compared favorably\nwith that of standard pathologists.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 14:09:51 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 08:03:24 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Hashimoto", "Noriaki", ""], ["Fukushima", "Daisuke", ""], ["Koga", "Ryoichi", ""], ["Takagi", "Yusuke", ""], ["Ko", "Kaho", ""], ["Kohno", "Kei", ""], ["Nakaguro", "Masato", ""], ["Nakamura", "Shigeo", ""], ["Hontani", "Hidekata", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "2001.01600", "submitter": "Hongguang Zhang", "authors": "Hongguang Zhang, Philip H. S. Torr, Piotr Koniusz", "title": "Few-shot Learning with Multi-scale Self-supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning concepts from the limited number of datapoints is a challenging task\nusually addressed by the so-called one- or few-shot learning. Recently, an\napplication of second-order pooling in few-shot learning demonstrated its\nsuperior performance due to the aggregation step handling varying image\nresolutions without the need of modifying CNNs to fit to specific image sizes,\nyet capturing highly descriptive co-occurrences. However, using a single\nresolution per image (even if the resolution varies across a dataset) is\nsuboptimal as the importance of image contents varies across the coarse-to-fine\nlevels depending on the object and its class label e. g., generic objects and\nscenes rely on their global appearance while fine-grained objects rely more on\ntheir localized texture patterns. Multi-scale representations are popular in\nimage deblurring, super-resolution and image recognition but they have not been\ninvestigated in few-shot learning due to its relational nature complicating the\nuse of standard techniques. In this paper, we propose a novel multi-scale\nrelation network based on the properties of second-order pooling to estimate\nimage relations in few-shot setting. To optimize the model, we leverage a scale\nselector to re-weight scale-wise representations based on their second-order\nfeatures. Furthermore, we propose to a apply self-supervised scale prediction.\nSpecifically, we leverage an extra discriminator to predict the scale labels\nand the scale discrepancy between pairs of images. Our model achieves\nstate-of-the-art results on standard few-shot learning datasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 14:10:20 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zhang", "Hongguang", ""], ["Torr", "Philip H. S.", ""], ["Koniusz", "Piotr", ""]]}, {"id": "2001.01613", "submitter": "Nadine Rueegg", "authors": "Nadine Rueegg, Christoph Lassner, Michael J. Black, Konrad Schindler", "title": "Chained Representation Cycling: Learning to Estimate 3D Human Pose and\n  Shape by Cycling Between Representations", "comments": "To be published in proceedings of Thirty-Fourth AAAI Conference on\n  Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of many computer vision systems is to transform image pixels into 3D\nrepresentations. Recent popular models use neural networks to regress directly\nfrom pixels to 3D object parameters. Such an approach works well when\nsupervision is available, but in problems like human pose and shape estimation,\nit is difficult to obtain natural images with 3D ground truth. To go one step\nfurther, we propose a new architecture that facilitates unsupervised, or\nlightly supervised, learning. The idea is to break the problem into a series of\ntransformations between increasingly abstract representations. Each step\ninvolves a cycle designed to be learnable without annotated training data, and\nthe chain of cycles delivers the final solution. Specifically, we use 2D body\npart segments as an intermediate representation that contains enough\ninformation to be lifted to 3D, and at the same time is simple enough to be\nlearned in an unsupervised way. We demonstrate the method by learning 3D human\npose and shape from un-paired and un-annotated images. We also explore varying\namounts of paired data and show that cycling greatly alleviates the need for\npaired data. While we present results for modeling humans, our formulation is\ngeneral and can be applied to other vision problems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 14:54:00 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Rueegg", "Nadine", ""], ["Lassner", "Christoph", ""], ["Black", "Michael J.", ""], ["Schindler", "Konrad", ""]]}, {"id": "2001.01629", "submitter": "Sida Peng", "authors": "Sida Peng, Wen Jiang, Huaijin Pi, Xiuli Li, Hujun Bao, Xiaowei Zhou", "title": "Deep Snake for Real-Time Instance Segmentation", "comments": "Accepted to CVPR 2020 as Oral. Add experiments on MS COCO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel contour-based approach named deep snake for\nreal-time instance segmentation. Unlike some recent methods that directly\nregress the coordinates of the object boundary points from an image, deep snake\nuses a neural network to iteratively deform an initial contour to match the\nobject boundary, which implements the classic idea of snake algorithms with a\nlearning-based approach. For structured feature learning on the contour, we\npropose to use circular convolution in deep snake, which better exploits the\ncycle-graph structure of a contour compared against generic graph convolution.\nBased on deep snake, we develop a two-stage pipeline for instance segmentation:\ninitial contour proposal and contour deformation, which can handle errors in\nobject localization. Experiments show that the proposed approach achieves\ncompetitive performances on the Cityscapes, KINS, SBD and COCO datasets while\nbeing efficient for real-time applications with a speed of 32.3 fps for\n512$\\times$512 images on a 1080Ti GPU. The code is available at\nhttps://github.com/zju3dv/snake/.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 15:29:06 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 14:36:59 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 15:01:53 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Peng", "Sida", ""], ["Jiang", "Wen", ""], ["Pi", "Huaijin", ""], ["Li", "Xiuli", ""], ["Bao", "Hujun", ""], ["Zhou", "Xiaowei", ""]]}, {"id": "2001.01661", "submitter": "Konstantinos Semertzidis", "authors": "Konstantinos Semertzidis, Evaggelia Pitoura", "title": "A Hybrid Approach to Temporal Pattern Matching", "comments": "4 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary objective of graph pattern matching is to find all appearances of\nan input graph pattern query in a large data graph. Such appearances are called\nmatches. In this paper, we are interested in finding matches of interaction\npatterns in temporal graphs. To this end, we propose a hybrid approach that\nachieves effective filtering of potential matches based both on structure and\ntime. Our approach exploits a graph representation where edges are ordered by\ntime. We present experiments with real datasets that illustrate the efficiency\nof our approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 16:52:44 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 11:22:10 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Semertzidis", "Konstantinos", ""], ["Pitoura", "Evaggelia", ""]]}, {"id": "2001.01686", "submitter": "Omolbanin Yazdanbakhsh", "authors": "Omolbanin Yazdanbakhsh and Scott Dick", "title": "A Deep Neuro-Fuzzy Network for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of neural network and fuzzy systems into neuro-fuzzy systems\nintegrates fuzzy reasoning rules into the connectionist networks. However, the\nexisting neuro-fuzzy systems are developed under shallow structures having\nlower generalization capacity. We propose the first end-to-end deep neuro-fuzzy\nnetwork and investigate its application for image classification. Two new\noperations are developed based on definitions of Takagi-Sugeno-Kang (TSK) fuzzy\nmodel namely fuzzy inference operation and fuzzy pooling operations; stacks of\nthese operations comprise the layers in this network. We evaluate the network\non MNIST, CIFAR-10 and CIFAR-100 datasets, finding that the network has a\nreasonable accuracy in these benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 03:28:05 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Yazdanbakhsh", "Omolbanin", ""], ["Dick", "Scott", ""]]}, {"id": "2001.01744", "submitter": "Abhishek Badki", "authors": "Abhishek Badki, Orazio Gallo, Jan Kautz, and Pradeep Sen", "title": "Meshlet Priors for 3D Mesh Reconstruction", "comments": "To be presented at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a mesh from an unordered set of sparse, noisy 3D points is a\nchallenging problem that requires carefully selected priors. Existing\nhand-crafted priors, such as smoothness regularizers, impose an undesirable\ntrade-off between attenuating noise and preserving local detail. Recent\ndeep-learning approaches produce impressive results by learning priors directly\nfrom the data. However, the priors are learned at the object level, which makes\nthese algorithms class-specific and even sensitive to the pose of the object.\nWe introduce meshlets, small patches of mesh that we use to learn local shape\npriors. Meshlets act as a dictionary of local features and thus allow to use\nlearned priors to reconstruct object meshes in any pose and from unseen\nclasses, even when the noise is large and the samples sparse.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 19:13:40 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 18:58:52 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Badki", "Abhishek", ""], ["Gallo", "Orazio", ""], ["Kautz", "Jan", ""], ["Sen", "Pradeep", ""]]}, {"id": "2001.01786", "submitter": "Richard Wang", "authors": "Usman Sajid and Guanghui Wang", "title": "Plug-and-Play Rescaling Based Crowd Counting in Static Images", "comments": "10 pages, 6 figures, WACV conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is a challenging problem especially in the presence of huge\ncrowd diversity across images and complex cluttered crowd-like background\nregions, where most previous approaches do not generalize well and consequently\nproduce either huge crowd underestimation or overestimation. To address these\nchallenges, we propose a new image patch rescaling module (PRM) and three\nindependent PRM employed crowd counting methods. The proposed frameworks use\nthe PRM module to rescale the image regions (patches) that require special\ntreatment, whereas the classification process helps in recognizing and\ndiscarding any cluttered crowd-like background regions which may result in\noverestimation. Experiments on three standard benchmarks and cross-dataset\nevaluation show that our approach outperforms the state-of-the-art models in\nthe RMSE evaluation metric with an improvement up to 10.4%, and possesses\nsuperior generalization ability to new datasets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 21:43:25 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Sajid", "Usman", ""], ["Wang", "Guanghui", ""]]}, {"id": "2001.01788", "submitter": "Yiming Qian", "authors": "James H. Elder, Emilio J. Almaz\\`an, Yiming Qian and Ron Tal", "title": "MCMLSD: A Probabilistic Algorithm and Evaluation Framework for Line\n  Segment Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to line segment detection typically involve perceptual\ngrouping in the image domain and/or global accumulation in the Hough domain.\nHere we propose a probabilistic algorithm that merges the advantages of both\napproaches. In a first stage lines are detected using a global probabilistic\nHough approach. In the second stage each detected line is analyzed in the image\ndomain to localize the line segments that generated the peak in the Hough map.\nBy limiting search to a line, the distribution of segments over the sequence of\npoints on the line can be modeled as a Markov chain, and a probabilistically\noptimal labelling can be computed exactly using a standard dynamic programming\nalgorithm, in linear time. The Markov assumption also leads to an intuitive\nranking method that uses the local marginal posterior probabilities to estimate\nthe expected number of correctly labelled points on a segment. To assess the\nresulting Markov Chain Marginal Line Segment Detector (MCMLSD) we develop and\napply a novel quantitative evaluation methodology that controls for under- and\nover-segmentation. Evaluation on the YorkUrbanDB and Wireframe datasets shows\nthat the proposed MCMLSD method outperforms prior traditional approaches, as\nwell as more recent deep learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 21:51:45 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Elder", "James H.", ""], ["Almaz\u00e0n", "Emilio J.", ""], ["Qian", "Yiming", ""], ["Tal", "Ron", ""]]}, {"id": "2001.01802", "submitter": "Pablo Arias", "authors": "Thibaud Ehret, Pablo Arias", "title": "Implementation of the VBM3D Video Denoising Method and Some Variants", "comments": "18 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VBM3D is an extension to video of the well known image denoising algorithm\nBM3D, which takes advantage of the sparse representation of stacks of similar\npatches in a transform domain. The extension is rather straightforward: the\nsimilar 2D patches are taken from a spatio-temporal neighborhood which includes\nneighboring frames. In spite of its simplicity, the algorithm offers a good\ntrade-off between denoising performance and computational complexity. In this\nwork we revisit this method, providing an open-source C++ implementation\nreproducing the results. A detailed description is given and the choice of\nparameters is thoroughly discussed. Furthermore, we discuss several extensions\nof the original algorithm: (1) a multi-scale implementation, (2) the use of 3D\npatches, (3) the use of optical flow to guide the patch search. These\nextensions allow to obtain results which are competitive with even the most\nrecent state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 22:37:37 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Ehret", "Thibaud", ""], ["Arias", "Pablo", ""]]}, {"id": "2001.01815", "submitter": "Peng Liu", "authors": "Peng Liu and Ruogu Fang", "title": "Regression and Learning with Pixel-wise Attention for Retinal Fundus\n  Glaucoma Segmentation and Detection", "comments": "MICCAI 2018 workshop challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing retinal fundus images by an ophthalmologist is a major diagnosis\napproach for glaucoma. However, it is still difficult to distinguish the\nfeatures of the lesion solely through manual observations, especially, in\nglaucoma early phase. In this paper, we present two deep learning-based\nautomated algorithms for glaucoma detection and optic disc and cup\nsegmentation. We utilize the attention mechanism to learn pixel-wise features\nfor accurate prediction. In particular, we present two convolutional neural\nnetworks that can focus on learning various pixel-wise level features. In\naddition, we develop several attention strategies to guide the networks to\nlearn the important features that have a major impact on prediction accuracy.\nWe evaluate our methods on the validation dataset and The proposed both tasks'\nsolutions can achieve impressive results and outperform current\nstate-of-the-art methods. \\textit{The code is available at\n\\url{https://github.com/cswin/RLPA}}.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 23:54:27 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Liu", "Peng", ""], ["Fang", "Ruogu", ""]]}, {"id": "2001.01816", "submitter": "Alan Wagner", "authors": "Kasra Mokhtari and Alan R. Wagner", "title": "The Pedestrian Patterns Dataset", "comments": "Accepted in AAAI Fall Symposium 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the pedestrian patterns dataset for autonomous driving. The\ndataset was collected by repeatedly traversing the same three routes for one\nweek starting at different specific timeslots. The purpose of the dataset is to\ncapture the patterns of social and pedestrian behavior along the traversed\nroutes at different times and to eventually use this information to make\npredictions about the risk associated with autonomously traveling along\ndifferent routes. This dataset contains the Full HD videos and GPS data for\neach traversal. Fast R-CNN pedestrian detection method is applied to the\ncaptured videos to count the number of pedestrians at each video frame in order\nto assess the density of pedestrians along a route. By providing this\nlarge-scale dataset to researchers, we hope to accelerate autonomous driving\nresearch not only to estimate the risk, both to the public and to the\nautonomous vehicle but also accelerate research on long-term vision-based\nlocalization of mobile robots and autonomous vehicles of the future.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 23:58:39 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Mokhtari", "Kasra", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2001.01869", "submitter": "Chen Song", "authors": "Chen Song, Jiaru Song, Qixing Huang", "title": "HybridPose: 6D Object Pose Estimation under Hybrid Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HybridPose, a novel 6D object pose estimation approach.\nHybridPose utilizes a hybrid intermediate representation to express different\ngeometric information in the input image, including keypoints, edge vectors,\nand symmetry correspondences. Compared to a unitary representation, our hybrid\nrepresentation allows pose regression to exploit more and diverse features when\none type of predicted representation is inaccurate (e.g., because of\nocclusion). Different intermediate representations used by HybridPose can all\nbe predicted by the same simple neural network, and outliers in predicted\nintermediate representations are filtered by a robust regression module.\nCompared to state-of-the-art pose estimation approaches, HybridPose is\ncomparable in running time and accuracy. For example, on Occlusion Linemod\ndataset, our method achieves a prediction speed of 30 fps with a mean ADD(-S)\naccuracy of 47.5%, representing a state-of-the-art performance. The\nimplementation of HybridPose is available at\nhttps://github.com/chensong1995/HybridPose.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 03:08:27 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 19:01:22 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 17:31:18 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2020 04:34:15 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Song", "Chen", ""], ["Song", "Jiaru", ""], ["Huang", "Qixing", ""]]}, {"id": "2001.01870", "submitter": "Haodi Hou", "authors": "Haodi Hou, Jing Huo, Jing Wu, Yu-Kun Lai, and Yang Gao", "title": "MW-GAN: Multi-Warping GAN for Caricature Generation with Multi-Style\n  Geometric Exaggeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an input face photo, the goal of caricature generation is to produce\nstylized, exaggerated caricatures that share the same identity as the photo. It\nrequires simultaneous style transfer and shape exaggeration with rich\ndiversity, and meanwhile preserving the identity of the input. To address this\nchallenging problem, we propose a novel framework called Multi-Warping GAN\n(MW-GAN), including a style network and a geometric network that are designed\nto conduct style transfer and geometric exaggeration respectively. We bridge\nthe gap between the style and landmarks of an image with corresponding latent\ncode spaces by a dual way design, so as to generate caricatures with arbitrary\nstyles and geometric exaggeration, which can be specified either through random\nsampling of latent code or from a given caricature sample. Besides, we apply\nidentity preserving loss to both image space and landmark space, leading to a\ngreat improvement in quality of generated caricatures. Experiments show that\ncaricatures generated by MW-GAN have better quality than existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 03:08:30 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Hou", "Haodi", ""], ["Huo", "Jing", ""], ["Wu", "Jing", ""], ["Lai", "Yu-Kun", ""], ["Gao", "Yang", ""]]}, {"id": "2001.01886", "submitter": "Chunhua Shen", "authors": "Haipeng Xiong, Hao Lu, Chengxin Liu, Liang Liu, Chunhua Shen, Zhiguo\n  Cao", "title": "From Open Set to Closed Set: Supervised Spatial Divide-and-Conquer for\n  Object Counting", "comments": "Extended version of arXiv:1908.06473", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual counting, a task that aims to estimate the number of objects from an\nimage/video, is an open-set problem by nature, i.e., the number of population\ncan vary in [0, inf) in theory. However, collected data and labeled instances\nare limited in reality, which means that only a small closed set is observed.\nExisting methods typically model this task in a regression manner, while they\nare prone to suffer from an unseen scene with counts out of the scope of the\nclosed set. In fact, counting has an interesting and exclusive\nproperty---spatially decomposable. A dense region can always be divided until\nsub-region counts are within the previously observed closed set. We therefore\nintroduce the idea of spatial divide-and-conquer (S-DC) that transforms\nopen-set counting into a closed-set problem. This idea is implemented by a\nnovel Supervised Spatial Divide-and-Conquer Network (SS-DCNet). Thus, SS-DCNet\ncan only learn from a closed set but generalize well to open-set scenarios via\nS-DC. SS-DCNet is also efficient. To avoid repeatedly computing sub-region\nconvolutional features, S-DC is executed on the feature map instead of on the\ninput image. We provide theoretical analyses as well as a controlled experiment\non toy data, demonstrating why closed-set modeling makes sense. Extensive\nexperiments show that SS-DCNet achieves the state-of-the-art performance. Code\nand models are available at: https://tinyurl.com/SS-DCNet.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 04:36:53 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 08:59:29 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Xiong", "Haipeng", ""], ["Lu", "Hao", ""], ["Liu", "Chengxin", ""], ["Liu", "Liang", ""], ["Shen", "Chunhua", ""], ["Cao", "Zhiguo", ""]]}, {"id": "2001.01893", "submitter": "Toshimitsu Aritake", "authors": "Toshimitsu Aritake, Hideitsu Hino, Shigeyuki Namiki, Daisuke Asanuma,\n  Kenzo Hirose, Noboru Murata", "title": "Fast and robust multiplane single molecule localization microscopy using\n  deep neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single molecule localization microscopy is widely used in biological research\nfor measuring the nanostructures of samples smaller than the diffraction limit.\nThis study uses multifocal plane microscopy and addresses the 3D single\nmolecule localization problem, where lateral and axial locations of molecules\nare estimated. However, when we multifocal plane microscopy is used, the\nestimation accuracy of 3D localization is easily deteriorated by the small\nlateral drifts of camera positions. We formulate a 3D molecule localization\nproblem along with the estimation of the lateral drifts as a compressed sensing\nproblem, A deep neural network was applied to accurately and efficiently solve\nthis problem. The proposed method is robust to the lateral drifts and achieves\nan accuracy of 20 nm laterally and 50 nm axially without an explicit drift\ncorrection.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 05:12:14 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Aritake", "Toshimitsu", ""], ["Hino", "Hideitsu", ""], ["Namiki", "Shigeyuki", ""], ["Asanuma", "Daisuke", ""], ["Hirose", "Kenzo", ""], ["Murata", "Noboru", ""]]}, {"id": "2001.01912", "submitter": "Stephen Lee Huei Lau", "authors": "Stephen L. H. Lau, Edwin K. P. Chong, Xu Yang, and Xin Wang", "title": "Automated Pavement Crack Segmentation Using U-Net-based Convolutional\n  Neural Network", "comments": "Accepted for publication in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3003638", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated pavement crack image segmentation is challenging because of\ninherent irregular patterns, lighting conditions, and noise in images.\nConventional approaches require a substantial amount of feature engineering to\ndifferentiate crack regions from non-affected regions. In this paper, we\npropose a deep learning technique based on a convolutional neural network to\nperform segmentation tasks on pavement crack images. Our approach requires\nminimal feature engineering compared to other machine learning techniques. We\npropose a U-Net-based network architecture in which we replace the encoder with\na pretrained ResNet-34 neural network. We use a \"one-cycle\" training schedule\nbased on cyclical learning rates to speed up the convergence. Our method\nachieves an F1 score of 96% on the CFD dataset and 73% on the Crack500 dataset,\noutperforming other algorithms tested on these datasets. We perform ablation\nstudies on various techniques that helped us get marginal performance boosts,\ni.e., the addition of spatial and channel squeeze and excitation (SCSE)\nmodules, training with gradually increasing image sizes, and training various\nneural network layers with different learning rates.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 07:18:13 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 04:56:41 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 07:10:26 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 14:43:42 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Lau", "Stephen L. H.", ""], ["Chong", "Edwin K. P.", ""], ["Yang", "Xu", ""], ["Wang", "Xin", ""]]}, {"id": "2001.01921", "submitter": "Borja Bovcon", "authors": "Borja Bovcon and Matej Kristan", "title": "A water-obstacle separation and refinement network for unmanned surface\n  vehicles", "comments": "6 pages + references, 6 figures, submitted to ICRA2020. MODD2 and\n  MaSTr1325 datasets are available at\n  http://box.vicos.si/borja/viamaro/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obstacle detection by semantic segmentation shows a great promise for\nautonomous navigation in unmanned surface vehicles (USV). However, existing\nmethods suffer from poor estimation of the water edge in the presence of visual\nambiguities, poor detection of small obstacles and high false-positive rate on\nwater reflections and wakes. We propose a new deep encoder-decoder\narchitecture, a water-obstacle separation and refinement network (WaSR), to\naddress these issues. Detection and water edge accuracy are improved by a novel\ndecoder that gradually fuses inertial information from IMU with the visual\nfeatures from the encoder. In addition, a novel loss function is designed to\nincrease the separation between water and obstacle features early on in the\nnetwork. Subsequently, the capacity of the remaining layers in the decoder is\nbetter utilised, leading to a significant reduction in false positives and\nincreased true positives. Experimental results show that WaSR outperforms the\ncurrent state-of-the-art by a large margin, yielding a 14% increase in\nF-measure over the second-best method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 07:47:52 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Bovcon", "Borja", ""], ["Kristan", "Matej", ""]]}, {"id": "2001.01953", "submitter": "Faisal Ghaffar", "authors": "Faisal Ghaffar, Sarwar Khan, Bunyarit Uyyanonvara, Chanjira\n  Sinthanayothin and Hirohiko Kaneko", "title": "Detection of Diabetic Anomalies in Retinal Images using Morphological\n  Cascading Decision Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research aims to develop an efficient system for screening of diabetic\nretinopathy. Diabetic retinopathy is the major cause of blindness. Severity of\ndiabetic retinopathy is recognized by some features, such as blood vessel area,\nexudates, haemorrhages and microaneurysms. To grade the disease the screening\nsystem must efficiently detect these features. In this paper we are proposing a\nsimple and fast method for detection of diabetic retinopathy. We do\npre-processing of grey-scale image and find all labelled connected components\n(blobs) in an image regardless of whether it is haemorrhages, exudates,\nvessels, optic disc or anything else. Then we apply some constraints such as\ncompactness, area of blob, intensity and contrast for screening of candidate\nconnectedcomponent responsible for diabetic retinopathy. We obtain our final\nresults by doing some post processing. The results are compared with ground\ntruths. Performance is measured by finding the recall (sensitivity). We took 10\nimages of dimension 500 * 752. The mean recall is 90.03%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 10:20:11 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Ghaffar", "Faisal", ""], ["Khan", "Sarwar", ""], ["Uyyanonvara", "Bunyarit", ""], ["Sinthanayothin", "Chanjira", ""], ["Kaneko", "Hirohiko", ""]]}, {"id": "2001.02001", "submitter": "Firat Ozdemir", "authors": "Firat Ozdemir, Christine Tanner, Orcun Goksel", "title": "Delineating Bone Surfaces in B-Mode Images Constrained by Physics of\n  Ultrasound Propagation", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bone surface delineation in ultrasound is of interest due to its potential in\ndiagnosis, surgical planning, and post-operative follow-up in orthopedics, as\nwell as the potential of using bones as anatomical landmarks in surgical\nnavigation. We herein propose a method to encode the physics of ultrasound\npropagation into a factor graph formulation for the purpose of bone surface\ndelineation. In this graph structure, unary node potentials encode the local\nlikelihood for being a soft tissue or acoustic-shadow (behind bone surface)\nregion, both learned through image descriptors. Pair-wise edge potentials\nencode ultrasound propagation constraints of bone surfaces given their large\nacoustic-impedance difference. We evaluate the proposed method in comparison\nwith four earlier approaches, on in-vivo ultrasound images collected from\ndorsal and volar views of the forearm. The proposed method achieves an average\nroot-mean-square error and symmetric Hausdorff distance of 0.28mm and 1.78mm,\nrespectively. It detects 99.9% of the annotated bone surfaces with a mean\nscanline error (distance to annotations) of 0.39mm.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 12:34:42 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Ozdemir", "Firat", ""], ["Tanner", "Christine", ""], ["Goksel", "Orcun", ""]]}, {"id": "2001.02024", "submitter": "Erik G\\\"artner", "authors": "Erik G\\\"artner, Aleksis Pirinen, Cristian Sminchisescu", "title": "Deep Reinforcement Learning for Active Human Pose Estimation", "comments": "Accepted to The Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI-20). Submission updated to include supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most 3d human pose estimation methods assume that input -- be it images of a\nscene collected from one or several viewpoints, or from a video -- is given.\nConsequently, they focus on estimates leveraging prior knowledge and\nmeasurement by fusing information spatially and/or temporally, whenever\navailable. In this paper we address the problem of an active observer with\nfreedom to move and explore the scene spatially -- in `time-freeze' mode --\nand/or temporally, by selecting informative viewpoints that improve its\nestimation accuracy. Towards this end, we introduce Pose-DRL, a fully trainable\ndeep reinforcement learning-based active pose estimation architecture which\nlearns to select appropriate views, in space and time, to feed an underlying\nmonocular pose estimator. We evaluate our model using single- and multi-target\nestimators with strong result in both settings. Our system further learns\nautomatic stopping conditions in time and transition functions to the next\ntemporal processing step in videos. In extensive experiments with the Panoptic\nmulti-view setup, and for complex scenes containing multiple people, we show\nthat our model learns to select viewpoints that yield significantly more\naccurate pose estimates compared to strong multi-view baselines.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 13:35:41 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 10:23:42 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["G\u00e4rtner", "Erik", ""], ["Pirinen", "Aleksis", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "2001.02040", "submitter": "Ali Hatamizadeh", "authors": "Andriy Myronenko and Ali Hatamizadeh", "title": "Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs", "comments": "Accepted to 2019 International MICCAI Brainlesion Workshop --\n  Multimodal Brain Tumor Segmentation Challenge (BraTS) 2019. arXiv admin note:\n  substantial text overlap with arXiv:1810.11654", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal brain tumor segmentation challenge (BraTS) brings together\nresearchers to improve automated methods for 3D MRI brain tumor segmentation.\nTumor segmentation is one of the fundamental vision tasks necessary for\ndiagnosis and treatment planning of the disease. Previous years winning methods\nwere all deep-learning based, thanks to the advent of modern GPUs, which allow\nfast optimization of deep convolutional neural network architectures. In this\nwork, we explore best practices of 3D semantic segmentation, including\nconventional encoder-decoder architecture, as well combined loss functions, in\nattempt to further improve the segmentation accuracy. We evaluate the method on\nBraTS 2019 challenge.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 07:47:42 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Myronenko", "Andriy", ""], ["Hatamizadeh", "Ali", ""]]}, {"id": "2001.02090", "submitter": "Joosung Lee", "authors": "Joosung Lee, Sangwon Hwang, Kyungjae Lee, Woo Jin Kim, Junhyeop Lee,\n  Tae-young Chung, Sangyoun Lee", "title": "AD-VO: Scale-Resilient Visual Odometry Using Attentive Disparity Map", "comments": "5 pages, 5 figures, 2018.02 papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual odometry is an essential key for a localization module in SLAM\nsystems. However, previous methods require tuning the system to adapt\nenvironment changes. In this paper, we propose a learning-based approach for\nframe-to-frame monocular visual odometry estimation. The proposed network is\nonly learned by disparity maps for not only covering the environment changes\nbut also solving the scale problem. Furthermore, attention block and\nskip-ordering scheme are introduced to achieve robust performance in various\ndriving environment. Our network is compared with the conventional methods\nwhich use common domain such as color or optical flow. Experimental results\nconfirm that the proposed network shows better performance than other\napproaches with higher and more stable results.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 15:01:57 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Lee", "Joosung", ""], ["Hwang", "Sangwon", ""], ["Lee", "Kyungjae", ""], ["Kim", "Woo Jin", ""], ["Lee", "Junhyeop", ""], ["Chung", "Tae-young", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2001.02101", "submitter": "Homayoun Valafar", "authors": "Chrisogonas O. Odhiambo, Casey A. Cole, Alaleh Torkjazi, Homayoun\n  Valafar", "title": "State Transition Modeling of the Smoking Behavior using LSTM Recurrent\n  Neural Networks", "comments": "8 pages, CSCI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of sensors has pervaded everyday life in several applications\nincluding human activity monitoring, healthcare, and social networks. In this\nstudy, we focus on the use of smartwatch sensors to recognize smoking activity.\nMore specifically, we have reformulated the previous work in detection of\nsmoking to include in-context recognition of smoking. Our presented\nreformulation of the smoking gesture as a state-transition model that consists\nof the mini-gestures hand-to-lip, hand-on-lip, and hand-off-lip, has\ndemonstrated improvement in detection rates nearing 100% using conventional\nneural networks. In addition, we have begun the utilization of Long-Short-Term\nMemory (LSTM) neural networks to allow for in-context detection of gestures\nwith accuracy nearing 97%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 15:06:28 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Odhiambo", "Chrisogonas O.", ""], ["Cole", "Casey A.", ""], ["Torkjazi", "Alaleh", ""], ["Valafar", "Homayoun", ""]]}, {"id": "2001.02129", "submitter": "Longguang Wang", "authors": "Longguang Wang, Yulan Guo, Li Liu, Zaiping Lin, Xinpu Deng and Wei An", "title": "Deep Video Super-Resolution using HR Optical Flow Estimation", "comments": "Accepted by IEEE TIP. arXiv admin note: text overlap with\n  arXiv:1809.08573", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (SR) aims at generating a sequence of high-resolution\n(HR) frames with plausible and temporally consistent details from their\nlow-resolution (LR) counterparts. The key challenge for video SR lies in the\neffective exploitation of temporal dependency between consecutive frames.\nExisting deep learning based methods commonly estimate optical flows between LR\nframes to provide temporal dependency. However, the resolution conflict between\nLR optical flows and HR outputs hinders the recovery of fine details. In this\npaper, we propose an end-to-end video SR network to super-resolve both optical\nflows and images. Optical flow SR from LR frames provides accurate temporal\ndependency and ultimately improves video SR performance. Specifically, we first\npropose an optical flow reconstruction network (OFRnet) to infer HR optical\nflows in a coarse-to-fine manner. Then, motion compensation is performed using\nHR optical flows to encode temporal dependency. Finally, compensated LR inputs\nare fed to a super-resolution network (SRnet) to generate SR results. Extensive\nexperiments have been conducted to demonstrate the effectiveness of HR optical\nflows for SR performance improvement. Comparative results on the Vid4 and\nDAVIS-10 datasets show that our network achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 07:25:24 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Wang", "Longguang", ""], ["Guo", "Yulan", ""], ["Liu", "Li", ""], ["Lin", "Zaiping", ""], ["Deng", "Xinpu", ""], ["An", "Wei", ""]]}, {"id": "2001.02149", "submitter": "Sinisa Stekovic", "authors": "Sinisa Stekovic, Shreyas Hampali, Mahdi Rad, Sayan Deb Sarkar,\n  Friedrich Fraundorfer, Vincent Lepetit", "title": "General 3D Room Layout from a Single View by Render-and-Compare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to reconstruct the 3D layout of a room (walls,\nfloors, ceilings) from a single perspective view in challenging conditions, by\ncontrast with previous single-view methods restricted to cuboid-shaped layouts.\nThis input view can consist of a color image only, but considering a depth map\nresults in a more accurate reconstruction. Our approach is formalized as\nsolving a constrained discrete optimization problem to find the set of 3D\npolygons that constitute the layout. In order to deal with occlusions between\ncomponents of the layout, which is a problem ignored by previous works, we\nintroduce an analysis-by-synthesis method to iteratively refine the 3D layout\nestimate. As no dataset was available to evaluate our method quantitatively, we\ncreated one together with several appropriate metrics. Our dataset consists of\n293 images from ScanNet, which we annotated with precise 3D layouts. It offers\nthree times more samples than the popular NYUv2 303 benchmark, and a much\nlarger variety of layouts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 16:14:00 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 15:41:56 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Stekovic", "Sinisa", ""], ["Hampali", "Shreyas", ""], ["Rad", "Mahdi", ""], ["Sarkar", "Sayan Deb", ""], ["Fraundorfer", "Friedrich", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2001.02160", "submitter": "Duc Hoang", "authors": "Duc Hoang (1), Jesse Hamer (2), Gabriel N. Perdue (3), Steven R. Young\n  (4), Jonathan Miller (5), Anushree Ghosh (5) ((1) Rhodes College, (2) The\n  University of Iowa, (3) Fermi National Accelerator Laboratory, (4) Oak Ridge\n  National Laboratory, (5) Universidad T\\'ecnica Federico Santa Mar\\'ia)", "title": "Inferring Convolutional Neural Networks' accuracies from their\n  architectural characterizations", "comments": "6 pages, 5 figures, 5 tables, to appear in proceedings of the 18th\n  International Conference on Machine Learning and Applications - ICMLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have shown strong promise for analyzing\nscientific data from many domains including particle imaging detectors.\nHowever, the challenge of choosing the appropriate network architecture (depth,\nkernel shapes, activation functions, etc.) for specific applications and\ndifferent data sets is still poorly understood. In this paper, we study the\nrelationships between a CNN's architecture and its performance by proposing a\nsystematic language that is useful for comparison between different CNN's\narchitectures before training time. We characterize CNN's architecture by\ndifferent attributes, and demonstrate that the attributes can be predictive of\nthe networks' performance in two specific computer vision-based physics\nproblems -- event vertex finding and hadron multiplicity classification in the\nMINERvA experiment at Fermi National Accelerator Laboratory. In doing so, we\nextract several architectural attributes from optimized networks' architecture\nfor the physics problems, which are outputs of a model selection algorithm\ncalled Multi-node Evolutionary Neural Networks for Deep Learning (MENNDL). We\nuse machine learning models to predict whether a network can perform better\nthan a certain threshold accuracy before training. The models perform 16-20%\nbetter than random guessing. Additionally, we found an coefficient of\ndetermination of 0.966 for an Ordinary Least Squares model in a regression on\naccuracy over a large population of networks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 16:41:58 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 03:52:48 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Hoang", "Duc", ""], ["Hamer", "Jesse", ""], ["Perdue", "Gabriel N.", ""], ["Young", "Steven R.", ""], ["Miller", "Jonathan", ""], ["Ghosh", "Anushree", ""]]}, {"id": "2001.02161", "submitter": "Senthil Yogamani", "authors": "Nivedita Tripathi and Senthil Yogamani", "title": "Trained Trajectory based Automated Parking System using Visual SLAM on\n  Surround View Cameras", "comments": "Accepted for presentation at CVPR 2021 Workshop on Women in Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Parking is becoming a standard feature in modern vehicles. Existing\nparking systems build a local map to be able to plan for maneuvering towards a\ndetected slot. Next generation parking systems have an use case where they\nbuild a persistent map of the environment where the car is frequently parked,\nsay for example, home parking or office parking. The pre-built map helps in\nre-localizing the vehicle better when its trying to park the next time. This is\nachieved by augmenting the parking system with a Visual SLAM pipeline and the\nfeature is called trained trajectory parking in the automotive industry. In\nthis paper, we discuss the use cases, design and implementation of a trained\ntrajectory automated parking system. The proposed system is deployed on\ncommercial vehicles and the consumer application is illustrated in\n\\url{https://youtu.be/nRWF5KhyJZU}. The focus of this paper is on the\napplication and the details of vision algorithms are kept at high level.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 16:43:27 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 19:41:29 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 20:34:33 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Tripathi", "Nivedita", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2001.02192", "submitter": "Santhosh Kumar Ramakrishnan", "authors": "Santhosh K. Ramakrishnan, Dinesh Jayaraman, Kristen Grauman", "title": "An Exploration of Embodied Visual Exploration", "comments": "30 main + 21 appendix pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied computer vision considers perception for robots in novel,\nunstructured environments. Of particular importance is the embodied visual\nexploration problem: how might a robot equipped with a camera scope out a new\nenvironment? Despite the progress thus far, many basic questions pertinent to\nthis problem remain unanswered: (i) What does it mean for an agent to explore\nits environment well? (ii) Which methods work well, and under which assumptions\nand environmental settings? (iii) Where do current approaches fall short, and\nwhere might future work seek to improve? Seeking answers to these questions, we\nfirst present a taxonomy for existing visual exploration algorithms and create\na standard framework for benchmarking them. We then perform a thorough\nempirical study of the four state-of-the-art paradigms using the proposed\nframework with two photorealistic simulated 3D environments, a state-of-the-art\nexploration architecture, and diverse evaluation metrics. Our experimental\nresults offer insights and suggest new performance metrics and baselines for\nfuture work in visual exploration. Code, models and data are publicly\navailable: https://github.com/facebookresearch/exploring_exploration\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 17:40:32 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 02:58:53 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Ramakrishnan", "Santhosh K.", ""], ["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "2001.02219", "submitter": "Dong Zichao", "authors": "ZiChao Dong, JiLong Wu, TingTing Ren, Yue Wang, MengYing Ge", "title": "DAF-NET: a saliency based weakly supervised method of dual attention\n  fusion for fine-grained image classification", "comments": "arXiv admin note: text overlap with arXiv:1809.00287 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image classification is a challenging problem, since the\ndifficulty of finding discriminative features. To handle this circumstance,\nbasically, there are two ways to go. One is use attention based method to focus\non informative areas, while the other one aims to find high order between\nfeatures. Further, for attention based method there are two directions,\nactivation based and detection based, which are proved effective by scholars.\nHowever ,rare work focus on fusing two types of attention with high order\nfeature. In this paper, we propose a novel DAF method which fuse two types of\nattention and use them to as PAF(part attention filter) in deep bilinear\ntransformation module to mine the relationship between separate parts of an\nobject. Briefly, our network constructed by a student net who attempt to output\ntwo attention maps and a teacher net uses these two maps as empirical\ninformation to refine the result. The experiment result shows that only student\nnet could get 87.6% accuracy in CUB dataset while cooperating with teacher net\ncould achieve 89.1% accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 12:59:48 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Dong", "ZiChao", ""], ["Wu", "JiLong", ""], ["Ren", "TingTing", ""], ["Wang", "Yue", ""], ["Ge", "MengYing", ""]]}, {"id": "2001.02223", "submitter": "Senthil Yogamani", "authors": "Isabelle Leang, Ganesh Sistu, Fabian Burger, Andrei Bursuc and Senthil\n  Yogamani", "title": "Dynamic Task Weighting Methods for Multi-task Networks in Autonomous\n  Driving Systems", "comments": "Accepted for Oral Presentation at IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep multi-task networks are of particular interest for autonomous driving\nsystems. They can potentially strike an excellent trade-off between predictive\nperformance, hardware constraints and efficient use of information from\nmultiple types of annotations and modalities. However, training such models is\nnon-trivial and requires balancing learning over all tasks as their respective\nlosses display different scales, ranges and dynamics across training. Multiple\ntask weighting methods that adjust the losses in an adaptive way have been\nproposed recently on different datasets and combinations of tasks, making it\ndifficult to compare them. In this work, we review and systematically evaluate\nnine task weighting strategies on common grounds on three automotive datasets\n(KITTI, Cityscapes and WoodScape). We then propose a novel method combining\nevolutionary meta-learning and task-based selective backpropagation, for\ncomputing task weights leading to reliable network training. Our method\noutperforms state-of-the-art methods by a significant margin on a two-task\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 18:54:21 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 20:01:15 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Leang", "Isabelle", ""], ["Sistu", "Ganesh", ""], ["Burger", "Fabian", ""], ["Bursuc", "Andrei", ""], ["Yogamani", "Senthil", ""]]}, {"id": "2001.02302", "submitter": "Zhijun Liang", "authors": "Zhijun Liang, Juan Rojas, Junfa Liu, Yisheng Guan", "title": "Visual-Semantic Graph Attention Networks for Human-Object Interaction\n  Detection", "comments": "7 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scene understanding, robotics benefit from not only detecting individual\nscene instances but also from learning their possible interactions.\nHuman-Object Interaction (HOI) Detection infers the action predicate on a\n<human, predicate, object> triplet. Contextual information has been found\ncritical in inferring interactions. However, most works only use local features\nfrom single human-object pair for inference. Few works have studied the\ndisambiguating contribution of subsidiary relations made available via graph\nnetworks. Similarly, few have learned to effectively leverage visual cues along\nwith the intrinsic semantic regularities contained in HOIs. We contribute a\ndual-graph attention network that effectively aggregates contextual visual,\nspatial, and semantic information dynamically from primary human-object\nrelations as well as subsidiary relations through attention mechanisms for\nstrong disambiguating power. We achieve comparable results on two benchmarks:\nV-COCO and HICO-DET. Code is available at\n\\url{https://github.com/birlrobotics/vs-gats}.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 22:22:46 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 03:43:59 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 15:11:24 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2020 14:53:18 GMT"}, {"version": "v5", "created": "Mon, 19 Oct 2020 10:07:28 GMT"}, {"version": "v6", "created": "Sat, 6 Mar 2021 05:42:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liang", "Zhijun", ""], ["Rojas", "Juan", ""], ["Liu", "Junfa", ""], ["Guan", "Yisheng", ""]]}, {"id": "2001.02307", "submitter": "Keuntaek Lee", "authors": "Keuntaek Lee, Jason Gibson, Evangelos A. Theodorou", "title": "Aggressive Perception-Aware Navigation using Deep Optical Flow Dynamics\n  and PixelMPC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, vision-based control has gained traction by leveraging the power of\nmachine learning. In this work, we couple a model predictive control (MPC)\nframework to a visual pipeline. We introduce deep optical flow (DOF) dynamics,\nwhich is a combination of optical flow and robot dynamics. Using the DOF\ndynamics, MPC explicitly incorporates the predicted movement of relevant pixels\ninto the planned trajectory of a robot. Our implementation of DOF is\nmemory-efficient, data-efficient, and computationally cheap so that it can be\ncomputed in real-time for use in an MPC framework. The suggested Pixel Model\nPredictive Control (PixelMPC) algorithm controls the robot to accomplish a\nhigh-speed racing task while maintaining visibility of the important features\n(gates). This improves the reliability of vision-based estimators for\nlocalization and can eventually lead to safe autonomous flight. The proposed\nalgorithm is tested in a photorealistic simulation with a high-speed drone\nracing task.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 22:33:12 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Lee", "Keuntaek", ""], ["Gibson", "Jason", ""], ["Theodorou", "Evangelos A.", ""]]}, {"id": "2001.02314", "submitter": "Alireza Zareian", "authors": "Alireza Zareian, Svebor Karaman, Shih-Fu Chang", "title": "Bridging Knowledge Graphs to Generate Scene Graphs", "comments": "To be presented at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graphs are powerful representations that parse images into their\nabstract semantic elements, i.e., objects and their interactions, which\nfacilitates visual comprehension and explainable reasoning. On the other hand,\ncommonsense knowledge graphs are rich repositories that encode how the world is\nstructured, and how general concepts interact. In this paper, we present a\nunified formulation of these two constructs, where a scene graph is seen as an\nimage-conditioned instantiation of a commonsense knowledge graph. Based on this\nnew perspective, we re-formulate scene graph generation as the inference of a\nbridge between the scene and commonsense graphs, where each entity or predicate\ninstance in the scene graph has to be linked to its corresponding entity or\npredicate class in the commonsense graph. To this end, we propose a novel\ngraph-based neural network that iteratively propagates information between the\ntwo graphs, as well as within each of them, while gradually refining their\nbridge in each iteration. Our Graph Bridging Network, GB-Net, successively\ninfers edges and nodes, allowing to simultaneously exploit and refine the rich,\nheterogeneous structure of the interconnected scene and commonsense graphs.\nThrough extensive experimentation, we showcase the superior accuracy of GB-Net\ncompared to the most recent methods, resulting in a new state of the art. We\npublicly release the source code of our method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 23:35:52 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 01:21:09 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 11:35:22 GMT"}, {"version": "v4", "created": "Sat, 18 Jul 2020 10:59:53 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zareian", "Alireza", ""], ["Karaman", "Svebor", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2001.02319", "submitter": "Yang Tang", "authors": "Yang Tang, Chaoqiang Zhao, Jianrui Wang, Chongzhen Zhang, Qiyu Sun,\n  Weixing Zheng, Wenli Du, Feng Qian, Juergen Kurths", "title": "An Overview of Perception and Decision-Making in Autonomous Systems in\n  the Era of Learning", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems possess the features of inferring their own ego-motion,\nautonomously understanding their surroundings, and planning trajectories. With\nthe applications of deep learning and reinforcement learning, the perception\nand decision-making abilities of autonomous systems are being efficiently\naddressed, and many new learning-based algorithms have surfaced with respect to\nautonomous perception and decision-making. In this review, we focus on the\napplications of learning-based approaches in perception and decision-making in\nautonomous systems, which is different from previous reviews that discussed\ntraditional methods. First, we delineate the existing classical simultaneous\nlocalization and mapping (SLAM) solutions and review the environmental\nperception and understanding methods based on deep learning, including deep\nlearning-based monocular depth estimation, ego-motion prediction, image\nenhancement, object detection, semantic segmentation, and their combinations\nwith traditional SLAM frameworks. Second, we briefly summarize the existing\nmotion planning techniques, such as path planning and trajectory planning\nmethods, and discuss the navigation methods based on reinforcement learning.\nFinally, we examine the several challenges and promising directions discussed\nand concluded in related research for future works in the era of computer\nscience, automatic control, and robotics.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 00:28:12 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 00:59:24 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 04:46:13 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Tang", "Yang", ""], ["Zhao", "Chaoqiang", ""], ["Wang", "Jianrui", ""], ["Zhang", "Chongzhen", ""], ["Sun", "Qiyu", ""], ["Zheng", "Weixing", ""], ["Du", "Wenli", ""], ["Qian", "Feng", ""], ["Kurths", "Juergen", ""]]}, {"id": "2001.02354", "submitter": "Deheng Qian", "authors": "Yanliang Zhu, Deheng Qian, Dongchun Ren, Huaxia Xia", "title": "VisionNet: A Drivable-space-based Interactive Motion Prediction Network\n  for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The comprehension of environmental traffic situation largely ensures the\ndriving safety of autonomous vehicles. Recently, the mission has been\ninvestigated by plenty of researches, while it is hard to be well addressed due\nto the limitation of collective influence in complex scenarios. These\napproaches model the interactions through the spatial relations between the\ntarget obstacle and its neighbors. However, they oversimplify the challenge\nsince the training stage of the interactions lacks effective supervision. As a\nresult, these models are far from promising. More intuitively, we transform the\nproblem into calculating the interaction-aware drivable spaces and propose the\nCNN-based VisionNet for trajectory prediction. The VisionNet accepts a sequence\nof motion states, i.e., location, velocity, and acceleration, to estimate the\nfuture drivable spaces. The reified interactions significantly increase the\ninterpretation ability of the VisionNet and refine the prediction. To further\nadvance the performance, we propose an interactive loss to guide the generation\nof the drivable spaces. Experiments on multiple public datasets demonstrate the\neffectiveness of the proposed VisionNet.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 03:29:53 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zhu", "Yanliang", ""], ["Qian", "Deheng", ""], ["Ren", "Dongchun", ""], ["Xia", "Huaxia", ""]]}, {"id": "2001.02359", "submitter": "Alireza Zareian", "authors": "Alireza Zareian, Svebor Karaman, Shih-Fu Chang", "title": "Weakly Supervised Visual Semantic Parsing", "comments": "To be presented at CVPR 2020 (oral paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene Graph Generation (SGG) aims to extract entities, predicates and their\nsemantic structure from images, enabling deep understanding of visual content,\nwith many applications such as visual reasoning and image retrieval.\nNevertheless, existing SGG methods require millions of manually annotated\nbounding boxes for training, and are computationally inefficient, as they\nexhaustively process all pairs of object proposals to detect predicates. In\nthis paper, we address those two limitations by first proposing a generalized\nformulation of SGG, namely Visual Semantic Parsing, which disentangles entity\nand predicate recognition, and enables sub-quadratic performance. Then we\npropose the Visual Semantic Parsing Network, VSPNet, based on a dynamic,\nattention-based, bipartite message passing framework that jointly infers graph\nnodes and edges through an iterative process. Additionally, we propose the\nfirst graph-based weakly supervised learning framework, based on a novel graph\nalignment algorithm, which enables training without bounding box annotations.\nThrough extensive experiments, we show that VSPNet outperforms weakly\nsupervised baselines significantly and approaches fully supervised performance,\nwhile being several times faster. We publicly release the source code of our\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 03:46:13 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 18:54:06 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zareian", "Alireza", ""], ["Karaman", "Svebor", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2001.02364", "submitter": "William Qi", "authors": "William Qi, Ravi Teja Mullapudi, Saurabh Gupta, Deva Ramanan", "title": "Learning to Move with Affordance Maps", "comments": "Published at ICLR 2020. For code, see https://github.com/wqi/A2L", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to autonomously explore and navigate a physical space is a\nfundamental requirement for virtually any mobile autonomous agent, from\nhousehold robotic vacuums to autonomous vehicles. Traditional SLAM-based\napproaches for exploration and navigation largely focus on leveraging scene\ngeometry, but fail to model dynamic objects (such as other agents) or semantic\nconstraints (such as wet floors or doorways). Learning-based RL agents are an\nattractive alternative because they can incorporate both semantic and geometric\ninformation, but are notoriously sample inefficient, difficult to generalize to\nnovel settings, and are difficult to interpret. In this paper, we combine the\nbest of both worlds with a modular approach that learns a spatial\nrepresentation of a scene that is trained to be effective when coupled with\ntraditional geometric planners. Specifically, we design an agent that learns to\npredict a spatial affordance map that elucidates what parts of a scene are\nnavigable through active self-supervised experience gathering. In contrast to\nmost simulation environments that assume a static world, we evaluate our\napproach in the VizDoom simulator, using large-scale randomly-generated maps\ncontaining a variety of dynamic actors and hazards. We show that learned\naffordance maps can be used to augment traditional approaches for both\nexploration and navigation, providing significant improvements in performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 04:05:11 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 19:01:26 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Qi", "William", ""], ["Mullapudi", "Ravi Teja", ""], ["Gupta", "Saurabh", ""], ["Ramanan", "Deva", ""]]}, {"id": "2001.02366", "submitter": "Feras Dayoub", "authors": "Peter Corke and Feras Dayoub and David Hall and John Skinner and Niko\n  S\\\"underhauf", "title": "What can robotics research learn from computer vision research?", "comments": "15 pages, to appear in the proceeding of the International Symposium\n  on Robotics Research (ISRR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer vision and robotics research communities are each strong.\nHowever progress in computer vision has become turbo-charged in recent years\ndue to big data, GPU computing, novel learning algorithms and a very effective\nresearch methodology. By comparison, progress in robotics seems slower. It is\ntrue that robotics came later to exploring the potential of learning -- the\nadvantages over the well-established body of knowledge in dynamics, kinematics,\nplanning and control is still being debated, although reinforcement learning\nseems to offer real potential. However, the rapid development of computer\nvision compared to robotics cannot be only attributed to the former's adoption\nof deep learning. In this paper, we argue that the gains in computer vision are\ndue to research methodology -- evaluation under strict constraints versus\nexperiments; bold numbers versus videos.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 04:32:10 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 01:04:07 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Corke", "Peter", ""], ["Dayoub", "Feras", ""], ["Hall", "David", ""], ["Skinner", "John", ""], ["S\u00fcnderhauf", "Niko", ""]]}, {"id": "2001.02372", "submitter": "Bernhard Bermeitinger", "authors": "Simon Donig, Maria Christoforaki, Bernhard Bermeitinger, Siegfried\n  Handschuh", "title": "Multimodal Semantic Transfer from Text to Image. Fine-Grained Image\n  Classification by Distributional Semantics", "comments": "19 pages, second half in German as published in DHd2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last years, image classification processes like neural networks in the\narea of art-history and Heritage Informatics have experienced a broad\ndistribution (Lang and Ommer 2018). These methods face several challenges,\nincluding the handling of comparatively small amounts of data as well as\nhigh-dimensional data in the Digital Humanities. Here, a Convolutional Neural\nNetwork (CNN) is used that output is not, as usual, a series of flat text\nlabels but a series of semantically loaded vectors. These vectors result from a\nDistributional Semantic Model (DSM) which is generated from an in-domain text\ncorpus.\n  -----\n  In den letzten Jahren hat die Verwendung von Bildklassifizierungsverfahren\nwie neuronalen Netzwerken auch im Bereich der historischen Bildwissenschaften\nund der Heritage Informatics weite Verbreitung gefunden (Lang und Ommer 2018).\nDiese Verfahren stehen dabei vor einer Reihe von Herausforderungen, darunter\ndem Umgangmit den vergleichsweise kleinen Datenmengen sowie zugleich\nhochdimensionalen Da-tenr\\\"aumen in den digitalen Geisteswissenschaften. Meist\nbilden diese Methoden dieKlassifizierung auf einen vergleichsweise flachen Raum\nab. Dieser flache Zugang verliert im Bem\\\"uhen um ontologische Eindeutigkeit\neine Reihe von relevanten Dimensionen, darunter taxonomische, mereologische und\nassoziative Beziehungen zwischenden Klassen beziehungsweise dem nicht\nformalisierten Kontext. Dabei wird ein Convolutional Neural Network (CNN)\ngenutzt, dessen Ausgabe im Trainingsprozess, anders als herk\\\"ommlich, nicht\nauf einer Serie flacher Textlabel beruht, sondern auf einer Serie von Vektoren.\nDiese Vektoren resultieren aus einem Distributional Semantic Model (DSM),\nwelches aus einem Dom\\\"ane-Textkorpus generiert wird.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 14:26:06 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Donig", "Simon", ""], ["Christoforaki", "Maria", ""], ["Bermeitinger", "Bernhard", ""], ["Handschuh", "Siegfried", ""]]}, {"id": "2001.02381", "submitter": "Dong Gong", "authors": "Dong Gong, Wei Sun, Qinfeng Shi, Anton van den Hengel, Yanning Zhang", "title": "Learning to Zoom-in via Learning to Zoom-out: Real-world\n  Super-resolution by Generating and Adapting Degradation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most learning-based super-resolution (SR) methods aim to recover\nhigh-resolution (HR) image from a given low-resolution (LR) image via learning\non LR-HR image pairs. The SR methods learned on synthetic data do not perform\nwell in real-world, due to the domain gap between the artificially synthesized\nand real LR images. Some efforts are thus taken to capture real-world image\npairs. The captured LR-HR image pairs usually suffer from unavoidable\nmisalignment, which hampers the performance of end-to-end learning, however.\nHere, focusing on the real-world SR, we ask a different question: since\nmisalignment is unavoidable, can we propose a method that does not need LR-HR\nimage pairing and alignment at all and utilize real images as they are? Hence\nwe propose a framework to learn SR from an arbitrary set of unpaired LR and HR\nimages and see how far a step can go in such a realistic and \"unsupervised\"\nsetting. To do so, we firstly train a degradation generation network to\ngenerate realistic LR images and, more importantly, to capture their\ndistribution (i.e., learning to zoom out). Instead of assuming the domain gap\nhas been eliminated, we minimize the discrepancy between the generated data and\nreal data while learning a degradation adaptive SR network (i.e., learning to\nzoom in). The proposed unpaired method achieves state-of-the-art SR results on\nreal-world images, even in the datasets that favor the paired-learning methods\nmore.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 05:17:02 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Gong", "Dong", ""], ["Sun", "Wei", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""], ["Zhang", "Yanning", ""]]}, {"id": "2001.02384", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Shuguang Cui, and Zhi Ding", "title": "Hypergraph Spectral Analysis and Processing in 3D Point Cloud", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3042088", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with increasingly popular virtual reality applications, the\nthree-dimensional (3D) point cloud has become a fundamental data structure to\ncharacterize 3D objects and surroundings. To process 3D point clouds\nefficiently, a suitable model for the underlying structure and outlier noises\nis always critical. In this work, we propose a hypergraph-based new point cloud\nmodel that is amenable to efficient analysis and processing. We introduce\ntensor-based methods to estimate hypergraph spectrum components and frequency\ncoefficients of point clouds in both ideal and noisy settings. We establish an\nanalytical connection between hypergraph frequencies and structural features.\nWe further evaluate the efficacy of hypergraph spectrum estimation in two\ncommon point cloud applications of sampling and denoising for which also we\nelaborate specific hypergraph filter design and spectral properties. The\nempirical performance demonstrates the strength of hypergraph signal processing\nas a tool in 3D point clouds and the underlying properties.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 05:30:16 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Songyang", ""], ["Cui", "Shuguang", ""], ["Ding", "Zhi", ""]]}, {"id": "2001.02387", "submitter": "Balamurali Murugesan", "authors": "Balamurali Murugesan, Kaushik Sarveswaran, Vijaya Raghavan S, Sharath\n  M Shankaranarayana, Keerthi Ram and Mohanasankar Sivaprakasam", "title": "A context based deep learning approach for unbalanced medical image\n  segmentation", "comments": "Accepted in ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated medical image segmentation is an important step in many medical\nprocedures. Recently, deep learning networks have been widely used for various\nmedical image segmentation tasks, with U-Net and generative adversarial nets\n(GANs) being some of the commonly used ones. Foreground-background class\nimbalance is a common occurrence in medical images, and U-Net has difficulty in\nhandling class imbalance because of its cross entropy (CE) objective function.\nSimilarly, GAN also suffers from class imbalance because the discriminator\nlooks at the entire image to classify it as real or fake. Since the\ndiscriminator is essentially a deep learning classifier, it is incapable of\ncorrectly identifying minor changes in small structures. To address these\nissues, we propose a novel context based CE loss function for U-Net, and a\nnovel architecture Seg-GLGAN. The context based CE is a linear combination of\nCE obtained over the entire image and its region of interest (ROI). In\nSeg-GLGAN, we introduce a novel context discriminator to which the entire image\nand its ROI are fed as input, thus enforcing local context. We conduct\nextensive experiments using two challenging unbalanced datasets: PROMISE12 and\nACDC. We observe that segmentation results obtained from our methods give\nbetter segmentation metrics as compared to various baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 05:46:02 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Murugesan", "Balamurali", ""], ["Sarveswaran", "Kaushik", ""], ["S", "Vijaya Raghavan", ""], ["Shankaranarayana", "Sharath M", ""], ["Ram", "Keerthi", ""], ["Sivaprakasam", "Mohanasankar", ""]]}, {"id": "2001.02390", "submitter": "Corey Lammie", "authors": "Corey Lammie, Wei Xiang, Mostafa Rahimi Azghadi", "title": "Training Progressively Binarizing Deep Networks Using FPGAs", "comments": "Accepted at 2020 IEEE International Symposium on Circuits and Systems\n  (ISCAS)", "journal-ref": "2020 IEEE International Symposium on Circuits and Systems (ISCAS)", "doi": "10.1109/ISCAS45731.2020.9181099", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While hardware implementations of inference routines for Binarized Neural\nNetworks (BNNs) are plentiful, current realizations of efficient BNN hardware\ntraining accelerators, suitable for Internet of Things (IoT) edge devices,\nleave much to be desired. Conventional BNN hardware training accelerators\nperform forward and backward propagations with parameters adopting binary\nrepresentations, and optimization using parameters adopting floating or\nfixed-point real-valued representations--requiring two distinct sets of network\nparameters. In this paper, we propose a hardware-friendly training method that,\ncontrary to conventional methods, progressively binarizes a singular set of\nfixed-point network parameters, yielding notable reductions in power and\nresource utilizations. We use the Intel FPGA SDK for OpenCL development\nenvironment to train our progressively binarizing DNNs on an OpenVINO FPGA. We\nbenchmark our training approach on both GPUs and FPGAs using CIFAR-10 and\ncompare it to conventional BNNs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 06:01:13 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lammie", "Corey", ""], ["Xiang", "Wei", ""], ["Azghadi", "Mostafa Rahimi", ""]]}, {"id": "2001.02394", "submitter": "Gao Huang", "authors": "Gao Huang and Zhuang Liu and Geoff Pleiss and Laurens van der Maaten\n  and Kilian Q. Weinberger", "title": "Convolutional Networks with Dense Connectivity", "comments": "Journal(PAMI) version of DenseNet(CVPR'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion.Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, encourage feature\nreuse and substantially improve parameter efficiency. We evaluate our proposed\narchitecture on four highly competitive object recognition benchmark tasks\n(CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant\nimprovements over the state-of-the-art on most of them, whilst requiring less\nparameters and computation to achieve high performance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 06:54:53 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Huang", "Gao", ""], ["Liu", "Zhuang", ""], ["Pleiss", "Geoff", ""], ["van der Maaten", "Laurens", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "2001.02397", "submitter": "Sriprabha Ramanarayanan", "authors": "Sriprabha Ramanarayanan, Balamurali Murugesan, Keerthi Ram and\n  Mohanasankar Sivaprakasam", "title": "DC-WCNN: A deep cascade of wavelet based convolutional neural networks\n  for MR Image Reconstruction", "comments": "Accepted in ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several variants of Convolutional Neural Networks (CNN) have been developed\nfor Magnetic Resonance (MR) image reconstruction. Among them, U-Net has shown\nto be the baseline architecture for MR image reconstruction. However,\nsub-sampling is performed by its pooling layers, causing information loss which\nin turn leads to blur and missing fine details in the reconstructed image. We\npropose a modification to the U-Net architecture to recover fine structures.\nThe proposed network is a wavelet packet transform based encoder-decoder CNN\nwith residual learning called CNN. The proposed WCNN has discrete wavelet\ntransform instead of pooling and inverse wavelet transform instead of unpooling\nlayers and residual connections. We also propose a deep cascaded framework\n(DC-WCNN) which consists of cascades of WCNN and k-space data fidelity units to\nachieve high quality MR reconstruction. Experimental results show that WCNN and\nDC-WCNN give promising results in terms of evaluation metrics and better\nrecovery of fine details as compared to other methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 07:04:22 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Ramanarayanan", "Sriprabha", ""], ["Murugesan", "Balamurali", ""], ["Ram", "Keerthi", ""], ["Sivaprakasam", "Mohanasankar", ""]]}, {"id": "2001.02407", "submitter": "Yi-Fu Wu", "authors": "Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam\n  Singh, Fei Deng, Jindong Jiang, Sungjin Ahn", "title": "SPACE: Unsupervised Object-Oriented Scene Representation via Spatial\n  Attention and Decomposition", "comments": "Accepted in ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to decompose complex multi-object scenes into meaningful\nabstractions like objects is fundamental to achieve higher-level cognition.\nPrevious approaches for unsupervised object-oriented scene representation\nlearning are either based on spatial-attention or scene-mixture approaches and\nlimited in scalability which is a main obstacle towards modeling real-world\nscenes. In this paper, we propose a generative latent variable model, called\nSPACE, that provides a unified probabilistic modeling framework that combines\nthe best of spatial-attention and scene-mixture approaches. SPACE can\nexplicitly provide factorized object representations for foreground objects\nwhile also decomposing background segments of complex morphology. Previous\nmodels are good at either of these, but not both. SPACE also resolves the\nscalability problems of previous methods by incorporating parallel\nspatial-attention and thus is applicable to scenes with a large number of\nobjects without performance degradations. We show through experiments on Atari\nand 3D-Rooms that SPACE achieves the above properties consistently in\ncomparison to SPAIR, IODINE, and GENESIS. Results of our experiments can be\nfound on our project website: https://sites.google.com/view/space-project-page\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 07:44:32 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 19:09:05 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 20:21:38 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lin", "Zhixuan", ""], ["Wu", "Yi-Fu", ""], ["Peri", "Skand Vishwanath", ""], ["Sun", "Weihao", ""], ["Singh", "Gautam", ""], ["Deng", "Fei", ""], ["Jiang", "Jindong", ""], ["Ahn", "Sungjin", ""]]}, {"id": "2001.02408", "submitter": "Nengli Lim", "authors": "Sarthak Bhagat, Shagun Uppal, Zhuyun Yin and Nengli Lim", "title": "Disentangling Multiple Features in Video Sequences using Gaussian\n  Processes in Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce MGP-VAE (Multi-disentangled-features Gaussian Processes\nVariational AutoEncoder), a variational autoencoder which uses Gaussian\nprocesses (GP) to model the latent space for the unsupervised learning of\ndisentangled representations in video sequences. We improve upon previous work\nby establishing a framework by which multiple features, static or dynamic, can\nbe disentangled. Specifically we use fractional Brownian motions (fBM) and\nBrownian bridges (BB) to enforce an inter-frame correlation structure in each\nindependent channel, and show that varying this structure enables one to\ncapture different factors of variation in the data. We demonstrate the quality\nof our representations with experiments on three publicly available datasets,\nand also quantify the improvement using a video prediction task. Moreover, we\nintroduce a novel geodesic loss function which takes into account the curvature\nof the data manifold to improve learning. Our experiments show that the\ncombination of the improved representations with the novel loss function enable\nMGP-VAE to outperform the baselines in video prediction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 08:08:01 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 04:16:12 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 14:31:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bhagat", "Sarthak", ""], ["Uppal", "Shagun", ""], ["Yin", "Zhuyun", ""], ["Lim", "Nengli", ""]]}, {"id": "2001.02501", "submitter": "Saqib Ali Khan", "authors": "Saqib Ali Khan, Syed Muhammad Daniyal Khalid, Muhammad Ali Shahzad and\n  Faisal Shafait", "title": "Table Structure Extraction with Bi-directional Gated Recurrent Unit\n  Networks", "comments": "Proceedings of the 15th International Conference on Document Analysis\n  and Recognition (ICDAR) 2019, Sydney, Australia", "journal-ref": null, "doi": "10.1109/ICDAR.2019.00220", "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tables present summarized and structured information to the reader, which\nmakes table structure extraction an important part of document understanding\napplications. However, table structure identification is a hard problem not\nonly because of the large variation in the table layouts and styles, but also\nowing to the variations in the page layouts and the noise contamination levels.\nA lot of research has been done to identify table structure, most of which is\nbased on applying heuristics with the aid of optical character recognition\n(OCR) to hand pick layout features of the tables. These methods fail to\ngeneralize well because of the variations in the table layouts and the errors\ngenerated by OCR. In this paper, we have proposed a robust deep learning based\napproach to extract rows and columns from a detected table in document images\nwith a high precision. In the proposed solution, the table images are first\npre-processed and then fed to a bi-directional Recurrent Neural Network with\nGated Recurrent Units (GRU) followed by a fully-connected layer with soft max\nactivation. The network scans the images from top-to-bottom as well as\nleft-to-right and classifies each input as either a row-separator or a\ncolumn-separator. We have benchmarked our system on publicly available UNLV as\nwell as ICDAR 2013 datasets on which it outperformed the state-of-the-art table\nstructure extraction systems by a significant margin.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 13:17:44 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Khan", "Saqib Ali", ""], ["Khalid", "Syed Muhammad Daniyal", ""], ["Shahzad", "Muhammad Ali", ""], ["Shafait", "Faisal", ""]]}, {"id": "2001.02512", "submitter": "Julian Hossbach", "authors": "Julian Hossbach, Lennart Husvogt, Martin F. Kraus, James G. Fujimoto,\n  Andreas K. Maier", "title": "Deep OCT Angiography Image Generation for Motion Artifact Suppression", "comments": "Accepted at BVM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements, blinking and other motion during the acquisition of optical\ncoherence tomography (OCT) can lead to artifacts, when processed to OCT\nangiography (OCTA) images. Affected scans emerge as high intensity (white) or\nmissing (black) regions, resulting in lost information. The aim of this\nresearch is to fill these gaps using a deep generative model for OCT to OCTA\nimage translation relying on a single intact OCT scan. Therefore, a U-Net is\ntrained to extract the angiographic information from OCT patches. At inference,\na detection algorithm finds outlier OCTA scans based on their surroundings,\nwhich are then replaced by the trained network. We show that generative models\ncan augment the missing scans. The augmented volumes could then be used for 3-D\nsegmentation or increase the diagnostic value.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 13:31:51 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Hossbach", "Julian", ""], ["Husvogt", "Lennart", ""], ["Kraus", "Martin F.", ""], ["Fujimoto", "James G.", ""], ["Maier", "Andreas K.", ""]]}, {"id": "2001.02518", "submitter": "Tullie Murrell", "authors": "Florian Knoll, Tullie Murrell, Anuroop Sriram, Nafissa Yakubova, Jure\n  Zbontar, Michael Rabbat, Aaron Defazio, Matthew J. Muckley, Daniel K.\n  Sodickson, C. Lawrence Zitnick and Michael P. Recht", "title": "Advancing machine learning for MR image reconstruction with an open\n  competition: Overview of the 2019 fastMRI challenge", "comments": null, "journal-ref": null, "doi": "10.1002/mrm.28338", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To advance research in the field of machine learning for MR image\nreconstruction with an open challenge. Methods: We provided participants with a\ndataset of raw k-space data from 1,594 consecutive clinical exams of the knee.\nThe goal of the challenge was to reconstruct images from these data. In order\nto strike a balance between realistic data and a shallow learning curve for\nthose not already familiar with MR image reconstruction, we ran multiple tracks\nfor multi-coil and single-coil data. We performed a two-stage evaluation based\non quantitative image metrics followed by evaluation by a panel of\nradiologists. The challenge ran from June to December of 2019. Results: We\nreceived a total of 33 challenge submissions. All participants chose to submit\nresults from supervised machine learning approaches. Conclusion: The challenge\nled to new developments in machine learning for image reconstruction, provided\ninsight into the current state of the art in the field, and highlighted\nremaining hurdles for clinical adoption.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 23:00:56 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Knoll", "Florian", ""], ["Murrell", "Tullie", ""], ["Sriram", "Anuroop", ""], ["Yakubova", "Nafissa", ""], ["Zbontar", "Jure", ""], ["Rabbat", "Michael", ""], ["Defazio", "Aaron", ""], ["Muckley", "Matthew J.", ""], ["Sodickson", "Daniel K.", ""], ["Zitnick", "C. Lawrence", ""], ["Recht", "Michael P.", ""]]}, {"id": "2001.02525", "submitter": "Jiemin Fang", "authors": "Jiemin Fang, Yuzhu Sun, Kangjian Peng, Qian Zhang, Yuan Li, Wenyu Liu,\n  Xinggang Wang", "title": "Fast Neural Network Adaptation via Parameter Remapping and Architecture\n  Search", "comments": "Accepted by ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks achieve remarkable performance in many computer vision\ntasks. Most state-of-the-art (SOTA) semantic segmentation and object detection\napproaches reuse neural network architectures designed for image classification\nas the backbone, commonly pre-trained on ImageNet. However, performance gains\ncan be achieved by designing network architectures specifically for detection\nand segmentation, as shown by recent neural architecture search (NAS) research\nfor detection and segmentation. One major challenge though, is that ImageNet\npre-training of the search space representation (a.k.a. super network) or the\nsearched networks incurs huge computational cost. In this paper, we propose a\nFast Neural Network Adaptation (FNA) method, which can adapt both the\narchitecture and parameters of a seed network (e.g. a high performing manually\ndesigned backbone) to become a network with different depth, width, or kernels\nvia a Parameter Remapping technique, making it possible to utilize NAS for\ndetection/segmentation tasks a lot more efficiently. In our experiments, we\nconduct FNA on MobileNetV2 to obtain new networks for both segmentation and\ndetection that clearly out-perform existing networks designed both manually and\nby NAS. The total computation cost of FNA is significantly less than SOTA\nsegmentation/detection NAS approaches: 1737$\\times$ less than DPC, 6.8$\\times$\nless than Auto-DeepLab and 7.4$\\times$ less than DetNAS. The code is available\nat https://github.com/JaminFong/FNA.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 13:45:15 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 08:45:48 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Fang", "Jiemin", ""], ["Sun", "Yuzhu", ""], ["Peng", "Kangjian", ""], ["Zhang", "Qian", ""], ["Li", "Yuan", ""], ["Liu", "Wenyu", ""], ["Wang", "Xinggang", ""]]}, {"id": "2001.02593", "submitter": "Ross Goroshin", "authors": "Ross Goroshin, Jonathan Tompson, Debidatta Dwibedi", "title": "An Analysis of Object Representations in Deep Visual Trackers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional deep correlation networks are integral components of\nstate-of the-art approaches to single object visual tracking. It is commonly\nassumed that these networks perform tracking by detection by matching features\nof the object instance with features of the entire frame. Strong architectural\npriors and conditioning on the object representation is thought to encourage\nthis tracking strategy. Despite these strong priors, we show that deep trackers\noften default to tracking by saliency detection - without relying on the object\ninstance representation. Our analysis shows that despite being a useful prior,\nsalience detection can prevent the emergence of more robust tracking strategies\nin deep networks. This leads us to introduce an auxiliary detection task that\nencourages more discriminative object representations that improve tracking\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 16:03:57 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Goroshin", "Ross", ""], ["Tompson", "Jonathan", ""], ["Dwibedi", "Debidatta", ""]]}, {"id": "2001.02595", "submitter": "Youssef Alami Mejjati", "authors": "Youssef Alami Mejjati and Zejiang Shen and Michael Snower and Aaron\n  Gokaslan and Oliver Wang and James Tompkin and Kwang In Kim", "title": "Generating Object Stamps", "comments": "27 pages, 25 figures, 11 tables. Paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to generate diverse foreground objects and composite\nthem into background images using a GAN architecture. Given an object class, a\nuser-provided bounding box, and a background image, we first use a mask\ngenerator to create an object shape, and then use a texture generator to fill\nthe mask such that the texture integrates with the background. By separating\nthe problem of object insertion into these two stages, we show that our model\nallows us to improve the realism of diverse object generation that also agrees\nwith the provided background image. Our results on the challenging COCO dataset\nshow improved overall quality and diversity compared to state-of-the-art object\ninsertion approaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 14:36:43 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 12:09:46 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Mejjati", "Youssef Alami", ""], ["Shen", "Zejiang", ""], ["Snower", "Michael", ""], ["Gokaslan", "Aaron", ""], ["Wang", "Oliver", ""], ["Tompkin", "James", ""], ["Kim", "Kwang In", ""]]}, {"id": "2001.02600", "submitter": "Peng Xu", "authors": "Peng Xu, Timothy M. Hospedales, Qiyue Yin, Yi-Zhe Song, Tao Xiang,\n  Liang Wang", "title": "Deep Learning for Free-Hand Sketch: A Survey and A Toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free-hand sketches are highly illustrative, and have been widely used by\nhumans to depict objects or stories from ancient times to the present. The\nrecent prevalence of touchscreen devices has made sketch creation a much easier\ntask than ever and consequently made sketch-oriented applications increasingly\npopular. The progress of deep learning has immensely benefited free-hand sketch\nresearch and applications. This paper presents a comprehensive survey of the\ndeep learning techniques oriented at free-hand sketch data, and the\napplications that they enable. The main contents of this survey include: (i) A\ndiscussion of the intrinsic traits and unique challenges of free-hand sketch,\nto highlight the essential differences between sketch data and other data\nmodalities, e.g., natural photos. (ii) A review of the developments of\nfree-hand sketch research in the deep learning era, by surveying existing\ndatasets, research topics, and the state-of-the-art methods through a detailed\ntaxonomy and experimental evaluation. (iii) Promotion of future work via a\ndiscussion of bottlenecks, open problems, and potential research directions for\nthe community. Finally, to support future sketch research and applications, we\ncontribute TorchSketch -- the first sketch-oriented open-source deep learning\nlibrary, which is built on PyTorch and available at\nhttps://github.com/PengBoXiangShang/torchsketch/.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 16:23:56 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 14:23:27 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Xu", "Peng", ""], ["Hospedales", "Timothy M.", ""], ["Yin", "Qiyue", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Wang", "Liang", ""]]}, {"id": "2001.02606", "submitter": "Thiago Gomes", "authors": "Thiago L. Gomes and Renato Martins and Jo\\~ao Ferreira and Erickson R.\n  Nascimento", "title": "Do As I Do: Transferring Human Motion and Appearance between Monocular\n  Videos with Spatial and Temporal Constraints", "comments": "10 pages, 8 figures, to appear in Proceedings of the IEEE Winter\n  Conference on Applications of Computer Vision (WACV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating plausible virtual actors from images of real actors remains one of\nthe key challenges in computer vision and computer graphics. Marker-less human\nmotion estimation and shape modeling from images in the wild bring this\nchallenge to the fore. Although the recent advances on view synthesis and\nimage-to-image translation, currently available formulations are limited to\ntransfer solely style and do not take into account the character's motion and\nshape, which are by nature intermingled to produce plausible human forms. In\nthis paper, we propose a unifying formulation for transferring appearance and\nretargeting human motion from monocular videos that regards all these aspects.\nOur method synthesizes new videos of people in a different context where they\nwere initially recorded. Differently from recent appearance transferring\nmethods, our approach takes into account body shape, appearance, and motion\nconstraints. The evaluation is performed with several experiments using\npublicly available real videos containing hard conditions. Our method is able\nto transfer both human motion and appearance outperforming state-of-the-art\nmethods, while preserving specific features of the motion that must be\nmaintained (e.g., feet touching the floor, hands touching a particular object)\nand holding the best visual quality and appearance metrics such as Structural\nSimilarity (SSIM) and Learned Perceptual Image Patch Similarity (LPIPS).\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 16:39:16 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 17:26:48 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Gomes", "Thiago L.", ""], ["Martins", "Renato", ""], ["Ferreira", "Jo\u00e3o", ""], ["Nascimento", "Erickson R.", ""]]}, {"id": "2001.02613", "submitter": "Vaishakh Patil", "authors": "Vaishakh Patil, Wouter Van Gansbeke, Dengxin Dai, Luc Van Gool", "title": "Don't Forget The Past: Recurrent Depth Estimation from Monocular Video", "comments": "Please refer to our webpage for details\n  https://www.trace.ethz.ch/publications/2020/rec_depth_estimation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous cars need continuously updated depth information. Thus far, depth\nis mostly estimated independently for a single frame at a time, even if the\nmethod starts from video input. Our method produces a time series of depth\nmaps, which makes it an ideal candidate for online learning approaches. In\nparticular, we put three different types of depth estimation (supervised depth\nprediction, self-supervised depth prediction, and self-supervised depth\ncompletion) into a common framework. We integrate the corresponding networks\nwith a ConvLSTM such that the spatiotemporal structures of depth across frames\ncan be exploited to yield a more accurate depth estimation. Our method is\nflexible. It can be applied to monocular videos only or be combined with\ndifferent types of sparse depth patterns. We carefully study the architecture\nof the recurrent network and its training strategy. We are first to\nsuccessfully exploit recurrent networks for real-time self-supervised monocular\ndepth estimation and completion. Extensive experiments show that our recurrent\nmethod outperforms its image-based counterpart consistently and significantly\nin both self-supervised scenarios. It also outperforms previous depth\nestimation methods of the three popular groups. Please refer to\nhttps://www.trace.ethz.ch/publications/2020/rec_depth_estimation/ for details.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 16:50:51 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 10:27:13 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Patil", "Vaishakh", ""], ["Van Gansbeke", "Wouter", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2001.02643", "submitter": "Florian Kluger", "authors": "Florian Kluger, Eric Brachmann, Hanno Ackermann, Carsten Rother,\n  Michael Ying Yang, Bodo Rosenhahn", "title": "CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust estimator for fitting multiple parametric models of the\nsame form to noisy measurements. Applications include finding multiple\nvanishing points in man-made scenes, fitting planes to architectural imagery,\nor estimating multiple rigid motions within the same sequence. In contrast to\nprevious works, which resorted to hand-crafted search strategies for multiple\nmodel detection, we learn the search strategy from data. A neural network\nconditioned on previously detected models guides a RANSAC estimator to\ndifferent subsets of all measurements, thereby finding model instances one\nafter another. We train our method supervised as well as self-supervised. For\nsupervised training of the search strategy, we contribute a new dataset for\nvanishing point estimation. Leveraging this dataset, the proposed algorithm is\nsuperior with respect to other robust estimators as well as to designated\nvanishing point estimation algorithms. For self-supervised learning of the\nsearch, we evaluate the proposed algorithm on multi-homography estimation and\ndemonstrate an accuracy that is superior to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 17:37:01 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 16:01:32 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 11:48:39 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Kluger", "Florian", ""], ["Brachmann", "Eric", ""], ["Ackermann", "Hanno", ""], ["Rother", "Carsten", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2001.02658", "submitter": "Lucas Fidon", "authors": "Lucas Fidon, Sebastien Ourselin, Tom Vercauteren", "title": "Distributionally Robust Deep Learning using Hardness Weighted Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limiting failures of machine learning systems is vital for safety-critical\napplications. In order to improve the robustness of machine learning systems,\nDistributionally Robust Optimization (DRO) has been proposed as a\ngeneralization of Empirical Risk Minimization (ERM)aiming at addressing this\nneed. However, its use in deep learning has been severely restricted due to the\nrelative inefficiency of the optimizers available for DRO in comparison to the\nwide-spread variants of Stochastic Gradient Descent (SGD) optimizers for ERM.\nWe propose SGD with hardness weighted sampling, a principled and efficient\noptimization method for DRO in machine learning that is particularly suited in\nthe context of deep learning. Similar to a hard example mining strategy in\nessence and in practice, the proposed algorithm is straightforward to implement\nand computationally as efficient as SGD-based optimizers used for deep\nlearning, requiring minimal overhead computation. In contrast to typical ad hoc\nhard mining approaches, and exploiting recent theoretical results in deep\nlearning optimization, we prove the convergence of our DRO algorithm for\nover-parameterized deep learning networks with ReLU activation and finite\nnumber of layers and parameters. Our experiments on brain tumor segmentation in\nMRI demonstrate the feasibility and the usefulness of our approach. Using our\nhardness weighted sampling leads to a decrease of 2% of the interquartile range\nof the Dice scores for the enhanced tumor and the tumor core regions. The code\nfor the proposed hard weighted sampler will be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 18:02:56 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 09:43:22 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Fidon", "Lucas", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "2001.02728", "submitter": "Siavash Bigdeli", "authors": "Siavash A. Bigdeli, Geng Lin, Tiziano Portenier, L. Andrea Dunbar,\n  Matthias Zwicker", "title": "Learning Generative Models using Denoising Density Estimators", "comments": "Code and models available at\n  https://drive.google.com/file/d/1EzKRxnFG1Hd8g6Ggvt-jvKkgpDDwK2bY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning probabilistic models that can estimate the density of a given set of\nsamples, and generate samples from that density, is one of the fundamental\nchallenges in unsupervised machine learning. We introduce a new generative\nmodel based on denoising density estimators (DDEs), which are scalar functions\nparameterized by neural networks, that are efficiently trained to represent\nkernel density estimators of the data. Leveraging DDEs, our main contribution\nis a novel technique to obtain generative models by minimizing the\nKL-divergence directly. We prove that our algorithm for obtaining generative\nmodels is guaranteed to converge to the correct solution. Our approach does not\nrequire specific network architecture as in normalizing flows, nor use ordinary\ndifferential equation solvers as in continuous normalizing flows. Experimental\nresults demonstrate substantial improvement in density estimation and\ncompetitive performance in generative model training.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 20:30:40 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 21:26:44 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bigdeli", "Siavash A.", ""], ["Lin", "Geng", ""], ["Portenier", "Tiziano", ""], ["Dunbar", "L. Andrea", ""], ["Zwicker", "Matthias", ""]]}, {"id": "2001.02741", "submitter": "Luciano da Fontoura Costa", "authors": "Luciano da F. Costa", "title": "Toward Generalized Clustering through an One-Dimensional Approach", "comments": "8 pages, 7 figures, a working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After generalizing the concept of clusters to incorporate clusters that are\nlinked to other clusters through some relatively narrow bridges, an approach\nfor detecting patches of separation between these clusters is developed based\non an agglomerative clustering, more specifically the single-linkage, applied\nto one-dimensional slices obtained from respective feature spaces. The\npotential of this method is illustrated with respect to the analyses of\nclusterless uniform and normal distributions of points, as well as a\none-dimensional clustering model characterized by two intervals with high\ndensity of points separated by a less dense interstice. This partial clustering\nmethod is then considered as a means of feature selection and cluster\nidentification, and two simple but potentially effective respective methods are\ndescribed and illustrated with respect to some hypothetical situations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 16:52:05 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Costa", "Luciano da F.", ""]]}, {"id": "2001.02767", "submitter": "Yun-Cheng Tsai", "authors": "Jun-Hao Chen, Samuel Yen-Chi Chen, Yun-Cheng Tsai, Chih-Shiang Shur", "title": "Explainable Deep Convolutional Candlestick Learner", "comments": "Accepted by The 32nd International Conference on Software Engineering\n  & Knowledge Engineering (SEKE 2020), KSIR Virtual Conference Cener,\n  Pittsburgh, USA, July 9--July 19, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Candlesticks are graphical representations of price movements for a given\nperiod. The traders can discovery the trend of the asset by looking at the\ncandlestick patterns. Although deep convolutional neural networks have achieved\ngreat success for recognizing the candlestick patterns, their reasoning hides\ninside a black box. The traders cannot make sure what the model has learned. In\nthis contribution, we provide a framework which is to explain the reasoning of\nthe learned model determining the specific candlestick patterns of time series.\nBased on the local search adversarial attacks, we show that the learned model\nperceives the pattern of the candlesticks in a way similar to the human trader.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 22:11:13 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 09:12:55 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 09:09:02 GMT"}, {"version": "v4", "created": "Fri, 29 May 2020 06:04:54 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Chen", "Jun-Hao", ""], ["Chen", "Samuel Yen-Chi", ""], ["Tsai", "Yun-Cheng", ""], ["Shur", "Chih-Shiang", ""]]}, {"id": "2001.02799", "submitter": "Xi Yan", "authors": "Xi Yan, David Acuna, Sanja Fidler", "title": "Neural Data Server: A Large-Scale Search Engine for Transfer Learning\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has proven to be a successful technique to train deep\nlearning models in the domains where little training data is available. The\ndominant approach is to pretrain a model on a large generic dataset such as\nImageNet and finetune its weights on the target domain. However, in the new era\nof an ever-increasing number of massive datasets, selecting the relevant data\nfor pretraining is a critical issue. We introduce Neural Data Server (NDS), a\nlarge-scale search engine for finding the most useful transfer learning data to\nthe target domain. NDS consists of a dataserver which indexes several large\npopular image datasets, and aims to recommend data to a client, an end-user\nwith a target application with its own small labeled dataset. The dataserver\nrepresents large datasets with a much more compact mixture-of-experts model,\nand employs it to perform data search in a series of dataserver-client\ntransactions at a low computational cost. We show the effectiveness of NDS in\nvarious transfer learning scenarios, demonstrating state-of-the-art performance\non several target datasets and tasks such as image classification, object\ndetection and instance segmentation. Neural Data Server is available as a\nweb-service at http://aidemos.cs.toronto.edu/nds/.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 01:21:30 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 23:20:35 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 00:43:03 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Yan", "Xi", ""], ["Acuna", "David", ""], ["Fidler", "Sanja", ""]]}, {"id": "2001.02801", "submitter": "Olga Moskvyak", "authors": "Olga Moskvyak, Frederic Maire, Feras Dayoub and Mahsa Baktashmotlagh", "title": "Learning landmark guided embeddings for animal re-identification", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-identification of individual animals in images can be ambiguous due to\nsubtle variations in body markings between different individuals and no\nconstraints on the poses of animals in the wild. Person re-identification is a\nsimilar task and it has been approached with a deep convolutional neural\nnetwork (CNN) that learns discriminative embeddings for images of people.\nHowever, learning discriminative features for an individual animal is more\nchallenging than for a person's appearance due to the relatively small size of\necological datasets compared to labelled datasets of person's identities. We\npropose to improve embedding learning by exploiting body landmarks information\nexplicitly. Body landmarks are provided to the input of a CNN as confidence\nheatmaps that can be obtained from a separate body landmark predictor. The\nmodel is encouraged to use heatmaps by learning an auxiliary task of\nreconstructing input heatmaps. Body landmarks guide a feature extraction\nnetwork to learn the representation of a distinctive pattern and its position\non the body. We evaluate the proposed method on a large synthetic dataset and a\nsmall real dataset. Our method outperforms the same model without body\nlandmarks input by 26% and 18% on the synthetic and the real datasets\nrespectively. The method is robust to noise in input coordinates and can\ntolerate an error in coordinates up to 10% of the image size.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 01:31:00 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Moskvyak", "Olga", ""], ["Maire", "Frederic", ""], ["Dayoub", "Feras", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "2001.02814", "submitter": "Yuanlong Yu", "authors": "You Huang, Yuanlong Yu", "title": "An Internal Covariate Shift Bounding Algorithm for Deep Neural Networks\n  by Unitizing Layers' Outputs", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) techniques have been proposed to reduce the\nso-called Internal Covariate Shift (ICS) by attempting to keep the\ndistributions of layer outputs unchanged. Experiments have shown their\neffectiveness on training deep neural networks. However, since only the first\ntwo moments are controlled in these BN techniques, it seems that a weak\nconstraint is imposed on layer distributions and furthermore whether such\nconstraint can reduce ICS is unknown. Thus this paper proposes a measure for\nICS by using the Earth Mover (EM) distance and then derives the upper and lower\nbounds for the measure to provide a theoretical analysis of BN. The upper bound\nhas shown that BN techniques can control ICS only for the outputs with low\ndimensions and small noise whereas their control is NOT effective in other\ncases. This paper also proves that such control is just a bounding of ICS\nrather than a reduction of ICS. Meanwhile, the analysis shows that the\nhigh-order moments and noise, which BN cannot control, have great impact on the\nlower bound. Based on such analysis, this paper furthermore proposes an\nalgorithm that unitizes the outputs with an adjustable parameter to further\nbound ICS in order to cope with the problems of BN. The upper bound for the\nproposed unitization is noise-free and only dominated by the parameter. Thus,\nthe parameter can be trained to tune the bound and further to control ICS.\nBesides, the unitization is embedded into the framework of BN to reduce the\ninformation loss. The experiments show that this proposed algorithm outperforms\nexisting BN techniques on CIFAR-10, CIFAR-100 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 02:35:58 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Huang", "You", ""], ["Yu", "Yuanlong", ""]]}, {"id": "2001.02816", "submitter": "Shubhra Aich", "authors": "Shubhra Aich, Ian Stavness, Yasuhiro Taniguchi, Masaki Yamazaki", "title": "Multi-Scale Weight Sharing Network for Image Recognition", "comments": "Accepted in Pattern Recognition Letters, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the idea of weight sharing over multiple scales in\nconvolutional networks. Inspired by traditional computer vision approaches, we\nshare the weights of convolution kernels over different scales in the same\nlayers of the network. Although multi-scale feature aggregation and sharing\ninside convolutional networks are common in practice, none of the previous\nworks address the issue of convolutional weight sharing. We evaluate our weight\nsharing scheme on two heterogeneous image recognition datasets - ImageNet\n(object recognition) and Places365-Standard (scene classification). With\napproximately 25% fewer parameters, our shared-weight ResNet model provides\nsimilar performance compared to baseline ResNets. Shared-weight models are\nfurther validated via transfer learning experiments on four additional image\nrecognition datasets - Caltech256 and Stanford 40 Actions (object-centric) and\nSUN397 and MIT Inddor67 (scene-centric). Experimental results demonstrate\nsignificant redundancy in the vanilla implementations of the deeper networks,\nand also indicate that a shift towards increasing the receptive field per\nparameter may improve future convolutional network architectures.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 02:42:00 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Aich", "Shubhra", ""], ["Stavness", "Ian", ""], ["Taniguchi", "Yasuhiro", ""], ["Yamazaki", "Masaki", ""]]}, {"id": "2001.02823", "submitter": "Ji Liu", "authors": "Yan Lin and Ji Liu and Jianlin Zhou", "title": "A novel tree-structured point cloud dataset for skeletonization\n  algorithm evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curve skeleton extraction from unorganized point cloud is a fundamental task\nof computer vision and three-dimensional data preprocessing and visualization.\nA great amount of work has been done to extract skeleton from point cloud. but\nthe lack of standard datasets of point cloud with ground truth skeleton makes\nit difficult to evaluate these algorithms. In this paper, we construct a brand\nnew tree-structured point cloud dataset, including ground truth skeletons, and\npoint cloud models. In addition, four types of point cloud are built on clean\npoint cloud: point clouds with noise, point clouds with missing data, point\nclouds with different density, and point clouds with uneven density\ndistribution. We first use tree editor to build the tree skeleton and\ncorresponding mesh model. Since the implicit surface is sufficiently expressive\nto retain the edges and details of the complex branches model, we use the\nimplicit surface to model the triangular mesh. With the implicit surface,\nvirtual scanner is applied to the sampling of point cloud. Finally, considering\nthe challenges in skeleton extraction, we introduce different methods to build\nfour different types of point cloud models. This dataset can be used as\nstandard dataset for skeleton extraction algorithms. And the evaluation between\nskeleton extraction algorithms can be performed by comparing the ground truth\nskeleton with the extracted skeleton.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 03:35:57 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Lin", "Yan", ""], ["Liu", "Ji", ""], ["Zhou", "Jianlin", ""]]}, {"id": "2001.02858", "submitter": "Yashil Sukurdeep", "authors": "Yashil Sukurdeep, Martin Bauer, Nicolas Charon", "title": "An inexact matching approach for the comparison of plane curves with\n  general elastic metrics", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": "10.1109/IEEECONF44664.2019.9049031", "report-no": null, "categories": "cs.CG cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new mathematical formulation and numerical approach\nfor the computation of distances and geodesics between immersed planar curves.\nOur approach combines the general simplifying transform for first-order elastic\nmetrics that was recently introduced by Kurtek and Needham, together with a\nrelaxation of the matching constraint using parametrization-invariant fidelity\nmetrics. The main advantages of this formulation are that it leads to a simple\noptimization problem for discretized curves, and that it provides a flexible\napproach to deal with noisy, inconsistent or corrupted data. These benefits are\nillustrated via a few preliminary numerical results.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 06:45:50 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sukurdeep", "Yashil", ""], ["Bauer", "Martin", ""], ["Charon", "Nicolas", ""]]}, {"id": "2001.02865", "submitter": "Hai-Ming Xu", "authors": "Hai-Ming Xu, Lingqiao Liu, Dong Gong", "title": "Semi-supervised Learning via Conditional Rotation Angle Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning (SlfSL), aiming at learning feature representations\nthrough ingeniously designed pretext tasks without human annotation, has\nachieved compelling progress in the past few years. Very recently, SlfSL has\nalso been identified as a promising solution for semi-supervised learning\n(SemSL) since it offers a new paradigm to utilize unlabeled data. This work\nfurther explores this direction by proposing to couple SlfSL with SemSL. Our\ninsight is that the prediction target in SemSL can be modeled as the latent\nfactor in the predictor for the SlfSL target. Marginalizing over the latent\nfactor naturally derives a new formulation which marries the prediction targets\nof these two learning processes. By implementing this idea through a\nsimple-but-effective SlfSL approach -- rotation angle prediction, we create a\nnew SemSL approach called Conditional Rotation Angle Estimation (CRAE).\nSpecifically, CRAE is featured by adopting a module which predicts the image\nrotation angle conditioned on the candidate image class. Through experimental\nevaluation, we show that CRAE achieves superior performance over the other\nexisting ways of combining SlfSL and SemSL. To further boost CRAE, we propose\ntwo extensions to strengthen the coupling between SemSL target and SlfSL target\nin basic CRAE. We show that this leads to an improved CRAE method which can\nachieve the state-of-the-art SemSL performance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 07:06:20 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Xu", "Hai-Ming", ""], ["Liu", "Lingqiao", ""], ["Gong", "Dong", ""]]}, {"id": "2001.02870", "submitter": "Ruigang Niu", "authors": "Ruigang Niu, Xian Sun, Yu Tian, Wenhui Diao, Kaiqiang Chen, Kun Fu", "title": "Hybrid Multiple Attention Network for Semantic Segmentation in Aerial\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation in very high resolution (VHR) aerial images is one of\nthe most challenging tasks in remote sensing image understanding. Most of the\ncurrent approaches are based on deep convolutional neural networks (DCNNs).\nHowever, standard convolution with local receptive fields fails in modeling\nglobal dependencies. Prior researches have indicated that attention-based\nmethods can capture long-range dependencies and further reconstruct the feature\nmaps for better representation. Nevertheless, limited by the mere perspective\nof spacial and channel attention and huge computation complexity of\nself-attention mechanism, it is unlikely to model the effective semantic\ninterdependencies between each pixel-pair of remote sensing data of complex\nspectra. In this work, we propose a novel attention-based framework named\nHybrid Multiple Attention Network (HMANet) to adaptively capture global\ncorrelations from the perspective of space, channel and category in a more\neffective and efficient manner. Concretely, a class augmented attention (CAA)\nmodule embedded with a class channel attention (CCA) module can be used to\ncompute category-based correlation and recalibrate the class-level information.\nAdditionally, we introduce a simple yet effective region shuffle attention\n(RSA) module to reduce feature redundant and improve the efficiency of\nself-attention mechanism via region-wise representations. Extensive\nexperimental results on the ISPRS Vaihingen and Potsdam benchmark demonstrate\nthe effectiveness and efficiency of our HMANet over other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 07:47:51 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 06:35:38 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 02:17:37 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Niu", "Ruigang", ""], ["Sun", "Xian", ""], ["Tian", "Yu", ""], ["Diao", "Wenhui", ""], ["Chen", "Kaiqiang", ""], ["Fu", "Kun", ""]]}, {"id": "2001.02890", "submitter": "Shuai Yang", "authors": "Shuai Yang, Zhangyang Wang, Jiaying Liu, Zongming Guo", "title": "Deep Plastic Surgery: Robust and Controllable Image Editing with\n  Human-Drawn Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based image editing aims to synthesize and modify photos based on the\nstructural information provided by the human-drawn sketches. Since sketches are\ndifficult to collect, previous methods mainly use edge maps instead of sketches\nto train models (referred to as edge-based models). However, sketches display\ngreat structural discrepancy with edge maps, thus failing edge-based models.\nMoreover, sketches often demonstrate huge variety among different users,\ndemanding even higher generalizability and robustness for the editing model to\nwork. In this paper, we propose Deep Plastic Surgery, a novel, robust and\ncontrollable image editing framework that allows users to interactively edit\nimages using hand-drawn sketch inputs. We present a sketch refinement strategy,\nas inspired by the coarse-to-fine drawing process of the artists, which we show\ncan help our model well adapt to casual and varied sketches without the need\nfor real sketch training data. Our model further provides a refinement level\ncontrol parameter that enables users to flexibly define how \"reliable\" the\ninput sketch should be considered for the final output, balancing between\nsketch faithfulness and output verisimilitude (as the two goals might\ncontradict if the input sketch is drawn poorly). To achieve the multi-level\nrefinement, we introduce a style-based module for level conditioning, which\nallows adaptive feature representations for different levels in a singe\nnetwork. Extensive experimental results demonstrate the superiority of our\napproach in improving the visual quality and user controllablity of image\nediting over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 08:57:50 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Yang", "Shuai", ""], ["Wang", "Zhangyang", ""], ["Liu", "Jiaying", ""], ["Guo", "Zongming", ""]]}, {"id": "2001.02899", "submitter": "Seunghwan Lee", "authors": "Seunghwan Lee, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim", "title": "Self-Supervised Fast Adaptation for Denoising via Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under certain statistical assumptions of noise, recent self-supervised\napproaches for denoising have been introduced to learn network parameters\nwithout true clean images, and these methods can restore an image by exploiting\ninformation available from the given input (i.e., internal statistics) at test\ntime. However, self-supervised methods are not yet combined with conventional\nsupervised denoising methods which train the denoising networks with a large\nnumber of external training samples. Thus, we propose a new denoising approach\nthat can greatly outperform the state-of-the-art supervised denoising methods\nby adapting their network parameters to the given input through selfsupervision\nwithout changing the networks architectures. Moreover, we propose a\nmeta-learning algorithm to enable quick adaptation of parameters to the\nspecific input at test time. We demonstrate that the proposed method can be\neasily employed with state-of-the-art denoising networks without additional\nparameters, and achieve state-of-the-art performance on numerous benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 09:40:53 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Lee", "Seunghwan", ""], ["Cho", "Donghyeon", ""], ["Kim", "Jiwon", ""], ["Kim", "Tae Hyun", ""]]}, {"id": "2001.02905", "submitter": "Seobin Park", "authors": "Seobin Park, Jinsu Yoo, Donghyeon Cho, Jiwon Kim and Tae Hyun Kim", "title": "Fast Adaptation to Super-Resolution Networks via Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional supervised super-resolution (SR) approaches are trained with\nmassive external SR datasets but fail to exploit desirable properties of the\ngiven test image. On the other hand, self-supervised SR approaches utilize the\ninternal information within a test image but suffer from computational\ncomplexity in run-time. In this work, we observe the opportunity for further\nimprovement of the performance of SISR without changing the architecture of\nconventional SR networks by practically exploiting additional information given\nfrom the input image. In the training stage, we train the network via\nmeta-learning; thus, the network can quickly adapt to any input image at test\ntime. Then, in the test stage, parameters of this meta-learned network are\nrapidly fine-tuned with only a few iterations by only using the given\nlow-resolution image. The adaptation at the test time takes full advantage of\npatch-recurrence property observed in natural images. Our method effectively\nhandles unknown SR kernels and can be applied to any existing model. We\ndemonstrate that the proposed model-agnostic approach consistently improves the\nperformance of conventional SR networks on various benchmark SR datasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 09:59:02 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 04:49:19 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 09:24:50 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Park", "Seobin", ""], ["Yoo", "Jinsu", ""], ["Cho", "Donghyeon", ""], ["Kim", "Jiwon", ""], ["Kim", "Tae Hyun", ""]]}, {"id": "2001.02915", "submitter": "Shuai Yang", "authors": "Yueyu Hu, Shuai Yang, Wenhan Yang, Ling-Yu Duan, Jiaying Liu", "title": "Towards Coding for Human and Machine Vision: A Scalable Image Coding\n  Approach", "comments": "Project page: https://williamyang1991.github.io/projects/VCM-Face/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decades have witnessed the rapid development of image and video\ncoding techniques in the era of big data. However, the signal fidelity-driven\ncoding pipeline design limits the capability of the existing image/video coding\nframeworks to fulfill the needs of both machine and human vision. In this\npaper, we come up with a novel image coding framework by leveraging both the\ncompressive and the generative models, to support machine vision and human\nperception tasks jointly. Given an input image, the feature analysis is first\napplied, and then the generative model is employed to perform image\nreconstruction with features and additional reference pixels, in which compact\nedge maps are extracted in this work to connect both kinds of vision in a\nscalable way. The compact edge map serves as the basic layer for machine vision\ntasks, and the reference pixels act as a sort of enhanced layer to guarantee\nsignal fidelity for human vision. By introducing advanced generative models, we\ntrain a flexible network to reconstruct images from compact feature\nrepresentations and the reference pixels. Experimental results demonstrate the\nsuperiority of our framework in both human visual quality and facial landmark\ndetection, which provide useful evidence on the emerging standardization\nefforts on MPEG VCM (Video Coding for Machine).\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 10:37:17 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 03:18:56 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Hu", "Yueyu", ""], ["Yang", "Shuai", ""], ["Yang", "Wenhan", ""], ["Duan", "Ling-Yu", ""], ["Liu", "Jiaying", ""]]}, {"id": "2001.02950", "submitter": "Pietro Morerio", "authors": "Pietro Morerio, Riccardo Volpi, Ruggero Ragonesi, Vittorio Murino", "title": "Generative Pseudo-label Refinement for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate and characterize the inherent resilience of conditional\nGenerative Adversarial Networks (cGANs) against noise in their conditioning\nlabels, and exploit this fact in the context of Unsupervised Domain Adaptation\n(UDA). In UDA, a classifier trained on the labelled source set can be used to\ninfer pseudo-labels on the unlabelled target set. However, this will result in\na significant amount of misclassified examples (due to the well-known domain\nshift issue), which can be interpreted as noise injection in the ground-truth\nlabels for the target set. We show that cGANs are, to some extent, robust\nagainst such \"shift noise\". Indeed, cGANs trained with noisy pseudo-labels, are\nable to filter such noise and generate cleaner target samples. We exploit this\nfinding in an iterative procedure where a generative model and a classifier are\njointly trained: in turn, the generator allows to sample cleaner data from the\ntarget distribution, and the classifier allows to associate better labels to\ntarget samples, progressively refining target pseudo-labels. Results on common\nbenchmarks show that our method performs better or comparably with the\nunsupervised domain adaptation state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 12:46:55 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Morerio", "Pietro", ""], ["Volpi", "Riccardo", ""], ["Ragonesi", "Ruggero", ""], ["Murino", "Vittorio", ""]]}, {"id": "2001.02988", "submitter": "Lin Zhou", "authors": "Lin Zhou and Haoran Wei and Hao Li and Wenzhe Zhao and Yi Zhang and\n  Yue Zhang", "title": "Objects detection for remote sensing images based on polar coordinates", "comments": "The paper needs a lot of revision. Some problem are not well\n  described. However, this paper has spread out. I think the impact of an\n  imperfect first draft is not good, so we want to withdraw and revise", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary-oriented object detection is an important task in the field of\nremote sensing object detection. Existing studies have shown that the polar\ncoordinate system has obvious advantages in dealing with the problem of\nrotating object modeling, that is, using fewer parameters to achieve more\naccurate rotating object detection. However, present state-of-the-art detectors\nbased on deep learning are all modeled in Cartesian coordinates. In this\narticle, we introduce the polar coordinate system to the deep learning detector\nfor the first time, and propose an anchor free Polar Remote Sensing Object\nDetector (P-RSDet), which can achieve competitive detection accuracy via uses\nsimpler object representation model and less regression parameters. In P-RSDet\nmethod, arbitrary-oriented object detection can be achieved by predicting the\ncenter point and regressing one polar radius and two polar angles. Besides, in\norder to express the geometric constraint relationship between the polar radius\nand the polar angle, a Polar Ring Area Loss function is proposed to improve the\nprediction accuracy of the corner position. Experiments on DOTA, UCAS-AOD and\nNWPU VHR-10 datasets show that our P-RSDet achieves state-of-the-art\nperformances with simpler model and less regression parameters.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 14:02:51 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 01:26:03 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 09:59:45 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2020 03:06:18 GMT"}, {"version": "v5", "created": "Thu, 10 Sep 2020 09:14:14 GMT"}, {"version": "v6", "created": "Sat, 12 Sep 2020 05:56:40 GMT"}, {"version": "v7", "created": "Mon, 21 Sep 2020 02:31:48 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhou", "Lin", ""], ["Wei", "Haoran", ""], ["Li", "Hao", ""], ["Zhao", "Wenzhe", ""], ["Zhang", "Yi", ""], ["Zhang", "Yue", ""]]}, {"id": "2001.02993", "submitter": "Takayuki Hara", "authors": "Takayuki Hara and Tatsuya Harada", "title": "Spherical Image Generation from a Single Normal Field of View Image by\n  Considering Scene Symmetry", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spherical images taken in all directions (360 degrees) allow representing the\nsurroundings of the subject and the space itself, providing an immersive\nexperience to the viewers. Generating a spherical image from a single\nnormal-field-of-view (NFOV) image is convenient and considerably expands the\nusage scenarios because there is no need to use a specific panoramic camera or\ntake images from multiple directions; however, it is still a challenging and\nunsolved problem. The primary challenge is controlling the high degree of\nfreedom involved in generating a wide area that includes the all directions of\nthe desired plausible spherical image. On the other hand, scene symmetry is a\nbasic property of the global structure of the spherical images, such as\nrotation symmetry, plane symmetry and asymmetry. We propose a method to\ngenerate spherical image from a single NFOV image, and control the degree of\nfreedom of the generated regions using scene symmetry. We incorporate\nscene-symmetry parameters as latent variables into conditional variational\nautoencoders, following which we learn the conditional probability of spherical\nimages for NFOV images and scene symmetry. Furthermore, the probability density\nfunctions are represented using neural networks, and scene symmetry is\nimplemented using both circular shift and flip of the hidden variables. Our\nexperiments show that the proposed method can generate various plausible\nspherical images, controlled from symmetric to asymmetric.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 14:09:12 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Hara", "Takayuki", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2001.03004", "submitter": "Sifeng Xia", "authors": "Sifeng Xia, Kunchangtai Liang, Wenhan Yang, Ling-Yu Duan and Jiaying\n  Liu", "title": "An Emerging Coding Paradigm VCM: A Scalable Coding Approach Beyond\n  Feature and Signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a new problem arising from the emerging MPEG\nstandardization effort Video Coding for Machine (VCM), which aims to bridge the\ngap between visual feature compression and classical video coding. VCM is\ncommitted to address the requirement of compact signal representation for both\nmachine and human vision in a more or less scalable way. To this end, we make\nendeavors in leveraging the strength of predictive and generative models to\nsupport advanced compression techniques for both machine and human vision tasks\nsimultaneously, in which visual features serve as a bridge to connect\nsignal-level and task-level compact representations in a scalable manner.\nSpecifically, we employ a conditional deep generation network to reconstruct\nvideo frames with the guidance of learned motion pattern. By learning to\nextract sparse motion pattern via a predictive model, the network elegantly\nleverages the feature representation to generate the appearance of to-be-coded\nframes via a generative model, relying on the appearance of the coded key\nframes. Meanwhile, the sparse motion pattern is compact and highly effective\nfor high-level vision tasks, e.g. action recognition. Experimental results\ndemonstrate that our method yields much better reconstruction quality compared\nwith the traditional video codecs (0.0063 gain in SSIM), as well as\nstate-of-the-art action recognition performance over highly compressed videos\n(9.4% gain in recognition accuracy), which showcases a promising paradigm of\ncoding signal for both human and machine vision.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 14:18:18 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Xia", "Sifeng", ""], ["Liang", "Kunchangtai", ""], ["Yang", "Wenhan", ""], ["Duan", "Ling-Yu", ""], ["Liu", "Jiaying", ""]]}, {"id": "2001.03024", "submitter": "Liming Jiang", "authors": "Liming Jiang, Ren Li, Wayne Wu, Chen Qian, Chen Change Loy", "title": "DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery\n  Detection", "comments": "CVPR 2020. Project page:\n  https://liming-jiang.com/projects/DrF1/DrF1.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our on-going effort of constructing a large-scale benchmark for\nface forgery detection. The first version of this benchmark,\nDeeperForensics-1.0, represents the largest face forgery detection dataset by\nfar, with 60,000 videos constituted by a total of 17.6 million frames, 10 times\nlarger than existing datasets of the same kind. Extensive real-world\nperturbations are applied to obtain a more challenging benchmark of larger\nscale and higher diversity. All source videos in DeeperForensics-1.0 are\ncarefully collected, and fake videos are generated by a newly proposed\nend-to-end face swapping framework. The quality of generated videos outperforms\nthose in existing datasets, validated by user studies. The benchmark features a\nhidden test set, which contains manipulated videos achieving high deceptive\nscores in human evaluations. We further contribute a comprehensive study that\nevaluates five representative detection baselines and make a thorough analysis\nof different settings.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 14:37:17 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 11:24:04 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Jiang", "Liming", ""], ["Li", "Ren", ""], ["Wu", "Wayne", ""], ["Qian", "Chen", ""], ["Loy", "Chen Change", ""]]}, {"id": "2001.03032", "submitter": "Luca Ciampi", "authors": "Luca Ciampi, Nicola Messina, Fabrizio Falchi, Claudio Gennaro,\n  Giuseppe Amato", "title": "Virtual to Real adaptation of Pedestrian Detectors", "comments": null, "journal-ref": "Sensors 20.18 (2020): 5250", "doi": "10.3390/s20185250", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection through Computer Vision is a building block for a\nmultitude of applications. Recently, there was an increasing interest in\nConvolutional Neural Network-based architectures for the execution of such a\ntask. One of these supervised networks' critical goals is to generalize the\nknowledge learned during the training phase to new scenarios with different\ncharacteristics. A suitably labeled dataset is essential to achieve this\npurpose. The main problem is that manually annotating a dataset usually\nrequires a lot of human effort, and it is costly. To this end, we introduce\nViPeD (Virtual Pedestrian Dataset), a new synthetically generated set of images\ncollected with the highly photo-realistic graphical engine of the video game\nGTA V - Grand Theft Auto V, where annotations are automatically acquired.\nHowever, when training solely on the synthetic dataset, the model experiences a\nSynthetic2Real Domain Shift leading to a performance drop when applied to\nreal-world images. To mitigate this gap, we propose two different Domain\nAdaptation techniques suitable for the pedestrian detection task, but possibly\napplicable to general object detection. Experiments show that the network\ntrained with ViPeD can generalize over unseen real-world scenarios better than\nthe detector trained over real-world data, exploiting the variety of our\nsynthetic dataset. Furthermore, we demonstrate that with our Domain Adaptation\ntechniques, we can reduce the Synthetic2Real Domain Shift, making closer the\ntwo domains and obtaining a performance improvement when testing the network\nover the real-world images. The code, the models, and the dataset are made\nfreely available at https://ciampluca.github.io/viped/\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 14:50:11 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 08:47:12 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2020 14:14:19 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ciampi", "Luca", ""], ["Messina", "Nicola", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Amato", "Giuseppe", ""]]}, {"id": "2001.03063", "submitter": "Antigoni Tsiami Dr", "authors": "Antigoni Tsiami, Petros Koutras and Petros Maragos", "title": "STAViS: Spatio-Temporal AudioVisual Saliency Network", "comments": "CVPR 2020. Project page: https://github.com/atsiami/STAViS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce STAViS, a spatio-temporal audiovisual saliency network that\ncombines spatio-temporal visual and auditory information in order to\nefficiently address the problem of saliency estimation in videos. Our approach\nemploys a single network that combines visual saliency and auditory features\nand learns to appropriately localize sound sources and to fuse the two\nsaliencies in order to obtain a final saliency map. The network has been\ndesigned, trained end-to-end, and evaluated on six different databases that\ncontain audiovisual eye-tracking data of a large variety of videos. We compare\nour method against 8 different state-of-the-art visual saliency models.\nEvaluation results across databases indicate that our STAViS model outperforms\nour visual only variant as well as the other state-of-the-art models in the\nmajority of cases. Also, the consistently good performance it achieves for all\ndatabases indicates that it is appropriate for estimating saliency\n\"in-the-wild\". The code is available at https://github.com/atsiami/STAViS.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 15:34:04 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 18:45:08 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Tsiami", "Antigoni", ""], ["Koutras", "Petros", ""], ["Maragos", "Petros", ""]]}, {"id": "2001.03071", "submitter": "Chris Dulhanty", "authors": "Chris Dulhanty, Alexander Wong", "title": "Investigating the Impact of Inclusion in Face Recognition Training Data\n  on Individual Face Identification", "comments": "2020 AAAI/ACM Conference on AI, Ethics, and Society (AIES 20) | V2\n  updated latex character rendering issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern face recognition systems leverage datasets containing images of\nhundreds of thousands of specific individuals' faces to train deep\nconvolutional neural networks to learn an embedding space that maps an\narbitrary individual's face to a vector representation of their identity. The\nperformance of a face recognition system in face verification (1:1) and face\nidentification (1:N) tasks is directly related to the ability of an embedding\nspace to discriminate between identities. Recently, there has been significant\npublic scrutiny into the source and privacy implications of large-scale face\nrecognition training datasets such as MS-Celeb-1M and MegaFace, as many people\nare uncomfortable with their face being used to train dual-use technologies\nthat can enable mass surveillance. However, the impact of an individual's\ninclusion in training data on a derived system's ability to recognize them has\nnot previously been studied. In this work, we audit ArcFace, a\nstate-of-the-art, open source face recognition system, in a large-scale face\nidentification experiment with more than one million distractor images. We find\na Rank-1 face identification accuracy of 79.71% for individuals present in the\nmodel's training data and an accuracy of 75.73% for those not present. This\nmodest difference in accuracy demonstrates that face recognition systems using\ndeep learning work better for individuals they are trained on, which has\nserious privacy implications when one considers all major open source face\nrecognition training datasets do not obtain informed consent from individuals\nduring their collection.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 15:50:28 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 21:17:59 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Dulhanty", "Chris", ""], ["Wong", "Alexander", ""]]}, {"id": "2001.03102", "submitter": "Roy Miles", "authors": "Roy Miles, Krystian Mikolajczyk", "title": "Compression of descriptor models for mobile applications", "comments": "ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have demonstrated state-of-the-art performance for\nfeature-based image matching through the advent of new large and diverse\ndatasets. However, there has been little work on evaluating the computational\ncost, model size, and matching accuracy tradeoffs for these models. This paper\nexplicitly addresses these practical metrics by considering the\nstate-of-the-art HardNet model. We observe a significant redundancy in the\nlearned weights, which we exploit through the use of depthwise separable layers\nand an efficient Tucker decomposition. We demonstrate that a combination of\nthese methods is very effective, but still sacrifices the top-end accuracy. To\nresolve this, we propose the Convolution-Depthwise-Pointwise(CDP) layer, which\nprovides a means of interpolating between the standard and depthwise separable\nconvolutions. With this proposed layer, we can achieve an 8 times reduction in\nthe number of parameters on the HardNet model, 13 times reduction in the\ncomputational complexity, while sacrificing less than 1% on the overall\naccuracy across theHPatchesbenchmarks. To further demonstrate the\ngeneralisation of this approach, we apply it to the state-of-the-art SuperPoint\nmodel, where we can significantly reduce the number of parameters and\nfloating-point operations, with minimal degradation in the matching accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 17:00:21 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 20:37:33 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 10:41:09 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Miles", "Roy", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2001.03111", "submitter": "Qi Dou", "authors": "Qi Dou, Quande Liu, Pheng Ann Heng, Ben Glocker", "title": "Unpaired Multi-modal Segmentation via Knowledge Distillation", "comments": "IEEE TMI, code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal learning is typically performed with network architectures\ncontaining modality-specific layers and shared layers, utilizing co-registered\nimages of different modalities. We propose a novel learning scheme for unpaired\ncross-modality image segmentation, with a highly compact architecture achieving\nsuperior segmentation accuracy. In our method, we heavily reuse network\nparameters, by sharing all convolutional kernels across CT and MRI, and only\nemploy modality-specific internal normalization layers which compute respective\nstatistics. To effectively train such a highly compact model, we introduce a\nnovel loss term inspired by knowledge distillation, by explicitly constraining\nthe KL-divergence of our derived prediction distributions between modalities.\nWe have extensively validated our approach on two multi-class segmentation\nproblems: i) cardiac structure segmentation, and ii) abdominal organ\nsegmentation. Different network settings, i.e., 2D dilated network and 3D\nU-net, are utilized to investigate our method's general efficacy. Experimental\nresults on both tasks demonstrate that our novel multi-modal learning scheme\nconsistently outperforms single-modal training and previous multi-modal\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 20:03:17 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Dou", "Qi", ""], ["Liu", "Quande", ""], ["Heng", "Pheng Ann", ""], ["Glocker", "Ben", ""]]}, {"id": "2001.03113", "submitter": "Seyed Mehdi Iranmanesh", "authors": "Seyed Mehdi Iranmanesh, Ali Dabouei, Sobhan Soleymani, Hadi Kazemi,\n  Nasser M. Nasrabadi", "title": "Robust Facial Landmark Detection via Aggregation on Geometrically\n  Manipulated Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a practical approach to the problem of facial\nlandmark detection. The proposed method can deal with large shape and\nappearance variations under the rich shape deformation. To handle the shape\nvariations we equip our method with the aggregation of manipulated face images.\nThe proposed framework generates different manipulated faces using only one\ngiven face image. The approach utilizes the fact that small but carefully\ncrafted geometric manipulation in the input domain can fool deep face\nrecognition models. We propose three different approaches to generate\nmanipulated faces in which two of them perform the manipulations via\nadversarial attacks and the other one uses known transformations. Aggregating\nthe manipulated faces provides a more robust landmark detection approach which\nis able to capture more important deformations and variations of the face\nshapes. Our approach is demonstrated its superiority compared to the\nstate-of-the-art method on benchmark datasets AFLW, 300-W, and COFW.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 16:43:09 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Iranmanesh", "Seyed Mehdi", ""], ["Dabouei", "Ali", ""], ["Soleymani", "Sobhan", ""], ["Kazemi", "Hadi", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2001.03152", "submitter": "Krishna Kumar Singh", "authors": "Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee,\n  Matt Feiszli, Deepti Ghadiyaram", "title": "Don't Judge an Object by Its Context: Learning to Overcome Contextual\n  Bias", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing models often leverage co-occurrences between objects and their\ncontext to improve recognition accuracy. However, strongly relying on context\nrisks a model's generalizability, especially when typical co-occurrence\npatterns are absent. This work focuses on addressing such contextual biases to\nimprove the robustness of the learnt feature representations. Our goal is to\naccurately recognize a category in the absence of its context, without\ncompromising on performance when it co-occurs with context. Our key idea is to\ndecorrelate feature representations of a category from its co-occurring\ncontext. We achieve this by learning a feature subspace that explicitly\nrepresents categories occurring in the absence of context along side a joint\nfeature subspace that represents both categories and context. Our very simple\nyet effective method is extensible to two multi-label tasks -- object and\nattribute classification. On 4 challenging datasets, we demonstrate the\neffectiveness of our method in reducing contextual bias.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 18:31:55 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 23:20:53 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Singh", "Krishna Kumar", ""], ["Mahajan", "Dhruv", ""], ["Grauman", "Kristen", ""], ["Lee", "Yong Jae", ""], ["Feiszli", "Matt", ""], ["Ghadiyaram", "Deepti", ""]]}, {"id": "2001.03182", "submitter": "Yun-Chun Chen", "authors": "Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, Jia-Bin Huang", "title": "CrDoCo: Pixel-level Domain Transfer with Cross-Domain Consistency", "comments": "Code: https://github.com/YunChunChen/CrDoCo-pytorch Project:\n  https://yunchunchen.github.io/CrDoCo/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation algorithms aim to transfer the knowledge\nlearned from one domain to another (e.g., synthetic to real images). The\nadapted representations often do not capture pixel-level domain shifts that are\ncrucial for dense prediction tasks (e.g., semantic segmentation). In this\npaper, we present a novel pixel-wise adversarial domain adaptation algorithm.\nBy leveraging image-to-image translation methods for data augmentation, our key\ninsight is that while the translated images between domains may differ in\nstyles, their predictions for the task should be consistent. We exploit this\nproperty and introduce a cross-domain consistency loss that enforces our\nadapted model to produce consistent predictions. Through extensive experimental\nresults, we show that our method compares favorably against the\nstate-of-the-art on a wide variety of unsupervised domain adaptation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 19:00:35 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Chen", "Yun-Chun", ""], ["Lin", "Yen-Yu", ""], ["Yang", "Ming-Hsuan", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2001.03187", "submitter": "Jingru Yi", "authors": "Jingru Yi, Pengxiang Wu, Qiaoying Huang, Hui Qu, Dimitris N. Metaxas", "title": "Vertebra-Focused Landmark Detection for Scoliosis Assessment", "comments": "Accepted to ISBI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adolescent idiopathic scoliosis (AIS) is a lifetime disease that arises in\nchildren. Accurate estimation of Cobb angles of the scoliosis is essential for\nclinicians to make diagnosis and treatment decisions. The Cobb angles are\nmeasured according to the vertebrae landmarks. Existing regression-based\nmethods for the vertebra landmark detection typically suffer from large dense\nmapping parameters and inaccurate landmark localization. The segmentation-based\nmethods tend to predict connected or corrupted vertebra masks. In this paper,\nwe propose a novel vertebra-focused landmark detection method. Our model first\nlocalizes the vertebra centers, based on which it then traces the four corner\nlandmarks of the vertebra through the learned corner offset. In this way, our\nmethod is able to keep the order of the landmarks. The comparison results\ndemonstrate the merits of our method in both Cobb angle measurement and\nlandmark detection on low-contrast and ambiguous X-ray images. Code is\navailable at: \\url{https://github.com/yijingru/Vertebra-Landmark-Detection}.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 19:17:41 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Yi", "Jingru", ""], ["Wu", "Pengxiang", ""], ["Huang", "Qiaoying", ""], ["Qu", "Hui", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "2001.03194", "submitter": "Abdullah Rashwan", "authors": "Abdullah Rashwan, Rishav Agarwal, Agastya Kalra, and Pascal Poupart", "title": "MatrixNets: A New Scale and Aspect Ratio Aware Architecture for Object\n  Detection", "comments": "This is the full paper for arXiv:1908.04646 with more applications,\n  experiments, and ablation study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MatrixNets (xNets), a new deep architecture for object detection.\nxNets map objects with similar sizes and aspect ratios into many specialized\nlayers, allowing xNets to provide a scale and aspect ratio aware architecture.\nWe leverage xNets to enhance single-stage object detection frameworks. First,\nwe apply xNets on anchor-based object detection, for which we predict object\ncenters and regress the top-left and bottom-right corners. Second, we use\nMatrixNets for corner-based object detection by predicting top-left and\nbottom-right corners. Each corner predicts the center location of the object.\nWe also enhance corner-based detection by replacing the embedding layer with\ncenter regression. Our final architecture achieves mAP of 47.8 on MS COCO,\nwhich is higher than its CornerNet counterpart by +5.6 mAP while also closing\nthe gap between single-stage and two-stage detectors. The code is available at\nhttps://github.com/arashwan/matrixnet.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 19:32:22 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Rashwan", "Abdullah", ""], ["Agarwal", "Rishav", ""], ["Kalra", "Agastya", ""], ["Poupart", "Pascal", ""]]}, {"id": "2001.03205", "submitter": "William Beksi", "authors": "Aditya Rajguru, Christopher Collander, William J. Beksi", "title": "Camera-Based Adaptive Trajectory Guidance via Neural Networks", "comments": "To be published in the 2020 6th International Conference on\n  Mechatronics and Robotics Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel method to capture visual trajectories for\nnavigating an indoor robot in dynamic settings using streaming image data.\nFirst, an image processing pipeline is proposed to accurately segment\ntrajectories from noisy backgrounds. Next, the captured trajectories are used\nto design, train, and compare two neural network architectures for predicting\nacceleration and steering commands for a line following robot over a continuous\nspace in real time. Lastly, experimental results demonstrate the performance of\nthe neural networks versus human teleoperation of the robot and the viability\nof the system in environments with occlusions and/or low-light conditions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 20:05:25 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Rajguru", "Aditya", ""], ["Collander", "Christopher", ""], ["Beksi", "William J.", ""]]}, {"id": "2001.03233", "submitter": "Tim Tang", "authors": "Tim Y. Tang, Daniele De Martini, Dan Barnes, Paul Newman", "title": "RSL-Net: Localising in Satellite Images From a Radar on the Ground", "comments": "Accepted to IEEE Robotics and Automation Letters (RA-L)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about localising a vehicle in an overhead image using FMCW\nradar mounted on a ground vehicle. FMCW radar offers extraordinary promise and\nefficacy for vehicle localisation. It is impervious to all weather types and\nlighting conditions. However the complexity of the interactions between\nmillimetre radar wave and the physical environment makes it a challenging\ndomain. Infrastructure-free large-scale radar-based localisation is in its\ninfancy. Typically here a map is built and suitable techniques, compatible with\nthe nature of sensor, are brought to bear. In this work we eschew the need for\na radar-based map; instead we simply use an overhead image -- a resource\nreadily available everywhere. This paper introduces a method that not only\nnaturally deals with the complexity of the signal type but does so in the\ncontext of cross modal processing.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 21:53:24 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 19:45:16 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Tang", "Tim Y.", ""], ["De Martini", "Daniele", ""], ["Barnes", "Dan", ""], ["Newman", "Paul", ""]]}, {"id": "2001.03251", "submitter": "Shadrokh Samavi", "authors": "Mahnoosh Bagheri, Majid Mohrekesh, Nader Karimi, Shadrokh Samavi", "title": "Adaptive Control of Embedding Strength in Image Watermarking using\n  Neural Networks", "comments": "4 pages 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image watermarking has been widely used in different applications\nsuch as copyright protection of digital media, such as audio, image, and video\nfiles. Two opposing criteria of robustness and transparency are the goals of\nwatermarking methods. In this paper, we propose a framework for determining the\nappropriate embedding strength factor. The framework can use most DWT and DCT\nbased blind watermarking approaches. We use Mask R-CNN on the COCO dataset to\nfind a good strength factor for each sub-block. Experiments show that this\nmethod is robust against different attacks and has good transparency.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 23:08:34 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Bagheri", "Mahnoosh", ""], ["Mohrekesh", "Majid", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2001.03257", "submitter": "Liming Jiang", "authors": "Liming Jiang, Yuanchang Xie, Tianzhu Ren", "title": "A Deep Neural Networks Approach for Pixel-Level Runway Pavement Crack\n  Segmentation Using Drone-Captured Images", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pavement conditions are a critical aspect of asset management and directly\naffect safety. This study introduces a deep neural network method called U-Net\nfor pavement crack segmentation based on drone-captured images to reduce the\ncost and time needed for airport runway inspection. The proposed approach can\nalso be used for highway pavement conditions assessment during off-peak periods\nwhen there are few vehicles on the road. In this study, runway pavement images\nare collected using drone at various heights from the Fitchburg Municipal\nAirport (FMA) in Massachusetts to evaluate their quality and applicability for\ncrack segmentation, from which an optimal height is determined. Drone images\ncaptured at the optimal height are then used to evaluate the crack segmentation\nperformance of the U-Net model. Deep learning methods typically require a huge\nset of annotated training datasets for model development, which can be a major\nobstacle for their applications. An online annotated pavement image dataset is\nused together with the FMA data to train the U-Net model. The results show that\nU-Net performs well on the FMA testing data even with limited FMA training\nimages, suggesting that it has good generalization ability and great potential\nto be used for both airport runways and highway pavements.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 23:30:50 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Jiang", "Liming", ""], ["Xie", "Yuanchang", ""], ["Ren", "Tianzhu", ""]]}, {"id": "2001.03270", "submitter": "Shadrokh Samavi", "authors": "Ghazale Ghorbanzade, Zahra Nabizadeh, Nader Karimi, Shadrokh Samavi", "title": "Image Inpainting by Multiscale Spline Interpolation", "comments": "six pages and five figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the missing regions of an image is a task that is called image\ninpainting. Depending on the shape of missing areas, different methods are\npresented in the literature. One of the challenges of this problem is\nextracting features that lead to better results. Experimental results show that\nboth global and local features are useful for this purpose. In this paper, we\npropose a multi-scale image inpainting method that utilizes both local and\nglobal features. The first step of this method is to determine how many scales\nwe need to use, which depends on the width of the lines in the map of the\nmissing region. Then we apply adaptive image inpainting to the damaged areas of\nthe image, and the lost pixels are predicted. Each scale is inpainted and the\nresult is resized to the original size. Then a voting process produces the\nfinal result. The proposed method is tested on damaged images with scratches\nand creases. The metric that we use to evaluate our approach is PSNR. On\naverage, we achieved 1.2 dB improvement over some existing inpainting\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 01:15:14 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Ghorbanzade", "Ghazale", ""], ["Nabizadeh", "Zahra", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2001.03288", "submitter": "Juhyun Lee", "authors": "Yury Pisarchyk and Juhyun Lee", "title": "Efficient Memory Management for Deep Neural Net Inference", "comments": "6 pages, 6 figures, MLSys 2020 Workshop on Resource-Constrained\n  Machine Learning (ReCoML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural net inference was considered a task for servers only,\nlatest advances in technology allow the task of inference to be moved to mobile\nand embedded devices, desired for various reasons ranging from latency to\nprivacy. These devices are not only limited by their compute power and battery,\nbut also by their inferior physical memory and cache, and thus, an efficient\nmemory manager becomes a crucial component for deep neural net inference at the\nedge. We explore various strategies to smartly share memory buffers among\nintermediate tensors in deep neural nets. Employing these can result in up to\n11% smaller memory footprint than the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 02:45:41 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 01:21:43 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 02:32:54 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Pisarchyk", "Yury", ""], ["Lee", "Juhyun", ""]]}, {"id": "2001.03305", "submitter": "Rodney LaLonde Iii", "authors": "Rodney LaLonde, Pujan Kandel, Concetto Spampinato, Michael B. Wallace,\n  Ulas Bagci", "title": "Diagnosing Colorectal Polyps in the Wild with Capsule Networks", "comments": "Accepted for publication at ISBI 2020 (IEEE International Symposium\n  on Biomedical Imaging). Code is publicly available at\n  https://github.com/lalonderodney/D-Caps", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer, largely arising from precursor lesions called polyps,\nremains one of the leading causes of cancer-related death worldwide. Current\nclinical standards require the resection and histopathological analysis of\npolyps due to test accuracy and sensitivity of optical biopsy methods falling\nsubstantially below recommended levels. In this study, we design a novel\ncapsule network architecture (D-Caps) to improve the viability of optical\nbiopsy of colorectal polyps. Our proposed method introduces several technical\nnovelties including a novel capsule architecture with a capsule-average pooling\n(CAP) method to improve efficiency in large-scale image classification. We\ndemonstrate improved results over the previous state-of-the-art convolutional\nneural network (CNN) approach by as much as 43%. This work provides an\nimportant benchmark on the new Mayo Polyp dataset, a significantly more\nchallenging and larger dataset than previous polyp studies, with results\nstratified across all available categories, imaging devices and modalities, and\nfocus modes to promote future direction into AI-driven colorectal cancer\nscreening systems. Code is publicly available at\nhttps://github.com/lalonderodney/D-Caps .\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 04:55:01 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["LaLonde", "Rodney", ""], ["Kandel", "Pujan", ""], ["Spampinato", "Concetto", ""], ["Wallace", "Michael B.", ""], ["Bagci", "Ulas", ""]]}, {"id": "2001.03329", "submitter": "Kitsuchart Pasupa", "authors": "Kitsuchart Pasupa, Supawit Vatathanavaro, Suchat Tungjitnob", "title": "Convolutional Neural Networks based Focal Loss for Class Imbalance\n  Problem: A Case Study of Canine Red Blood Cells Morphology Classification", "comments": null, "journal-ref": "Journal of Ambient Intelligence and Humanized Computing (2020),\n  https://link.springer.com/article/10.1007/s12652-020-01773-x", "doi": "10.1007/s12652-020-01773-x", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphologies of red blood cells are normally interpreted by a pathologist. It\nis time-consuming and laborious. Furthermore, a misclassified red blood cell\nmorphology will lead to false disease diagnosis and improper treatment. Thus, a\ndecent pathologist must truly be an expert in classifying red blood cell\nmorphology. In the past decade, many approaches have been proposed for\nclassifying human red blood cell morphology. However, those approaches have not\naddressed the class imbalance problem in classification. A class imbalance\nproblem---a problem where the numbers of samples in classes are very\ndifferent---is one of the problems that can lead to a biased model towards the\nmajority class. Due to the rarity of every type of abnormal blood cell\nmorphology, the data from the collection process are usually imbalanced. In\nthis study, we aimed to solve this problem specifically for classification of\ndog red blood cell morphology by using a Convolutional Neural Network (CNN)---a\nwell-known deep learning technique---in conjunction with a focal loss function,\nadept at handling class imbalance problem. The proposed technique was conducted\non a well-designed framework: two different CNNs were used to verify the\neffectiveness of the focal loss function and the optimal hyper-parameters were\ndetermined by 5-fold cross-validation. The experimental results show that both\nCNNs models augmented with the focal loss function achieved higher\n$F_{1}$-scores, compared to the models augmented with a conventional\ncross-entropy loss function that does not address class imbalance problem. In\nother words, the focal loss function truly enabled the CNNs models to be less\nbiased towards the majority class than the cross-entropy did in the\nclassification task of imbalanced dog red blood cell data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 07:31:57 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Pasupa", "Kitsuchart", ""], ["Vatathanavaro", "Supawit", ""], ["Tungjitnob", "Suchat", ""]]}, {"id": "2001.03339", "submitter": "Shih-Han Chou", "authors": "Shih-Han Chou, Wei-Lun Chao, Wei-Sheng Lai, Min Sun, Ming-Hsuan Yang", "title": "Visual Question Answering on 360{\\deg} Images", "comments": "Accepted to WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce VQA 360, a novel task of visual question answering\non 360 images. Unlike a normal field-of-view image, a 360 image captures the\nentire visual content around the optical center of a camera, demanding more\nsophisticated spatial understanding and reasoning. To address this problem, we\ncollect the first VQA 360 dataset, containing around 17,000 real-world\nimage-question-answer triplets for a variety of question types. We then study\ntwo different VQA models on VQA 360, including one conventional model that\ntakes an equirectangular image (with intrinsic distortion) as input and one\ndedicated model that first projects a 360 image onto cubemaps and subsequently\naggregates the information from multiple spatial resolutions. We demonstrate\nthat the cubemap-based model with multi-level fusion and attention diffusion\nperforms favorably against other variants and the equirectangular-based models.\nNevertheless, the gap between the humans' and machines' performance reveals the\nneed for more advanced VQA 360 algorithms. We, therefore, expect our dataset\nand studies to serve as the benchmark for future development in this\nchallenging task. Dataset, code, and pre-trained models are available online.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 08:18:21 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Chou", "Shih-Han", ""], ["Chao", "Wei-Lun", ""], ["Lai", "Wei-Sheng", ""], ["Sun", "Min", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2001.03340", "submitter": "Matthias Weissenbacher", "authors": "Matthias Weissenbacher", "title": "Temporally Folded Convolutional Neural Networks for Sequence Forecasting", "comments": "8 pages, 4 figures, submitted to IJCAI 2020 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel approach to utilize convolutional neural\nnetworks for time series forecasting. The time direction of the sequential data\nwith spatial dimensions $D=1,2$ is considered democratically as the input of a\nspatiotemporal $(D+1)$-dimensional convolutional neural network. Latter then\nreduces the data stream from $D +1 \\to D$ dimensions followed by an\nincriminator cell which uses this information to forecast the subsequent time\nstep. We empirically compare this strategy to convolutional LSTM's and LSTM's\non their performance on the sequential MNIST and the JSB chorals dataset,\nrespectively. We conclude that temporally folded convolutional neural networks\n(TFC's) may outperform the conventional recurrent strategies.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 08:18:39 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Weissenbacher", "Matthias", ""]]}, {"id": "2001.03343", "submitter": "Peixuan Li", "authors": "Peixuan Li, Huaici Zhao, Pengfei Liu, Feidao Cao", "title": "RTM3D: Real-time Monocular 3D Detection from Object Keypoints for\n  Autonomous Driving", "comments": "11 pages, 4 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an efficient and accurate monocular 3D detection\nframework in single shot. Most successful 3D detectors take the projection\nconstraint from the 3D bounding box to the 2D box as an important component.\nFour edges of a 2D box provide only four constraints and the performance\ndeteriorates dramatically with the small error of the 2D detector. Different\nfrom these approaches, our method predicts the nine perspective keypoints of a\n3D bounding box in image space, and then utilize the geometric relationship of\n3D and 2D perspectives to recover the dimension, location, and orientation in\n3D space. In this method, the properties of the object can be predicted stably\neven when the estimation of keypoints is very noisy, which enables us to obtain\nfast detection speed with a small architecture. Training our method only uses\nthe 3D properties of the object without the need for external networks or\nsupervision data. Our method is the first real-time system for monocular image\n3D detection while achieves state-of-the-art performance on the KITTI\nbenchmark. Code will be released at https://github.com/Banconxuan/RTM3D.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 08:29:20 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Li", "Peixuan", ""], ["Zhao", "Huaici", ""], ["Liu", "Pengfei", ""], ["Cao", "Feidao", ""]]}, {"id": "2001.03360", "submitter": "Junyu Gao", "authors": "Qi Wang, Junyu Gao, Wei Lin, Xuelong Li", "title": "NWPU-Crowd: A Large-Scale Benchmark for Crowd Counting and Localization", "comments": "Accepted by T-PAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3013269", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, crowd counting and localization attract much attention of\nresearchers due to its wide-spread applications, including crowd monitoring,\npublic safety, space design, etc. Many Convolutional Neural Networks (CNN) are\ndesigned for tackling this task. However, currently released datasets are so\nsmall-scale that they can not meet the needs of the supervised CNN-based\nalgorithms. To remedy this problem, we construct a large-scale congested crowd\ncounting and localization dataset, NWPU-Crowd, consisting of 5,109 images, in a\ntotal of 2,133,375 annotated heads with points and boxes. Compared with other\nreal-world datasets, it contains various illumination scenes and has the\nlargest density range (0~20,033). Besides, a benchmark website is developed for\nimpartially evaluating the different methods, which allows researchers to\nsubmit the results of the test set. Based on the proposed dataset, we further\ndescribe the data characteristics, evaluate the performance of some mainstream\nstate-of-the-art (SOTA) methods, and analyze the new problems that arise on the\nnew data. What's more, the benchmark is deployed at\n\\url{https://www.crowdbenchmark.com/}, and the dataset/code/models/results are\navailable at \\url{https://gjy3035.github.io/NWPU-Crowd-Sample-Code/}.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 09:26:04 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 11:55:30 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 02:13:47 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2020 00:56:49 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wang", "Qi", ""], ["Gao", "Junyu", ""], ["Lin", "Wei", ""], ["Li", "Xuelong", ""]]}, {"id": "2001.03376", "submitter": "Gon\\c{c}alo Mordido", "authors": "Gon\\c{c}alo Mordido, Haojin Yang, Christoph Meinel", "title": "microbatchGAN: Stimulating Diversity with Multi-Adversarial\n  Discrimination", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to tackle the mode collapse problem in generative adversarial\nnetworks (GANs) by using multiple discriminators and assigning a different\nportion of each minibatch, called microbatch, to each discriminator. We\ngradually change each discriminator's task from distinguishing between real and\nfake samples to discriminating samples coming from inside or outside its\nassigned microbatch by using a diversity parameter $\\alpha$. The generator is\nthen forced to promote variety in each minibatch to make the microbatch\ndiscrimination harder to achieve by each discriminator. Thus, all models in our\nframework benefit from having variety in the generated set to reduce their\nrespective losses. We show evidence that our solution promotes sample diversity\nsince early training stages on multiple datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 10:31:27 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Mordido", "Gon\u00e7alo", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "2001.03390", "submitter": "Aleksandr Koryagin", "authors": "Alexander Koryagin, Darima Mylzenova, Roman Khudorozhkov, Sergey\n  Tsimfer", "title": "Seismic horizon detection with neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, Convolutional Neural Networks (CNNs) were\nsuccessfully adopted in numerous domains to solve various image-related tasks,\nranging from simple classification to fine borders annotation. Tracking seismic\nhorizons is no different, and there are a lot of papers proposing the usage of\nsuch models to avoid time-consuming hand-picking. Unfortunately, most of them\nare (i) either trained on synthetic data, which can't fully represent the\ncomplexity of subterranean structures, (ii) trained and tested on the same\ncube, or (iii) lack reproducibility and precise descriptions of the\nmodel-building process. With all that in mind, the main contribution of this\npaper is an open-sourced research of applying binary segmentation approach to\nthe task of horizon detection on multiple real seismic cubes with a focus on\ninter-cube generalization of the predictive model.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 11:30:50 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Koryagin", "Alexander", ""], ["Mylzenova", "Darima", ""], ["Khudorozhkov", "Roman", ""], ["Tsimfer", "Sergey", ""]]}, {"id": "2001.03398", "submitter": "Yilun Chen", "authors": "Yilun Chen, Shu Liu, Xiaoyong Shen, Jiaya Jia", "title": "DSGN: Deep Stereo Geometry Network for 3D Object Detection", "comments": "Accepted by CVPR 2020 (Camera Ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art 3D object detectors heavily rely on LiDAR sensors\nbecause there is a large performance gap between image-based and LiDAR-based\nmethods. It is caused by the way to form representation for the prediction in\n3D scenarios. Our method, called Deep Stereo Geometry Network (DSGN),\nsignificantly reduces this gap by detecting 3D objects on a differentiable\nvolumetric representation -- 3D geometric volume, which effectively encodes 3D\ngeometric structure for 3D regular space. With this representation, we learn\ndepth information and semantic cues simultaneously. For the first time, we\nprovide a simple and effective one-stage stereo-based 3D detection pipeline\nthat jointly estimates the depth and detects 3D objects in an end-to-end\nlearning manner. Our approach outperforms previous stereo-based 3D detectors\n(about 10 higher in terms of AP) and even achieves comparable performance with\nseveral LiDAR-based methods on the KITTI 3D object detection leaderboard. Our\ncode is publicly available at https://github.com/chenyilun95/DSGN.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 11:44:37 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 03:20:30 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 03:30:28 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Chen", "Yilun", ""], ["Liu", "Shu", ""], ["Shen", "Xiaoyong", ""], ["Jia", "Jiaya", ""]]}, {"id": "2001.03444", "submitter": "Gustav Grund Pihlgren", "authors": "Gustav Grund Pihlgren (1), Fredrik Sandin (1), Marcus Liwicki (1) ((1)\n  Lule\\r{a} University of Technology)", "title": "Improving Image Autoencoder Embeddings with Perceptual Loss", "comments": "Accepted at IJCNN/WCCI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders are commonly trained using element-wise loss. However,\nelement-wise loss disregards high-level structures in the image which can lead\nto embeddings that disregard them as well. A recent improvement to autoencoders\nthat helps alleviate this problem is the use of perceptual loss. This work\ninvestigates perceptual loss from the perspective of encoder embeddings\nthemselves. Autoencoders are trained to embed images from three different\ncomputer vision datasets using perceptual loss based on a pretrained model as\nwell as pixel-wise loss. A host of different predictors are trained to perform\nobject positioning and classification on the datasets given the embedded images\nas input. The two kinds of losses are evaluated by comparing how the predictors\nperformed with embeddings from the differently trained autoencoders. The\nresults show that, in the image domain, the embeddings generated by\nautoencoders trained with perceptual loss enable more accurate predictions than\nthose trained with element-wise loss. Furthermore, the results show that, on\nthe task of object positioning of a small-scale feature, perceptual loss can\nimprove the results by a factor 10. The experimental setup is available online:\nhttps://github.com/guspih/Perceptual-Autoencoders\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 13:48:09 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 09:39:35 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Pihlgren", "Gustav Grund", ""], ["Sandin", "Fredrik", ""], ["Liwicki", "Marcus", ""]]}, {"id": "2001.03455", "submitter": "Marco Cannici", "authors": "Marco Cannici, Marco Ciccone, Andrea Romanoni, Matteo Matteucci", "title": "A Differentiable Recurrent Surface for Asynchronous Event-Based Data", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Vision Sensors (DVSs) asynchronously stream events in correspondence\nof pixels subject to brightness changes. Differently from classic vision\ndevices, they produce a sparse representation of the scene. Therefore, to apply\nstandard computer vision algorithms, events need to be integrated into a frame\nor event-surface. This is usually attained through hand-crafted grids that\nreconstruct the frame using ad-hoc heuristics. In this paper, we propose\nMatrix-LSTM, a grid of Long Short-Term Memory (LSTM) cells that efficiently\nprocess events and learn end-to-end task-dependent event-surfaces. Compared to\nexisting reconstruction approaches, our learned event-surface shows good\nflexibility and expressiveness on optical flow estimation on the MVSEC\nbenchmark and it improves the state-of-the-art of event-based object\nclassification on the N-Cars dataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 14:09:40 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 08:56:34 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Cannici", "Marco", ""], ["Ciccone", "Marco", ""], ["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "2001.03460", "submitter": "Dou Yan Liu Goodman", "authors": "Dou Goodman", "title": "Transferability of Adversarial Examples to Attack Cloud-based Image\n  Classifier Service", "comments": "Accepted by Defcon China 2019. arXiv admin note: substantial text\n  overlap with arXiv:1906.07997", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Deep Learning(DL) techniques have been extensively deployed\nfor computer vision tasks, particularly visual classification problems, where\nnew algorithms reported to achieve or even surpass the human performance. While\nmany recent works demonstrated that DL models are vulnerable to adversarial\nexamples. Fortunately, generating adversarial examples usually requires\nwhite-box access to the victim model, and real-world cloud-based image\nclassification services are more complex than white-box classifier,the\narchitecture and parameters of DL models on cloud platforms cannot be obtained\nby the attacker. The attacker can only access the APIs opened by cloud\nplatforms. Thus, keeping models in the cloud can usually give a (false) sense\nof security. In this paper, we mainly focus on studying the security of\nreal-world cloud-based image classification services. Specifically, (1) We\npropose a novel attack method, Fast Featuremap Loss PGD (FFL-PGD) attack based\non Substitution model, which achieves a high bypass rate with a very limited\nnumber of queries. Instead of millions of queries in previous studies, our\nmethod finds the adversarial examples using only two queries per image; and (2)\nwe make the first attempt to conduct an extensive empirical study of black-box\nattacks against real-world cloud-based classification services. Through\nevaluations on four popular cloud platforms including Amazon, Google,\nMicrosoft, Clarifai, we demonstrate that FFL-PGD attack has a success rate over\n90\\% among different classification services. (3) We discuss the possible\ndefenses to address these security challenges in cloud-based classification\nservices. Our defense technology is mainly divided into model training stage\nand image preprocessing stage.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 23:03:35 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 08:24:48 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 02:07:37 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Goodman", "Dou", ""]]}, {"id": "2001.03463", "submitter": "Ronak Gupta", "authors": "Ronak Gupta, Prashant Anand, Santanu Chaudhury, Brejesh Lall, Sanjay\n  Singh", "title": "Compressive sensing based privacy for fall detection", "comments": "accepted in NCVPRIPG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fall detection holds immense importance in the field of healthcare, where\ntimely detection allows for instant medical assistance. In this context, we\npropose a 3D ConvNet architecture which consists of 3D Inception modules for\nfall detection. The proposed architecture is a custom version of Inflated 3D\n(I3D) architecture, that takes compressed measurements of video sequence as\nspatio-temporal input, obtained from compressive sensing framework, rather than\nvideo sequence as input, as in the case of I3D convolutional neural network.\nThis is adopted since privacy raises a huge concern for patients being\nmonitored through these RGB cameras. The proposed framework for fall detection\nis flexible enough with respect to a wide variety of measurement matrices. Ten\naction classes randomly selected from Kinetics-400 with no fall examples, are\nemployed to train our 3D ConvNet post compressive sensing with different types\nof sensing matrices on the original video clips. Our results show that 3D\nConvNet performance remains unchanged with different sensing matrices. Also,\nthe performance obtained with Kinetics pre-trained 3D ConvNet on compressively\nsensed fall videos from benchmark datasets is better than the state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 14:26:03 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Gupta", "Ronak", ""], ["Anand", "Prashant", ""], ["Chaudhury", "Santanu", ""], ["Lall", "Brejesh", ""], ["Singh", "Sanjay", ""]]}, {"id": "2001.03483", "submitter": "Steve Dias Da Cruz", "authors": "Steve Dias Da Cruz, Oliver Wasenm\\\"uller, Hans-Peter Beise, Thomas\n  Stifter, Didier Stricker", "title": "SVIRO: Synthetic Vehicle Interior Rear Seat Occupancy Dataset and\n  Benchmark", "comments": "This paper is accepted at IEEE Winter Conference on Applications of\n  Computer Vision (WACV), 2020. Supplementary material is available under\n  https://sviro.kl.dfki.de/downloads/papers/wacv_supplementary.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We release SVIRO, a synthetic dataset for sceneries in the passenger\ncompartment of ten different vehicles, in order to analyze machine\nlearning-based approaches for their generalization capacities and reliability\nwhen trained on a limited number of variations (e.g. identical backgrounds and\ntextures, few instances per class). This is in contrast to the intrinsically\nhigh variability of common benchmark datasets, which focus on improving the\nstate-of-the-art of general tasks. Our dataset contains bounding boxes for\nobject detection, instance segmentation masks, keypoints for pose estimation\nand depth images for each synthetic scenery as well as images for each\nindividual seat for classification. The advantage of our use-case is twofold:\nThe proximity to a realistic application to benchmark new approaches under\nnovel circumstances while reducing the complexity to a more tractable\nenvironment, such that applications and theoretical questions can be tested on\na more challenging dataset as toy problems. The data and evaluation server are\navailable under https://sviro.kl.dfki.de.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 14:44:23 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Da Cruz", "Steve Dias", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Beise", "Hans-Peter", ""], ["Stifter", "Thomas", ""], ["Stricker", "Didier", ""]]}, {"id": "2001.03509", "submitter": "Roland Haase", "authors": "Roland Haase, Stefan Heldmann, Jan Lellmann", "title": "Deformable Groupwise Image Registration using Low-Rank and Sparse\n  Decomposition", "comments": "22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank and sparse decompositions and robust PCA (RPCA) are highly\nsuccessful techniques in image processing and have recently found use in\ngroupwise image registration. In this paper, we investigate the drawbacks of\nthe most common RPCA-dissimi\\-larity metric in image registration and derive an\nimproved version. In particular, this new metric models low-rank requirements\nthrough explicit constraints instead of penalties and thus avoids the pitfalls\nof the established metric. Equipped with total variation regularization, we\npresent a theoretically justified multilevel scheme based on first-order\nprimal-dual optimization to solve the resulting non-parametric registration\nproblem. As confirmed by numerical experiments, our metric especially lends\nitself to data involving recurring changes in object appearance and potential\nsparse perturbations. We numerically compare its peformance to a number of\nrelated approaches.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 15:25:36 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Haase", "Roland", ""], ["Heldmann", "Stefan", ""], ["Lellmann", "Jan", ""]]}, {"id": "2001.03535", "submitter": "Pengfei Xu", "authors": "Pengfei Xu, Xiaofan Zhang, Cong Hao, Yang Zhao, Yongan Zhang, Yue\n  Wang, Chaojian Li, Zetong Guan, Deming Chen, Yingyan Lin", "title": "AutoDNNchip: An Automated DNN Chip Predictor and Builder for Both FPGAs\n  and ASICs", "comments": "Accepted by 28th ACM/SIGDA International Symposium on\n  Field-Programmable Gate Arrays (FPGA'2020)", "journal-ref": null, "doi": "10.1145/3373087.3375306", "report-no": null, "categories": "cs.DC cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in Deep Neural Networks (DNNs) have fueled a growing\ndemand for DNN chips. However, designing DNN chips is non-trivial because: (1)\nmainstream DNNs have millions of parameters and operations; (2) the large\ndesign space due to the numerous design choices of dataflows, processing\nelements, memory hierarchy, etc.; and (3) an algorithm/hardware co-design is\nneeded to allow the same DNN functionality to have a different decomposition,\nwhich would require different hardware IPs to meet the application\nspecifications. Therefore, DNN chips take a long time to design and require\ncross-disciplinary experts. To enable fast and effective DNN chip design, we\npropose AutoDNNchip - a DNN chip generator that can automatically generate both\nFPGA- and ASIC-based DNN chip implementation given DNNs from machine learning\nframeworks (e.g., PyTorch) for a designated application and dataset.\nSpecifically, AutoDNNchip consists of two integrated enablers: (1) a Chip\nPredictor, built on top of a graph-based accelerator representation, which can\naccurately and efficiently predict a DNN accelerator's energy, throughput, and\narea based on the DNN model parameters, hardware configuration,\ntechnology-based IPs, and platform constraints; and (2) a Chip Builder, which\ncan automatically explore the design space of DNN chips (including IP\nselection, block configuration, resource balancing, etc.), optimize chip design\nvia the Chip Predictor, and then generate optimized synthesizable RTL to\nachieve the target design metrics. Experimental results show that our Chip\nPredictor's predicted performance differs from real-measured ones by < 10% when\nvalidated using 15 DNN models and 4 platforms (edge-FPGA/TPU/GPU and ASIC).\nFurthermore, accelerators generated by our AutoDNNchip can achieve better (up\nto 3.86X improvement) performance than that of expert-crafted state-of-the-art\naccelerators.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 05:32:15 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 17:02:51 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2020 18:53:08 GMT"}, {"version": "v4", "created": "Wed, 10 Jun 2020 23:50:57 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Xu", "Pengfei", ""], ["Zhang", "Xiaofan", ""], ["Hao", "Cong", ""], ["Zhao", "Yang", ""], ["Zhang", "Yongan", ""], ["Wang", "Yue", ""], ["Li", "Chaojian", ""], ["Guan", "Zetong", ""], ["Chen", "Deming", ""], ["Lin", "Yingyan", ""]]}, {"id": "2001.03554", "submitter": "Mathilde Caron", "authors": "Mathilde Caron, Ari Morcos, Piotr Bojanowski, Julien Mairal and Armand\n  Joulin", "title": "Pruning Convolutional Neural Networks with Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks trained without supervision come close to\nmatching performance with supervised pre-training, but sometimes at the cost of\nan even higher number of parameters. Extracting subnetworks from these large\nunsupervised convnets with preserved performance is of particular interest to\nmake them less computationally intensive. Typical pruning methods operate\nduring training on a task while trying to maintain the performance of the\npruned network on the same task. However, in self-supervised feature learning,\nthe training objective is agnostic on the representation transferability to\ndownstream tasks. Thus, preserving performance for this objective does not\nensure that the pruned subnetwork remains effective for solving downstream\ntasks. In this work, we investigate the use of standard pruning methods,\ndeveloped primarily for supervised learning, for networks trained without\nlabels (i.e. on self-supervised tasks). We show that pruned masks obtained with\nor without labels reach comparable performance when re-trained on labels,\nsuggesting that pruning operates similarly for self-supervised and supervised\nlearning. Interestingly, we also find that pruning preserves the transfer\nperformance of self-supervised subnetwork representations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 16:44:41 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Caron", "Mathilde", ""], ["Morcos", "Ari", ""], ["Bojanowski", "Piotr", ""], ["Mairal", "Julien", ""], ["Joulin", "Armand", ""]]}, {"id": "2001.03569", "submitter": "Wenhan Yang", "authors": "Ling-Yu Duan, Jiaying Liu, Wenhan Yang, Tiejun Huang, Wen Gao", "title": "Video Coding for Machines: A Paradigm of Collaborative Compression and\n  Intelligent Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video coding, which targets to compress and reconstruct the whole frame, and\nfeature compression, which only preserves and transmits the most critical\ninformation, stand at two ends of the scale. That is, one is with compactness\nand efficiency to serve for machine vision, and the other is with full\nfidelity, bowing to human perception. The recent endeavors in imminent trends\nof video compression, e.g. deep learning based coding tools and end-to-end\nimage/video coding, and MPEG-7 compact feature descriptor standards, i.e.\nCompact Descriptors for Visual Search and Compact Descriptors for Video\nAnalysis, promote the sustainable and fast development in their own directions,\nrespectively. In this paper, thanks to booming AI technology, e.g. prediction\nand generation models, we carry out exploration in the new area, Video Coding\nfor Machines (VCM), arising from the emerging MPEG standardization efforts1.\nTowards collaborative compression and intelligent analytics, VCM attempts to\nbridge the gap between feature coding for machine vision and video coding for\nhuman vision. Aligning with the rising Analyze then Compress instance Digital\nRetina, the definition, formulation, and paradigm of VCM are given first.\nMeanwhile, we systematically review state-of-the-art techniques in video\ncompression and feature compression from the unique perspective of MPEG\nstandardization, which provides the academic and industrial evidence to realize\nthe collaborative compression of video and feature streams in a broad range of\nAI applications. Finally, we come up with potential VCM solutions, and the\npreliminary results have demonstrated the performance and efficiency gains.\nFurther direction is discussed as well.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 17:24:13 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 16:03:58 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Duan", "Ling-Yu", ""], ["Liu", "Jiaying", ""], ["Yang", "Wenhan", ""], ["Huang", "Tiejun", ""], ["Gao", "Wen", ""]]}, {"id": "2001.03615", "submitter": "Huaizu Jiang", "authors": "Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller,\n  Xinlei Chen", "title": "In Defense of Grid Features for Visual Question Answering", "comments": null, "journal-ref": "CVPR, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popularized as 'bottom-up' attention, bounding box (or region) based visual\nfeatures have recently surpassed vanilla grid-based convolutional features as\nthe de facto standard for vision and language tasks like visual question\nanswering (VQA). However, it is not clear whether the advantages of regions\n(e.g. better localization) are the key reasons for the success of bottom-up\nattention. In this paper, we revisit grid features for VQA, and find they can\nwork surprisingly well - running more than an order of magnitude faster with\nthe same accuracy (e.g. if pre-trained in a similar fashion). Through extensive\nexperiments, we verify that this observation holds true across different VQA\nmodels (reporting a state-of-the-art accuracy on VQA 2.0 test-std, 72.71),\ndatasets, and generalizes well to other tasks like image captioning. As grid\nfeatures make the model design and training process much simpler, this enables\nus to train them end-to-end and also use a more flexible network design. We\nlearn VQA models end-to-end, from pixels directly to answers, and show that\nstrong performance is achievable without using any region annotations in\npre-training. We hope our findings help further improve the scientific\nunderstanding and the practical application of VQA. Code and features will be\nmade available.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 18:59:13 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 19:36:27 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Jiang", "Huaizu", ""], ["Misra", "Ishan", ""], ["Rohrbach", "Marcus", ""], ["Learned-Miller", "Erik", ""], ["Chen", "Xinlei", ""]]}, {"id": "2001.03637", "submitter": "Daniel Ruiz", "authors": "Daniel V. Ruiz, Gabriel Salomon, Eduardo Todt", "title": "Can Giraffes Become Birds? An Evaluation of Image-to-image Translation\n  for Data Generation", "comments": "Accepted for presentation at the Computer on the Beach (COTB'20) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in image-to-image translation with\napplications ranging from generating maps from satellite images to creating\nentire clothes' images from only contours. In the present work, we investigate\nimage-to-image translation using Generative Adversarial Networks (GANs) for\ngenerating new data, taking as a case study the morphing of giraffes images\ninto bird images. Morphing a giraffe into a bird is a challenging task, as they\nhave different scales, textures, and morphology. An unsupervised cross-domain\ntranslator entitled InstaGAN was trained on giraffes and birds, along with\ntheir respective masks, to learn translation between both domains. A dataset of\nsynthetic bird images was generated using translation from originally giraffe\nimages while preserving the original spatial arrangement and background. It is\nimportant to stress that the generated birds do not exist, being only the\nresult of a latent representation learned by InstaGAN. Two subsets of common\nliterature datasets were used for training the GAN and generating the\ntranslated images: COCO and Caltech-UCSD Birds 200-2011. To evaluate the\nrealness and quality of the generated images and masks, qualitative and\nquantitative analyses were made. For the quantitative analysis, a pre-trained\nMask R-CNN was used for the detection and segmentation of birds on Pascal VOC,\nCaltech-UCSD Birds 200-2011, and our new dataset entitled FakeSet. The\ngenerated dataset achieved detection and segmentation results close to the real\ndatasets, suggesting that the generated images are realistic enough to be\ndetected and segmented by a state-of-the-art deep neural network.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 19:29:11 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 03:25:39 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Ruiz", "Daniel V.", ""], ["Salomon", "Gabriel", ""], ["Todt", "Eduardo", ""]]}, {"id": "2001.03659", "submitter": "Alex Ter-Sarkisov", "authors": "Aram Ter-Sarkisov", "title": "Network of Steel: Neural Font Style Transfer from Heavy Metal to\n  Corporate Logos", "comments": "Accepted for oral presentation at ICPRAM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for transferring style from the logos of heavy metal\nbands onto corporate logos using a VGG16 network. We establish the contribution\nof different layers and loss coefficients to the learning of style,\nminimization of artefacts and maintenance of readability of corporate logos. We\nfind layers and loss coefficients that produce a good tradeoff between heavy\nmetal style and corporate logo readability. This is the first step both towards\nsparse font style transfer and corporate logo decoration using generative\nnetworks. Heavy metal and corporate logos are very different artistically, in\nthe way they emphasize emotions and readability, therefore training a model to\nfuse the two is an interesting problem.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 20:41:15 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Ter-Sarkisov", "Aram", ""]]}, {"id": "2001.03671", "submitter": "Harsh Mehta", "authors": "Harsh Mehta, Yoav Artzi, Jason Baldridge, Eugene Ie, Piotr Mirowski", "title": "Retouchdown: Adding Touchdown to StreetLearn as a Shareable Resource for\n  Language Grounding Tasks in Street View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Touchdown dataset (Chen et al., 2019) provides instructions by human\nannotators for navigation through New York City streets and for resolving\nspatial descriptions at a given location. To enable the wider research\ncommunity to work effectively with the Touchdown tasks, we are publicly\nreleasing the 29k raw Street View panoramas needed for Touchdown. We follow the\nprocess used for the StreetLearn data release (Mirowski et al., 2019) to check\npanoramas for personally identifiable information and blur them as necessary.\nThese have been added to the StreetLearn dataset and can be obtained via the\nsame process as used previously for StreetLearn. We also provide a reference\nimplementation for both of the Touchdown tasks: vision and language navigation\n(VLN) and spatial description resolution (SDR). We compare our model results to\nthose given in Chen et al. (2019) and show that the panoramas we have added to\nStreetLearn fully support both Touchdown tasks and can be used effectively for\nfurther research and comparison.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 21:35:28 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Mehta", "Harsh", ""], ["Artzi", "Yoav", ""], ["Baldridge", "Jason", ""], ["Ie", "Eugene", ""], ["Mirowski", "Piotr", ""]]}, {"id": "2001.03674", "submitter": "Manpreet Singh Minhas", "authors": "Manpreet Singh Minhas, John Zelek", "title": "Semi-supervised Anomaly Detection using AutoEncoders", "comments": null, "journal-ref": "JCVIS, vol. 5, no. 1, p. 3, Jan. 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection refers to the task of finding unusual instances that stand\nout from the normal data. In several applications, these outliers or anomalous\ninstances are of greater interest compared to the normal ones. Specifically in\nthe case of industrial optical inspection and infrastructure asset management,\nfinding these defects (anomalous regions) is of extreme importance.\nTraditionally and even today this process has been carried out manually. Humans\nrely on the saliency of the defects in comparison to the normal texture to\ndetect the defects. However, manual inspection is slow, tedious, subjective and\nsusceptible to human biases. Therefore, the automation of defect detection is\ndesirable. But for defect detection lack of availability of a large number of\nanomalous instances and labelled data is a problem. In this paper, we present a\nconvolutional auto-encoder architecture for anomaly detection that is trained\nonly on the defect-free (normal) instances. For the test images, residual masks\nthat are obtained by subtracting the original image from the auto-encoder\noutput are thresholded to obtain the defect segmentation masks. The approach\nwas tested on two data-sets and achieved an impressive average F1 score of\n0.885. The network learnt to detect the actual shape of the defects even though\nno defected images were used during the training.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 23:06:28 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Minhas", "Manpreet Singh", ""], ["Zelek", "John", ""]]}, {"id": "2001.03676", "submitter": "Markus Hiller", "authors": "Markus Hiller, Chen Qiu, Florian Particke, Christian Hofmann and\n  J\\\"orn Thielecke", "title": "Learning Topometric Semantic Maps from Occupancy Grids", "comments": "Presented at the 2019 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's mobile robots are expected to operate in complex environments they\nshare with humans. To allow intuitive human-robot collaboration, robots require\na human-like understanding of their surroundings in terms of semantically\nclassified instances. In this paper, we propose a new approach for deriving\nsuch instance-based semantic maps purely from occupancy grids. We employ a\ncombination of deep learning techniques to detect, segment and extract door\nhypotheses from a random-sized map. The extraction is followed by a\npost-processing chain to further increase the accuracy of our approach, as well\nas place categorization for the three classes room, door and corridor. All\ndetected and classified entities are described as instances specified in a\ncommon coordinate system, while a topological map is derived to capture their\nspatial links. To train our two neural networks used for detection and map\nsegmentation, we contribute a simulator that automatically creates and\nannotates the required training data. We further provide insight into which\nfeatures are learned to detect doorways, and how the simulated training data\ncan be augmented to train networks for the direct application on real-world\ngrid maps. We evaluate our approach on several publicly available real-world\ndata sets. Even though the used networks are solely trained on simulated data,\nour approach demonstrates high robustness and effectiveness in various\nreal-world indoor environments.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 22:06:10 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Hiller", "Markus", ""], ["Qiu", "Chen", ""], ["Particke", "Florian", ""], ["Hofmann", "Christian", ""], ["Thielecke", "J\u00f6rn", ""]]}, {"id": "2001.03690", "submitter": "Jong Chul Ye", "authors": "Byung-Hoon Kim and Jong Chul Ye", "title": "Understanding Graph Isomorphism Network for rs-fMRI Functional\n  Connectivity Analysis", "comments": "This paper is accepted for Frontiers in Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNN) rely on graph operations that include neural\nnetwork training for various graph related tasks. Recently, several attempts\nhave been made to apply the GNNs to functional magnetic resonance image (fMRI)\ndata. Despite recent progresses, a common limitation is its difficulty to\nexplain the classification results in a neuroscientifically explainable way.\nHere, we develop a framework for analyzing the fMRI data using the Graph\nIsomorphism Network (GIN), which was recently proposed as a powerful GNN for\ngraph classification. One of the important contributions of this paper is the\nobservation that the GIN is a dual representation of convolutional neural\nnetwork (CNN) in the graph space where the shift operation is defined using the\nadjacency matrix. This understanding enables us to exploit CNN-based saliency\nmap techniques for the GNN, which we tailor to the proposed GIN with one-hot\nencoding, to visualize the important regions of the brain. We validate our\nproposed framework using large-scale resting-state fMRI (rs-fMRI) data for\nclassifying the sex of the subject based on the graph structure of the brain.\nThe experiment was consistent with our expectation such that the obtained\nsaliency map show high correspondence with previous neuroimaging evidences\nrelated to sex differences.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 23:40:09 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 02:53:52 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kim", "Byung-Hoon", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2001.03698", "submitter": "Dongsheng An", "authors": "Dongsheng An, Yang Guo, Min Zhang, Xin Qi, Na Lei, Shing-Tung Yau, and\n  Xianfeng Gu", "title": "AE-OT-GAN: Training GANs from data specific latent distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though generative adversarial networks (GANs) areprominent models to generate\nrealistic and crisp images,they often encounter the mode collapse problems and\narehard to train, which comes from approximating the intrinsicdiscontinuous\ndistribution transform map with continuousDNNs. The recently proposed AE-OT\nmodel addresses thisproblem by explicitly computing the discontinuous\ndistribu-tion transform map through solving a semi-discrete optimaltransport\n(OT) map in the latent space of the autoencoder.However the generated images\nare blurry. In this paper, wepropose the AE-OT-GAN model to utilize the\nadvantages ofthe both models: generate high quality images and at thesame time\novercome the mode collapse/mixture problems.Specifically, we first faithfully\nembed the low dimensionalimage manifold into the latent space by training an\nautoen-coder (AE). Then we compute the optimal transport (OT)map that pushes\nforward the uniform distribution to the la-tent distribution supported on the\nlatent manifold. Finally,our GAN model is trained to generate high quality\nimagesfrom the latent distribution, the distribution transform mapfrom which to\nthe empirical data distribution will be con-tinuous. The paired data between\nthe latent code and thereal images gives us further constriction about the\ngenerator.Experiments on simple MNIST dataset and complex datasetslike Cifar-10\nand CelebA show the efficacy and efficiency ofour proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 01:18:00 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 15:25:28 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["An", "Dongsheng", ""], ["Guo", "Yang", ""], ["Zhang", "Min", ""], ["Qi", "Xin", ""], ["Lei", "Na", ""], ["Yau", "Shing-Tung", ""], ["Gu", "Xianfeng", ""]]}, {"id": "2001.03712", "submitter": "Geondo Park", "authors": "Geondo Park, Chihye Han, Wonjun Yoon, Daeshik Kim", "title": "MHSAN: Multi-Head Self-Attention Network for Visual Semantic Embedding", "comments": "Accepted by the 2020 IEEE Winter Conference on Applications of\n  Computer Vision (WACV 20), 9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-semantic embedding enables various tasks such as image-text retrieval,\nimage captioning, and visual question answering. The key to successful\nvisual-semantic embedding is to express visual and textual data properly by\naccounting for their intricate relationship. While previous studies have\nachieved much advance by encoding the visual and textual data into a joint\nspace where similar concepts are closely located, they often represent data by\na single vector ignoring the presence of multiple important components in an\nimage or text. Thus, in addition to the joint embedding space, we propose a\nnovel multi-head self-attention network to capture various components of visual\nand textual data by attending to important parts in data. Our approach achieves\nthe new state-of-the-art results in image-text retrieval tasks on MS-COCO and\nFlicker30K datasets. Through the visualization of the attention maps that\ncapture distinct semantic components at multiple positions in the image and the\ntext, we demonstrate that our method achieves an effective and interpretable\nvisual-semantic joint space.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 05:50:19 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Park", "Geondo", ""], ["Han", "Chihye", ""], ["Yoon", "Wonjun", ""], ["Kim", "Daeshik", ""]]}, {"id": "2001.03725", "submitter": "Moi Hoon Yap", "authors": "Jireh Jam, Connah Kendrick, Vincent Drouard, Kevin Walker, Gee-Sern\n  Hsu, and Moi Hoon Yap", "title": "Symmetric Skip Connection Wasserstein GAN for High-Resolution Facial\n  Image Inpainting", "comments": "10 pages, 9 figures and 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art facial image inpainting methods achieved promising\nresults but face realism preservation remains a challenge. This is due to\nlimitations such as; failures in preserving edges and blurry artefacts. To\novercome these limitations, we propose a Symmetric Skip Connection Wasserstein\nGenerative Adversarial Network (S-WGAN) for high-resolution facial image\ninpainting. The architecture is an encoder-decoder with convolutional blocks,\nlinked by skip connections. The encoder is a feature extractor that captures\ndata abstractions of an input image to learn an end-to-end mapping from an\ninput (binary masked image) to the ground-truth. The decoder uses learned\nabstractions to reconstruct the image. With skip connections, S-WGAN transfers\nimage details to the decoder. Additionally, we propose a Wasserstein-Perceptual\nloss function to preserve colour and maintain realism on a reconstructed image.\nWe evaluate our method and the state-of-the-art methods on CelebA-HQ dataset.\nOur results show S-WGAN produces sharper and more realistic images when\nvisually compared with other methods. The quantitative measures show our\nproposed S-WGAN achieves the best Structure Similarity Index Measure (SSIM) of\n0.94.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 09:09:23 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 21:16:39 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Jam", "Jireh", ""], ["Kendrick", "Connah", ""], ["Drouard", "Vincent", ""], ["Walker", "Kevin", ""], ["Hsu", "Gee-Sern", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "2001.03728", "submitter": "Duygu Sarikaya", "authors": "Duygu Sarikaya, Pierre Jannin", "title": "Towards Generalizable Surgical Activity Recognition Using Spatial\n  Temporal Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and recognition of surgical activities poses an interesting research\nproblem. Although a number of recent works studied automatic recognition of\nsurgical activities, generalizability of these works across different tasks and\ndifferent datasets remains a challenge. We introduce a modality that is robust\nto scene variation, and that is able to infer part information such as\norientational and relative spatial relationships. The proposed modality is\nbased on spatial temporal graph representations of surgical tools in videos,\nfor surgical activity recognition. To explore its effectiveness, we model and\nrecognize surgical gestures with the proposed modality. We construct spatial\ngraphs connecting the joint pose estimations of surgical tools. Then, we\nconnect each joint to the corresponding joint in the consecutive frames forming\ninter-frame edges representing the trajectory of the joint over time. We then\nlearn hierarchical spatial temporal graph representations using Spatial\nTemporal Graph Convolutional Networks (ST-GCN). Our experiments show that\nlearned spatial temporal graph representations perform well in surgical gesture\nrecognition even when used individually. We experiment with the Suturing task\nof the JIGSAWS dataset where the chance baseline for gesture recognition is\n10%. Our results demonstrate 68% average accuracy which suggests a significant\nimprovement. Learned hierarchical spatial temporal graph representations can be\nused either individually, in cascades or as a complementary modality in\nsurgical activity recognition, therefore provide a benchmark for future\nstudies. To our knowledge, our paper is the first to use spatial temporal graph\nrepresentations of surgical tools, and pose-based skeleton representations in\ngeneral, for surgical activity recognition.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 09:11:32 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 12:46:04 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 21:35:10 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2020 20:58:03 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Sarikaya", "Duygu", ""], ["Jannin", "Pierre", ""]]}, {"id": "2001.03734", "submitter": "Dongyang Jin", "authors": "Dongyang Jin, Saiping Zhang, Xiao Huo, Wei Zhang, Fuzheng Yang", "title": "A Two-step Calibration Method for Unfocused Light Field Camera Based on\n  Projection Model Analysis", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately calibrating light field camera is essential to its applications.\nRapid progress has been made in this area in the past decades. In this paper,\ndetailed analysis was first performed towards the state of the art projection\nmodels for calibration which were further interpreted in three representations,\nincluding the correspondence between rays and pixels, 3D physical points and\npixels and between 3D physical points and 3D signal structure of the captured\nlight field. Based on the analysis, parameters in the projection model were\ngrouped into direction parameter set and depth parameter set. A two-step\ncalibration method was then proposed with each step dealing with each set of\nparameters. The proposed method is able to reuse traditional camera calibration\nmethods for the direction parameter set. A simply raw image-based calibration\nof depth parameter set was further proposed. Systematic validations were\nconducted to evaluate the performance of the proposed calibration method.\nExperimental results show that the accuracy and robustness of the proposed\nmethod outperforms its counterparts under various benchmark criteria.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 10:37:56 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:47:14 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Jin", "Dongyang", ""], ["Zhang", "Saiping", ""], ["Huo", "Xiao", ""], ["Zhang", "Wei", ""], ["Yang", "Fuzheng", ""]]}, {"id": "2001.03754", "submitter": "Huanqian Yan", "authors": "Huanqian Yan, Xingxing Wei, and Bo Li", "title": "Sparse Black-box Video Attack with Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box adversarial attacks on video recognition models have been explored.\nConsidering the temporal interactions between frames, a few methods try to\nselect some key frames, and then perform attacks on them. Unfortunately, their\nselecting strategy is independent with the attacking step, resulting in the\nlimited performance. Instead, we argue the frame selection phase is closely\nrelevant with the attacking phase. The key frames should be adjusted according\nto the attacking results. For that, we formulate the black-box video attacks\ninto Reinforcement Learning (RL) framework. Specifically, the environment in RL\nis set as the threat model, and the agent in RL plays the role of frame\nselecting. By continuously querying the threat models and receiving the\nattacking feedback, the agent gradually adjusts its frame selection strategy\nand adversarial perturbations become smaller and smaller. A series of\nexperiments demonstrate that our method can significantly reduce the\nadversarial perturbations with efficient query times.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 14:09:49 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 04:39:07 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Yan", "Huanqian", ""], ["Wei", "Xingxing", ""], ["Li", "Bo", ""]]}, {"id": "2001.03779", "submitter": "Alona Baruhov", "authors": "Alona Baruhov and Guy Gilboa", "title": "Unsupervised Enhancement of Real-World Depth Images Using Tri-Cycle GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low quality depth poses a considerable challenge to computer vision\nalgorithms. In this work we aim to enhance highly degraded, real-world depth\nimages acquired by a low-cost sensor, for which an analytical noise model is\nunavailable. In the absence of clean ground-truth, we approach the task as an\nunsupervised domain-translation between the low-quality sensor domain and a\nhigh-quality sensor domain, represented using two unpaired training sets. We\nemploy the highly-successful Cycle-GAN to this task, but find it to perform\npoorly in this case. Identifying the sources of the failure, we introduce\nseveral modifications to the framework, including a larger generator\narchitecture, depth-specific losses that take into account missing pixels, and\na novel Tri-Cycle loss which promotes information-preservation while addressing\nthe asymmetry between the domains. We show that the resulting framework\ndramatically improves over the original Cycle-GAN both visually and\nquantitatively, extending its applicability to more challenging and asymmetric\ntranslation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 18:19:09 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Baruhov", "Alona", ""], ["Gilboa", "Guy", ""]]}, {"id": "2001.03799", "submitter": "Bo Zhou", "authors": "Bo Zhou and S. Kevin Zhou", "title": "DuDoRNet: Learning a Dual-Domain Recurrent Network for Fast MRI\n  Reconstruction with Deep T1 Prior", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MRI with multiple protocols is commonly used for diagnosis, but it suffers\nfrom a long acquisition time, which yields the image quality vulnerable to say\nmotion artifacts. To accelerate, various methods have been proposed to\nreconstruct full images from under-sampled k-space data. However, these\nalgorithms are inadequate for two main reasons. Firstly, aliasing artifacts\ngenerated in the image domain are structural and non-local, so that sole image\ndomain restoration is insufficient. Secondly, though MRI comprises multiple\nprotocols during one exam, almost all previous studies only employ the\nreconstruction of an individual protocol using a highly distorted undersampled\nimage as input, leaving the use of fully-sampled short protocol (say T1) as\ncomplementary information highly underexplored. In this work, we address the\nabove two limitations by proposing a Dual Domain Recurrent Network (DuDoRNet)\nwith deep T1 prior embedded to simultaneously recover k-space and images for\naccelerating the acquisition of MRI with a long imaging protocol. Specifically,\na Dilated Residual Dense Network (DRDNet) is customized for dual domain\nrestorations from undersampled MRI data. Extensive experiments on different\nsampling patterns and acceleration rates demonstrate that our method\nconsistently outperforms state-of-the-art methods, and can reconstruct\nhigh-quality MRI.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 21:34:48 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 03:05:23 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Zhou", "Bo", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2001.03801", "submitter": "Hua Ma", "authors": "Hua Ma, Ihor Smal, Joost Daemen, Theo van Walsum", "title": "Dynamic Coronary Roadmapping via Catheter Tip Tracking in X-ray\n  Fluoroscopy with Deep Learning Based Bayesian Filtering", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2020.101634", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Percutaneous coronary intervention (PCI) is typically performed with image\nguidance using X-ray angiograms in which coronary arteries are opacified with\nX-ray opaque contrast agents. Interventional cardiologists typically navigate\ninstruments using non-contrast-enhanced fluoroscopic images, since higher use\nof contrast agents increases the risk of kidney failure. When using\nfluoroscopic images, the interventional cardiologist needs to rely on a mental\nanatomical reconstruction. This paper reports on the development of a novel\ndynamic coronary roadmapping approach for improving visual feedback and\nreducing contrast use during PCI. The approach compensates cardiac and\nrespiratory induced vessel motion by ECG alignment and catheter tip tracking in\nX-ray fluoroscopy, respectively. In particular, for accurate and robust\ntracking of the catheter tip, we proposed a new deep learning based Bayesian\nfiltering method that integrates the detection outcome of a convolutional\nneural network and the motion estimation between frames using a particle\nfiltering framework. The proposed roadmapping and tracking approaches were\nvalidated on clinical X-ray images, achieving accurate performance on both\ncatheter tip tracking and dynamic coronary roadmapping experiments. In\naddition, our approach runs in real-time on a computer with a single GPU and\nhas the potential to be integrated into the clinical workflow of PCI\nprocedures, providing cardiologists with visual guidance during interventions\nwithout the need of extra use of contrast agent.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 22:08:06 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Ma", "Hua", ""], ["Smal", "Ihor", ""], ["Daemen", "Joost", ""], ["van Walsum", "Theo", ""]]}, {"id": "2001.03831", "submitter": "Xiaoran Zhang", "authors": "Xiaoran Zhang, Hexiang Dong, Di Gao and Xiao Zhao", "title": "A Comparative Study for Non-rigid Image Registration and Rigid Image\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration algorithms can be generally categorized into two groups:\nnon-rigid and rigid. Recently, many deep learning-based algorithms employ a\nneural net to characterize non-rigid image registration function. However, do\nthey always perform better? In this study, we compare the state-of-art deep\nlearning-based non-rigid registration approach with rigid registration\napproach. The data is generated from Kaggle Dog vs Cat Competition\n\\url{https://www.kaggle.com/c/dogs-vs-cats/} and we test the algorithms'\nperformance on rigid transformation including translation, rotation, scaling,\nshearing and pixelwise non-rigid transformation. The Voxelmorph is trained on\nrigidset and nonrigidset separately for comparison and we also add a gaussian\nblur layer to its original architecture to improve registration performance.\nThe best quantitative results in both root-mean-square error (RMSE) and mean\nabsolute error (MAE) metrics for rigid registration are produced by\nSimpleElastix and non-rigid registration by Voxelmorph. We select\nrepresentative samples for visual assessment.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 02:32:32 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Zhang", "Xiaoran", ""], ["Dong", "Hexiang", ""], ["Gao", "Di", ""], ["Zhao", "Xiao", ""]]}, {"id": "2001.03847", "submitter": "Lijun Zhao", "authors": "Lijun Zhao, Jinjing Zhang, Fan Zhang, Anhong Wang, Huihui Bai, Yao\n  Zhao", "title": "Concurrently Extrapolating and Interpolating Networks for Continuous\n  Model Generation", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Most deep image smoothing operators are always trained repetitively when\ndifferent explicit structure-texture pairs are employed as label images for\neach algorithm configured with different parameters. This kind of training\nstrategy often takes a long time and spends equipment resources in a costly\nmanner. To address this challenging issue, we generalize continuous network\ninterpolation as a more powerful model generation tool, and then propose a\nsimple yet effective model generation strategy to form a sequence of models\nthat only requires a set of specific-effect label images. To precisely learn\nimage smoothing operators, we present a double-state aggregation (DSA) module,\nwhich can be easily inserted into most of current network architecture. Based\non this module, we design a double-state aggregation neural network structure\nwith a local feature aggregation block and a nonlocal feature aggregation block\nto obtain operators with large expression capacity. Through the evaluation of\nmany objective and visual experimental results, we show that the proposed\nmethod is capable of producing a series of continuous models and achieves\nbetter performance than that of several state-of-the-art methods for image\nsmoothing.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 04:44:44 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Zhao", "Lijun", ""], ["Zhang", "Jinjing", ""], ["Zhang", "Fan", ""], ["Wang", "Anhong", ""], ["Bai", "Huihui", ""], ["Zhao", "Yao", ""]]}, {"id": "2001.03851", "submitter": "Lijun Zhao", "authors": "Lijun Zhao, Huihui Bai, Anhong Wang, Yao Zhao", "title": "Deep Optimized Multiple Description Image Coding via Scalar Quantization\n  Learning", "comments": "14 PAGES, 10 FIGURES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we introduce a deep multiple description coding (MDC)\nframework optimized by minimizing multiple description (MD) compressive loss.\nFirst, MD multi-scale-dilated encoder network generates multiple description\ntensors, which are discretized by scalar quantizers, while these quantized\ntensors are decompressed by MD cascaded-ResBlock decoder networks. To greatly\nreduce the total amount of artificial neural network parameters, an\nauto-encoder network composed of these two types of network is designed as a\nsymmetrical parameter sharing structure. Second, this autoencoder network and a\npair of scalar quantizers are simultaneously learned in an end-to-end\nself-supervised way. Third, considering the variation in the image spatial\ndistribution, each scalar quantizer is accompanied by an importance-indicator\nmap to generate MD tensors, rather than using direct quantization. Fourth, we\nintroduce the multiple description structural similarity distance loss, which\nimplicitly regularizes the diversified multiple description generations, to\nexplicitly supervise multiple description diversified decoding in addition to\nMD reconstruction loss. Finally, we demonstrate that our MDC framework performs\nbetter than several state-of-the-art MDC approaches regarding image coding\nefficiency when tested on several commonly available datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 05:03:16 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Zhao", "Lijun", ""], ["Bai", "Huihui", ""], ["Wang", "Anhong", ""], ["Zhao", "Yao", ""]]}, {"id": "2001.03856", "submitter": "Wei Xiong", "authors": "Wei Xiong, Yutong He, Yixuan Zhang, Wenhan Luo, Lin Ma, Jiebo Luo", "title": "Fine-grained Image-to-Image Transformation towards Visual Recognition", "comments": "CVPR 2020 Camera Ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image-to-image transformation approaches primarily focus on\nsynthesizing visually pleasing data. Generating images with correct identity\nlabels is challenging yet much less explored. It is even more challenging to\ndeal with image transformation tasks with large deformation in poses,\nviewpoints, or scales while preserving the identity, such as face rotation and\nobject viewpoint morphing. In this paper, we aim at transforming an image with\na fine-grained category to synthesize new images that preserve the identity of\nthe input image, which can thereby benefit the subsequent fine-grained image\nrecognition and few-shot learning tasks. The generated images, transformed with\nlarge geometric deformation, do not necessarily need to be of high visual\nquality but are required to maintain as much identity information as possible.\nTo this end, we adopt a model based on generative adversarial networks to\ndisentangle the identity related and unrelated factors of an image. In order to\npreserve the fine-grained contextual details of the input image during the\ndeformable transformation, a constrained nonalignment connection method is\nproposed to construct learnable highways between intermediate convolution\nblocks in the generator. Moreover, an adaptive identity modulation mechanism is\nproposed to transfer the identity information into the output image\neffectively. Extensive experiments on the CompCars and Multi-PIE datasets\ndemonstrate that our model preserves the identity of the generated images much\nbetter than the state-of-the-art image-to-image transformation models, and as a\nresult significantly boosts the visual recognition performance in fine-grained\nfew-shot learning.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 05:26:47 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 02:18:53 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Xiong", "Wei", ""], ["He", "Yutong", ""], ["Zhang", "Yixuan", ""], ["Luo", "Wenhan", ""], ["Ma", "Lin", ""], ["Luo", "Jiebo", ""]]}, {"id": "2001.03857", "submitter": "Xuhua Ren", "authors": "Xuhua Ren, Jiayu Huo, Kai Xuan, Dongming Wei, Lichi Zhang, Qian Wang", "title": "Robust Brain Magnetic Resonance Image Segmentation for Hydrocephalus\n  Patients: Hard and Soft Attention", "comments": "ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain magnetic resonance (MR) segmentation for hydrocephalus patients is\nconsidered as a challenging work. Encoding the variation of the brain\nanatomical structures from different individuals cannot be easily achieved. The\ntask becomes even more difficult especially when the image data from\nhydrocephalus patients are considered, which often have large deformations and\ndiffer significantly from the normal subjects. Here, we propose a novel\nstrategy with hard and soft attention modules to solve the segmentation\nproblems for hydrocephalus MR images. Our main contributions are three-fold: 1)\nthe hard-attention module generates coarse segmentation map using\nmulti-atlas-based method and the VoxelMorph tool, which guides subsequent\nsegmentation process and improves its robustness; 2) the soft-attention module\nincorporates position attention to capture precise context information, which\nfurther improves the segmentation accuracy; 3) we validate our method by\nsegmenting insula, thalamus and many other regions-of-interests (ROIs) that are\ncritical to quantify brain MR images of hydrocephalus patients in real clinical\nscenario. The proposed method achieves much improved robustness and accuracy\nwhen segmenting all 17 consciousness-related ROIs with high variations for\ndifferent subjects. To the best of our knowledge, this is the first work to\nemploy deep learning for solving the brain segmentation problems of\nhydrocephalus patients.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 05:27:06 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Ren", "Xuhua", ""], ["Huo", "Jiayu", ""], ["Xuan", "Kai", ""], ["Wei", "Dongming", ""], ["Zhang", "Lichi", ""], ["Wang", "Qian", ""]]}, {"id": "2001.03872", "submitter": "Huibing Wang", "authors": "Huibing Wang, Jinjia Peng, Dongyan Chen, Guangqi Jiang, Tongtong Zhao,\n  Xianping Fu", "title": "Attribute-guided Feature Learning Network for Vehicle Re-identification", "comments": "arXiv admin note: text overlap with arXiv:1912.10193", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (reID) plays an important role in the automatic\nanalysis of the increasing urban surveillance videos, which has become a hot\ntopic in recent years. However, it poses the critical but challenging problem\nthat is caused by various viewpoints of vehicles, diversified illuminations and\ncomplicated environments. Till now, most existing vehicle reID approaches focus\non learning metrics or ensemble to derive better representation, which are only\ntake identity labels of vehicle into consideration. However, the attributes of\nvehicle that contain detailed descriptions are beneficial for training reID\nmodel. Hence, this paper proposes a novel Attribute-Guided Network (AGNet),\nwhich could learn global representation with the abundant attribute features in\nan end-to-end manner. Specially, an attribute-guided module is proposed in\nAGNet to generate the attribute mask which could inversely guide to select\ndiscriminative features for category classification. Besides that, in our\nproposed AGNet, an attribute-based label smoothing (ALS) loss is presented to\nbetter train the reID model, which can strength the distinct ability of vehicle\nreID model to regularize AGNet model according to the attributes. Comprehensive\nexperimental results clearly demonstrate that our method achieves excellent\nperformance on both VehicleID dataset and VeRi-776 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 06:57:10 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Wang", "Huibing", ""], ["Peng", "Jinjia", ""], ["Chen", "Dongyan", ""], ["Jiang", "Guangqi", ""], ["Zhao", "Tongtong", ""], ["Fu", "Xianping", ""]]}, {"id": "2001.03886", "submitter": "Chuang Lin", "authors": "Chuang Lin, Sicheng Zhao, Lei Meng, Tat-Seng Chua", "title": "Multi-source Domain Adaptation for Visual Sentiment Classification", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing domain adaptation methods on visual sentiment classification\ntypically are investigated under the single-source scenario, where the\nknowledge learned from a source domain of sufficient labeled data is\ntransferred to the target domain of loosely labeled or unlabeled data. However,\nin practice, data from a single source domain usually have a limited volume and\ncan hardly cover the characteristics of the target domain. In this paper, we\npropose a novel multi-source domain adaptation (MDA) method, termed\nMulti-source Sentiment Generative Adversarial Network (MSGAN), for visual\nsentiment classification. To handle data from multiple source domains, it\nlearns to find a unified sentiment latent space where data from both the source\nand target domains share a similar distribution. This is achieved via cycle\nconsistent adversarial learning in an end-to-end manner. Extensive experiments\nconducted on four benchmark datasets demonstrate that MSGAN significantly\noutperforms the state-of-the-art MDA approaches for visual sentiment\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 08:37:42 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Lin", "Chuang", ""], ["Zhao", "Sicheng", ""], ["Meng", "Lei", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2001.03893", "submitter": "Xiaoqing Guo", "authors": "Xiaoqing Guo, Zhen Chen, Yixuan Yuan", "title": "Complementary Network with Adaptive Receptive Fields for Melanoma\n  Segmentation", "comments": "4 pages, 4 figures", "journal-ref": "IEEE International Symposium on Biomedical Imaging (ISBI 2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic melanoma segmentation in dermoscopic images is essential in\ncomputer-aided diagnosis of skin cancer. Existing methods may suffer from the\nhole and shrink problems with limited segmentation performance. To tackle these\nissues, we propose a novel complementary network with adaptive receptive filed\nlearning. Instead of regarding the segmentation task independently, we\nintroduce a foreground network to detect melanoma lesions and a background\nnetwork to mask non-melanoma regions. Moreover, we propose adaptive atrous\nconvolution (AAC) and knowledge aggregation module (KAM) to fill holes and\nalleviate the shrink problems. AAC explicitly controls the receptive field at\nmultiple scales and KAM convolves shallow feature maps by dilated convolutions\nwith adaptive receptive fields, which are adjusted according to deep feature\nmaps. In addition, a novel mutual loss is proposed to utilize the dependency\nbetween the foreground and background networks, thereby enabling the\nreciprocally influence within these two networks. Consequently, this mutual\ntraining strategy enables the semi-supervised learning and improve the\nboundary-sensitivity. Training with Skin Imaging Collaboration (ISIC) 2018 skin\nlesion segmentation dataset, our method achieves a dice co-efficient of 86.4%\nand shows better performance compared with state-of-the-art melanoma\nsegmentation methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 09:20:36 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Guo", "Xiaoqing", ""], ["Chen", "Zhen", ""], ["Yuan", "Yixuan", ""]]}, {"id": "2001.03905", "submitter": "Hongguang Zhang", "authors": "Hongguang Zhang, Li Zhang, Xiaojuan Qi, Hongdong Li, Philip H. S.\n  Torr, Piotr Koniusz", "title": "Few-shot Action Recognition with Permutation-invariant Attention", "comments": "ECCV2020 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many few-shot learning models focus on recognising images. In contrast, we\ntackle a challenging task of few-shot action recognition from videos. We build\non a C3D encoder for spatio-temporal video blocks to capture short-range action\npatterns. Such encoded blocks are aggregated by permutation-invariant pooling\nto make our approach robust to varying action lengths and long-range temporal\ndependencies whose patterns are unlikely to repeat even in clips of the same\nclass. Subsequently, the pooled representations are combined into simple\nrelation descriptors which encode so-called query and support clips. Finally,\nrelation descriptors are fed to the comparator with the goal of similarity\nlearning between query and support clips. Importantly, to re-weight block\ncontributions during pooling, we exploit spatial and temporal attention modules\nand self-supervision. In naturalistic clips (of the same class) there exists a\ntemporal distribution shift--the locations of discriminative temporal action\nhotspots vary. Thus, we permute blocks of a clip and align the resulting\nattention regions with similarly permuted attention regions of non-permuted\nclip to train the attention mechanism invariant to block (and thus long-term\nhotspot) permutations. Our method outperforms the state of the art on the\nHMDB51, UCF101, miniMIT datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 10:58:09 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 17:21:45 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 02:44:04 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhang", "Hongguang", ""], ["Zhang", "Li", ""], ["Qi", "Xiaojuan", ""], ["Li", "Hongdong", ""], ["Torr", "Philip H. S.", ""], ["Koniusz", "Piotr", ""]]}, {"id": "2001.03919", "submitter": "Hongguang Zhang", "authors": "Hongguang Zhang, Piotr Koniusz, Songlei Jian, Hongdong Li, Philip H.\n  S. Torr", "title": "Rethinking Class Relations: Absolute-relative Supervised and\n  Unsupervised Few-shot Learning", "comments": "IEEE/CVF Conference on Computer Vision and Pattern Recognition 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The majority of existing few-shot learning methods describe image relations\nwith binary labels. However, such binary relations are insufficient to teach\nthe network complicated real-world relations, due to the lack of decision\nsmoothness. Furthermore, current few-shot learning models capture only the\nsimilarity via relation labels, but they are not exposed to class concepts\nassociated with objects, which is likely detrimental to the classification\nperformance due to underutilization of the available class labels. To\nparaphrase, children learn the concept of tiger from a few of actual examples\nas well as from comparisons of tiger to other animals. Thus, we hypothesize\nthat in fact both similarity and class concept learning must be occurring\nsimultaneously. With these observations at hand, we study the fundamental\nproblem of simplistic class modeling in current few-shot learning methods. We\nrethink the relations between class concepts, and propose a novel\nAbsolute-relative Learning paradigm to fully take advantage of label\ninformation to refine the image representations and correct the relation\nunderstanding in both supervised and unsupervised scenarios. Our proposed\nparadigm improves the performance of several the state-of-the-art models on\npublicly available datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 12:25:46 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 17:20:13 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 15:48:27 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 04:36:13 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhang", "Hongguang", ""], ["Koniusz", "Piotr", ""], ["Jian", "Songlei", ""], ["Li", "Hongdong", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "2001.03923", "submitter": "Changhee Han", "authors": "Changhee Han, Leonardo Rundo, Kohei Murao, Takafumi Nemoto, Hideki\n  Nakayama", "title": "Bridging the gap between AI and Healthcare sides: towards developing\n  clinically relevant AI-powered diagnosis systems", "comments": "13 pages, 2 figure, accepted to AIAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of Convolutional Neural Network-based Computer-Aided\nDiagnosis research, its clinical applications remain challenging. Accordingly,\ndeveloping medical Artificial Intelligence (AI) fitting into a clinical\nenvironment requires identifying/bridging the gap between AI and Healthcare\nsides. Since the biggest problem in Medical Imaging lies in data paucity,\nconfirming the clinical relevance for diagnosis of research-proven image\naugmentation techniques is essential. Therefore, we hold a clinically valuable\nAI-envisioning workshop among Japanese Medical Imaging experts, physicians, and\ngeneralists in Healthcare/Informatics. Then, a questionnaire survey for\nphysicians evaluates our pathology-aware Generative Adversarial Network\n(GAN)-based image augmentation projects in terms of Data Augmentation and\nphysician training. The workshop reveals the intrinsic gap between\nAI/Healthcare sides and solutions on Why (i.e., clinical\nsignificance/interpretation) and How (i.e., data acquisition, commercial\ndeployment, and safety/feeling safe). This analysis confirms our\npathology-aware GANs' clinical relevance as a clinical decision support system\nand non-expert physician training tool. Our findings would play a key role in\nconnecting inter-disciplinary research and clinical applications, not limited\nto the Japanese medical context and pathology-aware GANs.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 12:45:46 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 16:24:51 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Han", "Changhee", ""], ["Rundo", "Leonardo", ""], ["Murao", "Kohei", ""], ["Nemoto", "Takafumi", ""], ["Nakayama", "Hideki", ""]]}, {"id": "2001.03960", "submitter": "\\\"Omer S\\\"umer", "authors": "\\\"Omer S\\\"umer, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci", "title": "Attention Flow: End-to-End Joint Attention Estimation", "comments": "Paper accepted in WACV 2020", "journal-ref": null, "doi": "10.1109/WACV45572.2020.9093515", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of understanding joint attention in\nthird-person social scene videos. Joint attention is the shared gaze behaviour\nof two or more individuals on an object or an area of interest and has a wide\nrange of applications such as human-computer interaction, educational\nassessment, treatment of patients with attention disorders, and many more. Our\nmethod, Attention Flow, learns joint attention in an end-to-end fashion by\nusing saliency-augmented attention maps and two novel convolutional attention\nmechanisms that determine to select relevant features and improve joint\nattention localization. We compare the effect of saliency maps and attention\nmechanisms and report quantitative and qualitative results on the detection and\nlocalization of joint attention in the VideoCoAtt dataset, which contains\ncomplex social scenes.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 16:47:21 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["S\u00fcmer", "\u00d6mer", ""], ["Gerjets", "Peter", ""], ["Trautwein", "Ulrich", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2001.03981", "submitter": "Aniket Anand Deshmukh", "authors": "Mansi Ranjit Mane, Aniket Anand Deshmukh, Adam J. Iliff", "title": "Head and Tail Localization of C. elegans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  C. elegans is commonly used in neuroscience for behaviour analysis because of\nit's compact nervous system with well-described connectivity. Localizing the\nanimal and distinguishing between its head and tail are important tasks to\ntrack the worm during behavioural assays and to perform quantitative analyses.\nWe demonstrate a neural network based approach to localize both the head and\nthe tail of the worm in an image. To make empirical results in the paper\nreproducible and promote open source machine learning based solutions for C.\nelegans behavioural analysis, we also make our code publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 19:18:27 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Mane", "Mansi Ranjit", ""], ["Deshmukh", "Aniket Anand", ""], ["Iliff", "Adam J.", ""]]}, {"id": "2001.03992", "submitter": "Muktabh Mayank Srivastava", "authors": "Muktabh Mayank Srivastava", "title": "Bag of Tricks for Retail Product Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Retail Product Image Classification is an important Computer Vision and\nMachine Learning problem for building real world systems like self-checkout\nstores and automated retail execution evaluation. In this work, we present\nvarious tricks to increase accuracy of Deep Learning models on different types\nof retail product image classification datasets. These tricks enable us to\nincrease the accuracy of fine tuned convnets for retail product image\nclassification by a large margin. As the most prominent trick, we introduce a\nnew neural network layer called Local-Concepts-Accumulation (LCA) layer which\ngives consistent gains across multiple datasets. Two other tricks we find to\nincrease accuracy on retail product identification are using an\ninstagram-pretrained Convnet and using Maximum Entropy as an auxiliary loss for\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 20:20:07 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Srivastava", "Muktabh Mayank", ""]]}, {"id": "2001.04011", "submitter": "Yeachan Park", "authors": "Yeachan Park, Myungjoo Kang", "title": "Membership Inference Attacks Against Object Detection Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Machine learning models can leak information regarding the dataset they have\ntrained. In this paper, we present the first membership inference attack\nagainst black-boxed object detection models that determines whether the given\ndata records are used in the training. To attack the object detection model, we\ndevise a novel method named as called a canvas method, in which predicted\nbounding boxes are drawn on an empty image for the attack model input. Based on\nthe experiments, we successfully reveal the membership status of privately\nsensitive data trained using one-stage and two-stage detection models. We then\npropose defense strategies and also conduct a transfer attack between the\nmodels and datasets. Our results show that object detection models are also\nvulnerable to inference attacks like other models.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 23:17:45 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 08:05:39 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Park", "Yeachan", ""], ["Kang", "Myungjoo", ""]]}, {"id": "2001.04026", "submitter": "Zijian Liu", "authors": "Zijian Liu, Chunbo Luo, Shuai Li, Peng Ren and Geyong Min", "title": "Fractional order graph neural network", "comments": "There are serious mistakes in the article and it needs to be\n  retracted and corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes fractional order graph neural networks (FGNNs), optimized\nby the approximation strategy to address the challenges of local optimum of\nclassic and fractional graph neural networks which are specialised at\naggregating information from the feature and adjacent matrices of connected\nnodes and their neighbours to solve learning tasks on non-Euclidean data such\nas graphs. Meanwhile the approximate calculation of fractional order gradients\nalso overcomes the high computational complexity of fractional order\nderivations. We further prove that such an approximation is feasible and the\nFGNN is unbiased towards global optimization solution. Extensive experiments on\ncitation networks show that FGNN achieves great advantage over baseline models\nwhen selected appropriate fractional order.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 11:55:55 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 08:30:29 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 06:23:18 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Zijian", ""], ["Luo", "Chunbo", ""], ["Li", "Shuai", ""], ["Ren", "Peng", ""], ["Min", "Geyong", ""]]}, {"id": "2001.04062", "submitter": "Shadrokh Samavi", "authors": "Morteza Mousa Pasandi, Mohsen Hajabdollahi, Nader Karimi, Shadrokh\n  Samavi", "title": "Modeling of Pruning Techniques for Deep Neural Networks Simplification", "comments": "six pages, eight figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) suffer from different issues, such as\ncomputational complexity and the number of parameters. In recent years pruning\ntechniques are employed to reduce the number of operations and model size in\nCNNs. Different pruning methods are proposed, which are based on pruning the\nconnections, channels, and filters. Various techniques and tricks accompany\npruning methods, and there is not a unifying framework to model all the pruning\nmethods. In this paper pruning methods are investigated, and a general model\nwhich is contained the majority of pruning techniques is proposed. The\nadvantages and disadvantages of the pruning methods can be identified, and all\nof them can be summarized under this model. The final goal of this model is to\nprovide a general approach for all of the pruning methods with different\nstructures and applications.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 04:51:59 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Pasandi", "Morteza Mousa", ""], ["Hajabdollahi", "Mohsen", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "2001.04066", "submitter": "Richard Wang", "authors": "Feng Cen and Guanghui Wang", "title": "Boosting Occluded Image Classification via Subspace Decomposition Based\n  Estimation of Deep Features", "comments": null, "journal-ref": "IEEE Transactions on Cybernetics, 2019", "doi": "10.1109/TCYB.2019.2931067", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of partially occluded images is a highly challenging computer\nvision problem even for the cutting edge deep learning technologies. To achieve\na robust image classification for occluded images, this paper proposes a novel\nscheme using subspace decomposition based estimation (SDBE). The proposed\nSDBE-based classification scheme first employs a base convolutional neural\nnetwork to extract the deep feature vector (DFV) and then utilizes the SDBE to\ncompute the DFV of the original occlusion-free image for classification. The\nSDBE is performed by projecting the DFV of the occluded image onto the linear\nspan of a class dictionary (CD) along the linear span of an occlusion error\ndictionary (OED). The CD and OED are constructed respectively by concatenating\nthe DFVs of a training set and the occlusion error vectors of an extra set of\nimage pairs. Two implementations of the SDBE are studied in this paper: the\n$l_1$-norm and the squared $l_2$-norm regularized least-squares estimates. By\nemploying the ResNet-152, pre-trained on the ILSVRC2012 training set, as the\nbase network, the proposed SBDE-based classification scheme is extensively\nevaluated on the Caltech-101 and ILSVRC2012 datasets. Extensive experimental\nresults demonstrate that the proposed SDBE-based scheme dramatically boosts the\nclassification accuracy for occluded images, and achieves around $22.25\\%$\nincrease in classification accuracy under $20\\%$ occlusion on the ILSVRC2012\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 05:36:27 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Cen", "Feng", ""], ["Wang", "Guanghui", ""]]}, {"id": "2001.04069", "submitter": "Yaoyi Li", "authors": "Yaoyi Li, Hongtao Lu", "title": "Natural Image Matting via Guided Contextual Attention", "comments": "AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, deep learning based approaches have achieved\noutstanding improvements in natural image matting. Many of these methods can\ngenerate visually plausible alpha estimations, but typically yield blurry\nstructures or textures in the semitransparent area. This is due to the local\nambiguity of transparent objects. One possible solution is to leverage the\nfar-surrounding information to estimate the local opacity. Traditional\naffinity-based methods often suffer from the high computational complexity,\nwhich are not suitable for high resolution alpha estimation. Inspired by\naffinity-based method and the successes of contextual attention in inpainting,\nwe develop a novel end-to-end approach for natural image matting with a guided\ncontextual attention module, which is specifically designed for image matting.\nGuided contextual attention module directly propagates high-level opacity\ninformation globally based on the learned low-level affinity. The proposed\nmethod can mimic information flow of affinity-based methods and utilize rich\nfeatures learned by deep neural networks simultaneously. Experiment results on\nComposition-1k testing set and alphamatting.com benchmark dataset demonstrate\nthat our method outperforms state-of-the-art approaches in natural image\nmatting. Code and models are available at\nhttps://github.com/Yaoyi-Li/GCA-Matting.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 05:59:17 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Li", "Yaoyi", ""], ["Lu", "Hongtao", ""]]}, {"id": "2001.04074", "submitter": "Farhana Sultana", "authors": "Farhana Sultana (1), Abu Sufian (1) and Paramartha Dutta (2), ((1)\n  Dept. of Computer Science, University of Gour Banga, (2) Dept. of Computer &\n  System Sciences, Visva-Bharati University)", "title": "Evolution of Image Segmentation using Deep Convolutional Neural Network:\n  A Survey", "comments": "38 pages, 29 figures, 8 tables", "journal-ref": "journal = \"Knowledge-Based Systems\", volume = \"201-202\", pages =\n  \"106062\", year = \"2020\"", "doi": "10.1016/j.knosys.2020.106062", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the autonomous car driving to medical diagnosis, the requirement of the\ntask of image segmentation is everywhere. Segmentation of an image is one of\nthe indispensable tasks in computer vision. This task is comparatively\ncomplicated than other vision tasks as it needs low-level spatial information.\nBasically, image segmentation can be of two types: semantic segmentation and\ninstance segmentation. The combined version of these two basic tasks is known\nas panoptic segmentation. In the recent era, the success of deep convolutional\nneural networks (CNN) has influenced the field of segmentation greatly and gave\nus various successful models to date. In this survey, we are going to take a\nglance at the evolution of both semantic and instance segmentation work based\non CNN. We have also specified comparative architectural details of some\nstate-of-the-art models and discuss their training details to present a lucid\nunderstanding of hyper-parameter tuning of those models. We have also drawn a\ncomparison among the performance of those models on different datasets. Lastly,\nwe have given a glimpse of some state-of-the-art panoptic segmentation models.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 06:07:27 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 11:01:02 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 07:23:43 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Sultana", "Farhana", ""], ["Sufian", "Abu", ""], ["Dutta", "Paramartha", ""]]}, {"id": "2001.04077", "submitter": "Seth Huang", "authors": "Seth H. Huang, Xu Lingjie, Jiang Congwei", "title": "Residual Attention Net for Superior Cross-Domain Time Sequence Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel architecture, residual attention net (RAN), which merges a\nsequence architecture, universal transformer, and a computer vision\narchitecture, residual net, with a high-way architecture for cross-domain\nsequence modeling. The architecture aims at addressing the long dependency\nissue often faced by recurrent-neural-net-based structures. This paper serves\nas a proof-of-concept for a new architecture, with RAN aiming at providing the\nmodel a higher level understanding of sequence patterns. To our best knowledge,\nwe are the first to propose such an architecture. Out of the standard 85 UCR\ndata sets, we have achieved 35 state-of-the-art results with 10 results\nmatching current state-of-the-art results without further model fine-tuning.\nThe results indicate that such architecture is promising in complex,\nlong-sequence modeling and may have vast, cross-domain applications.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 06:14:04 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Huang", "Seth H.", ""], ["Lingjie", "Xu", ""], ["Congwei", "Jiang", ""]]}, {"id": "2001.04086", "submitter": "Pengguang Chen", "authors": "Pengguang Chen, Shu Liu, Hengshuang Zhao, Jiaya Jia", "title": "GridMask Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel data augmentation method `GridMask' in this paper. It\nutilizes information removal to achieve state-of-the-art results in a variety\nof computer vision tasks. We analyze the requirement of information dropping.\nThen we show limitation of existing information dropping algorithms and propose\nour structured method, which is simple and yet very effective. It is based on\nthe deletion of regions of the input image. Our extensive experiments show that\nour method outperforms the latest AutoAugment, which is way more\ncomputationally expensive due to the use of reinforcement learning to find the\nbest policies. On the ImageNet dataset for recognition, COCO2017 object\ndetection, and on Cityscapes dataset for semantic segmentation, our method all\nnotably improves performance over baselines. The extensive experiments manifest\nthe effectiveness and generality of the new method.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 07:27:05 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 03:47:39 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Chen", "Pengguang", ""], ["Liu", "Shu", ""], ["Zhao", "Hengshuang", ""], ["Jia", "Jiaya", ""]]}, {"id": "2001.04118", "submitter": "Jonah Ong B.Eng", "authors": "Jonah Ong, Ba Tuong Vo, Ba Ngu Vo, Du Yong Kim, Sven Nordholm", "title": "A Bayesian Filter for Multi-view 3D Multi-object Tracking with Occlusion\n  Handling", "comments": "18 pages, 11 figures, TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3034435", "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an online multi-camera multi-object tracker that only\nrequires monocular detector training, independent of the multi-camera\nconfigurations, allowing seamless extension/deletion of cameras without\nretraining effort. The proposed algorithm has a linear complexity in the total\nnumber of detections across the cameras, and hence scales gracefully with the\nnumber of cameras. It operates in the 3D world frame, and provides 3D\ntrajectory estimates of the objects. The key innovation is a high fidelity yet\ntractable 3D occlusion model, amenable to optimal Bayesian multi-view\nmulti-object filtering, which seamlessly integrates, into a single Bayesian\nrecursion, the sub-tasks of track management, state estimation, clutter\nrejection, and occlusion/misdetection handling. The proposed algorithm is\nevaluated on the latest WILDTRACKS dataset, and demonstrated to work in very\ncrowded scenes on a new dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 09:34:07 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 06:02:54 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 10:55:41 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2020 10:06:49 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Ong", "Jonah", ""], ["Vo", "Ba Tuong", ""], ["Vo", "Ba Ngu", ""], ["Kim", "Du Yong", ""], ["Nordholm", "Sven", ""]]}, {"id": "2001.04123", "submitter": "Dong Gong", "authors": "Xinyu Zhang, Dong Gong, Jiewei Cao, Chunhua Shen", "title": "Memorizing Comprehensively to Learn Adaptively: Unsupervised\n  Cross-Domain Person Re-ID with Multi-level Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised cross-domain person re-identification (Re-ID) aims to adapt the\ninformation from the labelled source domain to an unlabelled target domain. Due\nto the lack of supervision in the target domain, it is crucial to identify the\nunderlying similarity-and-dissimilarity relationships among the unlabelled\nsamples in the target domain. In order to use the whole data relationships\nefficiently in mini-batch training, we apply a series of memory modules to\nmaintain an up-to-date representation of the entire dataset. Unlike the simple\nexemplar memory in previous works, we propose a novel multi-level memory\nnetwork (MMN) to discover multi-level complementary information in the target\ndomain, relying on three memory modules, i.e., part-level memory,\ninstance-level memory, and domain-level memory. The proposed memory modules\nstore multi-level representations of the target domain, which capture both the\nfine-grained differences between images and the global structure for the\nholistic target domain. The three memory modules complement each other and\nsystematically integrate multi-level supervision from bottom to up. Experiments\non three datasets demonstrate that the multi-level memory modules cooperatively\nboost the unsupervised cross-domain Re-ID task, and the proposed MMN achieves\ncompetitive results.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 09:48:03 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Zhang", "Xinyu", ""], ["Gong", "Dong", ""], ["Cao", "Jiewei", ""], ["Shen", "Chunhua", ""]]}, {"id": "2001.04129", "submitter": "Jorge Calvo-Zaragoza", "authors": "Antonio-Javier Gallego, Jorge Calvo-Zaragoza, Robert B. Fisher", "title": "Incremental Unsupervised Domain-Adversarial Training of Neural Networks", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of supervised statistical learning, it is typically assumed\nthat the training set comes from the same distribution that draws the test\nsamples. When this is not the case, the behavior of the learned model is\nunpredictable and becomes dependent upon the degree of similarity between the\ndistribution of the training set and the distribution of the test set. One of\nthe research topics that investigates this scenario is referred to as domain\nadaptation. Deep neural networks brought dramatic advances in pattern\nrecognition and that is why there have been many attempts to provide good\ndomain adaptation algorithms for these models. Here we take a different avenue\nand approach the problem from an incremental point of view, where the model is\nadapted to the new domain iteratively. We make use of an existing unsupervised\ndomain-adaptation algorithm to identify the target samples on which there is\ngreater confidence about their true label. The output of the model is analyzed\nin different ways to determine the candidate samples. The selected set is then\nadded to the source training set by considering the labels provided by the\nnetwork as ground truth, and the process is repeated until all target samples\nare labelled. Our results report a clear improvement with respect to the\nnon-incremental case in several datasets, also outperforming other\nstate-of-the-art domain adaptation algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 09:54:35 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Gallego", "Antonio-Javier", ""], ["Calvo-Zaragoza", "Jorge", ""], ["Fisher", "Robert B.", ""]]}, {"id": "2001.04163", "submitter": "Dan Liu", "authors": "Dan Liu, Libo Zhang, Tiejian Luo, Lili Tao, Yanjun Wu", "title": "Towards Interpretable and Robust Hand Detection via Pixel-wise\n  Prediction", "comments": "Accepted to Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2020.107202", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of interpretability of existing CNN-based hand detection methods\nmakes it difficult to understand the rationale behind their predictions. In\nthis paper, we propose a novel neural network model, which introduces\ninterpretability into hand detection for the first time. The main improvements\ninclude: (1) Detect hands at pixel level to explain what pixels are the basis\nfor its decision and improve transparency of the model. (2) The explainable\nHighlight Feature Fusion block highlights distinctive features among multiple\nlayers and learns discriminative ones to gain robust performance. (3) We\nintroduce a transparent representation, the rotation map, to learn rotation\nfeatures instead of complex and non-transparent rotation and derotation layers.\n(4) Auxiliary supervision accelerates the training process, which saves more\nthan 10 hours in our experiments. Experimental results on the VIVA and Oxford\nhand detection and tracking datasets show competitive accuracy of our method\ncompared with state-of-the-art methods with higher speed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 11:18:09 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Liu", "Dan", ""], ["Zhang", "Libo", ""], ["Luo", "Tiejian", ""], ["Tao", "Lili", ""], ["Wu", "Yanjun", ""]]}, {"id": "2001.04189", "submitter": "Canjie Luo", "authors": "Canjie Luo, Qingxiang Lin, Yuliang Liu, Lianwen Jin, Chunhua Shen", "title": "Separating Content from Style Using Adversarial Learning for Recognizing\n  Text in the Wild", "comments": "Accepted to appear in International Journal of Computer Vision (IJCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to improve text recognition from a new perspective by separating\nthe text content from complex backgrounds. As vanilla GANs are not sufficiently\nrobust to generate sequence-like characters in natural images, we propose an\nadversarial learning framework for the generation and recognition of multiple\ncharacters in an image. The proposed framework consists of an attention-based\nrecognizer and a generative adversarial architecture. Furthermore, to tackle\nthe issue of lacking paired training samples, we design an interactive joint\ntraining scheme, which shares attention masks from the recognizer to the\ndiscriminator, and enables the discriminator to extract the features of each\ncharacter for further adversarial training. Benefiting from the character-level\nadversarial training, our framework requires only unpaired simple data for\nstyle supervision. Each target style sample containing only one randomly chosen\ncharacter can be simply synthesized online during the training. This is\nsignificant as the training does not require costly paired samples or\ncharacter-level annotations. Thus, only the input images and corresponding text\nlabels are needed. In addition to the style normalization of the backgrounds,\nwe refine character patterns to ease the recognition task. A feedback mechanism\nis proposed to bridge the gap between the discriminator and the recognizer.\nTherefore, the discriminator can guide the generator according to the confusion\nof the recognizer, so that the generated patterns are clearer for recognition.\nExperiments on various benchmarks, including both regular and irregular text,\ndemonstrate that our method significantly reduces the difficulty of\nrecognition. Our framework can be integrated into recent recognition methods to\nachieve new state-of-the-art recognition accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 12:41:42 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 12:41:05 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2020 08:11:22 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Luo", "Canjie", ""], ["Lin", "Qingxiang", ""], ["Liu", "Yuliang", ""], ["Jin", "Lianwen", ""], ["Shen", "Chunhua", ""]]}, {"id": "2001.04193", "submitter": "Mang Ye", "authors": "Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, Steven C. H.\n  Hoi", "title": "Deep Learning for Person Re-identification: A Survey and Outlook", "comments": "20 pages, 8 figures. Accepted by IEEE TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) aims at retrieving a person of interest\nacross multiple non-overlapping cameras. With the advancement of deep neural\nnetworks and increasing demand of intelligent video surveillance, it has gained\nsignificantly increased interest in the computer vision community. By\ndissecting the involved components in developing a person Re-ID system, we\ncategorize it into the closed-world and open-world settings. The widely studied\nclosed-world setting is usually applied under various research-oriented\nassumptions, and has achieved inspiring success using deep learning techniques\non a number of datasets. We first conduct a comprehensive overview with\nin-depth analysis for closed-world person Re-ID from three different\nperspectives, including deep feature representation learning, deep metric\nlearning and ranking optimization. With the performance saturation under\nclosed-world setting, the research focus for person Re-ID has recently shifted\nto the open-world setting, facing more challenging issues. This setting is\ncloser to practical applications under specific scenarios. We summarize the\nopen-world Re-ID in terms of five different aspects. By analyzing the\nadvantages of existing methods, we design a powerful AGW baseline, achieving\nstate-of-the-art or at least comparable performance on twelve datasets for FOUR\ndifferent Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP)\nfor person Re-ID, indicating the cost for finding all the correct matches,\nwhich provides an additional criteria to evaluate the Re-ID system for real\napplications. Finally, some important yet under-investigated open issues are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 12:49:22 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 03:17:49 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Ye", "Mang", ""], ["Shen", "Jianbing", ""], ["Lin", "Gaojie", ""], ["Xiang", "Tao", ""], ["Shao", "Ling", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2001.04208", "submitter": "Bharath K P", "authors": "Sai Abhishikth Ayyadevara, P N V Sai Ram Teja, Bharath K P, Rajesh\n  Kumar M", "title": "Handwritten Character Recognition Using Unique Feature Extraction\n  Technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  One of the most arduous and captivating domains under image processing is\nhandwritten character recognition. In this paper we have proposed a feature\nextraction technique which is a combination of unique features of geometric,\nzone-based hybrid, gradient features extraction approaches and three different\nneural networks namely the Multilayer Perceptron network using Backpropagation\nalgorithm (MLP BP), the Multilayer Perceptron network using Levenberg-Marquardt\nalgorithm (MLP LM) and the Convolutional neural network (CNN) which have been\nimplemented along with the Minimum Distance Classifier (MDC). The procedures\nlead to the conclusion that the proposed feature extraction algorithm is more\naccurate than its individual counterparts and also that Convolutional Neural\nNetwork is the most efficient neural network of the three in consideration.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:06:06 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Ayyadevara", "Sai Abhishikth", ""], ["Teja", "P N V Sai Ram", ""], ["P", "Bharath K", ""], ["M", "Rajesh Kumar", ""]]}, {"id": "2001.04215", "submitter": "Bharath K P", "authors": "Karthik R, Anvita Dwivedi, Haripriya M, Bharath K P, Rajesh Kumar M", "title": "Radial Based Analysis of GRNN in Non-Textured Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Image inpainting algorithms are used to restore some damaged or missing\ninformation region of an image based on the surrounding information. The method\nproposed in this paper applies the radial based analysis of image inpainting on\nGRNN. The damaged areas are first isolated from rest of the areas and then\narranged by their size and then inpainted using GRNN. The training of the\nneural network is done using different radii to achieve a better outcome. A\ncomparative analysis is done for different regression-based algorithms. The\noverall results are compared with the results achieved by the other algorithms\nas LS-SVM with reference to the PSNR value.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:12:15 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["R", "Karthik", ""], ["Dwivedi", "Anvita", ""], ["M", "Haripriya", ""], ["P", "Bharath K", ""], ["M", "Rajesh Kumar", ""]]}, {"id": "2001.04269", "submitter": "Clint Sebastian", "authors": "Clint Sebastian, Raffaele Imbriaco, Egor Bondarev, Peter H.N. de With", "title": "Adversarial Loss for Semantic Segmentation of Aerial Imagery", "comments": "IEEE Symposium on Information Theory and Signal Processing in the\n  Benelux (May 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic building extraction from aerial imagery has several applications in\nurban planning, disaster management, and change detection. In recent years,\nseveral works have adopted deep convolutional neural networks (CNNs) for\nbuilding extraction, since they produce rich features that are invariant\nagainst lighting conditions, shadows, etc. Although several advances have been\nmade, building extraction from aerial imagery still presents multiple\nchallenges. Most of the deep learning segmentation methods optimize the\nper-pixel loss with respect to the ground truth without knowledge of the\ncontext. This often leads to imperfect outputs that may lead to missing or\nunrefined regions. In this work, we propose a novel loss function combining\nboth adversarial and cross-entropy losses that learn to understand both local\nand global contexts for semantic segmentation. The newly proposed loss function\ndeployed on the DeepLab v3+ network obtains state-of-the-art results on the\nMassachusetts buildings dataset. The loss function improves the structure and\nrefines the edges of buildings without requiring any of the commonly used\npost-processing methods, such as Conditional Random Fields. We also perform\nablation studies to understand the impact of the adversarial loss. Finally, the\nproposed method achieves a relaxed F1 score of 95.59% on the Massachusetts\nbuildings dataset compared to the previous best F1 of 94.88%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:22:54 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 10:21:37 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Sebastian", "Clint", ""], ["Imbriaco", "Raffaele", ""], ["Bondarev", "Egor", ""], ["de With", "Peter H. N.", ""]]}, {"id": "2001.04271", "submitter": "Luigi Tommaso Luppino", "authors": "Luigi Tommaso Luppino, Michael Kampffmeyer, Filippo Maria Bianchi,\n  Gabriele Moser, Sebastiano Bruno Serpico, Robert Jenssen, and Stian Normann\n  Anfinsen", "title": "Deep Image Translation with an Affinity-Based Change Prior for\n  Unsupervised Multimodal Change Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2021.3056196", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image translation with convolutional neural networks has recently been used\nas an approach to multimodal change detection. Existing approaches train the\nnetworks by exploiting supervised information of the change areas, which,\nhowever, is not always available. A main challenge in the unsupervised problem\nsetting is to avoid that change pixels affect the learning of the translation\nfunction. We propose two new network architectures trained with loss functions\nweighted by priors that reduce the impact of change pixels on the learning\nobjective. The change prior is derived in an unsupervised fashion from\nrelational pixel information captured by domain-specific affinity matrices.\nSpecifically, we use the vertex degrees associated with an absolute affinity\ndifference matrix and demonstrate their utility in combination with cycle\nconsistency and adversarial training. The proposed neural networks are compared\nwith state-of-the-art algorithms. Experiments conducted on three real datasets\nshow the effectiveness of our methodology.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:23:24 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 13:57:53 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Luppino", "Luigi Tommaso", ""], ["Kampffmeyer", "Michael", ""], ["Bianchi", "Filippo Maria", ""], ["Moser", "Gabriele", ""], ["Serpico", "Sebastiano Bruno", ""], ["Jenssen", "Robert", ""], ["Anfinsen", "Stian Normann", ""]]}, {"id": "2001.04286", "submitter": "Maani Ghaffari Jadidi", "authors": "William Clark, Maani Ghaffari, and Anthony Bloch", "title": "Nonparametric Continuous Sensor Registration", "comments": "46 pages. arXiv admin note: substantial text overlap with\n  arXiv:1904.02266", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new mathematical framework that enables nonparametric\njoint semantic and geometric representation of continuous functions using data.\nThe joint embedding is modeled by representing the processes in a reproducing\nkernel Hilbert space. The functions can be defined on arbitrary smooth\nmanifolds where the action of a Lie group aligns them. The continuous functions\nallow the registration to be independent of a specific signal resolution. The\nframework is fully analytical with a closed-form derivation of the Riemannian\ngradient and Hessian. We study a more specialized but widely used case where\nthe Lie group acts on functions isometrically. We solve the problem by\nmaximizing the inner product between two functions defined over data, while the\ncontinuous action of the rigid body motion Lie group is captured through the\nintegration of the flow in the corresponding Lie algebra. Low-dimensional cases\nare derived with numerical examples to show the generality of the proposed\nframework. The high-dimensional derivation for the special Euclidean group\nacting on the Euclidean space showcases the point cloud registration and\nbird's-eye view map registration abilities. An implementation of this framework\nfor RGB-D cameras outperform the state-of-the-art robust visual odometry and\nperforms well in texture and structure-scares environments.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 17:40:54 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 20:23:50 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 06:05:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Clark", "William", ""], ["Ghaffari", "Maani", ""], ["Bloch", "Anthony", ""]]}, {"id": "2001.04296", "submitter": "Seunghoon Hong", "authors": "Wonkwang Lee, Donggyun Kim, Seunghoon Hong, Honglak Lee", "title": "High-Fidelity Synthesis with Disentangled Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representation of data without supervision is an\nimportant step towards improving the interpretability of generative models.\nDespite recent advances in disentangled representation learning, existing\napproaches often suffer from the trade-off between representation learning and\ngeneration performance i.e. improving generation quality sacrifices\ndisentanglement performance). We propose an Information-Distillation Generative\nAdversarial Network (ID-GAN), a simple yet generic framework that easily\nincorporates the existing state-of-the-art models for both disentanglement\nlearning and high-fidelity synthesis. Our method learns disentangled\nrepresentation using VAE-based models, and distills the learned representation\nwith an additional nuisance variable to the separate GAN-based generator for\nhigh-fidelity synthesis. To ensure that both generative models are aligned to\nrender the same generative factors, we further constrain the GAN generator to\nmaximize the mutual information between the learned latent code and the output.\nDespite the simplicity, we show that the proposed method is highly effective,\nachieving comparable image generation quality to the state-of-the-art methods\nusing the disentangled representation. We also show that the proposed\ndecomposition leads to an efficient and stable model design, and we demonstrate\nphoto-realistic high-resolution image synthesis results (1024x1024 pixels) for\nthe first time using the disentangled representations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:39:40 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Lee", "Wonkwang", ""], ["Kim", "Donggyun", ""], ["Hong", "Seunghoon", ""], ["Lee", "Honglak", ""]]}, {"id": "2001.04316", "submitter": "Abhinav Shukla", "authors": "Abhinav Shukla, Konstantinos Vougioukas, Pingchuan Ma, Stavros\n  Petridis, Maja Pantic", "title": "Visually Guided Self Supervised Learning of Speech Representations", "comments": "Accepted at ICASSP 2020 v2: Updated to the ICASSP 2020 camera ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self supervised representation learning has recently attracted a lot of\nresearch interest for both the audio and visual modalities. However, most works\ntypically focus on a particular modality or feature alone and there has been\nvery limited work that studies the interaction between the two modalities for\nlearning self supervised representations. We propose a framework for learning\naudio representations guided by the visual modality in the context of\naudiovisual speech. We employ a generative audio-to-video training scheme in\nwhich we animate a still image corresponding to a given audio clip and optimize\nthe generated video to be as close as possible to the real video of the speech\nsegment. Through this process, the audio encoder network learns useful speech\nrepresentations that we evaluate on emotion recognition and speech recognition.\nWe achieve state of the art results for emotion recognition and competitive\nresults for speech recognition. This demonstrates the potential of visual\nsupervision for learning audio representations as a novel way for\nself-supervised learning which has not been explored in the past. The proposed\nunsupervised audio features can leverage a virtually unlimited amount of\ntraining data of unlabelled audiovisual speech and have a large number of\npotentially promising applications.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:53:22 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 12:51:50 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Shukla", "Abhinav", ""], ["Vougioukas", "Konstantinos", ""], ["Ma", "Pingchuan", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2001.04322", "submitter": "Olivier Guye", "authors": "Olivier Guye", "title": "Hierarchical Modeling of Multidimensional Data in Regularly Decomposed\n  Spaces: Synthesis and Perspective", "comments": "60 pages, 9 figures, research report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This fourth and last tome is focusing on describing the envisioned works for\na project that has been presented in the preceding tome. It is about a new\napproach dedicated to the coding of still and moving pictures, trying to bridge\nthe MPEG-4 and MPEG-7 standard bodies. The aim of this project is to define the\nprinciples of self-descriptive video coding. In order to establish them, the\ndocument is composed in five chapters that describe the various envisioned\ntechniques for developing such a new approach in visual coding: - image\nsegmentation, - computation of visual descriptors, - computation of perceptual\ngroupings, - building of visual dictionaries, - picture and video coding. Based\non the techniques of multiresolution computing, it is proposed to develop an\nimage segmentation made from piecewise regular components, to compute\nattributes on the frame and the rendering of so produced shapes, independently\nto the geometric transforms that can occur in the image plane, and to gather\nthem into perceptual groupings so as to be able in performing recognition of\npartially hidden patterns. Due to vector quantization of shapes frame and\nrendering, it will appear that simple shapes may be compared to a visual\nalphabet and that complex shapes then become words written using this alphabet\nand be recorded into a dictionary. With the help of a nearest neighbour\nscanning applied on the picture shapes, the self-descriptive coding will then\ngenerate a sentence made from words written using the simple shape alphabet.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:59:07 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Guye", "Olivier", ""]]}, {"id": "2001.04325", "submitter": "Eli Chen", "authors": "Oren Haik, Oded Perry, Eli Chen, Peter Klammer", "title": "A Novel Inspection System For Variable Data Printing Using Deep Learning", "comments": "Accepted for publication in: Winter Applications of Computer Vision\n  (WACV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for inspecting variable data prints (VDP) with an\nultra-low false alarm rate (0.005%) and potential applicability to other\nreal-world problems. The system is based on a comparison between two images: a\nreference image and an image captured by low-cost scanners. The comparison task\nis challenging as low-cost imaging systems create artifacts that may\nerroneously be classified as true (genuine) defects. To address this challenge\nwe introduce two new fusion methods, for change detection applications, which\nare both fast and efficient. The first is an early fusion method that combines\nthe two input images into a single pseudo-color image. The second, called\nChange-Detection Single Shot Detector (CD-SSD) leverages the SSD by fusing\nfeatures in the middle of the network. We demonstrate the effectiveness of the\nproposed deep learning-based approach with a large dataset from real-world\nprinting scenarios. Finally, we evaluate our models on a different domain of\naerial imagery change detection (AICD). Our best method clearly outperforms the\nstate-of-the-art baseline on this dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 15:07:13 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Haik", "Oren", ""], ["Perry", "Oded", ""], ["Chen", "Eli", ""], ["Klammer", "Peter", ""]]}, {"id": "2001.04360", "submitter": "Bertrand Luvison", "authors": "Sanaa Chafik and Astrid Orcesi and Romaric Audigier and Bertrand\n  Luvison", "title": "Classifying All Interacting Pairs in a Single Shot", "comments": "WACV 2020 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel human interaction detection approach,\nbased on CALIPSO (Classifying ALl Interacting Pairs in a Single shOt), a\nclassifier of human-object interactions. This new single-shot interaction\nclassifier estimates interactions simultaneously for all human-object pairs,\nregardless of their number and class. State-of-the-art approaches adopt a\nmulti-shot strategy based on a pairwise estimate of interactions for a set of\nhuman-object candidate pairs, which leads to a complexity depending, at least,\non the number of interactions or, at most, on the number of candidate pairs. In\ncontrast, the proposed method estimates the interactions on the whole image.\nIndeed, it simultaneously estimates all interactions between all human subjects\nand object targets by performing a single forward pass throughout the image.\nConsequently, it leads to a constant complexity and computation time\nindependent of the number of subjects, objects or interactions in the image. In\ndetail, interaction classification is achieved on a dense grid of anchors\nthanks to a joint multi-task network that learns three complementary tasks\nsimultaneously: (i) prediction of the types of interaction, (ii) estimation of\nthe presence of a target and (iii) learning of an embedding which maps\ninteracting subject and target to a same representation, by using a metric\nlearning strategy. In addition, we introduce an object-centric passive-voice\nverb estimation which significantly improves results. Evaluations on the two\nwell-known Human-Object Interaction image datasets, V-COCO and HICO-DET,\ndemonstrate the competitiveness of the proposed method (2nd place) compared to\nthe state-of-the-art while having constant computation time regardless of the\nnumber of objects and interactions in the image.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 15:51:45 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Chafik", "Sanaa", ""], ["Orcesi", "Astrid", ""], ["Audigier", "Romaric", ""], ["Luvison", "Bertrand", ""]]}, {"id": "2001.04388", "submitter": "Silvan Weder", "authors": "Silvan Weder, Johannes L. Sch\\\"onberger, Marc Pollefeys, Martin R.\n  Oswald", "title": "RoutedFusion: Learning Real-time Depth Map Fusion", "comments": "11 pages, 8 figures, accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient fusion of depth maps is a key part of most state-of-the-art 3D\nreconstruction methods. Besides requiring high accuracy, these depth fusion\nmethods need to be scalable and real-time capable. To this end, we present a\nnovel real-time capable machine learning-based method for depth map fusion.\nSimilar to the seminal depth map fusion approach by Curless and Levoy, we only\nupdate a local group of voxels to ensure real-time capability. Instead of a\nsimple linear fusion of depth information, we propose a neural network that\npredicts non-linear updates to better account for typical fusion errors. Our\nnetwork is composed of a 2D depth routing network and a 3D depth fusion network\nwhich efficiently handle sensor-specific noise and outliers. This is especially\nuseful for surface edges and thin objects for which the original approach\nsuffers from thickening artifacts. Our method outperforms the traditional\nfusion approach and related learned approaches on both synthetic and real data.\nWe demonstrate the performance of our method in reconstructing fine geometric\ndetails from noise and outlier contaminated data on various scenes.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 16:46:41 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 09:15:29 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Weder", "Silvan", ""], ["Sch\u00f6nberger", "Johannes L.", ""], ["Pollefeys", "Marc", ""], ["Oswald", "Martin R.", ""]]}, {"id": "2001.04420", "submitter": "Jesus Tordesillas Torres", "authors": "Jesus Tordesillas, Brett T. Lopez, Michael Everett, and Jonathan P.\n  How", "title": "FASTER: Fast and Safe Trajectory Planner for Flights in Unknown\n  Environments", "comments": "Journal paper. arXiv admin note: substantial text overlap with\n  arXiv:1903.03558", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning high-speed trajectories for UAVs in unknown environments requires\nalgorithmic techniques that enable fast reaction times to guarantee safety as\nmore information about the environment becomes available. The standard approach\nto ensure safety is to enforce a \"stop\" condition in the free-known space.\nHowever, this can severely limit the speed of the vehicle, especially in\nsituations where much of the world is unknown. Moreover, the ad-hoc time and\ninterval allocation scheme usually imposed on the trajectory also leads to\nconservative and slower trajectories. This work proposes FASTER (Fast and Safe\nTrajectory Planner) to ensure safety without sacrificing speed. FASTER obtains\nhigh-speed trajectories by enabling the local planner to optimize in both the\nfree-known and unknown spaces. Safety guarantees are ensured by always having a\nfeasible, safe back-up trajectory in the free-known space at the start of each\nreplanning step. The Mixed Integer Quadratic Program formulation proposed\nallows the solver to choose the trajectory interval allocation, and the time\nallocation is found by a line search algorithm initialized with a heuristic\ncomputed from the previous replanning iteration. This proposed algorithm is\ntested extensively both in simulation and in real hardware, showing agile\nflights in unknown cluttered environments with velocities up to 7.8 m/s. To\ndemonstrate the generality of the proposed framework, FASTER is also applied to\na skid-steer robot, and the maximum speed specified for the robot (2 m/s) is\nachieved in real hardware experiments.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 23:50:58 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Tordesillas", "Jesus", ""], ["Lopez", "Brett T.", ""], ["Everett", "Michael", ""], ["How", "Jonathan P.", ""]]}, {"id": "2001.04433", "submitter": "Timothy Woinoski", "authors": "Timothy Woinoski and Alon Harell and Ivan V. Bajic", "title": "Towards Automated Swimming Analytics Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for creating a system to automate the collection of swimming\nanalytics on a pool-wide scale are considered in this paper. There has not been\nmuch work on swimmer tracking or the creation of a swimmer database for machine\nlearning purposes. Consequently, methods for collecting swimmer data from\nvideos of swim competitions are explored and analyzed. The result is a guide to\nthe creation of a comprehensive collection of swimming data suitable for\ntraining swimmer detection and tracking systems. With this database in place,\nsystems can then be created to automate the collection of swimming analytics.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:06:53 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Woinoski", "Timothy", ""], ["Harell", "Alon", ""], ["Bajic", "Ivan V.", ""]]}, {"id": "2001.04446", "submitter": "Shanlin Sun", "authors": "Shanlin Sun, Yang Liu, Narisu Bai, Hao Tang, Xuming Chen, Qian Huang,\n  Yong Liu, Xiaohui Xie", "title": "AttentionAnatomy: A unified framework for whole-body organs at risk\n  segmentation using multiple partially annotated datasets", "comments": "accepted by ISBI 2020 (4 pages, 2 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organs-at-risk (OAR) delineation in computed tomography (CT) is an important\nstep in Radiation Therapy (RT) planning. Recently, deep learning based methods\nfor OAR delineation have been proposed and applied in clinical practice for\nseparate regions of the human body (head and neck, thorax, and abdomen).\nHowever, there are few researches regarding the end-to-end whole-body OARs\ndelineation because the existing datasets are mostly partially or incompletely\nannotated for such task. In this paper, our proposed end-to-end convolutional\nneural network model, called \\textbf{AttentionAnatomy}, can be jointly trained\nwith three partially annotated datasets, segmenting OARs from whole body. Our\nmain contributions are: 1) an attention module implicitly guided by body region\nlabel to modulate the segmentation branch output; 2) a prediction\nre-calibration operation, exploiting prior information of the input images, to\nhandle partial-annotation(HPA) problem; 3) a new hybrid loss function combining\nbatch Dice loss and spatially balanced focal loss to alleviate the organ size\nimbalance problem. Experimental results of our proposed framework presented\nsignificant improvements in both S{\\o}rensen-Dice coefficient (DSC) and 95\\%\nHausdorff distance compared to the baseline model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:31:34 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Sun", "Shanlin", ""], ["Liu", "Yang", ""], ["Bai", "Narisu", ""], ["Tang", "Hao", ""], ["Chen", "Xuming", ""], ["Huang", "Qian", ""], ["Liu", "Yong", ""], ["Xie", "Xiaohui", ""]]}, {"id": "2001.04463", "submitter": "Kangle Deng", "authors": "Kangle Deng and Aayush Bansal and Deva Ramanan", "title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders", "comments": "ICLR 2021; Project page -- https://www.cs.cmu.edu/~exemplar-ae/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an unsupervised approach that converts the input speech of any\nindividual into audiovisual streams of potentially-infinitely many output\nspeakers. Our approach builds on simple autoencoders that project out-of-sample\ndata onto the distribution of the training set. We use Exemplar Autoencoders to\nlearn the voice, stylistic prosody, and visual appearance of a specific target\nexemplar speech. In contrast to existing methods, the proposed approach can be\neasily extended to an arbitrarily large number of speakers and styles using\nonly 3 minutes of target audio-video data, without requiring {\\em any} training\ndata for the input speaker. To do so, we learn audiovisual bottleneck\nrepresentations that capture the structured linguistic content of speech. We\noutperform prior approaches on both audio and video synthesis, and provide\nextensive qualitative analysis on our project page --\nhttps://www.cs.cmu.edu/~exemplar-ae/.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:56:45 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 17:59:14 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 05:24:44 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Deng", "Kangle", ""], ["Bansal", "Aayush", ""], ["Ramanan", "Deva", ""]]}, {"id": "2001.04529", "submitter": "Madan Ravi Ganesh", "authors": "Madan Ravi Ganesh and Jason J. Corso", "title": "Rethinking Curriculum Learning with Incremental Labels and Adaptive\n  Compensation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like humans, deep networks have been shown to learn better when samples are\norganized and introduced in a meaningful order or curriculum. Conventional\ncurriculum learning schemes introduce samples in their order of difficulty.\nThis forces models to begin learning from a subset of the available data while\nadding the external overhead of evaluating the difficulty of samples. In this\nwork, we propose Learning with Incremental Labels and Adaptive Compensation\n(LILAC), a two-phase method that incrementally increases the number of unique\noutput labels rather than the difficulty of samples while consistently using\nthe entire dataset throughout training. In the first phase, Incremental Label\nIntroduction, we partition data into mutually exclusive subsets, one that\ncontains a subset of the ground-truth labels and another that contains the\nremaining data attached to a pseudo-label. Throughout the training process, we\nrecursively reveal unseen ground-truth labels in fixed increments until all the\nlabels are known to the model. In the second phase, Adaptive Compensation, we\noptimize the loss function using altered target vectors of previously\nmisclassified samples. The target vectors of such samples are modified to a\nsmoother distribution to help models learn better. On evaluating across three\nstandard image benchmarks, CIFAR-10, CIFAR-100, and STL-10, we show that LILAC\noutperforms all comparable baselines. Further, we detail the importance of\npacing the introduction of new labels to a model as well as the impact of using\na smooth target vector.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 21:00:46 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 14:24:06 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 16:00:02 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Ganesh", "Madan Ravi", ""], ["Corso", "Jason J.", ""]]}, {"id": "2001.04537", "submitter": "Sunyi Zheng", "authors": "Sunyi Zheng, Ludo J. Cornelissen, Xiaonan Cui, Xueping Jing, Raymond\n  N. J. Veldhuis, Matthijs Oudkerk, and Peter M.A. van Ooijen", "title": "Deep convolutional neural networks for multi-planar lung nodule\n  detection: improvement in small nodule identification", "comments": null, "journal-ref": null, "doi": "10.1002/mp.14648", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: In clinical practice, small lung nodules can be easily overlooked\nby radiologists. The paper aims to provide an efficient and accurate detection\nsystem for small lung nodules while keeping good performance for large nodules.\nMethods: We propose a multi-planar detection system using convolutional neural\nnetworks. The 2-D convolutional neural network model, U-net++, was trained by\naxial, coronal, and sagittal slices for the candidate detection task. All\npossible nodule candidates from the three different planes are combined. For\nfalse positive reduction, we apply 3-D multi-scale dense convolutional neural\nnetworks to efficiently remove false positive candidates. We use the public\nLIDC-IDRI dataset which includes 888 CT scans with 1186 nodules annotated by\nfour radiologists. Results: After ten-fold cross-validation, our proposed\nsystem achieves a sensitivity of 94.2% with 1.0 false positive/scan and a\nsensitivity of 96.0% with 2.0 false positives/scan. Although it is difficult to\ndetect small nodules (i.e. < 6 mm), our designed CAD system reaches a\nsensitivity of 93.4% (95.0%) of these small nodules at an overall false\npositive rate of 1.0 (2.0) false positives/scan. At the nodule candidate\ndetection stage, results show that a multi-planar method is capable to detect\nmore nodules compared to using a single plane. Conclusion: Our approach\nachieves good performance not only for small nodules, but also for large\nlesions on this dataset. This demonstrates the effectiveness and efficiency of\nour developed CAD system for lung nodule detection. Significance: The proposed\nsystem could provide support for radiologists on early detection of lung\ncancer.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 21:37:33 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 21:26:34 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 20:40:29 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zheng", "Sunyi", ""], ["Cornelissen", "Ludo J.", ""], ["Cui", "Xiaonan", ""], ["Jing", "Xueping", ""], ["Veldhuis", "Raymond N. J.", ""], ["Oudkerk", "Matthijs", ""], ["van Ooijen", "Peter M. A.", ""]]}, {"id": "2001.04541", "submitter": "Bowen Zhang", "authors": "Bowen Zhang, Hexiang Hu, Fei Sha", "title": "Visual Storytelling via Predicting Anchor Word Embeddings in the Stories", "comments": "Accepted by ICCV19 CLVL workshop: 3rd Workshop on Closing the Loop\n  Between Vision and Language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning model for the task of visual storytelling. The main\nidea is to predict anchor word embeddings from the images and use the\nembeddings and the image features jointly to generate narrative sentences. We\nuse the embeddings of randomly sampled nouns from the groundtruth stories as\nthe target anchor word embeddings to learn the predictor. To narrate a sequence\nof images, we use the predicted anchor word embeddings and the image features\nas the joint input to a seq2seq model. As opposed to state-of-the-art methods,\nthe proposed model is simple in design, easy to optimize, and attains the best\nresults in most automatic evaluation metrics. In human evaluation, the method\nalso outperforms competing methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 21:47:12 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Zhang", "Bowen", ""], ["Hu", "Hexiang", ""], ["Sha", "Fei", ""]]}, {"id": "2001.04547", "submitter": "Aparna Bharati", "authors": "Aparna Bharati, Daniel Moreira, Patrick Flynn, Anderson Rocha, Kevin\n  Bowyer, Walter Scheirer", "title": "Learning Transformation-Aware Embeddings for Image Forensics", "comments": "Supplemental material for this paper is available at\n  https://drive.google.com/file/d/1covDhaTN24zkmyQf1XCTZHNrUZdZqGyo/view?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dramatic rise in the flow of manipulated image content on the Internet has\nled to an aggressive response from the media forensics research community. New\nefforts have incorporated increased usage of techniques from computer vision\nand machine learning to detect and profile the space of image manipulations.\nThis paper addresses Image Provenance Analysis, which aims at discovering\nrelationships among different manipulated image versions that share content.\nOne of the main sub-problems for provenance analysis that has not yet been\naddressed directly is the edit ordering of images that share full content or\nare near-duplicates. The existing large networks that generate image\ndescriptors for tasks such as object recognition may not encode the subtle\ndifferences between these image covariates. This paper introduces a novel deep\nlearning-based approach to provide a plausible ordering to images that have\nbeen generated from a single image through transformations. Our approach learns\ntransformation-aware descriptors using weak supervision via composited\ntransformations and a rank-based quadruplet loss. To establish the efficacy of\nthe proposed approach, comparisons with state-of-the-art handcrafted and deep\nlearning-based descriptors, and image matching approaches are made. Further\nexperimentation validates the proposed approach in the context of image\nprovenance analysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 22:01:24 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Bharati", "Aparna", ""], ["Moreira", "Daniel", ""], ["Flynn", "Patrick", ""], ["Rocha", "Anderson", ""], ["Bowyer", "Kevin", ""], ["Scheirer", "Walter", ""]]}, {"id": "2001.04552", "submitter": "Luca Puglia", "authors": "Luca Puglia and Cormac Brick", "title": "Deep Learning Stereo Vision at the edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an overview of the methodology used to build a new stereo vision\nsolution that is suitable for System on Chip. This new solution was developed\nto bring computer vision capability to embedded devices that live in a power\nconstrained environment. The solution is constructured as a hybrid between\nclassical Stereo Vision techniques and deep learning approaches. The\nstereoscopic module is composed of two separate modules: one that accelerates\nthe neural network we trained and one that accelerates the front-end part. The\nsystem is completely passive and does not require any structured light to\nobtain very compelling accuracy. With respect to the previous Stereo Vision\nsolutions offered by the industries we offer a major improvement is robustness\nto noise. This is mainly possible due to the deep learning part of the chosen\narchitecture. We submitted our result to Middlebury dataset challenge. It\ncurrently ranks as the best System on Chip solution. The system has been\ndeveloped for low latency applications which require better than real time\nperformance on high definition videos.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 22:30:41 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Puglia", "Luca", ""], ["Brick", "Cormac", ""]]}, {"id": "2001.04559", "submitter": "Ali Dabouei", "authors": "Ali Dabouei, Fariborz Taherkhani, Sobhan Soleymani, Jeremy Dawson,\n  Nasser M. Nasrabadi", "title": "Boosting Deep Face Recognition via Disentangling Appearance and Geometry", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a framework for disentangling the appearance and\ngeometry representations in the face recognition task. To provide supervision\nfor this aim, we generate geometrically identical faces by incorporating\nspatial transformations. We demonstrate that the proposed approach enhances the\nperformance of deep face recognition models by assisting the training process\nin two ways. First, it enforces the early and intermediate convolutional layers\nto learn more representative features that satisfy the properties of\ndisentangled embeddings. Second, it augments the training set by altering faces\ngeometrically. Through extensive experiments, we demonstrate that integrating\nthe proposed approach into state-of-the-art face recognition methods\neffectively improves their performance on challenging datasets, such as LFW,\nYTF, and MegaFace. Both theoretical and practical aspects of the method are\nanalyzed rigorously by concerning ablation studies and knowledge transfer\ntasks. Furthermore, we show that the knowledge leaned by the proposed method\ncan favor other face-related tasks, such as attribute prediction.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 23:19:58 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Dabouei", "Ali", ""], ["Taherkhani", "Fariborz", ""], ["Soleymani", "Sobhan", ""], ["Dawson", "Jeremy", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2001.04568", "submitter": "Zhenqiang Ying", "authors": "Zhenqiang Ying and Alan Bovik", "title": "180-degree Outpainting from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Presenting context images to a viewer's peripheral vision is one of the most\neffective techniques to enhance immersive visual experiences. However, most\nimages only present a narrow view, since the field-of-view (FoV) of standard\ncameras is small. To overcome this limitation, we propose a deep learning\napproach that learns to predict a 180{\\deg} panoramic image from a narrow-view\nimage. Specifically, we design a foveated framework that applies different\nstrategies on near-periphery and mid-periphery regions. Two networks are\ntrained separately, and then are employed jointly to sequentially perform\nnarrow-to-90{\\deg} generation and 90{\\deg}-to-180{\\deg} generation. The\ngenerated outputs are then fused with their aligned inputs to produce expanded\nequirectangular images for viewing. Our experimental results show that\nsingle-view-to-panoramic image generation using deep learning is both feasible\nand promising.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 23:50:34 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Ying", "Zhenqiang", ""], ["Bovik", "Alan", ""]]}, {"id": "2001.04580", "submitter": "Xiyang Luo", "authors": "Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang, Peyman Milanfar", "title": "Distortion Agnostic Deep Watermarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watermarking is the process of embedding information into an image that can\nsurvive under distortions, while requiring the encoded image to have little or\nno perceptual difference from the original image. Recently, deep learning-based\nmethods achieved impressive results in both visual quality and message payload\nunder a wide variety of image distortions. However, these methods all require\ndifferentiable models for the image distortions at training time, and may\ngeneralize poorly to unknown distortions. This is undesirable since the types\nof distortions applied to watermarked images are usually unknown and\nnon-differentiable. In this paper, we propose a new framework for\ndistortion-agnostic watermarking, where the image distortion is not explicitly\nmodeled during training. Instead, the robustness of our system comes from two\nsources: adversarial training and channel coding. Compared to training on a\nfixed set of distortions and noise levels, our method achieves comparable or\nbetter results on distortions available during training, and better performance\non unknown distortions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 01:04:59 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Luo", "Xiyang", ""], ["Zhan", "Ruohan", ""], ["Chang", "Huiwen", ""], ["Yang", "Feng", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2001.04583", "submitter": "Tushar Nagarajan", "authors": "Tushar Nagarajan, Yanghao Li, Christoph Feichtenhofer, Kristen Grauman", "title": "EGO-TOPO: Environment Affordances from Egocentric Video", "comments": "Published in CVPR 2020, project page:\n  http://vision.cs.utexas.edu/projects/ego-topo/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-person video naturally brings the use of a physical environment to the\nforefront, since it shows the camera wearer interacting fluidly in a space\nbased on his intentions. However, current methods largely separate the observed\nactions from the persistent space itself. We introduce a model for environment\naffordances that is learned directly from egocentric video. The main idea is to\ngain a human-centric model of a physical space (such as a kitchen) that\ncaptures (1) the primary spatial zones of interaction and (2) the likely\nactivities they support. Our approach decomposes a space into a topological map\nderived from first-person activity, organizing an ego-video into a series of\nvisits to the different zones. Further, we show how to link zones across\nmultiple related environments (e.g., from videos of multiple kitchens) to\nobtain a consolidated representation of environment functionality. On\nEPIC-Kitchens and EGTEA+, we demonstrate our approach for learning scene\naffordances and anticipating future actions in long-form video.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 01:20:39 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 20:30:19 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Nagarajan", "Tushar", ""], ["Li", "Yanghao", ""], ["Feichtenhofer", "Christoph", ""], ["Grauman", "Kristen", ""]]}, {"id": "2001.04608", "submitter": "Zixu Wang", "authors": "Yixuan Li, Zixu Wang, Limin Wang, Gangshan Wu", "title": "Actions as Moving Points", "comments": "ECCV2020 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing action tubelet detectors often depend on heuristic anchor design\nand placement, which might be computationally expensive and sub-optimal for\nprecise localization. In this paper, we present a conceptually simple,\ncomputationally efficient, and more precise action tubelet detection framework,\ntermed as MovingCenter Detector (MOC-detector), by treating an action instance\nas a trajectory of moving points. Based on the insight that movement\ninformation could simplify and assist action tubelet detection, our\nMOC-detector is composed of three crucial head branches: (1) Center Branch for\ninstance center detection and action recognition, (2) Movement Branch for\nmovement estimation at adjacent frames to form trajectories of moving points,\n(3) Box Branch for spatial extent detection by directly regressing bounding box\nsize at each estimated center. These three branches work together to generate\nthe tubelet detection results, which could be further linked to yield\nvideo-level tubes with a matching strategy. Our MOC-detector outperforms the\nexisting state-of-the-art methods for both metrics of frame-mAP and video-mAP\non the JHMDB and UCF101-24 datasets. The performance gap is more evident for\nhigher video IoU, demonstrating that our MOC-detector is particularly effective\nfor more precise action detection. We provide the code at\nhttps://github.com/MCG-NJU/MOC-Detector.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 03:29:44 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 18:07:24 GMT"}, {"version": "v3", "created": "Sat, 22 Aug 2020 14:45:35 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Li", "Yixuan", ""], ["Wang", "Zixu", ""], ["Wang", "Limin", ""], ["Wu", "Gangshan", ""]]}, {"id": "2001.04609", "submitter": "Qi Wang", "authors": "Qi Wang, Qiang Li, and Xuelong Li", "title": "Spatial-Spectral Residual Network for Hyperspectral Image\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based hyperspectral image super-resolution (SR) methods have\nachieved great success recently. However, most existing models can not\neffectively explore spatial information and spectral information between bands\nsimultaneously, obtaining relatively low performance. To address this issue, in\nthis paper, we propose a novel spectral-spatial residual network for\nhyperspectral image super-resolution (SSRNet). Our method can effectively\nexplore spatial-spectral information by using 3D convolution instead of 2D\nconvolution, which enables the network to better extract potential information.\nFurthermore, we design a spectral-spatial residual module (SSRM) to adaptively\nlearn more effective features from all the hierarchical features in units\nthrough local feature fusion, significantly improving the performance of the\nalgorithm. In each unit, we employ spatial and temporal separable 3D\nconvolution to extract spatial and spectral information, which not only reduces\nunaffordable memory usage and high computational cost, but also makes the\nnetwork easier to train. Extensive evaluations and comparisons on three\nbenchmark datasets demonstrate that the proposed approach achieves superior\nperformance in comparison to existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 03:34:55 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Wang", "Qi", ""], ["Li", "Qiang", ""], ["Li", "Xuelong", ""]]}, {"id": "2001.04621", "submitter": "Yao Yongqiang", "authors": "Yongqiang Yao, Yan Wang, Yu Guo, Jiaojiao Lin, Hongwei Qin, Junjie Yan", "title": "Cross-dataset Training for Class Increasing Object Detection", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conceptually simple, flexible and general framework for\ncross-dataset training in object detection. Given two or more already labeled\ndatasets that target for different object classes, cross-dataset training aims\nto detect the union of the different classes, so that we do not have to label\nall the classes for all the datasets. By cross-dataset training, existing\ndatasets can be utilized to detect the merged object classes with a single\nmodel. Further more, in industrial applications, the object classes usually\nincrease on demand. So when adding new classes, it is quite time-consuming if\nwe label the new classes on all the existing datasets. While using\ncross-dataset training, we only need to label the new classes on the new\ndataset. We experiment on PASCAL VOC, COCO, WIDER FACE and WIDER Pedestrian\nwith both solo and cross-dataset settings. Results show that our cross-dataset\npipeline can achieve similar impressive performance simultaneously on these\ndatasets compared with training independently.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 04:40:47 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Yao", "Yongqiang", ""], ["Wang", "Yan", ""], ["Guo", "Yu", ""], ["Lin", "Jiaojiao", ""], ["Qin", "Hongwei", ""], ["Yan", "Junjie", ""]]}, {"id": "2001.04625", "submitter": "Lu Wang", "authors": "Lu Wang, Jie Yang", "title": "Asymmetric Correlation Quantization Hashing for Cross-modal Retrieval", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the superiority in similarity computation and database storage for\nlarge-scale multiple modalities data, cross-modal hashing methods have\nattracted extensive attention in similarity retrieval across the heterogeneous\nmodalities. However, there are still some limitations to be further taken into\naccount: (1) most current CMH methods transform real-valued data points into\ndiscrete compact binary codes under the binary constraints, limiting the\ncapability of representation for original data on account of abundant loss of\ninformation and producing suboptimal hash codes; (2) the discrete binary\nconstraint learning model is hard to solve, where the retrieval performance may\ngreatly reduce by relaxing the binary constraints for large quantization error;\n(3) handling the learning problem of CMH in a symmetric framework, leading to\ndifficult and complex optimization objective. To address above challenges, in\nthis paper, a novel Asymmetric Correlation Quantization Hashing (ACQH) method\nis proposed. Specifically, ACQH learns the projection matrixs of heterogeneous\nmodalities data points for transforming query into a low-dimensional\nreal-valued vector in latent semantic space and constructs the stacked\ncompositional quantization embedding in a coarse-to-fine manner for indicating\ndatabase points by a series of learnt real-valued codeword in the codebook with\nthe help of pointwise label information regression simultaneously. Besides, the\nunified hash codes across modalities can be directly obtained by the discrete\niterative optimization framework devised in the paper. Comprehensive\nexperiments on diverse three benchmark datasets have shown the effectiveness\nand rationality of ACQH.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 04:53:30 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Wang", "Lu", ""], ["Yang", "Jie", ""]]}, {"id": "2001.04627", "submitter": "Piotr Koniusz", "authors": "Lei Wang and Piotr Koniusz", "title": "Hallucinating Statistical Moment and Subspace Descriptors from Object\n  and Saliency Detectors for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we build on a deep translational action recognition network\nwhich takes RGB frames as input to learn to predict both action concepts and\nauxiliary supervisory feature descriptors e.g., Optical Flow Features and/or\nImproved Dense Trajectory descriptors. The translation is performed by\nso-called hallucination streams trained to predict auxiliary cues which are\nsimultaneously fed into classification layers, and then hallucinated for free\nat the testing stage to boost recognition.\n  In this paper, we design and hallucinate two descriptors, one leveraging four\npopular object detectors applied to training videos, and the other leveraging\nimage- and video-level saliency detectors. The first descriptor encodes the\ndetector- and ImageNet-wise class prediction scores, confidence scores, and\nspatial locations of bounding boxes and frame indexes to capture the\nspatio-temporal distribution of features per video. Another descriptor encodes\nspatio-angular gradient distributions of saliency maps and intensity patterns.\n  Inspired by the characteristic function of the probability distribution, we\ncapture four statistical moments on the above intermediate descriptors. As\nnumbers of coefficients in the mean, covariance, coskewness and cokurtotsis\ngrow linearly, quadratically, cubically and quartically w.r.t. the dimension of\nfeature vectors, we describe the covariance matrix by its leading n'\neigenvectors (so-called subspace) and we capture skewness/kurtosis rather than\ncostly coskewness/cokurtosis. We obtain state of the art on three popular\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 05:03:54 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Wang", "Lei", ""], ["Koniusz", "Piotr", ""]]}, {"id": "2001.04642", "submitter": "Jeong Joon Park", "authors": "Jeong Joon Park and Aleksander Holynski and Steve Seitz", "title": "Seeing the World in a Bag of Chips", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the dual problems of novel view synthesis and environment\nreconstruction from hand-held RGBD sensors. Our contributions include 1)\nmodeling highly specular objects, 2) modeling inter-reflections and Fresnel\neffects, and 3) enabling surface light field reconstruction with the same input\nneeded to reconstruct shape alone. In cases where scene surface has a strong\nmirror-like material component, we generate highly detailed environment images,\nrevealing room composition, objects, people, buildings, and trees visible\nthrough windows. Our approach yields state of the art view synthesis\ntechniques, operates on low dynamic range imagery, and is robust to geometric\nand calibration errors.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 06:44:44 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 16:15:06 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Park", "Jeong Joon", ""], ["Holynski", "Aleksander", ""], ["Seitz", "Steve", ""]]}, {"id": "2001.04647", "submitter": "JongMok Kim", "authors": "Jongmok Kim, Jooyoung Jang, Hyunwoo Park", "title": "Structured Consistency Loss for semi-supervised semantic segmentation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consistency loss has played a key role in solving problems in recent\nstudies on semi-supervised learning. Yet extant studies with the consistency\nloss are limited to its application to classification tasks; extant studies on\nsemi-supervised semantic segmentation rely on pixel-wise classification, which\ndoes not reflect the structured nature of characteristics in prediction. We\npropose a structured consistency loss to address this limitation of extant\nstudies. Structured consistency loss promotes consistency in inter-pixel\nsimilarity between teacher and student networks. Specifically, collaboration\nwith CutMix optimizes the efficient performance of semi-supervised semantic\nsegmentation with structured consistency loss by reducing computational burden\ndramatically. The superiority of proposed method is verified with the\nCityscapes; The Cityscapes benchmark results with validation and with test data\nare 81.9 mIoU and 83.84 mIoU respectively. This ranks the first place on the\npixel-level semantic labeling task of Cityscapes benchmark suite. To the best\nof our knowledge, we are the first to present the superiority of\nstate-of-the-art semi-supervised learning in semantic segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 07:08:45 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Kim", "Jongmok", ""], ["Jang", "Jooyoung", ""], ["Park", "Hyunwoo", ""]]}, {"id": "2001.04663", "submitter": "Pargorn Puttapirat", "authors": "Jiangbo Shi, Zeyu Gao, Haichuan Zhang, Pargorn Puttapirat, Chunbao\n  Wang, Xiangrong Zhang, Chen Li", "title": "Effects of annotation granularity in deep learning models for\n  histopathological images", "comments": "Accepted by AIPath2019 Workshop in BIBM2019. 7 pages, 4 figures, 4\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pathological is crucial to cancer diagnosis. Usually, Pathologists draw their\nconclusion based on observed cell and tissue structure on histology slides.\nRapid development in machine learning, especially deep learning have\nestablished robust and accurate classifiers. They are being used to analyze\nhistopathological slides and assist pathologists in diagnosis. Most machine\nlearning systems rely heavily on annotated data sets to gain experiences and\nknowledge to correctly and accurately perform various tasks such as\nclassification and segmentation. This work investigates different granularity\nof annotations in histopathological data set including image-wise, bounding\nbox, ellipse-wise, and pixel-wise to verify the influence of annotation in\npathological slide on deep learning models. We design corresponding experiments\nto test classification and segmentation performance of deep learning models\nbased on annotations with different annotation granularity. In classification,\nstate-of-the-art deep learning-based classifiers perform better when trained by\npixel-wise annotation dataset. On average, precision, recall and F1-score\nimproves by 7.87%, 8.83% and 7.85% respectively. Thus, it is suggested that\nfiner granularity annotations are better utilized by deep learning algorithms\nin classification tasks. Similarly, semantic segmentation algorithms can\nachieve 8.33% better segmentation accuracy when trained by pixel-wise\nannotations. Our study shows not only that finer-grained annotation can improve\nthe performance of deep learning models, but also help extracts more accurate\nphenotypic information from histopathological slides. Intelligence systems\ntrained on granular annotations may help pathologists inspecting certain\nregions for better diagnosis. The compartmentalized prediction approach similar\nto this work may contribute to phenotype and genotype association studies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 08:39:51 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Shi", "Jiangbo", ""], ["Gao", "Zeyu", ""], ["Zhang", "Haichuan", ""], ["Puttapirat", "Pargorn", ""], ["Wang", "Chunbao", ""], ["Zhang", "Xiangrong", ""], ["Li", "Chen", ""]]}, {"id": "2001.04665", "submitter": "Wenjie Ai", "authors": "X G Tu, Y Luo, H S Zhang, W J Ai, Z Ma, and M Xie", "title": "Face Attribute Invertion", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulating human facial images between two domains is an important and\ninteresting problem. Most of the existing methods address this issue by\napplying two generators or one generator with extra conditional inputs. In this\npaper, we proposed a novel self-perception method based on GANs for automatical\nface attribute inverse. The proposed method takes face images as inputs and\nemploys only one single generator without being conditioned on other inputs.\nProfiting from the multi-loss strategy and modified U-net structure, our model\nis quite stable in training and capable of preserving finer details of the\noriginal face images.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 08:41:52 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Tu", "X G", ""], ["Luo", "Y", ""], ["Zhang", "H S", ""], ["Ai", "W J", ""], ["Ma", "Z", ""], ["Xie", "M", ""]]}, {"id": "2001.04692", "submitter": "Marco Toldo", "authors": "Marco Toldo and Umberto Michieli and Gianluca Agresti and Pietro\n  Zanuttigh", "title": "Unsupervised Domain Adaptation for Mobile Semantic Segmentation based on\n  Cycle Consistency and Feature Alignment", "comments": "11 pages, 3 figures, 3 tables", "journal-ref": "Image and Vision Computing, Volume 95, March 2020", "doi": "10.1016/j.imavis.2020.103889", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The supervised training of deep networks for semantic segmentation requires a\nhuge amount of labeled real world data. To solve this issue, a commonly\nexploited workaround is to use synthetic data for training, but deep networks\nshow a critical performance drop when analyzing data with slightly different\nstatistical properties with respect to the training set. In this work, we\npropose a novel Unsupervised Domain Adaptation (UDA) strategy to address the\ndomain shift issue between real world and synthetic representations. An\nadversarial model, based on the cycle consistency framework, performs the\nmapping between the synthetic and real domain. The data is then fed to a\nMobileNet-v2 architecture that performs the semantic segmentation task. An\nadditional couple of discriminators, working at the feature level of the\nMobileNet-v2, allows to better align the features of the two domain\ndistributions and to further improve the performance. Finally, the consistency\nof the semantic maps is exploited. After an initial supervised training on\nsynthetic data, the whole UDA architecture is trained end-to-end considering\nall its components at once. Experimental results show how the proposed strategy\nis able to obtain impressive performance in adapting a segmentation network\ntrained on synthetic data to real world scenarios. The usage of the lightweight\nMobileNet-v2 architecture allows its deployment on devices with limited\ncomputational resources as the ones employed in autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 10:12:20 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 10:22:43 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Toldo", "Marco", ""], ["Michieli", "Umberto", ""], ["Agresti", "Gianluca", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "2001.04708", "submitter": "Ibrahim Halfaoui", "authors": "Ibrahim Halfaoui, Fahd Bouzaraa, Onay Urfalioglu, Li Minzhen", "title": "Real-Time Lane ID Estimation Using Recurrent Neural Networks With Dual\n  Convention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring information about the road lane structure is a crucial step for\nautonomous navigation. To this end, several approaches tackle this task from\ndifferent perspectives such as lane marking detection or semantic lane\nsegmentation. However, to the best of our knowledge, there is yet no purely\nvision based end-to-end solution to answer the precise question: How to\nestimate the relative number or \"ID\" of the current driven lane within a\nmulti-lane road or a highway? In this work, we propose a real-time, vision-only\n(i.e. monocular camera) solution to the problem based on a dual left-right\nconvention. We interpret this task as a classification problem by limiting the\nmaximum number of lane candidates to eight. Our approach is designed to meet\nlow-complexity specifications and limited runtime requirements. It harnesses\nthe temporal dimension inherent to the input sequences to improve upon\nhigh-complexity state-of-the-art models. We achieve more than 95% accuracy on a\nchallenging test set with extreme conditions and different routes.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 10:52:30 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Halfaoui", "Ibrahim", ""], ["Bouzaraa", "Fahd", ""], ["Urfalioglu", "Onay", ""], ["Minzhen", "Li", ""]]}, {"id": "2001.04716", "submitter": "Sergio Vitale", "authors": "Sergio Vitale, Giampaolo Ferraioli, Vito Pascazio", "title": "Edge Preserving CNN SAR Despeckling Algorithm", "comments": "Accepted to LAGIRS 2020", "journal-ref": "2020 IEEE Latin American GRSS & ISPRS Remote Sensing Conference\n  (LAGIRS)", "doi": "10.1109/LAGIRS48042.2020.9165559", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SAR despeckling is a key tool for Earth Observation. Interpretation of SAR\nimages are impaired by speckle, a multiplicative noise related to interference\nof backscattering from the illuminated scene towards the sensor. Reducing the\nnoise is a crucial task for the understanding of the scene. Based on the\nresults of our previous solution KL-DNN, in this work we define a new cost\nfunction for training a convolutional neural network for despeckling. The aim\nis to control the edge preservation and to better filter manmade structures and\nurban areas that are very challenging for KL-DNN. The results show a very good\nimprovement on the not homogeneous areas keeping the good results in the\nhomogeneous ones. Result on both simulated and real data are shown in the\npaper.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 11:26:43 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 11:23:24 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 13:04:13 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Vitale", "Sergio", ""], ["Ferraioli", "Giampaolo", ""], ["Pascazio", "Vito", ""]]}, {"id": "2001.04732", "submitter": "Andr\\'es Mafla", "authors": "Andres Mafla, Sounak Dey, Ali Furkan Biten, Lluis Gomez, Dimosthenis\n  Karatzas", "title": "Fine-grained Image Classification and Retrieval by Combining Visual and\n  Locally Pooled Textual Features", "comments": "Winter Conference on Applications of Computer Vision (WACV 2020)\n  Accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text contained in an image carries high-level semantics that can be exploited\nto achieve richer image understanding. In particular, the mere presence of text\nprovides strong guiding content that should be employed to tackle a diversity\nof computer vision tasks such as image retrieval, fine-grained classification,\nand visual question answering. In this paper, we address the problem of\nfine-grained classification and image retrieval by leveraging textual\ninformation along with visual cues to comprehend the existing intrinsic\nrelation between the two modalities. The novelty of the proposed model consists\nof the usage of a PHOC descriptor to construct a bag of textual words along\nwith a Fisher Vector Encoding that captures the morphology of text. This\napproach provides a stronger multimodal representation for this task and as our\nexperiments demonstrate, it achieves state-of-the-art results on two different\ntasks, fine-grained classification and image retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 12:06:12 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Mafla", "Andres", ""], ["Dey", "Sounak", ""], ["Biten", "Ali Furkan", ""], ["Gomez", "Lluis", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "2001.04735", "submitter": "Yuren Cong", "authors": "Cong Yuren, Hanno Ackermann, Wentong Liao, Michael Ying Yang, and Bodo\n  Rosenhahn", "title": "NODIS: Neural Ordinary Differential Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image understanding is a challenging topic in computer vision. It\nrequires to detect all objects in an image, but also to identify all the\nrelations between them. Detected objects, their labels and the discovered\nrelations can be used to construct a scene graph which provides an abstract\nsemantic interpretation of an image. In previous works, relations were\nidentified by solving an assignment problem formulated as Mixed-Integer Linear\nPrograms. In this work, we interpret that formulation as Ordinary Differential\nEquation (ODE). The proposed architecture performs scene graph inference by\nsolving a neural variant of an ODE by end-to-end learning. It achieves\nstate-of-the-art results on all three benchmark tasks: scene graph generation\n(SGGen), classification (SGCls) and visual relationship detection (PredCls) on\nVisual Genome benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 12:17:18 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 21:37:07 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 20:41:19 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yuren", "Cong", ""], ["Ackermann", "Hanno", ""], ["Liao", "Wentong", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2001.04753", "submitter": "Sharon Ayzik", "authors": "Sharon Ayzik and Shai Avidan", "title": "Deep Image Compression using Decoder Side Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Deep Image Compression neural network that relies on side\ninformation, which is only available to the decoder. We base our algorithm on\nthe assumption that the image available to the encoder and the image available\nto the decoder are correlated, and we let the network learn these correlations\nin the training phase.\n  Then, at run time, the encoder side encodes the input image without knowing\nanything about the decoder side image and sends it to the decoder. The decoder\nthen uses the encoded input image and the side information image to reconstruct\nthe original image.\n  This problem is known as Distributed Source Coding in Information Theory, and\nwe discuss several use cases for this technology. We compare our algorithm to\nseveral image compression algorithms and show that adding decoder-only side\ninformation does indeed improve results. Our code is publicly available at\nhttps://github.com/ayziksha/DSIN.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 12:55:27 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 15:13:40 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Ayzik", "Sharon", ""], ["Avidan", "Shai", ""]]}, {"id": "2001.04758", "submitter": "Hao Zhu", "authors": "Hao Zhu, Mandi Luo, Rui Wang, Aihua Zheng, and Ran He", "title": "Deep Audio-Visual Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-visual learning, aimed at exploiting the relationship between audio and\nvisual modalities, has drawn considerable attention since deep learning started\nto be used successfully. Researchers tend to leverage these two modalities\neither to improve the performance of previously considered single-modality\ntasks or to address new challenging problems. In this paper, we provide a\ncomprehensive survey of recent audio-visual learning development. We divide the\ncurrent audio-visual learning tasks into four different subfields: audio-visual\nseparation and localization, audio-visual correspondence learning, audio-visual\ngeneration, and audio-visual representation learning. State-of-the-art methods\nas well as the remaining challenges of each subfield are further discussed.\nFinally, we summarize the commonly used datasets and performance metrics.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 13:11:21 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Zhu", "Hao", ""], ["Luo", "Mandi", ""], ["Wang", "Rui", ""], ["Zheng", "Aihua", ""], ["He", "Ran", ""]]}, {"id": "2001.04775", "submitter": "Audrey Richard", "authors": "Audrey Richard, Ian Cherabier, Martin R. Oswald, Vagia Tsiminaki, Marc\n  Pollefeys and Konrad Schindler", "title": "Learned Multi-View Texture Super-Resolution", "comments": "11 pages, 5 figures, 2019 International Conference on 3D Vision (3DV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a super-resolution method capable of creating a high-resolution\ntexture map for a virtual 3D object from a set of lower-resolution images of\nthat object. Our architecture unifies the concepts of (i) multi-view\nsuper-resolution based on the redundancy of overlapping views and (ii)\nsingle-view super-resolution based on a learned prior of high-resolution (HR)\nimage structure. The principle of multi-view super-resolution is to invert the\nimage formation process and recover the latent HR texture from multiple\nlower-resolution projections. We map that inverse problem into a block of\nsuitably designed neural network layers, and combine it with a standard\nencoder-decoder network for learned single-image super-resolution. Wiring the\nimage formation model into the network avoids having to learn perspective\nmapping from textures to images, and elegantly handles a varying number of\ninput views. Experiments demonstrate that the combination of multi-view\nobservations and learned prior yields improved texture maps.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 13:49:22 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Richard", "Audrey", ""], ["Cherabier", "Ian", ""], ["Oswald", "Martin R.", ""], ["Tsiminaki", "Vagia", ""], ["Pollefeys", "Marc", ""], ["Schindler", "Konrad", ""]]}, {"id": "2001.04776", "submitter": "Andrew Gilbert", "authors": "Kary Ho, Andrew Gilbert, Hailin Jin, John Collomosse", "title": "Neural Architecture Search for Deep Image Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural architecture search (NAS) technique to enhance the\nperformance of unsupervised image de-noising, in-painting and super-resolution\nunder the recently proposed Deep Image Prior (DIP). We show that evolutionary\nsearch can automatically optimize the encoder-decoder (E-D) structure and\nmeta-parameters of the DIP network, which serves as a content-specific prior to\nregularize these single image restoration tasks. Our binary representation\nencodes the design space for an asymmetric E-D network that typically converges\nto yield a content-specific DIP within 10-20 generations using a population\nsize of 500. The optimized architectures consistently improve upon the visual\nquality of classical DIP for a diverse range of photographic and artistic\ncontent.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 13:51:32 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Ho", "Kary", ""], ["Gilbert", "Andrew", ""], ["Jin", "Hailin", ""], ["Collomosse", "John", ""]]}, {"id": "2001.04782", "submitter": "Thomas Haugland Johansen", "authors": "Thomas Haugland Johansen and Steffen Aagaard S{\\o}rensen", "title": "Towards detection and classification of microscopic foraminifera using\n  transfer learning", "comments": "6 pages, 5 figures. To be published in proceedings of Northern Lights\n  Deep Learning Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Foraminifera are single-celled marine organisms, which may have a planktic or\nbenthic lifestyle. During their life cycle they construct shells consisting of\none or more chambers, and these shells remain as fossils in marine sediments.\nClassifying and counting these fossils have become an important tool in e.g.\noceanography and climatology. Currently the process of identifying and counting\nmicrofossils is performed manually using a microscope and is very time\nconsuming. Developing methods to automate this process is therefore considered\nimportant across a range of research fields. The first steps towards developing\na deep learning model that can detect and classify microscopic foraminifera are\nproposed. The proposed model is based on a VGG16 model that has been pretrained\non the ImageNet dataset, and adapted to the foraminifera task using transfer\nlearning. Additionally, a novel image dataset consisting of microscopic\nforaminifera and sediments from the Barents Sea region is introduced.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 13:57:08 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Johansen", "Thomas Haugland", ""], ["S\u00f8rensen", "Steffen Aagaard", ""]]}, {"id": "2001.04803", "submitter": "Ke Chen", "authors": "Lulu Tang, Ke Chen, Chaozheng Wu, Yu Hong, Kui Jia and Zhixin Yang", "title": "Improving Semantic Analysis on Point Clouds via Auxiliary Supervision of\n  Local Geometric Priors", "comments": "11 pages, 8 figures, 9 tables; accepted by the IEEE Transactions on\n  Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning algorithms for point cloud analysis mainly concern\ndiscovering semantic patterns from global configuration of local geometries in\na supervised learning manner. However, very few explore geometric properties\nrevealing local surface manifolds embedded in 3D Euclidean space to\ndiscriminate semantic classes or object parts as additional supervision\nsignals. This paper is the first attempt to propose a unique multi-task\ngeometric learning network to improve semantic analysis by auxiliary geometric\nlearning with local shape properties, which can be either generated via\nphysical computation from point clouds themselves as self-supervision signals\nor provided as privileged information. Owing to explicitly encoding local shape\nmanifolds in favor of semantic analysis, the proposed geometric self-supervised\nand privileged learning algorithms can achieve superior performance to their\nbackbone baselines and other state-of-the-art methods, which are verified in\nthe experiments on the popular benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 14:33:31 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 03:11:53 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Tang", "Lulu", ""], ["Chen", "Ke", ""], ["Wu", "Chaozheng", ""], ["Hong", "Yu", ""], ["Jia", "Kui", ""], ["Yang", "Zhixin", ""]]}, {"id": "2001.04835", "submitter": "Maarten Bieshaar", "authors": "Kristina Scharei, Florian Heidecker, Maarten Bieshaar", "title": "Knowledge Representations in Technical Systems -- A Taxonomy", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent usage of technical systems in human-centric environments leads to\nthe question, how to teach technical systems, e.g., robots, to understand,\nlearn, and perform tasks desired by the human. Therefore, an accurate\nrepresentation of knowledge is essential for the system to work as expected.\nThis article mainly gives insight into different knowledge representation\ntechniques and their categorization into various problem domains in artificial\nintelligence. Additionally, applications of presented knowledge representations\nare introduced in everyday robotics tasks. By means of the provided taxonomy,\nthe search for a proper knowledge representation technique regarding a specific\nproblem should be facilitated.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 15:00:09 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 07:07:10 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Scharei", "Kristina", ""], ["Heidecker", "Florian", ""], ["Bieshaar", "Maarten", ""]]}, {"id": "2001.04893", "submitter": "Inseok Hwang", "authors": "Inseok Hwang, Jinho Lee, Frank Liu, Minsik Cho", "title": "SimEx: Express Prediction of Inter-dataset Similarity by a Fleet of\n  Autoencoders", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing the similarity between sets of data has a number of positive\nimplications in training an effective model, such as assisting an informed\nselection out of known datasets favorable to model transfer or data\naugmentation problems with an unknown dataset. Common practices to estimate the\nsimilarity between data include comparing in the original sample space,\ncomparing in the embedding space from a model performing a certain task, or\nfine-tuning a pretrained model with different datasets and evaluating the\nperformance changes therefrom. However, these practices would suffer from\nshallow comparisons, task-specific biases, or extensive time and computations\nrequired to perform comparisons. We present SimEx, a new method for early\nprediction of inter-dataset similarity using a set of pretrained autoencoders\neach of which is dedicated to reconstructing a specific part of known data.\nSpecifically, our method takes unknown data samples as input to those\npretrained autoencoders, and evaluate the difference between the reconstructed\noutput samples against their original input samples. Our intuition is that, the\nmore similarity exists between the unknown data samples and the part of known\ndata that an autoencoder was trained with, the better chances there could be\nthat this autoencoder makes use of its trained knowledge, reconstructing output\nsamples closer to the originals. We demonstrate that our method achieves more\nthan 10x speed-up in predicting inter-dataset similarity compared to common\nsimilarity-estimating practices. We also demonstrate that the inter-dataset\nsimilarity estimated by our method is well-correlated with common practices and\noutperforms the baselines approaches of comparing at sample- or\nembedding-spaces, without newly training anything at the comparison time.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 16:52:50 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Hwang", "Inseok", ""], ["Lee", "Jinho", ""], ["Liu", "Frank", ""], ["Cho", "Minsik", ""]]}, {"id": "2001.04911", "submitter": "Han Gong", "authors": "Han Gong", "title": "Convolutional Mean: A Simple Convolutional Neural Network for Illuminant\n  Estimation", "comments": "Accepted by BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Convolutional Mean (CM) - a simple and fast convolutional neural\nnetwork for illuminant estimation. Our proposed method only requires a small\nneural network model (1.1K parameters) and a 48 x 32 thumbnail input image. Our\nunoptimized Python implementation takes 1 ms/image, which is arguably 3-3750x\nfaster than the current leading solutions with similar accuracy. Using two\npublic datasets, we show that our proposed light-weight method offers accuracy\ncomparable to the current leading methods' (which consist of thousands/millions\nof parameters) across several measures.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:11:17 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Gong", "Han", ""]]}, {"id": "2001.04928", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Parthipan Siva, Paul Marchwica and Alexander Wong", "title": "Unsupervised Domain Adaptation in Person re-ID via k-Reciprocal\n  Clustering and Large-Scale Heterogeneous Environment Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ongoing major challenge in computer vision is the task of person\nre-identification, where the goal is to match individuals across different,\nnon-overlapping camera views. While recent success has been achieved via\nsupervised learning using deep neural networks, such methods have limited\nwidespread adoption due to the need for large-scale, customized data\nannotation. As such, there has been a recent focus on unsupervised learning\napproaches to mitigate the data annotation issue; however, current approaches\nin literature have limited performance compared to supervised learning\napproaches as well as limited applicability for adoption in new environments.\nIn this paper, we address the aforementioned challenges faced in person\nre-identification for real-world, practical scenarios by introducing a novel,\nunsupervised domain adaptation approach for person re-identification. This is\naccomplished through the introduction of: i) k-reciprocal tracklet Clustering\nfor Unsupervised Domain Adaptation (ktCUDA) (for pseudo-label generation on\ntarget domain), and ii) Synthesized Heterogeneous RE-id Domain (SHRED) composed\nof large-scale heterogeneous independent source environments (for improving\nrobustness and adaptability to a wide diversity of target environments).\nExperimental results across four different image and video benchmark datasets\nshow that the proposed ktCUDA and SHRED approach achieves an average\nimprovement of +5.7 mAP in re-identification performance when compared to\nexisting state-of-the-art methods, as well as demonstrate better adaptability\nto different types of environments.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:43:52 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Kumar", "Devinder", ""], ["Siva", "Parthipan", ""], ["Marchwica", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "2001.04932", "submitter": "Michael Fischer", "authors": "Michael H. Fischer, Richard R. Yang, Monica S. Lam", "title": "ImagineNet: Restyling Apps Using Neural Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents ImagineNet, a tool that uses a novel neural style\ntransfer model to enable end-users and app developers to restyle GUIs using an\nimage of their choice. Former neural style transfer techniques are inadequate\nfor this application because they produce GUIs that are illegible and hence\nnonfunctional. We propose a neural solution by adding a new loss term to the\noriginal formulation, which minimizes the squared error in the uncentered\ncross-covariance of features from different levels in a CNN between the style\nand output images. ImagineNet retains the details of GUIs, while transferring\nthe colors and textures of the art. We presented GUIs restyled with ImagineNet\nas well as other style transfer techniques to 50 evaluators and all preferred\nthose of ImagineNet. We show how ImagineNet can be used to restyle (1) the\ngraphical assets of an app, (2) an app with user-supplied content, and (3) an\napp with dynamically generated GUIs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:47:50 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 20:23:40 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Fischer", "Michael H.", ""], ["Yang", "Richard R.", ""], ["Lam", "Monica S.", ""]]}, {"id": "2001.04947", "submitter": "Lingjie Liu", "authors": "Lingjie Liu, Weipeng Xu, Marc Habermann, Michael Zollhoefer, Florian\n  Bernard, Hyeongwoo Kim, Wenping Wang, Christian Theobalt", "title": "Neural Human Video Rendering by Learning Dynamic Textures and\n  Rendering-to-Video Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing realistic videos of humans using neural networks has been a\npopular alternative to the conventional graphics-based rendering pipeline due\nto its high efficiency. Existing works typically formulate this as an\nimage-to-image translation problem in 2D screen space, which leads to artifacts\nsuch as over-smoothing, missing body parts, and temporal instability of\nfine-scale detail, such as pose-dependent wrinkles in the clothing. In this\npaper, we propose a novel human video synthesis method that approaches these\nlimiting factors by explicitly disentangling the learning of time-coherent\nfine-scale details from the embedding of the human in 2D screen space. More\nspecifically, our method relies on the combination of two convolutional neural\nnetworks (CNNs). Given the pose information, the first CNN predicts a dynamic\ntexture map that contains time-coherent high-frequency details, and the second\nCNN conditions the generation of the final video on the temporally coherent\noutput of the first CNN. We demonstrate several applications of our approach,\nsuch as human reenactment and novel view synthesis from monocular video, where\nwe show significant improvement over the state of the art both qualitatively\nand quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 18:06:27 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 17:29:29 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 21:08:52 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Lingjie", ""], ["Xu", "Weipeng", ""], ["Habermann", "Marc", ""], ["Zollhoefer", "Michael", ""], ["Bernard", "Florian", ""], ["Kim", "Hyeongwoo", ""], ["Wang", "Wenping", ""], ["Theobalt", "Christian", ""]]}, {"id": "2001.04982", "submitter": "Qizhu Li", "authors": "Qizhu Li, Xiaojuan Qi, Philip H.S. Torr", "title": "Unifying Training and Inference for Panoptic Segmentation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end network to bridge the gap between training and\ninference pipeline for panoptic segmentation, a task that seeks to partition an\nimage into semantic regions for \"stuff\" and object instances for \"things\". In\ncontrast to recent works, our network exploits a parametrised, yet lightweight\npanoptic segmentation submodule, powered by an end-to-end learnt dense instance\naffinity, to capture the probability that any pair of pixels belong to the same\ninstance. This panoptic submodule gives rise to a novel propagation mechanism\nfor panoptic logits and enables the network to output a coherent panoptic\nsegmentation map for both \"stuff\" and \"thing\" classes, without any\npost-processing. Reaping the benefits of end-to-end training, our full system\nsets new records on the popular street scene dataset, Cityscapes, achieving\n61.4 PQ with a ResNet-50 backbone using only the fine annotations. On the\nchallenging COCO dataset, our ResNet-50-based network also delivers\nstate-of-the-art accuracy of 43.4 PQ. Moreover, our network flexibly works with\nand without object mask cues, performing competitively under both settings,\nwhich is of interest for applications with computation budgets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 18:58:24 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 18:44:52 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Li", "Qizhu", ""], ["Qi", "Xiaojuan", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "2001.05005", "submitter": "Thomas Pock", "authors": "Erich Kobler and Alexander Effland and Karl Kunisch and Thomas Pock", "title": "Total Deep Variation for Linear Inverse Problems", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diverse inverse problems in imaging can be cast as variational problems\ncomposed of a task-specific data fidelity term and a regularization term. In\nthis paper, we propose a novel learnable general-purpose regularizer exploiting\nrecent architectural design patterns from deep learning. We cast the learning\nproblem as a discrete sampled optimal control problem, for which we derive the\nadjoint state equations and an optimality condition. By exploiting the\nvariational structure of our approach, we perform a sensitivity analysis with\nrespect to the learned parameters obtained from different training datasets.\nMoreover, we carry out a nonlinear eigenfunction analysis, which reveals\ninteresting properties of the learned regularizer. We show state-of-the-art\nperformance for classical image restoration and medical image reconstruction\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 19:01:50 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 19:39:23 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Kobler", "Erich", ""], ["Effland", "Alexander", ""], ["Kunisch", "Karl", ""], ["Pock", "Thomas", ""]]}, {"id": "2001.05006", "submitter": "Oscar De Felice", "authors": "Oscar de Felice, Gustavo de Felice", "title": "A smile I could recognise in a thousand: Automatic identification of\n  identity from dental radiography", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method to automatically compare multiple\nradiographs in order to find the identity of a patient out of the dental\nfeatures. The method is based on the matching of image features, previously\nextracted by computer vision algorithms for image descriptor recognition. The\nprincipal application (being also our motivation to study the problem) of such\na method would be in victim identification in mass disasters.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 19:02:06 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["de Felice", "Oscar", ""], ["de Felice", "Gustavo", ""]]}, {"id": "2001.05017", "submitter": "Sagie Benaim", "authors": "Ori Press, Tomer Galanti, Sagie Benaim, Lior Wolf", "title": "Emerging Disentanglement in Auto-Encoder Based Unsupervised Image\n  Content Transfer", "comments": null, "journal-ref": "ICLR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning to map, in an unsupervised way, between\ndomains A and B, such that the samples b in B contain all the information that\nexists in samples a in A and some additional information. For example, ignoring\nocclusions, B can be people with glasses, A people without, and the glasses,\nwould be the added information. When mapping a sample a from the first domain\nto the other domain, the missing information is replicated from an independent\nreference sample b in B. Thus, in the above example, we can create, for every\nperson without glasses a version with the glasses observed in any face image.\n  Our solution employs a single two-pathway encoder and a single decoder for\nboth domains. The common part of the two domains and the separate part are\nencoded as two vectors, and the separate part is fixed at zero for domain A.\nThe loss terms are minimal and involve reconstruction losses for the two\ndomains and a domain confusion term. Our analysis shows that under mild\nassumptions, this architecture, which is much simpler than the literature\nguided-translation methods, is enough to ensure disentanglement between the two\ndomains. We present convincing results in a few visual domains, such as\nno-glasses to glasses, adding facial hair based on a reference image, etc.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 19:36:47 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Press", "Ori", ""], ["Galanti", "Tomer", ""], ["Benaim", "Sagie", ""], ["Wolf", "Lior", ""]]}, {"id": "2001.05022", "submitter": "Catherine Groschner", "authors": "C.K. Groschner, Christina Choi, and M.C. Scott", "title": "Machine Learning Pipeline for Segmentation and Defect Identification\n  from High Resolution Transmission Electron Microscopy Data", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": "10.1017/S1431927621000386", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of transmission electron microscopy, data interpretation often\nlags behind acquisition methods, as image processing methods often have to be\nmanually tailored to individual datasets. Machine learning offers a promising\napproach for fast, accurate analysis of electron microscopy data. Here, we\ndemonstrate a flexible two step pipeline for analysis of high resolution\ntransmission electron microscopy data, which uses a U-Net for segmentation\nfollowed by a random forest for detection of stacking faults. Our trained U-Net\nis able to segment nanoparticle regions from amorphous background with a Dice\ncoefficient of 0.8 and significantly outperforms traditional image segmentation\nmethods. Using these segmented regions, we are then able to classify whether\nnanoparticles contain a visible stacking fault with 86% accuracy. We provide\nthis adaptable pipeline as an open source tool for the community. The combined\noutput of the segmentation network and classifier offer a way to determine\nstatistical distributions of features of interest, such as size, shape and\ndefect presence, enabling detection of correlations between these features.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 19:49:30 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 23:30:56 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Groschner", "C. K.", ""], ["Choi", "Christina", ""], ["Scott", "M. C.", ""]]}, {"id": "2001.05027", "submitter": "Andre Araujo", "authors": "Bingyi Cao, Andre Araujo, Jack Sim", "title": "Unifying Deep Local and Global Features for Image Search", "comments": "ECCV'20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image retrieval is the problem of searching an image database for items that\nare similar to a query image. To address this task, two main types of image\nrepresentations have been studied: global and local image features. In this\nwork, our key contribution is to unify global and local features into a single\ndeep model, enabling accurate retrieval with efficient feature extraction. We\nrefer to the new model as DELG, standing for DEep Local and Global features. We\nleverage lessons from recent feature learning work and propose a model that\ncombines generalized mean pooling for global features and attentive selection\nfor local features. The entire network can be learned end-to-end by carefully\nbalancing the gradient flow between two heads -- requiring only image-level\nlabels. We also introduce an autoencoder-based dimensionality reduction\ntechnique for local features, which is integrated into the model, improving\ntraining efficiency and matching performance. Comprehensive experiments show\nthat our model achieves state-of-the-art image retrieval on the Revisited\nOxford and Paris datasets, and state-of-the-art single-model instance-level\nrecognition on the Google Landmarks dataset v2. Code and models are available\nat https://github.com/tensorflow/models/tree/master/research/delf .\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 19:59:51 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 17:40:50 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 00:26:13 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2020 18:21:56 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Cao", "Bingyi", ""], ["Araujo", "Andre", ""], ["Sim", "Jack", ""]]}, {"id": "2001.05036", "submitter": "Shir Gur", "authors": "Shir Gur, Lior Wolf", "title": "Single Image Depth Estimation Trained via Depth from Defocus Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating depth from a single RGB images is a fundamental task in computer\nvision, which is most directly solved using supervised deep learning. In the\nfield of unsupervised learning of depth from a single RGB image, depth is not\ngiven explicitly. Existing work in the field receives either a stereo pair, a\nmonocular video, or multiple views, and, using losses that are based on\nstructure-from-motion, trains a depth estimation network. In this work, we\nrely, instead of different views, on depth from focus cues. Learning is based\non a novel Point Spread Function convolutional layer, which applies location\nspecific kernels that arise from the Circle-Of-Confusion in each image\nlocation. We evaluate our method on data derived from five common datasets for\ndepth estimation and lightfield images, and present results that are on par\nwith supervised methods on KITTI and Make3D datasets and outperform\nunsupervised learning approaches. Since the phenomenon of depth from defocus is\nnot dataset specific, we hypothesize that learning based on it would overfit\nless to the specific content in each dataset. Our experiments show that this is\nindeed the case, and an estimator learned on one dataset using our method\nprovides better results on other datasets, than the directly supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 20:22:54 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Gur", "Shir", ""], ["Wolf", "Lior", ""]]}, {"id": "2001.05049", "submitter": "Jan Czarnowski", "authors": "Jan Czarnowski, Tristan Laidlow, Ronald Clark and Andrew J. Davison", "title": "DeepFactors: Real-Time Probabilistic Dense Monocular SLAM", "comments": "RA-L", "journal-ref": null, "doi": "10.1109/LRA.2020.2965415", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to estimate rich geometry and camera motion from monocular\nimagery is fundamental to future interactive robotics and augmented reality\napplications. Different approaches have been proposed that vary in scene\ngeometry representation (sparse landmarks, dense maps), the consistency metric\nused for optimising the multi-view problem, and the use of learned priors. We\npresent a SLAM system that unifies these methods in a probabilistic framework\nwhile still maintaining real-time performance. This is achieved through the use\nof a learned compact depth map representation and reformulating three different\ntypes of errors: photometric, reprojection and geometric, which we make use of\nwithin standard factor graph software. We evaluate our system on trajectory\nestimation and depth reconstruction on real-world sequences and present various\nexamples of estimated dense geometry.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 21:08:51 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Czarnowski", "Jan", ""], ["Laidlow", "Tristan", ""], ["Clark", "Ronald", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2001.05058", "submitter": "Diedre Carmo", "authors": "Diedre Carmo, Bruna Silva, Clarissa Yasuda, Let\\'icia Rittner and\n  Roberto Lotufo", "title": "Hippocampus Segmentation on Epilepsy and Alzheimer's Disease Studies\n  with Multiple Convolutional Neural Networks", "comments": "Code is available at https://github.com/dscarmo/e2dhipseg Published\n  in Heliyon:\n  https://www.sciencedirect.com/science/article/pii/S2405844021003315", "journal-ref": "Heliyon, Volume 7, Issue 2, 2021", "doi": "10.1016/j.heliyon.2021.e06226", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hippocampus segmentation on magnetic resonance imaging is of key importance\nfor the diagnosis, treatment decision and investigation of neuropsychiatric\ndisorders. Automatic segmentation is an active research field, with many recent\nmodels using deep learning. Most current state-of-the art hippocampus\nsegmentation methods train their methods on healthy or Alzheimer's disease\npatients from public datasets. This raises the question whether these methods\nare capable of recognizing the hippocampus on a different domain, that of\nepilepsy patients with hippocampus resection. In this paper we present a\nstate-of-the-art, open source, ready-to-use, deep learning based hippocampus\nsegmentation method. It uses an extended 2D multi-orientation approach, with\nautomatic pre-processing and orientation alignment. The methodology was\ndeveloped and validated using HarP, a public Alzheimer's disease hippocampus\nsegmentation dataset. We test this methodology alongside other recent deep\nlearning methods, in two domains: The HarP test set and an in-house epilepsy\ndataset, containing hippocampus resections, named HCUnicamp. We show that our\nmethod, while trained only in HarP, surpasses others from the literature in\nboth the HarP test set and HCUnicamp in Dice. Additionally, Results from\ntraining and testing in HCUnicamp volumes are also reported separately,\nalongside comparisons between training and testing in epilepsy and Alzheimer's\ndata and vice versa. Although current state-of-the-art methods, including our\nown, achieve upwards of 0.9 Dice in HarP, all tested methods, including our\nown, produced false positives in HCUnicamp resection regions, showing that\nthere is still room for improvement for hippocampus segmentation methods when\nresection is involved.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 21:57:46 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 21:23:29 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Carmo", "Diedre", ""], ["Silva", "Bruna", ""], ["Yasuda", "Clarissa", ""], ["Rittner", "Let\u00edcia", ""], ["Lotufo", "Roberto", ""]]}, {"id": "2001.05060", "submitter": "Tianshu Yu", "authors": "Yikang Li, Tianshu Yu, Baoxin Li", "title": "Recognizing Video Events with Varying Rhythms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing Video events in long, complex videos with multiple sub-activities\nhas received persistent attention recently. This task is more challenging than\ntraditional action recognition with short, relatively homogeneous video clips.\nIn this paper, we investigate the problem of recognizing long and complex\nevents with varying action rhythms, which has not been considered in the\nliterature but is a practical challenge. Our work is inspired in part by how\nhumans identify events with varying rhythms: quickly catching frames\ncontributing most to a specific event. We propose a two-stage \\emph{end-to-end}\nframework, in which the first stage selects the most significant frames while\nthe second stage recognizes the event using the selected frames. Our model\nneeds only \\emph{event-level labels} in the training stage, and thus is more\npractical when the sub-activity labels are missing or difficult to obtain. The\nresults of extensive experiments show that our model can achieve significant\nimprovement in event recognition from long videos while maintaining high\naccuracy even if the test videos suffer from severe rhythm changes. This\ndemonstrates the potential of our method for real-world video-based\napplications, where test and training videos can differ drastically in rhythms\nof sub-activities.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 22:06:48 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Li", "Yikang", ""], ["Yu", "Tianshu", ""], ["Li", "Baoxin", ""]]}, {"id": "2001.05071", "submitter": "Omri Lifshitz", "authors": "Omri Lifshitz and Lior Wolf", "title": "A Sample Selection Approach for Universal Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of unsupervised domain adaption in the universal\nscenario, in which only some of the classes are shared between the source and\ntarget domains. We present a scoring scheme that is effective in identifying\nthe samples of the shared classes. The score is used to select which samples in\nthe target domain to pseudo-label during training. Another loss term encourages\ndiversity of labels within each batch. Taken together, our method is shown to\noutperform, by a sizable margin, the current state of the art on the literature\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 22:28:43 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Lifshitz", "Omri", ""], ["Wolf", "Lior", ""]]}, {"id": "2001.05076", "submitter": "Shir Gur", "authors": "Shir Gur, Lior Wolf, Lior Golgher, Pablo Blinder", "title": "Microvascular Dynamics from 4D Microscopy Using Temporal Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently developed methods for rapid continuous volumetric two-photon\nmicroscopy facilitate the observation of neuronal activity in hundreds of\nindividual neurons and changes in blood flow in adjacent blood vessels across a\nlarge volume of living brain at unprecedented spatio-temporal resolution.\nHowever, the high imaging rate necessitates fully automated image analysis,\nwhereas tissue turbidity and photo-toxicity limitations lead to extremely\nsparse and noisy imagery. In this work, we extend a recently proposed deep\nlearning volumetric blood vessel segmentation network, such that it supports\ntemporal analysis. With this technology, we are able to track changes in\ncerebral blood volume over time and identify spontaneous arterial dilations\nthat propagate towards the pial surface. This new capability is a promising\nstep towards characterizing the hemodynamic response function upon which\nfunctional magnetic resonance imaging (fMRI) is based.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 22:55:03 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Gur", "Shir", ""], ["Wolf", "Lior", ""], ["Golgher", "Lior", ""], ["Blinder", "Pablo", ""]]}, {"id": "2001.05080", "submitter": "\\\"Omer S\\\"umer", "authors": "\\\"Omer S\\\"umer, Peter Gerjets, Ulrich Trautwein, Enkelejda Kasneci", "title": "Automated Anonymisation of Visual and Audio Data in Classroom Studies", "comments": "The Workshops of the Thirty-Fourth AAAI Conference on Artificial\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding students' and teachers' verbal and non-verbal behaviours during\ninstruction may help infer valuable information regarding the quality of\nteaching. In education research, there have been many studies that aim to\nmeasure students' attentional focus on learning-related tasks: Based on\naudio-visual recordings and manual or automated ratings of behaviours of\nteachers and students. Student data is, however, highly sensitive. Therefore,\nensuring high standards of data protection and privacy has the utmost\nimportance in current practices. For example, in the context of teaching\nmanagement studies, data collection is carried out with the consent of pupils,\nparents, teachers and school administrations. Nevertheless, there may often be\nstudents whose data cannot be used for research purposes. Excluding these\nstudents from the classroom is an unnatural intrusion into the organisation of\nthe classroom. A possible solution would be to request permission to record the\naudio-visual recordings of all students (including those who do not voluntarily\nparticipate in the study) and to anonymise their data. Yet, the manual\nanonymisation of audio-visual data is very demanding. In this study, we examine\nthe use of artificial intelligence methods to automatically anonymise the\nvisual and audio data of a particular person.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 23:29:05 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["S\u00fcmer", "\u00d6mer", ""], ["Gerjets", "Peter", ""], ["Trautwein", "Ulrich", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2001.05086", "submitter": "Peng Tang", "authors": "Peng Tang, Chetan Ramaiah, Yan Wang, Ran Xu, Caiming Xiong", "title": "Proposal Learning for Semi-Supervised Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on semi-supervised object detection to boost\nperformance of proposal-based object detectors (a.k.a. two-stage object\ndetectors) by training on both labeled and unlabeled data. However, it is\nnon-trivial to train object detectors on unlabeled data due to the\nunavailability of ground truth labels. To address this problem, we present a\nproposal learning approach to learn proposal features and predictions from both\nlabeled and unlabeled data. The approach consists of a self-supervised proposal\nlearning module and a consistency-based proposal learning module. In the\nself-supervised proposal learning module, we present a proposal location loss\nand a contrastive loss to learn context-aware and noise-robust proposal\nfeatures respectively. In the consistency-based proposal learning module, we\napply consistency losses to both bounding box classification and regression\npredictions of proposals to learn noise-robust proposal features and\npredictions. Our approach enjoys the following benefits: 1) encouraging more\ncontext information to delivered in the proposals learning procedure; 2) noisy\nproposal features and enforcing consistency to allow noise-robust object\ndetection; 3) building a general and high-performance semi-supervised object\ndetection framework, which can be easily adapted to proposal-based object\ndetectors with different backbone architectures. Experiments are conducted on\nthe COCO dataset with all available labeled and unlabeled data. Results\ndemonstrate that our approach consistently improves the performance of\nfully-supervised baselines. In particular, after combining with data\ndistillation, our approach improves AP by about 2.0% and 0.9% on average\ncompared to fully-supervised baselines and data distillation baselines\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 00:06:59 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 18:13:23 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Tang", "Peng", ""], ["Ramaiah", "Chetan", ""], ["Wang", "Yan", ""], ["Xu", "Ran", ""], ["Xiong", "Caiming", ""]]}, {"id": "2001.05097", "submitter": "Dong-Hyun Hwang", "authors": "Dong-Hyun Hwang, Suntae Kim, Nicolas Monet, Hideki Koike, Soonmin Bae", "title": "Lightweight 3D Human Pose Estimation Network Training Using\n  Teacher-Student Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present MoVNect, a lightweight deep neural network to capture 3D human\npose using a single RGB camera. To improve the overall performance of the\nmodel, we apply the teacher-student learning method based knowledge\ndistillation to 3D human pose estimation. Real-time post-processing makes the\nCNN output yield temporally stable 3D skeletal information, which can be used\nin applications directly. We implement a 3D avatar application running on\nmobile in real-time to demonstrate that our network achieves both high accuracy\nand fast inference time. Extensive evaluations show the advantages of our\nlightweight model with the proposed training method over previous 3D pose\nestimation methods on the Human3.6M dataset and mobile devices.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 01:31:01 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Hwang", "Dong-Hyun", ""], ["Kim", "Suntae", ""], ["Monet", "Nicolas", ""], ["Koike", "Hideki", ""], ["Bae", "Soonmin", ""]]}, {"id": "2001.05119", "submitter": "Zan Gojcic", "authors": "Zan Gojcic, Caifa Zhou, Jan D. Wegner, Leonidas J. Guibas, Tolga\n  Birdal", "title": "Learning multiview 3D point cloud registration", "comments": "CVPR2020 - Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, end-to-end learnable, multiview 3D point cloud\nregistration algorithm. Registration of multiple scans typically follows a\ntwo-stage pipeline: the initial pairwise alignment and the globally consistent\nrefinement. The former is often ambiguous due to the low overlap of neighboring\npoint clouds, symmetries and repetitive scene parts. Therefore, the latter\nglobal refinement aims at establishing the cyclic consistency across multiple\nscans and helps in resolving the ambiguous cases. In this paper we propose, to\nthe best of our knowledge, the first end-to-end algorithm for joint learning of\nboth parts of this two-stage problem. Experimental evaluation on well accepted\nbenchmark datasets shows that our approach outperforms the state-of-the-art by\na significant margin, while being end-to-end trainable and computationally less\ncostly. Moreover, we present detailed analysis and an ablation study that\nvalidate the novel components of our approach. The source code and pretrained\nmodels are publicly available under\nhttps://github.com/zgojcic/3D_multiview_reg.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 03:42:14 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 07:53:36 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Gojcic", "Zan", ""], ["Zhou", "Caifa", ""], ["Wegner", "Jan D.", ""], ["Guibas", "Leonidas J.", ""], ["Birdal", "Tolga", ""]]}, {"id": "2001.05130", "submitter": "Jordan Malof", "authors": "Fanjie Kong, Bohao Huang, Kyle Bradbury, Jordan M. Malof", "title": "The Synthinel-1 dataset: a collection of high resolution synthetic\n  overhead imagery for building segmentation", "comments": "Preprint of paper accepted for publication at Winter Conference on\n  Applications of Computer Vision (WACV) 2020", "journal-ref": "F. Kong, The Synthinel-1 dataset: a collection of high resolution\n  synthetic overhead imagery for building segmentation, 2020, IEEE Winter\n  Conference on Applications of Computer Vision (WACV)", "doi": "10.1109/wacv45572.2020.9093339", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep learning - namely convolutional neural networks (CNNs) - have\nyielded impressive performance for the task of building segmentation on large\noverhead (e.g., satellite) imagery benchmarks. However, these benchmark\ndatasets only capture a small fraction of the variability present in real-world\noverhead imagery, limiting the ability to properly train, or evaluate, models\nfor real-world application. Unfortunately, developing a dataset that captures\neven a small fraction of real-world variability is typically infeasible due to\nthe cost of imagery, and manual pixel-wise labeling of the imagery. In this\nwork we develop an approach to rapidly and cheaply generate large and diverse\nvirtual environments from which we can capture synthetic overhead imagery for\ntraining segmentation CNNs. Using this approach, generate and publicly-release\na collection of synthetic overhead imagery - termed Synthinel-1 with full\npixel-wise building labels. We use several benchmark dataset to demonstrate\nthat Synthinel-1 is consistently beneficial when used to augment real-world\ntraining imagery, especially when CNNs are tested on novel geographic locations\nor conditions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 04:30:45 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Kong", "Fanjie", ""], ["Huang", "Bohao", ""], ["Bradbury", "Kyle", ""], ["Malof", "Jordan M.", ""]]}, {"id": "2001.05137", "submitter": "Maryam Hashemi Miss", "authors": "Maryam Hashemi, Alireza Mirrashid, Aliasghar Beheshti Shirazi", "title": "Driver Safety Development Real Time Driver Drowsiness Detection System\n  Based on Convolutional Neural Network", "comments": "Hashemi, M., Mirrashid, A. & Beheshti Shirazi, A. Driver Safety\n  Development: Real-Time Driver Drowsiness Detection System Based on\n  Convolutional Neural Network. SN COMPUT. SCI. 1, 289 (2020).\n  https://doi.org/10.1007/s42979-020-00306-9", "journal-ref": null, "doi": "10.1007/s42979-020-00306-9", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the challenge of driver safety on the road and presents\na novel system for driver drowsiness detection. In this system, to detect the\nfalling sleep state of the driver as the sign of drowsiness, Convolutional\nNeural Networks (CNN) are used with regarding the two goals of real-time\napplication, including high accuracy and fastness. Three networks introduced as\na potential network for eye status classifcation in which one of them is a\nFully Designed Neural Network (FD-NN) and others use Transfer Learning in VGG16\nand VGG19 with extra designed layers (TL-VGG). Lack of an available and\naccurate eye dataset strongly feels in the area of eye closure detection.\nTherefore, a new comprehensive dataset proposed. The experimental results show\nthe high accuracy and low computational complexity of the eye closure\nestimation and the ability of the proposed framework on drowsiness detection.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 05:38:24 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 05:29:38 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 04:30:09 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hashemi", "Maryam", ""], ["Mirrashid", "Alireza", ""], ["Shirazi", "Aliasghar Beheshti", ""]]}, {"id": "2001.05152", "submitter": "Nilavra Bhattacharya", "authors": "Nilavra Bhattacharya, Somnath Rakshit, Jacek Gwizdka, Paul Kogut", "title": "Relevance Prediction from Eye-movements Using Semi-interpretable\n  Convolutional Neural Networks", "comments": null, "journal-ref": "2020 Conference on Human Information Interaction and Retrieval\n  (CHIIR '20), March 14--18, 2020, Vancouver, BC, Canada", "doi": "10.1145/3343413.3377960", "report-no": null, "categories": "cs.HC cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image-classification method to predict the perceived-relevance\nof text documents from eye-movements. An eye-tracking study was conducted where\nparticipants read short news articles, and rated them as relevant or irrelevant\nfor answering a trigger question. We encode participants' eye-movement\nscanpaths as images, and then train a convolutional neural network classifier\nusing these scanpath images. The trained classifier is used to predict\nparticipants' perceived-relevance of news articles from the corresponding\nscanpath images. This method is content-independent, as the classifier does not\nrequire knowledge of the screen-content, or the user's information-task. Even\nwith little data, the image classifier can predict perceived-relevance with up\nto 80% accuracy. When compared to similar eye-tracking studies from the\nliterature, this scanpath image classification method outperforms previously\nreported metrics by appreciable margins. We also attempt to interpret how the\nimage classifier differentiates between scanpaths on relevant and irrelevant\ndocuments.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 07:02:14 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Bhattacharya", "Nilavra", ""], ["Rakshit", "Somnath", ""], ["Gwizdka", "Jacek", ""], ["Kogut", "Paul", ""]]}, {"id": "2001.05153", "submitter": "Bum Jun Kim", "authors": "Bum Jun Kim, Gyogwon Koo, Hyeyeon Choi, and Sang Woo Kim", "title": "Extending Class Activation Mapping Using Gaussian Receptive Field", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the visualization task of deep learning models. To\nimprove Class Activation Mapping (CAM) based visualization method, we offer two\noptions. First, we propose Gaussian upsampling, an improved upsampling method\nthat can reflect the characteristics of deep learning models. Second, we\nidentify and modify unnatural terms in the mathematical derivation of the\nexisting CAM studies. Based on two options, we propose Extended-CAM, an\nadvanced CAM-based visualization method, which exhibits improved theoretical\nproperties. Experimental results show that Extended-CAM provides more accurate\nvisualization than the existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 07:04:07 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Kim", "Bum Jun", ""], ["Koo", "Gyogwon", ""], ["Choi", "Hyeyeon", ""], ["Kim", "Sang Woo", ""]]}, {"id": "2001.05158", "submitter": "Pargorn Puttapirat", "authors": "Pargorn Puttapirat, Haichuan Zhang, Jingyi Deng, Yuxin Dong, Jiangbo\n  Shi, Hongyu He, Zeyu Gao, Chunbao Wang, Xiangrong Zhang, Chen Li", "title": "OpenHI2 -- Open source histopathological image platform", "comments": "Preprint version accepted to AIPath2019 workshop at BIBM2019. 6\n  pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.NI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transition from conventional to digital pathology requires a new category of\nbiomedical informatic infrastructure which could facilitate delicate\npathological routine. Pathological diagnoses are sensitive to many external\nfactors and is known to be subjective. Only systems that can meet strict\nrequirements in pathology would be able to run along pathological routines and\neventually digitized the study area, and the developed platform should comply\nwith existing pathological routines and international standards. Currently,\nthere are a number of available software tools which can perform\nhistopathological tasks including virtual slide viewing, annotating, and basic\nimage analysis, however, none of them can serve as a digital platform for\npathology. Here we describe OpenHI2, an enhanced version Open Histopathological\nImage platform which is capable of supporting all basic pathological tasks and\nfile formats; ready to be deployed in medical institutions on a standard server\nenvironment or cloud computing infrastructure. In this paper, we also describe\nthe development decisions for the platform and propose solutions to overcome\ntechnical challenges so that OpenHI2 could be used as a platform for\nhistopathological images. Further addition can be made to the platform since\neach component is modularized and fully documented. OpenHI2 is free,\nopen-source, and available at https://gitlab.com/BioAI/OpenHI.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 07:29:29 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Puttapirat", "Pargorn", ""], ["Zhang", "Haichuan", ""], ["Deng", "Jingyi", ""], ["Dong", "Yuxin", ""], ["Shi", "Jiangbo", ""], ["He", "Hongyu", ""], ["Gao", "Zeyu", ""], ["Wang", "Chunbao", ""], ["Zhang", "Xiangrong", ""], ["Li", "Chen", ""]]}, {"id": "2001.05161", "submitter": "Jing Li", "authors": "Jing Li and Jing Xu and Fangwei Zhong and Xiangyu Kong and Yu Qiao and\n  Yizhou Wang", "title": "Pose-Assisted Multi-Camera Collaboration for Active Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Object Tracking (AOT) is crucial to many visionbased applications,\ne.g., mobile robot, intelligent surveillance. However, there are a number of\nchallenges when deploying active tracking in complex scenarios, e.g., target is\nfrequently occluded by obstacles. In this paper, we extend the single-camera\nAOT to a multi-camera setting, where cameras tracking a target in a\ncollaborative fashion. To achieve effective collaboration among cameras, we\npropose a novel Pose-Assisted Multi-Camera Collaboration System, which enables\na camera to cooperate with the others by sharing camera poses for active object\ntracking. In the system, each camera is equipped with two controllers and a\nswitcher: The vision-based controller tracks targets based on observed images.\nThe pose-based controller moves the camera in accordance to the poses of the\nother cameras. At each step, the switcher decides which action to take from the\ntwo controllers according to the visibility of the target. The experimental\nresults demonstrate that our system outperforms all the baselines and is\ncapable of generalizing to unseen environments. The code and demo videos are\navailable on our website\nhttps://sites.google.com/view/pose-assistedcollaboration.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 07:49:49 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Li", "Jing", ""], ["Xu", "Jing", ""], ["Zhong", "Fangwei", ""], ["Kong", "Xiangyu", ""], ["Qiao", "Yu", ""], ["Wang", "Yizhou", ""]]}, {"id": "2001.05197", "submitter": "Xin Jin", "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen", "title": "Uncertainty-Aware Multi-Shot Knowledge Distillation for Image-Based\n  Object Re-Identification", "comments": "Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object re-identification (re-id) aims to identify a specific object across\ntimes or camera views, with the person re-id and vehicle re-id as the most\nwidely studied applications. Re-id is challenging because of the variations in\nviewpoints, (human) poses, and occlusions. Multi-shots of the same object can\ncover diverse viewpoints/poses and thus provide more comprehensive information.\nIn this paper, we propose exploiting the multi-shots of the same identity to\nguide the feature learning of each individual image. Specifically, we design an\nUncertainty-aware Multi-shot Teacher-Student (UMTS) Network. It consists of a\nteacher network (T-net) that learns the comprehensive features from multiple\nimages of the same object, and a student network (S-net) that takes a single\nimage as input. In particular, we take into account the data dependent\nheteroscedastic uncertainty for effectively transferring the knowledge from the\nT-net to S-net. To the best of our knowledge, we are the first to make use of\nmulti-shots of an object in a teacher-student learning manner for effectively\nboosting the single image based re-id. We validate the effectiveness of our\napproach on the popular vehicle re-id and person re-id datasets. In inference,\nthe S-net alone significantly outperforms the baselines and achieves the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 09:39:05 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 17:21:07 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Jin", "Xin", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""]]}, {"id": "2001.05200", "submitter": "Ikram Achar", "authors": "Rabie Hachemi, Ikram Achar, Biasi Wiga, Mahfoud Sidi Ali Mebarek", "title": "Evaluating image matching methods for book cover identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are capable of identifying a book only by looking at its cover, but\nhow can computers do the same? In this paper, we explore different feature\ndetectors and matching methods for book cover identification, and compare their\nperformances in terms of both speed and accuracy. This will allow, for example,\nlibraries to develop interactive services based on cover book picture. Only one\nsingle image of a cover book needs to be available through a database. Tests\nhave been performed by taking into account different transformations of each\nbook cover image. Encouraging results have been achieved.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 09:52:38 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Hachemi", "Rabie", ""], ["Achar", "Ikram", ""], ["Wiga", "Biasi", ""], ["Mebarek", "Mahfoud Sidi Ali", ""]]}, {"id": "2001.05201", "submitter": "Wayne Wu", "authors": "Linsen Song, Wayne Wu, Chen Qian, Ran He, Chen Change Loy", "title": "Everybody's Talkin': Let Me Talk as You Want", "comments": "Technical report. Project page:\n  https://wywu.github.io/projects/EBT/EBT.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to edit a target portrait footage by taking a sequence of\naudio as input to synthesize a photo-realistic video. This method is unique\nbecause it is highly dynamic. It does not assume a person-specific rendering\nnetwork yet capable of translating arbitrary source audio into arbitrary video\noutput. Instead of learning a highly heterogeneous and nonlinear mapping from\naudio to the video directly, we first factorize each target video frame into\northogonal parameter spaces, i.e., expression, geometry, and pose, via\nmonocular 3D face reconstruction. Next, a recurrent network is introduced to\ntranslate source audio into expression parameters that are primarily related to\nthe audio content. The audio-translated expression parameters are then used to\nsynthesize a photo-realistic human subject in each video frame, with the\nmovement of the mouth regions precisely mapped to the source audio. The\ngeometry and pose parameters of the target human portrait are retained,\ntherefore preserving the context of the original video footage. Finally, we\nintroduce a novel video rendering network and a dynamic programming method to\nconstruct a temporally coherent and photo-realistic video. Extensive\nexperiments demonstrate the superiority of our method over existing approaches.\nOur method is end-to-end learnable and robust to voice variations in the source\naudio.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 09:54:23 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Song", "Linsen", ""], ["Wu", "Wayne", ""], ["Qian", "Chen", ""], ["He", "Ran", ""], ["Loy", "Chen Change", ""]]}, {"id": "2001.05216", "submitter": "Lior Wolf", "authors": "Irad Peleg and Lior Wolf", "title": "Structured GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Generative Adversarial Networks (GANs), in which the symmetric\nproperty of the generated images is controlled. This is obtained through the\ngenerator network's architecture, while the training procedure and the loss\nremain the same. The symmetric GANs are applied to face image synthesis in\norder to generate novel faces with a varying amount of symmetry. We also\npresent an unsupervised face rotation capability, which is based on the novel\nnotion of one-shot fine tuning.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 10:25:39 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Peleg", "Irad", ""], ["Wolf", "Lior", ""]]}, {"id": "2001.05238", "submitter": "Marie-Neige Chapel", "authors": "Marie-Neige Chapel and Thierry Bouwmans", "title": "Moving Objects Detection with a Moving Camera: A Comprehensive Review", "comments": "Submitted to Computer Science Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During about 30 years, a lot of research teams have worked on the big\nchallenge of detection of moving objects in various challenging environments.\nFirst applications concern static cameras but with the rise of the mobile\nsensors studies on moving cameras have emerged over time. In this survey, we\npropose to identify and categorize the different existing methods found in the\nliterature. For this purpose, we propose to classify these methods according to\nthe choose of the scene representation: one plane or several parts. Inside\nthese two categories, the methods are grouped according to eight different\napproaches: panoramic background subtraction, dual cameras, motion\ncompensation, subspace segmentation, motion segmentation, plane+parallax, multi\nplanes and split image in blocks. A reminder of methods for static cameras is\nprovided as well as the challenges with both static and moving cameras.\nPublicly available datasets and evaluation metrics are also surveyed in this\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 11:12:51 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Chapel", "Marie-Neige", ""], ["Bouwmans", "Thierry", ""]]}, {"id": "2001.05246", "submitter": "Yafei Song", "authors": "Yafei Song and Jia Li and Xiaogang Wang and Xiaowu Chen", "title": "Single Image Dehazing Using Ranking Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2017.2771472", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image dehazing, which aims to recover the clear image solely from an\ninput hazy or foggy image, is a challenging ill-posed problem. Analysing\nexisting approaches, the common key step is to estimate the haze density of\neach pixel. To this end, various approaches often heuristically designed\nhaze-relevant features. Several recent works also automatically learn the\nfeatures via directly exploiting Convolutional Neural Networks (CNN). However,\nit may be insufficient to fully capture the intrinsic attributes of hazy\nimages. To obtain effective features for single image dehazing, this paper\npresents a novel Ranking Convolutional Neural Network (Ranking-CNN). In\nRanking-CNN, a novel ranking layer is proposed to extend the structure of CNN\nso that the statistical and structural attributes of hazy images can be\nsimultaneously captured. By training Ranking-CNN in a well-designed manner,\npowerful haze-relevant features can be automatically learned from massive hazy\nimage patches. Based on these features, haze can be effectively removed by\nusing a haze density prediction model trained through the random forest\nregression. Experimental results show that our approach outperforms several\nprevious dehazing approaches on synthetic and real-world benchmark images.\nComprehensive analyses are also conducted to interpret the proposed Ranking-CNN\nfrom both the theoretical and experimental aspects.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 11:25:08 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Song", "Yafei", ""], ["Li", "Jia", ""], ["Wang", "Xiaogang", ""], ["Chen", "Xiaowu", ""]]}, {"id": "2001.05264", "submitter": "Diego Valsesia", "authors": "Andrea Bordone Molini, Diego Valsesia, Giulia Fracastoro, Enrico Magli", "title": "Towards Deep Unsupervised SAR Despeckling with Blind-Spot Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAR despeckling is a problem of paramount importance in remote sensing, since\nit represents the first step of many scene analysis algorithms. Recently, deep\nlearning techniques have outperformed classical model-based despeckling\nalgorithms. However, such methods require clean ground truth images for\ntraining, thus resorting to synthetically speckled optical images since clean\nSAR images cannot be acquired. In this paper, inspired by recent works on\nblind-spot denoising networks, we propose a self-supervised Bayesian\ndespeckling method. The proposed method is trained employing only noisy images\nand can therefore learn features of real SAR images rather than synthetic data.\nWe show that the performance of the proposed network is very close to the\nsupervised training approach on synthetic data and competitive on real data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 12:21:12 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Molini", "Andrea Bordone", ""], ["Valsesia", "Diego", ""], ["Fracastoro", "Giulia", ""], ["Magli", "Enrico", ""]]}, {"id": "2001.05267", "submitter": "Jon Natanael Muhovi\\v{c}", "authors": "Jon Muhovi\\v{c}, Janez Per\\v{s}", "title": "Correcting Decalibration of Stereo Cameras in Self-Driving Vehicles", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of optical decalibration in mobile stereo camera\nsetups, especially in context of autonomous vehicles. In real world conditions,\nan optical system is subject to various sources of anticipated and\nunanticipated mechanical stress (vibration, rough handling, collisions).\nMechanical stress changes the geometry between the cameras that make up the\nstereo pair, and as a consequence, the pre-calculated epipolar geometry is no\nlonger valid. Our method is based on optimization of camera geometry parameters\nand plugs directly into the output of the stereo matching algorithm. Therefore,\nit is able to recover calibration parameters on image pairs obtained from a\ndecalibrated stereo system with minimal use of additional computing resources.\nThe number of successfully recovered depth pixels is used as an objective\nfunction, which we aim to maximize. Our simulation confirms that the method can\nrun constantly in parallel to stereo estimation and thus help keep the system\ncalibrated in real time. Results confirm that the method is able to recalibrate\nall the parameters except for the baseline distance, which scales the absolute\ndepth readings. However, that scaling factor could be uniquely determined using\nany kind of absolute range finding methods (e.g. a single beam time-of-flight\nsensor).\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 12:28:01 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Muhovi\u010d", "Jon", ""], ["Per\u0161", "Janez", ""]]}, {"id": "2001.05290", "submitter": "Kiran Raja Dr", "authors": "Kiran B. Raja and R. Raghavendra and Sushma Venkatesh and Christoph\n  Busch", "title": "Morton Filters for Superior Template Protection for Iris Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the fundamental performance issues of template protection (TP) for\niris verification. We base our work on the popular Bloom-Filter templates\nprotection & address the key challenges like sub-optimal performance and low\nunlinkability. Specifically, we focus on cases where Bloom-filter templates\nresults in non-ideal performance due to presence of large degradations within\niris images. Iris recognition is challenged with number of occluding factors\nsuch as presence of eye-lashes within captured image, occlusion due to eyelids,\nlow quality iris images due to motion blur. All of such degrading factors\nresult in obtaining non-reliable iris codes & thereby provide non-ideal\nbiometric performance. These factors directly impact the protected templates\nderived from iris images when classical Bloom-filters are employed. To this\nend, we propose and extend our earlier ideas of Morton-filters for obtaining\nbetter and reliable templates for iris. Morton filter based TP for iris codes\nis based on leveraging the intra and inter-class distribution by exploiting\nlow-rank iris codes to derive the stable bits across iris images for a\nparticular subject and also analyzing the discriminable bits across various\nsubjects. Such low-rank non-noisy iris codes enables realizing the template\nprotection in a superior way which not only can be used in constrained setting,\nbut also in relaxed iris imaging. We further extend the work to analyze the\napplicability to VIS iris images by employing a large scale public iris image\ndatabase - UBIRIS(v1 & v2), captured in a unconstrained setting. Through a set\nof experiments, we demonstrate the applicability of proposed approach and vet\nthe strengths and weakness. Yet another contribution of this work stems in\nassessing the security of the proposed approach where factors of Unlinkability\nis studied to indicate the antagonistic nature to relaxed iris imaging\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 13:15:33 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Raja", "Kiran B.", ""], ["Raghavendra", "R.", ""], ["Venkatesh", "Sushma", ""], ["Busch", "Christoph", ""]]}, {"id": "2001.05372", "submitter": "Mingi Lim", "authors": "Mingi Lim and Sung-eui Yoon", "title": "A Method for Estimating Reflectance map and Material using Deep Learning\n  with Synthetic Dataset", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of decomposing target images into their internal properties is a\ndifficult task due to the inherent ill-posed nature of the problem. The lack of\ndata required to train a network is a one of the reasons why the decomposing\nappearance task is difficult. In this paper, we propose a deep learning-based\nreflectance map prediction system for material estimation of target objects in\nthe image, so as to alleviate the ill-posed problem that occurs in this image\ndecomposition operation. We also propose a network architecture for\nBidirectional Reflectance Distribution Function (BRDF) parameter estimation,\nenvironment map estimation. We also use synthetic data to solve the lack of\ndata problems. We get out of the previously proposed Deep Learning-based\nnetwork architecture for reflectance map, and we newly propose to use\nconditional Generative Adversarial Network (cGAN) structures for estimating the\nreflectance map, which enables better results in many applications. To improve\nthe efficiency of learning in this structure, we newly utilized the loss\nfunction using the normal map of the target object.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 15:25:08 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Lim", "Mingi", ""], ["Yoon", "Sung-eui", ""]]}, {"id": "2001.05406", "submitter": "Kentaro Wada", "authors": "Kentaro Wada, Masaki Murooka, Kei Okada, Masayuki Inaba", "title": "3D Object Segmentation for Shelf Bin Picking by Humanoid with Deep\n  Learning and Occupancy Voxel Grid Map", "comments": "6 pages, 9 figures, IEEE-RAS International Conference on Humanoid\n  Robots 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Picking objects in a narrow space such as shelf bins is an important task for\nhumanoid to extract target object from environment. In those situations,\nhowever, there are many occlusions between the camera and objects, and this\nmakes it difficult to segment the target object three dimensionally because of\nthe lack of three dimentional sensor inputs. We address this problem with\naccumulating segmentation result with multiple camera angles, and generating\nvoxel model of the target object. Our approach consists of two components:\nfirst is object probability prediction for input image with convolutional\nnetworks, and second is generating voxel grid map which is designed for object\nsegmentation. We evaluated the method with the picking task experiment for\ntarget objects in narrow shelf bins. Our method generates dense 3D object\nsegments even with occlusions, and the real robot successfuly picked target\nobjects from the narrow space.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 16:20:46 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 08:54:39 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Wada", "Kentaro", ""], ["Murooka", "Masaki", ""], ["Okada", "Kei", ""], ["Inaba", "Masayuki", ""]]}, {"id": "2001.05419", "submitter": "Ev Zisselman", "authors": "Ev Zisselman and Aviv Tamar", "title": "Deep Residual Flow for Out of Distribution Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective application of neural networks in the real-world relies on\nproficiently detecting out-of-distribution examples. Contemporary methods seek\nto model the distribution of feature activations in the training data for\nadequately distinguishing abnormalities, and the state-of-the-art method uses\nGaussian distribution models. In this work, we present a novel approach that\nimproves upon the state-of-the-art by leveraging an expressive density model\nbased on normalizing flows. We introduce the residual flow, a novel flow\narchitecture that learns the residual distribution from a base Gaussian\ndistribution. Our model is general, and can be applied to any data that is\napproximately Gaussian. For out of distribution detection in image datasets,\nour approach provides a principled improvement over the state-of-the-art.\nSpecifically, we demonstrate the effectiveness of our method in ResNet and\nDenseNet architectures trained on various image datasets. For example, on a\nResNet trained on CIFAR-100 and evaluated on detection of out-of-distribution\nsamples from the ImageNet dataset, holding the true positive rate (TPR) at\n$95\\%$, we improve the true negative rate (TNR) from $56.7\\%$ (current\nstate-of-the-art) to $77.5\\%$ (ours).\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 16:38:47 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 16:20:47 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 17:44:12 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zisselman", "Ev", ""], ["Tamar", "Aviv", ""]]}, {"id": "2001.05422", "submitter": "Jieyu Li", "authors": "Jieyu Li, Robert L Stevenson", "title": "Indoor Layout Estimation by 2D LiDAR and Camera Fusion", "comments": "Fast track article for IS&T International Symposium on Electronic\n  Imaging 2020: Computational Imaging XVIII", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for indoor layout estimation and\nreconstruction through the fusion of a sequence of captured images and LiDAR\ndata sets. In the proposed system, a movable platform collects both intensity\nimages and 2D LiDAR information. Pose estimation and semantic segmentation is\ncomputed jointly by aligning the LiDAR points to line segments from the images.\nFor indoor scenes with walls orthogonal to floor, the alignment problem is\ndecoupled into top-down view projection and a 2D similarity transformation\nestimation and solved by the recursive random sample consensus (R-RANSAC)\nalgorithm. Hypotheses can be generated, evaluated and optimized by integrating\nnew scans as the platform moves throughout the environment. The proposed method\navoids the need of extensive prior training or a cuboid layout assumption,\nwhich is more effective and practical compared to most previous indoor layout\nestimation methods. Multi-sensor fusion allows the capability of providing\naccurate depth estimation and high resolution visual information.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 16:43:35 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Li", "Jieyu", ""], ["Stevenson", "Robert L", ""]]}, {"id": "2001.05425", "submitter": "Jonathon Luiten", "authors": "Jonathon Luiten, Idil Esen Zulfikar, Bastian Leibe", "title": "UnOVOST: Unsupervised Offline Video Object Segmentation and Tracking", "comments": "Accepted for publication at WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address Unsupervised Video Object Segmentation (UVOS), the task of\nautomatically generating accurate pixel masks for salient objects in a video\nsequence and of tracking these objects consistently through time, without any\ninput about which objects should be tracked. Towards solving this task, we\npresent UnOVOST (Unsupervised Offline Video Object Segmentation and Tracking)\nas a simple and generic algorithm which is able to track and segment a large\nvariety of objects. This algorithm builds up tracks in a number stages, first\ngrouping segments into short tracklets that are spatio-temporally consistent,\nbefore merging these tracklets into long-term consistent object tracks based on\ntheir visual similarity. In order to achieve this we introduce a novel\ntracklet-based Forest Path Cutting data association algorithm which builds up a\ndecision forest of track hypotheses before cutting this forest into paths that\nform long-term consistent object tracks. When evaluating our approach on the\nDAVIS 2017 Unsupervised dataset we obtain state-of-the-art performance with a\nmean J &F score of 67.9% on the val, 58% on the test-dev and 56.4% on the\ntest-challenge benchmarks, obtaining first place in the DAVIS 2019 Unsupervised\nVideo Object Segmentation Challenge. UnOVOST even performs competitively with\nmany semi-supervised video object segmentation algorithms even though it is not\ngiven any input as to which objects should be tracked and segmented.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 16:49:31 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Luiten", "Jonathon", ""], ["Zulfikar", "Idil Esen", ""], ["Leibe", "Bastian", ""]]}, {"id": "2001.05459", "submitter": "Qian Wang", "authors": "Qian Wang, Najla Megherbi, Toby P. Breckon", "title": "A Reference Architecture for Plausible Threat Image Projection (TIP)\n  Within 3D X-ray Computed Tomography Volumes", "comments": "Technical Report, Durham University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Threat Image Projection (TIP) is a technique used in X-ray security baggage\nscreening systems that superimposes a threat object signature onto a benign\nX-ray baggage image in a plausible and realistic manner. It has been shown to\nbe highly effective in evaluating the ongoing performance of human operators,\nimproving their vigilance and performance on threat detection. However, with\nthe increasing use of 3D Computed Tomography (CT) in aviation security for both\nhold and cabin baggage screening a significant challenge arises in extending\nTIP to 3D CT volumes due to the difficulty in 3D CT volume segmentation and the\nproper insertion location determination. In this paper, we present an approach\nfor 3D TIP in CT volumes targeting realistic and plausible threat object\ninsertion within 3D CT baggage images. The proposed approach consists of dual\nthreat (source) and baggage (target) volume segmentation, particle swarm\noptimisation based insertion determination and metal artefact generation. In\naddition, we propose a TIP quality score metric to evaluate the quality of\ngenerated TIP volumes. Qualitative evaluations on real 3D CT baggage imagery\nshow that our approach is able to generate realistic and plausible TIP which\nare indiscernible from real CT volumes and the TIP quality scores are\nconsistent with human evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 18:25:23 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Wang", "Qian", ""], ["Megherbi", "Najla", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2001.05488", "submitter": "Jennifer J. Sun", "authors": "Jennifer J. Sun, Ting Liu, Alan S. Cowen, Florian Schroff, Hartwig\n  Adam, Gautam Prasad", "title": "EEV: A Large-Scale Dataset for Studying Evoked Expressions from Video", "comments": "Data subset at https://github.com/google-research-datasets/eev", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos can evoke a range of affective responses in viewers. The ability to\npredict evoked affect from a video, before viewers watch the video, can help in\ncontent creation and video recommendation. We introduce the Evoked Expressions\nfrom Videos (EEV) dataset, a large-scale dataset for studying viewer responses\nto videos. Each video is annotated at 6 Hz with 15 continuous evoked expression\nlabels, corresponding to the facial expression of viewers who reacted to the\nvideo. We use an expression recognition model within our data collection\nframework to achieve scalability. In total, there are 36.7 million annotations\nof viewer facial reactions to 23,574 videos (1,700 hours). We use a publicly\navailable video corpus to obtain a diverse set of video content. We establish\nbaseline performance on the EEV dataset using an existing multimodal recurrent\nmodel. Transfer learning experiments show an improvement in performance on the\nLIRIS-ACCEDE video dataset when pre-trained on EEV. We hope that the size and\ndiversity of the EEV dataset will encourage further explorations in video\nunderstanding and affective computing. A subset of EEV is released at\nhttps://github.com/google-research-datasets/eev.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 18:59:51 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 18:33:20 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Sun", "Jennifer J.", ""], ["Liu", "Ting", ""], ["Cowen", "Alan S.", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""], ["Prasad", "Gautam", ""]]}, {"id": "2001.05489", "submitter": "Shiv Ram Dubey", "authors": "Kancharagunta Kishan Babu, Shiv Ram Dubey", "title": "CDGAN: Cyclic Discriminative Generative Adversarial Networks for\n  Image-to-Image Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image transformation is a kind of problem, where the input image\nfrom one visual representation is transformed into the output image of another\nvisual representation. Since 2014, Generative Adversarial Networks (GANs) have\nfacilitated a new direction to tackle this problem by introducing the generator\nand the discriminator networks in its architecture. Many recent works, like\nPix2Pix, CycleGAN, DualGAN, PS2MAN and CSGAN handled this problem with the\nrequired generator and discriminator networks and choice of the different\nlosses that are used in the objective functions. In spite of these works, still\nthere is a gap to fill in terms of both the quality of the images generated\nthat should look more realistic and as much as close to the ground truth\nimages. In this work, we introduce a new Image-to-Image Transformation network\nnamed Cyclic Discriminative Generative Adversarial Networks (CDGAN) that fills\nthe above mentioned gaps. The proposed CDGAN generates high quality and more\nrealistic images by incorporating the additional discriminator networks for\ncycled images in addition to the original architecture of the CycleGAN. To\ndemonstrate the performance of the proposed CDGAN, it is tested over three\ndifferent baseline image-to-image transformation datasets. The quantitative\nmetrics such as pixel-wise similarity, structural level similarity and\nperceptual level similarity are used to judge the performance. Moreover, the\nqualitative results are also analyzed and compared with the state-of-the-art\nmethods. The proposed CDGAN method clearly outperformed all the\nstate-of-the-art methods when compared over the three baseline Image-to-Image\ntransformation datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 05:12:32 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Babu", "Kancharagunta Kishan", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "2001.05545", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma, Pravendra Singh, Vinay P. Namboodiri, Piyush Rai", "title": "A \"Network Pruning Network\" Approach to Deep Model Compression", "comments": "Accepted in WACV'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a filter pruning approach for deep model compression, using a\nmultitask network. Our approach is based on learning a a pruner network to\nprune a pre-trained target network. The pruner is essentially a multitask deep\nneural network with binary outputs that help identify the filters from each\nlayer of the original network that do not have any significant contribution to\nthe model and can therefore be pruned. The pruner network has the same\narchitecture as the original network except that it has a\nmultitask/multi-output last layer containing binary-valued outputs (one per\nfilter), which indicate which filters have to be pruned. The pruner's goal is\nto minimize the number of filters from the original network by assigning zero\nweights to the corresponding output feature-maps. In contrast to most of the\nexisting methods, instead of relying on iterative pruning, our approach can\nprune the network (original network) in one go and, moreover, does not require\nspecifying the degree of pruning for each layer (and can learn it instead). The\ncompressed model produced by our approach is generic and does not need any\nspecial hardware/software support. Moreover, augmenting with other methods such\nas knowledge distillation, quantization, and connection pruning can increase\nthe degree of compression for the proposed approach. We show the efficacy of\nour proposed approach for classification and object detection tasks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 20:38:23 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Singh", "Pravendra", ""], ["Namboodiri", "Vinay P.", ""], ["Rai", "Piyush", ""]]}, {"id": "2001.05548", "submitter": "Chen Liu", "authors": "Nanyan Zhu, Chen Liu, Zakary S. Singer, Tal Danino, Andrew F. Laine,\n  Jia Guo", "title": "Segmentation with Residual Attention U-Net and an Edge-Enhancement\n  Approach Preserves Cell Shape Features", "comments": "7 pages, 4 figures, 1 table. Nanyan Zhu and Chen Liu share equal\n  contribution and are listed as co-first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to extrapolate gene expression dynamics in living single cells\nrequires robust cell segmentation, and one of the challenges is the amorphous\nor irregularly shaped cell boundaries. To address this issue, we modified the\nU-Net architecture to segment cells in fluorescence widefield microscopy images\nand quantitatively evaluated its performance. We also proposed a novel loss\nfunction approach that emphasizes the segmentation accuracy on cell boundaries\nand encourages shape feature preservation. With a 97% sensitivity, 93%\nspecificity, 91% Jaccard similarity, and 95% Dice coefficient, our proposed\nmethod called Residual Attention U-Net with edge-enhancement surpassed the\nstate-of-the-art U-Net in segmentation performance as evaluated by the\ntraditional metrics. More remarkably, the same proposed candidate also\nperformed the best in terms of the preservation of valuable shape features,\nnamely area, eccentricity, major axis length, solidity and orientation. These\nimprovements on shape feature preservation can serve as useful assets for\ndownstream cell tracking and quantification of changes in cell statistics or\nfeatures over time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 20:44:39 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Zhu", "Nanyan", ""], ["Liu", "Chen", ""], ["Singer", "Zakary S.", ""], ["Danino", "Tal", ""], ["Laine", "Andrew F.", ""], ["Guo", "Jia", ""]]}, {"id": "2001.05549", "submitter": "Ilker Ali Ozkan", "authors": "Esra Kaya, \\.Ismail Sar{\\i}ta\\c{s}, Ilker Ali Ozkan", "title": "Supervised Segmentation of Retinal Vessel Structures Using ANN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, a supervised retina blood vessel segmentation process was\nperformed on the green channel of the RGB image using artificial neural network\n(ANN). The green channel is preferred because the retinal vessel structures can\nbe distinguished most clearly from the green channel of the RGB image. The\nstudy was performed using 20 images in the DRIVE data set which is one of the\nmost common retina data sets known. The images went through some preprocessing\nstages like contrastlimited adaptive histogram equalization (CLAHE), color\nintensity adjustment, morphological operations and median and Gaussian\nfiltering to obtain a good segmentation. Retinal vessel structures were\nhighlighted with top-hat and bot-hat morphological operations and converted to\nbinary image by using global thresholding. Then, the network was trained by the\nbinary version of the images specified as training images in the dataset and\nthe targets are the images segmented manually by a specialist. The average\nsegmentation accuracy for 20 images was found as 0.9492.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 20:48:03 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Kaya", "Esra", ""], ["Sar\u0131ta\u015f", "\u0130smail", ""], ["Ozkan", "Ilker Ali", ""]]}, {"id": "2001.05551", "submitter": "Haoran Sun", "authors": "Haoran Sun, Xueqing Liu, Xinyang Feng, Chen Liu, Nanyan Zhu, Sabrina\n  J. Gjerswold-Selleck, Hong-Jian Wei, Pavan S. Upadhyayula, Angeliki Mela,\n  Cheng-Chia Wu, Peter D. Canoll, Andrew F. Laine, J. Thomas Vaughan, Scott A.\n  Small, Jia Guo", "title": "Substituting Gadolinium in Brain MRI Using DeepContrast", "comments": null, "journal-ref": "The IEEE International Symposium on Biomedical Imaging (ISBI) 2020", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cerebral blood volume (CBV) is a hemodynamic correlate of oxygen metabolism\nand reflects brain activity and function. High-resolution CBV maps can be\ngenerated using the steady-state gadolinium-enhanced MRI technique. Such a\ntechnique requires an intravenous injection of exogenous gadolinium based\ncontrast agent (GBCA) and recent studies suggest that the GBCA can accumulate\nin the brain after frequent use. We hypothesize that endogenous sources of\ncontrast might exist within the most conventional and commonly acquired\nstructural MRI, potentially obviating the need for exogenous contrast. Here, we\ntest this hypothesis by developing and optimizing a deep learning algorithm,\nwhich we call DeepContrast, in mice. We find that DeepContrast performs equally\nwell as exogenous GBCA in mapping CBV of the normal brain tissue and enhancing\nglioblastoma. Together, these studies validate our hypothesis that a deep\nlearning approach can potentially replace the need for GBCAs in brain MRI.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 20:53:40 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Sun", "Haoran", ""], ["Liu", "Xueqing", ""], ["Feng", "Xinyang", ""], ["Liu", "Chen", ""], ["Zhu", "Nanyan", ""], ["Gjerswold-Selleck", "Sabrina J.", ""], ["Wei", "Hong-Jian", ""], ["Upadhyayula", "Pavan S.", ""], ["Mela", "Angeliki", ""], ["Wu", "Cheng-Chia", ""], ["Canoll", "Peter D.", ""], ["Laine", "Andrew F.", ""], ["Vaughan", "J. Thomas", ""], ["Small", "Scott A.", ""], ["Guo", "Jia", ""]]}, {"id": "2001.05566", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser\n  Kehtarnavaz, and Demetri Terzopoulos", "title": "Image Segmentation Using Deep Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is a key topic in image processing and computer vision\nwith applications such as scene understanding, medical image analysis, robotic\nperception, video surveillance, augmented reality, and image compression, among\nmany others. Various algorithms for image segmentation have been developed in\nthe literature. Recently, due to the success of deep learning models in a wide\nrange of vision applications, there has been a substantial amount of works\naimed at developing image segmentation approaches using deep learning models.\nIn this survey, we provide a comprehensive review of the literature at the time\nof this writing, covering a broad spectrum of pioneering works for semantic and\ninstance-level segmentation, including fully convolutional pixel-labeling\nnetworks, encoder-decoder architectures, multi-scale and pyramid based\napproaches, recurrent networks, visual attention models, and generative models\nin adversarial settings. We investigate the similarity, strengths and\nchallenges of these deep learning models, examine the most widely used\ndatasets, report performances, and discuss promising future research directions\nin this area.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 21:37:47 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 01:40:34 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 14:56:07 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 15:19:39 GMT"}, {"version": "v5", "created": "Sun, 15 Nov 2020 04:51:11 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Minaee", "Shervin", ""], ["Boykov", "Yuri", ""], ["Porikli", "Fatih", ""], ["Plaza", "Antonio", ""], ["Kehtarnavaz", "Nasser", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "2001.05578", "submitter": "Yuzhen Ding", "authors": "Yuzhen Ding, Baoxin Li", "title": "VSEC-LDA: Boosting Topic Modeling with Embedded Vocabulary Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling has found wide application in many problems where latent\nstructures of the data are crucial for typical inference tasks. When applying a\ntopic model, a relatively standard pre-processing step is to first build a\nvocabulary of frequent words. Such a general pre-processing step is often\nindependent of the topic modeling stage, and thus there is no guarantee that\nthe pre-generated vocabulary can support the inference of some optimal (or even\nmeaningful) topic models appropriate for a given task, especially for computer\nvision applications involving \"visual words\". In this paper, we propose a new\napproach to topic modeling, termed Vocabulary-Selection-Embedded\nCorrespondence-LDA (VSEC-LDA), which learns the latent model while\nsimultaneously selecting most relevant words. The selection of words is driven\nby an entropy-based metric that measures the relative contribution of the words\nto the underlying model, and is done dynamically while the model is learned. We\npresent three variants of VSEC-LDA and evaluate the proposed approach with\nexperiments on both synthetic and real databases from different applications.\nThe results demonstrate the effectiveness of built-in vocabulary selection and\nits importance in improving the performance of topic modeling.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 22:16:24 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Ding", "Yuzhen", ""], ["Li", "Baoxin", ""]]}, {"id": "2001.05613", "submitter": "Takuya Ohashi", "authors": "Takuya Ohashi, Yosuke Ikegami, Yoshihiko Nakamura", "title": "Synergetic Reconstruction from 2D Pose and 3D Motion for Wide-Space\n  Multi-Person Video Motion Capture in the Wild", "comments": null, "journal-ref": "Image and Vision Computing, Volume 104, 2020", "doi": "10.1016/j.imavis.2020.104028", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many studies have investigated markerless motion capture, the\ntechnology has not been applied to real sports or concerts. In this paper, we\npropose a markerless motion capture method with spatiotemporal accuracy and\nsmoothness from multiple cameras in wide-space and multi-person environments.\nThe proposed method predicts each person's 3D pose and determines the bounding\nbox of multi-camera images small enough. This prediction and spatiotemporal\nfiltering based on human skeletal model enables 3D reconstruction of the person\nand demonstrates high-accuracy. The accurate 3D reconstruction is then used to\npredict the bounding box of each camera image in the next frame. This is\nfeedback from the 3D motion to 2D pose, and provides a synergetic effect on the\noverall performance of video motion capture. We evaluated the proposed method\nusing various datasets and a real sports field. The experimental results\ndemonstrate that the mean per joint position error (MPJPE) is 31.5 mm and the\npercentage of correct parts (PCP) is 99.5% for five people dynamically moving\nwhile satisfying the range of motion (RoM). Video demonstration, datasets, and\nadditional materials are posted on our project page.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 02:14:59 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 04:08:05 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Ohashi", "Takuya", ""], ["Ikegami", "Yosuke", ""], ["Nakamura", "Yoshihiko", ""]]}, {"id": "2001.05614", "submitter": "Haoran Chen", "authors": "Haoran Chen, Jianmin Li and Xiaolin Hu", "title": "Delving Deeper into the Decoder for Video Captioning", "comments": "8 pages, 3 figures, European Conference on Artificial Intelligence.\n  ECAI 2020", "journal-ref": null, "doi": "10.3233/FAIA200204", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning is an advanced multi-modal task which aims to describe a\nvideo clip using a natural language sentence. The encoder-decoder framework is\nthe most popular paradigm for this task in recent years. However, there exist\nsome problems in the decoder of a video captioning model. We make a thorough\ninvestigation into the decoder and adopt three techniques to improve the\nperformance of the model. First of all, a combination of variational dropout\nand layer normalization is embedded into a recurrent unit to alleviate the\nproblem of overfitting. Secondly, a new online method is proposed to evaluate\nthe performance of a model on a validation set so as to select the best\ncheckpoint for testing. Finally, a new training strategy called professional\nlearning is proposed which uses the strengths of a captioning model and\nbypasses its weaknesses. It is demonstrated in the experiments on Microsoft\nResearch Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT)\ndatasets that our model has achieved the best results evaluated by BLEU, CIDEr,\nMETEOR and ROUGE-L metrics with significant gains of up to 18% on MSVD and 3.5%\non MSR-VTT compared with the previous state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 02:18:27 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 06:36:17 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 01:31:21 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Chen", "Haoran", ""], ["Li", "Jianmin", ""], ["Hu", "Xiaolin", ""]]}, {"id": "2001.05634", "submitter": "Vishal Keshav", "authors": "Vishal Keshav and Fabien Delattre", "title": "Self-supervised visual feature learning with curriculum", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning techniques have shown their abilities to learn\nmeaningful feature representation. This is made possible by training a model on\npretext tasks that only requires to find correlations between inputs or parts\nof inputs. However, such pretext tasks need to be carefully hand selected to\navoid low level signals that could make those pretext tasks trivial. Moreover,\nremoving those shortcuts often leads to the loss of some semantically valuable\ninformation. We show that it directly impacts the speed of learning of the\ndownstream task. In this paper we took inspiration from curriculum learning to\nprogressively remove low level signals and show that it significantly increase\nthe speed of convergence of the downstream task.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 03:28:58 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Keshav", "Vishal", ""], ["Delattre", "Fabien", ""]]}, {"id": "2001.05643", "submitter": "Saeed Amirgholipour Kasmani", "authors": "Saeed Amirgholipour, Xiangjian He, Wenjing Jia, Dadong Wang, and Lei\n  Liu", "title": "PDANet: Pyramid Density-aware Attention Net for Accurate Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting, i.e., estimating the number of people in a crowded area, has\nattracted much interest in the research community. Although many attempts have\nbeen reported, crowd counting remains an open real-world problem due to the\nvast scale variations in crowd density within the interested area, and severe\nocclusion among the crowd. In this paper, we propose a novel Pyramid\nDensity-Aware Attention-based network, abbreviated as PDANet, that leverages\nthe attention, pyramid scale feature and two branch decoder modules for\ndensity-aware crowd counting. The PDANet utilizes these modules to extract\ndifferent scale features, focus on the relevant information, and suppress the\nmisleading ones. We also address the variation of crowdedness levels among\ndifferent images with an exclusive Density-Aware Decoder (DAD). For this\npurpose, a classifier evaluates the density level of the input features and\nthen passes them to the corresponding high and low crowded DAD modules.\nFinally, we generate an overall density map by considering the summation of low\nand high crowded density maps as spatial attention. Meanwhile, we employ two\nlosses to create a precise density map for the input scene. Extensive\nevaluations conducted on the challenging benchmark datasets well demonstrate\nthe superior performance of the proposed PDANet in terms of the accuracy of\ncounting and generated density maps over the well-known state of the arts.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 04:26:05 GMT"}, {"version": "v10", "created": "Wed, 29 Apr 2020 03:04:05 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 05:25:17 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 05:29:56 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2020 05:31:50 GMT"}, {"version": "v5", "created": "Tue, 25 Feb 2020 05:56:25 GMT"}, {"version": "v6", "created": "Tue, 25 Feb 2020 05:57:54 GMT"}, {"version": "v7", "created": "Thu, 26 Mar 2020 02:44:49 GMT"}, {"version": "v8", "created": "Sat, 4 Apr 2020 12:20:18 GMT"}, {"version": "v9", "created": "Wed, 15 Apr 2020 02:21:27 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Amirgholipour", "Saeed", ""], ["He", "Xiangjian", ""], ["Jia", "Wenjing", ""], ["Wang", "Dadong", ""], ["Liu", "Lei", ""]]}, {"id": "2001.05650", "submitter": "Jesse Haviland", "authors": "Jesse Haviland, Feras Dayoub, Peter Corke", "title": "Control of the Final-Phase of Closed-Loop Visual Grasping using\n  Image-Based Visual Servoing", "comments": "Under review for RA-L and IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the final approach phase of visual-closed-loop grasping\nwhere the RGB-D camera is no longer able to provide valid depth information.\nMany current robotic grasping controllers are not closed-loop and therefore\nfail for moving objects. Closed-loop grasp controllers based on RGB-D imagery\ncan track a moving object, but fail when the sensor's minimum object distance\nis violated just before grasping. To overcome this we propose the use of\nimage-based visual servoing (IBVS) to guide the robot to the object-relative\ngrasp pose using camera RGB information. IBVS robustly moves the camera to a\ngoal pose defined implicitly in terms of an image-plane feature configuration.\nIn this work, the goal image feature coordinates are predicted from RGB-D data\nto enable RGB-only tracking once depth data becomes unavailable -- this enables\nmore reliable grasping of previously unseen moving objects. Experimental\nresults are provided.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 05:07:01 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 03:14:44 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Haviland", "Jesse", ""], ["Dayoub", "Feras", ""], ["Corke", "Peter", ""]]}, {"id": "2001.05651", "submitter": "Dezhao Wang", "authors": "Dezhao Wang, Sifeng Xia, Wenhan Yang, and Jiaying Liu", "title": "Combining Progressive Rethinking and Collaborative Learning: A Deep\n  Framework for In-Loop Filtering", "comments": "Accepted for publication in IEEE Transactions on Image Processing\n  (TIP). Website available at https://dezhao-wang.github.io/PRN-v2/", "journal-ref": null, "doi": "10.1109/TIP.2021.3068638", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to address issues of (1) joint spatial-temporal\nmodeling and (2) side information injection for deep-learning based in-loop\nfilter. For (1), we design a deep network with both progressive rethinking and\ncollaborative learning mechanisms to improve quality of the reconstructed\nintra-frames and inter-frames, respectively. For intra coding, a Progressive\nRethinking Network (PRN) is designed to simulate the human decision mechanism\nfor effective spatial modeling. Our designed block introduces an additional\ninter-block connection to bypass a high-dimensional informative feature before\nthe bottleneck module across blocks to review the complete past memorized\nexperiences and rethinks progressively. For inter coding, the current\nreconstructed frame interacts with reference frames (peak quality frame and the\nnearest adjacent frame) collaboratively at the feature level. For (2), we\nextract both intra-frame and inter-frame side information for better context\nmodeling. A coarse-to-fine partition map based on HEVC partition trees is built\nas the intra-frame side information. Furthermore, the warped features of the\nreference frames are offered as the inter-frame side information. Our PRN with\nintra-frame side information provides 9.0% BD-rate reduction on average\ncompared to HEVC baseline under All-intra (AI) configuration. While under\nLow-Delay B (LDB), Low-Delay P (LDP) and Random Access (RA) configuration, our\nPRN with inter-frame side information provides 9.0%, 10.6% and 8.0% BD-rate\nreduction on average respectively. Our project webpage is\nhttps://dezhao-wang.github.io/PRN-v2/.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 05:14:34 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 06:47:36 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 09:33:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Dezhao", ""], ["Xia", "Sifeng", ""], ["Yang", "Wenhan", ""], ["Liu", "Jiaying", ""]]}, {"id": "2001.05654", "submitter": "Hongwei Xie", "authors": "Hongwei Xie, Jiafang Wang, Baitao Shao, Jian Gu, Mingyang Li", "title": "LE-HGR: A Lightweight and Efficient RGB-based Online Gesture Recognition\n  Network for Embedded AR Devices", "comments": "Published in: 2019 IEEE International Symposium on Mixed and\n  Augmented Reality Adjunct (ISMAR-Adjunct)", "journal-ref": null, "doi": "10.1109/ISMAR-Adjunct.2019.00-30", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online hand gesture recognition (HGR) techniques are essential in augmented\nreality (AR) applications for enabling natural human-to-computer interaction\nand communication. In recent years, the consumer market for low-cost AR devices\nhas been rapidly growing, while the technology maturity in this domain is still\nlimited. Those devices are typical of low prices, limited memory, and\nresource-constrained computational units, which makes online HGR a challenging\nproblem. To tackle this problem, we propose a lightweight and computationally\nefficient HGR framework, namely LE-HGR, to enable real-time gesture recognition\non embedded devices with low computing power. We also show that the proposed\nmethod is of high accuracy and robustness, which is able to reach high-end\nperformance in a variety of complicated interaction environments. To achieve\nour goal, we first propose a cascaded multi-task convolutional neural network\n(CNN) to simultaneously predict probabilities of hand detection and regress\nhand keypoint locations online. We show that, with the proposed cascaded\narchitecture design, false-positive estimates can be largely eliminated.\nAdditionally, an associated mapping approach is introduced to track the hand\ntrace via the predicted locations, which addresses the interference of\nmulti-handedness. Subsequently, we propose a trace sequence neural network\n(TraceSeqNN) to recognize the hand gesture by exploiting the motion features of\nthe tracked trace. Finally, we provide a variety of experimental results to\nshow that the proposed framework is able to achieve state-of-the-art accuracy\nwith significantly reduced computational cost, which are the key properties for\nenabling real-time applications in low-cost commercial devices such as mobile\ndevices and AR/VR headsets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 05:23:24 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Xie", "Hongwei", ""], ["Wang", "Jiafang", ""], ["Shao", "Baitao", ""], ["Gu", "Jian", ""], ["Li", "Mingyang", ""]]}, {"id": "2001.05661", "submitter": "Li Tao", "authors": "Li Tao, Xueting Wang, Toshihiko Yamasaki", "title": "Rethinking Motion Representation: Residual Frames with 3D ConvNets for\n  Better Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, 3D convolutional networks yield good performance in action\nrecognition. However, optical flow stream is still needed to ensure better\nperformance, the cost of which is very high. In this paper, we propose a fast\nbut effective way to extract motion features from videos utilizing residual\nframes as the input data in 3D ConvNets. By replacing traditional stacked RGB\nframes with residual ones, 20.5% and 12.5% points improvements over top-1\naccuracy can be achieved on the UCF101 and HMDB51 datasets when trained from\nscratch. Because residual frames contain little information of object\nappearance, we further use a 2D convolutional network to extract appearance\nfeatures and combine them with the results from residual frames to form a\ntwo-path solution. In three benchmark datasets, our two-path solution achieved\nbetter or comparable performances than those using additional optical flow\nmethods, especially outperformed the state-of-the-art models on Mini-kinetics\ndataset. Further analysis indicates that better motion features can be\nextracted using residual frames with 3D ConvNets, and our residual-frame-input\npath is a good supplement for existing RGB-frame-input models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 05:49:13 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Tao", "Li", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2001.05673", "submitter": "Hsu-Kuang Chiu", "authors": "Hsu-kuang Chiu, Antonio Prioletti, Jie Li, Jeannette Bohg", "title": "Probabilistic 3D Multi-Object Tracking for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D multi-object tracking is a key module in autonomous driving applications\nthat provides a reliable dynamic representation of the world to the planning\nmodule. In this paper, we present our on-line tracking method, which made the\nfirst place in the NuScenes Tracking Challenge, held at the AI Driving Olympics\nWorkshop at NeurIPS 2019. Our method estimates the object states by adopting a\nKalman Filter. We initialize the state covariance as well as the process and\nobservation noise covariance with statistics from the training set. We also use\nthe stochastic information from the Kalman Filter in the data association step\nby measuring the Mahalanobis distance between the predicted object states and\ncurrent object detections. Our experimental results on the NuScenes validation\nand test set show that our method outperforms the AB3DMOT baseline method by a\nlarge margin in the Average Multi-Object Tracking Accuracy (AMOTA) metric.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 06:38:02 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Chiu", "Hsu-kuang", ""], ["Prioletti", "Antonio", ""], ["Li", "Jie", ""], ["Bohg", "Jeannette", ""]]}, {"id": "2001.05691", "submitter": "Limin Wang", "authors": "Tianhao Li, Limin Wang", "title": "Learning Spatiotemporal Features via Video and Text Pair Discrimination", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current video representations heavily rely on learning from manually\nannotated video datasets which are time-consuming and expensive to acquire. We\nobserve videos are naturally accompanied by abundant text information such as\nYouTube titles and Instagram captions. In this paper, we leverage this\nvisual-textual connection to learn spatiotemporal features in an efficient\nweakly-supervised manner. We present a general cross-modal pair discrimination\n(CPD) framework to capture this correlation between a video and its associated\ntext. Specifically, we adopt noise-contrastive estimation to tackle the\ncomputational issue imposed by the huge amount of pair instance classes and\ndesign a practical curriculum learning strategy. We train our CPD models on\nboth standard video dataset (Kinetics-210k) and uncurated web video dataset\n(Instagram-300k) to demonstrate its effectiveness. Without further fine-tuning,\nthe learnt models obtain competitive results for action classification on\nKinetics under the linear classification protocol. Moreover, our visual model\nprovides an effective initialization to fine-tune on downstream tasks, which\nyields a remarkable performance gain for action recognition on UCF101 and\nHMDB51, compared with the existing state-of-the-art self-supervised training\nmethods. In addition, our CPD model yields a new state of the art for zero-shot\naction recognition on UCF101 by directly utilizing the learnt visual-textual\nembeddings. The code will be made available at\nhttps://github.com/MCG-NJU/CPD-Video.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 08:28:57 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 00:52:10 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 01:43:34 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Li", "Tianhao", ""], ["Wang", "Limin", ""]]}, {"id": "2001.05703", "submitter": "Linh K\\\"astner", "authors": "Linh K\\\"astner, Daniel Dimitrov, Jens Lambrecht", "title": "A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for\n  with Mobile Robots using RGB Data", "comments": "6 pages,5 figures. arXiv admin note: text overlap with\n  arXiv:1912.12101", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality has been subject to various integration efforts within\nindustries due to its ability to enhance human machine interaction and\nunderstanding. Neural networks have achieved remarkable results in areas of\ncomputer vision, which bear great potential to assist and facilitate an\nenhanced Augmented Reality experience. However, most neural networks are\ncomputationally intensive and demand huge processing power thus, are not\nsuitable for deployment on Augmented Reality devices. In this work we propose a\nmethod to deploy state of the art neural networks for real time 3D object\nlocalization on augmented reality devices. As a result, we provide a more\nautomated method of calibrating the AR devices with mobile robotic systems. To\naccelerate the calibration process and enhance user experience, we focus on\nfast 2D detection approaches which are extracting the 3D pose of the object\nfast and accurately by using only 2D input. The results are implemented into an\nAugmented Reality application for intuitive robot control and sensor data\nvisualization. For the 6D annotation of 2D images, we developed an annotation\ntool, which is, to our knowledge, the first open source tool to be available.\nWe achieve feasible results which are generally applicable to any AR device\nthus making this work promising for further research in combining high\ndemanding neural networks with Internet of Things devices.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 09:13:31 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["K\u00e4stner", "Linh", ""], ["Dimitrov", "Daniel", ""], ["Lambrecht", "Jens", ""]]}, {"id": "2001.05717", "submitter": "Ezgi Demircan-Tureyen", "authors": "Ezgi Demircan-Tureyen and Mustafa E. Kamasak", "title": "Adaptive Direction-Guided Structure Tensor Total Variation", "comments": "13 pages, 6 figures, article", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direction-guided structure tensor total variation (DSTV) is a recently\nproposed regularization term that aims at increasing the sensitivity of the\nstructure tensor total variation (STV) to the changes towards a predetermined\ndirection. Despite of the plausible results obtained on the uni-directional\nimages, the DSTV model is not applicable to the multi-directional images of\nreal-world. In this study, we build a two-stage framework that brings\nadaptivity to DSTV. We design an alternative to STV, which encodes the\nfirst-order information within a local neighborhood under the guidance of\nspatially varying directional descriptors (i.e., orientation and the dose of\nanisotropy). In order to estimate those descriptors, we propose an efficient\npreprocessor that captures the local geometry based on the structure tensor.\nThrough the extensive experiments, we demonstrate how beneficial the\ninvolvement of the directional information in STV is, by comparing the proposed\nmethod with the state-of-the-art analysis-based denoising models, both in terms\nof restoration quality and computational efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 09:49:29 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Demircan-Tureyen", "Ezgi", ""], ["Kamasak", "Mustafa E.", ""]]}, {"id": "2001.05744", "submitter": "Deng Yu", "authors": "Deng Yu, Lei Li, Youyi Zheng, Manfred Lau, Yi-Zhe Song, Chiew-Lan Tai,\n  Hongbo Fu", "title": "SketchDesc: Learning Local Sketch Descriptors for Multi-view\n  Correspondence", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of multi-view sketch correspondence,\nwhere we take as input multiple freehand sketches with different views of the\nsame object and predict as output the semantic correspondence among the\nsketches. This problem is challenging since the visual features of\ncorresponding points at different views can be very different. To this end, we\ntake a deep learning approach and learn a novel local sketch descriptor from\ndata. We contribute a training dataset by generating the pixel-level\ncorrespondence for the multi-view line drawings synthesized from 3D shapes. To\nhandle the sparsity and ambiguity of sketches, we design a novel multi-branch\nneural network that integrates a patch-based representation and a multi-scale\nstrategy to learn the pixel-level correspondence among multi-view sketches. We\ndemonstrate the effectiveness of our proposed approach with extensive\nexperiments on hand-drawn sketches and multi-view line drawings rendered from\nmultiple 3D shape datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 11:31:21 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 02:12:56 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 23:18:16 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Yu", "Deng", ""], ["Li", "Lei", ""], ["Zheng", "Youyi", ""], ["Lau", "Manfred", ""], ["Song", "Yi-Zhe", ""], ["Tai", "Chiew-Lan", ""], ["Fu", "Hongbo", ""]]}, {"id": "2001.05745", "submitter": "Ali Asadipour", "authors": "A. Asadipour, K. Debattista, V. Patel, A. Chalmers", "title": "A Technology-aided Multi-modal Training Approach to Assist Abdominal\n  Palpation Training and its Assessment in Medical Education", "comments": "In Press", "journal-ref": null, "doi": "10.1016/j.ijhcs.2020.102394", "report-no": null, "categories": "cs.HC cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer-assisted multimodal training is an effective way of learning complex\nmotor skills in various applications. In particular disciplines (eg.\nhealthcare) incompetency in performing dexterous hands-on examinations\n(clinical palpation) may result in misdiagnosis of symptoms, serious injuries\nor even death. Furthermore, a high quality clinical examination can help to\nexclude significant pathology, and reduce time and cost of diagnosis by\neliminating the need for unnecessary medical imaging. Medical palpation is used\nregularly as an effective preliminary diagnosis method all around the world but\nyears of training are required currently to achieve competency. This paper\nfocuses on a multimodal palpation training system to teach and improve clinical\nexamination skills in relation to the abdomen. It is our aim to shorten\nsignificantly the palpation training duration by increasing the frequency of\nrehearsals as well as providing essential augmented feedback on how to perform\nvarious abdominal palpation techniques which has been captured and modelled\nfrom medical experts. Twenty three first year medical students divided into a\ncontrol group (n=8), a semi-visually trained group (n=8), and a fully visually\ntrained group (n=7) were invited to perform three palpation tasks (superficial,\ndeep and liver). The medical students performances were assessed using both\ncomputer-based and human-based methods where a positive correlation was shown\nbetween the generated scores, r=.62, p(one-tailed)<.05. The visually-trained\ngroup significantly outperformed the control group in which abstract\nvisualisation of applied forces and their palmar locations were provided to the\nstudents during each palpation examination (p<.05). Moreover, a positive trend\nwas observed between groups when visual feedback was presented, J=132, z=2.62,\nr=0.55.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 11:31:46 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Asadipour", "A.", ""], ["Debattista", "K.", ""], ["Patel", "V.", ""], ["Chalmers", "A.", ""]]}, {"id": "2001.05752", "submitter": "Kentaro Wada", "authors": "Kentaro Wada, Kei Okada, Masayuki Inaba", "title": "Probabilistic 3D Multilabel Real-time Mapping for Multi-object\n  Manipulation", "comments": "8 pages, 8 figures, IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic 3D map has been applied to object segmentation with multiple\ncamera viewpoints, however, conventional methods lack of real-time efficiency\nand functionality of multilabel object mapping. In this paper, we propose a\nmethod to generate three-dimensional map with multilabel occupancy in\nreal-time. Extending our previous work in which only target label occupancy is\nmapped, we achieve multilabel object segmentation in a single looking around\naction. We evaluate our method by testing segmentation accuracy with 39\ndifferent objects, and applying it to a manipulation task of multiple objects\nin the experiments. Our mapping-based method outperforms the conventional\nprojection-based method by 40 - 96\\% relative (12.6 mean $IU_{3d}$), and robot\nsuccessfully recognizes (86.9\\%) and manipulates multiple objects (60.7\\%) in\nan environment with heavy occlusions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 12:00:20 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Wada", "Kentaro", ""], ["Okada", "Kei", ""], ["Inaba", "Masayuki", ""]]}, {"id": "2001.05755", "submitter": "Eden Belouadah", "authors": "Eden Belouadah and Adrian Popescu", "title": "ScaIL: Classifier Weights Scaling for Class Incremental Learning", "comments": "8 pages, 4 figures, 2 tables, accepted in WACV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental learning is useful if an AI agent needs to integrate data from a\nstream. The problem is non trivial if the agent runs on a limited computational\nbudget and has a bounded memory of past data. In a deep learning approach, the\nconstant computational budget requires the use of a fixed architecture for all\nincremental states. The bounded memory generates data imbalance in favor of new\nclasses and a prediction bias toward them appears. This bias is commonly\ncountered by introducing a data balancing step in addition to the basic network\ntraining. We depart from this approach and propose simple but efficient scaling\nof past class classifier weights to make them more comparable to those of new\nclasses. Scaling exploits incremental state level statistics and is applied to\nthe classifiers learned in the initial state of classes in order to profit from\nall their available data. We also question the utility of the widely used\ndistillation loss component of incremental learning algorithms by comparing it\nto vanilla fine tuning in presence of a bounded memory. Evaluation is done\nagainst competitive baselines using four public datasets. Results show that the\nclassifier weights scaling and the removal of the distillation are both\nbeneficial.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 12:10:45 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Belouadah", "Eden", ""], ["Popescu", "Adrian", ""]]}, {"id": "2001.05829", "submitter": "Shaoming Zheng", "authors": "Shaoming Zheng, Tianyang Zhang, Jiawei Zhuang, Hao Wang, Jiang Liu", "title": "A Two-Stream Meticulous Processing Network for Retinal Vessel\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vessel segmentation in fundus is a key diagnostic capability in\nophthalmology, and there are various challenges remained in this essential\ntask. Early approaches indicate that it is often difficult to obtain desirable\nsegmentation performance on thin vessels and boundary areas due to the\nimbalance of vessel pixels with different thickness levels. In this paper, we\npropose a novel two-stream Meticulous-Processing Network (MP-Net) for tackling\nthis problem. To pay more attention to the thin vessels and boundary areas, we\nfirstly propose an efficient hierarchical model automatically stratifies the\nground-truth masks into different thickness levels. Then a novel two-stream\nadversarial network is introduced to use the stratification results with a\nbalanced loss function and an integration operation to achieve a better\nperformance, especially in thin vessels and boundary areas detecting. Our model\nis proved to outperform state-of-the-art methods on DRIVE, STARE, and CHASE_DB1\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 17:06:10 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Zheng", "Shaoming", ""], ["Zhang", "Tianyang", ""], ["Zhuang", "Jiawei", ""], ["Wang", "Hao", ""], ["Liu", "Jiang", ""]]}, {"id": "2001.05833", "submitter": "Chong Wang", "authors": "Yi Zhang, Chong Wang, Ye Zheng, Jieyu Zhao, Yuqi Li and Xijiong Xie", "title": "Short-Term Temporal Convolutional Networks for Dynamic Hand Gesture\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of gesture recognition is to recognize meaningful movements of\nhuman bodies, and gesture recognition is an important issue in computer vision.\nIn this paper, we present a multimodal gesture recognition method based on 3D\ndensely convolutional networks (3D-DenseNets) and improved temporal\nconvolutional networks (TCNs). The key idea of our approach is to find a\ncompact and effective representation of spatial and temporal features, which\norderly and separately divide task of gesture video analysis into two parts:\nspatial analysis and temporal analysis. In spatial analysis, we adopt\n3D-DenseNets to learn short-term spatio-temporal features effectively.\nSubsequently, in temporal analysis, we use TCNs to extract temporal features\nand employ improved Squeeze-and-Excitation Networks (SENets) to strengthen the\nrepresentational power of temporal features from each TCNs' layers. The method\nhas been evaluated on the VIVA and the NVIDIA Gesture Dynamic Hand Gesture\nDatasets. Our approach obtains very competitive performance on VIVA benchmarks\nwith the classification accuracies of 91.54%, and achieve state-of-the art\nperformance with 86.37% accuracy on NVIDIA benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 23:30:27 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Zhang", "Yi", ""], ["Wang", "Chong", ""], ["Zheng", "Ye", ""], ["Zhao", "Jieyu", ""], ["Li", "Yuqi", ""], ["Xie", "Xijiong", ""]]}, {"id": "2001.05834", "submitter": "Georg Hille", "authors": "Georg Hille and Johannes Steffen and Max D\\\"unnwald and Mathias Becker\n  and Sylvia Saalfeld and Klaus T\\\"onnies", "title": "Spinal Metastases Segmentation in MR Imaging using Deep Convolutional\n  Neural Networks", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study's objective was to segment spinal metastases in diagnostic MR\nimages using a deep learning-based approach. Segmentation of such lesions can\npresent a pivotal step towards enhanced therapy planning and validation, as\nwell as intervention support during minimally invasive and image-guided\nsurgeries like radiofrequency ablations. For this purpose, we used a U-Net like\narchitecture trained with 40 clinical cases including both, lytic and sclerotic\nlesion types and various MR sequences. Our proposed method was evaluated with\nregards to various factors influencing the segmentation quality, e.g. the used\nMR sequences and the input dimension. We quantitatively assessed our\nexperiments using Dice coefficients, sensitivity and specificity rates.\nCompared to expertly annotated lesion segmentations, the experiments yielded\npromising results with average Dice scores up to 77.6% and mean sensitivity\nrates up to 78.9%. To our best knowledge, our proposed study is one of the\nfirst to tackle this particular issue, which limits direct comparability with\nrelated works. In respect to similar deep learning-based lesion segmentations,\ne.g. in liver MR images or spinal CT images, our experiments showed similar or\nin some respects superior segmentation quality. Overall, our automatic approach\ncan provide almost expert-like segmentation accuracy in this challenging and\nambitious task.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 10:59:31 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 10:21:08 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Hille", "Georg", ""], ["Steffen", "Johannes", ""], ["D\u00fcnnwald", "Max", ""], ["Becker", "Mathias", ""], ["Saalfeld", "Sylvia", ""], ["T\u00f6nnies", "Klaus", ""]]}, {"id": "2001.05835", "submitter": "Gilberto Luis De Conto Junior", "authors": "Gilberto Luis De Conto Junior", "title": "Diabetic Retinopathy detection by retinal image recognizing", "comments": "10 source codes, 12 figures, 48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people are affected by diabetes around the world. This disease may have\ntype 1 and 2. Diabetes brings with it several complications including diabetic\nretinopathy, which is a disease that if not treated correctly can lead to\nirreversible damage in the patient's vision. The earlier it is detected, the\nbetter the chances that the patient will not lose vision. Methods of automating\nmanual procedures are currently in evidence and the diagnostic process for\nretinopathy is manual with the physician analyzing the patient's retina on the\nmonitor. The practice of image recognition can aid this detection by\nrecognizing Diabetic Retinopathy patterns and comparing it with the patient's\nretina in diagnosis. This method can also assist in the act of telemedicine, in\nwhich people without access to the exam can benefit from the diagnosis provided\nby the application. The application development took place through\nconvolutional neural networks, which do digital image processing analyzing each\nimage pixel. The use of VGG-16 as a pre-trained model to the application basis\nwas very useful and the final model accuracy was 82%.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 16:36:59 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Junior", "Gilberto Luis De Conto", ""]]}, {"id": "2001.05837", "submitter": "German I. Parisi", "authors": "German I. Parisi", "title": "Human Action Recognition and Assessment via Deep Neural Network\n  Self-Organization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust recognition and assessment of human actions are crucial in\nhuman-robot interaction (HRI) domains. While state-of-the-art models of action\nperception show remarkable results in large-scale action datasets, they mostly\nlack the flexibility, robustness, and scalability needed to operate in natural\nHRI scenarios which require the continuous acquisition of sensory information\nas well as the classification or assessment of human body patterns in real\ntime. In this chapter, I introduce a set of hierarchical models for the\nlearning and recognition of actions from depth maps and RGB images through the\nuse of neural network self-organization. A particularity of these models is the\nuse of growing self-organizing networks that quickly adapt to non-stationary\ndistributions and implement dedicated mechanisms for continual learning from\ntemporally correlated input.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 15:58:06 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 16:56:09 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Parisi", "German I.", ""]]}, {"id": "2001.05838", "submitter": "Anandhanarayanan K", "authors": "Anandhanarayanan Kamalakannan, Shiva Shankar Ganesan and Govindaraj\n  Rajamanickam", "title": "Self-Learning AI Framework for Skin Lesion Image Segmentation and\n  Classification", "comments": "10 pages, 3 figures, 2 tables", "journal-ref": null, "doi": "10.5121/ijcsit.2019.11604", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation and classification are the two main fundamental steps in\npattern recognition. To perform medical image segmentation or classification\nwith deep learning models, it requires training on large image dataset with\nannotation. The dermoscopy images (ISIC archive) considered for this work does\nnot have ground truth information for lesion segmentation. Performing manual\nlabelling on this dataset is time-consuming. To overcome this issue,\nself-learning annotation scheme was proposed in the two-stage deep learning\nalgorithm. The two-stage deep learning algorithm consists of U-Net segmentation\nmodel with the annotation scheme and CNN classifier model. The annotation\nscheme uses a K-means clustering algorithm along with merging conditions to\nachieve initial labelling information for training the U-Net model. The\nclassifier models namely ResNet-50 and LeNet-5 were trained and tested on the\nimage dataset without segmentation for comparison and with the U-Net\nsegmentation for implementing the proposed self-learning Artificial\nIntelligence (AI) framework. The classification results of the proposed AI\nframework achieved training accuracy of 93.8% and testing accuracy of 82.42%\nwhen compared with the two classifier models directly trained on the input\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 09:31:11 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Kamalakannan", "Anandhanarayanan", ""], ["Ganesan", "Shiva Shankar", ""], ["Rajamanickam", "Govindaraj", ""]]}, {"id": "2001.05839", "submitter": "David Noever", "authors": "David Noever, Wes Regian, Matt Ciolino, Josh Kalin, Dom Hambrick, Kaye\n  Blankenship", "title": "Discoverability in Satellite Imagery: A Good Sentence is Worth a\n  Thousand Pictures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small satellite constellations provide daily global coverage of the earth's\nlandmass, but image enrichment relies on automating key tasks like change\ndetection or feature searches. For example, to extract text annotations from\nraw pixels requires two dependent machine learning models, one to analyze the\noverhead image and the other to generate a descriptive caption. We evaluate\nseven models on the previously largest benchmark for satellite image captions.\nWe extend the labeled image samples five-fold, then augment, correct and prune\nthe vocabulary to approach a rough min-max (minimum word, maximum description).\nThis outcome compares favorably to previous work with large pre-trained image\nmodels but offers a hundred-fold reduction in model size without sacrificing\noverall accuracy (when measured with log entropy loss). These smaller models\nprovide new deployment opportunities, particularly when pushed to edge\nprocessors, on-board satellites, or distributed ground stations. To quantify a\ncaption's descriptiveness, we introduce a novel multi-class confusion or error\nmatrix to score both human-labeled test data and never-labeled images that\ninclude bounding box detection but lack full sentence captions. This work\nsuggests future captioning strategies, particularly ones that can enrich the\nclass coverage beyond land use applications and that lessen color-centered and\nadjacency adjectives (\"green\", \"near\", \"between\", etc.). Many modern language\ntransformers present novel and exploitable models with world knowledge gleaned\nfrom training from their vast online corpus. One interesting, but easy example\nmight learn the word association between wind and waves, thus enriching a beach\nscene with more than just color descriptions that otherwise might be accessed\nfrom raw pixels without text annotation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 20:41:18 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Noever", "David", ""], ["Regian", "Wes", ""], ["Ciolino", "Matt", ""], ["Kalin", "Josh", ""], ["Hambrick", "Dom", ""], ["Blankenship", "Kaye", ""]]}, {"id": "2001.05840", "submitter": "Peng Gao", "authors": "Lei Shi, Shijie Geng, Kai Shuang, Chiori Hori, Songxiang Liu, Peng\n  Gao, Sen Su", "title": "Multi-Layer Content Interaction Through Quaternion Product For Visual\n  Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modality fusion technologies have greatly improved the performance of\nneural network-based Video Description/Caption, Visual Question Answering (VQA)\nand Audio Visual Scene-aware Dialog (AVSD) over the recent years. Most previous\napproaches only explore the last layers of multiple layer feature fusion while\nomitting the importance of intermediate layers. To solve the issue for the\nintermediate layers, we propose an efficient Quaternion Block Network (QBN) to\nlearn interaction not only for the last layer but also for all intermediate\nlayers simultaneously. In our proposed QBN, we use the holistic text features\nto guide the update of visual features. In the meantime, Hamilton quaternion\nproducts can efficiently perform information flow from higher layers to lower\nlayers for both visual and text modalities. The evaluation results show our QBN\nimproved the performance on VQA 2.0, even though using surpass large scale BERT\nor visual BERT pre-trained models. Extensive ablation study has been carried\nout to testify the influence of each proposed module in this study.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 02:25:18 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 07:25:24 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Shi", "Lei", ""], ["Geng", "Shijie", ""], ["Shuang", "Kai", ""], ["Hori", "Chiori", ""], ["Liu", "Songxiang", ""], ["Gao", "Peng", ""], ["Su", "Sen", ""]]}, {"id": "2001.05841", "submitter": "Georgin Jacob", "authors": "Georgin Jacob, Harish Katti", "title": "Predicting population neural activity in the Algonauts challenge using\n  end-to-end trained Siamese networks and group convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Algonauts challenge is about predicting the object representations in the\nform of Representational Dissimilarity Matrices (RDMS) derived from visual\nbrain regions. We used a customized deep learning model using the concept of\nSiamese networks and group convolutions to predict neural distances\ncorresponding to a pair of images. Training data was best explained by\ndistances computed over the last layer.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 08:36:10 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Jacob", "Georgin", ""], ["Katti", "Harish", ""]]}, {"id": "2001.05842", "submitter": "Mohammad Hadi Kefayati", "authors": "Mohammad Hadi Kefayati, Vahid Pourahmadi and Hassan Aghaeinia", "title": "Wi2Vi: Generating Video Frames from WiFi CSI Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects in an environment affect electromagnetic waves. While this effect\nvaries across frequencies, there exists a correlation between them, and a model\nwith enough capacity can capture this correlation between the measurements in\ndifferent frequencies. In this paper, we propose the Wi2Vi model for\nassociating variations in the WiFi channel state information with video frames.\nThe proposed Wi2Vi system can generate video frames entirely using CSI\nmeasurements. The produced video frames by the Wi2Vi provide auxiliary\ninformation to the conventional surveillance system in critical circumstances.\nOur implementation of the Wi2Vi system confirms the feasibility of constructing\na system capable of deriving the correlations between measurements in different\nfrequency spectrums.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 13:36:37 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Kefayati", "Mohammad Hadi", ""], ["Pourahmadi", "Vahid", ""], ["Aghaeinia", "Hassan", ""]]}, {"id": "2001.05843", "submitter": "Yoav Chai", "authors": "Yoav Chai, Raja Giryes, Lior Wolf", "title": "Supervised and Unsupervised Learning of Parameterized Color Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We treat the problem of color enhancement as an image translation task, which\nwe tackle using both supervised and unsupervised learning. Unlike traditional\nimage to image generators, our translation is performed using a global\nparameterized color transformation instead of learning to directly map image\ninformation. In the supervised case, every training image is paired with a\ndesired target image and a convolutional neural network (CNN) learns from the\nexpert retouched images the parameters of the transformation. In the unpaired\ncase, we employ two-way generative adversarial networks (GANs) to learn these\nparameters and apply a circularity constraint. We achieve state-of-the-art\nresults compared to both supervised (paired data) and unsupervised (unpaired\ndata) image enhancement methods on the MIT-Adobe FiveK benchmark. Moreover, we\nshow the generalization capability of our method, by applying it on photos from\nthe early 20th century and to dark video frames.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 13:57:06 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Chai", "Yoav", ""], ["Giryes", "Raja", ""], ["Wolf", "Lior", ""]]}, {"id": "2001.05844", "submitter": "Satoshi Ono", "authors": "Takahiro Suzuki, Shingo Takeshita, Satoshi Ono", "title": "Adversarial Example Generation using Evolutionary Multi-objective\n  Optimization", "comments": null, "journal-ref": "2019 IEEE Congress on Evolutionary Computation (CEC), Wellington,\n  New Zealand, 2019, pp. 2136-2144", "doi": "10.1109/CEC.2019.8790123", "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Evolutionary Multi-objective Optimization (EMO)-based\nAdversarial Example (AE) design method that performs under black-box setting.\nPrevious gradient-based methods produce AEs by changing all pixels of a target\nimage, while previous EC-based method changes small number of pixels to produce\nAEs. Thanks to EMO's property of population based-search, the proposed method\nproduces various types of AEs involving ones locating between AEs generated by\nthe previous two approaches, which helps to know the characteristics of a\ntarget model or to know unknown attack patterns. Experimental results showed\nthe potential of the proposed method, e.g., it can generate robust AEs and,\nwith the aid of DCT-based perturbation pattern generation, AEs for high\nresolution images.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 07:34:09 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Suzuki", "Takahiro", ""], ["Takeshita", "Shingo", ""], ["Ono", "Satoshi", ""]]}, {"id": "2001.05845", "submitter": "Sara Mousavi", "authors": "Sara Mousavi, Dylan Lee, Tatianna Griffin, Dawnie Steadman and Audris\n  Mockus", "title": "An Analytical Workflow for Clustering Forensic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large collections of images, if curated, drastically contribute to the\nquality of research in many domains. Unsupervised clustering is an intuitive,\nyet effective step towards curating such datasets. In this work, we present a\nworkflow for unsupervisedly clustering a large collection of forensic images.\nThe workflow utilizes classic clustering on deep feature representation of the\nimages in addition to domain-related data to group them together. Our manual\nevaluation shows a purity of 89\\% for the resulted clusters.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 18:20:07 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Mousavi", "Sara", ""], ["Lee", "Dylan", ""], ["Griffin", "Tatianna", ""], ["Steadman", "Dawnie", ""], ["Mockus", "Audris", ""]]}, {"id": "2001.05846", "submitter": "Hongxin Wang", "authors": "Hongxin Wang, Huatian Wang, Jiannan Zhao, Cheng Hu, Jigen Peng and\n  Shigang Yue", "title": "A Time-Delay Feedback Neural Network for Discriminating Small,\n  Fast-Moving Targets in Complex Dynamic Environments", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discriminating small moving objects within complex visual environments is a\nsignificant challenge for autonomous micro robots that are generally limited in\ncomputational power. By exploiting their highly evolved visual systems, flying\ninsects can effectively detect mates and track prey during rapid pursuits, even\nthough the small targets equate to only a few pixels in their visual field. The\nhigh degree of sensitivity to small target movement is supported by a class of\nspecialized neurons called small target motion detectors (STMDs). Existing\nSTMD-based computational models normally comprise four sequentially arranged\nneural layers interconnected via feedforward loops to extract information on\nsmall target motion from raw visual inputs. However, feedback, another\nimportant regulatory circuit for motion perception, has not been investigated\nin the STMD pathway and its functional roles for small target motion detection\nare not clear. In this paper, we propose an STMD-based neural network with\nfeedback connection (Feedback STMD), where the network output is temporally\ndelayed, then fed back to the lower layers to mediate neural responses. We\ncompare the properties of the model with and without the time-delay feedback\nloop, and find it shows preference for high-velocity objects. Extensive\nexperiments suggest that the Feedback STMD achieves superior detection\nperformance for fast-moving small targets, while significantly suppressing\nbackground false positive movements which display lower velocities. The\nproposed feedback model provides an effective solution in robotic visual\nsystems for detecting fast-moving small targets that are always salient and\npotentially threatening.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 03:10:36 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 03:19:12 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 02:29:13 GMT"}, {"version": "v4", "created": "Tue, 19 May 2020 00:30:03 GMT"}, {"version": "v5", "created": "Mon, 28 Jun 2021 02:02:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Hongxin", ""], ["Wang", "Huatian", ""], ["Zhao", "Jiannan", ""], ["Hu", "Cheng", ""], ["Peng", "Jigen", ""], ["Yue", "Shigang", ""]]}, {"id": "2001.05847", "submitter": "Pablo Lanillos", "authors": "Cansu Sancaktar, Marcel van Gerven, Pablo Lanillos", "title": "End-to-End Pixel-Based Deep Active Inference for Body Perception and\n  Action", "comments": null, "journal-ref": null, "doi": "10.1109/ICDL-EpiRob48136.2020.9278105", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a pixel-based deep active inference algorithm (PixelAI) inspired\nby human body perception and action. Our algorithm combines the free-energy\nprinciple from neuroscience, rooted in variational inference, with deep\nconvolutional decoders to scale the algorithm to directly deal with raw visual\ninput and provide online adaptive inference. Our approach is validated by\nstudying body perception and action in a simulated and a real Nao robot.\nResults show that our approach allows the robot to perform 1) dynamical body\nestimation of its arm using only monocular camera images and 2) autonomous\nreaching to \"imagined\" arm poses in the visual space. This suggests that robot\nand human body perception and action can be efficiently solved by viewing both\nas an active inference problem guided by ongoing sensory input.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 12:19:09 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 14:15:16 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 21:30:12 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Sancaktar", "Cansu", ""], ["van Gerven", "Marcel", ""], ["Lanillos", "Pablo", ""]]}, {"id": "2001.05848", "submitter": "Xiao Huang", "authors": "Xiao Huang, Dong Xu, Zhenlong Li, Cuizhen Wang", "title": "Translating multispectral imagery to nighttime imagery via conditional\n  generative adversarial networks", "comments": "4 pages, 3 figures, submitted to the 2020 IEEE International\n  Geoscience and Remote Sensing Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nighttime satellite imagery has been applied in a wide range of fields.\nHowever, our limited understanding of how observed light intensity is formed\nand whether it can be simulated greatly hinders its further application. This\nstudy explores the potential of conditional Generative Adversarial Networks\n(cGAN) in translating multispectral imagery to nighttime imagery. A popular\ncGAN framework, pix2pix, was adopted and modified to facilitate this\ntranslation using gridded training image pairs derived from Landsat 8 and\nVisible Infrared Imaging Radiometer Suite (VIIRS). The results of this study\nprove the possibility of multispectral-to-nighttime translation and further\nindicate that, with the additional social media data, the generated nighttime\nimagery can be very similar to the ground-truth imagery. This study fills the\ngap in understanding the composition of satellite observed nighttime light and\nprovides new paradigms to solve the emerging problems in nighttime remote\nsensing fields, including nighttime series construction, light desaturation,\nand multi-sensor calibration.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 03:20:29 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Huang", "Xiao", ""], ["Xu", "Dong", ""], ["Li", "Zhenlong", ""], ["Wang", "Cuizhen", ""]]}, {"id": "2001.05849", "submitter": "Zohreh Shaghaghian", "authors": "Zohreh Shaghaghian, Wei Yan", "title": "Application of Deep Learning in Generating Desired Design Options:\n  Experiments Using Synthetic Training Dataset", "comments": "10 pages, 12 figures, 1 table", "journal-ref": "Proceedings of the 2020 Building Performance Analysis Conference\n  and SimBuild. 2020 535-544", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most design methods contain a forward framework, asking for primary\nspecifications of a building to generate an output or assess its performance.\nHowever, architects urge for specific objectives though uncertain of the proper\ndesign parameters. Deep Learning (DL) algorithms provide an intelligent\nworkflow in which the system can learn from sequential training experiments.\nThis study applies a method using DL algorithms towards generating demanded\ndesign options. In this study, an object recognition problem is investigated to\ninitially predict the label of unseen sample images based on training dataset\nconsisting of different types of synthetic 2D shapes; later, a generative DL\nalgorithm is applied to be trained and generate new shapes for given labels. In\nthe next step, the algorithm is trained to generate a window/wall pattern for\ndesired light/shadow performance based on the spatial daylight autonomy (sDA)\nmetrics. The experiments show promising results both in predicting unseen\nsample shapes and generating new design options.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 01:26:20 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 19:33:48 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Shaghaghian", "Zohreh", ""], ["Yan", "Wei", ""]]}, {"id": "2001.05851", "submitter": "Alberto Rossi", "authors": "Alberto Rossi, Markus Hagenbuchner, Franco Scarselli, Ah Chung Tsoi", "title": "Embedding of FRPN in CNN architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the fully recursive perceptron network (FRPN) model for\nvectorial inputs to include deep convolutional neural networks (CNNs) which can\naccept multi-dimensional inputs. A FRPN consists of a recursive layer, which,\ngiven a fixed input, iteratively computes an equilibrium state. The unfolding\nrealized with this kind of iterative mechanism allows to simulate a deep neural\nnetwork with any number of layers. The extension of the FRPN to CNN results in\nan architecture, which we call convolutional-FRPN (C-FRPN), where the\nconvolutional layers are recursive. The method is evaluated on several image\nclassification benchmarks. It is shown that the C-FRPN consistently outperforms\nstandard CNNs having the same number of parameters. The gap in performance is\nparticularly large for small networks, showing that the C-FRPN is a very\npowerful architecture, since it allows to obtain equivalent performance with\nfewer parameters when compared with deep CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 12:41:15 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Rossi", "Alberto", ""], ["Hagenbuchner", "Markus", ""], ["Scarselli", "Franco", ""], ["Tsoi", "Ah Chung", ""]]}, {"id": "2001.05852", "submitter": "Mingxin Zhao", "authors": "Mingxin Zhao, Li Cheng, Xu Yang, Peng Feng, Liyuan Liu, and Nanjian Wu", "title": "TBC-Net: A real-time detector for infrared small target detection using\n  semantic constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared small target detection is a key technique in infrared search and\ntracking (IRST) systems. Although deep learning has been widely used in the\nvision tasks of visible light images recently, it is rarely used in infrared\nsmall target detection due to the difficulty in learning small target features.\nIn this paper, we propose a novel lightweight convolutional neural network\nTBC-Net for infrared small target detection. The TBCNet consists of a target\nextraction module (TEM) and a semantic constraint module (SCM), which are used\nto extract small targets from infrared images and to classify the extracted\ntarget images during the training, respectively. Meanwhile, we propose a joint\nloss function and a training method. The SCM imposes a semantic constraint on\nTEM by combining the high-level classification task and solve the problem of\nthe difficulty to learn features caused by class imbalance problem. During the\ntraining, the targets are extracted from the input image and then be classified\nby SCM. During the inference, only the TEM is used to detect the small targets.\nWe also propose a data synthesis method to generate training data. The\nexperimental results show that compared with the traditional methods, TBC-Net\ncan better reduce the false alarm caused by complicated background, the\nproposed network structure and joint loss have a significant improvement on\nsmall target feature learning. Besides, TBC-Net can achieve real-time detection\non the NVIDIA Jetson AGX Xavier development board, which is suitable for\napplications such as field research with drones equipped with infrared sensors.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 05:25:39 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Zhao", "Mingxin", ""], ["Cheng", "Li", ""], ["Yang", "Xu", ""], ["Feng", "Peng", ""], ["Liu", "Liyuan", ""], ["Wu", "Nanjian", ""]]}, {"id": "2001.05853", "submitter": "Mark Rowan", "authors": "Nataliya Le Vine, Claus Horn, Matthew Zeigenfuse, Mark Rowan", "title": "Identifying Table Structure in Documents using Conditional Generative\n  Adversarial Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.01947", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many industries, as well as in academic research, information is primarily\ntransmitted in the form of unstructured documents (this article, for example).\nHierarchically-related data is rendered as tables, and extracting information\nfrom tables in such documents presents a significant challenge. Many existing\nmethods take a bottom-up approach, first integrating lines into cells, then\ncells into rows or columns, and finally inferring a structure from the\nresulting 2-D layout. But such approaches neglect the available prior\ninformation relating to table structure, namely that the table is merely an\narbitrary representation of a latent logical structure. We propose a top-down\napproach, first using a conditional generative adversarial network to map a\ntable image into a standardised `skeleton' table form denoting approximate row\nand column borders without table content, then deriving latent table structure\nusing xy-cut projection and Genetic Algorithm optimisation. The approach is\neasily adaptable to different table configurations and requires small data set\nsizes for training.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 20:42:40 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Vine", "Nataliya Le", ""], ["Horn", "Claus", ""], ["Zeigenfuse", "Matthew", ""], ["Rowan", "Mark", ""]]}, {"id": "2001.05855", "submitter": "Jacob Decoto", "authors": "Jacob J Decoto, David RC Dayton", "title": "Deep Learning Enabled Uncorrelated Space Observation Association", "comments": "Approved for public release by Department of Defense Prepublication\n  Office, Ref: 20-S-0428", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncorrelated optical space observation association represents a classic\nneedle in a haystack problem. The objective being to find small groups of\nobservations that are likely of the same resident space objects (RSOs) from\namongst the much larger population of all uncorrelated observations. These\nobservations being potentially widely disparate both temporally and with\nrespect to the observing sensor position. By training on a large representative\ndata set this paper shows that a deep learning enabled learned model with no\nencoded knowledge of physics or orbital mechanics can learn a model for\nidentifying observations of common objects. When presented with balanced input\nsets of 50% matching observation pairs the learned model was able to correctly\nidentify if the observation pairs were of the same RSO 83.1% of the time. The\nresulting learned model is then used in conjunction with a search algorithm on\nan unbalanced demonstration set of 1,000 disparate simulated uncorrelated\nobservations and is shown to be able to successfully identify true three\nobservation sets representing 111 out of 142 objects in the population. With\nmost objects being identified in multiple three observation triplets. This is\naccomplished while only exploring 0.06% of the search space of 1.66e8 possible\nunique triplet combinations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 23:33:11 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Decoto", "Jacob J", ""], ["Dayton", "David RC", ""]]}, {"id": "2001.05856", "submitter": "Mohit Vohra", "authors": "Siddhartha Vibhu Pharswan, Mohit Vohra, Ashish Kumar, and Laxmidhar\n  Behera", "title": "Domain Independent Unsupervised Learning to grasp the Novel Objects", "comments": "Paper has been accepted for publication in IROS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in the vision-based grasping is the selection of\nfeasible grasp regions while interacting with novel objects. Recent approaches\nexploit the power of the convolutional neural network (CNN) to achieve accurate\ngrasping at the cost of high computational power and time. In this paper, we\npresent a novel unsupervised learning based algorithm for the selection of\nfeasible grasp regions. Unsupervised learning infers the pattern in data-set\nwithout any external labels. We apply k-means clustering on the image plane to\nidentify the grasp regions, followed by an axis assignment method. We define a\nnovel concept of Grasp Decide Index (GDI) to select the best grasp pose in\nimage plane. We have conducted several experiments in clutter or isolated\nenvironment on standard objects of Amazon Robotics Challenge 2017 and Amazon\nPicking Challenge 2016. We compare the results with prior learning based\napproaches to validate the robustness and adaptive nature of our algorithm for\na variety of novel objects in different domains.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 12:05:37 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Pharswan", "Siddhartha Vibhu", ""], ["Vohra", "Mohit", ""], ["Kumar", "Ashish", ""], ["Behera", "Laxmidhar", ""]]}, {"id": "2001.05857", "submitter": "Aysu Can", "authors": "Ethem F. Can, Aysu Ezen-Can", "title": "The Effect of Data Ordering in Image Classification", "comments": null, "journal-ref": "Under consideration at Pattern Recognition Letters 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success stories from deep learning models increase every day spanning\ndifferent tasks from image classification to natural language understanding.\nWith the increasing popularity of these models, scientists spend more and more\ntime finding the optimal parameters and best model architectures for their\ntasks. In this paper, we focus on the ingredient that feeds these machines: the\ndata. We hypothesize that the data ordering affects how well a model performs.\nTo that end, we conduct experiments on an image classification task using\nImageNet dataset and show that some data orderings are better than others in\nterms of obtaining higher classification accuracies. Experimental results show\nthat independent of model architecture, learning rate and batch size, ordering\nof the data significantly affects the outcome. We show these findings using\ndifferent metrics: NDCG, accuracy @ 1 and accuracy @ 5. Our goal here is to\nshow that not only parameters and model architectures but also the data\nordering has a say in obtaining better results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 20:34:00 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Can", "Ethem F.", ""], ["Ezen-Can", "Aysu", ""]]}, {"id": "2001.05858", "submitter": "Ylva Jansson", "authors": "Lukas Finnveden, Ylva Jansson, Tony Lindeberg", "title": "The problems with using STNs to align CNN feature maps", "comments": "Accepted to Northern Lights Deep Learning Workshop 2020, Troms{\\o}, 2\n  pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial transformer networks (STNs) were designed to enable CNNs to learn\ninvariance to image transformations. STNs were originally proposed to transform\nCNN feature maps as well as input images. This enables the use of more complex\nfeatures when predicting transformation parameters. However, since STNs perform\na purely spatial transformation, they do not, in the general case, have the\nability to align the feature maps of a transformed image and its original. We\npresent a theoretical argument for this and investigate the practical\nimplications, showing that this inability is coupled with decreased\nclassification accuracy. We advocate taking advantage of more complex features\nin deeper layers by instead sharing parameters between the classification and\nthe localisation network.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 12:59:56 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Finnveden", "Lukas", ""], ["Jansson", "Ylva", ""], ["Lindeberg", "Tony", ""]]}, {"id": "2001.05859", "submitter": "Yoshiro Suzuki", "authors": "Ayaka Suzuki, Yoshiro Suzuki", "title": "Deep learning achieves perfect anomaly detection on 108,308 retinal\n  images including unlearned diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) scanning is useful in detecting various\nretinal diseases. However, there are not enough ophthalmologists who can\ndiagnose retinal OCT images in much of the world. To provide OCT screening\ninexpensively and extensively, an automated diagnosis system is indispensable.\nAlthough many machine learning techniques have been presented for assisting\nophthalmologists in diagnosing retinal OCT images, there is no technique that\ncan diagnose independently without relying on an ophthalmologist, i.e., there\nis no technique that does not overlook any anomaly, including unlearned\ndiseases. As long as there is a risk of overlooking a disease with a technique,\nophthalmologists must double-check even those images that the technique\nclassifies as normal. Here, we show that our deep-learning-based binary\nclassifier (normal or abnormal) achieved a perfect classification on 108,308\ntwo-dimensional retinal OCT images, i.e., true positive rate = 1.000000 and\ntrue negative rate = 1.000000; hence, the area under the ROC curve = 1.0000000.\nAlthough the test set included three types of diseases, two of these were not\nused for training. However, all test images were correctly classified.\nFurthermore, we demonstrated that our scheme was able to cope with differences\nin patient race. No conventional approach has achieved the above performances.\nOur work has a sufficient possibility of raising automated diagnosis techniques\nfor retinal OCT images from \"assistant for ophthalmologists\" to \"independent\ndiagnosis system without ophthalmologists\".\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 08:32:47 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 14:37:45 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 08:11:20 GMT"}, {"version": "v4", "created": "Sat, 18 Apr 2020 08:10:06 GMT"}, {"version": "v5", "created": "Sun, 19 Jul 2020 13:38:07 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Suzuki", "Ayaka", ""], ["Suzuki", "Yoshiro", ""]]}, {"id": "2001.05862", "submitter": "Siming Bayer", "authors": "Siming Bayer, Ute Spiske, Jie Luo, Tobias Geimer, William M. Wells\n  III, Martin Ostermeier, Rebecca Fahrig, Arya Nabavi, Christoph Bert, Ilker\n  Eyupoglo, and Andreas Maier", "title": "An Investigation of Feature-based Nonrigid Image Registration using\n  Gaussian Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a wide range of clinical applications, such as adaptive treatment\nplanning or intraoperative image update, feature-based deformable registration\n(FDR) approaches are widely employed because of their simplicity and low\ncomputational complexity. FDR algorithms estimate a dense displacement field by\ninterpolating a sparse field, which is given by the established correspondence\nbetween selected features. In this paper, we consider the deformation field as\na Gaussian Process (GP), whereas the selected features are regarded as prior\ninformation on the valid deformations. Using GP, we are able to estimate the\nboth dense displacement field and a corresponding uncertainty map at once.\nFurthermore, we evaluated the performance of different hyperparameter settings\nfor squared exponential kernels with synthetic, phantom and clinical data\nrespectively. The quantitative comparison shows, GP-based interpolation has\nperformance on par with state-of-the-art B-spline interpolation. The greatest\nclinical benefit of GP-based interpolation is that it gives a reliable estimate\nof the mathematical uncertainty of the calculated dense displacement map.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 20:51:41 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Bayer", "Siming", ""], ["Spiske", "Ute", ""], ["Luo", "Jie", ""], ["Geimer", "Tobias", ""], ["Wells", "William M.", "III"], ["Ostermeier", "Martin", ""], ["Fahrig", "Rebecca", ""], ["Nabavi", "Arya", ""], ["Bert", "Christoph", ""], ["Eyupoglo", "Ilker", ""], ["Maier", "Andreas", ""]]}, {"id": "2001.05864", "submitter": "Yiyan Chen", "authors": "Yiyan Chen, Li Tao, Xueting Wang and Toshihiko Yamasaki", "title": "Weakly Supervised Video Summarization by Hierarchical Reinforcement\n  Learning", "comments": "mmasia 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional video summarization approaches based on reinforcement learning\nhave the problem that the reward can only be received after the whole summary\nis generated. Such kind of reward is sparse and it makes reinforcement learning\nhard to converge. Another problem is that labelling each frame is tedious and\ncostly, which usually prohibits the construction of large-scale datasets. To\nsolve these problems, we propose a weakly supervised hierarchical reinforcement\nlearning framework, which decomposes the whole task into several subtasks to\nenhance the summarization quality. This framework consists of a manager network\nand a worker network. For each subtask, the manager is trained to set a subgoal\nonly by a task-level binary label, which requires much fewer labels than\nconventional approaches. With the guide of the subgoal, the worker predicts the\nimportance scores for video frames in the subtask by policy gradient according\nto both global reward and innovative defined sub-rewards to overcome the sparse\nproblem. Experiments on two benchmark datasets show that our proposal has\nachieved the best performance, even better than supervised approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 07:47:02 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 15:31:24 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Yiyan", ""], ["Tao", "Li", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2001.05865", "submitter": "Raghav Goyal", "authors": "Shubham Agarwal, Raghav Goyal", "title": "Ensemble based discriminative models for Visual Dialog Challenge 2018", "comments": "Rankings: https://visualdialog.org/challenge/2018#winners", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This manuscript describes our approach for the Visual Dialog Challenge 2018.\nWe use an ensemble of three discriminative models with different encoders and\ndecoders for our final submission. Our best performing model on 'test-std'\nsplit achieves the NDCG score of 55.46 and the MRR value of 63.77, securing\nthird position in the challenge.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 08:20:54 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Agarwal", "Shubham", ""], ["Goyal", "Raghav", ""]]}, {"id": "2001.05868", "submitter": "Hao Cheng", "authors": "Fanxu Meng, Hao Cheng, Ke Li, Zhixin Xu, Rongrong Ji, Xing Sun,\n  Gaungming Lu", "title": "Filter Grafting for Deep Neural Networks", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new learning paradigm called filter grafting, which\naims to improve the representation capability of Deep Neural Networks (DNNs).\nThe motivation is that DNNs have unimportant (invalid) filters (e.g., l1 norm\nclose to 0). These filters limit the potential of DNNs since they are\nidentified as having little effect on the network. While filter pruning removes\nthese invalid filters for efficiency consideration, filter grafting\nre-activates them from an accuracy boosting perspective. The activation is\nprocessed by grafting external information (weights) into invalid filters. To\nbetter perform the grafting process, we develop an entropy-based criterion to\nmeasure the information of filters and an adaptive weighting strategy for\nbalancing the grafted information among networks. After the grafting operation,\nthe network has very few invalid filters compared with its untouched state,\nenpowering the model with more representation capacity. We also perform\nextensive experiments on the classification and recognition tasks to show the\nsuperiority of our method. For example, the grafted MobileNetV2 outperforms the\nnon-grafted MobileNetV2 by about 7 percent on CIFAR-100 dataset. Code is\navailable at https://github.com/fxmeng/filter-grafting.git.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 03:18:57 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 11:19:24 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 13:42:25 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Meng", "Fanxu", ""], ["Cheng", "Hao", ""], ["Li", "Ke", ""], ["Xu", "Zhixin", ""], ["Ji", "Rongrong", ""], ["Sun", "Xing", ""], ["Lu", "Gaungming", ""]]}, {"id": "2001.05870", "submitter": "Amir Erfan Eshratifar", "authors": "Amir Erfan Eshratifar and Massoud Pedram", "title": "Runtime Deep Model Multiplexing for Reduced Latency and Energy\n  Consumption Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning algorithm to design a light-weight neural multiplexer\nthat given the input and computational resource requirements, calls the model\nthat will consume the minimum compute resources for a successful inference.\nMobile devices can use the proposed algorithm to offload the hard inputs to the\ncloud while inferring the easy ones locally. Besides, in the large scale\ncloud-based intelligent applications, instead of replicating the most-accurate\nmodel, a range of small and large models can be multiplexed from depending on\nthe input's complexity which will save the cloud's computational resources. The\ninput complexity or hardness is determined by the number of models that can\npredict the correct label. For example, if no model can predict the label\ncorrectly, then the input is considered as the hardest. The proposed algorithm\nallows the mobile device to detect the inputs that can be processed locally and\nthe ones that require a larger model and should be sent a cloud server.\nTherefore, the mobile user benefits from not only the local processing but also\nfrom an accurate model hosted on a cloud server. Our experimental results show\nthat the proposed algorithm improves mobile's model accuracy by 8.52% which is\nbecause of those inputs that are properly selected and offloaded to the cloud\nserver. In addition, it saves the cloud providers' compute resources by a\nfactor of 2.85x as small models are chosen for easier inputs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 23:49:51 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 17:07:31 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Eshratifar", "Amir Erfan", ""], ["Pedram", "Massoud", ""]]}, {"id": "2001.05873", "submitter": "Harshitha Machiraju", "authors": "Harshitha Machiraju, Vineeth N Balasubramanian", "title": "A Little Fog for a Large Turn", "comments": "Accepted to WACV 2020", "journal-ref": null, "doi": "10.1109/WACV45572.2020.9093549", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small, carefully crafted perturbations called adversarial perturbations can\neasily fool neural networks. However, these perturbations are largely additive\nand not naturally found. We turn our attention to the field of Autonomous\nnavigation wherein adverse weather conditions such as fog have a drastic effect\non the predictions of these systems. These weather conditions are capable of\nacting like natural adversaries that can help in testing models. To this end,\nwe introduce a general notion of adversarial perturbations, which can be\ncreated using generative models and provide a methodology inspired by\nCycle-Consistent Generative Adversarial Networks to generate adversarial\nweather conditions for a given image. Our formulation and results show that\nthese images provide a suitable testbed for steering models used in Autonomous\nnavigation models. Our work also presents a more natural and general definition\nof Adversarial perturbations based on Perceptual Similarity.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 15:09:48 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Machiraju", "Harshitha", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2001.05876", "submitter": "Li Wang", "authors": "Li Wang, Zechen Bai, Yonghua Zhang, Hongtao Lu", "title": "Show, Recall, and Tell: Image Captioning with Recall Mechanism", "comments": "Published in AAAI 2020", "journal-ref": null, "doi": "10.1609/aaai.v34i07.6898", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating natural and accurate descriptions in image cap-tioning has always\nbeen a challenge. In this paper, we pro-pose a novel recall mechanism to\nimitate the way human con-duct captioning. There are three parts in our recall\nmecha-nism : recall unit, semantic guide (SG) and recalled-wordslot (RWS).\nRecall unit is a text-retrieval module designedto retrieve recalled words for\nimages. SG and RWS are de-signed for the best use of recalled words. SG branch\ncangenerate a recalled context, which can guide the process ofgenerating\ncaption. RWS branch is responsible for copyingrecalled words to the caption.\nInspired by pointing mecha-nism in text summarization, we adopt a soft switch\nto balancethe generated-word probabilities between SG and RWS. Inthe CIDEr\noptimization step, we also introduce an individualrecalled-word reward (WR) to\nboost training. Our proposedmethods (SG+RWS+WR) achieve BLEU-4 / CIDEr /\nSPICEscores of 36.6 / 116.9 / 21.3 with cross-entropy loss and 38.7 /129.1 /\n22.4 with CIDEr optimization on MSCOCO Karpathytest split, which surpass the\nresults of other state-of-the-artmethods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 16:32:51 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 13:09:06 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 05:00:56 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wang", "Li", ""], ["Bai", "Zechen", ""], ["Zhang", "Yonghua", ""], ["Lu", "Hongtao", ""]]}, {"id": "2001.05878", "submitter": "Sourav Mishra", "authors": "Sourav Mishra, Subhajit Chaudhury, Hideaki Imaizumi, Toshihiko\n  Yamasaki", "title": "Assessing Robustness of Deep learning Methods in Dermatological Workflow", "comments": "Accepted in ACM CHIL 2020 Workshop (Oral and poster, without\n  publication)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to evaluate the suitability of current deep learning methods\nfor clinical workflow especially by focusing on dermatology. Although deep\nlearning methods have been attempted to get dermatologist level accuracy in\nseveral individual conditions, it has not been rigorously tested for common\nclinical complaints. Most projects involve data acquired in well-controlled\nlaboratory conditions. This may not reflect regular clinical evaluation where\ncorresponding image quality is not always ideal. We test the robustness of deep\nlearning methods by simulating non-ideal characteristics on user submitted\nimages of ten classes of diseases. Assessing via imitated conditions, we have\nfound the overall accuracy to drop and individual predictions change\nsignificantly in many cases despite of robust training.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 14:15:38 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 13:51:35 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Mishra", "Sourav", ""], ["Chaudhury", "Subhajit", ""], ["Imaizumi", "Hideaki", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2001.05887", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu, Xudong Li, Shun Lu, Bo Zhang, and Jixiang Li", "title": "MixPath: A Unified Approach for One-shot Neural Architecture Search", "comments": "Bridge the gap between one shot NAS and multi branch using shadow BN\n  with good ranking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blending multiple convolutional kernels is proved advantageous in neural\narchitectural design. However, current neural architecture search approaches\nare mainly limited to stacked single-path search space. How can the one-shot\ndoctrine search for multi-path models remains unresolved. Specifically, we are\nmotivated to train a multi-path supernet to accurately evaluate the candidate\narchitectures. In this paper, we discover that in the studied search space,\nfeature vectors summed from multiple paths are nearly multiples of those from a\nsingle path, which perturbs supernet training and its ranking ability. In this\nregard, we propose a novel mechanism called Shadow Batch Normalization(SBN) to\nregularize the disparate feature statistics. Extensive experiments prove that\nSBN is capable of stabilizing the training and improving the ranking\nperformance (e.g. Kendall Tau 0.597 tested on NAS-Bench-101). We call our\nunified multi-path one-shot approach as MixPath, which generates a series of\nmodels that achieve state-of-the-art results on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 15:24:26 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 11:05:45 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2020 10:47:27 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Li", "Xudong", ""], ["Lu", "Shun", ""], ["Zhang", "Bo", ""], ["Li", "Jixiang", ""]]}, {"id": "2001.05922", "submitter": "Matthias Lenga", "authors": "Matthias Lenga, Heinrich Schulz, Axel Saalbach", "title": "Continual Learning for Domain Adaptation in Chest X-ray Classification", "comments": null, "journal-ref": "Proceedings of the Third Conference on Medical Imaging with Deep\n  Learning, PMLR 121:413-423, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, Deep Learning has been successfully applied to a broad\nrange of medical applications. Especially in the context of chest X-ray\nclassification, results have been reported which are on par, or even superior\nto experienced radiologists. Despite this success in controlled experimental\nenvironments, it has been noted that the ability of Deep Learning models to\ngeneralize to data from a new domain (with potentially different tasks) is\noften limited. In order to address this challenge, we investigate techniques\nfrom the field of Continual Learning (CL) including Joint Training (JT),\nElastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Using\nthe ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically that\nthese methods provide promising options to improve the performance of Deep\nLearning models on a target domain and to mitigate effectively catastrophic\nforgetting for the source domain. To this end, the best overall performance was\nobtained using JT, while for LWF competitive results could be achieved - even\nwithout accessing data from the source domain.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 16:20:43 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Lenga", "Matthias", ""], ["Schulz", "Heinrich", ""], ["Saalbach", "Axel", ""]]}, {"id": "2001.05923", "submitter": "Adam Van Etten", "authors": "Adam Van Etten, Jacob Shermeyer, Daniel Hogan, Nicholas Weir, Ryan\n  Lewis", "title": "Road Network and Travel Time Extraction from Multiple Look Angles with\n  SpaceNet Data", "comments": "4 pages, 5 figures. To appear at the 2020 IEEE International\n  Geoscience and Remote Sensing Symposium", "journal-ref": null, "doi": "10.1109/IGARSS39084.2020.9324091", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Identification of road networks and optimal routes directly from remote\nsensing is of critical importance to a broad array of humanitarian and\ncommercial applications. Yet while identification of road pixels has been\nattempted before, estimation of route travel times from overhead imagery\nremains a novel problem, particularly for off-nadir overhead imagery. To this\nend, we extract road networks with travel time estimates from the SpaceNet MVOI\ndataset. Utilizing the CRESIv2 framework, we demonstrate the ability to extract\nroad networks in various observation angles and quantify performance at 27\nunique nadir angles with the graph-theoretic APLS_length and APLS_time metrics.\nA minimal gap of 0.03 between APLS_length and APLS_time scores indicates that\nour approach yields speed limits and travel times with very high fidelity. We\nalso explore the utility of incorporating all available angles during model\ntraining, and find a peak score of APLS_time = 0.56. The combined model\nexhibits greatly improved robustness over angle-specific models, despite the\nvery different appearance of road networks at extremely oblique off-nadir\nangles versus images captured from directly overhead.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 16:23:59 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 16:48:50 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Van Etten", "Adam", ""], ["Shermeyer", "Jacob", ""], ["Hogan", "Daniel", ""], ["Weir", "Nicholas", ""], ["Lewis", "Ryan", ""]]}, {"id": "2001.05936", "submitter": "Joseph Bethge", "authors": "Joseph Bethge, Christian Bartz, Haojin Yang, Ying Chen, and Christoph\n  Meinel", "title": "MeliusNet: Can Binary Neural Networks Achieve MobileNet-level Accuracy?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Networks (BNNs) are neural networks which use binary weights\nand activations instead of the typical 32-bit floating point values. They have\nreduced model sizes and allow for efficient inference on mobile or embedded\ndevices with limited power and computational resources. However, the\nbinarization of weights and activations leads to feature maps of lower quality\nand lower capacity and thus a drop in accuracy compared to traditional\nnetworks. Previous work has increased the number of channels or used multiple\nbinary bases to alleviate these problems. In this paper, we instead present an\narchitectural approach: MeliusNet. It consists of alternating a DenseBlock,\nwhich increases the feature capacity, and our proposed ImprovementBlock, which\nincreases the feature quality. Experiments on the ImageNet dataset demonstrate\nthe superior performance of our MeliusNet over a variety of popular binary\narchitectures with regards to both computation savings and accuracy.\nFurthermore, with our method we trained BNN models, which for the first time\ncan match the accuracy of the popular compact network MobileNet-v1 in terms of\nmodel size, number of operations and accuracy. Our code is published online at\nhttps://github.com/hpi-xnor/BMXNet-v2\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 16:56:10 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 11:52:06 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Bethge", "Joseph", ""], ["Bartz", "Christian", ""], ["Yang", "Haojin", ""], ["Chen", "Ying", ""], ["Meinel", "Christoph", ""]]}, {"id": "2001.05979", "submitter": "Marc Bosch", "authors": "Marc Bosch, Joseph Nassar, Benjamin Ortiz, Brendan Lammers, David\n  Lindenbaum, John Wahl, Robert Mangum, and Margaret Smith", "title": "Contextual Sense Making by Fusing Scene Classification, Detections, and\n  Events in Full Motion Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of imaging sensors, the volume of multi-modal imagery\nfar exceeds the ability of human analysts to adequately consume and exploit it.\nFull motion video (FMV) possesses the extra challenge of containing large\namounts of redundant temporal data. We aim to address the needs of human\nanalysts to consume and exploit data given aerial FMV. We have investigated and\ndesigned a system capable of detecting events and activities of interest that\ndeviate from the baseline patterns of observation given FMV feeds. We have\ndivided the problem into three tasks: (1) Context awareness, (2) object\ncataloging, and (3) event detection. The goal of context awareness is to\nconstraint the problem of visual search and detection in video data. A custom\nimage classifier categorizes the scene with one or multiple labels to identify\nthe operating context and environment. This step helps reducing the semantic\nsearch space of downstream tasks in order to increase their accuracy. The\nsecond step is object cataloging, where an ensemble of object detectors locates\nand labels any known objects found in the scene (people, vehicles, boats,\nplanes, buildings, etc.). Finally, context information and detections are sent\nto the event detection engine to monitor for certain behaviors. A series of\nanalytics monitor the scene by tracking object counts, and object interactions.\nIf these object interactions are not declared to be commonly observed in the\ncurrent scene, the system will report, geolocate, and log the event. Events of\ninterest include identifying a gathering of people as a meeting and/or a crowd,\nalerting when there are boats on a beach unloading cargo, increased count of\npeople entering a building, people getting in and/or out of vehicles of\ninterest, etc. We have applied our methods on data from different sensors at\ndifferent resolutions in a variety of geographical areas.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 18:26:34 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Bosch", "Marc", ""], ["Nassar", "Joseph", ""], ["Ortiz", "Benjamin", ""], ["Lammers", "Brendan", ""], ["Lindenbaum", "David", ""], ["Wahl", "John", ""], ["Mangum", "Robert", ""], ["Smith", "Margaret", ""]]}, {"id": "2001.05982", "submitter": "Marc Bosch", "authors": "Benjamin Ortiz, David Lindenbaum, Joseph Nassar, Brendan Lammers, John\n  Wahl, Robert Mangum, Margaret Smith, and Marc Bosch", "title": "A Common Operating Picture Framework Leveraging Data Fusion and Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organizations are starting to realize of the combined power of data and\ndata-driven algorithmic models to gain insights, situational awareness, and\nadvance their mission. A common challenge to gaining insights is connecting\ninherently different datasets. These datasets (e.g. geocoded features, video\nstreams, raw text, social network data, etc.) per separate they provide very\nnarrow answers; however collectively they can provide new capabilities. In this\nwork, we present a data fusion framework for accelerating solutions for\nProcessing, Exploitation, and Dissemination (PED). Our platform is a collection\nof services that extract information from several data sources (per separate)\nby leveraging deep learning and other means of processing. This information is\nfused by a set of analytical engines that perform data correlations, searches,\nand other modeling operations to combine information from the disparate data\nsources. As a result, events of interest are detected, geolocated, logged, and\npresented into a common operating picture. This common operating picture allows\nthe user to visualize in real time all the data sources, per separate and their\ncollective cooperation. In addition, forensic activities have been implemented\nand made available through the framework. Users can review archived results and\ncompare them to the most recent snapshot of the operational environment. In our\nfirst iteration we have focused on visual data (FMV, WAMI, CCTV/PTZ-Cameras,\nopen source video, etc.) and AIS data streams (satellite and terrestrial\nsources). As a proof-of-concept, in our experiments we show how FMV detections\ncan be combined with vessel tracking signals from AIS sources to confirm\nidentity, tip-and-cue aerial reconnaissance, and monitor vessel activity in an\narea.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 18:32:19 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 15:13:47 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Ortiz", "Benjamin", ""], ["Lindenbaum", "David", ""], ["Nassar", "Joseph", ""], ["Lammers", "Brendan", ""], ["Wahl", "John", ""], ["Mangum", "Robert", ""], ["Smith", "Margaret", ""], ["Bosch", "Marc", ""]]}, {"id": "2001.06001", "submitter": "Paola Cascante-Bonilla", "authors": "Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, Vicente Ordonez", "title": "Curriculum Labeling: Revisiting Pseudo-Labeling for Semi-Supervised\n  Learning", "comments": "In the 35th AAAI Conference on Artificial Intelligence. AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we revisit the idea of pseudo-labeling in the context of\nsemi-supervised learning where a learning algorithm has access to a small set\nof labeled samples and a large set of unlabeled samples. Pseudo-labeling works\nby applying pseudo-labels to samples in the unlabeled set by using a model\ntrained on the combination of the labeled samples and any previously\npseudo-labeled samples, and iteratively repeating this process in a\nself-training cycle. Current methods seem to have abandoned this approach in\nfavor of consistency regularization methods that train models under a\ncombination of different styles of self-supervised losses on the unlabeled\nsamples and standard supervised losses on the labeled samples. We empirically\ndemonstrate that pseudo-labeling can in fact be competitive with the\nstate-of-the-art, while being more resilient to out-of-distribution samples in\nthe unlabeled set. We identify two key factors that allow pseudo-labeling to\nachieve such remarkable results (1) applying curriculum learning principles and\n(2) avoiding concept drift by restarting model parameters before each\nself-training cycle. We obtain 94.91% accuracy on CIFAR-10 using only 4,000\nlabeled samples, and 68.87% top-1 accuracy on Imagenet-ILSVRC using only 10% of\nthe labeled samples. The code is available at\nhttps://github.com/uvavision/Curriculum-Labeling\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 03:24:27 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 16:14:35 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Cascante-Bonilla", "Paola", ""], ["Tan", "Fuwen", ""], ["Qi", "Yanjun", ""], ["Ordonez", "Vicente", ""]]}, {"id": "2001.06057", "submitter": "Evgenia Rusak", "authors": "Evgenia Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf,\n  Oliver Bringmann, Matthias Bethge, Wieland Brendel", "title": "A simple way to make neural networks robust against diverse image\n  corruptions", "comments": "Oral presentation at the European Conference for Computer Vision\n  (ECCV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system is remarkably robust against a wide range of\nnaturally occurring variations and corruptions like rain or snow. In contrast,\nthe performance of modern image recognition models strongly degrades when\nevaluated on previously unseen corruptions. Here, we demonstrate that a simple\nbut properly tuned training with additive Gaussian and Speckle noise\ngeneralizes surprisingly well to unseen corruptions, easily reaching the\nprevious state of the art on the corruption benchmark ImageNet-C (with\nResNet50) and on MNIST-C. We build on top of these strong baseline results and\nshow that an adversarial training of the recognition model against uncorrelated\nworst-case noise distributions leads to an additional increase in performance.\nThis regularization can be combined with previously proposed defense methods\nfor further improvement.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 20:10:25 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 16:19:26 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 16:33:23 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 15:41:26 GMT"}, {"version": "v5", "created": "Wed, 22 Jul 2020 12:25:10 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Rusak", "Evgenia", ""], ["Schott", "Lukas", ""], ["Zimmermann", "Roland S.", ""], ["Bitterwolf", "Julian", ""], ["Bringmann", "Oliver", ""], ["Bethge", "Matthias", ""], ["Brendel", "Wieland", ""]]}, {"id": "2001.06066", "submitter": "Fatih Gokce", "authors": "Fatih G\\\"ok\\c{c}e", "title": "Tracking of Micro Unmanned Aerial Vehicles: A Comparative Study", "comments": "In proceedings of the International Conference on Artificial\n  Intelligence and Applied Mathematics in Engineering (ICAIAME 2019), 13 pages,\n  9 Figures", "journal-ref": "F. G\\\"ok\\c{c}e. Tracking of Micro Unmanned Aerial Vehicles: A\n  Comparative Study. In Proceedings of the International Conference on\n  Artificial Intelligence and Applied Mathematics in Engineering, Antalya,\n  Turkey, 20-22 Apr. 2019, pp.374-386", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro unmanned aerial vehicles (mUAV) became very common in recent years. As\na result of their widespread usage, when they are flown by hobbyists illegally,\ncrucial risks are imposed and such mUAVs need to be sensed by security systems.\nFurthermore, the sensing of mUAVs are essential for also swarm robotics\nresearch where the individuals in a flock of robots require systems to sense\nand localize each other for coordinated operation. In order to obtain such\nsystems, there are studies to detect mUAVs utilizing different sensing mediums,\nsuch as vision, infrared and sound signals, and small-scale radars. However,\nthere are still challenges that awaits to be handled in this field such as\nintegrating tracking approaches to the vision-based detection systems to\nenhance accuracy and computational complexity. For this reason, in this study,\nwe combine various tracking approaches to a vision-based mUAV detection system\navailable in the literature, in order to evaluate different tracking approaches\nin terms of accuracy and as well as investigate the effect of such integration\nto the computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 20:46:08 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["G\u00f6k\u00e7e", "Fatih", ""]]}, {"id": "2001.06094", "submitter": "Debi Prasanna Mohanty Mr", "authors": "Sumit Kumar, Gopi Ramena, Manoj Goyal, Debi Mohanty, Ankur Agarwal,\n  Benu Changmai, Sukumar Moharana", "title": "On- Device Information Extraction from Screenshots in form of tags", "comments": null, "journal-ref": null, "doi": "10.1145/3371158.3371200", "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a method to make mobile screenshots easily searchable. In this\npaper, we present the workflow in which we: 1) preprocessed a collection of\nscreenshots, 2) identified script presentin image, 3) extracted unstructured\ntext from images, 4) identifiedlanguage of the extracted text, 5) extracted\nkeywords from the text, 6) identified tags based on image features, 7) expanded\ntag set by identifying related keywords, 8) inserted image tags with relevant\nimages after ranking and indexed them to make it searchable on device. We made\nthe pipeline which supports multiple languages and executed it on-device, which\naddressed privacy concerns. We developed novel architectures for components in\nthe pipeline, optimized performance and memory for on-device computation. We\nobserved from experimentation that the solution developed can reduce overall\nuser effort and improve end user experience while searching, whose results are\npublished.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 12:15:30 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kumar", "Sumit", ""], ["Ramena", "Gopi", ""], ["Goyal", "Manoj", ""], ["Mohanty", "Debi", ""], ["Agarwal", "Ankur", ""], ["Changmai", "Benu", ""], ["Moharana", "Sukumar", ""]]}, {"id": "2001.06099", "submitter": "Farnaz Behnia", "authors": "Farnaz Behnia, Ali Mirzaeian, Mohammad Sabokrou, Sai Manoj, Tinoosh\n  Mohsenin, Khaled N. Khasawneh, Liang Zhao, Houman Homayoun, Avesta Sasan", "title": "Code-Bridged Classifier (CBC): A Low or Negative Overhead Defense for\n  Making a CNN Classifier Robust Against Adversarial Attacks", "comments": "6 pages, Accepted and to appear in ISQED 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Code-Bridged Classifier (CBC), a framework for\nmaking a Convolutional Neural Network (CNNs) robust against adversarial attacks\nwithout increasing or even by decreasing the overall models' computational\ncomplexity. More specifically, we propose a stacked encoder-convolutional\nmodel, in which the input image is first encoded by the encoder module of a\ndenoising auto-encoder, and then the resulting latent representation (without\nbeing decoded) is fed to a reduced complexity CNN for image classification. We\nillustrate that this network not only is more robust to adversarial examples\nbut also has a significantly lower computational complexity when compared to\nthe prior art defenses.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 22:16:58 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Behnia", "Farnaz", ""], ["Mirzaeian", "Ali", ""], ["Sabokrou", "Mohammad", ""], ["Manoj", "Sai", ""], ["Mohsenin", "Tinoosh", ""], ["Khasawneh", "Khaled N.", ""], ["Zhao", "Liang", ""], ["Homayoun", "Houman", ""], ["Sasan", "Avesta", ""]]}, {"id": "2001.06103", "submitter": "Vansh Narula", "authors": "Vansh Narula, Zhangyang (Atlas) Wang and Theodora Chaspari", "title": "An adversarial learning framework for preserving users' anonymity in\n  face-based emotion recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and video-capturing technologies have permeated our every-day life.\nSuch technologies can continuously monitor individuals' expressions in\nreal-life settings, affording us new insights into their emotional states and\ntransitions, thus paving the way to novel well-being and healthcare\napplications. Yet, due to the strong privacy concerns, the use of such\ntechnologies is met with strong skepticism, since current face-based emotion\nrecognition systems relying on deep learning techniques tend to preserve\nsubstantial information related to the identity of the user, apart from the\nemotion-specific information. This paper proposes an adversarial learning\nframework which relies on a convolutional neural network (CNN) architecture\ntrained through an iterative procedure for minimizing identity-specific\ninformation and maximizing emotion-dependent information. The proposed approach\nis evaluated through emotion classification and face identification metrics,\nand is compared against two CNNs, one trained solely for emotion recognition\nand the other trained solely for face identification. Experiments are performed\nusing the Yale Face Dataset and Japanese Female Facial Expression Database.\nResults indicate that the proposed approach can learn a convolutional\ntransformation for preserving emotion recognition accuracy and degrading face\nidentity recognition, providing a foundation toward privacy-aware emotion\nrecognition technologies.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 22:45:52 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Narula", "Vansh", "", "Atlas"], ["Zhangyang", "", "", "Atlas"], ["Wang", "", ""], ["Chaspari", "Theodora", ""]]}, {"id": "2001.06122", "submitter": "William Theisen", "authors": "William Theisen, Joel Brogan, Pamela Bilo Thomas, Daniel Moreira,\n  Pascal Phoa, Tim Weninger, Walter Scheirer", "title": "Automatic Discovery of Political Meme Genres with Diverse Appearances", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forms of human communication are not static -- we expect some evolution in\nthe way information is conveyed over time because of advances in technology.\nOne example of this phenomenon is the image-based meme, which has emerged as a\ndominant form of political messaging in the past decade. While originally used\nto spread jokes on social media, memes are now having an outsized impact on\npublic perception of world events. A significant challenge in automatic meme\nanalysis has been the development of a strategy to match memes from within a\nsingle genre when the appearances of the images vary. Such variation is\nespecially common in memes exhibiting mimicry. For example, when voters perform\na common hand gesture to signal their support for a candidate. In this paper we\nintroduce a scalable automated visual recognition pipeline for discovering\npolitical meme genres of diverse appearance. This pipeline can ingest meme\nimages from a social network, apply computer vision-based techniques to extract\nlocal features and index new images into a database, and then organize the\nmemes into related genres. To validate this approach, we perform a large case\nstudy on the 2019 Indonesian Presidential Election using a new dataset of over\ntwo million images collected from Twitter and Instagram. Results show that this\napproach can discover new meme genres with visually diverse images that share\ncommon stylistic elements, paving the way forward for further work in semantic\nanalysis and content attribution.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 00:45:02 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 18:10:16 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Theisen", "William", ""], ["Brogan", "Joel", ""], ["Thomas", "Pamela Bilo", ""], ["Moreira", "Daniel", ""], ["Phoa", "Pascal", ""], ["Weninger", "Tim", ""], ["Scheirer", "Walter", ""]]}, {"id": "2001.06127", "submitter": "Anoop Cherian", "authors": "Anoop Cherian, Jue Wang, Chiori Hori, Tim K. Marks", "title": "Spatio-Temporal Ranked-Attention Networks for Video Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating video descriptions automatically is a challenging task that\ninvolves a complex interplay between spatio-temporal visual features and\nlanguage models. Given that videos consist of spatial (frame-level) features\nand their temporal evolutions, an effective captioning model should be able to\nattend to these different cues selectively. To this end, we propose a\nSpatio-Temporal and Temporo-Spatial (STaTS) attention model which, conditioned\non the language state, hierarchically combines spatial and temporal attention\nto videos in two different orders: (i) a spatio-temporal (ST) sub-model, which\nfirst attends to regions that have temporal evolution, then temporally pools\nthe features from these regions; and (ii) a temporo-spatial (TS) sub-model,\nwhich first decides a single frame to attend to, then applies spatial attention\nwithin that frame. We propose a novel LSTM-based temporal ranking function,\nwhich we call ranked attention, for the ST model to capture action dynamics.\nOur entire framework is trained end-to-end. We provide experiments on two\nbenchmark datasets: MSVD and MSR-VTT. Our results demonstrate the synergy\nbetween the ST and TS modules, outperforming recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 01:00:45 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Cherian", "Anoop", ""], ["Wang", "Jue", ""], ["Hori", "Chiori", ""], ["Marks", "Tim K.", ""]]}, {"id": "2001.06144", "submitter": "Wenxuan Wang", "authors": "Wenxuan Wang, Yanwei Fu, Qiang Sun, Tao Chen, Chenjie Cao, Ziqi Zheng,\n  Guoqiang Xu, Han Qiu, Yu-Gang Jiang, Xiangyang Xue", "title": "Learning to Augment Expressions for Few-shot Fine-grained Facial\n  Expression Recognition", "comments": "17 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective computing and cognitive theory are widely used in modern\nhuman-computer interaction scenarios. Human faces, as the most prominent and\neasily accessible features, have attracted great attention from researchers.\nSince humans have rich emotions and developed musculature, there exist a lot of\nfine-grained expressions in real-world applications. However, it is extremely\ntime-consuming to collect and annotate a large number of facial images, of\nwhich may even require psychologists to correctly categorize them. To the best\nof our knowledge, the existing expression datasets are only limited to several\nbasic facial expressions, which are not sufficient to support our ambitions in\ndeveloping successful human-computer interaction systems. To this end, a novel\nFine-grained Facial Expression Database - F2ED is contributed in this paper,\nand it includes more than 200k images with 54 facial expressions from 119\npersons. Considering the phenomenon of uneven data distribution and lack of\nsamples is common in real-world scenarios, we further evaluate several tasks of\nfew-shot expression learning by virtue of our F2ED, which are to recognize the\nfacial expressions given only few training instances. These tasks mimic human\nperformance to learn robust and general representation from few examples. To\naddress such few-shot tasks, we propose a unified task-driven framework -\nCompositional Generative Adversarial Network (Comp-GAN) learning to synthesize\nfacial images and thus augmenting the instances of few-shot expression classes.\nExtensive experiments are conducted on F2ED and existing facial expression\ndatasets, i.e., JAFFE and FER2013, to validate the efficacy of our F2ED in\npre-training facial expression recognition network and the effectiveness of our\nproposed approach Comp-GAN to improve the performance of few-shot recognition\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 03:26:32 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Wang", "Wenxuan", ""], ["Fu", "Yanwei", ""], ["Sun", "Qiang", ""], ["Chen", "Tao", ""], ["Cao", "Chenjie", ""], ["Zheng", "Ziqi", ""], ["Xu", "Guoqiang", ""], ["Qiu", "Han", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2001.06151", "submitter": "Heyi Li", "authors": "Heyi Li, Yuewei Lin, Klaus Mueller, Wei Xu", "title": "Interpreting Galaxy Deblender GAN from the Discriminator's Perspective", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are well known for their unsupervised\nlearning capabilities. A recent success in the field of astronomy is deblending\ntwo overlapping galaxy images via a branched GAN model. However, it remains a\nsignificant challenge to comprehend how the network works, which is\nparticularly difficult for non-expert users. This research focuses on behaviors\nof one of the network's major components, the Discriminator, which plays a\nvital role but is often overlooked, Specifically, we enhance the Layer-wise\nRelevance Propagation (LRP) scheme to generate a heatmap-based visualization.\nWe call this technique Polarized-LRP and it consists of two parts i.e. positive\ncontribution heatmaps for ground truth images and negative contribution\nheatmaps for generated images. Using the Galaxy Zoo dataset we demonstrate that\nour method clearly reveals attention areas of the Discriminator when\ndifferentiating generated galaxy images from ground truth images. To connect\nthe Discriminator's impact on the Generator, we visualize the gradual changes\nof the Generator across the training process. An interesting result we have\nachieved there is the detection of a problematic data augmentation procedure\nthat would else have remained hidden. We find that our proposed method serves\nas a useful visual analytical tool for a deeper understanding of GAN models.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 04:05:46 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Li", "Heyi", ""], ["Lin", "Yuewei", ""], ["Mueller", "Klaus", ""], ["Xu", "Wei", ""]]}, {"id": "2001.06171", "submitter": "Xiaolin Song", "authors": "Xiaolin Song, Yuyang Zhao, Jingyu Yang, Cuiling Lan, and Wenjun Zeng", "title": "FPCR-Net: Feature Pyramidal Correlation and Residual Reconstruction for\n  Semi-supervised Optical Flow Estimation", "comments": "8 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation is an important yet challenging problem in the field\nof video analytics. The features of different semantics levels/layers of a\nconvolutional neural network can provide information of different granularity.\nTo exploit such flexible and comprehensive information, we propose a\nsemi-supervised Feature Pyramidal Correlation and Residual Reconstruction\nNetwork (FPCR-Net) for optical flow estimation from frame pairs. It consists of\ntwo main modules: pyramid correlation mapping and residual reconstruction. The\npyramid correlation mapping module takes advantage of the multi-scale\ncorrelations of global/local patches by aggregating features of different\nscales to form a multi-level cost volume. The residual reconstruction module\naims to reconstruct the sub-band high-frequency residuals of finer optical flow\nin each stage. Based on the pyramid correlation mapping, we further propose a\ncorrelation-warping-normalization (CWN) module to efficiently exploit the\ncorrelation dependency. Experiment results show that the proposed scheme\nachieves the state-of-the-art performance, with improvement by 0.80, 1.15 and\n0.10 in terms of average end-point error (AEE) against competing baseline\nmethods - FlowNet2, LiteFlowNet and PWC-Net on the Final pass of Sintel\ndataset, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 07:13:51 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 08:56:28 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 04:59:57 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Song", "Xiaolin", ""], ["Zhao", "Yuyang", ""], ["Yang", "Jingyu", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2001.06175", "submitter": "Chanoh Park", "authors": "Chanoh Park, Peyman Moghadam, Soohwan Kim, Sridha Sridharan, Clinton\n  Fookes", "title": "Spatiotemporal Camera-LiDAR Calibration: A Targetless and Structureless\n  Approach", "comments": "8 pages, To appear, IEEE Robotics and Automation Letters 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for multimodal sensing systems for robotics is growing due to the\nincrease in robustness, reliability and accuracy offered by these systems.\nThese systems also need to be spatially and temporally co-registered to be\neffective. In this paper, we propose a targetless and structureless\nspatiotemporal camera-LiDAR calibration method. Our method combines a\nclosed-form solution with a modified structureless bundle adjustment where the\ncoarse-to-fine approach does not {require} an initial guess on the\nspatiotemporal parameters. Also, as 3D features (structure) are calculated from\ntriangulation only, there is no need to have a calibration target or to match\n2D features with the 3D point cloud which provides flexibility in the\ncalibration process and sensor configuration. We demonstrate the accuracy and\nrobustness of the proposed method through both simulation and real data\nexperiments using multiple sensor payload configurations mounted to hand-held,\naerial and legged robot systems. Also, qualitative results are given in the\nform of a colorized point cloud visualization.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 07:25:59 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Park", "Chanoh", ""], ["Moghadam", "Peyman", ""], ["Kim", "Soohwan", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2001.06202", "submitter": "Anbu Huang", "authors": "Yang Liu, Anbu Huang, Yun Luo, He Huang, Youzhi Liu, Yuanyuan Chen,\n  Lican Feng, Tianjian Chen, Han Yu, Qiang Yang", "title": "FedVision: An Online Visual Object Detection Platform Powered by\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object detection is a computer vision-based artificial intelligence\n(AI) technique which has many practical applications (e.g., fire hazard\nmonitoring). However, due to privacy concerns and the high cost of transmitting\nvideo data, it is highly challenging to build object detection models on\ncentrally stored large training datasets following the current approach.\nFederated learning (FL) is a promising approach to resolve this challenge.\nNevertheless, there currently lacks an easy to use tool to enable computer\nvision application developers who are not experts in federated learning to\nconveniently leverage this technology and apply it in their systems. In this\npaper, we report FedVision - a machine learning engineering platform to support\nthe development of federated learning powered computer vision applications. The\nplatform has been deployed through a collaboration between WeBank and Extreme\nVision to help customers develop computer vision-based safety monitoring\nsolutions in smart city applications. Over four months of usage, it has\nachieved significant efficiency improvement and cost reduction while removing\nthe need to transmit sensitive data for three major corporate customers. To the\nbest of our knowledge, this is the first real application of FL in computer\nvision-based tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 09:02:36 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Liu", "Yang", ""], ["Huang", "Anbu", ""], ["Luo", "Yun", ""], ["Huang", "He", ""], ["Liu", "Youzhi", ""], ["Chen", "Yuanyuan", ""], ["Feng", "Lican", ""], ["Chen", "Tianjian", ""], ["Yu", "Han", ""], ["Yang", "Qiang", ""]]}, {"id": "2001.06209", "submitter": "Florentin Liebmann MSc", "authors": "Florentin Liebmann, Simon Roner, Marco von Atzigen, Florian\n  Wanivenhaus, Caroline Neuhaus, Jos\\'e Spirig, Davide Scaramuzza, Reto Sutter,\n  Jess Snedeker, Mazda Farshad, Philipp F\\\"urnstahl", "title": "Registration made easy -- standalone orthopedic navigation with HoloLens", "comments": "6 pages, 5 figures, accepted at CVPR 2019 workshop on Computer Vision\n  Applications for Mixed Reality Headsets\n  (https://docs.microsoft.com/en-us/windows/mixed-reality/cvpr-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In surgical navigation, finding correspondence between preoperative plan and\nintraoperative anatomy, the so-called registration task, is imperative. One\npromising approach is to intraoperatively digitize anatomy and register it with\nthe preoperative plan. State-of-the-art commercial navigation systems implement\nsuch approaches for pedicle screw placement in spinal fusion surgery. Although\nthese systems improve surgical accuracy, they are not gold standard in clinical\npractice. Besides economical reasons, this may be due to their difficult\nintegration into clinical workflows and unintuitive navigation feedback.\nAugmented Reality has the potential to overcome these limitations.\nConsequently, we propose a surgical navigation approach comprising\nintraoperative surface digitization for registration and intuitive holographic\nnavigation for pedicle screw placement that runs entirely on the Microsoft\nHoloLens. Preliminary results from phantom experiments suggest that the method\nmay meet clinical accuracy requirements.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 09:22:21 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Liebmann", "Florentin", ""], ["Roner", "Simon", ""], ["von Atzigen", "Marco", ""], ["Wanivenhaus", "Florian", ""], ["Neuhaus", "Caroline", ""], ["Spirig", "Jos\u00e9", ""], ["Scaramuzza", "Davide", ""], ["Sutter", "Reto", ""], ["Snedeker", "Jess", ""], ["Farshad", "Mazda", ""], ["F\u00fcrnstahl", "Philipp", ""]]}, {"id": "2001.06232", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Grzegorz Swirszcz and Joao Carreira and Viorica\n  Patraucean", "title": "Sideways: Depth-Parallel Training of Video Models", "comments": "Accepted at CVPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Sideways, an approximate backpropagation scheme for training video\nmodels. In standard backpropagation, the gradients and activations at every\ncomputation step through the model are temporally synchronized. The forward\nactivations need to be stored until the backward pass is executed, preventing\ninter-layer (depth) parallelization. However, can we leverage smooth, redundant\ninput streams such as videos to develop a more efficient training scheme? Here,\nwe explore an alternative to backpropagation; we overwrite network activations\nwhenever new ones, i.e., from new frames, become available. Such a more gradual\naccumulation of information from both passes breaks the precise correspondence\nbetween gradients and activations, leading to theoretically more noisy weight\nupdates. Counter-intuitively, we show that Sideways training of deep\nconvolutional video networks not only still converges, but can also potentially\nexhibit better generalization compared to standard synchronized\nbackpropagation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 10:49:55 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 18:16:44 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 22:48:10 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Swirszcz", "Grzegorz", ""], ["Carreira", "Joao", ""], ["Patraucean", "Viorica", ""]]}, {"id": "2001.06236", "submitter": "Zhenbing Zhao", "authors": "Zhenbing Zhao, Hongyu Qi, Yincheng Qi, Ke Zhang, Yongjie Zhai, Wenqing\n  Zhao", "title": "Detection Method Based on Automatic Visual Shape Clustering for\n  Pin-Missing Defect in Transmission Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bolts are the most numerous fasteners in transmission lines and are prone to\nlosing their split pins. How to realize the automatic pin-missing defect\ndetection for bolts in transmission lines so as to achieve timely and efficient\ntrouble shooting is a difficult problem and the long-term research target of\npower systems. In this paper, an automatic detection model called Automatic\nVisual Shape Clustering Network (AVSCNet) for pin-missing defect is\nconstructed. Firstly, an unsupervised clustering method for the visual shapes\nof bolts is proposed and applied to construct a defect detection model which\ncan learn the difference of visual shape. Next, three deep convolutional neural\nnetwork optimization methods are used in the model: the feature enhancement,\nfeature fusion and region feature extraction. The defect detection results are\nobtained by applying the regression calculation and classification to the\nregional features. In this paper, the object detection model of different\nnetworks is used to test the dataset of pin-missing defect constructed by the\naerial images of transmission lines from multiple locations, and it is\nevaluated by various indicators and is fully verified. The results show that\nour method can achieve considerably satisfactory detection effect.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 10:57:37 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Zhao", "Zhenbing", ""], ["Qi", "Hongyu", ""], ["Qi", "Yincheng", ""], ["Zhang", "Ke", ""], ["Zhai", "Yongjie", ""], ["Zhao", "Wenqing", ""]]}, {"id": "2001.06252", "submitter": "Xinzheng Zhang Prof.", "authors": "Xinzheng Zhang, Guo Liu, Ce Zhang, Peter M Atkinson, Xiaoheng Tan, Xin\n  Jian, Xichuan Zhou, Yongming Li", "title": "Two-Phase Object-Based Deep Learning for Multi-temporal SAR Image Change\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection is one of the fundamental applications of synthetic aperture\nradar (SAR) images. However, speckle noise presented in SAR images has a much\nnegative effect on change detection. In this research, a novel two-phase\nobject-based deep learning approach is proposed for multi-temporal SAR image\nchange detection. Compared with traditional methods, the proposed approach\nbrings two main innovations. One is to classify all pixels into three\ncategories rather than two categories: unchanged pixels, changed pixels caused\nby strong speckle (false changes), and changed pixels formed by real terrain\nvariation (real changes). The other is to group neighboring pixels into\nsegmented into superpixel objects (from pixels) such as to exploit local\nspatial context. Two phases are designed in the methodology: 1) Generate\nobjects based on the simple linear iterative clustering algorithm, and\ndiscriminate these objects into changed and unchanged classes using fuzzy\nc-means (FCM) clustering and a deep PCANet. The prediction of this Phase is the\nset of changed and unchanged superpixels. 2) Deep learning on the pixel sets\nover the changed superpixels only, obtained in the first phase, to discriminate\nreal changes from false changes. SLIC is employed again to achieve new\nsuperpixels in the second phase. Low rank and sparse decomposition are applied\nto these new superpixels to suppress speckle noise significantly. A further\nclustering step is applied to these new superpixels via FCM. A new PCANet is\nthen trained to classify two kinds of changed superpixels to achieve the final\nchange maps. Numerical experiments demonstrate that, compared with benchmark\nmethods, the proposed approach can distinguish real changes from false changes\neffectively with significantly reduced false alarm rates, and achieve up to\n99.71% change detection accuracy using multi-temporal SAR imagery.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 11:51:35 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Zhang", "Xinzheng", ""], ["Liu", "Guo", ""], ["Zhang", "Ce", ""], ["Atkinson", "Peter M", ""], ["Tan", "Xiaoheng", ""], ["Jian", "Xin", ""], ["Zhou", "Xichuan", ""], ["Li", "Yongming", ""]]}, {"id": "2001.06265", "submitter": "Ayush Chopra", "authors": "Surgan Jandial, Ayush Chopra, Kumar Ayush, Mayur Hemani, Abhijeet\n  Kumar, and Balaji Krishnamurthy", "title": "SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On", "comments": "Accepted at IEEE WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based virtual try-on for fashion has gained considerable attention\nrecently. The task requires trying on a clothing item on a target model image.\nAn efficient framework for this is composed of two stages: (1) warping\n(transforming) the try-on cloth to align with the pose and shape of the target\nmodel, and (2) a texture transfer module to seamlessly integrate the warped\ntry-on cloth onto the target model image. Existing methods suffer from\nartifacts and distortions in their try-on output. In this work, we present\nSieveNet, a framework for robust image-based virtual try-on. Firstly, we\nintroduce a multi-stage coarse-to-fine warping network to better model\nfine-grained intricacies (while transforming the try-on cloth) and train it\nwith a novel perceptual geometric matching loss. Next, we introduce a try-on\ncloth conditioned segmentation mask prior to improve the texture transfer\nnetwork. Finally, we also introduce a dueling triplet loss strategy for\ntraining the texture translation network which further improves the quality of\nthe generated try-on results. We present extensive qualitative and quantitative\nevaluations of each component of the proposed pipeline and show significant\nperformance improvements against the current state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 12:33:54 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Jandial", "Surgan", ""], ["Chopra", "Ayush", ""], ["Ayush", "Kumar", ""], ["Hemani", "Mayur", ""], ["Kumar", "Abhijeet", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "2001.06268", "submitter": "Kiho Hong", "authors": "Jungkyu Lee, Taeryun Won, Tae Kwan Lee, Hyemin Lee, Geonmo Gu, Kiho\n  Hong", "title": "Compounding the Performance Improvements of Assembled Techniques in a\n  Convolutional Neural Network", "comments": "9 pages, 2 figures, 18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies in image classification have demonstrated a variety of\ntechniques for improving the performance of Convolutional Neural Networks\n(CNNs). However, attempts to combine existing techniques to create a practical\nmodel are still uncommon. In this study, we carry out extensive experiments to\nvalidate that carefully assembling these techniques and applying them to basic\nCNN models (e.g. ResNet and MobileNet) can improve the accuracy and robustness\nof the models while minimizing the loss of throughput. Our proposed assembled\nResNet-50 shows improvements in top-1 accuracy from 76.3\\% to 82.78\\%, mCE from\n76.0\\% to 48.9\\% and mFR from 57.7\\% to 32.3\\% on ILSVRC2012 validation set.\nWith these improvements, inference throughput only decreases from 536 to 312.\nTo verify the performance improvement in transfer learning, fine grained\nclassification and image retrieval tasks were tested on several public datasets\nand showed that the improvement to backbone network performance boosted\ntransfer learning performance significantly. Our approach achieved 1st place in\nthe iFood Competition Fine-Grained Visual Recognition at CVPR 2019, and the\nsource code and trained models are available at\nhttps://github.com/clovaai/assembled-cnn\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 12:42:08 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 10:27:45 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Lee", "Jungkyu", ""], ["Won", "Taeryun", ""], ["Lee", "Tae Kwan", ""], ["Lee", "Hyemin", ""], ["Gu", "Geonmo", ""], ["Hong", "Kiho", ""]]}, {"id": "2001.06280", "submitter": "Saifullahi Aminu Bello", "authors": "Saifullahi Aminu Bello, Shangshu Yu, Cheng Wang", "title": "Review: deep learning on 3D point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud is point sets defined in 3D metric space. Point cloud has become\none of the most significant data format for 3D representation. Its gaining\nincreased popularity as a result of increased availability of acquisition\ndevices, such as LiDAR, as well as increased application in areas such as\nrobotics, autonomous driving, augmented and virtual reality. Deep learning is\nnow the most powerful tool for data processing in computer vision, becoming the\nmost preferred technique for tasks such as classification, segmentation, and\ndetection. While deep learning techniques are mainly applied to data with a\nstructured grid, point cloud, on the other hand, is unstructured. The\nunstructuredness of point clouds makes use of deep learning for its processing\ndirectly very challenging. Earlier approaches overcome this challenge by\npreprocessing the point cloud into a structured grid format at the cost of\nincreased computational cost or lost of depth information. Recently, however,\nmany state-of-the-arts deep learning techniques that directly operate on point\ncloud are being developed. This paper contains a survey of the recent\nstate-of-the-art deep learning techniques that mainly focused on point cloud\ndata. We first briefly discussed the major challenges faced when using deep\nlearning directly on point cloud, we also briefly discussed earlier approaches\nwhich overcome the challenges by preprocessing the point cloud into a\nstructured grid. We then give the review of the various state-of-the-art deep\nlearning approaches that directly process point cloud in its unstructured form.\nWe introduced the popular 3D point cloud benchmark datasets. And we also\nfurther discussed the application of deep learning in popular 3D vision tasks\nincluding classification, segmentation and detection.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 12:55:23 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Bello", "Saifullahi Aminu", ""], ["Yu", "Shangshu", ""], ["Wang", "Cheng", ""]]}, {"id": "2001.06291", "submitter": "Davis Rempe", "authors": "Davis Rempe, Srinath Sridhar, He Wang, Leonidas J. Guibas", "title": "Predicting the Physical Dynamics of Unseen 3D Objects", "comments": "In Proceedings of Winter Conference on Applications of Computer\n  Vision (WACV) 2020. arXiv admin note: text overlap with arXiv:1901.00466", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machines that can predict the effect of physical interactions on the dynamics\nof previously unseen object instances are important for creating better robots\nand interactive virtual worlds. In this work, we focus on predicting the\ndynamics of 3D objects on a plane that have just been subjected to an impulsive\nforce. In particular, we predict the changes in state - 3D position, rotation,\nvelocities, and stability. Different from previous work, our approach can\ngeneralize dynamics predictions to object shapes and initial conditions that\nwere unseen during training. Our method takes the 3D object's shape as a point\ncloud and its initial linear and angular velocities as input. We extract shape\nfeatures and use a recurrent neural network to predict the full change in state\nat each time step. Our model can support training with data from both a physics\nengine or the real world. Experiments show that we can accurately predict the\nchanges in state for unseen object geometries and initial conditions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 06:27:59 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Rempe", "Davis", ""], ["Sridhar", "Srinath", ""], ["Wang", "He", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2001.06303", "submitter": "Dawei Du", "authors": "Pengfei Zhu, Longyin Wen, Dawei Du, Xiao Bian, Qinghua Hu, Haibin Ling", "title": "Vision Meets Drones: Past, Present and Future", "comments": "arXiv admin note: text overlap with arXiv:1804.07437", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones, or general UAVs, equipped with cameras have been fast deployed with a\nwide range of applications, including agriculture, aerial photography, and\nsurveillance. Consequently, automatic understanding of visual data collected\nfrom drones becomes highly demanding, bringing computer vision and drones more\nand more closely. To promote and track the evelopments of object detection and\ntracking algorithms, we have organized two challenge workshops in conjunction\nwith ECCV 2018, and ICCV 2019, attracting more than 100 teams around the world.\nWe provide a large-scale drone captured dataset, VisDrone, which includes four\ntracks, i.e., (1) image object detection, (2) video object detection, (3)\nsingle object tracking, and (4) multi-object tracking. In this paper, we first\npresents a thorough review of object detection and tracking datasets and\nbenchmarks, and discuss the challenges of collecting large-scale drone-based\nobject detection and tracking datasets with fully manual annotations. After\nthat, we describe our VisDrone dataset, which is captured over various\nurban/suburban areas of 14 different cities across China from North to South.\nBeing the largest such dataset ever published, VisDrone enables extensive\nevaluation and investigation of visual analysis algorithms on the drone\nplatform. We provide a detailed analysis of the current state of the field of\nlarge-scale object detection and tracking on drones, and conclude the challenge\nas well as propose future directions. We expect the benchmark largely boost the\nresearch and development in video analysis on drone platforms. All the datasets\nand experimental results can be downloaded from the website:\nhttps://github.com/VisDrone/VisDrone-Dataset.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 00:11:56 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 22:52:27 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Zhu", "Pengfei", ""], ["Wen", "Longyin", ""], ["Du", "Dawei", ""], ["Bian", "Xiao", ""], ["Hu", "Qinghua", ""], ["Ling", "Haibin", ""]]}, {"id": "2001.06338", "submitter": "Henrique Da Costa Siqueira", "authors": "Henrique Siqueira, Sven Magg and Stefan Wermter", "title": "Efficient Facial Feature Learning with Wide Ensemble-based Convolutional\n  Neural Networks", "comments": "Accepted at the Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI-20), 1-1, New York, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble methods, traditionally built with independently trained\nde-correlated models, have proven to be efficient methods for reducing the\nremaining residual generalization error, which results in robust and accurate\nmethods for real-world applications. In the context of deep learning, however,\ntraining an ensemble of deep networks is costly and generates high redundancy\nwhich is inefficient. In this paper, we present experiments on Ensembles with\nShared Representations (ESRs) based on convolutional networks to demonstrate,\nquantitatively and qualitatively, their data processing efficiency and\nscalability to large-scale datasets of facial expressions. We show that\nredundancy and computational load can be dramatically reduced by varying the\nbranching level of the ESR without loss of diversity and generalization power,\nwhich are both important for ensemble performance. Experiments on large-scale\ndatasets suggest that ESRs reduce the remaining residual generalization error\non the AffectNet and FER+ datasets, reach human-level performance, and\noutperform state-of-the-art methods on facial expression recognition in the\nwild using emotion and affect concepts.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 14:32:27 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Siqueira", "Henrique", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "2001.06342", "submitter": "Diego Valsesia", "authors": "Andrea Bordone Molini, Diego Valsesia, Giulia Fracastoro, Enrico Magli", "title": "DeepSUM++: Non-local Deep Neural Network for Super-Resolution of\n  Unregistered Multitemporal Images", "comments": "arXiv admin note: text overlap with arXiv:1907.06490", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods for super-resolution of a remote sensing scene from\nmultiple unregistered low-resolution images have recently gained attention\nthanks to a challenge proposed by the European Space Agency. This paper\npresents an evolution of the winner of the challenge, showing how incorporating\nnon-local information in a convolutional neural network allows to exploit\nself-similar patterns that provide enhanced regularization of the\nsuper-resolution problem. Experiments on the dataset of the challenge show\nimproved performance over the state-of-the-art, which does not exploit\nnon-local information.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 11:17:19 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Molini", "Andrea Bordone", ""], ["Valsesia", "Diego", ""], ["Fracastoro", "Giulia", ""], ["Magli", "Enrico", ""]]}, {"id": "2001.06347", "submitter": "Xuesu Xiao", "authors": "Xuesu Xiao, Jan Dufek, Robin R. Murphy", "title": "Tethered Aerial Visual Assistance", "comments": "Submitted to special issue of \"Field and Service Robotics\" of the\n  Journal of Field Robotics (JFR). arXiv admin note: text overlap with\n  arXiv:1904.00078", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an autonomous tethered Unmanned Aerial Vehicle (UAV) is\ndeveloped into a visual assistant in a marsupial co-robots team, collaborating\nwith a tele-operated Unmanned Ground Vehicle (UGV) for robot operations in\nunstructured or confined environments. These environments pose extreme\nchallenges to the remote tele-operator due to the lack of sufficient\nsituational awareness, mostly caused by the unstructuredness and confinement,\nstationary and limited field-of-view and lack of depth perception from the\nrobot's onboard cameras. To overcome these problems, a secondary tele-operated\nrobot is used in current practices, who acts as a visual assistant and provides\nexternal viewpoints to overcome the perceptual limitations of the primary\nrobot's onboard sensors. However, a second tele-operated robot requires extra\nmanpower and teamwork demand between primary and secondary operators. The\nmanually chosen viewpoints tend to be subjective and sub-optimal. Considering\nthese intricacies, we develop an autonomous tethered aerial visual assistant in\nplace of the secondary tele-operated robot and operator, to reduce human robot\nratio from 2:2 to 1:2. Using a fundamental viewpoint quality theory, a formal\nrisk reasoning framework, and a newly developed tethered motion suite, our\nvisual assistant is able to autonomously navigate to good-quality viewpoints in\na risk-aware manner through unstructured or confined spaces with a tether. The\ndeveloped marsupial co-robots team could improve tele-operation efficiency in\nnuclear operations, bomb squad, disaster robots, and other domains with novel\ntasks or highly occluded environments, by reducing manpower and teamwork\ndemand, and achieving better visual assistance quality with trustworthy\nrisk-aware motion.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 06:41:04 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Xiao", "Xuesu", ""], ["Dufek", "Jan", ""], ["Murphy", "Robin R.", ""]]}, {"id": "2001.06354", "submitter": "Hyounghun Kim", "authors": "Hyounghun Kim, Hao Tan, Mohit Bansal", "title": "Modality-Balanced Models for Visual Dialogue", "comments": "AAAI 2020 (11 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Visual Dialog task requires a model to exploit both image and\nconversational context information to generate the next response to the\ndialogue. However, via manual analysis, we find that a large number of\nconversational questions can be answered by only looking at the image without\nany access to the context history, while others still need the conversation\ncontext to predict the correct answers. We demonstrate that due to this reason,\nprevious joint-modality (history and image) models over-rely on and are more\nprone to memorizing the dialogue history (e.g., by extracting certain keywords\nor patterns in the context information), whereas image-only models are more\ngeneralizable (because they cannot memorize or extract keywords from history)\nand perform substantially better at the primary normalized discounted\ncumulative gain (NDCG) task metric which allows multiple correct answers.\nHence, this observation encourages us to explicitly maintain two models, i.e.,\nan image-only model and an image-history joint model, and combine their\ncomplementary abilities for a more balanced multimodal model. We present\nmultiple methods for this integration of the two models, via ensemble and\nconsensus dropout fusion with shared parameters. Empirically, our models\nachieve strong results on the Visual Dialog challenge 2019 (rank 3 on NDCG and\nhigh balance across metrics), and substantially outperform the winner of the\nVisual Dialog challenge 2018 on most metrics.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 14:57:12 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Kim", "Hyounghun", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2001.06372", "submitter": "Gencer Sumbul", "authors": "Gencer Sumbul, Jian Kang, Tristan Kreuziger, Filipe Marcelino, Hugo\n  Costa, Pedro Benevides, Mario Caetano, Beg\\\"um Demir", "title": "BigEarthNet Dataset with A New Class-Nomenclature for Remote Sensing\n  Image Understanding", "comments": "This paper has been withdrawn by the authors. This paper has been\n  superseded by arXiv:2105.07921", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents BigEarthNet that is a large-scale Sentinel-2\nmultispectral image dataset with a new class nomenclature to advance deep\nlearning (DL) studies in remote sensing (RS). BigEarthNet is made up of 590,326\nimage patches annotated with multi-labels provided by the CORINE Land Cover\n(CLC) map of 2018 based on its most thematic detailed Level-3 class\nnomenclature. Initial research demonstrates that some CLC classes are\nchallenging to be accurately described by considering only Sentinel-2 images.\nTo increase the effectiveness of BigEarthNet, in this paper we introduce an\nalternative class-nomenclature to allow DL models for better learning and\ndescribing the complex spatial and spectral information content of the\nSentinel-2 images. This is achieved by interpreting and arranging the CLC\nLevel-3 nomenclature based on the properties of Sentinel-2 images in a new\nnomenclature of 19 classes. Then, the new class-nomenclature of BigEarthNet is\nused within state-of-the-art DL models in the context of multi-label\nclassification. Results show that the models trained from scratch on\nBigEarthNet outperform those pre-trained on ImageNet, especially in relation to\nsome complex classes including agriculture, other vegetated and natural\nenvironments. All DL models are made publicly available at\nhttp://bigearth.net/#downloads, offering an important resource to guide future\nprogress on RS image analysis.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 15:29:24 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 09:51:00 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 15:30:44 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sumbul", "Gencer", ""], ["Kang", "Jian", ""], ["Kreuziger", "Tristan", ""], ["Marcelino", "Filipe", ""], ["Costa", "Hugo", ""], ["Benevides", "Pedro", ""], ["Caetano", "Mario", ""], ["Demir", "Beg\u00fcm", ""]]}, {"id": "2001.06392", "submitter": "Lingxi Xie", "authors": "Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Bowen Shi, Qi Tian,\n  Hongkai Xiong", "title": "Latency-Aware Differentiable Neural Architecture Search", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable neural architecture search methods became popular in recent\nyears, mainly due to their low search costs and flexibility in designing the\nsearch space. However, these methods suffer the difficulty in optimizing\nnetwork, so that the searched network is often unfriendly to hardware. This\npaper deals with this problem by adding a differentiable latency loss term into\noptimization, so that the search process can tradeoff between accuracy and\nlatency with a balancing coefficient. The core of latency prediction is to\nencode each network architecture and feed it into a multi-layer regressor, with\nthe training data which can be easily collected from randomly sampling a number\nof architectures and evaluating them on the hardware. We evaluate our approach\non NVIDIA Tesla-P100 GPUs. With 100K sampled architectures (requiring a few\nhours), the latency prediction module arrives at a relative error of lower than\n10%. Equipped with this module, the search method can reduce the latency by 20%\nmeanwhile preserving the accuracy. Our approach also enjoys the ability of\nbeing transplanted to a wide range of hardware platforms with very few efforts,\nor being used to optimizing other non-differentiable factors such as power\nconsumption.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 15:55:21 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 02:20:32 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Xu", "Yuhui", ""], ["Xie", "Lingxi", ""], ["Zhang", "Xiaopeng", ""], ["Chen", "Xin", ""], ["Shi", "Bowen", ""], ["Tian", "Qi", ""], ["Xiong", "Hongkai", ""]]}, {"id": "2001.06404", "submitter": "Jhony Heriberto Giraldo Zuluaga", "authors": "Jhony H. Giraldo, Thierry Bouwmans", "title": "GraphBGS: Background Subtraction via Recovery of Graph Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background subtraction is a fundamental pre-processing task in computer\nvision. This task becomes challenging in real scenarios due to variations in\nthe background for both static and moving camera sequences. Several deep\nlearning methods for background subtraction have been proposed in the\nliterature with competitive performances. However, these models show\nperformance degradation when tested on unseen videos; and they require huge\namount of data to avoid overfitting. Recently, graph-based algorithms have been\nsuccessful approaching unsupervised and semi-supervised learning problems.\nFurthermore, the theory of graph signal processing and semi-supervised learning\nhave been combined leading to new insights in the field of machine learning. In\nthis paper, concepts of recovery of graph signals are introduced in the problem\nof background subtraction. We propose a new algorithm called Graph BackGround\nSubtraction (GraphBGS), which is composed of: instance segmentation, background\ninitialization, graph construction, graph sampling, and a semi-supervised\nalgorithm inspired from the theory of recovery of graph signals. Our algorithm\nhas the advantage of requiring less labeled data than deep learning methods\nwhile having competitive results on both: static and moving camera videos.\nGraphBGS outperforms unsupervised and supervised methods in several challenging\nconditions on the publicly available Change Detection (CDNet2014), and UCSD\nbackground subtraction databases.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 16:17:51 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 16:23:49 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Giraldo", "Jhony H.", ""], ["Bouwmans", "Thierry", ""]]}, {"id": "2001.06409", "submitter": "Hui Men", "authors": "Hui Men, Vlad Hosu, Hanhe Lin, Andr\\'es Bruhn and Dietmar Saupe", "title": "Subjective Annotation for a Frame Interpolation Benchmark using Artefact\n  Amplification", "comments": "arXiv admin note: text overlap with arXiv:1901.05362", "journal-ref": null, "doi": "10.1007/s41233-020-00037-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current benchmarks for optical flow algorithms evaluate the estimation either\ndirectly by comparing the predicted flow fields with the ground truth or\nindirectly by using the predicted flow fields for frame interpolation and then\ncomparing the interpolated frames with the actual frames. In the latter case,\nobjective quality measures such as the mean squared error are typically\nemployed. However, it is well known that for image quality assessment, the\nactual quality experienced by the user cannot be fully deduced from such simple\nmeasures. Hence, we conducted a subjective quality assessment crowdscouring\nstudy for the interpolated frames provided by one of the optical flow\nbenchmarks, the Middlebury benchmark. We collected forced-choice paired\ncomparisons between interpolated images and corresponding ground truth. To\nincrease the sensitivity of observers when judging minute difference in paired\ncomparisons we introduced a new method to the field of full-reference quality\nassessment, called artefact amplification. From the crowdsourcing data, we\nreconstructed absolute quality scale values according to Thurstone's model. As\na result, we obtained a re-ranking of the 155 participating algorithms w.r.t.\nthe visual quality of the interpolated frames. This re-ranking not only shows\nthe necessity of visual quality assessment as another evaluation metric for\noptical flow and frame interpolation benchmarks, the results also provide the\nground truth for designing novel image quality assessment (IQA) methods\ndedicated to perceptual quality of interpolated images. As a first step, we\nproposed such a new full-reference method, called WAE-IQA. By weighing the\nlocal differences between an interpolated image and its ground truth WAE-IQA\nperformed slightly better than the currently best FR-IQA approach from the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 18:20:37 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 19:06:04 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Men", "Hui", ""], ["Hosu", "Vlad", ""], ["Lin", "Hanhe", ""], ["Bruhn", "Andr\u00e9s", ""], ["Saupe", "Dietmar", ""]]}, {"id": "2001.06416", "submitter": "Aleksandr Koryagin", "authors": "Alexander Koryagin, Roman Khudorozhkov, Sergey Tsimfer, Darima\n  Mylzenova", "title": "SeismiQB -- a novel framework for deep learning with seismic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Deep Neural Networks were successfully adopted in numerous\ndomains to solve various image-related tasks, ranging from simple\nclassification to fine borders annotation. Naturally, many researches proposed\nto use it to solve geological problems. Unfortunately, many of the seismic\nprocessing tools were developed years before the era of machine learning,\nincluding the most popular SEG-Y data format for storing seismic cubes. Its\nslow loading speed heavily hampers experimentation speed, which is essential\nfor getting acceptable results. Worse yet, there is no widely-used format for\nstoring surfaces inside the volume (for example, seismic horizons). To address\nthese problems, we've developed an open-sourced Python framework with emphasis\non working with neural networks, that provides convenient tools for (i) fast\nloading seismic cubes in multiple data formats and converting between them,\n(ii) generating crops of desired shape and augmenting them with various\ntransformations, and (iii) pairing cube data with labeled horizons or other\ntypes of geobodies.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 10:45:56 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Koryagin", "Alexander", ""], ["Khudorozhkov", "Roman", ""], ["Tsimfer", "Sergey", ""], ["Mylzenova", "Darima", ""]]}, {"id": "2001.06427", "submitter": "Lele Chen", "authors": "Lele Chen, Justin Tian, Guo Li, Cheng-Haw Wu, Erh-Kan King, Kuan-Ting\n  Chen, Shao-Hang Hsieh, Chenliang Xu", "title": "TailorGAN: Making User-Defined Fashion Designs", "comments": "fashion", "journal-ref": "2020 Winter Conference on Applications of Computer Vision", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Attribute editing has become an important and emerging topic of computer\nvision. In this paper, we consider a task: given a reference garment image A\nand another image B with target attribute (collar/sleeve), generate a\nphoto-realistic image which combines the texture from reference A and the new\nattribute from reference B. The highly convoluted attributes and the lack of\npaired data are the main challenges to the task. To overcome those limitations,\nwe propose a novel self-supervised model to synthesize garment images with\ndisentangled attributes (e.g., collar and sleeves) without paired data. Our\nmethod consists of a reconstruction learning step and an adversarial learning\nstep. The model learns texture and location information through reconstruction\nlearning. And, the model's capability is generalized to achieve\nsingle-attribute manipulation by adversarial learning. Meanwhile, we compose a\nnew dataset, named GarmentSet, with annotation of landmarks of collars and\nsleeves on clean garment images. Extensive experiments on this dataset and\nreal-world samples demonstrate that our method can synthesize much better\nresults than the state-of-the-art methods in both quantitative and qualitative\ncomparisons.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 16:54:46 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 03:33:33 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chen", "Lele", ""], ["Tian", "Justin", ""], ["Li", "Guo", ""], ["Wu", "Cheng-Haw", ""], ["King", "Erh-Kan", ""], ["Chen", "Kuan-Ting", ""], ["Hsieh", "Shao-Hang", ""], ["Xu", "Chenliang", ""]]}, {"id": "2001.06440", "submitter": "Francesco Marra", "authors": "Davide Cozzolino, Francesco Marra, Diego Gragnaniello, Giovanni Poggi,\n  and Luisa Verdoliva", "title": "Combining PRNU and noiseprint for robust and efficient device source\n  identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PRNU-based image processing is a key asset in digital multimedia forensics.\nIt allows for reliable device identification and effective detection and\nlocalization of image forgeries, in very general conditions. However,\nperformance impairs significantly in challenging conditions involving low\nquality and quantity of data. These include working on compressed and cropped\nimages, or estimating the camera PRNU pattern based on only a few images. To\nboost the performance of PRNU-based analyses in such conditions we propose to\nleverage the image noiseprint, a recently proposed camera-model fingerprint\nthat has proved effective for several forensic tasks. Numerical experiments on\ndatasets widely used for source identification prove that the proposed method\nensures a significant performance improvement in a wide range of challenging\nsituations.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 17:32:32 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Cozzolino", "Davide", ""], ["Marra", "Francesco", ""], ["Gragnaniello", "Diego", ""], ["Poggi", "Giovanni", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "2001.06479", "submitter": "Seyed Shahabeddin Nabavi", "authors": "Seyed Shahabeddin Nabavi, Mehrdad Hosseinzadeh, Ramin Fahimi, Yang\n  Wang", "title": "Unsupervised Learning of Camera Pose with Compositional Re-estimation", "comments": "Accepted to WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of unsupervised camera pose estimation. Given an\ninput video sequence, our goal is to estimate the camera pose (i.e. the camera\nmotion) between consecutive frames. Traditionally, this problem is tackled by\nplacing strict constraints on the transformation vector or by incorporating\noptical flow through a complex pipeline. We propose an alternative approach\nthat utilizes a compositional re-estimation process for camera pose estimation.\nGiven an input, we first estimate a depth map. Our method then iteratively\nestimates the camera motion based on the estimated depth map. Our approach\nsignificantly improves the predicted camera motion both quantitatively and\nvisually. Furthermore, the re-estimation resolves the problem of\nout-of-boundaries pixels in a novel and simple way. Another advantage of our\napproach is that it is adaptable to other camera pose estimation approaches.\nExperimental analysis on KITTI benchmark dataset demonstrates that our method\noutperforms existing state-of-the-art approaches in unsupervised camera\nego-motion estimation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 18:59:07 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Nabavi", "Seyed Shahabeddin", ""], ["Hosseinzadeh", "Mehrdad", ""], ["Fahimi", "Ramin", ""], ["Wang", "Yang", ""]]}, {"id": "2001.06499", "submitter": "Hao Shao", "authors": "Hao Shao, Shengju Qian, Yu Liu", "title": "Temporal Interlacing Network", "comments": "Accepted to AAAI 2020. Winning entry of ICCV Multi-Moments in Time\n  Challenge 2019. Code is available at https://github.com/deepcs233/TIN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a long time, the vision community tries to learn the spatio-temporal\nrepresentation by combining convolutional neural network together with various\ntemporal models, such as the families of Markov chain, optical flow, RNN and\ntemporal convolution. However, these pipelines consume enormous computing\nresources due to the alternately learning process for spatial and temporal\ninformation. One natural question is whether we can embed the temporal\ninformation into the spatial one so the information in the two domains can be\njointly learned once-only. In this work, we answer this question by presenting\na simple yet powerful operator -- temporal interlacing network (TIN). Instead\nof learning the temporal features, TIN fuses the two kinds of information by\ninterlacing spatial representations from the past to the future, and vice\nversa. A differentiable interlacing target can be learned to control the\ninterlacing process. In this way, a heavy temporal model is replaced by a\nsimple interlacing operator. We theoretically prove that with a learnable\ninterlacing target, TIN performs equivalently to the regularized temporal\nconvolution network (r-TCN), but gains 4% more accuracy with 6x less latency on\n6 challenging benchmarks. These results push the state-of-the-art performances\nof video understanding by a considerable margin. Not surprising, the ensemble\nmodel of the proposed TIN won the $1^{st}$ place in the ICCV19 - Multi Moments\nin Time challenge. Code is made available to facilitate further research at\nhttps://github.com/deepcs233/TIN\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 19:06:05 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Shao", "Hao", ""], ["Qian", "Shengju", ""], ["Liu", "Yu", ""]]}, {"id": "2001.06535", "submitter": "Ali Emre Kavur", "authors": "A. Emre Kavur, N. Sinem Gezer, Mustafa Bar{\\i}\\c{s}, Sinem Aslan,\n  Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee, Philipp\n  Ernst, Sava\\c{s} \\\"Ozkan, Bora Baydar, Dmitry Lachinov, Shuo Han, Josef\n  Pauli, Fabian Isensee, Matthias Perkonigg, Rachana Sathish, Ronnie Rajan,\n  Debdoot Sheet, Gurbandurdy Dovletov, Oliver Speck, Andreas N\\\"urnberger,\n  Klaus H. Maier-Hein, G\\\"ozde Bozda\\u{g}{\\i} Akar, G\\\"ozde \\\"Unal, O\\u{g}uz\n  Dicle, M. Alper Selver", "title": "CHAOS Challenge -- Combined (CT-MR) Healthy Abdominal Organ Segmentation", "comments": "23 pages, 11 tables, 9 figures", "journal-ref": "Med. Image Anal. 69 (2021) 101950", "doi": "10.1016/j.media.2020.101950", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of abdominal organs has been a comprehensive, yet unresolved,\nresearch field for many years. In the last decade, intensive developments in\ndeep learning (DL) have introduced new state-of-the-art segmentation systems.\nIn order to expand the knowledge on these topics, the CHAOS - Combined (CT-MR)\nHealthy Abdominal Organ Segmentation challenge has been organized in\nconjunction with IEEE International Symposium on Biomedical Imaging (ISBI),\n2019, in Venice, Italy. CHAOS provides both abdominal CT and MR data from\nhealthy subjects for single and multiple abdominal organ segmentation. Five\ndifferent but complementary tasks have been designed to analyze the\ncapabilities of current approaches from multiple perspectives. The results are\ninvestigated thoroughly, compared with manual annotations and interactive\nmethods. The analysis shows that the performance of DL models for single\nmodality (CT / MR) can show reliable volumetric analysis performance (DICE:\n0.98 $\\pm$ 0.00 / 0.95 $\\pm$ 0.01) but the best MSSD performance remain limited\n(21.89 $\\pm$ 13.94 / 20.85 $\\pm$ 10.63 mm). The performances of participating\nmodels decrease significantly for cross-modality tasks for the liver (DICE:\n0.88 $\\pm$ 0.15 MSSD: 36.33 $\\pm$ 21.97 mm) and all organs (DICE: 0.85 $\\pm$\n0.21 MSSD: 33.17 $\\pm$ 38.93 mm). Despite contrary examples on different\napplications, multi-tasking DL models designed to segment all organs seem to\nperform worse compared to organ-specific ones (performance drop around 5\\%).\nBesides, such directions of further research for cross-modality segmentation\nwould significantly support real-world clinical applications. Moreover, having\nmore than 1500 participants, another important contribution of the paper is the\nanalysis on shortcomings of challenge organizations such as the effects of\nmultiple submissions and peeking phenomena.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 21:11:46 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 10:33:05 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 14:30:59 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Kavur", "A. Emre", ""], ["Gezer", "N. Sinem", ""], ["Bar\u0131\u015f", "Mustafa", ""], ["Aslan", "Sinem", ""], ["Conze", "Pierre-Henri", ""], ["Groza", "Vladimir", ""], ["Pham", "Duc Duy", ""], ["Chatterjee", "Soumick", ""], ["Ernst", "Philipp", ""], ["\u00d6zkan", "Sava\u015f", ""], ["Baydar", "Bora", ""], ["Lachinov", "Dmitry", ""], ["Han", "Shuo", ""], ["Pauli", "Josef", ""], ["Isensee", "Fabian", ""], ["Perkonigg", "Matthias", ""], ["Sathish", "Rachana", ""], ["Rajan", "Ronnie", ""], ["Sheet", "Debdoot", ""], ["Dovletov", "Gurbandurdy", ""], ["Speck", "Oliver", ""], ["N\u00fcrnberger", "Andreas", ""], ["Maier-Hein", "Klaus H.", ""], ["Akar", "G\u00f6zde Bozda\u011f\u0131", ""], ["\u00dcnal", "G\u00f6zde", ""], ["Dicle", "O\u011fuz", ""], ["Selver", "M. Alper", ""]]}, {"id": "2001.06538", "submitter": "Jianhui Chen Mr", "authors": "Lei Chen, Jianhui Chen, Hossein Hajimirsadeghi and Greg Mori", "title": "Adapting Grad-CAM for Embedding Networks", "comments": "WACV 2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gradient-weighted class activation mapping (Grad-CAM) method can\nfaithfully highlight important regions in images for deep model prediction in\nimage classification, image captioning and many other tasks. It uses the\ngradients in back-propagation as weights (grad-weights) to explain network\ndecisions. However, applying Grad-CAM to embedding networks raises significant\nchallenges because embedding networks are trained by millions of dynamically\npaired examples (e.g. triplets). To overcome these challenges, we propose an\nadaptation of the Grad-CAM method for embedding networks. First, we aggregate\ngrad-weights from multiple training examples to improve the stability of\nGrad-CAM. Then, we develop an efficient weight-transfer method to explain\ndecisions for any image without back-propagation. We extensively validate the\nmethod on the standard CUB200 dataset in which our method produces more\naccurate visual attention than the original Grad-CAM method. We also apply the\nmethod to a house price estimation application using images. The method\nproduces convincing qualitative results, showcasing the practicality of our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 21:21:23 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chen", "Lei", ""], ["Chen", "Jianhui", ""], ["Hajimirsadeghi", "Hossein", ""], ["Mori", "Greg", ""]]}, {"id": "2001.06564", "submitter": "Luisa Verdoliva", "authors": "Luisa Verdoliva", "title": "Media Forensics and DeepFakes: an overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid progress of recent years, techniques that generate and\nmanipulate multimedia content can now guarantee a very advanced level of\nrealism. The boundary between real and synthetic media has become very thin. On\nthe one hand, this opens the door to a series of exciting applications in\ndifferent fields such as creative arts, advertising, film production, video\ngames. On the other hand, it poses enormous security threats. Software packages\nfreely available on the web allow any individual, without special skills, to\ncreate very realistic fake images and videos. So-called deepfakes can be used\nto manipulate public opinion during elections, commit fraud, discredit or\nblackmail people. Potential abuses are limited only by human imagination.\nTherefore, there is an urgent need for automated tools capable of detecting\nfalse multimedia content and avoiding the spread of dangerous false\ninformation. This review paper aims to present an analysis of the methods for\nvisual media integrity verification, that is, the detection of manipulated\nimages and videos. Special emphasis will be placed on the emerging phenomenon\nof deepfakes and, from the point of view of the forensic analyst, on modern\ndata-driven forensic methods. The analysis will help to highlight the limits of\ncurrent forensic tools, the most relevant issues, the upcoming challenges, and\nsuggest future directions for research.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 00:13:32 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Verdoliva", "Luisa", ""]]}, {"id": "2001.06570", "submitter": "Matej Ulicny", "authors": "Matej Ulicny, Vladimir A. Krylov, Rozenn Dahyot", "title": "Harmonic Convolutional Networks based on Discrete Cosine Transform", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.03205", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) learn filters in order to capture local\ncorrelation patterns in feature space. We propose to learn these filters as\ncombinations of preset spectral filters defined by the Discrete Cosine\nTransform (DCT). Our proposed DCT-based harmonic blocks replace conventional\nconvolutional layers to produce partially or fully harmonic versions of new or\nexisting CNN architectures. Using DCT energy compaction properties, we\ndemonstrate how the harmonic networks can be efficiently compressed by\ntruncating high-frequency information in harmonic blocks thanks to the\nredundancies in the spectral domain. We report extensive experimental\nvalidation demonstrating benefits of the introduction of harmonic blocks into\nstate-of-the-art CNN models in image classification, object detection and\nsemantic segmentation applications.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 01:13:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 16:31:40 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Ulicny", "Matej", ""], ["Krylov", "Vladimir A.", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "2001.06580", "submitter": "Lirong Wu", "authors": "Lirong Wu, Kejie Huang and Haibin Shen", "title": "A GAN-based Tunable Image Compression System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of importance map has been widely adopted in DNN-based lossy image\ncompression to achieve bit allocation according to the importance of image\ncontents. However, insufficient allocation of bits in non-important regions\noften leads to severe distortion at low bpp (bits per pixel), which hampers the\ndevelopment of efficient content-weighted image compression systems. This paper\nrethinks content-based compression by using Generative Adversarial Network\n(GAN) to reconstruct the non-important regions. Moreover, multiscale pyramid\ndecomposition is applied to both the encoder and the discriminator to achieve\nglobal compression of high-resolution images. A tunable compression scheme is\nalso proposed in this paper to compress an image to any specific compression\nratio without retraining the model. The experimental results show that our\nproposed method improves MS-SSIM by more than 10.3% compared to the recently\nreported GAN-based method to achieve the same low bpp (0.05) on the Kodak\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 02:40:09 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wu", "Lirong", ""], ["Huang", "Kejie", ""], ["Shen", "Haibin", ""]]}, {"id": "2001.06590", "submitter": "Lirong Wu", "authors": "Lirong Wu, Kejie Huang, Haibin Shen and Lianli Gao", "title": "A Foreground-background Parallel Compression with Residual Encoding for\n  Surveillance Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data storage has been one of the bottlenecks in surveillance systems. The\nconventional video compression algorithms such as H.264 and H.265 do not fully\nutilize the low information density characteristic of the surveillance video.\nIn this paper, we propose a video compression method that extracts and\ncompresses the foreground and background of the video separately. The\ncompression ratio is greatly improved by sharing background information among\nmultiple adjacent frames through an adaptive background updating and\ninterpolation module. Besides, we present two different schemes to compress the\nforeground and compare their performance in the ablation study to show the\nimportance of temporal information for video compression. In the decoding end,\na coarse-to-fine two-stage module is applied to achieve the composition of the\nforeground and background and the enhancements of frame quality. Furthermore,\nan adaptive sampling method for surveillance cameras is proposed, and we have\nshown its effects through software simulation. The experimental results show\nthat our proposed method requires 69.5% less bpp (bits per pixel) than the\nconventional algorithm H.265 to achieve the same PSNR (36 dB) on the HECV\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 03:35:00 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 05:22:11 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 08:49:06 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wu", "Lirong", ""], ["Huang", "Kejie", ""], ["Shen", "Haibin", ""], ["Gao", "Lianli", ""]]}, {"id": "2001.06612", "submitter": "Pedro Marrero", "authors": "Pedro D. Marrero Fernandez, Tsang Ing Ren, Tsang Ing Jyh, Fidel A.\n  Guerrero Pe\\~na, Alexandre Cunha", "title": "Deep Metric Structured Learning For Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep metric learning model to create embedded sub-spaces with a\nwell defined structure. A new loss function that imposes Gaussian structures on\nthe output space is introduced to create these sub-spaces thus shaping the\ndistribution of the data. Having a mixture of Gaussians solution space is\nadvantageous given its simplified and well established structure. It allows\nfast discovering of classes within classes and the identification of mean\nrepresentatives at the centroids of individual classes. We also propose a new\nsemi-supervised method to create sub-classes. We illustrate our methods on the\nfacial expression recognition problem and validate results on the FER+,\nAffectNet, Extended Cohn-Kanade (CK+), BU-3DFE, and JAFFE datasets. We\nexperimentally demonstrate that the learned embedding can be successfully used\nfor various applications including expression retrieval and emotion\nrecognition.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 06:23:18 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Fernandez", "Pedro D. Marrero", ""], ["Ren", "Tsang Ing", ""], ["Jyh", "Tsang Ing", ""], ["Pe\u00f1a", "Fidel A. Guerrero", ""], ["Cunha", "Alexandre", ""]]}, {"id": "2001.06613", "submitter": "Hari Om Aggrawal", "authors": "Hari Om Aggrawal, Jan Modersitzki", "title": "Accelerating the Registration of Image Sequences by Spatio-temporal\n  Multilevel Strategies", "comments": "Accepted at ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel strategies are an integral part of many image registration\nalgorithms. These strategies are very well-known for avoiding undesirable local\nminima, providing an outstanding initial guess, and reducing overall\ncomputation time. State-of-the-art multilevel strategies build a hierarchy of\ndiscretization in the spatial dimensions. In this paper, we present a\nspatio-temporal strategy, where we introduce a hierarchical discretization in\nthe temporal dimension at each spatial level. This strategy is suitable for a\nmotion estimation problem where the motion is assumed smooth over time. Our\nstrategy exploits the temporal smoothness among image frames by following a\npredictor-corrector approach. The strategy predicts the motion by a novel\ninterpolation method and later corrects it by registration. The prediction step\nprovides a good initial guess for the correction step, hence reduces the\noverall computational time for registration. The acceleration is achieved by a\nfactor of 2.5 on average, over the state-of-the-art multilevel methods on three\nexamined optical coherence tomography datasets.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 06:46:16 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Aggrawal", "Hari Om", ""], ["Modersitzki", "Jan", ""]]}, {"id": "2001.06640", "submitter": "Shuo Wang", "authors": "Shuo Wang, Tianle Chen, Shangyu Chen, Carsten Rudolph, Surya Nepal,\n  Marthie Grobler", "title": "OIAD: One-for-all Image Anomaly Detection with Disentanglement Learning", "comments": "arXiv admin note: text overlap with arXiv:1802.05983,\n  arXiv:1909.02755, arXiv:1804.03599 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection aims to recognize samples with anomalous and unusual\npatterns with respect to a set of normal data. This is significant for numerous\ndomain applications, such as industrial inspection, medical imaging, and\nsecurity enforcement. There are two key research challenges associated with\nexisting anomaly detection approaches: (1) many approaches perform well on\nlow-dimensional problems however the performance on high-dimensional instances,\nsuch as images, is limited; (2) many approaches often rely on traditional\nsupervised approaches and manual engineering of features, while the topic has\nnot been fully explored yet using modern deep learning approaches, even when\nthe well-label samples are limited. In this paper, we propose a One-for-all\nImage Anomaly Detection system (OIAD) based on disentangled learning using only\nclean samples. Our key insight is that the impact of small perturbation on the\nlatent representation can be bounded for normal samples while anomaly images\nare usually outside such bounded intervals, referred to as structure\nconsistency. We implement this idea and evaluate its performance for anomaly\ndetection. Our experiments with three datasets show that OIAD can detect over\n$90\\%$ of anomalies while maintaining a low false alarm rate. It can also\ndetect suspicious samples from samples labeled as clean, coincided with what\nhumans would deem unusual.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 09:57:37 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 09:00:14 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Wang", "Shuo", ""], ["Chen", "Tianle", ""], ["Chen", "Shangyu", ""], ["Rudolph", "Carsten", ""], ["Nepal", "Surya", ""], ["Grobler", "Marthie", ""]]}, {"id": "2001.06657", "submitter": "Vinay Verma Kumar", "authors": "Anubha Pandey, Ashish Mishra, Vinay Kumar Verma, Anurag Mittal and\n  Hema A. Murthy", "title": "Stacked Adversarial Network for Zero-Shot Sketch based Image Retrieval", "comments": "Accepted in WACV'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches to Sketch-Based Image Retrieval (SBIR) assume that\nthe data of all the classes are available during training. The assumption may\nnot always be practical since the data of a few classes may be unavailable, or\nthe classes may not appear at the time of training. Zero-Shot Sketch-Based\nImage Retrieval (ZS-SBIR) relaxes this constraint and allows the algorithm to\nhandle previously unseen classes during the test. This paper proposes a\ngenerative approach based on the Stacked Adversarial Network (SAN) and the\nadvantage of Siamese Network (SN) for ZS-SBIR. While SAN generates a\nhigh-quality sample, SN learns a better distance metric compared to that of the\nnearest neighbor search. The capability of the generative model to synthesize\nimage features based on the sketch reduces the SBIR problem to that of an\nimage-to-image retrieval problem. We evaluate the efficacy of our proposed\napproach on TU-Berlin, and Sketchy database in both standard ZSL and\ngeneralized ZSL setting. The proposed method yields a significant improvement\nin standard ZSL as well as in a more challenging generalized ZSL setting (GZSL)\nfor SBIR.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 12:18:28 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Pandey", "Anubha", ""], ["Mishra", "Ashish", ""], ["Verma", "Vinay Kumar", ""], ["Mittal", "Anurag", ""], ["Murthy", "Hema A.", ""]]}, {"id": "2001.06658", "submitter": "Shakeeb Murtaza", "authors": "Tehseen Zia, Shahan Arif, Shakeeb Murtaza, and Mirza Ahsan Ullah", "title": "Text-to-Image Generation with Attention Based Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional image modeling based on textual descriptions is a relatively new\ndomain in unsupervised learning. Previous approaches use a latent variable\nmodel and generative adversarial networks. While the formers are approximated\nby using variational auto-encoders and rely on the intractable inference that\ncan hamper their performance, the latter is unstable to train due to Nash\nequilibrium based objective function. We develop a tractable and stable\ncaption-based image generation model. The model uses an attention-based encoder\nto learn word-to-pixel dependencies. A conditional autoregressive based decoder\nis used for learning pixel-to-pixel dependencies and generating images.\nExperimentations are performed on Microsoft COCO, and MNIST-with-captions\ndatasets and performance is evaluated by using the Structural Similarity Index.\nResults show that the proposed model performs better than contemporary\napproaches and generate better quality images. Keywords: Generative image\nmodeling, autoregressive image modeling, caption-based image generation, neural\nattention, recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 12:19:19 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zia", "Tehseen", ""], ["Arif", "Shahan", ""], ["Murtaza", "Shakeeb", ""], ["Ullah", "Mirza Ahsan", ""]]}, {"id": "2001.06659", "submitter": "Min Li", "authors": "Min Li, Zhenglong Zhou, Zhe Wu, Boxin Shi, Changyu Diao, and Ping Tan", "title": "Multi-View Photometric Stereo: A Robust Solution and Benchmark Dataset\n  for Spatially Varying Isotropic Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to capture both 3D shape and spatially varying\nreflectance with a multi-view photometric stereo (MVPS) technique that works\nfor general isotropic materials. Our algorithm is suitable for perspective\ncameras and nearby point light sources. Our data capture setup is simple, which\nconsists of only a digital camera, some LED lights, and an optional automatic\nturntable. From a single viewpoint, we use a set of photometric stereo images\nto identify surface points with the same distance to the camera. We collect\nthis information from multiple viewpoints and combine it with\nstructure-from-motion to obtain a precise reconstruction of the complete 3D\nshape. The spatially varying isotropic bidirectional reflectance distribution\nfunction (BRDF) is captured by simultaneously inferring a set of basis BRDFs\nand their mixing weights at each surface point. In experiments, we demonstrate\nour algorithm with two different setups: a studio setup for highest precision\nand a desktop setup for best usability. According to our experiments, under the\nstudio setting, the captured shapes are accurate to 0.5 millimeters and the\ncaptured reflectance has a relative root-mean-square error (RMSE) of 9%. We\nalso quantitatively evaluate state-of-the-art MVPS on a newly collected\nbenchmark dataset, which is publicly available for inspiring future research.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 12:26:22 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Li", "Min", ""], ["Zhou", "Zhenglong", ""], ["Wu", "Zhe", ""], ["Shi", "Boxin", ""], ["Diao", "Changyu", ""], ["Tan", "Ping", ""]]}, {"id": "2001.06673", "submitter": "Shuang Lu", "authors": "Pietro Falco, Shuang Lu, Ciro Natale, Salvatore Pirozzi, and Dongheui\n  Lee", "title": "A Transfer Learning Approach to Cross-Modal Object Recognition: From\n  Visual Observation to Robotic Haptic Exploration", "comments": null, "journal-ref": "IEEE Transactions on Robotics ( Volume: 35 , Issue: 4 , Aug. 2019\n  )", "doi": "10.1109/TRO.2019.2914772", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce the problem of cross-modal visuo-tactile object\nrecognition with robotic active exploration. With this term, we mean that the\nrobot observes a set of objects with visual perception and, later on, it is\nable to recognize such objects only with tactile exploration, without having\ntouched any object before. Using a machine learning terminology, in our\napplication we have a visual training set and a tactile test set, or vice\nversa. To tackle this problem, we propose an approach constituted by four\nsteps: finding a visuo-tactile common representation, defining a suitable set\nof features, transferring the features across the domains, and classifying the\nobjects. We show the results of our approach using a set of 15 objects,\ncollecting 40 visual examples and five tactile examples for each object. The\nproposed approach achieves an accuracy of 94.7%, which is comparable with the\naccuracy of the monomodal case, i.e., when using visual data both as training\nset and test set. Moreover, it performs well compared to the human ability,\nwhich we have roughly estimated carrying out an experiment with ten\nparticipants.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 14:47:02 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Falco", "Pietro", ""], ["Lu", "Shuang", ""], ["Natale", "Ciro", ""], ["Pirozzi", "Salvatore", ""], ["Lee", "Dongheui", ""]]}, {"id": "2001.06678", "submitter": "Jiahong Wei", "authors": "Zhun Fan, Jiahong Wei, Guijie Zhu, Jiajie Mo, Wenji Li", "title": "Evolutionary Neural Architecture Search for Retinal Vessel Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate retinal vessel segmentation (RVS) is of great significance to\nassist doctors in the diagnosis of ophthalmology diseases and other systemic\ndiseases. Manually designing a valid neural network architecture for retinal\nvessel segmentation requires high expertise and a large workload. In order to\nimprove the performance of vessel segmentation and reduce the workload of\nmanually designing neural network, we propose novel approach which applies\nneural architecture search (NAS) to optimize an encoder-decoder architecture\nfor retinal vessel segmentation. A modified evolutionary algorithm is used to\nevolve the architectures of encoder-decoder framework with limited computing\nresources. The evolved model obtained by the proposed approach achieves top\nperformance among all compared methods on the three datasets, namely DRIVE,\nSTARE and CHASE_DB1, but with much fewer parameters. Moreover, the results of\ncross-training show that the evolved model is with considerable scalability,\nwhich indicates a great potential for clinical disease diagnosis.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 15:07:26 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 15:43:55 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 05:58:25 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Fan", "Zhun", ""], ["Wei", "Jiahong", ""], ["Zhu", "Guijie", ""], ["Mo", "Jiajie", ""], ["Li", "Wenji", ""]]}, {"id": "2001.06680", "submitter": "Guanbin Li", "authors": "Jie Wu, Guanbin Li, Si Liu, Liang Lin", "title": "Tree-Structured Policy based Progressive Reinforcement Learning for\n  Temporally Language Grounding in Video", "comments": "To appear in AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporally language grounding in untrimmed videos is a newly-raised task in\nvideo understanding. Most of the existing methods suffer from inferior\nefficiency, lacking interpretability, and deviating from the human perception\nmechanism. Inspired by human's coarse-to-fine decision-making paradigm, we\nformulate a novel Tree-Structured Policy based Progressive Reinforcement\nLearning (TSP-PRL) framework to sequentially regulate the temporal boundary by\nan iterative refinement process. The semantic concepts are explicitly\nrepresented as the branches in the policy, which contributes to efficiently\ndecomposing complex policies into an interpretable primitive action.\nProgressive reinforcement learning provides correct credit assignment via two\ntask-oriented rewards that encourage mutual promotion within the\ntree-structured policy. We extensively evaluate TSP-PRL on the Charades-STA and\nActivityNet datasets, and experimental results show that TSP-PRL achieves\ncompetitive performance over existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 15:08:04 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wu", "Jie", ""], ["Li", "Guanbin", ""], ["Liu", "Si", ""], ["Lin", "Liang", ""]]}, {"id": "2001.06690", "submitter": "Yanwei Pang", "authors": "Yazhao Li, Yanwei Pang, Jianbing Shen, Jiale Cao, Ling Shao", "title": "NETNet: Neighbor Erasing and Transferring Network for Better Single Shot\n  Object Detection", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advantages of real-time detection and improved performance,\nsingle-shot detectors have gained great attention recently. To solve the\ncomplex scale variations, single-shot detectors make scale-aware predictions\nbased on multiple pyramid layers. However, the features in the pyramid are not\nscale-aware enough, which limits the detection performance. Two common problems\nin single-shot detectors caused by object scale variations can be observed: (1)\nsmall objects are easily missed; (2) the salient part of a large object is\nsometimes detected as an object. With this observation, we propose a new\nNeighbor Erasing and Transferring (NET) mechanism to reconfigure the pyramid\nfeatures and explore scale-aware features. In NET, a Neighbor Erasing Module\n(NEM) is designed to erase the salient features of large objects and emphasize\nthe features of small objects in shallow layers. A Neighbor Transferring Module\n(NTM) is introduced to transfer the erased features and highlight large objects\nin deep layers. With this mechanism, a single-shot network called NETNet is\nconstructed for scale-aware object detection. In addition, we propose to\naggregate nearest neighboring pyramid features to enhance our NET. NETNet\nachieves 38.5% AP at a speed of 27 FPS and 32.0% AP at a speed of 55 FPS on MS\nCOCO dataset. As a result, NETNet achieves a better trade-off for real-time and\naccurate object detection.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 15:21:29 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Li", "Yazhao", ""], ["Pang", "Yanwei", ""], ["Shen", "Jianbing", ""], ["Cao", "Jiale", ""], ["Shao", "Ling", ""]]}, {"id": "2001.06769", "submitter": "Kaiyu Shan", "authors": "Kaiyu Shan, Yongtao Wang, Zhuoying Wang, Tingting Liang, Zhi Tang,\n  Ying Chen, and Yangyan Li", "title": "MixTConv: Mixed Temporal Convolutional Kernels for Efficient Action\n  Recogntion", "comments": "None", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To efficiently extract spatiotemporal features of video for action\nrecognition, most state-of-the-art methods integrate 1D temporal convolution\ninto a conventional 2D CNN backbone. However, they all exploit 1D temporal\nconvolution of fixed kernel size (i.e., 3) in the network building block, thus\nhave suboptimal temporal modeling capability to handle both long-term and\nshort-term actions. To address this problem, we first investigate the impacts\nof different kernel sizes for the 1D temporal convolutional filters. Then, we\npropose a simple yet efficient operation called Mixed Temporal Convolution\n(MixTConv), which consists of multiple depthwise 1D convolutional filters with\ndifferent kernel sizes. By plugging MixTConv into the conventional 2D CNN\nbackbone ResNet-50, we further propose an efficient and effective network\narchitecture named MSTNet for action recognition, and achieve state-of-the-art\nresults on multiple benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 04:21:51 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 15:51:20 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 03:40:31 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Shan", "Kaiyu", ""], ["Wang", "Yongtao", ""], ["Wang", "Zhuoying", ""], ["Liang", "Tingting", ""], ["Tang", "Zhi", ""], ["Chen", "Ying", ""], ["Li", "Yangyan", ""]]}, {"id": "2001.06774", "submitter": "Hui Zhu", "authors": "Hui Zhu, Zhulin An, Kaiqiang Xu, Xiaolong Hu, Yongjun Xu", "title": "Towards More Efficient and Effective Inference: The Joint Decision of\n  Multi-Participants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to improve the performances of convolutional neural\nnetworks by optimizing the local architectures or deepening the networks tend\nto increase the size of models significantly. In order to deploy and apply the\nneural networks to edge devices which are in great demand, reducing the scale\nof networks are quite crucial. However, It is easy to degrade the performance\nof image processing by compressing the networks. In this paper, we propose a\nmethod which is suitable for edge devices while improving the efficiency and\neffectiveness of inference. The joint decision of multi-participants, mainly\ncontain multi-layers and multi-networks, can achieve higher classification\naccuracy (0.26% on CIFAR-10 and 4.49% on CIFAR-100 at most) with similar total\nnumber of parameters for classical convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 04:52:05 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zhu", "Hui", ""], ["An", "Zhulin", ""], ["Xu", "Kaiqiang", ""], ["Hu", "Xiaolong", ""], ["Xu", "Yongjun", ""]]}, {"id": "2001.06780", "submitter": "Canhong Wen", "authors": "Quan Xiao, Canhong Wen, Zirui Yan", "title": "Image denoising via K-SVD with primal-dual active set algorithm", "comments": "9 pages, 6 figures. The paper was accepted by IEEE. WACV 2020 and\n  will placed in the IEEE Xplore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-SVD algorithm has been successfully applied to image denoising tasks dozens\nof years but the big bottleneck in speed and accuracy still needs attention to\nbreak. For the sparse coding stage in K-SVD, which involves $\\ell_{0}$\nconstraint, prevailing methods usually seek approximate solutions greedily but\nare less effective once the noise level is high. The alternative $\\ell_{1}$\noptimization is proved to be powerful than $\\ell_{0}$, however, the time\nconsumption prevents it from the implementation. In this paper, we propose a\nnew K-SVD framework called K-SVD$_P$ by applying the Primal-dual active set\n(PDAS) algorithm to it. Different from the greedy algorithms based K-SVD, the\nK-SVD$_P$ algorithm develops a selection strategy motivated by KKT\n(Karush-Kuhn-Tucker) condition and yields to an efficient update in the sparse\ncoding stage. Since the K-SVD$_P$ algorithm seeks for an equivalent solution to\nthe dual problem iteratively with simple explicit expression in this denoising\nproblem, speed and quality of denoising can be reached simultaneously.\nExperiments are carried out and demonstrate the comparable denoising\nperformance of our K-SVD$_P$ with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 06:03:39 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Xiao", "Quan", ""], ["Wen", "Canhong", ""], ["Yan", "Zirui", ""]]}, {"id": "2001.06782", "submitter": "Tianhe Yu", "authors": "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol\n  Hausman, Chelsea Finn", "title": "Gradient Surgery for Multi-Task Learning", "comments": "NeurIPS 2020. Code is available at\n  https://github.com/tianheyu927/PCGrad", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning and deep reinforcement learning (RL) systems have\ndemonstrated impressive results in domains such as image classification, game\nplaying, and robotic control, data efficiency remains a major challenge.\nMulti-task learning has emerged as a promising approach for sharing structure\nacross multiple tasks to enable more efficient learning. However, the\nmulti-task setting presents a number of optimization challenges, making it\ndifficult to realize large efficiency gains compared to learning tasks\nindependently. The reasons why multi-task learning is so challenging compared\nto single-task learning are not fully understood. In this work, we identify a\nset of three conditions of the multi-task optimization landscape that cause\ndetrimental gradient interference, and develop a simple yet general approach\nfor avoiding such interference between task gradients. We propose a form of\ngradient surgery that projects a task's gradient onto the normal plane of the\ngradient of any other task that has a conflicting gradient. On a series of\nchallenging multi-task supervised and multi-task RL problems, this approach\nleads to substantial gains in efficiency and performance. Further, it is\nmodel-agnostic and can be combined with previously-proposed multi-task\narchitectures for enhanced performance.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 06:33:47 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 23:41:08 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 06:07:19 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2020 00:35:46 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Yu", "Tianhe", ""], ["Kumar", "Saurabh", ""], ["Gupta", "Abhishek", ""], ["Levine", "Sergey", ""], ["Hausman", "Karol", ""], ["Finn", "Chelsea", ""]]}, {"id": "2001.06804", "submitter": "Wenguan Wang", "authors": "Wenguan Wang, Zhijie Zhang, Siyuan Qi, Jianbing Shen, Yanwei Pang, and\n  Ling Shao", "title": "Learning Compositional Neural Information Fusion for Human Parsing", "comments": "ICCV2019. Websie:\n  https://github.com/ZzzjzzZ/CompositionalHumanParsing", "journal-ref": "ICCV2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes to combine neural networks with the compositional\nhierarchy of human bodies for efficient and complete human parsing. We\nformulate the approach as a neural information fusion framework. Our model\nassembles the information from three inference processes over the hierarchy:\ndirect inference (directly predicting each part of a human body using image\ninformation), bottom-up inference (assembling knowledge from constituent\nparts), and top-down inference (leveraging context from parent nodes). The\nbottom-up and top-down inferences explicitly model the compositional and\ndecompositional relations in human bodies, respectively. In addition, the\nfusion of multi-source information is conditioned on the inputs, i.e., by\nestimating and considering the confidence of the sources. The whole model is\nend-to-end differentiable, explicitly modeling information flows and\nstructures. Our approach is extensively evaluated on four popular datasets,\noutperforming the state-of-the-arts in all cases, with a fast processing speed\nof 23fps. Our code and results have been released to help ease future research\nin this direction.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 10:35:14 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Wenguan", ""], ["Zhang", "Zhijie", ""], ["Qi", "Siyuan", ""], ["Shen", "Jianbing", ""], ["Pang", "Yanwei", ""], ["Shao", "Ling", ""]]}, {"id": "2001.06807", "submitter": "Wenguan Wang", "authors": "Wenguan Wang, Xiankai Lu, Jianbing Shen, David Crandall, and Ling Shao", "title": "Zero-Shot Video Object Segmentation via Attentive Graph Neural Networks", "comments": "ICCV2019(Oral). Website: https://github.com/carrierlxk/AGNN", "journal-ref": "ICCV2019(Oral)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel attentive graph neural network (AGNN) for\nzero-shot video object segmentation (ZVOS). The suggested AGNN recasts this\ntask as a process of iterative information fusion over video graphs.\nSpecifically, AGNN builds a fully connected graph to efficiently represent\nframes as nodes, and relations between arbitrary frame pairs as edges. The\nunderlying pair-wise relations are described by a differentiable attention\nmechanism. Through parametric message passing, AGNN is able to efficiently\ncapture and mine much richer and higher-order relations between video frames,\nthus enabling a more complete understanding of video content and more accurate\nforeground estimation. Experimental results on three video segmentation\ndatasets show that AGNN sets a new state-of-the-art in each case. To further\ndemonstrate the generalizability of our framework, we extend AGNN to an\nadditional task: image object co-segmentation (IOCS). We perform experiments on\ntwo famous IOCS datasets and observe again the superiority of our AGNN model.\nThe extensive experiments verify that AGNN is able to learn the underlying\nsemantic/appearance relationships among video frames or related images, and\ndiscover the common objects.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 10:45:27 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Wenguan", ""], ["Lu", "Xiankai", ""], ["Shen", "Jianbing", ""], ["Crandall", "David", ""], ["Shao", "Ling", ""]]}, {"id": "2001.06810", "submitter": "Wenguan Wang", "authors": "Xiankai Lu, Wenguan Wang, Chao Ma, Jianbing Shen, Ling Shao, and Fatih\n  Porikli", "title": "See More, Know More: Unsupervised Video Object Segmentation with\n  Co-Attention Siamese Networks", "comments": "CVPR2019. Weblink: https://github.com/carrierlxk/COSNet", "journal-ref": "CVPR2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel network, called CO-attention Siamese Network (COSNet),\nto address the unsupervised video object segmentation task from a holistic\nview. We emphasize the importance of inherent correlation among video frames\nand incorporate a global co-attention mechanism to improve further the\nstate-of-the-art deep learning based solutions that primarily focus on learning\ndiscriminative foreground representations over appearance and motion in\nshort-term temporal segments. The co-attention layers in our network provide\nefficient and competent stages for capturing global correlations and scene\ncontext by jointly computing and appending co-attention responses into a joint\nfeature space. We train COSNet with pairs of video frames, which naturally\naugments training data and allows increased learning capacity. During the\nsegmentation stage, the co-attention model encodes useful information by\nprocessing multiple reference frames together, which is leveraged to infer the\nfrequently reappearing and salient foreground objects better. We propose a\nunified and end-to-end trainable framework where different co-attention\nvariants can be derived for mining the rich context within videos. Our\nextensive experiments over three large benchmarks manifest that COSNet\noutperforms the current alternatives by a large margin.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 11:10:39 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Lu", "Xiankai", ""], ["Wang", "Wenguan", ""], ["Ma", "Chao", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""], ["Porikli", "Fatih", ""]]}, {"id": "2001.06812", "submitter": "Shizhen Zhao", "authors": "Shizhen Zhao, Changxin Gao, Yuanjie Shao, Lerenhan Li, Changqian Yu,\n  Zhong Ji, Nong Sang", "title": "GTNet: Generative Transfer Network for Zero-Shot Object Detection", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Generative Transfer Network (GTNet) for zero shot object\ndetection (ZSD). GTNet consists of an Object Detection Module and a Knowledge\nTransfer Module. The Object Detection Module can learn large-scale seen domain\nknowledge. The Knowledge Transfer Module leverages a feature synthesizer to\ngenerate unseen class features, which are applied to train a new classification\nlayer for the Object Detection Module. In order to synthesize features for each\nunseen class with both the intra-class variance and the IoU variance, we design\nan IoU-Aware Generative Adversarial Network (IoUGAN) as the feature\nsynthesizer, which can be easily integrated into GTNet. Specifically, IoUGAN\nconsists of three unit models: Class Feature Generating Unit (CFU), Foreground\nFeature Generating Unit (FFU), and Background Feature Generating Unit (BFU).\nCFU generates unseen features with the intra-class variance conditioned on the\nclass semantic embeddings. FFU and BFU add the IoU variance to the results of\nCFU, yielding class-specific foreground and background features, respectively.\nWe evaluate our method on three public datasets and the results demonstrate\nthat our method performs favorably against the state-of-the-art ZSD approaches.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 11:49:30 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 13:27:36 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Zhao", "Shizhen", ""], ["Gao", "Changxin", ""], ["Shao", "Yuanjie", ""], ["Li", "Lerenhan", ""], ["Yu", "Changqian", ""], ["Ji", "Zhong", ""], ["Sang", "Nong", ""]]}, {"id": "2001.06816", "submitter": "Ziyi Shen", "authors": "Ziyi Shen, Wenguan Wang, Xiankai Lu, Jianbing Shen, Haibin Ling,\n  Tingfa Xu, and Ling Shao", "title": "Human-Aware Motion Deblurring", "comments": "ICCV2019 paper. Website: https://github.com/joanshen0508/HA_deblur", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a human-aware deblurring model that disentangles the\nmotion blur between foreground (FG) humans and background (BG). The proposed\nmodel is based on a triple-branch encoder-decoder architecture. The first two\nbranches are learned for sharpening FG humans and BG details, respectively;\nwhile the third one produces global, harmonious results by comprehensively\nfusing multi-scale deblurring information from the two domains. The proposed\nmodel is further endowed with a supervised, human-aware attention mechanism in\nan end-to-end fashion. It learns a soft mask that encodes FG human information\nand explicitly drives the FG/BG decoder-branches to focus on their specific\ndomains. To further benefit the research towards Human-aware Image Deblurring,\nwe introduce a large-scale dataset, named HIDE, which consists of 8,422 blurry\nand sharp image pairs with 65,784 densely annotated FG human bounding boxes.\nHIDE is specifically built to span a broad range of scenes, human object sizes,\nmotion patterns, and background complexities. Extensive experiments on public\nbenchmarks and our dataset demonstrate that our model performs favorably\nagainst the state-of-the-art motion deblurring methods, especially in capturing\nsemantic details.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 12:16:39 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Shen", "Ziyi", ""], ["Wang", "Wenguan", ""], ["Lu", "Xiankai", ""], ["Shen", "Jianbing", ""], ["Ling", "Haibin", ""], ["Xu", "Tingfa", ""], ["Shao", "Ling", ""]]}, {"id": "2001.06819", "submitter": "Hong Zhang", "authors": "Qichuan Geng, Hong Zhang, Xiaojuan Qi, Ruigang Yang, Zhong Zhou, Gao\n  Huang", "title": "Gated Path Selection Network for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3046921", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a challenging task that needs to handle large scale\nvariations, deformations and different viewpoints. In this paper, we develop a\nnovel network named Gated Path Selection Network (GPSNet), which aims to learn\nadaptive receptive fields. In GPSNet, we first design a two-dimensional\nmulti-scale network - SuperNet, which densely incorporates features from\ngrowing receptive fields. To dynamically select desirable semantic context, a\ngate prediction module is further introduced. In contrast to previous works\nthat focus on optimizing sample positions on the regular grids, GPSNet can\nadaptively capture free form dense semantic contexts. The derived adaptive\nreceptive fields are data-dependent, and are flexible that can model different\nobject geometric transformations. On two representative semantic segmentation\ndatasets, i.e., Cityscapes, and ADE20K, we show that the proposed approach\nconsistently outperforms previous methods and achieves competitive performance\nwithout bells and whistles.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 12:32:17 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Geng", "Qichuan", ""], ["Zhang", "Hong", ""], ["Qi", "Xiaojuan", ""], ["Yang", "Ruigang", ""], ["Zhou", "Zhong", ""], ["Huang", "Gao", ""]]}, {"id": "2001.06822", "submitter": "Ziyi Shen", "authors": "Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, and Ming-Hsuan Yang", "title": "Exploiting Semantics for Face Image Deblurring", "comments": "Submitted to International Journal of Computer Vision (IJCV). arXiv\n  admin note: text overlap with arXiv:1803.03345", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective and efficient face deblurring\nalgorithm by exploiting semantic cues via deep convolutional neural networks.\nAs the human faces are highly structured and share unified facial components\n(e.g., eyes and mouths), such semantic information provides a strong prior for\nrestoration. We incorporate face semantic labels as input priors and propose an\nadaptive structural loss to regularize facial local structures within an\nend-to-end deep convolutional neural network. Specifically, we first use a\ncoarse deblurring network to reduce the motion blur on the input face image. We\nthen adopt a parsing network to extract the semantic features from the coarse\ndeblurred image. Finally, the fine deblurring network utilizes the semantic\ninformation to restore a clear face image. We train the network with perceptual\nand adversarial losses to generate photo-realistic results. The proposed method\nrestores sharp images with more accurate facial features and details.\nQuantitative and qualitative evaluations demonstrate that the proposed face\ndeblurring algorithm performs favorably against the state-of-the-art methods in\nterms of restoration quality, face recognition and execution speed.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 13:06:27 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 06:00:17 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Shen", "Ziyi", ""], ["Lai", "Wei-Sheng", ""], ["Xu", "Tingfa", ""], ["Kautz", "Jan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2001.06823", "submitter": "David Morris", "authors": "David Morris, Eric M\\\"uller-Budack, Ralph Ewerth", "title": "SlideImages: A Dataset for Educational Image Classification", "comments": "8 pages, 2 figures, to be presented at ECIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, convolutional neural networks (CNNs) have achieved\nimpressive results in computer vision tasks, which however mainly focus on\nphotos with natural scene content. Besides, non-sensor derived images such as\nillustrations, data visualizations, figures, etc. are typically used to convey\ncomplex information or to explore large datasets. However, this kind of images\nhas received little attention in computer vision. CNNs and similar techniques\nuse large volumes of training data. Currently, many document analysis systems\nare trained in part on scene images due to the lack of large datasets of\neducational image data. In this paper, we address this issue and present\nSlideImages, a dataset for the task of classifying educational illustrations.\nSlideImages contains training data collected from various sources, e.g.,\nWikimedia Commons and the AI2D dataset, and test data collected from\neducational slides. We have reserved all the actual educational images as a\ntest dataset in order to ensure that the approaches using this dataset\ngeneralize well to new educational images, and potentially other domains.\nFurthermore, we present a baseline system using a standard deep neural\narchitecture and discuss dealing with the challenge of limited training data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 13:11:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Morris", "David", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2001.06826", "submitter": "Chongyi Li", "authors": "Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam\n  Kwong, and Runmin Cong", "title": "Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement", "comments": "10 pages", "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel method, Zero-Reference Deep Curve Estimation\n(Zero-DCE), which formulates light enhancement as a task of image-specific\ncurve estimation with a deep network. Our method trains a lightweight deep\nnetwork, DCE-Net, to estimate pixel-wise and high-order curves for dynamic\nrange adjustment of a given image. The curve estimation is specially designed,\nconsidering pixel value range, monotonicity, and differentiability. Zero-DCE is\nappealing in its relaxed assumption on reference images, i.e., it does not\nrequire any paired or unpaired data during training. This is achieved through a\nset of carefully formulated non-reference loss functions, which implicitly\nmeasure the enhancement quality and drive the learning of the network. Our\nmethod is efficient as image enhancement can be achieved by an intuitive and\nsimple nonlinear curve mapping. Despite its simplicity, we show that it\ngeneralizes well to diverse lighting conditions. Extensive experiments on\nvarious benchmarks demonstrate the advantages of our method over\nstate-of-the-art methods qualitatively and quantitatively. Furthermore, the\npotential benefits of our Zero-DCE to face detection in the dark are discussed.\nCode and model will be available at https://github.com/Li-Chongyi/Zero-DCE.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 13:49:15 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 09:49:06 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Guo", "Chunle", ""], ["Li", "Chongyi", ""], ["Guo", "Jichang", ""], ["Loy", "Chen Change", ""], ["Hou", "Junhui", ""], ["Kwong", "Sam", ""], ["Cong", "Runmin", ""]]}, {"id": "2001.06838", "submitter": "Ruosi Wan", "authors": "Junjie Yan, Ruosi Wan, Xiangyu Zhang, Wei Zhang, Yichen Wei, Jian Sun", "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch\n  Normalization", "comments": "ICLR2020; https://github.com/megvii-model/MABN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) is one of the most widely used techniques in Deep\nLearning field. But its performance can awfully degrade with insufficient batch\nsize. This weakness limits the usage of BN on many computer vision tasks like\ndetection or segmentation, where batch size is usually small due to the\nconstraint of memory consumption. Therefore many modified normalization\ntechniques have been proposed, which either fail to restore the performance of\nBN completely, or have to introduce additional nonlinear operations in\ninference procedure and increase huge consumption. In this paper, we reveal\nthat there are two extra batch statistics involved in backward propagation of\nBN, on which has never been well discussed before. The extra batch statistics\nassociated with gradients also can severely affect the training of deep neural\nnetwork. Based on our analysis, we propose a novel normalization method, named\nMoving Average Batch Normalization (MABN). MABN can completely restore the\nperformance of vanilla BN in small batch cases, without introducing any\nadditional nonlinear operations in inference procedure. We prove the benefits\nof MABN by both theoretical analysis and experiments. Our experiments\ndemonstrate the effectiveness of MABN in multiple computer vision tasks\nincluding ImageNet and COCO. The code has been released in\nhttps://github.com/megvii-model/MABN.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 14:41:22 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 10:06:09 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Yan", "Junjie", ""], ["Wan", "Ruosi", ""], ["Zhang", "Xiangyu", ""], ["Zhang", "Wei", ""], ["Wei", "Yichen", ""], ["Sun", "Jian", ""]]}, {"id": "2001.06875", "submitter": "Seong Hun Lee", "authors": "Javier Civera and Seong Hun Lee", "title": "RGB-D Odometry and SLAM", "comments": "This is the pre-submission version of the manuscript that was later\n  edited and published as a chapter in RGB-D Image Analysis and Processing", "journal-ref": null, "doi": "10.1007/978-3-030-28603-3_6", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of modern RGB-D sensors had a significant impact in many\napplication fields, including robotics, augmented reality (AR) and 3D scanning.\nThey are low-cost, low-power and low-size alternatives to traditional range\nsensors such as LiDAR. Moreover, unlike RGB cameras, RGB-D sensors provide the\nadditional depth information that removes the need of frame-by-frame\ntriangulation for 3D scene reconstruction. These merits have made them very\npopular in mobile robotics and AR, where it is of great interest to estimate\nego-motion and 3D scene structure. Such spatial understanding can enable robots\nto navigate autonomously without collisions and allow users to insert virtual\nentities consistent with the image stream. In this chapter, we review common\nformulations of odometry and Simultaneous Localization and Mapping (known by\nits acronym SLAM) using RGB-D stream input. The two topics are closely related,\nas the former aims to track the incremental camera motion with respect to a\nlocal map of the scene, and the latter to jointly estimate the camera\ntrajectory and the global map with consistency. In both cases, the standard\napproaches minimize a cost function using nonlinear optimization techniques.\nThis chapter consists of three main parts: In the first part, we introduce the\nbasic concept of odometry and SLAM and motivate the use of RGB-D sensors. We\nalso give mathematical preliminaries relevant to most odometry and SLAM\nalgorithms. In the second part, we detail the three main components of SLAM\nsystems: camera pose tracking, scene mapping and loop closing. For each\ncomponent, we describe different approaches proposed in the literature. In the\nfinal part, we provide a brief discussion on advanced research topics with the\nreferences to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 17:56:11 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Civera", "Javier", ""], ["Lee", "Seong Hun", ""]]}, {"id": "2001.06891", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, Lianli Gao", "title": "Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form\n  Sentences", "comments": "The camera ready version for CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a novel task, Spatio-Temporal Video Grounding for\nMulti-Form Sentences (STVG). Given an untrimmed video and a\ndeclarative/interrogative sentence depicting an object, STVG aims to localize\nthe spatio-temporal tube of the queried object. STVG has two challenging\nsettings: (1) We need to localize spatio-temporal object tubes from untrimmed\nvideos, where the object may only exist in a very small segment of the video;\n(2) We deal with multi-form sentences, including the declarative sentences with\nexplicit objects and interrogative sentences with unknown objects. Existing\nmethods cannot tackle the STVG task due to the ineffective tube pre-generation\nand the lack of object relationship modeling. Thus, we then propose a novel\nSpatio-Temporal Graph Reasoning Network (STGRN) for this task. First, we build\na spatio-temporal region graph to capture the region relationships with\ntemporal object dynamics, which involves the implicit and explicit spatial\nsubgraphs in each frame and the temporal dynamic subgraph across frames. We\nthen incorporate textual clues into the graph and develop the multi-step\ncross-modal graph reasoning. Next, we introduce a spatio-temporal localizer\nwith a dynamic selection method to directly retrieve the spatio-temporal tubes\nwithout tube pre-generation. Moreover, we contribute a large-scale video\ngrounding dataset VidSTG based on video relation dataset VidOR. The extensive\nexperiments demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 19:53:22 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 13:46:00 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 21:34:44 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Zhang", "Zhu", ""], ["Zhao", "Zhou", ""], ["Zhao", "Yang", ""], ["Wang", "Qi", ""], ["Liu", "Huasheng", ""], ["Gao", "Lianli", ""]]}, {"id": "2001.06894", "submitter": "Sandy Engelhardt", "authors": "Chandrakanth Jayachandran Preetha, Jonathan Kloss, Fabian Siegfried\n  Wehrtmann, Lalith Sharan, Carolyn Fan, Beat Peter M\\\"uller-Stich, Felix\n  Nickel, Sandy Engelhardt", "title": "Towards Augmented Reality-based Suturing in Monocular Laparoscopic\n  Training", "comments": "Accepted for SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Minimally Invasive Surgery (MIS) techniques have gained rapid popularity\namong surgeons since they offer significant clinical benefits including reduced\nrecovery time and diminished post-operative adverse effects. However,\nconventional endoscopic systems output monocular video which compromises depth\nperception, spatial orientation and field of view. Suturing is one of the most\ncomplex tasks performed under these circumstances. Key components of this tasks\nare the interplay between needle holder and the surgical needle. Reliable 3D\nlocalization of needle and instruments in real time could be used to augment\nthe scene with additional parameters that describe their quantitative geometric\nrelation, e.g. the relation between the estimated needle plane and its rotation\ncenter and the instrument. This could contribute towards standardization and\ntraining of basic skills and operative techniques, enhance overall surgical\nperformance, and reduce the risk of complications. The paper proposes an\nAugmented Reality environment with quantitative and qualitative visual\nrepresentations to enhance laparoscopic training outcomes performed on a\nsilicone pad. This is enabled by a multi-task supervised deep neural network\nwhich performs multi-class segmentation and depth map prediction. Scarcity of\nlabels has been conquered by creating a virtual environment which resembles the\nsurgical training scenario to generate dense depth maps and segmentation maps.\nThe proposed convolutional neural network was tested on real surgical training\nscenarios and showed to be robust to occlusion of the needle. The network\nachieves a dice score of 0.67 for surgical needle segmentation, 0.81 for needle\nholder instrument segmentation and a mean absolute error of 6.5 mm for depth\nestimation.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 19:59:58 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Preetha", "Chandrakanth Jayachandran", ""], ["Kloss", "Jonathan", ""], ["Wehrtmann", "Fabian Siegfried", ""], ["Sharan", "Lalith", ""], ["Fan", "Carolyn", ""], ["M\u00fcller-Stich", "Beat Peter", ""], ["Nickel", "Felix", ""], ["Engelhardt", "Sandy", ""]]}, {"id": "2001.06902", "submitter": "Simon Vandenhende", "authors": "Simon Vandenhende, Stamatios Georgoulis and Luc Van Gool", "title": "MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning", "comments": "Accepted at ECCV2020 (spotlight) - Code:\n  https://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we argue about the importance of considering task interactions\nat multiple scales when distilling task information in a multi-task learning\nsetup. In contrast to common belief, we show that tasks with high affinity at a\ncertain scale are not guaranteed to retain this behaviour at other scales, and\nvice versa. We propose a novel architecture, namely MTI-Net, that builds upon\nthis finding in three ways. First, it explicitly models task interactions at\nevery scale via a multi-scale multi-modal distillation unit. Second, it\npropagates distilled task information from lower to higher scales via a feature\npropagation module. Third, it aggregates the refined task features from all\nscales via a feature aggregation unit to produce the final per-task\npredictions.\n  Extensive experiments on two multi-task dense labeling datasets show that,\nunlike prior work, our multi-task model delivers on the full potential of\nmulti-task learning, that is, smaller memory footprint, reduced number of\ncalculations, and better performance w.r.t. single-task learning. The code is\nmade publicly available:\nhttps://github.com/SimonVandenhende/Multi-Task-Learning-PyTorch.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 21:02:36 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 06:54:50 GMT"}, {"version": "v3", "created": "Sat, 21 Mar 2020 08:58:21 GMT"}, {"version": "v4", "created": "Wed, 8 Apr 2020 19:15:35 GMT"}, {"version": "v5", "created": "Wed, 8 Jul 2020 19:58:22 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Vandenhende", "Simon", ""], ["Georgoulis", "Stamatios", ""], ["Van Gool", "Luc", ""]]}, {"id": "2001.06927", "submitter": "Ramprasaath R. Selvaraju", "authors": "Ramprasaath R. Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz,\n  Marco Ribeiro, Besmira Nushi, Ece Kamar", "title": "SQuINTing at VQA Models: Introspecting VQA Models with Sub-Questions", "comments": "Accepted to CVPR'20 as an Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing VQA datasets contain questions with varying levels of complexity.\nWhile the majority of questions in these datasets require perception for\nrecognizing existence, properties, and spatial relationships of entities, a\nsignificant portion of questions pose challenges that correspond to reasoning\ntasks - tasks that can only be answered through a synthesis of perception and\nknowledge about the world, logic and / or reasoning. Analyzing performance\nacross this distinction allows us to notice when existing VQA models have\nconsistency issues; they answer the reasoning questions correctly but fail on\nassociated low-level perception questions. For example, in Figure 1, models\nanswer the complex reasoning question \"Is the banana ripe enough to eat?\"\ncorrectly, but fail on the associated perception question \"Are the bananas\nmostly green or yellow?\" indicating that the model likely answered the\nreasoning question correctly but for the wrong reason. We quantify the extent\nto which this phenomenon occurs by creating a new Reasoning split of the VQA\ndataset and collecting VQA-introspect, a new dataset1 which consists of 238K\nnew perception questions which serve as sub questions corresponding to the set\nof perceptual tasks needed to effectively answer the complex reasoning\nquestions in the Reasoning split. Our evaluation shows that state-of-the-art\nVQA models have comparable performance in answering perception and reasoning\nquestions, but suffer from consistency problems. To address this shortcoming,\nwe propose an approach called Sub-Question Importance-aware Network Tuning\n(SQuINT), which encourages the model to attend to the same parts of the image\nwhen answering the reasoning question and the perception sub question. We show\nthat SQuINT improves model consistency by ~5%, also marginally improving\nperformance on the Reasoning questions in VQA, while also displaying better\nattention maps.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 01:02:36 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 17:54:16 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Selvaraju", "Ramprasaath R.", ""], ["Tendulkar", "Purva", ""], ["Parikh", "Devi", ""], ["Horvitz", "Eric", ""], ["Ribeiro", "Marco", ""], ["Nushi", "Besmira", ""], ["Kamar", "Ece", ""]]}, {"id": "2001.06963", "submitter": "Abdul Muqeet", "authors": "Saad Bin Sami, Abdul Muqeet, Humera Tariq", "title": "A Novel Image Dehazing and Assessment Method", "comments": "Accepted in IBA-ICICT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images captured in hazy weather conditions often suffer from color contrast\nand color fidelity. This degradation is represented by transmission map which\nrepresents the amount of attenuation and airlight which represents the color of\nadditive noise. In this paper, we have proposed a method to estimate the\ntransmission map using haze levels instead of airlight color since there are\nsome ambiguities in estimation of airlight. Qualitative and quantitative\nresults of proposed method show competitiveness of the method given. In\naddition we have proposed two metrics which are based on statistics of natural\noutdoor images for assessment of haze removal algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 04:01:09 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Sami", "Saad Bin", ""], ["Muqeet", "Abdul", ""], ["Tariq", "Humera", ""]]}, {"id": "2001.06965", "submitter": "Chao Zhang", "authors": "Chao Zhang, Xuequan Lu, Katsuya Hotta, and Xi Yang", "title": "G2MF-WA: Geometric Multi-Model Fitting with Weakly Annotated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we attempt to address the problem of geometric multi-model\nfitting with resorting to a few weakly annotated (WA) data points, which has\nbeen sparsely studied so far. In weak annotating, most of the manual\nannotations are supposed to be correct yet inevitably mixed with incorrect\nones. The WA data can be naturally obtained in an interactive way for specific\ntasks, for example, in the case of homography estimation, one can easily\nannotate points on the same plane/object with a single label by observing the\nimage. Motivated by this, we propose a novel method to make full use of the WA\ndata to boost the multi-model fitting performance. Specifically, a graph for\nmodel proposal sampling is first constructed using the WA data, given the prior\nthat the WA data annotated with the same weak label has a high probability of\nbeing assigned to the same model. By incorporating this prior knowledge into\nthe calculation of edge probabilities, vertices (i.e., data points) lie on/near\nthe latent model are likely to connect together and further form a\nsubset/cluster for effective proposals generation. With the proposals\ngenerated, the $\\alpha$-expansion is adopted for labeling, and our method in\nreturn updates the proposals. This works in an iterative way. Extensive\nexperiments validate our method and show that the proposed method produces\nnoticeably better results than state-of-the-art techniques in most cases.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 04:22:01 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zhang", "Chao", ""], ["Lu", "Xuequan", ""], ["Hotta", "Katsuya", ""], ["Yang", "Xi", ""]]}, {"id": "2001.06967", "submitter": "Subhayan Mukherjee", "authors": "Subhayan Mukherjee and Ram Mohana Reddy Guddeti", "title": "A hybrid algorithm for disparity calculation from sparse disparity\n  estimates based on stereo vision", "comments": "2014 SPCOM", "journal-ref": null, "doi": "10.1109/SPCOM.2014.6983949", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have proposed a novel method for stereo disparity\nestimation by combining the existing methods of block based and region based\nstereo matching. Our method can generate dense disparity maps from disparity\nmeasurements of only 18% pixels of either the left or the right image of a\nstereo image pair. It works by segmenting the lightness values of image pixels\nusing a fast implementation of K-Means clustering. It then refines those\nsegment boundaries by morphological filtering and connected components\nanalysis, thus removing a lot of redundant boundary pixels. This is followed by\ndetermining the boundaries' disparities by the SAD cost function. Lastly, we\nreconstruct the entire disparity map of the scene from the boundaries'\ndisparities through disparity propagation along the scan lines and disparity\nprediction of regions of uncertainty by considering disparities of the\nneighboring regions. Experimental results on the Middlebury stereo vision\ndataset demonstrate that the proposed method outperforms traditional disparity\ndetermination methods like SAD and NCC by up to 30% and achieves an improvement\nof 2.6% when compared to a recent approach based on absolute difference (AD)\ncost function for disparity calculations [1].\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 04:33:28 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mukherjee", "Subhayan", ""], ["Guddeti", "Ram Mohana Reddy", ""]]}, {"id": "2001.06968", "submitter": "Yihao Liu", "authors": "Yu Dong, Yihao Liu, He Zhang, Shifeng Chen, Yu Qiao", "title": "FD-GAN: Generative Adversarial Networks with Fusion-discriminator for\n  Single Image Dehazing", "comments": "Accepted by AAAI2020 (with supplementary files)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks (CNNs) have achieved great\nimprovements in single image dehazing and attained much attention in research.\nMost existing learning-based dehazing methods are not fully end-to-end, which\nstill follow the traditional dehazing procedure: first estimate the medium\ntransmission and the atmospheric light, then recover the haze-free image based\non the atmospheric scattering model. However, in practice, due to lack of\npriors and constraints, it is hard to precisely estimate these intermediate\nparameters. Inaccurate estimation further degrades the performance of dehazing,\nresulting in artifacts, color distortion and insufficient haze removal. To\naddress this, we propose a fully end-to-end Generative Adversarial Networks\nwith Fusion-discriminator (FD-GAN) for image dehazing. With the proposed\nFusion-discriminator which takes frequency information as additional priors,\nour model can generator more natural and realistic dehazed images with less\ncolor distortion and fewer artifacts. Moreover, we synthesize a large-scale\ntraining dataset including various indoor and outdoor hazy images to boost the\nperformance and we reveal that for learning-based dehazing methods, the\nperformance is strictly influenced by the training data. Experiments have shown\nthat our method reaches state-of-the-art performance on both public synthetic\ndatasets and real-world images with more visually pleasing dehazed results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 04:36:11 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 08:27:44 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Dong", "Yu", ""], ["Liu", "Yihao", ""], ["Zhang", "He", ""], ["Chen", "Shifeng", ""], ["Qiao", "Yu", ""]]}, {"id": "2001.06983", "submitter": "Subhayan Mukherjee", "authors": "Subhayan Mukherjee, Guan-Ming Su, and Irene Cheng", "title": "Adaptive Dithering Using Curved Markov-Gaussian Noise in the Quantized\n  Domain for Mapping SDR to HDR Image", "comments": "2018 International Conference on Smart Multimedia", "journal-ref": null, "doi": "10.1007/978-3-030-04375-9_17", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Dynamic Range (HDR) imaging is gaining increased attention due to its\nrealistic content, for not only regular displays but also smartphones. Before\nsufficient HDR content is distributed, HDR visualization still relies mostly on\nconverting Standard Dynamic Range (SDR) content. SDR images are often\nquantized, or bit depth reduced, before SDR-to-HDR conversion, e.g. for video\ntransmission. Quantization can easily lead to banding artefacts. In some\ncomputing and/or memory I/O limited environment, the traditional solution using\nspatial neighborhood information is not feasible. Our method includes noise\ngeneration (offline) and noise injection (online), and operates on pixels of\nthe quantized image. We vary the magnitude and structure of the noise pattern\nadaptively based on the luma of the quantized pixel and the slope of the\ninverse-tone mapping function. Subjective user evaluations confirm the superior\nperformance of our technique.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 05:30:16 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mukherjee", "Subhayan", ""], ["Su", "Guan-Ming", ""], ["Cheng", "Irene", ""]]}, {"id": "2001.07002", "submitter": "Faizal Hafiz", "authors": "Renoh Johnson Chalakkal, Faizal Hafiz, Waleed Abdulla, and Akshya\n  Swain", "title": "An Efficient Framework for Automated Screening of Clinically Significant\n  Macular Edema", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study proposes a new approach to automated screening of\nClinically Significant Macular Edema (CSME) and addresses two major challenges\nassociated with such screenings, i.e., exudate segmentation and imbalanced\ndatasets. The proposed approach replaces the conventional exudate segmentation\nbased feature extraction by combining a pre-trained deep neural network with\nmeta-heuristic feature selection. A feature space over-sampling technique is\nbeing used to overcome the effects of skewed datasets and the screening is\naccomplished by a k-NN based classifier. The role of each data-processing step\n(e.g., class balancing, feature selection) and the effects of limiting the\nregion-of-interest to fovea on the classification performance are critically\nanalyzed. Finally, the selection and implication of operating point on Receiver\nOperating Characteristic curve are discussed. The results of this study\nconvincingly demonstrate that by following these fundamental practices of\nmachine learning, a basic k-NN based classifier could effectively accomplish\nthe CSME screening.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 07:34:13 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chalakkal", "Renoh Johnson", ""], ["Hafiz", "Faizal", ""], ["Abdulla", "Waleed", ""], ["Swain", "Akshya", ""]]}, {"id": "2001.07026", "submitter": "Daniel J. Trosten", "authors": "Daniel J. Trosten, Michael C. Kampffmeyer, Robert Jenssen", "title": "Deep Image Clustering with Tensor Kernels and Unsupervised Companion\n  Objectives", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a new model for deep image clustering, using\nconvolutional neural networks and tensor kernels. The proposed Deep Tensor\nKernel Clustering (DTKC) consists of a convolutional neural network (CNN),\nwhich is trained to reflect a common cluster structure at the output of its\nintermediate layers. Encouraging a consistent cluster structure throughout the\nnetwork has the potential to guide it towards meaningful clusters, even though\nthese clusters might appear to be nonlinear in the input space. The cluster\nstructure is enforced through the idea of unsupervised companion objectives,\nwhere separate loss functions are attached to layers in the network. These\nunsupervised companion objectives are constructed based on a proposed\ngeneralization of the Cauchy-Schwarz (CS) divergence, from vectors to tensors\nof arbitrary rank. Generalizing the CS divergence to tensor-valued data is a\ncrucial step, due to the tensorial nature of the intermediate representations\nin the CNN. Several experiments are conducted to thoroughly assess the\nperformance of the proposed DTKC model. The results indicate that the model\noutperforms, or performs comparable to, a wide range of baseline algorithms. We\nalso empirically demonstrate that our model does not suffer from objective\nfunction mismatch, which can be a problematic artifact in autoencoder-based\nclustering models.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 09:07:59 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 15:12:18 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Trosten", "Daniel J.", ""], ["Kampffmeyer", "Michael C.", ""], ["Jenssen", "Robert", ""]]}, {"id": "2001.07058", "submitter": "Adrien Kaiser", "authors": "Adrien Kaiser, Jos\\'e Alonso Ybanez Zepeda, Tamy Boubekeur", "title": "Plane Pair Matching for Efficient 3D View Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to estimate the motion matrix between overlapping\npairs of 3D views in the context of indoor scenes. We use the Manhattan world\nassumption to introduce lightweight geometric constraints under the form of\nplanes into the problem, which reduces complexity by taking into account the\nstructure of the scene. In particular, we define a stochastic framework to\ncategorize planes as vertical or horizontal and parallel or non-parallel. We\nleverage this classification to match pairs of planes in overlapping views with\npoint-of-view agnostic structural metrics. We propose to split the motion\ncomputation using the classification and estimate separately the rotation and\ntranslation of the sensor, using a quadric minimizer. We validate our approach\non a toy example and present quantitative experiments on a public RGB-D\ndataset, comparing against recent state-of-the-art methods. Our evaluation\nshows that planar constraints only add low computational overhead while\nimproving results in precision when applied after a prior coarse estimate. We\nconclude by giving hints towards extensions and improvements of current\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 11:15:26 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Kaiser", "Adrien", ""], ["Zepeda", "Jos\u00e9 Alonso Ybanez", ""], ["Boubekeur", "Tamy", ""]]}, {"id": "2001.07059", "submitter": "Moshiur R Farazi", "authors": "Moshiur R. Farazi, Salman H. Khan, Nick Barnes", "title": "Accuracy vs. Complexity: A Trade-off in Visual Question Answering Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) has emerged as a Visual Turing Test to\nvalidate the reasoning ability of AI agents. The pivot to existing VQA models\nis the joint embedding that is learned by combining the visual features from an\nimage and the semantic features from a given question. Consequently, a large\nbody of literature has focused on developing complex joint embedding strategies\ncoupled with visual attention mechanisms to effectively capture the interplay\nbetween these two modalities. However, modelling the visual and semantic\nfeatures in a high dimensional (joint embedding) space is computationally\nexpensive, and more complex models often result in trivial improvements in the\nVQA accuracy. In this work, we systematically study the trade-off between the\nmodel complexity and the performance on the VQA task. VQA models have a diverse\narchitecture comprising of pre-processing, feature extraction, multimodal\nfusion, attention and final classification stages. We specifically focus on the\neffect of \"multi-modal fusion\" in VQA models that is typically the most\nexpensive step in a VQA pipeline. Our thorough experimental evaluation leads us\nto two proposals, one optimized for minimal complexity and the other one\noptimized for state-of-the-art VQA performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 11:27:21 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Farazi", "Moshiur R.", ""], ["Khan", "Salman H.", ""], ["Barnes", "Nick", ""]]}, {"id": "2001.07090", "submitter": "He-Feng Yin", "authors": "Zi-Qi Li, Jun Sun, Xiao-Jun Wu and He-Feng Yin", "title": "Multiplication fusion of sparse and collaborative-competitive\n  representation for image classification", "comments": "submitted to International Journal of Machine Learning and\n  Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation based classification methods have become a hot research topic\nduring the past few years, and the two most prominent approaches are sparse\nrepresentation based classification (SRC) and collaborative representation\nbased classification (CRC). CRC reveals that it is the collaborative\nrepresentation rather than the sparsity that makes SRC successful.\nNevertheless, the dense representation of CRC may not be discriminative which\nwill degrade its performance for classification tasks. To alleviate this\nproblem to some extent, we propose a new method called sparse and\ncollaborative-competitive representation based classification (SCCRC) for image\nclassification. Firstly, the coefficients of the test sample are obtained by\nSRC and CCRC, respectively. Then the fused coefficient is derived by\nmultiplying the coefficients of SRC and CCRC. Finally, the test sample is\ndesignated to the class that has the minimum residual. Experimental results on\nseveral benchmark databases demonstrate the efficacy of our proposed SCCRC. The\nsource code of SCCRC is accessible at https://github.com/li-zi-qi/SCCRC.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 12:55:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Li", "Zi-Qi", ""], ["Sun", "Jun", ""], ["Wu", "Xiao-Jun", ""], ["Yin", "He-Feng", ""]]}, {"id": "2001.07092", "submitter": "Grace Lindsay", "authors": "Grace W. Lindsay", "title": "Convolutional Neural Networks as a Model of the Visual System: Past,\n  Present, and Future", "comments": "Review Article to be published in Journal of Cognitive Neuroscience,\n  18 pages, 5 figures plus 8 pages of references", "journal-ref": null, "doi": "10.1162/jocn_a_01544", "report-no": null, "categories": "q-bio.NC cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional neural networks (CNNs) were inspired by early findings in the\nstudy of biological vision. They have since become successful tools in computer\nvision and state-of-the-art models of both neural activity and behavior on\nvisual tasks. This review highlights what, in the context of CNNs, it means to\nbe a good model in computational neuroscience and the various ways models can\nprovide insight. Specifically, it covers the origins of CNNs and the methods by\nwhich we validate them as models of biological vision. It then goes on to\nelaborate on what we can learn about biological vision by understanding and\nexperimenting on CNNs and discusses emerging opportunities for the use of CNNS\nin vision research beyond basic object recognition.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:04:37 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 11:37:16 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Lindsay", "Grace W.", ""]]}, {"id": "2001.07093", "submitter": "Zhen-Liang Ni", "authors": "Zhen-Liang Ni, Gui-Bin Bian, Guan-An Wang, Xiao-Hu Zhou, Zeng-Guang\n  Hou, Xiao-Liang Xie, Zhen Li and Yu-Han Wang", "title": "BARNet: Bilinear Attention Network with Adaptive Receptive Fields for\n  Surgical Instrument Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical instrument segmentation is extremely important for computer-assisted\nsurgery. Different from common object segmentation, it is more challenging due\nto the large illumination and scale variation caused by the special surgical\nscenes. In this paper, we propose a novel bilinear attention network with\nadaptive receptive field to solve these two challenges. For the illumination\nvariation, the bilinear attention module can capture second-order statistics to\nencode global contexts and semantic dependencies between local pixels. With\nthem, semantic features in challenging areas can be inferred from their\nneighbors and the distinction of various semantics can be boosted. For the\nscale variation, our adaptive receptive field module aggregates multi-scale\nfeatures and automatically fuses them with different weights. Specifically, it\nencodes the semantic relationship between channels to emphasize feature maps\nwith appropriate scales, changing the receptive field of subsequent\nconvolutions. The proposed network achieves the best performance 97.47% mean\nIOU on Cata7 and comes first place on EndoVis 2017 by 10.10% IOU overtaking\nsecond-ranking method.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:05:08 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 08:16:52 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 09:45:53 GMT"}, {"version": "v4", "created": "Fri, 22 May 2020 03:12:14 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Ni", "Zhen-Liang", ""], ["Bian", "Gui-Bin", ""], ["Wang", "Guan-An", ""], ["Zhou", "Xiao-Hu", ""], ["Hou", "Zeng-Guang", ""], ["Xie", "Xiao-Liang", ""], ["Li", "Zhen", ""], ["Wang", "Yu-Han", ""]]}, {"id": "2001.07100", "submitter": "Clemens-Alexander Brust", "authors": "Clemens-Alexander Brust and Christoph K\\\"ading and Joachim Denzler", "title": "Active and Incremental Learning with Weak Supervision", "comments": "Accepted for publication in KI - K\\\"unstliche Intelligenz", "journal-ref": null, "doi": "10.1007/s13218-020-00631-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amounts of labeled training data are one of the main contributors to\nthe great success that deep models have achieved in the past. Label acquisition\nfor tasks other than benchmarks can pose a challenge due to requirements of\nboth funding and expertise. By selecting unlabeled examples that are promising\nin terms of model improvement and only asking for respective labels, active\nlearning can increase the efficiency of the labeling process in terms of time\nand cost.\n  In this work, we describe combinations of an incremental learning scheme and\nmethods of active learning. These allow for continuous exploration of newly\nobserved unlabeled data. We describe selection criteria based on model\nuncertainty as well as expected model output change (EMOC). An object detection\ntask is evaluated in a continuous exploration context on the PASCAL VOC\ndataset. We also validate a weakly supervised system based on active and\nincremental learning in a real-world biodiversity application where images from\ncamera traps are analyzed. Labeling only 32 images by accepting or rejecting\nproposals generated by our method yields an increase in accuracy from 25.4% to\n42.6%.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:21:14 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Brust", "Clemens-Alexander", ""], ["K\u00e4ding", "Christoph", ""], ["Denzler", "Joachim", ""]]}, {"id": "2001.07108", "submitter": "Tinghuai Wang", "authors": "Tinghuai Wang, Guangming Wang, Kuan Eeik Tan, Donghui Tan", "title": "Spectral Pyramid Graph Attention Network for Hyperspectral Image\n  Classification", "comments": "7 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have made significant advances in\nhyperspectral image (HSI) classification. However, standard convolutional\nkernel neglects the intrinsic connections between data points, resulting in\npoor region delineation and small spurious predictions. Furthermore, HSIs have\na unique continuous data distribution along the high dimensional spectrum\ndomain - much remains to be addressed in characterizing the spectral contexts\nconsidering the prohibitively high dimensionality and improving reasoning\ncapability in light of the limited amount of labelled data. This paper presents\na novel architecture which explicitly addresses these two issues. Specifically,\nwe design an architecture to encode the multiple spectral contextual\ninformation in the form of spectral pyramid of multiple embedding spaces. In\neach spectral embedding space, we propose graph attention mechanism to\nexplicitly perform interpretable reasoning in the spatial domain based on the\nconnection in spectral feature space. Experiments on three HSI datasets\ndemonstrate that the proposed architecture can significantly improve the\nclassification accuracy compared with the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:49:43 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Tinghuai", ""], ["Wang", "Guangming", ""], ["Tan", "Kuan Eeik", ""], ["Tan", "Donghui", ""]]}, {"id": "2001.07150", "submitter": "Wei Wang", "authors": "Wei Wang, Xiang-Gen Xia, Chuanjiang He, Zemin Ren, Jian Lu, Tianfu\n  Wang and Baiying Lei", "title": "A deep network for sinogram and CT image reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A CT image can be well reconstructed when the sampling rate of the sinogram\nsatisfies the Nyquist criteria and the sampled signal is noise-free. However,\nin practice, the sinogram is usually contaminated by noise, which degrades the\nquality of a reconstructed CT image. In this paper, we design a deep network\nfor sinogram and CT image reconstruction. The network consists of two cascaded\nblocks that are linked by a filter backprojection (FBP) layer, where the former\nblock is responsible for denoising and completing the sinograms while the\nlatter is used to removing the noise and artifacts of the CT images.\nExperimental results show that the reconstructed CT images by our methods have\nthe highest PSNR and SSIM in average compared to state of the art methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 15:50:16 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Wei", ""], ["Xia", "Xiang-Gen", ""], ["He", "Chuanjiang", ""], ["Ren", "Zemin", ""], ["Lu", "Jian", ""], ["Wang", "Tianfu", ""], ["Lei", "Baiying", ""]]}, {"id": "2001.07183", "submitter": "Enzo Ferrante", "authors": "Lucas Mansilla, Diego H. Milone, Enzo Ferrante", "title": "Learning Deformable Registration of Medical Images with Anatomical\n  Constraints", "comments": "Accepted for publication in Neural Networks (Elsevier). Source code\n  and resulting segmentation masks for the NIH Chest-XRay14 dataset with\n  estimated quality index available at\n  https://github.com/lucasmansilla/ACRN_Chest_X-ray_IA", "journal-ref": null, "doi": "10.1016/j.neunet.2020.01.023", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable image registration is a fundamental problem in the field of\nmedical image analysis. During the last years, we have witnessed the advent of\ndeep learning-based image registration methods which achieve state-of-the-art\nperformance, and drastically reduce the required computational time. However,\nlittle work has been done regarding how can we encourage our models to produce\nnot only accurate, but also anatomically plausible results, which is still an\nopen question in the field. In this work, we argue that incorporating\nanatomical priors in the form of global constraints into the learning process\nof these models, will further improve their performance and boost the realism\nof the warped images after registration. We learn global non-linear\nrepresentations of image anatomy using segmentation masks, and employ them to\nconstraint the registration process. The proposed AC-RegNet architecture is\nevaluated in the context of chest X-ray image registration using three\ndifferent datasets, where the high anatomical variability makes the task\nextremely challenging. Our experiments show that the proposed anatomically\nconstrained registration model produces more realistic and accurate results\nthan state-of-the-art methods, demonstrating the potential of this approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 17:44:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 13:25:16 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Mansilla", "Lucas", ""], ["Milone", "Diego H.", ""], ["Ferrante", "Enzo", ""]]}, {"id": "2001.07194", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Shaunak Mishra, Manisha Verma, Narayan Bhamidipati, and\n  Wei Wang", "title": "Recommending Themes for Ad Creative Design via Visual-Linguistic\n  Representations", "comments": "7 pages, 8 figures, 2 tables, accepted by The Web Conference 2020", "journal-ref": null, "doi": "10.1145/3366423.3380001", "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a perennial need in the online advertising industry to refresh ad\ncreatives, i.e., images and text used for enticing online users towards a\nbrand. Such refreshes are required to reduce the likelihood of ad fatigue among\nonline users, and to incorporate insights from other successful campaigns in\nrelated product categories. Given a brand, to come up with themes for a new ad\nis a painstaking and time consuming process for creative strategists.\nStrategists typically draw inspiration from the images and text used for past\nad campaigns, as well as world knowledge on the brands. To automatically infer\nad themes via such multimodal sources of information in past ad campaigns, we\npropose a theme (keyphrase) recommender system for ad creative strategists. The\ntheme recommender is based on aggregating results from a visual question\nanswering (VQA) task, which ingests the following: (i) ad images, (ii) text\nassociated with the ads as well as Wikipedia pages on the brands in the ads,\nand (iii) questions around the ad. We leverage transformer based cross-modality\nencoders to train visual-linguistic representations for our VQA task. We study\ntwo formulations for the VQA task along the lines of classification and\nranking; via experiments on a public dataset, we show that cross-modal\nrepresentations lead to significantly better classification accuracy and\nranking precision-recall metrics. Cross-modal representations show better\nperformance compared to separate image and text representations. In addition,\nthe use of multimodal information shows a significant lift over using only\ntextual or visual information.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 18:04:10 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 23:05:46 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhou", "Yichao", ""], ["Mishra", "Shaunak", ""], ["Verma", "Manisha", ""], ["Bhamidipati", "Narayan", ""], ["Wang", "Wei", ""]]}, {"id": "2001.07243", "submitter": "Aman Jain", "authors": "Aman Gajendra Jain, Nicolas Saunier", "title": "Autocamera Calibration for traffic surveillance cameras with wide angle\n  lenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for automatic calibration of a traffic surveillance\ncamera with wide-angle lenses. Video footage of a few minutes is sufficient for\nthe entire calibration process to take place. This method takes in the height\nof the camera from the ground plane as the only user input to overcome the\nscale ambiguity. The calibration is performed in two stages, 1. Intrinsic\nCalibration 2. Extrinsic Calibration. Intrinsic calibration is achieved by\nassuming an equidistant fisheye distortion and an ideal camera model. Extrinsic\ncalibration is accomplished by estimating the two vanishing points, on the\nground plane, from the motion of vehicles at perpendicular intersections. The\nfirst stage of intrinsic calibration is also valid for thermal cameras.\nExperiments have been conducted to demonstrate the effectiveness of this\napproach on visible as well as thermal cameras.\n  Index Terms: fish-eye, calibration, thermal camera, intelligent\ntransportation systems, vanishing points\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 20:37:02 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Jain", "Aman Gajendra", ""], ["Saunier", "Nicolas", ""]]}, {"id": "2001.07252", "submitter": "Tsun-Yi Yang", "authors": "Tsun-Yi Yang and Duy-Kien Nguyen and Huub Heijnen and Vassileios\n  Balntas", "title": "UR2KiD: Unifying Retrieval, Keypoint Detection, and Keypoint Description\n  without Local Correspondence Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore how three related tasks, namely keypoint detection,\ndescription, and image retrieval can be jointly tackled using a single unified\nframework, which is trained without the need of training data with point to\npoint correspondences. By leveraging diverse information from sequential layers\nof a standard ResNet-based architecture, we are able to extract keypoints and\ndescriptors that encode local information using generic techniques such as\nlocal activation norms, channel grouping and dropping, and self-distillation.\nSubsequently, global information for image retrieval is encoded in an\nend-to-end pipeline, based on pooling of the aforementioned local responses. In\ncontrast to previous methods in local matching, our method does not depend on\npointwise/pixelwise correspondences, and requires no such supervision at all\ni.e. no depth-maps from an SfM model nor manually created synthetic affine\ntransformations. We illustrate that this simple and direct paradigm, is able to\nachieve very competitive results against the state-of-the-art methods in\nvarious challenging benchmark conditions such as viewpoint changes, scale\nchanges, and day-night shifting localization.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 21:01:38 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Yang", "Tsun-Yi", ""], ["Nguyen", "Duy-Kien", ""], ["Heijnen", "Huub", ""], ["Balntas", "Vassileios", ""]]}, {"id": "2001.07253", "submitter": "Jane Wu", "authors": "Jane Wu, Yongxu Jin, Zhenglin Geng, Hui Zhou, Ronald Fedkiw", "title": "Recovering Geometric Information with Learned Texture Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is used to avoid overfitting when training a neural network;\nunfortunately, this reduces the attainable level of detail hindering the\nability to capture high-frequency information present in the training data.\nEven though various approaches may be used to re-introduce high-frequency\ndetail, it typically does not match the training data and is often not time\ncoherent. In the case of network inferred cloth, these sentiments manifest\nthemselves via either a lack of detailed wrinkles or unnaturally appearing\nand/or time incoherent surrogate wrinkles. Thus, we propose a general strategy\nwhereby high-frequency information is procedurally embedded into low-frequency\ndata so that when the latter is smeared out by the network the former still\nretains its high-frequency detail. We illustrate this approach by learning\ntexture coordinates which when smeared do not in turn smear out the\nhigh-frequency detail in the texture itself but merely smoothly distort it.\nNotably, we prescribe perturbed texture coordinates that are subsequently used\nto correct the over-smoothed appearance of inferred cloth, and correcting the\nappearance from multiple camera views naturally recovers lost geometric\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 21:15:13 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wu", "Jane", ""], ["Jin", "Yongxu", ""], ["Geng", "Zhenglin", ""], ["Zhou", "Hui", ""], ["Fedkiw", "Ronald", ""]]}, {"id": "2001.07267", "submitter": "Aydogan Ozcan", "authors": "Yijie Zhang, Kevin de Haan, Yair Rivenson, Jingxi Li, Apostolos Delis,\n  Aydogan Ozcan", "title": "Digital synthesis of histological stains using micro-structured and\n  multiplexed virtual staining of label-free tissue", "comments": "19 pages, 5 figures, 2 tables", "journal-ref": "Light: Science & Applications (2020)", "doi": "10.1038/s41377-020-0315-y", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histological staining is a vital step used to diagnose various diseases and\nhas been used for more than a century to provide contrast to tissue sections,\nrendering the tissue constituents visible for microscopic analysis by medical\nexperts. However, this process is time-consuming, labor-intensive, expensive\nand destructive to the specimen. Recently, the ability to virtually-stain\nunlabeled tissue sections, entirely avoiding the histochemical staining step,\nhas been demonstrated using tissue-stain specific deep neural networks. Here,\nwe present a new deep learning-based framework which generates\nvirtually-stained images using label-free tissue, where different stains are\nmerged following a micro-structure map defined by the user. This approach uses\na single deep neural network that receives two different sources of information\nat its input: (1) autofluorescence images of the label-free tissue sample, and\n(2) a digital staining matrix which represents the desired microscopic map of\ndifferent stains to be virtually generated at the same tissue section. This\ndigital staining matrix is also used to virtually blend existing stains,\ndigitally synthesizing new histological stains. We trained and blindly tested\nthis virtual-staining network using unlabeled kidney tissue sections to\ngenerate micro-structured combinations of Hematoxylin and Eosin (H&E), Jones\nsilver stain, and Masson's Trichrome stain. Using a single network, this\napproach multiplexes virtual staining of label-free tissue with multiple types\nof stains and paves the way for synthesizing new digital histological stains\nthat can be created on the same tissue cross-section, which is currently not\nfeasible with standard histochemical staining methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 22:14:06 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhang", "Yijie", ""], ["de Haan", "Kevin", ""], ["Rivenson", "Yair", ""], ["Li", "Jingxi", ""], ["Delis", "Apostolos", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2001.07321", "submitter": "Gantugs Atarsaikhan", "authors": "Gantugs Atarsaikhan, Brian Kenji Iwana and Seiichi Uchida", "title": "Neural Style Difference Transfer and Its Application to Font Generation", "comments": "Submitted to DAS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing fonts requires a great deal of time and effort. It requires\nprofessional skills, such as sketching, vectorizing, and image editing.\nAdditionally, each letter has to be designed individually. In this paper, we\nwill introduce a method to create fonts automatically. In our proposed method,\nthe difference of font styles between two different fonts is found and\ntransferred to another font using neural style transfer. Neural style transfer\nis a method of stylizing the contents of an image with the styles of another\nimage. We proposed a novel neural style difference and content difference loss\nfor the neural style transfer. With these losses, new fonts can be generated by\nadding or removing font styles from a font. We provided experimental results\nwith various combinations of input fonts and discussed limitations and future\ndevelopment for the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 03:32:44 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Atarsaikhan", "Gantugs", ""], ["Iwana", "Brian Kenji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2001.07322", "submitter": "Bahareh Behboodi", "authors": "Bahareh Behboodi, Mina Amiri, Rupert Brooks, Hassan Rivaz", "title": "Breast lesion segmentation in ultrasound images with limited annotated\n  data", "comments": "Accepted to ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) is one of the most commonly used imaging modalities in both\ndiagnosis and surgical interventions due to its low-cost, safety, and\nnon-invasive characteristic. US image segmentation is currently a unique\nchallenge because of the presence of speckle noise. As manual segmentation\nrequires considerable efforts and time, the development of automatic\nsegmentation algorithms has attracted researchers attention. Although recent\nmethodologies based on convolutional neural networks have shown promising\nperformances, their success relies on the availability of a large number of\ntraining data, which is prohibitively difficult for many applications.\nTherefore, in this study we propose the use of simulated US images and natural\nimages as auxiliary datasets in order to pre-train our segmentation network,\nand then to fine-tune with limited in vivo data. We show that with as little as\n19 in vivo images, fine-tuning the pre-trained network improves the dice score\nby 21% compared to training from scratch. We also demonstrate that if the same\nnumber of natural and simulation US images is available, pre-training on\nsimulation data is preferable.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 03:34:42 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Behboodi", "Bahareh", ""], ["Amiri", "Mina", ""], ["Brooks", "Rupert", ""], ["Rivaz", "Hassan", ""]]}, {"id": "2001.07323", "submitter": "He-Feng Yin", "authors": "Ning Yuan, Xiao-Jun Wu and He-Feng Yin", "title": "Face Verification via learning the kernel matrix", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel function is introduced to solve the nonlinear pattern recognition\nproblem. The advantage of a kernel method often depends critically on a proper\nchoice of the kernel function. A promising approach is to learn the kernel from\ndata automatically. Over the past few years, some methods which have been\nproposed to learn the kernel have some limitations: learning the parameters of\nsome prespecified kernel function and so on. In this paper, the nonlinear face\nverification via learning the kernel matrix is proposed. A new criterion is\nused in the new algorithm to avoid inverting the possibly singular within-class\nwhich is a computational problem. The experimental results obtained on the\nfacial database XM2VTS using the Lausanne protocol show that the verification\nperformance of the new method is superior to that of the primary method Client\nSpecific Kernel Discriminant Analysis (CSKDA). The method CSKDA needs to choose\na proper kernel function through many experiments, while the new method could\nlearn the kernel from data automatically which could save a lot of time and\nhave the robust performance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 03:39:09 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Yuan", "Ning", ""], ["Wu", "Xiao-Jun", ""], ["Yin", "He-Feng", ""]]}, {"id": "2001.07342", "submitter": "Natarajan Subramanyam", "authors": "Rajath S, Sumukh Aithal K, Natarajan Subramanyam", "title": "Transfer Learning using Neural Ordinary Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A concept of using Neural Ordinary Differential Equations(NODE) for Transfer\nLearning has been introduced. In this paper we use the EfficientNets to explore\ntransfer learning on CIFAR-10 dataset. We use NODE for fine-tuning our model.\nUsing NODE for fine tuning provides more stability during training and\nvalidation.These continuous depth blocks can also have a trade off between\nnumerical precision and speed .Using Neural ODEs for transfer learning has\nresulted in much stable convergence of the loss function.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 04:59:08 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["S", "Rajath", ""], ["K", "Sumukh Aithal", ""], ["Subramanyam", "Natarajan", ""]]}, {"id": "2001.07354", "submitter": "Honglong Cai", "authors": "Honglong Cai, Yuedong Fang, Zhiguan Wang, Tingchun Yeh, Jinxing Cheng", "title": "VMRFANet:View-Specific Multi-Receptive Field Attention Network for\n  Person Re-identification", "comments": "Accepted by ICAART2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) aims to retrieve the same person across\ndifferent cameras. In practice, it still remains a challenging task due to\nbackground clutter, variations on body poses and view conditions, inaccurate\nbounding box detection, etc. To tackle these issues, in this paper, we propose\na novel multi-receptive field attention (MRFA) module that utilizes filters of\nvarious sizes to help network focusing on informative pixels. Besides, we\npresent a view-specific mechanism that guides attention module to handle the\nvariation of view conditions. Moreover, we introduce a Gaussian horizontal\nrandom cropping/padding method which further improves the robustness of our\nproposed network. Comprehensive experiments demonstrate the effectiveness of\neach component. Our method achieves 95.5% / 88.1% in rank-1 / mAP on\nMarket-1501, 88.9% / 80.0% on DukeMTMC-reID, 81.1% / 78.8% on CUHK03 labeled\ndataset and 78.9% / 75.3% on CUHK03 detected dataset, outperforming current\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 06:31:18 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Cai", "Honglong", ""], ["Fang", "Yuedong", ""], ["Wang", "Zhiguan", ""], ["Yeh", "Tingchun", ""], ["Cheng", "Jinxing", ""]]}, {"id": "2001.07360", "submitter": "Christiane Sommer", "authors": "Christiane Sommer and Yumin Sun and Leonidas Guibas and Daniel Cremers\n  and Tolga Birdal", "title": "From Planes to Corners: Multi-Purpose Primitive Detection in Unorganized\n  3D Point Clouds", "comments": "Accepted to IEEE Robotics and Automation Letters 2020 | Video:\n  https://youtu.be/nHWJrA6RcB0 | Code:\n  https://github.com/c-sommer/orthogonal-planes", "journal-ref": "IEEE Robotics and Automation Letters 5(2) 2020, 1764-1771", "doi": "10.1109/LRA.2020.2969936", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for segmentation-free joint estimation of orthogonal\nplanes, their intersection lines, relationship graph and corners lying at the\nintersection of three orthogonal planes. Such unified scene exploration under\northogonality allows for multitudes of applications such as semantic plane\ndetection or local and global scan alignment, which in turn can aid robot\nlocalization or grasping tasks. Our two-stage pipeline involves a rough yet\njoint estimation of orthogonal planes followed by a subsequent joint refinement\nof plane parameters respecting their orthogonality relations. We form a graph\nof these primitives, paving the way to the extraction of further reliable\nfeatures: lines and corners. Our experiments demonstrate the validity of our\napproach in numerous scenarios from wall detection to 6D tracking, both on\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 06:51:47 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 15:32:37 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Sommer", "Christiane", ""], ["Sun", "Yumin", ""], ["Guibas", "Leonidas", ""], ["Cremers", "Daniel", ""], ["Birdal", "Tolga", ""]]}, {"id": "2001.07434", "submitter": "Monika Grewal", "authors": "Monika Grewal, Timo M. Deist, Jan Wiersma, Peter A. N. Bosman, Tanja\n  Alderliesten", "title": "An End-to-end Deep Learning Approach for Landmark Detection and Matching\n  in Medical Images", "comments": "SPIE Medical Imaging Conference - 2020", "journal-ref": null, "doi": "10.1117/12.2549302", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Anatomical landmark correspondences in medical images can provide additional\nguidance information for the alignment of two images, which, in turn, is\ncrucial for many medical applications. However, manual landmark annotation is\nlabor-intensive. Therefore, we propose an end-to-end deep learning approach to\nautomatically detect landmark correspondences in pairs of two-dimensional (2D)\nimages. Our approach consists of a Siamese neural network, which is trained to\nidentify salient locations in images as landmarks and predict matching\nprobabilities for landmark pairs from two different images. We trained our\napproach on 2D transverse slices from 168 lower abdominal Computed Tomography\n(CT) scans. We tested the approach on 22,206 pairs of 2D slices with varying\nlevels of intensity, affine, and elastic transformations. The proposed approach\nfinds an average of 639, 466, and 370 landmark matches per image pair for\nintensity, affine, and elastic transformations, respectively, with spatial\nmatching errors of at most 1 mm. Further, more than 99% of the landmark pairs\nare within a spatial matching error of 2 mm, 4 mm, and 8 mm for image pairs\nwith intensity, affine, and elastic transformations, respectively. To\ninvestigate the utility of our developed approach in a clinical setting, we\nalso tested our approach on pairs of transverse slices selected from follow-up\nCT scans of three patients. Visual inspection of the results revealed landmark\nmatches in both bony anatomical regions as well as in soft tissues lacking\nprominent intensity gradients.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 10:35:48 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Grewal", "Monika", ""], ["Deist", "Timo M.", ""], ["Wiersma", "Jan", ""], ["Bosman", "Peter A. N.", ""], ["Alderliesten", "Tanja", ""]]}, {"id": "2001.07437", "submitter": "Hyunjung Shim Dr.", "authors": "Junsuk Choe, Seong Joon Oh, Seungho Lee, Sanghyuk Chun, Zeynep Akata,\n  Hyunjung Shim", "title": "Evaluating Weakly Supervised Object Localization Methods Right", "comments": "CVPR 2020 camera-ready. First two authors contributed equally. Code:\n  https://github.com/clovaai/wsolevaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised object localization (WSOL) has gained popularity over the\nlast years for its promise to train localization models with only image-level\nlabels. Since the seminal WSOL work of class activation mapping (CAM), the\nfield has focused on how to expand the attention regions to cover objects more\nbroadly and localize them better. However, these strategies rely on full\nlocalization supervision to validate hyperparameters and for model selection,\nwhich is in principle prohibited under the WSOL setup. In this paper, we argue\nthat WSOL task is ill-posed with only image-level labels, and propose a new\nevaluation protocol where full supervision is limited to only a small held-out\nset not overlapping with the test set. We observe that, under our protocol, the\nfive most recent WSOL methods have not made a major improvement over the CAM\nbaseline. Moreover, we report that existing WSOL methods have not reached the\nfew-shot learning baseline, where the full-supervision at validation time is\nused for model training instead. Based on our findings, we discuss some future\ndirections for WSOL.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 10:50:06 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 05:35:21 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Choe", "Junsuk", ""], ["Oh", "Seong Joon", ""], ["Lee", "Seungho", ""], ["Chun", "Sanghyuk", ""], ["Akata", "Zeynep", ""], ["Shim", "Hyunjung", ""]]}, {"id": "2001.07442", "submitter": "Xiaofu Wu Dr", "authors": "Ben Xie, Xiaofu Wu, Suofei Zhang, Shiliang Zhao, Ming Li", "title": "Learning Diverse Features with Part-Level Resolution for Person\n  Re-Identification", "comments": "8 pages, 5 figures, submitted to IEEE TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning diverse features is key to the success of person re-identification.\nVarious part-based methods have been extensively proposed for learning local\nrepresentations, which, however, are still inferior to the best-performing\nmethods for person re-identification. This paper proposes to construct a strong\nlightweight network architecture, termed PLR-OSNet, based on the idea of\nPart-Level feature Resolution over the Omni-Scale Network (OSNet) for achieving\nfeature diversity. The proposed PLR-OSNet has two branches, one branch for\nglobal feature representation and the other branch for local feature\nrepresentation. The local branch employs a uniform partition strategy for\npart-level feature resolution but produces only a single identity-prediction\nloss, which is in sharp contrast to the existing part-based methods. Empirical\nevidence demonstrates that the proposed PLR-OSNet achieves state-of-the-art\nperformance on popular person Re-ID datasets, including Market1501,\nDukeMTMC-reID and CUHK03, despite its small model size.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 11:01:56 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Xie", "Ben", ""], ["Wu", "Xiaofu", ""], ["Zhang", "Suofei", ""], ["Zhao", "Shiliang", ""], ["Li", "Ming", ""]]}, {"id": "2001.07444", "submitter": "Prabhat Kumar", "authors": "Prabhat Kumar, Mayank Vatsa and Richa Singh", "title": "Detecting Face2Face Facial Reenactment in Videos", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual content has become the primary source of information, as evident in\nthe billions of images and videos, shared and uploaded on the Internet every\nsingle day. This has led to an increase in alterations in images and videos to\nmake them more informative and eye-catching for the viewers worldwide. Some of\nthese alterations are simple, like copy-move, and are easily detectable, while\nother sophisticated alterations like reenactment based DeepFakes are hard to\ndetect. Reenactment alterations allow the source to change the target\nexpressions and create photo-realistic images and videos. While technology can\nbe potentially used for several applications, the malicious usage of automatic\nreenactment has a very large social implication. It is therefore important to\ndevelop detection techniques to distinguish real images and videos with the\naltered ones. This research proposes a learning-based algorithm for detecting\nreenactment based alterations. The proposed algorithm uses a multi-stream\nnetwork that learns regional artifacts and provides a robust performance at\nvarious compression levels. We also propose a loss function for the balanced\nlearning of the streams for the proposed network. The performance is evaluated\non the publicly available FaceForensics dataset. The results show\nstate-of-the-art classification accuracy of 99.96%, 99.10%, and 91.20% for no,\neasy, and hard compression factors, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 11:03:50 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Kumar", "Prabhat", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}, {"id": "2001.07466", "submitter": "Zhentan Zheng", "authors": "Zhentan Zheng, Jianyi Liu", "title": "P$^2$-GAN: Efficient Style Transfer Using Single Style Image", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer is a useful image synthesis technique that can re-render given\nimage into another artistic style while preserving its content information.\nGenerative Adversarial Network (GAN) is a widely adopted framework toward this\ntask for its better representation ability on local style patterns than the\ntraditional Gram-matrix based methods. However, most previous methods rely on\nsufficient amount of pre-collected style images to train the model. In this\npaper, a novel Patch Permutation GAN (P$^2$-GAN) network that can efficiently\nlearn the stroke style from a single style image is proposed. We use patch\npermutation to generate multiple training samples from the given style image. A\npatch discriminator that can simultaneously process patch-wise images and\nnatural images seamlessly is designed. We also propose a local texture\ndescriptor based criterion to quantitatively evaluate the style transfer\nquality. Experimental results showed that our method can produce finer quality\nre-renderings from single style image with improved computational efficiency\ncompared with many state-of-the-arts methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 12:08:08 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 16:37:22 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Zheng", "Zhentan", ""], ["Liu", "Jianyi", ""]]}, {"id": "2001.07475", "submitter": "Kentaro Wada", "authors": "Kentaro Wada, Shingo Kitagawa, Kei Okada, Masayuki Inaba", "title": "Instance Segmentation of Visible and Occluded Regions for Finding and\n  Picking Target from a Pile of Objects", "comments": "8 pages, 11 figures, IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robotic system for picking a target from a pile of objects that\nis capable of finding and grasping the target object by removing obstacles in\nthe appropriate order. The fundamental idea is to segment instances with both\nvisible and occluded masks, which we call `instance occlusion segmentation'. To\nachieve this, we extend an existing instance segmentation model with a novel\n`relook' architecture, in which the model explicitly learns the inter-instance\nrelationship. Also, by using image synthesis, we make the system capable of\nhandling new objects without human annotations. The experimental results show\nthe effectiveness of the relook architecture when compared with a conventional\nmodel and of the image synthesis when compared to a human-annotated dataset. We\nalso demonstrate the capability of our system to achieve picking a target in a\ncluttered environment with a real robot.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 12:28:37 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wada", "Kentaro", ""], ["Kitagawa", "Shingo", ""], ["Okada", "Kei", ""], ["Inaba", "Masayuki", ""]]}, {"id": "2001.07481", "submitter": "Kentaro Wada", "authors": "Kentaro Wada, Kei Okada, Masayuki Inaba", "title": "Joint Learning of Instance and Semantic Segmentation for Robotic\n  Pick-and-Place with Heavy Occlusions in Clutter", "comments": "7 pages, 13 figures, IEEE International Conference on Robotics and\n  Automation (ICRA) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present joint learning of instance and semantic segmentation for visible\nand occluded region masks. Sharing the feature extractor with instance\nocclusion segmentation, we introduce semantic occlusion segmentation into the\ninstance segmentation model. This joint learning fuses the instance- and\nimage-level reasoning of the mask prediction on the different segmentation\ntasks, which was missing in the previous work of learning instance segmentation\nonly (instance-only). In the experiments, we evaluated the proposed joint\nlearning comparing the instance-only learning on the test dataset. We also\napplied the joint learning model to 2 different types of robotic pick-and-place\ntasks (random and target picking) and evaluated its effectiveness to achieve\nreal-world robotic tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 12:37:08 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wada", "Kentaro", ""], ["Okada", "Kei", ""], ["Inaba", "Masayuki", ""]]}, {"id": "2001.07575", "submitter": "Iman Marivani", "authors": "Iman Marivani, Evaggelia Tsiligianni, Bruno Cornelis and Nikos\n  Deligiannis", "title": "Multimodal Deep Unfolding for Guided Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of a high resolution image given a low resolution\nobservation is an ill-posed inverse problem in imaging. Deep learning methods\nrely on training data to learn an end-to-end mapping from a low-resolution\ninput to a high-resolution output. Unlike existing deep multimodal models that\ndo not incorporate domain knowledge about the problem, we propose a multimodal\ndeep learning design that incorporates sparse priors and allows the effective\nintegration of information from another image modality into the network\narchitecture. Our solution relies on a novel deep unfolding operator,\nperforming steps similar to an iterative algorithm for convolutional sparse\ncoding with side information; therefore, the proposed neural network is\ninterpretable by design. The deep unfolding architecture is used as a core\ncomponent of a multimodal framework for guided image super-resolution. An\nalternative multimodal design is investigated by employing residual learning to\nimprove the training efficiency. The presented multimodal approach is applied\nto super-resolution of near-infrared and multi-spectral images as well as depth\nupsampling using RGB images as side information. Experimental results show that\nour model outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 14:41:53 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Marivani", "Iman", ""], ["Tsiligianni", "Evaggelia", ""], ["Cornelis", "Bruno", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "2001.07577", "submitter": "Adrien Kaiser", "authors": "Adrien Kaiser, Jos\\'e Alonso Ybanez Zepeda, Tamy Boubekeur", "title": "Geometric Proxies for Live RGB-D Stream Enhancement and Consolidation", "comments": "extension of our ECCV 2018 paper at\n  http://openaccess.thecvf.com/content_ECCV_2018/html/Adrien_Kaiser_Proxy_Clouds_for_ECCV_2018_paper.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometric superstructure for unified real-time processing of\nRGB-D data. Modern RGB-D sensors are widely used for indoor 3D capture, with\napplications ranging from modeling to robotics, through augmented reality.\nNevertheless, their use is limited by their low resolution, with frames often\ncorrupted with noise, missing data and temporal inconsistencies. Our approach\nconsists in generating and updating through time a single set of compact local\nstatistics parameterized over detected geometric proxies, which are fed from\nraw RGB-D data. Our proxies provide several processing primitives, which\nimprove the quality of the RGB-D stream on the fly or lighten further\noperations. Experimental results confirm that our lightweight analysis\nframework copes well with embedded execution as well as moderate memory and\ncomputational capabilities compared to state-of-the-art methods. Processing\nRGB-D data with our proxies allows noise and temporal flickering removal, hole\nfilling and resampling. As a substitute of the observed scene, our proxies can\nadditionally be applied to compression and scene reconstruction. We present\nexperiments performed with our framework in indoor scenes of different natures\nwithin a recent open RGB-D dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 14:42:35 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Kaiser", "Adrien", ""], ["Zepeda", "Jos\u00e9 Alonso Ybanez", ""], ["Boubekeur", "Tamy", ""]]}, {"id": "2001.07582", "submitter": "Yadong Zhang", "authors": "Yadong Zhang and Xin Chen", "title": "Motif Difference Field: A Simple and Effective Image Representation of\n  Time Series for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series motifs play an important role in the time series analysis. The\nmotif-based time series clustering is used for the discovery of higher-order\npatterns or structures in time series data. Inspired by the convolutional\nneural network (CNN) classifier based on the image representations of time\nseries, motif difference field (MDF) is proposed. Compared to other image\nrepresentations of time series, MDF is simple and easy to construct. With the\nFully Convolution Network (FCN) as the classifier, MDF demonstrates the\nsuperior performance on the UCR time series dataset in benchmark with other\ntime series classification methods. It is interesting to find that the triadic\ntime series motifs give the best result in the test. Due to the motif\nclustering reflected in MDF, the significant motifs are detected with the help\nof the Gradient-weighted Class Activation Mapping (Grad-CAM). The areas in MDF\nwith high weight in Grad-CAM have a high contribution from the significant\nmotifs with the desired ordinal patterns associated with the signature patterns\nin time series. However, the signature patterns cannot be identified with the\nneural network classifiers directly based on the time series.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 14:48:43 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zhang", "Yadong", ""], ["Chen", "Xin", ""]]}, {"id": "2001.07613", "submitter": "Kyoung-Woon On", "authors": "Kyoung-Woon On, Eun-Sol Kim, Yu-Jung Heo and Byoung-Tak Zhang", "title": "Cut-Based Graph Learning Networks to Discover Compositional Structure of\n  Sequential Video Data", "comments": "8 pages, 3 figures, Association for the Advancement of Artificial\n  Intelligence (AAAI2020). arXiv admin note: substantial text overlap with\n  arXiv:1907.01709", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional sequential learning methods such as Recurrent Neural Networks\n(RNNs) focus on interactions between consecutive inputs, i.e. first-order\nMarkovian dependency. However, most of sequential data, as seen with videos,\nhave complex dependency structures that imply variable-length semantic flows\nand their compositions, and those are hard to be captured by conventional\nmethods. Here, we propose Cut-Based Graph Learning Networks (CB-GLNs) for\nlearning video data by discovering these complex structures of the video. The\nCB-GLNs represent video data as a graph, with nodes and edges corresponding to\nframes of the video and their dependencies respectively. The CB-GLNs find\ncompositional dependencies of the data in multilevel graph forms via a\nparameterized kernel with graph-cut and a message passing framework. We\nevaluate the proposed method on the two different tasks for video\nunderstanding: Video theme classification (Youtube-8M dataset) and Video\nQuestion and Answering (TVQA dataset). The experimental results show that our\nmodel efficiently learns the semantic compositional structure of video data.\nFurthermore, our model achieves the highest performance in comparison to other\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 10:09:24 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["On", "Kyoung-Woon", ""], ["Kim", "Eun-Sol", ""], ["Heo", "Yu-Jung", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "2001.07626", "submitter": "Peter Hirsch", "authors": "Peter Hirsch, Lisa Mais, Dagmar Kainmueller", "title": "PatchPerPix for Instance Segmentation", "comments": "ECCV2020, code: https://github.com/Kainmueller-Lab/PatchPerPix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for proposal free instance segmentation that can\nhandle sophisticated object shapes which span large parts of an image and form\ndense object clusters with crossovers. Our method is based on predicting dense\nlocal shape descriptors, which we assemble to form instances. All instances are\nassembled simultaneously in one go. To our knowledge, our method is the first\nnon-iterative method that yields instances that are composed of learnt shape\npatches. We evaluate our method on a diverse range of data domains, where it\ndefines the new state of the art on four benchmarks, namely the ISBI 2012 EM\nsegmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d\nfluorescence microscopy data of cell nuclei. We show furthermore that our\nmethod also applies to 3d light microscopy data of Drosophila neurons, which\nexhibit extreme cases of complex shape clusters\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 16:06:51 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 10:41:28 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 15:45:11 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hirsch", "Peter", ""], ["Mais", "Lisa", ""], ["Kainmueller", "Dagmar", ""]]}, {"id": "2001.07627", "submitter": "Maciej A. Czyzewski", "authors": "Maciej A. Czyzewski", "title": "batchboost: regularization for stabilizing training with resistance to\n  underfitting & overfitting", "comments": "6 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overfitting & underfitting and stable training are an important challenges in\nmachine learning. Current approaches for these issues are mixup, SamplePairing\nand BC learning. In our work, we state the hypothesis that mixing many images\ntogether can be more effective than just two. Batchboost pipeline has three\nstages: (a) pairing: method of selecting two samples. (b) mixing: how to create\na new one from two samples. (c) feeding: combining mixed samples with new ones\nfrom dataset into batch (with ratio $\\gamma$). Note that sample that appears in\nour batch propagates with subsequent iterations with less and less importance\nuntil the end of training. Pairing stage calculates the error per sample, sorts\nthe samples and pairs with strategy: hardest with easiest one, than mixing\nstage merges two samples using mixup, $x_1 + (1-\\lambda)x_2$. Finally, feeding\nstage combines new samples with mixed by ratio 1:1. Batchboost has 0.5-3%\nbetter accuracy than the current state-of-the-art mixup regularization on\nCIFAR-10 & Fashion-MNIST. Our method is slightly better than SamplePairing\ntechnique on small datasets (up to 5%). Batchboost provides stable training on\nnot tuned parameters (like weight decay), thus its a good method to test\nperformance of different architectures. Source code is at:\nhttps://github.com/maciejczyzewski/batchboost\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 16:07:27 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Czyzewski", "Maciej A.", ""]]}, {"id": "2001.07631", "submitter": "Sizhe Chen", "authors": "Zhixing Ye, Sizhe Chen, Peidong Zhang, Chengjin Sun, Xiaolin Huang", "title": "HRFA: High-Resolution Feature-based Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks have long been developed for revealing the vulnerability\nof Deep Neural Networks (DNNs) by adding imperceptible perturbations to the\ninput. Most methods generate perturbations like normal noise, which is not\ninterpretable and without semantic meaning. In this paper, we propose\nHigh-Resolution Feature-based Attack (HRFA), yielding authentic adversarial\nexamples with up to $1024 \\times 1024$ resolution. HRFA exerts attack by\nmodifying the latent feature representation of the image, i.e., the gradients\nback propagate not only through the victim DNN, but also through the generative\nmodel that maps the feature space to the image space. In this way, HRFA\ngenerates adversarial examples that are in high-resolution, realistic,\nnoise-free, and hence is able to evade several denoising-based defenses. In the\nexperiment, the effectiveness of HRFA is validated by attacking the object\nclassification and face verification tasks with BigGAN and StyleGAN,\nrespectively. The advantages of HRFA are verified from the high quality, high\nauthenticity, and high attack success rate faced with defenses.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 16:21:20 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 13:08:04 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ye", "Zhixing", ""], ["Chen", "Sizhe", ""], ["Zhang", "Peidong", ""], ["Sun", "Chengjin", ""], ["Huang", "Xiaolin", ""]]}, {"id": "2001.07645", "submitter": "Jesse Sun", "authors": "Jesse Sun, Fatemeh Darbehani, Mark Zaidi, and Bo Wang", "title": "SAUNet: Shape Attentive U-Net for Interpretable Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is a difficult but important task for many\nclinical operations such as cardiac bi-ventricular volume estimation. More\nrecently, there has been a shift to utilizing deep learning and fully\nconvolutional neural networks (CNNs) to perform image segmentation that has\nyielded state-of-the-art results in many public benchmark datasets. Despite the\nprogress of deep learning in medical image segmentation, standard CNNs are\nstill not fully adopted in clinical settings as they lack robustness and\ninterpretability. Shapes are generally more meaningful features than solely\ntextures of images, which are features regular CNNs learn, causing a lack of\nrobustness. Likewise, previous works surrounding model interpretability have\nbeen focused on post hoc gradient-based saliency methods. However,\ngradient-based saliency methods typically require additional computations post\nhoc and have been shown to be unreliable for interpretability. Thus, we present\na new architecture called Shape Attentive U-Net (SAUNet) which focuses on model\ninterpretability and robustness. The proposed architecture attempts to address\nthese limitations by the use of a secondary shape stream that captures rich\nshape-dependent information in parallel with the regular texture stream.\nFurthermore, we suggest multi-resolution saliency maps can be learned using our\ndual-attention decoder module which allows for multi-level interpretability and\nmitigates the need for additional computations post hoc. Our method also\nachieves state-of-the-art results on the two large public cardiac MRI image\nsegmentation datasets of SUN09 and AC17.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 16:48:54 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 18:13:37 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 17:59:21 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Sun", "Jesse", ""], ["Darbehani", "Fatemeh", ""], ["Zaidi", "Mark", ""], ["Wang", "Bo", ""]]}, {"id": "2001.07685", "submitter": "Kihyuk Sohn", "authors": "Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas\n  Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel", "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and\n  Confidence", "comments": "Published at NeurIPS 2020 as a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) provides an effective means of leveraging\nunlabeled data to improve a model's performance. In this paper, we demonstrate\nthe power of a simple combination of two common SSL methods: consistency\nregularization and pseudo-labeling. Our algorithm, FixMatch, first generates\npseudo-labels using the model's predictions on weakly-augmented unlabeled\nimages. For a given image, the pseudo-label is only retained if the model\nproduces a high-confidence prediction. The model is then trained to predict the\npseudo-label when fed a strongly-augmented version of the same image. Despite\nits simplicity, we show that FixMatch achieves state-of-the-art performance\nacross a variety of standard semi-supervised learning benchmarks, including\n94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just\n4 labels per class. Since FixMatch bears many similarities to existing SSL\nmethods that achieve worse performance, we carry out an extensive ablation\nstudy to tease apart the experimental factors that are most important to\nFixMatch's success. We make our code available at\nhttps://github.com/google-research/fixmatch.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 18:32:27 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 17:22:06 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Sohn", "Kihyuk", ""], ["Berthelot", "David", ""], ["Li", "Chun-Liang", ""], ["Zhang", "Zizhao", ""], ["Carlini", "Nicholas", ""], ["Cubuk", "Ekin D.", ""], ["Kurakin", "Alex", ""], ["Zhang", "Han", ""], ["Raffel", "Colin", ""]]}, {"id": "2001.07710", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Wei Niu, Tianyun Zhang, Sijia Liu, Sheng Lin, Hongjia Li,\n  Xiang Chen, Jian Tang, Kaisheng Ma, Bin Ren, Yanzhi Wang", "title": "An Image Enhancing Pattern-based Sparsity for Real-time Inference on\n  Mobile Devices", "comments": "Paper accepted in the 16th European Conference on Computer Vision\n  (ECCV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight pruning has been widely acknowledged as a straightforward and\neffective method to eliminate redundancy in Deep Neural Networks (DNN), thereby\nachieving acceleration on various platforms. However, most of the pruning\ntechniques are essentially trade-offs between model accuracy and regularity\nwhich lead to impaired inference accuracy and limited on-device acceleration\nperformance. To solve the problem, we introduce a new sparsity dimension,\nnamely pattern-based sparsity that comprises pattern and connectivity sparsity,\nand becoming both highly accurate and hardware friendly. With carefully\ndesigned patterns, the proposed pruning unprecedentedly and consistently\nachieves accuracy enhancement and better feature extraction ability on\ndifferent DNN structures and datasets, and our pattern-aware pruning framework\nalso achieves pattern library extraction, pattern selection, pattern and\nconnectivity pruning and weight training simultaneously. Our approach on the\nnew pattern-based sparsity naturally fits into compiler optimization for highly\nefficient DNN execution on mobile platforms. To the best of our knowledge, it\nis the first time that mobile devices achieve real-time inference for the\nlarge-scale DNN models thanks to the unique spatial property of pattern-based\nsparsity and the help of the code generation capability of compilers.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 16:17:36 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 02:55:08 GMT"}, {"version": "v3", "created": "Sun, 5 Jul 2020 01:22:19 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ma", "Xiaolong", ""], ["Niu", "Wei", ""], ["Zhang", "Tianyun", ""], ["Liu", "Sijia", ""], ["Lin", "Sheng", ""], ["Li", "Hongjia", ""], ["Chen", "Xiang", ""], ["Tang", "Jian", ""], ["Ma", "Kaisheng", ""], ["Ren", "Bin", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2001.07715", "submitter": "Heng Yang", "authors": "Heng Yang, Jingnan Shi, Luca Carlone", "title": "TEASER: Fast and Certifiable Point Cloud Registration", "comments": "Accepted to IEEE Transactions on Robotics (T-RO). Code:\n  https://github.com/MIT-SPARK/TEASER-plusplus. 20 pages main text, 24 pages\n  appendix", "journal-ref": "IEEE Transactions on Robotics (T-RO), 2020", "doi": null, "report-no": null, "categories": "cs.RO cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first fast and certifiable algorithm for the registration of\ntwo sets of 3D points in the presence of large amounts of outlier\ncorrespondences. We first reformulate the registration problem using a\nTruncated Least Squares (TLS) cost that is insensitive to a large fraction of\nspurious correspondences. Then, we provide a general graph-theoretic framework\nto decouple scale, rotation, and translation estimation, which allows solving\nin cascade for the three transformations. Despite the fact that each subproblem\nis still non-convex and combinatorial in nature, we show that (i) TLS scale and\n(component-wise) translation estimation can be solved in polynomial time via\nadaptive voting, (ii) TLS rotation estimation can be relaxed to a semidefinite\nprogram (SDP) and the relaxation is tight, even in the presence of extreme\noutlier rates, and (iii) the graph-theoretic framework allows drastic pruning\nof outliers by finding the maximum clique. We name the resulting algorithm\nTEASER (Truncated least squares Estimation And SEmidefinite Relaxation). While\nsolving large SDP relaxations is typically slow, we develop a second fast and\ncertifiable algorithm, named TEASER++, that uses graduated non-convexity to\nsolve the rotation subproblem and leverages Douglas-Rachford Splitting to\nefficiently certify global optimality.\n  For both algorithms, we provide theoretical bounds on the estimation errors,\nwhich are the first of their kind for robust registration problems. Moreover,\nwe test their performance on standard, object detection, and the 3DMatch\nbenchmarks, and show that (i) both algorithms dominate the state of the art and\nare robust to more than 99% outliers, (ii) TEASER++ can run in milliseconds,\nand (iii) TEASER++ is so robust it can also solve problems without\ncorrespondences, where it largely outperforms ICP and it is more accurate than\nGo-ICP while being orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 18:56:00 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 20:03:04 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yang", "Heng", ""], ["Shi", "Jingnan", ""], ["Carlone", "Luca", ""]]}, {"id": "2001.07739", "submitter": "Joy Egede", "authors": "Joy O. Egede, Siyang Song, Temitayo A. Olugbade, Chongyang Wang,\n  Amanda Williams, Hongying Meng, Min Aung, Nicholas D. Lane, Michel Valstar\n  and Nadia Bianchi-Berthouze", "title": "EMOPAIN Challenge 2020: Multimodal Pain Evaluation from Facial and\n  Bodily Expressions", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EmoPain 2020 Challenge is the first international competition aimed at\ncreating a uniform platform for the comparison of machine learning and\nmultimedia processing methods of automatic chronic pain assessment from human\nexpressive behaviour, and also the identification of pain-related behaviours.\nThe objective of the challenge is to promote research in the development of\nassistive technologies that help improve the quality of life for people with\nchronic pain via real-time monitoring and feedback to help manage their\ncondition and remain physically active. The challenge also aims to encourage\nthe use of the relatively underutilised, albeit vital bodily expression signals\nfor automatic pain and pain-related emotion recognition. This paper presents a\ndescription of the challenge, competition guidelines, bench-marking dataset,\nand the baseline systems' architecture and performance on the three sub-tasks:\npain estimation from facial expressions, pain recognition from multimodal\nmovement, and protective movement behaviour detection.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 19:09:08 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 12:11:08 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 16:14:31 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Egede", "Joy O.", ""], ["Song", "Siyang", ""], ["Olugbade", "Temitayo A.", ""], ["Wang", "Chongyang", ""], ["Williams", "Amanda", ""], ["Meng", "Hongying", ""], ["Aung", "Min", ""], ["Lane", "Nicholas D.", ""], ["Valstar", "Michel", ""], ["Bianchi-Berthouze", "Nadia", ""]]}, {"id": "2001.07761", "submitter": "Koki Madono", "authors": "Koki Madono, Masayuki Tanaka, Masaki Onishi, Tetsuji Ogawa", "title": "Block-wise Scrambled Image Recognition Using Adaptation Network", "comments": "6 pages Artificial Intelligence of Things(AAAI-2020 WS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a perceptually hidden object-recognition method is\ninvestigated to generate secure images recognizable by humans but not machines.\nHence, both the perceptual information hiding and the corresponding object\nrecognition methods should be developed. Block-wise image scrambling is\nintroduced to hide perceptual information from a third party. In addition, an\nadaptation network is proposed to recognize those scrambled images.\nExperimental comparisons conducted using CIFAR datasets demonstrated that the\nproposed adaptation network performed well in incorporating simple perceptual\ninformation hiding into DNN-based image classification.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 20:22:10 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Madono", "Koki", ""], ["Tanaka", "Masayuki", ""], ["Onishi", "Masaki", ""], ["Ogawa", "Tetsuji", ""]]}, {"id": "2001.07766", "submitter": "Seyed Mehdi Ayyoubzadeh", "authors": "Seyed Mehdi Ayyoubzadeh, Xiaolin Wu", "title": "Adaptive Loss Function for Super Resolution Neural Networks Using Convex\n  Optimization Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Image Super-Resolution (SISR) task refers to learn a mapping from\nlow-resolution images to the corresponding high-resolution ones. This task is\nknown to be extremely difficult since it is an ill-posed problem. Recently,\nConvolutional Neural Networks (CNNs) have achieved state of the art performance\non SISR. However, the images produced by CNNs do not contain fine details of\nthe images. Generative Adversarial Networks (GANs) aim to solve this issue and\nrecover sharp details. Nevertheless, GANs are notoriously difficult to train.\nBesides that, they generate artifacts in the high-resolution images. In this\npaper, we have proposed a method in which CNNs try to align images in different\nspaces rather than only the pixel space. Such a space is designed using convex\noptimization techniques. CNNs are encouraged to learn high-frequency components\nof the images as well as low-frequency components. We have shown that the\nproposed method can recover fine details of the images and it is stable in the\ntraining process.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 20:31:10 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Ayyoubzadeh", "Seyed Mehdi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2001.07776", "submitter": "Jinzheng Cai", "authors": "Jinzheng Cai, Adam P. Harrison, Youjing Zheng, Ke Yan, Yuankai Huo,\n  Jing Xiao, Lin Yang, Le Lu", "title": "Lesion Harvester: Iteratively Mining Unlabeled Lesions and Hard-Negative\n  Examples at Scale", "comments": "13 pages, 13 figures, to appear in IEEE Transactions on Medical\n  Imaging", "journal-ref": "IEEE Transactions on Medical Imaging, 2020", "doi": "10.1109/TMI.2020.3022034", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring large-scale medical image data, necessary for training machine\nlearning algorithms, is frequently intractable, due to prohibitive\nexpert-driven annotation costs. Recent datasets extracted from hospital\narchives, e.g., DeepLesion, have begun to address this problem. However, these\nare often incompletely or noisily labeled, e.g., DeepLesion leaves over 50% of\nits lesions unlabeled. Thus, effective methods to harvest missing annotations\nare critical for continued progress in medical image analysis. This is the goal\nof our work, where we develop a powerful system to harvest missing lesions from\nthe DeepLesion dataset at high precision. Accepting the need for some degree of\nexpert labor to achieve high fidelity, we exploit a small fully-labeled subset\nof medical image volumes and use it to intelligently mine annotations from the\nremainder. To do this, we chain together a highly sensitive lesion proposal\ngenerator and a very selective lesion proposal classifier. While our framework\nis generic, we optimize our performance by proposing a 3D contextual lesion\nproposal generator and by using a multi-view multi-scale lesion proposal\nclassifier. These produce harvested and hard-negative proposals, which we then\nre-use to finetune our proposal generator by using a novel hard negative\nsuppression loss, continuing this process until no extra lesions are found.\nExtensive experimental analysis demonstrates that our method can harvest an\nadditional 9,805 lesions while keeping precision above 90%. To demonstrate the\nbenefits of our approach, we show that lesion detectors trained on our\nharvested lesions can significantly outperform the same variants only trained\non the original annotations, with boost of average precision of 7% to 10%. We\nopen source our annotations at\nhttps://github.com/JimmyCai91/DeepLesionAnnotation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 21:09:49 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 05:09:07 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 17:50:20 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Cai", "Jinzheng", ""], ["Harrison", "Adam P.", ""], ["Zheng", "Youjing", ""], ["Yan", "Ke", ""], ["Huo", "Yuankai", ""], ["Xiao", "Jing", ""], ["Yang", "Lin", ""], ["Lu", "Le", ""]]}, {"id": "2001.07791", "submitter": "Pallabi Ghosh", "authors": "Pallabi Ghosh, Vibhav Vineet, Larry S. Davis, Abhinav Shrivastava,\n  Sudipta Sinha, Neel Joshi", "title": "Depth Completion Using a View-constrained Deep Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that the structure of convolutional neural networks\n(CNNs) induces a strong prior that favors natural images. This prior, known as\na deep image prior (DIP), is an effective regularizer in inverse problems such\nas image denoising and inpainting. We extend the concept of the DIP to depth\nimages. Given color images and noisy and incomplete target depth maps, we\noptimize a randomly-initialized CNN model to reconstruct a depth map restored\nby virtue of using the CNN network structure as a prior combined with a\nview-constrained photo-consistency loss. This loss is computed using images\nfrom a geometrically calibrated camera from nearby viewpoints. We apply this\ndeep depth prior for inpainting and refining incomplete and noisy depth maps\nwithin both binocular and multi-view stereo pipelines. Our quantitative and\nqualitative evaluation shows that our refined depth maps are more accurate and\ncomplete, and after fusion, produces dense 3D models of higher quality.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 21:56:01 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 19:27:55 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 22:07:55 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Ghosh", "Pallabi", ""], ["Vineet", "Vibhav", ""], ["Davis", "Larry S.", ""], ["Shrivastava", "Abhinav", ""], ["Sinha", "Sudipta", ""], ["Joshi", "Neel", ""]]}, {"id": "2001.07792", "submitter": "Yanmao Man", "authors": "Yanmao Man, Ming Li, Ryan Gerdes", "title": "GhostImage: Remote Perception Attacks against Camera-based Image\n  Classification Systems", "comments": "Accepted by USENIX RAID 2020. Source code is available at\n  https://github.com/Harry1993/GhostImage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vision-based object classification systems imaging sensors perceive the\nenvironment and machine learning is then used to detect and classify objects\nfor decision-making purposes; e.g., to maneuver an automated vehicle around an\nobstacle or to raise an alarm to indicate the presence of an intruder in\nsurveillance settings. In this work we demonstrate how the perception domain\ncan be remotely and unobtrusively exploited to enable an attacker to create\nspurious objects or alter an existing object. An automated system relying on a\ndetection/classification framework subject to our attack could be made to\nundertake actions with catastrophic results due to attacker-induced\nmisperception.\n  We focus on camera-based systems and show that it is possible to remotely\nproject adversarial patterns into camera systems by exploiting two common\neffects in optical imaging systems, viz., lens flare/ghost effects and\nauto-exposure control. To improve the robustness of the attack to channel\neffects, we generate optimal patterns by integrating adversarial machine\nlearning techniques with a trained end-to-end channel model. We experimentally\ndemonstrate our attacks using a low-cost projector, on three different image\ndatasets, in indoor and outdoor environments, and with three different cameras.\nExperimental results show that, depending on the projector-camera distance,\nattack success rates can reach as high as 100% and under targeted conditions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 21:58:45 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 23:56:05 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 20:13:52 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Man", "Yanmao", ""], ["Li", "Ming", ""], ["Gerdes", "Ryan", ""]]}, {"id": "2001.07793", "submitter": "Ashraful Islam", "authors": "Ashraful Islam, Richard J. Radke", "title": "Weakly Supervised Temporal Action Localization Using Deep Metric\n  Learning", "comments": "accepted to WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization is an important step towards video\nunderstanding. Most current action localization methods depend on untrimmed\nvideos with full temporal annotations of action instances. However, it is\nexpensive and time-consuming to annotate both action labels and temporal\nboundaries of videos. To this end, we propose a weakly supervised temporal\naction localization method that only requires video-level action instances as\nsupervision during training. We propose a classification module to generate\naction labels for each segment in the video, and a deep metric learning module\nto learn the similarity between different action instances. We jointly optimize\na balanced binary cross-entropy loss and a metric loss using a standard\nbackpropagation algorithm. Extensive experiments demonstrate the effectiveness\nof both of these components in temporal localization. We evaluate our algorithm\non two challenging untrimmed video datasets: THUMOS14 and ActivityNet1.2. Our\napproach improves the current state-of-the-art result for THUMOS14 by 6.5% mAP\nat IoU threshold 0.5, and achieves competitive performance for ActivityNet1.2.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 22:01:17 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Islam", "Ashraful", ""], ["Radke", "Richard J.", ""]]}, {"id": "2001.07799", "submitter": "Ziyue Xiang", "authors": "Ziyue Xiang, Daniel E. Acuna", "title": "Scientific Image Tampering Detection Based On Noise Inconsistencies: A\n  Method And Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific image tampering is a problem that affects not only authors but\nalso the general perception of the research community. Although previous\nresearchers have developed methods to identify tampering in natural images,\nthese methods may not thrive under the scientific setting as scientific images\nhave different statistics, format, quality, and intentions. Therefore, we\npropose a scientific-image specific tampering detection method based on noise\ninconsistencies, which is capable of learning and generalizing to different\nfields of science. We train and test our method on a new dataset of manipulated\nwestern blot and microscopy imagery, which aims at emulating problematic images\nin science. The test results show that our method can detect various types of\nimage manipulation in different scenarios robustly, and it outperforms existing\ngeneral-purpose image tampering detection schemes. We discuss applications\nbeyond these two types of images and suggest next steps for making detection of\nproblematic images a systematic step in peer review and science in general.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 22:29:56 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 20:46:46 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Xiang", "Ziyue", ""], ["Acuna", "Daniel E.", ""]]}, {"id": "2001.07809", "submitter": "Subhayan Mukherjee", "authors": "Subhayan Mukherjee, Ram Mohana Reddy Guddeti", "title": "Depth-Based Selective Blurring in Stereo Images Using Accelerated\n  Framework", "comments": "arXiv admin note: text overlap with arXiv:2001.06967", "journal-ref": "3D Research (Springer) 5, Article number: 14 (2014)", "doi": "10.1007/s13319-014-0014-7", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid method for stereo disparity estimation by combining block\nand region-based stereo matching approaches. It generates dense depth maps from\ndisparity measurements of only 18 % image pixels (left or right). The\nmethodology involves segmenting pixel lightness values using fast K-Means\nimplementation, refining segment boundaries using morphological filtering and\nconnected components analysis; then determining boundaries' disparities using\nsum of absolute differences (SAD) cost function. Complete disparity maps are\nreconstructed from boundaries' disparities. We consider an application of our\nmethod for depth-based selective blurring of non-interest regions of stereo\nimages, using Gaussian blur to de-focus users' non-interest regions.\nExperiments on Middlebury dataset demonstrate that our method outperforms\ntraditional disparity estimation approaches using SAD and normalized cross\ncorrelation by up to 33.6 % and some recent methods by up to 6.1 %. Further,\nour method is highly parallelizable using CPU and GPU framework based on Java\nThread Pool and APARAPI with speed-up of 5.8 for 250 stereo video frames (4,096\nx 2,304).\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 23:26:39 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Mukherjee", "Subhayan", ""], ["Guddeti", "Ram Mohana Reddy", ""]]}, {"id": "2001.07832", "submitter": "Angfan Zhu", "authors": "Angfan Zhu, Jiaqi Yang, Weiyue Zhao, Zhiguo Cao", "title": "LRF-Net: Learning Local Reference Frames for 3D Local Shape Description\n  and Matching", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local reference frame (LRF) acts as a critical role in 3D local shape\ndescription and matching. However, most of existing LRFs are hand-crafted and\nsuffer from limited repeatability and robustness. This paper presents the first\nattempt to learn an LRF via a Siamese network that needs weak supervision only.\nIn particular, we argue that each neighboring point in the local surface gives\na unique contribution to LRF construction and measure such contributions via\nlearned weights. Extensive analysis and comparative experiments on three public\ndatasets addressing different application scenarios have demonstrated that\nLRF-Net is more repeatable and robust than several state-of-the-art LRF methods\n(LRF-Net is only trained on one dataset). In addition, LRF-Net can\nsignificantly boost the local shape description and 6-DoF pose estimation\nperformance when matching 3D point clouds.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 00:52:48 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 00:13:41 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhu", "Angfan", ""], ["Yang", "Jiaqi", ""], ["Zhao", "Weiyue", ""], ["Cao", "Zhiguo", ""]]}, {"id": "2001.07847", "submitter": "Hisaichi Shibata", "authors": "H. Shibata (1), S. Hanaoka (2), Y. Nomura (1), T. Nakao (1), I. Sato\n  (2 and 4 and 5), D. Sato (3), N. Hayashi (1) and O. Abe (2 and 3) ((1)\n  Department of Computational Diagnostic Radiology and Preventive Medicine, The\n  University of Tokyo Hospital, (2) Department of Radiology, The University of\n  Tokyo Hospital, (3) Division of Radiology and Biomedical Engineering,\n  Graduate School of Medicine, The University of Tokyo, (4) Department of\n  Computer Science, Graduate School of Information Science and Technology, The\n  University of Tokyo, (5) Center for Advanced Intelligence Project, RIKEN)", "title": "A versatile anomaly detection method for medical images with a\n  flow-based generative model in semi-supervision setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oversight in medical images is a crucial problem, and timely reporting of\nmedical images is desired. Therefore, an all-purpose anomaly detection method\nthat can detect virtually all types of lesions/diseases in a given image is\nstrongly desired. However, few commercially available and versatile anomaly\ndetection methods for medical images have been provided so far. Recently,\nanomaly detection methods built upon deep learning methods have been rapidly\ngrowing in popularity, and these methods seem to provide reasonable solutions\nto the problem. However, the workload to label the images necessary for\ntraining in deep learning remains heavy. In this study, we present an anomaly\ndetection method based on two trained flow-based generative models. With this\nmethod, the posterior probability can be computed as a normality metric for any\ngiven image. The training of the generative models requires two sets of images:\na set containing only normal images and another set containing both normal and\nabnormal images without any labels. In the latter set, each sample does not\nhave to be labeled as normal or abnormal; therefore, any mixture of images\n(e.g., all cases in a hospital) can be used as the dataset without cumbersome\nmanual labeling. The method was validated with two types of medical images:\nchest X-ray radiographs (CXRs) and brain computed tomographies (BCTs). The\nareas under the receiver operating characteristic curves for logarithm\nposterior probabilities of CXRs (0.868 for pneumonia-like opacities) and BCTs\n(0.904 for infarction) were comparable to those in previous studies with other\nanomaly detection methods. This result showed the versatility of our method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 02:01:57 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 13:00:36 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 07:09:03 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Shibata", "H.", "", "2 and 4 and 5"], ["Hanaoka", "S.", "", "2 and 4 and 5"], ["Nomura", "Y.", "", "2 and 4 and 5"], ["Nakao", "T.", "", "2 and 4 and 5"], ["Sato", "I.", "", "2 and 4 and 5"], ["Sato", "D.", "", "2 and 3"], ["Hayashi", "N.", "", "2 and 3"], ["Abe", "O.", "", "2 and 3"]]}, {"id": "2001.07871", "submitter": "Sara Shahsavarani", "authors": "Sara Shahsavarani, Morteza Analoui and Reza Shoja Ghiass", "title": "M^2 Deep-ID: A Novel Model for Multi-View Face Identification Using\n  Convolutional Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant advances in Deep Face Recognition (DFR) systems,\nintroducing new DFRs under specific constraints such as varying pose still\nremains a big challenge. Most particularly, due to the 3D nature of a human\nhead, facial appearance of the same subject introduces a high intra-class\nvariability when projected to the camera image plane. In this paper, we propose\na new multi-view Deep Face Recognition (MVDFR) system to address the mentioned\nchallenge. In this context, multiple 2D images of each subject under different\nviews are fed into the proposed deep neural network with a unique design to\nre-express the facial features in a single and more compact face descriptor,\nwhich in turn, produces a more informative and abstract way for face\nidentification using convolutional neural networks. To extend the functionality\nof our proposed system to multi-view facial images, the golden standard Deep-ID\nmodel is modified in our proposed model. The experimental results indicate that\nour proposed method yields a 99.8% accuracy, while the state-of-the-art method\nachieves a 97% accuracy. We also gathered the Iran University of Science and\nTechnology (IUST) face database with 6552 images of 504 subjects to accomplish\nour experiments.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 04:13:18 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Shahsavarani", "Sara", ""], ["Analoui", "Morteza", ""], ["Ghiass", "Reza Shoja", ""]]}, {"id": "2001.07884", "submitter": "Hao Liu", "authors": "Yuchen He, Sung Ha Kang, Hao Liu", "title": "Curvature Regularized Surface Reconstruction from Point Cloud", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variational functional and fast algorithms to reconstruct\nimplicit surface from point cloud data with a curvature constraint. The\nminimizing functional balances the distance function from the point cloud and\nthe mean curvature term. Only the point location is used, without any local\nnormal or curvature estimation at each point. With the added curvature\nconstraint, the computation becomes particularly challenging. To enhance the\ncomputational efficiency, we solve the problem by a novel operator splitting\nscheme. It replaces the original high-order PDEs by a decoupled PDE system,\nwhich is solved by a semi-implicit method. We also discuss approach using an\naugmented Lagrangian method. The proposed method shows robustness against\nnoise, and recovers concave features and sharp corners better compared to\nmodels without curvature constraint. Numerical experiments in two and three\ndimensional data sets, noisy and sparse data are presented to validate the\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 05:34:40 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 03:22:10 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["He", "Yuchen", ""], ["Kang", "Sung Ha", ""], ["Liu", "Hao", ""]]}, {"id": "2001.07895", "submitter": "Ryuhei Takahashi", "authors": "Ryuhei Takahashi, Atsushi Hashimoto, Motoharu Sonogashira, Masaaki\n  Iiyama", "title": "Partially-Shared Variational Auto-encoders for Unsupervised Domain\n  Adaptation with Target Shift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for unsupervised domain adaptation (UDA)\nwith target shift. Target shift is a problem of mismatch in label distribution\nbetween source and target domains. Typically it appears as class-imbalance in\ntarget domain. In practice, this is an important problem in UDA; as we do not\nknow labels in target domain datasets, we do not know whether or not its\ndistribution is identical to that in the source domain dataset. Many\ntraditional approaches achieve UDA with distribution matching by minimizing\nmean maximum discrepancy or adversarial training; however these approaches\nimplicitly assume a coincidence in the distributions and do not work under\nsituations with target shift. Some recent UDA approaches focus on class\nboundary and some of them are robust to target shift, but they are only\napplicable to classification and not to regression.\n  To overcome the target shift problem in UDA, the proposed method, partially\nshared variational autoencoders (PS-VAEs), uses pair-wise feature alignment\ninstead of feature distribution matching. PS-VAEs inter-convert domain of each\nsample by a CycleGAN-based architecture while preserving its label-related\ncontent. To evaluate the performance of PS-VAEs, we carried out two\nexperiments: UDA with class-unbalanced digits datasets (classification), and\nUDA from synthesized data to real observation in human-pose-estimation\n(regression). The proposed method presented its robustness against the\nclass-imbalance in the classification task, and outperformed the other methods\nin the regression task with a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 06:41:31 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 05:45:10 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 05:41:25 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Takahashi", "Ryuhei", ""], ["Hashimoto", "Atsushi", ""], ["Sonogashira", "Motoharu", ""], ["Iiyama", "Masaaki", ""]]}, {"id": "2001.07904", "submitter": "Jean-Rassaire Fouefack", "authors": "Jean-Rassaire Fouefack, Bhushan Borotikar, Tania S. Douglas, Val\\'erie\n  Burdin and Tinashe E.M. Mutsvangwa", "title": "Dynamic multi-object Gaussian process models: A framework for\n  data-driven functional modelling of human joints", "comments": "15 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical shape models (SSMs) are state-of-the-art medical image analysis\ntools for extracting and explaining features across a set of biological\nstructures. However, a principled and robust way to combine shape and pose\nfeatures has been illusive due to three main issues: 1) Non-homogeneity of the\ndata (data with linear and non-linear natural variation across features), 2)\nnon-optimal representation of the $3D$ motion (rigid transformation\nrepresentations that are not proportional to the kinetic energy that move an\nobject from one position to the other), and 3) artificial discretization of the\nmodels. In this paper, we propose a new framework for dynamic multi-object\nstatistical modelling framework for the analysis of human joints in a\ncontinuous domain. Specifically, we propose to normalise shape and dynamic\nspatial features in the same linearized statistical space permitting the use of\nlinear statistics; we adopt an optimal 3D motion representation for more\naccurate rigid transformation comparisons; and we provide a 3D shape and pose\nprediction protocol using a Markov chain Monte Carlo sampling-based fitting.\nThe framework affords an efficient generative dynamic multi-object modelling\nplatform for biological joints. We validate the framework using a controlled\nsynthetic data. Finally, the framework is applied to an analysis of the human\nshoulder joint to compare its performance with standard SSM approaches in\nprediction of shape while adding the advantage of determining relative pose\nbetween bones in a complex. Excellent validity is observed and the shoulder\njoint shape-pose prediction results suggest that the novel framework may have\nutility for a range of medical image analysis applications. Furthermore, the\nframework is generic and can be extended to n$>$2 objects, making it suitable\nfor clinical and diagnostic methods for the management of joint disorders.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 07:57:36 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Fouefack", "Jean-Rassaire", ""], ["Borotikar", "Bhushan", ""], ["Douglas", "Tania S.", ""], ["Burdin", "Val\u00e9rie", ""], ["Mutsvangwa", "Tinashe E. M.", ""]]}, {"id": "2001.07926", "submitter": "Tonmoy Saikia", "authors": "Tonmoy Saikia, Thomas Brox, Cordelia Schmid", "title": "Optimized Generic Feature Learning for Few-shot Classification across\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To learn models or features that generalize across tasks and domains is one\nof the grand goals of machine learning. In this paper, we propose to use\ncross-domain, cross-task data as validation objective for hyper-parameter\noptimization (HPO) to improve on this goal. Given a rich enough search space,\noptimization of hyper-parameters learn features that maximize validation\nperformance and, due to the objective, generalize across tasks and domains. We\ndemonstrate the effectiveness of this strategy on few-shot image classification\nwithin and across domains. The learned features outperform all previous\nfew-shot and meta-learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 09:31:39 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Saikia", "Tonmoy", ""], ["Brox", "Thomas", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2001.07960", "submitter": "Yi Zhang", "authors": "Yi Zhang, Lu Zhang, Wassim Hamidouche, Olivier Deforges", "title": "A Fixation-based 360{\\deg} Benchmark Dataset for Salient Object\n  Detection", "comments": "5 pages, 5 figures, accepted by ICIP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixation prediction (FP) in panoramic contents has been widely investigated\nalong with the booming trend of virtual reality (VR) applications. However,\nanother issue within the field of visual saliency, salient object detection\n(SOD), has been seldom explored in 360{\\deg} (or omnidirectional) images due to\nthe lack of datasets representative of real scenes with pixel-level\nannotations. Toward this end, we collect 107 equirectangular panoramas with\nchallenging scenes and multiple object classes. Based on the consistency\nbetween FP and explicit saliency judgements, we further manually annotate 1,165\nsalient objects over the collected images with precise masks under the guidance\nof real human eye fixation maps. Six state-of-the-art SOD models are then\nbenchmarked on the proposed fixation-based 360{\\deg} image dataset (F-360iSOD),\nby applying a multiple cubic projection-based fine-tuning method. Experimental\nresults show a limitation of the current methods when used for SOD in panoramic\nimages, which indicates the proposed dataset is challenging. Key issues for\n360{\\deg} SOD is also discussed. The proposed dataset is available at\nhttps://github.com/PanoAsh/F-360iSOD.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 11:16:39 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 15:26:21 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Zhang", "Yi", ""], ["Zhang", "Lu", ""], ["Hamidouche", "Wassim", ""], ["Deforges", "Olivier", ""]]}, {"id": "2001.07966", "submitter": "Lin Su", "authors": "Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, Arun Sacheti", "title": "ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised\n  Image-Text Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new vision-language pre-trained model --\nImageBERT -- for image-text joint embedding. Our model is a Transformer-based\nmodel, which takes different modalities as input and models the relationship\nbetween them. The model is pre-trained on four tasks simultaneously: Masked\nLanguage Modeling (MLM), Masked Object Classification (MOC), Masked Region\nFeature Regression (MRFR), and Image Text Matching (ITM). To further enhance\nthe pre-training quality, we have collected a Large-scale weAk-supervised\nImage-Text (LAIT) dataset from Web. We first pre-train the model on this\ndataset, then conduct a second stage pre-training on Conceptual Captions and\nSBU Captions. Our experiments show that multi-stage pre-training strategy\noutperforms single-stage pre-training. We also fine-tune and evaluate our\npre-trained ImageBERT model on image retrieval and text retrieval tasks, and\nachieve new state-of-the-art results on both MSCOCO and Flickr30k datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 11:35:58 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 08:03:27 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Qi", "Di", ""], ["Su", "Lin", ""], ["Song", "Jia", ""], ["Cui", "Edward", ""], ["Bharti", "Taroon", ""], ["Sacheti", "Arun", ""]]}, {"id": "2001.08001", "submitter": "Oliver Willers", "authors": "Oliver Willers, Sebastian Sudholt, Shervin Raafatnia, Stephanie\n  Abrecht", "title": "Safety Concerns and Mitigation Approaches Regarding the Use of Deep\n  Learning in Safety-Critical Perception Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods are widely regarded as indispensable when it comes to\ndesigning perception pipelines for autonomous agents such as robots, drones or\nautomated vehicles. The main reasons, however, for deep learning not being used\nfor autonomous agents at large scale already are safety concerns. Deep learning\napproaches typically exhibit a black-box behavior which makes it hard for them\nto be evaluated with respect to safety-critical aspects. While there have been\nsome work on safety in deep learning, most papers typically focus on high-level\nsafety concerns. In this work, we seek to dive into the safety concerns of deep\nlearning methods and present a concise enumeration on a deeply technical level.\nAdditionally, we present extensive discussions on possible mitigation methods\nand give an outlook regarding what mitigation methods are still missing in\norder to facilitate an argumentation for the safety of a deep learning method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 13:22:59 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Willers", "Oliver", ""], ["Sudholt", "Sebastian", ""], ["Raafatnia", "Shervin", ""], ["Abrecht", "Stephanie", ""]]}, {"id": "2001.08026", "submitter": "Corinne Stucker", "authors": "Corinne Stucker and Konrad Schindler", "title": "ResDepth: Learned Residual Stereo Reconstruction", "comments": "updated supplementary material", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops, 2020, pp. 707-716", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an embarrassingly simple but very effective scheme for\nhigh-quality dense stereo reconstruction: (i) generate an approximate\nreconstruction with your favourite stereo matcher; (ii) rewarp the input images\nwith that approximate model; (iii) with the initial reconstruction and the\nwarped images as input, train a deep network to enhance the reconstruction by\nregressing a residual correction; and (iv) if desired, iterate the refinement\nwith the new, improved reconstruction. The strategy to only learn the residual\ngreatly simplifies the learning problem. A standard Unet without bells and\nwhistles is enough to reconstruct even small surface details, like dormers and\nroof substructures in satellite images. We also investigate residual\nreconstruction with less information and find that even a single image is\nenough to greatly improve an approximate reconstruction. Our full model reduces\nthe mean absolute error of state-of-the-art stereo reconstruction systems by\n>50%, both in our target domain of satellite stereo and on stereo pairs from\nthe ETH3D benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 14:12:43 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 17:28:05 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 16:06:20 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Stucker", "Corinne", ""], ["Schindler", "Konrad", ""]]}, {"id": "2001.08034", "submitter": "Darryl Hannan", "authors": "Darryl Hannan, Akshay Jain, and Mohit Bansal", "title": "ManyModalQA: Modality Disambiguation and QA over Diverse Inputs", "comments": "AAAI 2020 (10 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new multimodal question answering challenge, ManyModalQA, in\nwhich an agent must answer a question by considering three distinct modalities:\ntext, images, and tables. We collect our data by scraping Wikipedia and then\nutilize crowdsourcing to collect question-answer pairs. Our questions are\nambiguous, in that the modality that contains the answer is not easily\ndetermined based solely upon the question. To demonstrate this ambiguity, we\nconstruct a modality selector (or disambiguator) network, and this model gets\nsubstantially lower accuracy on our challenge set, compared to existing\ndatasets, indicating that our questions are more ambiguous. By analyzing this\nmodel, we investigate which words in the question are indicative of the\nmodality. Next, we construct a simple baseline ManyModalQA model, which, based\non the prediction from the modality selector, fires a corresponding pre-trained\nstate-of-the-art unimodal QA model. We focus on providing the community with a\nnew manymodal evaluation set and only provide a fine-tuning set, with the\nexpectation that existing datasets and approaches will be transferred for most\nof the training, to encourage low-resource generalization without large,\nmonolithic training sets for each new task. There is a significant gap between\nour baseline models and human performance; therefore, we hope that this\nchallenge encourages research in end-to-end modality disambiguation and\nmultimodal QA models, as well as transfer learning. Code and data available at:\nhttps://github.com/hannandarryl/ManyModalQA\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 14:39:28 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Hannan", "Darryl", ""], ["Jain", "Akshay", ""], ["Bansal", "Mohit", ""]]}, {"id": "2001.08047", "submitter": "Nicholas Santavas Mr", "authors": "Nicholas Santavas, Ioannis Kansizoglou, Loukas Bampis, Evangelos\n  Karakasis and Antonios Gasteratos", "title": "Attention! A Lightweight 2D Hand Pose Estimation Approach", "comments": "updated version with ablation studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vision based human pose estimation is an non-invasive technology for\nHuman-Computer Interaction (HCI). Direct use of the hand as an input device\nprovides an attractive interaction method, with no need for specialized sensing\nequipment, such as exoskeletons, gloves etc, but a camera. Traditionally, HCI\nis employed in various applications spreading in areas including manufacturing,\nsurgery, entertainment industry and architecture, to mention a few. Deployment\nof vision based human pose estimation algorithms can give a breath of\ninnovation to these applications. In this letter, we present a novel\nConvolutional Neural Network architecture, reinforced with a Self-Attention\nmodule that it can be deployed on an embedded system, due to its lightweight\nnature, with just 1.9 Million parameters. The source code and qualitative\nresults are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 15:05:00 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 01:24:50 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Santavas", "Nicholas", ""], ["Kansizoglou", "Ioannis", ""], ["Bampis", "Loukas", ""], ["Karakasis", "Evangelos", ""], ["Gasteratos", "Antonios", ""]]}, {"id": "2001.08057", "submitter": "Guanbin Li", "authors": "Haofeng Li, Guanbin Li, Binbin Yang, Guanqi Chen, Liang Lin, Yizhou Yu", "title": "Depthwise Non-local Module for Fast Salient Object Detection Using a\n  Single Thread", "comments": "Accepted as a regular paper in the IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep convolutional neural networks have achieved significant success\nin salient object detection. However, existing state-of-the-art methods require\nhigh-end GPUs to achieve real-time performance, which makes them hard to adapt\nto low-cost or portable devices. Although generic network architectures have\nbeen proposed to speed up inference on mobile devices, they are tailored to the\ntask of image classification or semantic segmentation, and struggle to capture\nintra-channel and inter-channel correlations that are essential for contrast\nmodeling in salient object detection. Motivated by the above observations, we\ndesign a new deep learning algorithm for fast salient object detection. The\nproposed algorithm for the first time achieves competitive accuracy and high\ninference efficiency simultaneously with a single CPU thread. Specifically, we\npropose a novel depthwise non-local moudule (DNL), which implicitly models\ncontrast via harvesting intra-channel and inter-channel correlations in a\nself-attention manner. In addition, we introduce a depthwise non-local network\narchitecture that incorporates both depthwise non-local modules and inverted\nresidual blocks. Experimental results show that our proposed network attains\nvery competitive accuracy on a wide range of salient object detection datasets\nwhile achieving state-of-the-art efficiency among all existing deep learning\nbased algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 15:23:48 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Li", "Haofeng", ""], ["Li", "Guanbin", ""], ["Yang", "Binbin", ""], ["Chen", "Guanqi", ""], ["Lin", "Liang", ""], ["Yu", "Yizhou", ""]]}, {"id": "2001.08095", "submitter": "Bruno Artacho", "authors": "Bruno Artacho and Andreas Savakis", "title": "UniPose: Unified Human Pose Estimation in Single Images and Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose UniPose, a unified framework for human pose estimation, based on\nour \"Waterfall\" Atrous Spatial Pooling architecture, that achieves\nstate-of-art-results on several pose estimation metrics. Current pose\nestimation methods utilizing standard CNN architectures heavily rely on\nstatistical postprocessing or predefined anchor poses for joint localization.\nUniPose incorporates contextual segmentation and joint localization to estimate\nthe human pose in a single stage, with high accuracy, without relying on\nstatistical postprocessing methods. The Waterfall module in UniPose leverages\nthe efficiency of progressive filtering in the cascade architecture, while\nmaintaining multi-scale fields-of-view comparable to spatial pyramid\nconfigurations. Additionally, our method is extended to UniPose-LSTM for\nmulti-frame processing and achieves state-of-the-art results for temporal pose\nestimation in Video. Our results on multiple datasets demonstrate that UniPose,\nwith a ResNet backbone and Waterfall module, is a robust and efficient\narchitecture for pose estimation obtaining state-of-the-art results in single\nperson pose detection for both single images and videos.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 15:59:42 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Artacho", "Bruno", ""], ["Savakis", "Andreas", ""]]}, {"id": "2001.08098", "submitter": "\\c{S}tefan S\\u{a}ftescu", "authors": "\\c{S}tefan S\\u{a}ftescu and Paul Newman", "title": "Learning to Correct 3D Reconstructions from Multiple Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about reducing the cost of building good large-scale 3D\nreconstructions post-hoc. We render 2D views of an existing reconstruction and\ntrain a convolutional neural network (CNN) that refines inverse-depth to match\na higher-quality reconstruction. Since the views that we correct are rendered\nfrom the same reconstruction, they share the same geometry, so overlapping\nviews complement each other. We take advantage of that in two ways. Firstly, we\nimpose a loss during training which guides predictions on neighbouring views to\nhave the same geometry and has been shown to improve performance. Secondly, in\ncontrast to previous work, which corrects each view independently, we also make\npredictions on sets of neighbouring views jointly. This is achieved by warping\nfeature maps between views and thus bypassing memory-intensive 3D computation.\nWe make the observation that features in the feature maps are\nviewpoint-dependent, and propose a method for transforming features with\ndynamic filters generated by a multi-layer perceptron from the relative poses\nbetween views. In our experiments we show that this last step is necessary for\nsuccessfully fusing feature maps between views.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:02:23 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["S\u0103ftescu", "\u015etefan", ""], ["Newman", "Paul", ""]]}, {"id": "2001.08111", "submitter": "Catherine Tong", "authors": "Catherine Tong, Shyam A. Tailor, Nicholas D. Lane", "title": "Are Accelerometers for Activity Recognition a Dead-end?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerometer-based (and by extension other inertial sensors) research for\nHuman Activity Recognition (HAR) is a dead-end. This sensor does not offer\nenough information for us to progress in the core domain of HAR - to recognize\neveryday activities from sensor data. Despite continued and prolonged efforts\nin improving feature engineering and machine learning models, the activities\nthat we can recognize reliably have only expanded slightly and many of the same\nflaws of early models are still present today. Instead of relying on\nacceleration data, we should instead consider modalities with much richer\ninformation - a logical choice are images. With the rapid advance in image\nsensing hardware and modelling techniques, we believe that a widespread\nadoption of image sensors will open many opportunities for accurate and robust\ninference across a wide spectrum of human activities.\n  In this paper, we make the case for imagers in place of accelerometers as the\ndefault sensor for human activity recognition. Our review of past works has led\nto the observation that progress in HAR had stalled, caused by our reliance on\naccelerometers. We further argue for the suitability of images for activity\nrecognition by illustrating their richness of information and the marked\nprogress in computer vision. Through a feasibility analysis, we find that\ndeploying imagers and CNNs on device poses no substantial burden on modern\nmobile hardware. Overall, our work highlights the need to move away from\naccelerometers and calls for further exploration of using imagers for activity\nrecognition.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:12:54 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 10:51:07 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Tong", "Catherine", ""], ["Tailor", "Shyam A.", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2001.08113", "submitter": "Hanhe Lin", "authors": "Hanhe Lin, Vlad Hosu, Dietmar Saupe", "title": "DeepFL-IQA: Weak Supervision for Deep IQA Feature Learning", "comments": "dataset url: http://database.mmsp-kn.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-level deep-features have been driving state-of-the-art methods for\naesthetics and image quality assessment (IQA). However, most IQA benchmarks are\ncomprised of artificially distorted images, for which features derived from\nImageNet under-perform. We propose a new IQA dataset and a weakly supervised\nfeature learning approach to train features more suitable for IQA of\nartificially distorted images. The dataset, KADIS-700k, is far more extensive\nthan similar works, consisting of 140,000 pristine images, 25 distortions\ntypes, totaling 700k distorted versions. Our weakly supervised feature learning\nis designed as a multi-task learning type training, using eleven existing\nfull-reference IQA metrics as proxies for differential mean opinion scores. We\nalso introduce a benchmark database, KADID-10k, of artificially degraded\nimages, each subjectively annotated by 30 crowd workers. We make use of our\nderived image feature vectors for (no-reference) image quality assessment by\ntraining and testing a shallow regression network on this database and five\nother benchmark IQA databases. Our method, termed DeepFL-IQA, performs better\nthan other feature-based no-reference IQA methods and also better than all\ntested full-reference IQA methods on KADID-10k. For the other five benchmark\nIQA databases, DeepFL-IQA matches the performance of the best existing\nend-to-end deep learning-based methods on average.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 15:13:26 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Lin", "Hanhe", ""], ["Hosu", "Vlad", ""], ["Saupe", "Dietmar", ""]]}, {"id": "2001.08126", "submitter": "Sheng Zhong", "authors": "Sheng Zhong and Shifu Zhou (Agora.io)", "title": "Optimizing Generative Adversarial Networks for Image Super Resolution\n  via Latent Space Regularization", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural images can be regarded as residing in a manifold that is embedded in\na higher dimensional Euclidean space. Generative Adversarial Networks (GANs)\ntry to learn the distribution of the real images in the manifold to generate\nsamples that look real. But the results of existing methods still exhibit many\nunpleasant artifacts and distortions even for the cases where the desired\nground truth target images are available for supervised learning such as in\nsingle image super resolution (SISR). We probe for ways to alleviate these\nproblems for supervised GANs in this paper. We explicitly apply the Lipschitz\nContinuity Condition (LCC) to regularize the GAN. An encoding network that maps\nthe image space to a new optimal latent space is derived from the LCC, and it\nis used to augment the GAN as a coupling component. The LCC is also converted\nto new regularization terms in the generator loss function to enforce local\ninvariance. The GAN is optimized together with the encoding network in an\nattempt to make the generator converge to a more ideal and disentangled mapping\nthat can generate samples more faithful to the target images. When the proposed\nmodels are applied to the single image super resolution problem, the results\noutperform the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:27:20 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 04:40:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhong", "Sheng", "", "Agora.io"], ["Zhou", "Shifu", "", "Agora.io"]]}, {"id": "2001.08142", "submitter": "Csan\\'ad S\\'andor", "authors": "Csan\\'ad S\\'andor, Szabolcs P\\'avel, Lehel Csat\\'o", "title": "Pruning CNN's with linear filter ensembles", "comments": "accepted to ECAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the promising results of convolutional neural networks (CNNs), their\napplication on devices with limited resources is still a big challenge; this is\nmainly due to the huge memory and computation requirements of the CNN. To\ncounter the limitation imposed by the network size, we use pruning to reduce\nthe network size and -- implicitly -- the number of floating point operations\n(FLOPs). Contrary to the filter norm method -- used in ``conventional`` network\npruning -- based on the assumption that a smaller norm implies ``less\nimportance'' to its associated component, we develop a novel filter importance\nnorm that is based on the change in the empirical loss caused by the presence\nor removal of a component from the network architecture.\n  Since there are too many individual possibilities for filter configuration,\nwe repeatedly sample from these architectural components and measure the system\nperformance in the respective state of components being active or disabled. The\nresult is a collection of filter ensembles -- filter masks -- and associated\nperformance values. We rank the filters based on a linear and additive model\nand remove the least important ones such that the drop in network accuracy is\nminimal. We evaluate our method on a fully connected network, as well as on the\nResNet architecture trained on the CIFAR-10 dataset. Using our pruning method,\nwe managed to remove $60\\%$ of the parameters and $64\\%$ of the FLOPs from the\nResNet with an accuracy drop of less than $0.6\\%$.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:52:06 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 09:25:32 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["S\u00e1ndor", "Csan\u00e1d", ""], ["P\u00e1vel", "Szabolcs", ""], ["Csat\u00f3", "Lehel", ""]]}, {"id": "2001.08173", "submitter": "Peyman Hosseinzadeh Kassani", "authors": "Peyman Hosseinzadeh Kassani, Li Xiao, Gemeng Zhang, Julia M. Stephen,\n  Tony W. Wilson, Vince D. Calhoun, Yu Ping Wang", "title": "Causality based Feature Fusion for Brain Neuro-Developmental Analysis", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human brain development is a complex and dynamic process that is affected by\nseveral factors such as genetics, sex hormones, and environmental changes. A\nnumber of recent studies on brain development have examined functional\nconnectivity (FC) defined by the temporal correlation between time series of\ndifferent brain regions. We propose to add the directional flow of information\nduring brain maturation. To do so, we extract effective connectivity (EC)\nthrough Granger causality (GC) for two different groups of subjects, i.e.,\nchildren and young adults. The motivation is that the inclusion of causal\ninteraction may further discriminate brain connections between two age groups\nand help to discover new connections between brain regions. The contributions\nof this study are threefold. First, there has been a lack of attention to\nEC-based feature extraction in the context of brain development. To this end,\nwe propose a new kernel-based GC (KGC) method to learn nonlinearity of complex\nbrain network, where a reduced Sine hyperbolic polynomial (RSP) neural network\nwas used as our proposed learner. Second, we used causality values as the\nweight for the directional connectivity between brain regions. Our findings\nindicated that the strength of connections was significantly higher in young\nadults relative to children. In addition, our new EC-based feature outperformed\nFC-based analysis from Philadelphia neurocohort (PNC) study with better\ndiscrimination of the different age groups. Moreover, the fusion of these two\nsets of features (FC + EC) improved brain age prediction accuracy by more than\n4%, indicating that they should be used together for brain development studies.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 17:38:42 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Kassani", "Peyman Hosseinzadeh", ""], ["Xiao", "Li", ""], ["Zhang", "Gemeng", ""], ["Stephen", "Julia M.", ""], ["Wilson", "Tony W.", ""], ["Calhoun", "Vince D.", ""], ["Wang", "Yu Ping", ""]]}, {"id": "2001.08188", "submitter": "Richard Droste", "authors": "Richard Droste, Pierre Chatelain, Lior Drukker, Harshita Sharma, Aris\n  T. Papageorghiou, J. Alison Noble", "title": "Discovering Salient Anatomical Landmarks by Predicting Human Gaze", "comments": "Accepted at IEEE International Symposium on Biomedical Imaging 2020\n  (ISBI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anatomical landmarks are a crucial prerequisite for many medical imaging\ntasks. Usually, the set of landmarks for a given task is predefined by experts.\nThe landmark locations for a given image are then annotated manually or via\nmachine learning methods trained on manual annotations. In this paper, in\ncontrast, we present a method to automatically discover and localize anatomical\nlandmarks in medical images. Specifically, we consider landmarks that attract\nthe visual attention of humans, which we term visually salient landmarks. We\nillustrate the method for fetal neurosonographic images. First, full-length\nclinical fetal ultrasound scans are recorded with live sonographer\ngaze-tracking. Next, a convolutional neural network (CNN) is trained to predict\nthe gaze point distribution (saliency map) of the sonographers on scan video\nframes. The CNN is then used to predict saliency maps of unseen fetal\nneurosonographic images, and the landmarks are extracted as the local maxima of\nthese saliency maps. Finally, the landmarks are matched across images by\nclustering the landmark CNN features. We show that the discovered landmarks can\nbe used within affine image registration, with average landmark alignment\nerrors between 4.1% and 10.9% of the fetal head long axis length.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 18:17:14 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Droste", "Richard", ""], ["Chatelain", "Pierre", ""], ["Drukker", "Lior", ""], ["Sharma", "Harshita", ""], ["Papageorghiou", "Aris T.", ""], ["Noble", "J. Alison", ""]]}, {"id": "2001.08189", "submitter": "Rafael Fricks", "authors": "Rafael B. Fricks, Justin Solomon, Ehsan Samei", "title": "Automatic phantom test pattern classification through transfer learning\n  with deep neural networks", "comments": null, "journal-ref": null, "doi": "10.1117/12.2549366", "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imaging phantoms are test patterns used to measure image quality in computer\ntomography (CT) systems. A new phantom platform (Mercury Phantom, Gammex)\nprovides test patterns for estimating the task transfer function (TTF) or noise\npower spectrum (NPF) and simulates different patient sizes. Determining which\nimage slices are suitable for analysis currently requires manual annotation of\nthese patterns by an expert, as subtle defects may make an image unsuitable for\nmeasurement. We propose a method of automatically classifying these test\npatterns in a series of phantom images using deep learning techniques. By\nadapting a convolutional neural network based on the VGG19 architecture with\nweights trained on ImageNet, we use transfer learning to produce a classifier\nfor this domain. The classifier is trained and evaluated with over 3,500\nphantom images acquired at a university medical center. Input channels for\ncolor images are successfully adapted to convey contextual information for\nphantom images. A series of ablation studies are employed to verify design\naspects of the classifier and evaluate its performance under varying training\nconditions. Our solution makes extensive use of image augmentation to produce a\nclassifier that accurately classifies typical phantom images with 98% accuracy,\nwhile maintaining as much as 86% accuracy when the phantom is improperly\nimaged.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 18:17:41 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Fricks", "Rafael B.", ""], ["Solomon", "Justin", ""], ["Samei", "Ehsan", ""]]}, {"id": "2001.08202", "submitter": "Andrew Rittenbach", "authors": "Andrew Rittenbach (1) and John Paul Walters (1) ((1) University of\n  Southern California Information Sciences Institute, Arlington VA)", "title": "RDAnet: A Deep Learning Based Approach for Synthetic Aperture Radar\n  Image Formation", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic Aperture Radar (SAR) imaging systems operate by emitting radar\nsignals from a moving object, such as a satellite, towards the target of\ninterest. Reflected radar echoes are received and later used by image formation\nalgorithms to form a SAR image. There is great interest in using SAR images in\ncomputer vision tasks such as classification or automatic target recognition.\nToday, however, SAR applications consist of multiple operations: image\nformation followed by image processing. In this work, we train a deep neural\nnetwork that performs both the image formation and image processing tasks,\nintegrating the SAR processing pipeline. Results show that our integrated\npipeline can output accurately classified SAR imagery with image quality\ncomparable to those formed using a traditional algorithm. We believe that this\nwork is the first demonstration of an integrated neural network based SAR\nprocessing pipeline using real data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 18:44:40 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 15:55:07 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Rittenbach", "Andrew", ""], ["Walters", "John Paul", ""]]}, {"id": "2001.08212", "submitter": "Weihao Yuan", "authors": "Weihao Yuan, Rui Fan, Michael Yu Wang, and Qifeng Chen", "title": "Active Perception with A Monocular Camera for Multiscopic Vision", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a multiscopic vision system that utilizes a low-cost monocular RGB\ncamera to acquire accurate depth estimation for robotic applications. Unlike\nmulti-view stereo with images captured at unconstrained camera poses, the\nproposed system actively controls a robot arm with a mounted camera to capture\na sequence of images in horizontally or vertically aligned positions with the\nsame parallax. In this system, we combine the cost volumes for stereo matching\nbetween the reference image and the surrounding images to form a fused cost\nvolume that is robust to outliers. Experiments on the Middlebury dataset and\nreal robot experiments show that our obtained disparity maps are more accurate\nthan two-frame stereo matching: the average absolute error is reduced by 50.2%\nin our experiments.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 08:46:45 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Yuan", "Weihao", ""], ["Fan", "Rui", ""], ["Wang", "Michael Yu", ""], ["Chen", "Qifeng", ""]]}, {"id": "2001.08247", "submitter": "Ziyang Tang", "authors": "Ziyang Tang, Xiang Liu, Guangyu Shen, and Baijian Yang", "title": "PENet: Object Detection using Points Estimation in Aerial Images", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial imagery has been increasingly adopted in mission-critical tasks, such\nas traffic surveillance, smart cities, and disaster assistance. However,\nidentifying objects from aerial images faces the following challenges: 1)\nobjects of interests are often too small and too dense relative to the images;\n2) objects of interests are often in different relative sizes; and 3) the\nnumber of objects in each category is imbalanced. A novel network structure,\nPoints Estimated Network (PENet), is proposed in this work to answer these\nchallenges. PENet uses a Mask Resampling Module (MRM) to augment the imbalanced\ndatasets, a coarse anchor-free detector (CPEN) to effectively predict the\ncenter points of the small object clusters, and a fine anchor-free detector\nFPEN to locate the precise positions of the small objects. An adaptive merge\nalgorithm Non-maximum Merge (NMM) is implemented in CPEN to address the issue\nof detecting dense small objects, and a hierarchical loss is defined in FPEN to\nfurther improve the classification accuracy. Our extensive experiments on\naerial datasets visDrone and UAVDT showed that PENet achieved higher precision\nresults than existing state-of-the-art approaches. Our best model achieved 8.7%\nimprovement on visDrone and 20.3% on UAVDT.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 19:43:17 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Tang", "Ziyang", ""], ["Liu", "Xiang", ""], ["Shen", "Guangyu", ""], ["Yang", "Baijian", ""]]}, {"id": "2001.08248", "submitter": "Md Amirul Islam", "authors": "Md Amirul Islam, Sen Jia, Neil D. B. Bruce", "title": "How Much Position Information Do Convolutional Neural Networks Encode?", "comments": "Accepted to ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to fully connected networks, Convolutional Neural Networks (CNNs)\nachieve efficiency by learning weights associated with local filters with a\nfinite spatial extent. An implication of this is that a filter may know what it\nis looking at, but not where it is positioned in the image. Information\nconcerning absolute position is inherently useful, and it is reasonable to\nassume that deep CNNs may implicitly learn to encode this information if there\nis a means to do so. In this paper, we test this hypothesis revealing the\nsurprising degree of absolute position information that is encoded in commonly\nused neural networks. A comprehensive set of experiments show the validity of\nthis hypothesis and shed light on how and where this information is represented\nwhile offering clues to where positional information is derived from in deep\nCNNs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 19:44:43 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Islam", "Md Amirul", ""], ["Jia", "Sen", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "2001.08311", "submitter": "Mikel Menta", "authors": "Mikel Menta, Adriana Romero, Joost van de Weijer", "title": "Learning to adapt class-specific features across domains for semantic\n  segmentation", "comments": "Master thesis dissertation for the Master in Computer Vision\n  (Barcelona). 11 pages main article and 3 pages appendices. Code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in unsupervised domain adaptation have shown the\neffectiveness of adversarial training to adapt features across domains,\nendowing neural networks with the capability of being tested on a target domain\nwithout requiring any training annotations in this domain. The great majority\nof existing domain adaptation models rely on image translation networks, which\noften contain a huge amount of domain-specific parameters. Additionally, the\nfeature adaptation step often happens globally, at a coarse level, hindering\nits applicability to tasks such as semantic segmentation, where details are of\ncrucial importance to provide sharp results. In this thesis, we present a novel\narchitecture, which learns to adapt features across domains by taking into\naccount per class information. To that aim, we design a conditional pixel-wise\ndiscriminator network, whose output is conditioned on the segmentation masks.\nMoreover, following recent advances in image translation, we adopt the recently\nintroduced StarGAN architecture as image translation backbone, since it is able\nto perform translations across multiple domains by means of a single generator\nnetwork. Preliminary results on a segmentation task designed to assess the\neffectiveness of the proposed approach highlight the potential of the model,\nimproving upon strong baselines and alternative designs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 23:51:30 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Menta", "Mikel", ""], ["Romero", "Adriana", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2001.08349", "submitter": "Satpreet Harcharan Singh", "authors": "Satpreet H. Singh, Steven M. Peterson, Rajesh P. N. Rao, Bingni W.\n  Brunton", "title": "Investigating naturalistic hand movements by behavior mining in\n  long-term video and neural recordings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological advances in brain recording and artificial intelligence\nare propelling a new paradigm in neuroscience beyond the traditional controlled\nexperiment. Rather than focusing on cued, repeated trials, naturalistic\nneuroscience studies neural processes underlying spontaneous behaviors\nperformed in unconstrained settings. However, analyzing such unstructured data\nlacking a priori experimental design remains a significant challenge,\nespecially when the data is multi-modal and long-term. Here we describe an\nautomated approach for analyzing simultaneously recorded long-term,\nnaturalistic electrocorticography (ECoG) and naturalistic behavior video data.\nWe take a behavior-first approach to analyzing the long-term recordings. Using\na combination of computer vision, discrete latent-variable modeling, and string\npattern-matching on the behavioral video data, we find and annotate spontaneous\nhuman upper-limb movement events. We show results from our approach applied to\ndata collected for 12 human subjects over 7--9 days for each subject. Our\npipeline discovers and annotates over 40,000 instances of naturalistic human\nupper-limb movement events in the behavioral videos. Analysis of the\nsimultaneously recorded brain data reveals neural signatures of movement that\ncorroborate prior findings from traditional controlled experiments. We also\nprototype a decoder for a movement initiation detection task to demonstrate the\nefficacy of our pipeline as a source of training data for brain-computer\ninterfacing applications. Our work addresses the unique data analysis\nchallenges in studying naturalistic human behaviors, and contributes methods\nthat may generalize to other neural recording modalities beyond ECoG. We\npublicly release our curated dataset, providing a resource to study\nnaturalistic neural and behavioral variability at a scale not previously\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 02:41:35 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 22:52:49 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Singh", "Satpreet H.", ""], ["Peterson", "Steven M.", ""], ["Rao", "Rajesh P. N.", ""], ["Brunton", "Bingni W.", ""]]}, {"id": "2001.08357", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Zhengang Li, Yifan Gong, Tianyun Zhang, Wei Niu, Zheng\n  Zhan, Pu Zhao, Jian Tang, Xue Lin, Bin Ren, Yanzhi Wang", "title": "BLK-REW: A Unified Block-based DNN Pruning Framework using Reweighted\n  Regularization Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating DNN execution on various resource-limited computing platforms\nhas been a long-standing problem. Prior works utilize l1-based group lasso or\ndynamic regularization such as ADMM to perform structured pruning on DNN models\nto leverage the parallel computing architectures. However, both of the pruning\ndimensions and pruning methods lack universality, which leads to degraded\nperformance and limited applicability. To solve the problem, we propose a new\nblock-based pruning framework that comprises a general and flexible structured\npruning dimension as well as a powerful and efficient reweighted regularization\nmethod. Our framework is universal, which can be applied to both CNNs and RNNs,\nimplying complete support for the two major kinds of computation-intensive\nlayers (i.e., CONV and FC layers). To complete all aspects of the\npruning-for-acceleration task, we also integrate compiler-based code\noptimization into our framework that can perform DNN inference in a real-time\nmanner. To the best of our knowledge, it is the first time that the weight\npruning framework achieves universal coverage for both CNNs and RNNs with\nreal-time mobile acceleration and no accuracy compromise.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 03:30:56 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 03:00:10 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Ma", "Xiaolong", ""], ["Li", "Zhengang", ""], ["Gong", "Yifan", ""], ["Zhang", "Tianyun", ""], ["Niu", "Wei", ""], ["Zhan", "Zheng", ""], ["Zhao", "Pu", ""], ["Tang", "Jian", ""], ["Lin", "Xue", ""], ["Ren", "Bin", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2001.08366", "submitter": "Canyu Le", "authors": "Canyu Le, Zhonggui Chen, Xihan Wei, Biao Wang, Lei Zhang", "title": "Continual Local Replacement for Few-shot Learning", "comments": "Update experiment results and reorganize paper writting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of few-shot learning is to learn a model that can recognize novel\nclasses based on one or few training data. It is challenging mainly due to two\naspects: (1) it lacks good feature representation of novel classes; (2) a few\nof labeled data could not accurately represent the true data distribution and\nthus it's hard to learn a good decision function for classification. In this\nwork, we use a sophisticated network architecture to learn better feature\nrepresentation and focus on the second issue. A novel continual local\nreplacement strategy is proposed to address the data deficiency problem. It\ntakes advantage of the content in unlabeled images to continually enhance\nlabeled ones. Specifically, a pseudo labeling method is adopted to constantly\nselect semantically similar images on the fly. Original labeled images will be\nlocally replaced by the selected images for the next epoch training. In this\nway, the model can directly learn new semantic information from unlabeled\nimages and the capacity of supervised signals in the embedding space can be\nsignificantly enlarged. This allows the model to improve generalization and\nlearn a better decision boundary for classification. Our method is conceptually\nsimple and easy to implement. Extensive experiments demonstrate that it can\nachieve state-of-the-art results on various few-shot image recognition\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 04:26:21 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 13:21:57 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Le", "Canyu", ""], ["Chen", "Zhonggui", ""], ["Wei", "Xihan", ""], ["Wang", "Biao", ""], ["Zhang", "Lei", ""]]}, {"id": "2001.08381", "submitter": "Jason Su", "authors": "Sadanand Singh, Thomas Paul Matthews, Meet Shah, Brent Mombourquette,\n  Trevor Tsue, Aaron Long, Ranya Almohsen, Stefano Pedemonte, and Jason Su", "title": "Adaptation of a deep learning malignancy model from full-field digital\n  mammography to digital breast tomosynthesis", "comments": "SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography-based screening has helped reduce the breast cancer mortality\nrate, but has also been associated with potential harms due to low specificity,\nleading to unnecessary exams or procedures, and low sensitivity. Digital breast\ntomosynthesis (DBT) improves on conventional mammography by increasing both\nsensitivity and specificity and is becoming common in clinical settings.\nHowever, deep learning (DL) models have been developed mainly on conventional\n2D full-field digital mammography (FFDM) or scanned film images. Due to a lack\nof large annotated DBT datasets, it is difficult to train a model on DBT from\nscratch. In this work, we present methods to generalize a model trained on FFDM\nimages to DBT images. In particular, we use average histogram matching (HM) and\nDL fine-tuning methods to generalize a FFDM model to the 2D maximum intensity\nprojection (MIP) of DBT images. In the proposed approach, the differences\nbetween the FFDM and DBT domains are reduced via HM and then the base model,\nwhich was trained on abundant FFDM images, is fine-tuned. When evaluating on\nimage patches extracted around identified findings, we are able to achieve\nsimilar areas under the receiver operating characteristic curve (ROC AUC) of\n$\\sim 0.9$ for FFDM and $\\sim 0.85$ for MIP images, as compared to a ROC AUC of\n$\\sim 0.75$ when tested directly on MIP images.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 05:44:11 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Singh", "Sadanand", ""], ["Matthews", "Thomas Paul", ""], ["Shah", "Meet", ""], ["Mombourquette", "Brent", ""], ["Tsue", "Trevor", ""], ["Long", "Aaron", ""], ["Almohsen", "Ranya", ""], ["Pedemonte", "Stefano", ""], ["Su", "Jason", ""]]}, {"id": "2001.08382", "submitter": "Jason Su", "authors": "Stefano Pedemonte, Brent Mombourquette, Alexis Goh, Trevor Tsue, Aaron\n  Long, Sadanand Singh, Thomas Paul Matthews, Meet Shah, and Jason Su", "title": "A Hypersensitive Breast Cancer Detector", "comments": "SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of breast cancer through screening mammography yields a\n20-35% increase in survival rate; however, there are not enough radiologists to\nserve the growing population of women seeking screening mammography. Although\ncommercial computer aided detection (CADe) software has been available to\nradiologists for decades, it has failed to improve the interpretation of\nfull-field digital mammography (FFDM) images due to its low sensitivity over\nthe spectrum of findings. In this work, we leverage a large set of FFDM images\nwith loose bounding boxes of mammographically significant findings to train a\ndeep learning detector with extreme sensitivity. Building upon work from the\nHourglass architecture, we train a model that produces segmentation-like images\nwith high spatial resolution, with the aim of producing 2D Gaussian blobs\ncentered on ground-truth boxes. We replace the pixel-wise $L_2$ norm with a\nweak-supervision loss designed to achieve high sensitivity, asymmetrically\npenalizing false positives and false negatives while softening the noise of the\nloose bounding boxes by permitting a tolerance in misaligned predictions. The\nresulting system achieves a sensitivity for malignant findings of 0.99 with\nonly 4.8 false positive markers per image. When utilized in a CADe system, this\nmodel could enable a novel workflow where radiologists can focus their\nattention with trust on only the locations proposed by the model, expediting\nthe interpretation process and bringing attention to potential findings that\ncould otherwise have been missed. Due to its nearly perfect sensitivity, the\nproposed detector can also be used as a high-performance proposal generator in\ntwo-stage detection systems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 05:44:39 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Pedemonte", "Stefano", ""], ["Mombourquette", "Brent", ""], ["Goh", "Alexis", ""], ["Tsue", "Trevor", ""], ["Long", "Aaron", ""], ["Singh", "Sadanand", ""], ["Matthews", "Thomas Paul", ""], ["Shah", "Meet", ""], ["Su", "Jason", ""]]}, {"id": "2001.08383", "submitter": "Thomas Matthews", "authors": "Thomas P. Matthews (1), Sadanand Singh (1), Brent Mombourquette (1),\n  Jason Su (1), Meet P. Shah (1), Stefano Pedemonte (1), Aaron Long (1), David\n  Maffit (2), Jenny Gurney (2), Rodrigo Morales Hoil (1), Nikita Ghare (1),\n  Douglas Smith (1), Stephen M. Moore (2), Susan C. Marks (3), Richard L. Wahl\n  (2), ((1) Whiterabbit AI, Inc., Santa Clara, CA, (2) Mallinckrodt Institute\n  of Radiology, Washington University School of Medicine, St. Louis, MO, (3)\n  Peninsula Diagnostic Imaging, San Mateo, CA)", "title": "A Multi-site Study of a Breast Density Deep Learning Model for\n  Full-field Digital Mammography Images and Synthetic Mammography Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop a Breast Imaging Reporting and Data System (BI-RADS)\nbreast density deep learning (DL) model in a multi-site setting for synthetic\ntwo-dimensional mammography (SM) images derived from digital breast\ntomosynthesis exams using full-field digital mammography (FFDM) images and\nlimited SM data.\n  Materials and Methods: A DL model was trained to predict BI-RADS breast\ndensity using FFDM images acquired from 2008 to 2017 (Site 1: 57492 patients,\n187627 exams, 750752 images) for this retrospective study. The FFDM model was\nevaluated using SM datasets from two institutions (Site 1: 3842 patients, 3866\nexams, 14472 images, acquired from 2016 to 2017; Site 2: 7557 patients, 16283\nexams, 63973 images, 2015 to 2019). Each of the three datasets were then split\ninto training, validation, and test datasets. Adaptation methods were\ninvestigated to improve performance on the SM datasets and the effect of\ndataset size on each adaptation method is considered. Statistical significance\nwas assessed using confidence intervals (CI), estimated by bootstrapping.\n  Results: Without adaptation, the model demonstrated substantial agreement\nwith the original reporting radiologists for all three datasets (Site 1 FFDM:\nlinearly-weighted $\\kappa_w$ = 0.75 [95% CI: 0.74, 0.76]; Site 1 SM: $\\kappa_w$\n= 0.71 [95% CI: 0.64, 0.78]; Site 2 SM: $\\kappa_w$ = 0.72 [95% CI: 0.70,\n0.75]). With adaptation, performance improved for Site 2 (Site 1: $\\kappa_w$ =\n0.72 [95% CI: 0.66, 0.79], 0.71 vs 0.72, P = .80; Site 2: $\\kappa_w$ = 0.79\n[95% CI: 0.76, 0.81], 0.72 vs 0.79, P $<$ .001) using only 500 SM images from\nthat site.\n  Conclusion: A BI-RADS breast density DL model demonstrated strong performance\non FFDM and SM images from two institutions without training on SM images and\nimproved using few SM images.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 05:51:27 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 20:07:03 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Matthews", "Thomas P.", ""], ["Singh", "Sadanand", ""], ["Mombourquette", "Brent", ""], ["Su", "Jason", ""], ["Shah", "Meet P.", ""], ["Pedemonte", "Stefano", ""], ["Long", "Aaron", ""], ["Maffit", "David", ""], ["Gurney", "Jenny", ""], ["Hoil", "Rodrigo Morales", ""], ["Ghare", "Nikita", ""], ["Smith", "Douglas", ""], ["Moore", "Stephen M.", ""], ["Marks", "Susan C.", ""], ["Wahl", "Richard L.", ""]]}, {"id": "2001.08388", "submitter": "Zhao Zhang", "authors": "Yanyan Wei, Zhao Zhang, Yang Wang, Haijun Zhang, Mingbo Zhao,\n  Mingliang Xu, Meng Wang", "title": "Semi-DerainGAN: A New Semi-supervised Single Image Deraining Network", "comments": "Please cite this work as: Yanyan Wei, Zhao Zhang, Yang Wang, Haijun\n  Zhang, Mingbo Zhao, Mingliang Xu and Meng Wang, \"Semi-DerainGAN: A New\n  Semi-supervised Single Image Deraining Network,\" In: Proceedings of the IEEE\n  International Conference on Multimedia and Expo (ICME), July 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing the rain streaks from single image is still a challenging task,\nsince the shapes and directions of rain streaks in the synthetic datasets are\nvery different from real images. Although supervised deep deraining networks\nhave obtained impressive results on synthetic datasets, they still cannot\nobtain satisfactory results on real images due to weak generalization of rain\nremoval capacity, i.e., the pre-trained models usually cannot handle new shapes\nand directions that may lead to over-derained/under-derained results. In this\npaper, we propose a new semi-supervised GAN-based deraining network termed\nSemi-DerainGAN, which can use both synthetic and real rainy images in a uniform\nnetwork using two supervised and unsupervised processes. Specifically, a\nsemi-supervised rain streak learner termed SSRML sharing the same parameters of\nboth processes is derived, which makes the real images contribute more rain\nstreak information. To deliver better deraining results, we design a paired\ndiscriminator for distinguishing the real pairs from fake pairs. Note that we\nalso contribute a new real-world rainy image dataset Real200 to alleviate the\ndifference between the synthetic and real image do-mains. Extensive results on\npublic datasets show that our model can obtain competitive performance,\nespecially on real images.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 07:01:30 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 02:02:37 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 08:27:50 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wei", "Yanyan", ""], ["Zhang", "Zhao", ""], ["Wang", "Yang", ""], ["Zhang", "Haijun", ""], ["Zhao", "Mingbo", ""], ["Xu", "Mingliang", ""], ["Wang", "Meng", ""]]}, {"id": "2001.08395", "submitter": "Qun Liu", "authors": "Qun Liu, Supratik Mukhopadhyay, Maria Ximena Bastidas Rodriguez, Xing\n  Fu, Sushant Sahu, David Burk, Manas Gartia", "title": "A One-Shot Learning Framework for Assessment of Fibrillar Collagen from\n  Second Harmonic Generation Images of an Infarcted Myocardium", "comments": "Paper was accepted at the IEEE International Symposium on Biomedical\n  Imaging (ISBI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myocardial infarction (MI) is a scientific term that refers to heart attack.\nIn this study, we infer highly relevant second harmonic generation (SHG) cues\nfrom collagen fibers exhibiting highly non-centrosymmetric assembly together\nwith two-photon excited cellular autofluorescence in infarcted mouse heart to\nquantitatively probe fibrosis, especially targeted at an early stage after MI.\nWe present a robust one-shot machine learning algorithm that enables\ndetermination of 2D assembly of collagen with high spatial resolution along\nwith its structural arrangement in heart tissues post-MI with spectral\nspecificity and sensitivity. Detection, evaluation, and precise quantification\nof fibrosis extent at early stage would guide one to develop treatment\ntherapies that may prevent further progression and determine heart transplant\nneeds for patient survival.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 07:35:56 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 04:49:13 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Liu", "Qun", ""], ["Mukhopadhyay", "Supratik", ""], ["Rodriguez", "Maria Ximena Bastidas", ""], ["Fu", "Xing", ""], ["Sahu", "Sushant", ""], ["Burk", "David", ""], ["Gartia", "Manas", ""]]}, {"id": "2001.08434", "submitter": "Sourav Garg", "authors": "Sourav Garg and Michael Milford", "title": "Fast, Compact and Highly Scalable Visual Place Recognition through\n  Sequence-based Matching of Overloaded Representations", "comments": "8 pages, 4 figures, Accepted for oral presentation at the 2020 IEEE\n  International Conference on Robotics and Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition algorithms trade off three key characteristics:\ntheir storage footprint, their computational requirements, and their resultant\nperformance, often expressed in terms of recall rate. Significant prior work\nhas investigated highly compact place representations, sub-linear computational\nscaling and sub-linear storage scaling techniques, but have always involved a\nsignificant compromise in one or more of these regards, and have only been\ndemonstrated on relatively small datasets. In this paper we present a novel\nplace recognition system which enables for the first time the combination of\nultra-compact place representations, near sub-linear storage scaling and\nextremely lightweight compute requirements. Our approach exploits the\ninherently sequential nature of much spatial data in the robotics domain and\ninverts the typical target criteria, through intentionally coarse scalar\nquantization-based hashing that leads to more collisions but is resolved by\nsequence-based matching. For the first time, we show how effective place\nrecognition rates can be achieved on a new very large 10 million place dataset,\nrequiring only 8 bytes of storage per place and 37K unitary operations to\nachieve over 50% recall for matching a sequence of 100 frames, where a\nconventional state-of-the-art approach both consumes 1300 times more compute\nand fails catastrophically. We present analysis investigating the effectiveness\nof our hashing overload approach under varying sizes of quantized vector\nlength, comparison of near miss matches with the actual match selections and\ncharacterise the effect of variance re-scaling of data on quantization.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 10:31:28 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 12:51:41 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Garg", "Sourav", ""], ["Milford", "Michael", ""]]}, {"id": "2001.08456", "submitter": "Aviad Aberdam", "authors": "Aviad Aberdam, Alona Golts, Michael Elad", "title": "Ada-LISTA: Learned Solvers Adaptive to Varying Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks that are based on unfolding of an iterative solver, such as\nLISTA (learned iterative soft threshold algorithm), are widely used due to\ntheir accelerated performance. Nevertheless, as opposed to non-learned solvers,\nthese networks are trained on a certain dictionary, and therefore they are\ninapplicable for varying model scenarios. This work introduces an adaptive\nlearned solver, termed Ada-LISTA, which receives pairs of signals and their\ncorresponding dictionaries as inputs, and learns a universal architecture to\nserve them all. We prove that this scheme is guaranteed to solve sparse coding\nin linear rate for varying models, including dictionary perturbations and\npermutations. We also provide an extensive numerical study demonstrating its\npractical adaptation capabilities. Finally, we deploy Ada-LISTA to natural\nimage inpainting, where the patch-masks vary spatially, thus requiring such an\nadaptation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 11:34:03 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 14:28:35 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Aberdam", "Aviad", ""], ["Golts", "Alona", ""], ["Elad", "Michael", ""]]}, {"id": "2001.08480", "submitter": "Timo Kepp", "authors": "Timo Kepp, Helge Sudkamp, Claus von der Burchard, Hendrik Schenke,\n  Peter Koch, Gereon H\\\"uttmann, Johann Roider, Mattias P. Heinrich, and Heinz\n  Handels", "title": "Segmentation of Retinal Low-Cost Optical Coherence Tomography Images\n  using Deep Learning", "comments": "Accepted for SPIE Medical Imaging 2020: Computer-Aided Diagnosis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The treatment of age-related macular degeneration (AMD) requires continuous\neye exams using optical coherence tomography (OCT). The need for treatment is\ndetermined by the presence or change of disease-specific OCT-based biomarkers.\nTherefore, the monitoring frequency has a significant influence on the success\nof AMD therapy. However, the monitoring frequency of current treatment schemes\nis not individually adapted to the patient and therefore often insufficient.\nWhile a higher monitoring frequency would have a positive effect on the success\nof treatment, in practice it can only be achieved with a home monitoring\nsolution. One of the key requirements of a home monitoring OCT system is a\ncomputer-aided diagnosis to automatically detect and quantify pathological\nchanges using specific OCT-based biomarkers. In this paper, for the first time,\nretinal scans of a novel self-examination low-cost full-field OCT (SELF-OCT)\nare segmented using a deep learning-based approach. A convolutional neural\nnetwork (CNN) is utilized to segment the total retina as well as pigment\nepithelial detachments (PED). It is shown that the CNN-based approach can\nsegment the retina with high accuracy, whereas the segmentation of the PED\nproves to be challenging. In addition, a convolutional denoising autoencoder\n(CDAE) refines the CNN prediction, which has previously learned retinal shape\ninformation. It is shown that the CDAE refinement can correct segmentation\nerrors caused by artifacts in the OCT image.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 12:55:53 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Kepp", "Timo", ""], ["Sudkamp", "Helge", ""], ["von der Burchard", "Claus", ""], ["Schenke", "Hendrik", ""], ["Koch", "Peter", ""], ["H\u00fcttmann", "Gereon", ""], ["Roider", "Johann", ""], ["Heinrich", "Mattias P.", ""], ["Handels", "Heinz", ""]]}, {"id": "2001.08481", "submitter": "Oier Mees", "authors": "Oier Mees, Alp Emek, Johan Vertens, Wolfram Burgard", "title": "Learning Object Placements For Relational Instructions by Hallucinating\n  Scene Representations", "comments": "Accepted at the 2020 IEEE International Conference on Robotics and\n  Automation (ICRA). Video at https://www.youtube.com/watch?v=zaZkHTWFMKM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots coexisting with humans in their environment and performing services\nfor them need the ability to interact with them. One particular requirement for\nsuch robots is that they are able to understand spatial relations and can place\nobjects in accordance with the spatial relations expressed by their user. In\nthis work, we present a convolutional neural network for estimating pixelwise\nobject placement probabilities for a set of spatial relations from a single\ninput image. During training, our network receives the learning signal by\nclassifying hallucinated high-level scene representations as an auxiliary task.\nUnlike previous approaches, our method does not require ground truth data for\nthe pixelwise relational probabilities or 3D models of the objects, which\nsignificantly expands the applicability in practical applications. Our results\nobtained using real-world data and human-robot experiments demonstrate the\neffectiveness of our method in reasoning about the best way to place objects to\nreproduce a spatial relation. Videos of our experiments can be found at\nhttps://youtu.be/zaZkHTWFMKM\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 12:58:50 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 18:14:11 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Mees", "Oier", ""], ["Emek", "Alp", ""], ["Vertens", "Johan", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2001.08499", "submitter": "Davide Migliore", "authors": "Pierre de Tournemire, Davide Nitti, Etienne Perot, Davide Migliore,\n  Amos Sironi", "title": "A Large Scale Event-based Detection Dataset for Automotive", "comments": "8 pages, 29 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first very large detection dataset for event cameras. The\ndataset is composed of more than 39 hours of automotive recordings acquired\nwith a 304x240 ATIS sensor. It contains open roads and very diverse driving\nscenarios, ranging from urban, highway, suburbs and countryside scenes, as well\nas different weather and illumination conditions. Manual bounding box\nannotations of cars and pedestrians contained in the recordings are also\nprovided at a frequency between 1 and 4Hz, yielding more than 255,000 labels in\ntotal. We believe that the availability of a labeled dataset of this size will\ncontribute to major advances in event-based vision tasks such as object\ndetection and classification. We also expect benefits in other tasks such as\noptical flow, structure from motion and tracking, where for example, the large\namount of data can be leveraged by self-supervised learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 13:40:51 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 10:45:21 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 13:35:45 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["de Tournemire", "Pierre", ""], ["Nitti", "Davide", ""], ["Perot", "Etienne", ""], ["Migliore", "Davide", ""], ["Sironi", "Amos", ""]]}, {"id": "2001.08514", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Liujuan Cao, Shaojie Li, Qixiang Ye, Yonghong Tian,\n  Jianzhuang Liu, Qi Tian, Rongrong Ji", "title": "Filter Sketch for Network Pruning", "comments": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (IEEE TNNLS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel network pruning approach by information preserving of\npre-trained network weights (filters). Network pruning with the information\npreserving is formulated as a matrix sketch problem, which is efficiently\nsolved by the off-the-shelf Frequent Direction method. Our approach, referred\nto as FilterSketch, encodes the second-order information of pre-trained\nweights, which enables the representation capacity of pruned networks to be\nrecovered with a simple fine-tuning procedure. FilterSketch requires neither\ntraining from scratch nor data-driven iterative optimization, leading to a\nseveral-orders-of-magnitude reduction of time cost in the optimization of\npruning. Experiments on CIFAR-10 show that FilterSketch reduces 63.3% of FLOPs\nand prunes 59.9% of network parameters with negligible accuracy cost for\nResNet-110. On ILSVRC-2012, it reduces 45.5% of FLOPs and removes 43.0% of\nparameters with only 0.69% accuracy drop for ResNet-50. Our code and pruned\nmodels can be found at https://github.com/lmbxmu/FilterSketch.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 13:57:08 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 09:26:07 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 02:11:57 GMT"}, {"version": "v4", "created": "Tue, 25 May 2021 03:12:53 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lin", "Mingbao", ""], ["Cao", "Liujuan", ""], ["Li", "Shaojie", ""], ["Ye", "Qixiang", ""], ["Tian", "Yonghong", ""], ["Liu", "Jianzhuang", ""], ["Tian", "Qi", ""], ["Ji", "Rongrong", ""]]}, {"id": "2001.08517", "submitter": "Karol Ch\\k{e}ci\\'nski", "authors": "Karol Ch\\k{e}ci\\'nski, Pawe{\\l} Wawrzy\\'nski", "title": "DCT-Conv: Coding filters in convolutional networks with Discrete Cosine\n  Transform", "comments": "6 pages, 2 figures, submitted for IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are based on a huge number of trained weights.\nConsequently, they are often data-greedy, sensitive to overtraining, and learn\nslowly. We follow the line of research in which filters of convolutional neural\nlayers are determined on the basis of a smaller number of trained parameters.\nIn this paper, the trained parameters define a frequency spectrum which is\ntransformed into convolutional filters with Inverse Discrete Cosine Transform\n(IDCT, the same is applied in decompression from JPEG). We analyze how\nswitching off selected components of the spectra, thereby reducing the number\nof trained weights of the network, affects its performance. Our experiments\nshow that coding the filters with trained DCT parameters leads to improvement\nover traditional convolution. Also, the performance of the networks modified\nthis way decreases very slowly with the increasing extent of switching off\nthese parameters. In some experiments, a good performance is observed when even\n99.9% of these parameters are switched off.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 13:58:17 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 13:54:04 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 15:47:59 GMT"}, {"version": "v4", "created": "Tue, 7 Apr 2020 10:59:07 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Ch\u0119ci\u0144ski", "Karol", ""], ["Wawrzy\u0144ski", "Pawe\u0142", ""]]}, {"id": "2001.08533", "submitter": "Fariba Zohrizadeh", "authors": "Mohsen Kheirandishfard, Fariba Zohrizadeh, Farhad Kamangar", "title": "Multi-Level Representation Learning for Deep Subspace Clustering", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel deep subspace clustering approach which uses\nconvolutional autoencoders to transform input images into new representations\nlying on a union of linear subspaces. The first contribution of our work is to\ninsert multiple fully-connected linear layers between the encoder layers and\ntheir corresponding decoder layers to promote learning more favorable\nrepresentations for subspace clustering. These connection layers facilitate the\nfeature learning procedure by combining low-level and high-level information\nfor generating multiple sets of self-expressive and informative representations\nat different levels of the encoder. Moreover, we introduce a novel loss\nminimization problem which leverages an initial clustering of the samples to\neffectively fuse the multi-level representations and recover the underlying\nsubspaces more accurately. The loss function is then minimized through an\niterative scheme which alternatively updates the network parameters and\nproduces new clusterings of the samples. Experiments on four real-world\ndatasets demonstrate that our approach exhibits superior performance compared\nto the state-of-the-art methods on most of the subspace clustering problems.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 23:29:50 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Kheirandishfard", "Mohsen", ""], ["Zohrizadeh", "Fariba", ""], ["Kamangar", "Farhad", ""]]}, {"id": "2001.08552", "submitter": "Arkadiy Dushatskiy", "authors": "Arkadiy Dushatskiy, Adri\\\"enne M. Mendrik, Peter A. N. Bosman, Tanja\n  Alderliesten", "title": "Observer variation-aware medical image segmentation by combining deep\n  learning and surrogate-assisted genetic algorithms", "comments": "11 pages, 5 figures, SPIE Medical Imaging Conference - 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been great progress in automatic segmentation of medical\nimages with deep learning algorithms. In most works observer variation is\nacknowledged to be a problem as it makes training data heterogeneous but so far\nno attempts have been made to explicitly capture this variation. Here, we\npropose an approach capable of mimicking different styles of segmentation,\nwhich potentially can improve quality and clinical acceptance of automatic\nsegmentation methods. In this work, instead of training one neural network on\nall available data, we train several neural networks on subgroups of data\nbelonging to different segmentation variations separately. Because a priori it\nmay be unclear what styles of segmentation exist in the data and because\ndifferent styles do not necessarily map one-on-one to different observers, the\nsubgroups should be automatically determined. We achieve this by searching for\nthe best data partition with a genetic algorithm. Therefore, each network can\nlearn a specific style of segmentation from grouped training data. We provide\nproof of principle results for open-sourced prostate segmentation MRI data with\nsimulated observer variations. Our approach provides an improvement of up to\n23% (depending on simulated variations) in terms of Dice and surface Dice\ncoefficients compared to one network trained on all data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 14:51:40 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Dushatskiy", "Arkadiy", ""], ["Mendrik", "Adri\u00ebnne M.", ""], ["Bosman", "Peter A. N.", ""], ["Alderliesten", "Tanja", ""]]}, {"id": "2001.08559", "submitter": "Zehao Wang", "authors": "Zehao Wang, Kaili Wang, Tinne Tuytelaars, Jose Oramas", "title": "Information Compensation for Deep Conditional Generative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, unsupervised/weakly-supervised conditional generative\nadversarial networks (GANs) have achieved many successes on the task of\nmodeling and generating data. However, one of their weaknesses lies in their\npoor ability to separate, or disentangle, the different factors that\ncharacterize the representation encoded in their latent space. To address this\nissue, we propose a novel structure for unsupervised conditional GANs powered\nby a novel Information Compensation Connection (IC-Connection). The proposed\nIC-Connection enables GANs to compensate for information loss incurred during\ndeconvolution operations. In addition, to quantify the degree of\ndisentanglement on both discrete and continuous latent variables, we design a\nnovel evaluation procedure. Our empirical results suggest that our method\nachieves better disentanglement compared to the state-of-the-art GANs in a\nconditional generation setting.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 14:39:53 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 12:00:10 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Wang", "Zehao", ""], ["Wang", "Kaili", ""], ["Tuytelaars", "Tinne", ""], ["Oramas", "Jose", ""]]}, {"id": "2001.08565", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Yuxin Zhang, Baochang Zhang, Yongjian Wu,\n  Yonghong Tian", "title": "Channel Pruning via Automatic Structure Search", "comments": "Accepted by IJCAI2020. SOLO copyright holder is IJCAI (International\n  Joint Conferences on Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Channel pruning is among the predominant approaches to compress deep neural\nnetworks. To this end, most existing pruning methods focus on selecting\nchannels (filters) by importance/optimization or regularization based on\nrule-of-thumb designs, which defects in sub-optimal pruning. In this paper, we\npropose a new channel pruning method based on artificial bee colony algorithm\n(ABC), dubbed as ABCPruner, which aims to efficiently find optimal pruned\nstructure, i.e., channel number in each layer, rather than selecting\n\"important\" channels as previous works did. To solve the intractably huge\ncombinations of pruned structure for deep networks, we first propose to shrink\nthe combinations where the preserved channels are limited to a specific space,\nthus the combinations of pruned structure can be significantly reduced. And\nthen, we formulate the search of optimal pruned structure as an optimization\nproblem and integrate the ABC algorithm to solve it in an automatic manner to\nlessen human interference. ABCPruner has been demonstrated to be more\neffective, which also enables the fine-tuning to be conducted efficiently in an\nend-to-end manner. The source codes can be available at\nhttps://github.com/lmbxmu/ABCPruner.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 14:51:19 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 09:20:20 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 01:52:28 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Zhang", "Yuxin", ""], ["Zhang", "Baochang", ""], ["Wu", "Yongjian", ""], ["Tian", "Yonghong", ""]]}, {"id": "2001.08570", "submitter": "Nathaniel Braman", "authors": "Nathaniel Braman, Mohammed El Adoui, Manasa Vulchi, Paulette Turk,\n  Maryam Etesami, Pingfu Fu, Kaustav Bera, Stylianos Drisis, Vinay Varadan,\n  Donna Plecha, Mohammed Benjelloun, Jame Abraham, Anant Madabhushi", "title": "Deep learning-based prediction of response to HER2-targeted neoadjuvant\n  chemotherapy from pre-treatment dynamic breast MRI: A multi-institutional\n  validation study", "comments": "Braman and El Adoui contributed equally to this work. 33 pages, 3\n  figures in main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting response to neoadjuvant therapy is a vexing challenge in breast\ncancer. In this study, we evaluate the ability of deep learning to predict\nresponse to HER2-targeted neo-adjuvant chemotherapy (NAC) from pre-treatment\ndynamic contrast-enhanced (DCE) MRI acquired prior to treatment. In a\nretrospective study encompassing DCE-MRI data from a total of 157 HER2+ breast\ncancer patients from 5 institutions, we developed and validated a deep learning\napproach for predicting pathological complete response (pCR) to HER2-targeted\nNAC prior to treatment. 100 patients who received HER2-targeted neoadjuvant\nchemotherapy at a single institution were used to train (n=85) and tune (n=15)\na convolutional neural network (CNN) to predict pCR. A multi-input CNN\nleveraging both pre-contrast and late post-contrast DCE-MRI acquisitions was\nidentified to achieve optimal response prediction within the validation set\n(AUC=0.93). This model was then tested on two independent testing cohorts with\npre-treatment DCE-MRI data. It achieved strong performance in a 28 patient\ntesting set from a second institution (AUC=0.85, 95% CI 0.67-1.0, p=.0008) and\na 29 patient multicenter trial including data from 3 additional institutions\n(AUC=0.77, 95% CI 0.58-0.97, p=0.006). Deep learning-based response prediction\nmodel was found to exceed a multivariable model incorporating predictive\nclinical variables (AUC < .65 in testing cohorts) and a model of\nsemi-quantitative DCE-MRI pharmacokinetic measurements (AUC < .60 in testing\ncohorts). The results presented in this work across multiple sites suggest that\nwith further validation deep learning could provide an effective and reliable\ntool to guide targeted therapy in breast cancer, thus reducing overtreatment\namong HER2+ patients.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 17:54:24 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Braman", "Nathaniel", ""], ["Adoui", "Mohammed El", ""], ["Vulchi", "Manasa", ""], ["Turk", "Paulette", ""], ["Etesami", "Maryam", ""], ["Fu", "Pingfu", ""], ["Bera", "Kaustav", ""], ["Drisis", "Stylianos", ""], ["Varadan", "Vinay", ""], ["Plecha", "Donna", ""], ["Benjelloun", "Mohammed", ""], ["Abraham", "Jame", ""], ["Madabhushi", "Anant", ""]]}, {"id": "2001.08572", "submitter": "Zengjie Song", "authors": "Zengjie Song, Oluwasanmi Koyejo, Jiangshe Zhang", "title": "Toward a Controllable Disentanglement Network", "comments": "Improved version of arXiv:1912.11675", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses two crucial problems of learning disentangled image\nrepresentations, namely controlling the degree of disentanglement during image\nediting, and balancing the disentanglement strength and the reconstruction\nquality. To encourage disentanglement, we devise a distance covariance based\ndecorrelation regularization. Further, for the reconstruction step, our model\nleverages a soft target representation combined with the latent image code. By\nexploring the real-valued space of the soft target representation, we are able\nto synthesize novel images with the designated properties. To improve the\nperceptual quality of images generated by autoencoder (AE)-based models, we\nextend the encoder-decoder architecture with the generative adversarial network\n(GAN) by collapsing the AE decoder and the GAN generator into one. We also\ndesign a classification based protocol to quantitatively evaluate the\ndisentanglement strength of our model. Experimental results showcase the\nbenefits of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:54:07 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 06:13:18 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 04:02:22 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Song", "Zengjie", ""], ["Koyejo", "Oluwasanmi", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "2001.08589", "submitter": "Daniel Freedman", "authors": "Daniel Freedman, Yochai Blau, Liran Katzir, Amit Aides, Ilan\n  Shimshoni, Danny Veikherman, Tomer Golany, Ariel Gordon, Greg Corrado, Yossi\n  Matias, and Ehud Rivlin", "title": "Detecting Deficient Coverage in Colonoscopies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colonoscopy is the tool of choice for preventing Colorectal Cancer, by\ndetecting and removing polyps before they become cancerous. However,\ncolonoscopy is hampered by the fact that endoscopists routinely miss 22-28% of\npolyps. While some of these missed polyps appear in the endoscopist's field of\nview, others are missed simply because of substandard coverage of the\nprocedure, i.e. not all of the colon is seen. This paper attempts to rectify\nthe problem of substandard coverage in colonoscopy through the introduction of\nthe C2D2 (Colonoscopy Coverage Deficiency via Depth) algorithm which detects\ndeficient coverage, and can thereby alert the endoscopist to revisit a given\narea. More specifically, C2D2 consists of two separate algorithms: the first\nperforms depth estimation of the colon given an ordinary RGB video stream;\nwhile the second computes coverage given these depth estimates. Rather than\ncompute coverage for the entire colon, our algorithm computes coverage locally,\non a segment-by-segment basis; C2D2 can then indicate in real-time whether a\nparticular area of the colon has suffered from deficient coverage, and if so\nthe endoscopist can return to that area. Our coverage algorithm is the first\nsuch algorithm to be evaluated in a large-scale way; while our depth estimation\ntechnique is the first calibration-free unsupervised method applied to\ncolonoscopies. The C2D2 algorithm achieves state of the art results in the\ndetection of deficient coverage. On synthetic sequences with ground truth, it\nis 2.4 times more accurate than human experts; while on real sequences, C2D2\nachieves a 93.0% agreement with experts.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 15:12:33 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 09:30:01 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 09:42:05 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Freedman", "Daniel", ""], ["Blau", "Yochai", ""], ["Katzir", "Liran", ""], ["Aides", "Amit", ""], ["Shimshoni", "Ilan", ""], ["Veikherman", "Danny", ""], ["Golany", "Tomer", ""], ["Gordon", "Ariel", ""], ["Corrado", "Greg", ""], ["Matias", "Yossi", ""], ["Rivlin", "Ehud", ""]]}, {"id": "2001.08590", "submitter": "Youbao Tang", "authors": "Vatsal Agarwal, Youbao Tang, Jing Xiao, Ronald M. Summers", "title": "Weakly-Supervised Lesion Segmentation on CT Scans using Co-Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion segmentation on computed tomography (CT) scans is an important step\nfor precisely monitoring changes in lesion/tumor growth. This task, however, is\nvery challenging since manual segmentation is prohibitively time-consuming,\nexpensive, and requires professional knowledge. Current practices rely on an\nimprecise substitute called response evaluation criteria in solid tumors\n(RECIST). Although these markers lack detailed information about the lesion\nregions, they are commonly found in hospitals' picture archiving and\ncommunication systems (PACS). Thus, these markers have the potential to serve\nas a powerful source of weak-supervision for 2D lesion segmentation. To\napproach this problem, this paper proposes a convolutional neural network (CNN)\nbased weakly-supervised lesion segmentation method, which first generates the\ninitial lesion masks from the RECIST measurements and then utilizes\nco-segmentation to leverage lesion similarities and refine the initial masks.\nIn this work, an attention-based co-segmentation model is adopted due to its\nability to learn more discriminative features from a pair of images.\nExperimental results on the NIH DeepLesion dataset demonstrate that the\nproposed co-segmentation approach significantly improves lesion segmentation\nperformance, e.g the Dice score increases about 4.0% (from 85.8% to 89.8%).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 15:15:53 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Agarwal", "Vatsal", ""], ["Tang", "Youbao", ""], ["Xiao", "Jing", ""], ["Summers", "Ronald M.", ""]]}, {"id": "2001.08593", "submitter": "Mariia Dobko", "authors": "Mariia Dobko, Bohdan Petryshak, Oles Dobosevych", "title": "CNN-CASS: CNN for Classification of Coronary Artery Stenosis Score in\n  MPR Images", "comments": "To be published in CVWW 2020 proceedings", "journal-ref": "25th Computer Vision Winter Workshop, 2020", "doi": null, "report-no": "12", "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To decrease patient waiting time for diagnosis of the Coronary Artery\nDisease, automatic methods are applied to identify its severity using Coronary\nComputed Tomography Angiography scans or extracted Multiplanar Reconstruction\n(MPR) images, giving doctors a second-opinion on the priority of each case. The\nmain disadvantage of previous studies is the lack of large set of data that\ncould guarantee their reliability. Another limitation is the usage of\nhandcrafted features requiring manual preprocessing, such as centerline\nextraction. We overcome both limitations by applying a different automated\napproach based on ShuffleNet V2 network architecture and testing it on the\nproposed collected dataset of MPR images, which is bigger than any other used\nin this field before. We also omit centerline extraction step and train and\ntest our model using whole curved MPR images of 708 and 105 patients,\nrespectively. The model predicts one of three classes: 'no stenosis' for\nnormal, 'non-significant' - 1-50% of stenosis detected, 'significant' - more\nthan 50% of stenosis. We demonstrate model's interpretability through\nvisualization of the most important features selected by the network. For\nstenosis score classification, the method shows improved performance comparing\nto previous works, achieving 80% accuracy on the patient level. Our code is\npublicly available.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 15:20:22 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Dobko", "Mariia", ""], ["Petryshak", "Bohdan", ""], ["Dobosevych", "Oles", ""]]}, {"id": "2001.08601", "submitter": "Siyuan Li", "authors": "Siyuan Li, Semih G\\\"unel, Mirela Ostrek, Pavan Ramdya, Pascal Fua, and\n  Helge Rhodin", "title": "Deformation-aware Unpaired Image Translation for Pose Estimation on\n  Laboratory Animals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to capture the pose of neuroscience model organisms, without\nusing any manual supervision, to be able to study how neural circuits\norchestrate behaviour. Human pose estimation attains remarkable accuracy when\ntrained on real or simulated datasets consisting of millions of frames.\nHowever, for many applications simulated models are unrealistic and real\ntraining datasets with comprehensive annotations do not exist. We address this\nproblem with a new sim2real domain transfer method. Our key contribution is the\nexplicit and independent modeling of appearance, shape and poses in an unpaired\nimage translation framework. Our model lets us train a pose estimator on the\ntarget domain by transferring readily available body keypoint locations from\nthe source domain to generated target images. We compare our approach with\nexisting domain transfer methods and demonstrate improved pose estimation\naccuracy on Drosophila melanogaster (fruit fly), Caenorhabditis elegans (worm)\nand Danio rerio (zebrafish), without requiring any manual annotation on the\ntarget domain and despite using simplistic off-the-shelf animal characters for\nsimulation, or simple geometric shapes as models. Our new datasets, code, and\ntrained models will be published to support future neuroscientific studies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 15:34:11 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Li", "Siyuan", ""], ["G\u00fcnel", "Semih", ""], ["Ostrek", "Mirela", ""], ["Ramdya", "Pavan", ""], ["Fua", "Pascal", ""], ["Rhodin", "Helge", ""]]}, {"id": "2001.08650", "submitter": "Gobinda Saha", "authors": "Gobinda Saha, Isha Garg, Aayush Ankit and Kaushik Roy", "title": "SPACE: Structured Compression and Sharing of Representational Space for\n  Continual Learning", "comments": "The first two authors contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans learn adaptively and efficiently throughout their lives. However,\nincrementally learning tasks causes artificial neural networks to overwrite\nrelevant information learned about older tasks, resulting in 'Catastrophic\nForgetting'. Efforts to overcome this phenomenon often utilize resources\npoorly, for instance, by growing the network architecture or needing to save\nparametric importance scores, or violate data privacy between tasks. To tackle\nthis, we propose SPACE, an algorithm that enables a network to learn\ncontinually and efficiently by partitioning the learnt space into a Core space,\nthat serves as the condensed knowledge base over previously learned tasks, and\na Residual space, which is akin to a scratch space for learning the current\ntask. After learning each task, the Residual is analyzed for redundancy, both\nwithin itself and with the learnt Core space. A minimal number of extra\ndimensions required to explain the current task are added to the Core space and\nthe remaining Residual is freed up for learning the next task. We evaluate our\nalgorithm on P-MNIST, CIFAR and a sequence of 8 different datasets, and achieve\ncomparable accuracy to the state-of-the-art methods while overcoming\ncatastrophic forgetting. Additionally, our algorithm is well suited for\npractical use. The partitioning algorithm analyzes all layers in one shot,\nensuring scalability to deeper networks. Moreover, the analysis of dimensions\ntranslates to filter-level sparsity, and the structured nature of the resulting\narchitecture gives us up to 5x improvement in energy efficiency during task\ninference over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 16:40:56 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 21:02:50 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 15:22:00 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 06:23:33 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Saha", "Gobinda", ""], ["Garg", "Isha", ""], ["Ankit", "Aayush", ""], ["Roy", "Kaushik", ""]]}, {"id": "2001.08651", "submitter": "Kilian Hett", "authors": "Kilian Hett, Hans Johnson, Pierrick Coup\\'e (LaBRI), Jane Paulsen,\n  Jeffrey Long, Ipek Oguz", "title": "Tensor-Based Grading: A Novel Patch-Based Grading Approach for the\n  Analysis of Deformation Fields in Huntington's Disease", "comments": null, "journal-ref": "IEEE ISBI 2020: International Symposium on Biomedical Imaging, Apr\n  2020, Iowa City, United States", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The improvements in magnetic resonance imaging have led to the development of\nnumerous techniques to better detect structural alterations caused by\nneurodegenerative diseases. Among these, the patch-based grading framework has\nbeen proposed to model local patterns of anatomical changes. This approach is\nattractive because of its low computational cost and its competitive\nperformance. Other studies have proposed to analyze the deformations of brain\nstructures using tensor-based morphometry, which is a highly interpretable\napproach. In this work, we propose to combine the advantages of these two\napproaches by extending the patch-based grading framework with a new\ntensor-based grading method that enables us to model patterns of local\ndeformation using a log-Euclidean metric. We evaluate our new method in a study\nof the putamen for the classification of patients with pre-manifest\nHuntington's disease and healthy controls. Our experiments show a substantial\nincrease in classification accuracy (87.5 $\\pm$ 0.5 vs. 81.3 $\\pm$ 0.6)\ncompared to the existing patch-based grading methods, and a good complement to\nputamen volume, which is a primary imaging-based marker for the study of\nHuntington's disease.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 16:42:24 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Hett", "Kilian", "", "LaBRI"], ["Johnson", "Hans", "", "LaBRI"], ["Coup\u00e9", "Pierrick", "", "LaBRI"], ["Paulsen", "Jane", ""], ["Long", "Jeffrey", ""], ["Oguz", "Ipek", ""]]}, {"id": "2001.08680", "submitter": "Zijie Zhuang", "authors": "Zijie Zhuang, Longhui Wei, Lingxi Xie, Tianyu Zhang, Hengheng Zhang,\n  Haozhe Wu, Haizhou Ai, and Qi Tian", "title": "Rethinking the Distribution Gap of Person Re-identification with\n  Camera-based Batch Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental difficulty in person re-identification (ReID) lies in\nlearning the correspondence among individual cameras. It strongly demands\ncostly inter-camera annotations, yet the trained models are not guaranteed to\ntransfer well to previously unseen cameras. These problems significantly limit\nthe application of ReID. This paper rethinks the working mechanism of\nconventional ReID approaches and puts forward a new solution. With an effective\noperator named Camera-based Batch Normalization (CBN), we force the image data\nof all cameras to fall onto the same subspace, so that the distribution gap\nbetween any camera pair is largely shrunk. This alignment brings two benefits.\nFirst, the trained model enjoys better abilities to generalize across scenarios\nwith unseen cameras as well as transfer across multiple training sets. Second,\nwe can rely on intra-camera annotations, which have been undervalued before due\nto the lack of cross-camera information, to achieve competitive ReID\nperformance. Experiments on a wide range of ReID tasks demonstrate the\neffectiveness of our approach. The code is available at\nhttps://github.com/automan000/Camera-based-Person-ReID.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:22:34 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 14:42:17 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 15:37:06 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhuang", "Zijie", ""], ["Wei", "Longhui", ""], ["Xie", "Lingxi", ""], ["Zhang", "Tianyu", ""], ["Zhang", "Hengheng", ""], ["Wu", "Haozhe", ""], ["Ai", "Haizhou", ""], ["Tian", "Qi", ""]]}, {"id": "2001.08699", "submitter": "Aaron Defazio", "authors": "Aaron Defazio and Tullie Murrell and Michael P. Recht", "title": "MRI Banding Removal via Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  MRI images reconstructed from sub-sampled Cartesian data using deep learning\ntechniques often show a characteristic banding (sometimes described as\nstreaking), which is particularly strong in low signal-to-noise regions of the\nreconstructed image. In this work, we propose the use of an adversarial loss\nthat penalizes banding structures without requiring any human annotation. Our\ntechnique greatly reduces the appearance of banding, without requiring any\nadditional computation or post-processing at reconstruction time. We report the\nresults of a blind comparison against a strong baseline by a group of expert\nevaluators (board-certified radiologists), where our approach is ranked\nsuperior at banding removal with no statistically significant loss of detail.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:46:14 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 19:15:01 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 15:54:45 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Defazio", "Aaron", ""], ["Murrell", "Tullie", ""], ["Recht", "Michael P.", ""]]}, {"id": "2001.08702", "submitter": "Stavros Petridis", "authors": "Brais Martinez, Pingchuan Ma, Stavros Petridis, Maja Pantic", "title": "Lipreading using Temporal Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip-reading has attracted a lot of research attention lately thanks to\nadvances in deep learning. The current state-of-the-art model for recognition\nof isolated words in-the-wild consists of a residual network and Bidirectional\nGated Recurrent Unit (BGRU) layers. In this work, we address the limitations of\nthis model and we propose changes which further improve its performance.\nFirstly, the BGRU layers are replaced with Temporal Convolutional Networks\n(TCN). Secondly, we greatly simplify the training procedure, which allows us to\ntrain the model in one single stage. Thirdly, we show that the current\nstate-of-the-art methodology produces models that do not generalize well to\nvariations on the sequence length, and we addresses this issue by proposing a\nvariable-length augmentation. We present results on the largest\npublicly-available datasets for isolated word recognition in English and\nMandarin, LRW and LRW1000, respectively. Our proposed model results in an\nabsolute improvement of 1.2% and 3.2%, respectively, in these datasets which is\nthe new state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:49:35 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Martinez", "Brais", ""], ["Ma", "Pingchuan", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2001.08714", "submitter": "Marc Masana Castrillo", "authors": "Marc Masana, Tinne Tuytelaars, Joost van de Weijer", "title": "Ternary Feature Masks: zero-forgetting for task-incremental learning", "comments": "To appear in the IEEE Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision\n  (CLVision) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach without any forgetting to continual learning for the\ntask-aware regime, where at inference the task-label is known. By using ternary\nmasks we can upgrade a model to new tasks, reusing knowledge from previous\ntasks while not forgetting anything about them. Using masks prevents both\ncatastrophic forgetting and backward transfer. We argue -- and show\nexperimentally -- that avoiding the former largely compensates for the lack of\nthe latter, which is rarely observed in practice. In contrast to earlier works,\nour masks are applied to the features (activations) of each layer instead of\nthe weights. This considerably reduces the number of mask parameters for each\nnew task; with more than three orders of magnitude for most networks. The\nencoding of the ternary masks into two bits per feature creates very little\noverhead to the network, avoiding scalability issues. To allow already learned\nfeatures to adapt to the current task without changing the behavior of these\nfeatures for previous tasks, we introduce task-specific feature normalization.\nExtensive experiments on several finegrained datasets and ImageNet show that\nour method outperforms current state-of-the-art while reducing memory overhead\nin comparison to weight-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 18:08:37 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 10:34:21 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Masana", "Marc", ""], ["Tuytelaars", "Tinne", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2001.08726", "submitter": "Jianyu Chen", "authors": "Jianyu Chen, Shengbo Eben Li, Masayoshi Tomizuka", "title": "Interpretable End-to-end Urban Autonomous Driving with Latent Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike popular modularized framework, end-to-end autonomous driving seeks to\nsolve the perception, decision and control problems in an integrated way, which\ncan be more adapting to new scenarios and easier to generalize at scale.\nHowever, existing end-to-end approaches are often lack of interpretability, and\ncan only deal with simple driving tasks like lane keeping. In this paper, we\npropose an interpretable deep reinforcement learning method for end-to-end\nautonomous driving, which is able to handle complex urban scenarios. A\nsequential latent environment model is introduced and learned jointly with the\nreinforcement learning process. With this latent model, a semantic birdeye mask\ncan be generated, which is enforced to connect with a certain intermediate\nproperty in today's modularized framework for the purpose of explaining the\nbehaviors of learned policy. The latent space also significantly reduces the\nsample complexity of reinforcement learning. Comparison tests with a simulated\nautonomous car in CARLA show that the performance of our method in urban\nscenarios with crowded surrounding vehicles dominates many baselines including\nDQN, DDPG, TD3 and SAC. Moreover, through masked outputs, the learned policy is\nable to provide a better explanation of how the car reasons about the driving\nenvironment. The codes and videos of this work are available at our github repo\nand project website.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 18:36:35 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 05:57:16 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 06:23:50 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Chen", "Jianyu", ""], ["Li", "Shengbo Eben", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2001.08730", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Shivansh Pate, and Vinay P. Namboodiri", "title": "Robust Explanations for Visual Question Answering", "comments": "WACV-2020 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a method to obtain robust explanations for visual\nquestion answering(VQA) that correlate well with the answers. Our model\nexplains the answers obtained through a VQA model by providing visual and\ntextual explanations. The main challenges that we address are i) Answers and\ntextual explanations obtained by current methods are not well correlated and\nii) Current methods for visual explanation do not focus on the right location\nfor explaining the answer. We address both these challenges by using a\ncollaborative correlated module which ensures that even if we do not train for\nnoise based attacks, the enhanced correlation ensures that the right\nexplanation and answer can be generated. We further show that this also aids in\nimproving the generated visual and textual explanations. The use of the\ncorrelated module can be thought of as a robust method to verify if the answer\nand explanations are coherent. We evaluate this model using VQA-X dataset. We\nobserve that the proposed method yields better textual and visual justification\nthat supports the decision. We showcase the robustness of the model against a\nnoise-based perturbation attack using corresponding visual and textual\nexplanations. A detailed empirical analysis is shown. Here we provide source\ncode link for our model \\url{https://github.com/DelTA-Lab-IITK/CCM-WACV}.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 18:43:34 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Patro", "Badri N.", ""], ["Pate", "Shivansh", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2001.08735", "submitter": "Hung-Yu Tseng", "authors": "Hung-Yu Tseng, Hsin-Ying Lee, Jia-Bin Huang, Ming-Hsuan Yang", "title": "Cross-Domain Few-Shot Classification via Learned Feature-Wise\n  Transformation", "comments": "ICLR 2020 (Spotlight). Project page:\n  http://vllab.ucmerced.edu/ym41608/projects/CrossDomainFewShot Code:\n  https://github.com/hytseng0509/CrossDomainFewShot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification aims to recognize novel categories with only few\nlabeled images in each class. Existing metric-based few-shot classification\nalgorithms predict categories by comparing the feature embeddings of query\nimages with those from a few labeled images (support examples) using a learned\nmetric function. While promising performance has been demonstrated, these\nmethods often fail to generalize to unseen domains due to large discrepancy of\nthe feature distribution across domains. In this work, we address the problem\nof few-shot classification under domain shifts for metric-based methods. Our\ncore idea is to use feature-wise transformation layers for augmenting the image\nfeatures using affine transforms to simulate various feature distributions\nunder different domains in the training stage. To capture variations of the\nfeature distributions under different domains, we further apply a\nlearning-to-learn approach to search for the hyper-parameters of the\nfeature-wise transformation layers. We conduct extensive experiments and\nablation studies under the domain generalization setting using five few-shot\nclassification datasets: mini-ImageNet, CUB, Cars, Places, and Plantae.\nExperimental results demonstrate that the proposed feature-wise transformation\nlayer is applicable to various metric-based models, and provides consistent\nimprovements on the few-shot classification performance under domain shift.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 18:55:43 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 05:18:22 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 08:58:10 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Tseng", "Hung-Yu", ""], ["Lee", "Hsin-Ying", ""], ["Huang", "Jia-Bin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2001.08740", "submitter": "Christoph Feichtenhofer", "authors": "Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik, Christoph\n  Feichtenhofer", "title": "Audiovisual SlowFast Networks for Video Recognition", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Audiovisual SlowFast Networks, an architecture for integrated\naudiovisual perception. AVSlowFast has Slow and Fast visual pathways that are\ndeeply integrated with a Faster Audio pathway to model vision and sound in a\nunified representation. We fuse audio and visual features at multiple layers,\nenabling audio to contribute to the formation of hierarchical audiovisual\nconcepts. To overcome training difficulties that arise from different learning\ndynamics for audio and visual modalities, we introduce DropPathway, which\nrandomly drops the Audio pathway during training as an effective regularization\ntechnique. Inspired by prior studies in neuroscience, we perform hierarchical\naudiovisual synchronization to learn joint audiovisual features. We report\nstate-of-the-art results on six video action classification and detection\ndatasets, perform detailed ablation studies, and show the generalization of\nAVSlowFast to learn self-supervised audiovisual features. Code will be made\navailable at: https://github.com/facebookresearch/SlowFast.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 18:59:46 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 00:50:19 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Xiao", "Fanyi", ""], ["Lee", "Yong Jae", ""], ["Grauman", "Kristen", ""], ["Malik", "Jitendra", ""], ["Feichtenhofer", "Christoph", ""]]}, {"id": "2001.08741", "submitter": "Leihao Wei", "authors": "Leihao Wei and Yannan Lin and William Hsu", "title": "Using a Generative Adversarial Network for CT Normalization and its\n  Impact on Radiomic Features", "comments": "ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-Aided-Diagnosis (CADx) systems assist radiologists with identifying\nand classifying potentially malignant pulmonary nodules on chest CT scans using\nmorphology and texture-based (radiomic) features. However, radiomic features\nare sensitive to differences in acquisitions due to variations in dose levels\nand slice thickness. This study investigates the feasibility of generating a\nnormalized scan from heterogeneous CT scans as input. We obtained projection\ndata from 40 low-dose chest CT scans, simulating acquisitions at 10%, 25% and\n50% dose and reconstructing the scans at 1.0mm and 2.0mm slice thickness. A 3D\ngenerative adversarial network (GAN) was used to simultaneously normalize\nreduced dose, thick slice (2.0mm) images to normal dose (100%), thinner slice\n(1.0mm) images. We evaluated the normalized image quality using peak\nsignal-to-noise ratio (PSNR), structural similarity index (SSIM) and Learned\nPerceptual Image Patch Similarity (LPIPS). Our GAN improved perceptual\nsimilarity by 35%, compared to a baseline CNN method. Our analysis also shows\nthat the GAN-based approach led to a significantly smaller error (p-value <\n0.05) in nine studied radiomic features. These results indicated that GANs\ncould be used to normalize heterogeneous CT images and reduce the variability\nin radiomic feature values.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 23:41:29 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Wei", "Leihao", ""], ["Lin", "Yannan", ""], ["Hsu", "William", ""]]}, {"id": "2001.08742", "submitter": "Debapriya Kundu", "authors": "Mayank Wadhwani, Debapriya Kundu, Deepayan Chakraborty, Bhabatosh\n  Chanda", "title": "Text Extraction and Restoration of Old Handwritten Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration is very crucial computer vision task. This paper describes\ntwo novel methods for the restoration of old degraded handwritten documents\nusing deep neural network. In addition to that, a small-scale dataset of 26\nheritage letters images is introduced. The ground truth data to train the\ndesired network is generated semi automatically involving a pragmatic\ncombination of color transformation, Gaussian mixture model based segmentation\nand shape correction by using mathematical morphological operators. In the\nfirst approach, a deep neural network has been used for text extraction from\nthe document image and later background reconstruction has been done using\nGaussian mixture modeling. But Gaussian mixture modelling requires to set\nparameters manually, to alleviate this we propose a second approach where the\nbackground reconstruction and foreground extraction (which which includes\nextracting text with its original colour) both has been done using deep neural\nnetwork. Experiments demonstrate that the proposed systems perform well on\nhandwritten document images with severe degradations, even when trained with\nsmall dataset. Hence, the proposed methods are ideally suited for digital\nheritage preservation repositories. It is worth mentioning that, these methods\ncan be extended easily for printed degraded documents.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 05:42:39 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Wadhwani", "Mayank", ""], ["Kundu", "Debapriya", ""], ["Chakraborty", "Deepayan", ""], ["Chanda", "Bhabatosh", ""]]}, {"id": "2001.08746", "submitter": "Mohammad Golbabaee", "authors": "Mohammad Golbabaee, Guido Buonincontri, Carolin Pirkl, Marion Menzel,\n  Bjoern Menze, Mike Davies, Pedro Gomez", "title": "Compressive MRI quantification using convex spatiotemporal priors and\n  deep auto-encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dictionary-matching-free pipeline for multi-parametric\nquantitative MRI image computing. Our approach has two stages based on\ncompressed sensing reconstruction and deep learned quantitative inference. The\nreconstruction phase is convex and incorporates efficient spatiotemporal\nregularisations within an accelerated iterative shrinkage algorithm. This\nminimises the under-sampling (aliasing) artefacts from aggressively short scan\ntimes. The learned quantitative inference phase is purely trained on physical\nsimulations (Bloch equations) that are flexible for producing rich training\nsamples. We propose a deep and compact auto-encoder network with residual\nblocks in order to embed Bloch manifold projections through multiscale\npiecewise affine approximations, and to replace the nonscalable\ndictionary-matching baseline. Tested on a number of datasets we demonstrate\neffectiveness of the proposed scheme for recovering accurate and consistent\nquantitative information from novel and aggressively subsampled 2D/3D\nquantitative MRI acquisition protocols.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:15:42 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 20:09:21 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Golbabaee", "Mohammad", ""], ["Buonincontri", "Guido", ""], ["Pirkl", "Carolin", ""], ["Menzel", "Marion", ""], ["Menze", "Bjoern", ""], ["Davies", "Mike", ""], ["Gomez", "Pedro", ""]]}, {"id": "2001.08747", "submitter": "Max Daniels", "authors": "Max Daniels, Paul Hand, Reinhard Heckel", "title": "Reducing the Representation Error of GAN Image Priors Using the Deep\n  Decoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models, such as GANs, learn an explicit low-dimensional\nrepresentation of a particular class of images, and so they may be used as\nnatural image priors for solving inverse problems such as image restoration and\ncompressive sensing. GAN priors have demonstrated impressive performance on\nthese tasks, but they can exhibit substantial representation error for both\nin-distribution and out-of-distribution images, because of the mismatch between\nthe learned, approximate image distribution and the data generating\ndistribution. In this paper, we demonstrate a method for reducing the\nrepresentation error of GAN priors by modeling images as the linear combination\nof a GAN prior with a Deep Decoder. The deep decoder is an underparameterized\nand most importantly unlearned natural signal model similar to the Deep Image\nPrior. No knowledge of the specific inverse problem is needed in the training\nof the GAN underlying our method. For compressive sensing and image\nsuperresolution, our hybrid model exhibits consistently higher PSNRs than both\nthe GAN priors and Deep Decoder separately, both on in-distribution and\nout-of-distribution images. This model provides a method for extensibly and\ncheaply leveraging both the benefits of learned and unlearned image recovery\npriors in inverse problems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 18:37:24 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Daniels", "Max", ""], ["Hand", "Paul", ""], ["Heckel", "Reinhard", ""]]}, {"id": "2001.08768", "submitter": "Sorour Mohajerani", "authors": "Sorour Mohajerani and Parvaneh Saeedi", "title": "Cloud and Cloud Shadow Segmentation for Remote Sensing Imagery via\n  Filtered Jaccard Loss Function and Parametric Augmentation", "comments": "12 pages. This version is a bit different from the one published in\n  IEEE JSTARS", "journal-ref": "IEEE Journal of Selected Topics in Applied Earth Observations and\n  Remote Sensing (JSTARS), 2021", "doi": "10.1109/JSTARS.2021.3070786", "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud and cloud shadow segmentation are fundamental processes in optical\nremote sensing image analysis. Current methods for cloud/shadow identification\nin geospatial imagery are not as accurate as they should, especially in the\npresence of snow and haze. This paper presents a deep learning-based framework\nfor the detection of cloud/shadow in Landsat 8 images. Our method benefits from\na convolutional neural network, Cloud-Net+ (a modification of our previously\nproposed Cloud-Net \\cite{myigarss}) that is trained with a novel loss function\n(Filtered Jaccard Loss). The proposed loss function is more sensitive to the\nabsence of foreground objects in an image and penalizes/rewards the predicted\nmask more accurately than other common loss functions. In addition, a sunlight\ndirection-aware data augmentation technique is developed for the task of cloud\nshadow detection to extend the generalization ability of the proposed model by\nexpanding existing training sets. The combination of Cloud-Net+, Filtered\nJaccard Loss function, and the proposed augmentation algorithm delivers\nsuperior results on four public cloud/shadow detection datasets. Our\nexperiments on Pascal VOC dataset exemplifies the applicability and quality of\nour proposed network and loss function in other computer vision applications.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 19:13:00 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 22:01:36 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mohajerani", "Sorour", ""], ["Saeedi", "Parvaneh", ""]]}, {"id": "2001.08779", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Vinod K. Kurmi, Sandeep Kumar, and Vinay P. Namboodiri", "title": "Deep Bayesian Network for Visual Question Generation", "comments": "WACV-2020 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generating natural questions from an image is a semantic task that requires\nusing vision and language modalities to learn multimodal representations.\nImages can have multiple visual and language cues such as places, captions, and\ntags. In this paper, we propose a principled deep Bayesian learning framework\nthat combines these cues to produce natural questions. We observe that with the\naddition of more cues and by minimizing uncertainty in the among cues, the\nBayesian network becomes more confident. We propose a Minimizing Uncertainty of\nMixture of Cues (MUMC), that minimizes uncertainty present in a mixture of cues\nexperts for generating probabilistic questions. This is a Bayesian framework\nand the results show a remarkable similarity to natural questions as validated\nby a human study. We observe that with the addition of more cues and by\nminimizing uncertainty among the cues, the Bayesian framework becomes more\nconfident. Ablation studies of our model indicate that a subset of cues is\ninferior at this task and hence the principled fusion of cues is preferred.\nFurther, we observe that the proposed approach substantially improves over\nstate-of-the-art benchmarks on the quantitative metrics (BLEU-n, METEOR, ROUGE,\nand CIDEr). Here we provide project link for Deep Bayesian VQG\n\\url{https://delta-lab-iitk.github.io/BVQG/}\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 19:37:20 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Patro", "Badri N.", ""], ["Kurmi", "Vinod K.", ""], ["Kumar", "Sandeep", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2001.08817", "submitter": "Evan Schwab", "authors": "Evan Schwab, Andr\\'e Goo{\\ss}en, Hrishikesh Deshpande, Axel Saalbach", "title": "Localization of Critical Findings in Chest X-Ray without Local\n  Annotations Using Multi-Instance Learning", "comments": "Accepted to International Symposium of Biomedical Imaging (ISBI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic detection of critical findings in chest X-rays (CXR), such as\npneumothorax, is important for assisting radiologists in their clinical\nworkflow like triaging time-sensitive cases and screening for incidental\nfindings. While deep learning (DL) models has become a promising predictive\ntechnology with near-human accuracy, they commonly suffer from a lack of\nexplainability, which is an important aspect for clinical deployment of DL\nmodels in the highly regulated healthcare industry. For example, localizing\ncritical findings in an image is useful for explaining the predictions of DL\nclassification algorithms. While there have been a host of joint classification\nand localization methods for computer vision, the state-of-the-art DL models\nrequire locally annotated training data in the form of pixel level labels or\nbounding box coordinates. In the medical domain, this requires an expensive\namount of manual annotation by medical experts for each critical finding. This\nrequirement becomes a major barrier for training models that can rapidly scale\nto various findings. In this work, we address these shortcomings with an\ninterpretable DL algorithm based on multi-instance learning that jointly\nclassifies and localizes critical findings in CXR without the need for local\nannotations. We show competitive classification results on three different\ncritical findings (pneumothorax, pneumonia, and pulmonary edema) from three\ndifferent CXR datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 21:29:14 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Schwab", "Evan", ""], ["Goo\u00dfen", "Andr\u00e9", ""], ["Deshpande", "Hrishikesh", ""], ["Saalbach", "Axel", ""]]}, {"id": "2001.08839", "submitter": "Xiaolong Ma", "authors": "Zhengang Li, Yifan Gong, Xiaolong Ma, Sijia Liu, Mengshu Sun, Zheng\n  Zhan, Zhenglun Kong, Geng Yuan, Yanzhi Wang", "title": "SS-Auto: A Single-Shot, Automatic Structured Weight Pruning Framework of\n  DNNs with Ultra-High Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured weight pruning is a representative model compression technique of\nDNNs for hardware efficiency and inference accelerations. Previous works in\nthis area leave great space for improvement since sparse structures with\ncombinations of different structured pruning schemes are not exploited fully\nand efficiently. To mitigate the limitations, we propose SS-Auto, a\nsingle-shot, automatic structured pruning framework that can achieve row\npruning and column pruning simultaneously. We adopt soft constraint-based\nformulation to alleviate the strong non-convexity of l0-norm constraints used\nin state-of-the-art ADMM-based methods for faster convergence and fewer\nhyperparameters. Instead of solving the problem directly, a Primal-Proximal\nsolution is proposed to avoid the pitfall of penalizing all weights equally,\nthereby enhancing the accuracy. Extensive experiments on CIFAR-10 and CIFAR-100\ndatasets demonstrate that the proposed framework can achieve ultra-high pruning\nrates while maintaining accuracy. Furthermore, significant inference speedup\nhas been observed from the proposed framework through actual measurements on\nthe smartphone.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 22:45:02 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Li", "Zhengang", ""], ["Gong", "Yifan", ""], ["Ma", "Xiaolong", ""], ["Liu", "Sijia", ""], ["Sun", "Mengshu", ""], ["Zhan", "Zheng", ""], ["Kong", "Zhenglun", ""], ["Yuan", "Geng", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2001.08844", "submitter": "Ali Mohammad Alqudah", "authors": "Ali Mohammad Alqudah, Hiam Alquraan, Isam Abu Qasmieh, Amin Alqudah,\n  Wafaa Al-Sharu", "title": "Brain Tumor Classification Using Deep Learning Technique -- A Comparison\n  between Cropped, Uncropped, and Segmented Lesion Images with Different Sizes", "comments": null, "journal-ref": "IJATCSE, 8(6), pp.3684-3691 (2019)", "doi": "10.30534/ijatcse/2019/155862019", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning is the newest and the current trend of the machine learning\nfield that paid a lot of the researchers' attention in the recent few years. As\na proven powerful machine learning tool, deep learning was widely used in\nseveral applications for solving various complex problems that require\nextremely high accuracy and sensitivity, particularly in the medical field. In\ngeneral, brain tumor is one of the most common and aggressive malignant tumor\ndiseases which is leading to a very short expected life if it is diagnosed at\nhigher grade. Based on that, brain tumor grading is a very critical step after\ndetecting the tumor in order to achieve an effective treating plan. In this\npaper, we used Convolutional Neural Network (CNN) which is one of the most\nwidely used deep learning architectures for classifying a dataset of 3064 T1\nweighted contrast-enhanced brain MR images for grading (classifying) the brain\ntumors into three classes (Glioma, Meningioma, and Pituitary Tumor). The\nproposed CNN classifier is a powerful tool and its overall performance with\naccuracy of 98.93% and sensitivity of 98.18% for the cropped lesions, while the\nresults for the uncropped lesions are 99% accuracy and 98.52% sensitivity and\nthe results for segmented lesion images are 97.62% for accuracy and 97.40%\nsensitivity.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 23:05:19 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Alqudah", "Ali Mohammad", ""], ["Alquraan", "Hiam", ""], ["Qasmieh", "Isam Abu", ""], ["Alqudah", "Amin", ""], ["Al-Sharu", "Wafaa", ""]]}, {"id": "2001.08854", "submitter": "Sriram Baireddy", "authors": "Changye Yang, Sriram Baireddy, Yuhao Chen, Enyu Cai, Denise Caldwell,\n  Val\\'erian M\\'eline, Anjali S. Iyer-Pascuzzi, Edward J. Delp", "title": "Plant Stem Segmentation Using Fast Ground Truth Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately phenotyping plant wilting is important for understanding responses\nto environmental stress. Analysis of the shape of plants can potentially be\nused to accurately quantify the degree of wilting. Plant shape analysis can be\nenhanced by locating the stem, which serves as a consistent reference point\nduring wilting. In this paper, we show that deep learning methods can\naccurately segment tomato plant stems. We also propose a control-point-based\nground truth method that drastically reduces the resources needed to create a\ntraining dataset for a deep learning approach. Experimental results show the\nviability of both our proposed ground truth approach and deep learning based\nstem segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 00:22:14 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Yang", "Changye", ""], ["Baireddy", "Sriram", ""], ["Chen", "Yuhao", ""], ["Cai", "Enyu", ""], ["Caldwell", "Denise", ""], ["M\u00e9line", "Val\u00e9rian", ""], ["Iyer-Pascuzzi", "Anjali S.", ""], ["Delp", "Edward J.", ""]]}, {"id": "2001.08856", "submitter": "Yahia Assiri", "authors": "Yahia Assiri", "title": "Stochastic Optimization of Plain Convolutional Neural Networks with\n  Simple methods", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.04552 by\n  other authors", "journal-ref": "15th International Conference on Machine Learning and Data Mining,\n  MLDM 2019, vol.II, New York, NY, USA, July 20-25, 2019, ibai-publishing, ISSN\n  1864-9734 ISBN 978-3-942952-63-7, pages(833-844)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have been achieving the best possible\naccuracies in many visual pattern classification problems. However, due to the\nmodel capacity required to capture such representations, they are often\noversensitive to overfitting and therefore require proper regularization to\ngeneralize well. In this paper, we present a combination of regularization\ntechniques which work together to get better performance, we built plain CNNs,\nand then we used data augmentation, dropout and customized early stopping\nfunction, we tested and evaluated these techniques by applying models on five\nfamous datasets, MNIST, CIFAR10, CIFAR100, SVHN, STL10, and we achieved three\nstate-of-the-art-of (MNIST, SVHN, STL10) and very high-Accuracy on the other\ntwo datasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 01:20:59 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Assiri", "Yahia", ""]]}, {"id": "2001.08869", "submitter": "Yifei Chen", "authors": "Yifei Chen, Haoyu Ma, Deying Kong, Xiangyi Yan, Jianbao Wu, Wei Fan,\n  Xiaohui Xie", "title": "Nonparametric Structure Regularization Machine for 2D Hand Pose\n  Estimation", "comments": "The paper has be accepted and will be presented at 2020 IEEE Winter\n  Conference on Applications of Computer Vision (WACV). The code is freely\n  available at https://github.com/HowieMa/NSRMhand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pose estimation is more challenging than body pose estimation due to\nsevere articulation, self-occlusion and high dexterity of the hand. Current\napproaches often rely on a popular body pose algorithm, such as the\nConvolutional Pose Machine (CPM), to learn 2D keypoint features. These\nalgorithms cannot adequately address the unique challenges of hand pose\nestimation, because they are trained solely based on keypoint positions without\nseeking to explicitly model structural relationship between them. We propose a\nnovel Nonparametric Structure Regularization Machine (NSRM) for 2D hand pose\nestimation, adopting a cascade multi-task architecture to learn hand structure\nand keypoint representations jointly. The structure learning is guided by\nsynthetic hand mask representations, which are directly computed from keypoint\npositions, and is further strengthened by a novel probabilistic representation\nof hand limbs and an anatomically inspired composition strategy of mask\nsynthesis. We conduct extensive studies on two public datasets - OneHand 10k\nand CMU Panoptic Hand. Experimental results demonstrate that explicitly\nenforcing structure learning consistently improves pose estimation accuracy of\nCPM baseline models, by 1.17% on the first dataset and 4.01% on the second one.\nThe implementation and experiment code is freely available online. Our proposal\nof incorporating structural learning to hand pose estimation requires no\nadditional training information, and can be a generic add-on module to other\npose estimation models.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 03:27:32 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Chen", "Yifei", ""], ["Ma", "Haoyu", ""], ["Kong", "Deying", ""], ["Yan", "Xiangyi", ""], ["Wu", "Jianbao", ""], ["Fan", "Wei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "2001.08878", "submitter": "Xiaodong Wang", "authors": "Xiaodong Wang, Zhedong Zheng, Yang He, Fei Yan, Zhiqiang Zeng, Yi Yang", "title": "Progressive Local Filter Pruning for Image Retrieval Acceleration", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on network pruning for image retrieval acceleration.\nPrevailing image retrieval works target at the discriminative feature learning,\nwhile little attention is paid to how to accelerate the model inference, which\nshould be taken into consideration in real-world practice. The challenge of\npruning image retrieval models is that the middle-level feature should be\npreserved as much as possible. Such different requirements of the retrieval and\nclassification model make the traditional pruning methods not that suitable for\nour task. To solve the problem, we propose a new Progressive Local Filter\nPruning (PLFP) method for image retrieval acceleration. Specifically, layer by\nlayer, we analyze the local geometric properties of each filter and select the\none that can be replaced by the neighbors. Then we progressively prune the\nfilter by gradually changing the filter weights. In this way, the\nrepresentation ability of the model is preserved. To verify this, we evaluate\nour method on two widely-used image retrieval datasets,i.e., Oxford5k and\nParis6K, and one person re-identification dataset,i.e., Market-1501. The\nproposed method arrives with superior performance to the conventional pruning\nmethods, suggesting the effectiveness of the proposed method for image\nretrieval.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 04:28:44 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Wang", "Xiaodong", ""], ["Zheng", "Zhedong", ""], ["He", "Yang", ""], ["Yan", "Fei", ""], ["Zeng", "Zhiqiang", ""], ["Yang", "Yi", ""]]}, {"id": "2001.08893", "submitter": "Daichi Haraguchi", "authors": "Daichi Haraguchi, Shota Harada, Brian Kenji Iwana, Yuto Shinahara,\n  Seiichi Uchida", "title": "Character-independent font identification", "comments": "submitted DAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a countless number of fonts with various shapes and styles. In\naddition, there are many fonts that only have subtle differences in features.\nDue to this, font identification is a difficult task. In this paper, we propose\na method of determining if any two characters are from the same font or not.\nThis is difficult due to the difference between fonts typically being smaller\nthan the difference between alphabet classes. Additionally, the proposed method\ncan be used with fonts regardless of whether they exist in the training or not.\nIn order to accomplish this, we use a Convolutional Neural Network (CNN)\ntrained with various font image pairs. In the experiment, the network is\ntrained on image pairs of various fonts. We then evaluate the model on a\ndifferent set of fonts that are unseen by the network. The evaluation is\nperformed with an accuracy of 92.27%. Moreover, we analyzed the relationship\nbetween character classes and font identification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 05:59:53 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Haraguchi", "Daichi", ""], ["Harada", "Shota", ""], ["Iwana", "Brian Kenji", ""], ["Shinahara", "Yuto", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2001.08895", "submitter": "Abhijit Suprem", "authors": "Abhijit Suprem, Calton Pu, Joao Eduardo Ferreira", "title": "Small, Accurate, and Fast Vehicle Re-ID on the Edge: the SAFR Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Small, Accurate, and Fast Re-ID (SAFR) design for flexible\nvehicle re-id under a variety of compute environments such as cloud, mobile,\nedge, or embedded devices by only changing the re-id model backbone. Through\nbest-fit design choices, feature extraction, training tricks, global attention,\nand local attention, we create a reid model design that optimizes\nmulti-dimensionally along model size, speed, & accuracy for deployment under\nvarious memory and compute constraints. We present several variations of our\nflexible SAFR model: SAFR-Large for cloud-type environments with large compute\nresources, SAFR-Small for mobile devices with some compute constraints, and\nSAFR-Micro for edge devices with severe memory & compute constraints.\n  SAFR-Large delivers state-of-the-art results with mAP 81.34 on the VeRi-776\nvehicle re-id dataset (15% better than related work). SAFR-Small trades a 5.2%\ndrop in performance (mAP 77.14 on VeRi-776) for over 60% model compression and\n150% speedup. SAFR-Micro, at only 6MB and 130MFLOPS, trades 6.8% drop in\naccuracy (mAP 75.80 on VeRi-776) for 95% compression and 33x speedup compared\nto SAFR-Large.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 06:07:17 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Suprem", "Abhijit", ""], ["Pu", "Calton", ""], ["Ferreira", "Joao Eduardo", ""]]}, {"id": "2001.08942", "submitter": "Ge Gao", "authors": "Ge Gao, Mikko Lauri, Yulong Wang, Xiaolin Hu, Jianwei Zhang and Simone\n  Frintrop", "title": "6D Object Pose Regression via Supervised Learning on Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the task of estimating the 6 degrees of freedom pose of\na known 3D object from depth information represented by a point cloud. Deep\nfeatures learned by convolutional neural networks from color information have\nbeen the dominant features to be used for inferring object poses, while depth\ninformation receives much less attention. However, depth information contains\nrich geometric information of the object shape, which is important for\ninferring the object pose. We use depth information represented by point clouds\nas the input to both deep networks and geometry-based pose refinement and use\nseparate networks for rotation and translation regression. We argue that the\naxis-angle representation is a suitable rotation representation for deep\nlearning, and use a geodesic loss function for rotation regression. Ablation\nstudies show that these design choices outperform alternatives such as the\nquaternion representation and L2 loss, or regressing translation and rotation\nwith the same network. Our simple yet effective approach clearly outperforms\nstate-of-the-art methods on the YCB-video dataset. The implementation and\ntrained model are avaliable at: https://github.com/GeeeG/CloudPose.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 10:29:54 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Gao", "Ge", ""], ["Lauri", "Mikko", ""], ["Wang", "Yulong", ""], ["Hu", "Xiaolin", ""], ["Zhang", "Jianwei", ""], ["Frintrop", "Simone", ""]]}, {"id": "2001.08957", "submitter": "Rujikorn Charakorn", "authors": "Rujikorn Charakorn, Yuttapong Thawornwattana, Sirawaj Itthipuripat,\n  Nick Pawlowski, Poramate Manoonpong, Nat Dilokthanakul", "title": "An Explicit Local and Global Representation Disentanglement Framework\n  with Applications in Deep Clustering and Unsupervised Object Detection", "comments": "13 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data can be understood at different levels of granularity, where\nglobal features correspond to semantic-level information and local features\ncorrespond to texture patterns. In this work, we propose a framework, called\nSPLIT, which allows us to disentangle local and global information into two\nseparate sets of latent variables within the variational autoencoder (VAE)\nframework. Our framework adds generative assumption to the VAE by requiring a\nsubset of the latent variables to generate an auxiliary set of observable data.\nThis additional generative assumption primes the latent variables to local\ninformation and encourages the other latent variables to represent global\ninformation. We examine three different flavours of VAEs with different\ngenerative assumptions. We show that the framework can effectively disentangle\nlocal and global information within these models leads to improved\nrepresentation, with better clustering and unsupervised object detection\nbenchmarks. Finally, we establish connections between SPLIT and recent research\nin cognitive neuroscience regarding the disentanglement in human visual\nperception. The code for our experiments is at\nhttps://github.com/51616/split-vae .\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 12:09:20 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 10:11:49 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Charakorn", "Rujikorn", ""], ["Thawornwattana", "Yuttapong", ""], ["Itthipuripat", "Sirawaj", ""], ["Pawlowski", "Nick", ""], ["Manoonpong", "Poramate", ""], ["Dilokthanakul", "Nat", ""]]}, {"id": "2001.08960", "submitter": "Stefan Schubert", "authors": "Stefan Schubert and Peer Neubert and Peter Protzel", "title": "Unsupervised Learning Methods for Visual Place Recognition in Discretely\n  and Continuously Changing Environments", "comments": "Accepted for publication at International Conference on Robotics and\n  Automation (ICRA) 2020. This is the submitted version", "journal-ref": null, "doi": "10.1109/ICRA40945.2020.9197044", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition in changing environments is the problem of finding\nmatchings between two sets of observations, a query set and a reference set,\ndespite severe appearance changes. Recently, image comparison using CNN-based\ndescriptors showed very promising results. However, existing experiments from\nthe literature typically assume a single distinctive condition within each set\n(e.g., reference: day, query: night). We demonstrate that as soon as the\nconditions change within one set (e.g., reference: day, query: traversal\ndaytime-dusk-night-dawn), different places under the same condition can\nsuddenly look more similar than same places under different conditions and\nstate-of-the-art approaches like CNN-based descriptors fail. This paper\ndiscusses this practically very important problem of in-sequence condition\nchanges and defines a hierarchy of problem setups from (1) no in-sequence\nchanges, (2) discrete in-sequence changes, to (3) continuous in-sequence\nchanges. We will experimentally evaluate the effect of these changes on two\nstate-of-the-art CNN-descriptors. Our experiments emphasize the importance of\nstatistical standardization of descriptors and shows its limitations in case of\ncontinuous changes. To address this practically most relevant setup, we\ninvestigate and experimentally evaluate the application of unsupervised\nlearning methods using two available PCA-based approaches and propose a novel\nclustering-based extension of the statistical normalization.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 12:23:15 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Schubert", "Stefan", ""], ["Neubert", "Peer", ""], ["Protzel", "Peter", ""]]}, {"id": "2001.08972", "submitter": "Tony Ng", "authors": "Tony Ng, Vassileios Balntas, Yurun Tian, Krystian Mikolajczyk", "title": "SOLAR: Second-Order Loss and Attention for Image Retrieval", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in deep-learning have shown that second-order information is\nbeneficial in many computer-vision tasks. Second-order information can be\nenforced both in the spatial context and the abstract feature dimensions. In\nthis work, we explore two second-order components. One is focused on\nsecond-order spatial information to increase the performance of image\ndescriptors, both local and global. It is used to re-weight feature maps, and\nthus emphasise salient image locations that are subsequently used for\ndescription. The second component is concerned with a second-order similarity\n(SOS) loss, that we extend to global descriptors for image retrieval, and is\nused to enhance the triplet loss with hard-negative mining. We validate our\napproach on two different tasks and datasets for image retrieval and image\nmatching. The results show that our two second-order components complement each\nother, bringing significant performance improvements in both tasks and lead to\nstate-of-the-art results across the public benchmarks. Code available at:\nhttp://github.com/tonyngjichun/SOLAR\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 12:54:23 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 16:47:42 GMT"}, {"version": "v3", "created": "Sun, 9 Feb 2020 12:09:07 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 13:43:47 GMT"}, {"version": "v5", "created": "Tue, 4 Aug 2020 17:36:45 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Ng", "Tony", ""], ["Balntas", "Vassileios", ""], ["Tian", "Yurun", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2001.09017", "submitter": "Bogdan Savchynskyy", "authors": "Bogdan Savchynskyy", "title": "Discrete graphical models -- an optimization perspective", "comments": "270 pages", "journal-ref": "Foundations and Trends in Computer Graphics and Vision: Vol. 11:\n  No. 3-4, pp 160-429 (2019)", "doi": "10.1561/0600000084", "report-no": null, "categories": "math.OC cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This monograph is about discrete energy minimization for discrete graphical\nmodels. It considers graphical models, or, more precisely, maximum a posteriori\ninference for graphical models, purely as a combinatorial optimization problem.\nModeling, applications, probabilistic interpretations and many other aspects\nare either ignored here or find their place in examples and remarks only. It\ncovers the integer linear programming formulation of the problem as well as its\nlinear programming, Lagrange and Lagrange decomposition-based relaxations. In\nparticular, it provides a detailed analysis of the polynomially solvable\nacyclic and submodular problems, along with the corresponding exact\noptimization methods. Major approximate methods, such as message passing and\ngraph cut techniques are also described and analyzed comprehensively. The\nmonograph can be useful for undergraduate and graduate students studying\noptimization or graphical models, as well as for experts in optimization who\nwant to have a look into graphical models. To make the monograph suitable for\nboth categories of readers we explicitly separate the mathematical optimization\nbackground chapters from those specific to graphical models.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 14:01:50 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Savchynskyy", "Bogdan", ""]]}, {"id": "2001.09021", "submitter": "Zhao Zhang", "authors": "Zhao Zhang, Zemin Tang, Yang Wang, Zheng Zhang, Choujun Zhan, Zhengjun\n  Zha, Meng Wang", "title": "Dense Residual Network: Enhancing Global Dense Feature Flow for\n  Character Recognition", "comments": "Please cite this work as: Zhao Zhang, Zemin Tang, Yang Wang, Zheng\n  Zhang, Choujun Zhan, Zhengjun Zha and Meng Wang, \"Dense Residual Network:\n  Enhancing Global Dense Feature Flow for Character Recognition,\" Neural\n  Networks (NN), Feb 2021. arXiv admin note: text overlap with arXiv:1912.07016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs), such as Dense Convolutional\nNetworks (DenseNet), have achieved great success for image representation by\ndiscovering deep hierarchical information. However, most existing networks\nsimply stacks the convolutional layers and hence failing to fully discover\nlocal and global feature information among layers. In this paper, we mainly\nexplore how to enhance the local and global dense feature flow by exploiting\nhierarchical features fully from all the convolution layers. Technically, we\npropose an efficient and effective CNN framework, i.e., Fast Dense Residual\nNetwork (FDRN), for text recognition. To construct FDRN, we propose a new fast\nresidual dense block (f-RDB) to retain the ability of local feature fusion and\nlocal residual learning of original RDB, which can reduce the computing efforts\nat the same time. After fully learning local residual dense features, we\nutilize the sum operation and several f-RDBs to define a new block termed\nglobal dense block (GDB) by imitating the construction of dense blocks to learn\nglobal dense residual features adaptively in a holistic way. Finally, we use\ntwo convolution layers to construct a down-sampling block to reduce the global\nfeature size and extract deeper features. Extensive simulations show that FDRN\nobtains the enhanced recognition results, compared with other related models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 06:55:08 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 08:44:30 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 10:47:35 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2021 02:35:30 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhang", "Zhao", ""], ["Tang", "Zemin", ""], ["Wang", "Yang", ""], ["Zhang", "Zheng", ""], ["Zhan", "Choujun", ""], ["Zha", "Zhengjun", ""], ["Wang", "Meng", ""]]}, {"id": "2001.09046", "submitter": "Bart Smets", "authors": "Bart Smets, Jim Portegies, Erik Bekkers, Remco Duits", "title": "PDE-based Group Equivariant Convolutional Neural Networks", "comments": "27 pages, 18 figures. v2 changes: - mentioned KerCNNs - added section\n  Generalization of G-CNNs - clarification that the experiments utilized\n  automatic differentiation and SGD. v3 changes: - streamlined theoretical\n  framework - formulation and proof Thm.1 & 2 - expanded experiments. v4\n  changes: typos in Prop.5 and (20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n  Having all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n  We solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article. Just like for linear convolution a\nmorphological convolution is specified by a kernel that we train in our\nPDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as max/min-pooling\nand ReLUs as they are already subsumed by morphological convolutions.\n  We present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 15:00:46 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 14:16:16 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 07:56:22 GMT"}, {"version": "v4", "created": "Sat, 24 Jul 2021 11:14:06 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Smets", "Bart", ""], ["Portegies", "Jim", ""], ["Bekkers", "Erik", ""], ["Duits", "Remco", ""]]}, {"id": "2001.09061", "submitter": "Jonas Teuwen", "authors": "Nikita Moriakov, Jonas Adler, Jonas Teuwen", "title": "Kernel of CycleGAN as a Principle homogeneous space", "comments": "Accepted at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unpaired image-to-image translation has attracted significant interest due to\nthe invention of CycleGAN, a method which utilizes a combination of adversarial\nand cycle consistency losses to avoid the need for paired data. It is known\nthat the CycleGAN problem might admit multiple solutions, and our goal in this\npaper is to analyze the space of exact solutions and to give perturbation\nbounds for approximate solutions. We show theoretically that the exact solution\nspace is invariant with respect to automorphisms of the underlying probability\nspaces, and, furthermore, that the group of automorphisms acts freely and\ntransitively on the space of exact solutions. We examine the case of zero\n`pure' CycleGAN loss first in its generality, and, subsequently, expand our\nanalysis to approximate solutions for `extended' CycleGAN loss where identity\nloss term is included. In order to demonstrate that these results are\napplicable, we show that under mild conditions nontrivial smooth automorphisms\nexist. Furthermore, we provide empirical evidence that neural networks can\nlearn these automorphisms with unexpected and unwanted results. We conclude\nthat finding optimal solutions to the CycleGAN loss does not necessarily lead\nto the envisioned result in image-to-image translation tasks and that\nunderlying hidden symmetries can render the result utterly useless.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 15:47:12 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Moriakov", "Nikita", ""], ["Adler", "Jonas", ""], ["Teuwen", "Jonas", ""]]}, {"id": "2001.09067", "submitter": "Matthias Hullin", "authors": "Javier Grau Chopite, Matthias B. Hullin, Michael Wand and Julian\n  Iseringhausen", "title": "Deep Non-Line-of-Sight Reconstruction", "comments": "Minor editorial update (typos, figure captions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent years have seen a surge of interest in methods for imaging beyond\nthe direct line of sight. The most prominent techniques rely on time-resolved\noptical impulse responses, obtained by illuminating a diffuse wall with an\nultrashort light pulse and observing multi-bounce indirect reflections with an\nultrafast time-resolved imager. Reconstruction of geometry from such data,\nhowever, is a complex non-linear inverse problem that comes with substantial\ncomputational demands. In this paper, we employ convolutional feed-forward\nnetworks for solving the reconstruction problem efficiently while maintaining\ngood reconstruction quality. Specifically, we devise a tailored autoencoder\narchitecture, trained end-to-end, that maps transient images directly to a\ndepth map representation. Training is done using an efficient transient\nrenderer for diffuse three-bounce indirect light transport that enables the\nquick generation of large amounts of training data for the network. We examine\nthe performance of our method on a variety of synthetic and experimental\ndatasets and its dependency on the choice of training data and augmentation\nstrategies, as well as architectural features. We demonstrate that our\nfeed-forward network, even though it is trained solely on synthetic data,\ngeneralizes to measured data from SPAD sensors and is able to obtain results\nthat are competitive with model-based reconstruction methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 16:05:50 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 12:42:53 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Chopite", "Javier Grau", ""], ["Hullin", "Matthias B.", ""], ["Wand", "Michael", ""], ["Iseringhausen", "Julian", ""]]}, {"id": "2001.09087", "submitter": "Yuan Xie", "authors": "Jiachen Xu, Jingyu Gong, Jie Zhou, Xin Tan, Yuan Xie, Lizhuang Ma", "title": "SceneEncoder: Scene-Aware Semantic Segmentation of Point Clouds with A\n  Learnable Scene Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides local features, global information plays an essential role in\nsemantic segmentation, while recent works usually fail to explicitly extract\nthe meaningful global information and make full use of it. In this paper, we\npropose a SceneEncoder module to impose a scene-aware guidance to enhance the\neffect of global information. The module predicts a scene descriptor, which\nlearns to represent the categories of objects existing in the scene and\ndirectly guides the point-level semantic segmentation through filtering out\ncategories not belonging to this scene. Additionally, to alleviate segmentation\nnoise in local region, we design a region similarity loss to propagate\ndistinguishing features to their own neighboring points with the same label,\nleading to the enhancement of the distinguishing ability of point-wise\nfeatures. We integrate our methods into several prevailing networks and conduct\nextensive experiments on benchmark datasets ScanNet and ShapeNet. Results show\nthat our methods greatly improve the performance of baselines and achieve\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 16:53:30 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Xu", "Jiachen", ""], ["Gong", "Jingyu", ""], ["Zhou", "Jie", ""], ["Tan", "Xin", ""], ["Xie", "Yuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2001.09099", "submitter": "Jie Lei", "authors": "Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal", "title": "TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval", "comments": "ECCV 2020 (extended version, with TVC dataset+models; 35 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TV show Retrieval (TVR), a new multimodal retrieval dataset. TVR\nrequires systems to understand both videos and their associated subtitle\n(dialogue) texts, making it more realistic. The dataset contains 109K queries\ncollected on 21.8K videos from 6 TV shows of diverse genres, where each query\nis associated with a tight temporal window. The queries are also labeled with\nquery types that indicate whether each of them is more related to video or\nsubtitle or both, allowing for in-depth analysis of the dataset and the methods\nthat built on top of it. Strict qualification and post-annotation verification\ntests are applied to ensure the quality of the collected data. Further, we\npresent several baselines and a novel Cross-modal Moment Localization (XML )\nnetwork for multimodal moment retrieval tasks. The proposed XML model uses a\nlate fusion design with a novel Convolutional Start-End detector (ConvSE),\nsurpassing baselines by a large margin and with better efficiency, providing a\nstrong starting point for future work. We have also collected additional\ndescriptions for each annotated moment in TVR to form a new multimodal\ncaptioning dataset with 262K captions, named TV show Caption (TVC). Both\ndatasets are publicly available. TVR: https://tvr.cs.unc.edu, TVC:\nhttps://tvr.cs.unc.edu/tvc.html.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 17:09:39 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 15:12:14 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Lei", "Jie", ""], ["Yu", "Licheng", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "2001.09136", "submitter": "Adam Byerly", "authors": "Adam Byerly, Tatiana Kalganova, Ian Dear", "title": "No Routing Needed Between Capsules", "comments": "13 pages, 7 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most capsule network designs rely on traditional matrix multiplication\nbetween capsule layers and computationally expensive routing mechanisms to deal\nwith the capsule dimensional entanglement that the matrix multiplication\nintroduces. By using Homogeneous Vector Capsules (HVCs), which use element-wise\nmultiplication rather than matrix multiplication, the dimensions of the\ncapsules remain unentangled. In this work, we study HVCs as applied to the\nhighly structured MNIST dataset in order to produce a direct comparison to the\ncapsule research direction of Geoffrey Hinton, et al. In our study, we show\nthat a simple convolutional neural network using HVCs performs as well as the\nprior best performing capsule network on MNIST using 5.5x fewer parameters, 4x\nfewer training epochs, no reconstruction sub-network, and requiring no routing\nmechanism. The addition of multiple classification branches to the network\nestablishes a new state of the art for the MNIST dataset with an accuracy of\n99.87% for an ensemble of these models, as well as establishing a new state of\nthe art for a single model (99.83% accurate).\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 18:37:41 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 17:04:40 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 15:47:01 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2020 15:55:18 GMT"}, {"version": "v5", "created": "Wed, 27 Jan 2021 01:44:46 GMT"}, {"version": "v6", "created": "Thu, 17 Jun 2021 20:14:13 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Byerly", "Adam", ""], ["Kalganova", "Tatiana", ""], ["Dear", "Ian", ""]]}, {"id": "2001.09138", "submitter": "Juan Miguel Valverde", "authors": "Juan Miguel Valverde, Artem Shatillo, Riccardo de Feo, Olli Gr\\\"ohn,\n  Alejandra Sierra, Jussi Tohka", "title": "RatLesNetv2: A Fully Convolutional Network for Rodent Brain Lesion\n  Segmentation", "comments": "Added new changes", "journal-ref": null, "doi": "10.3389/fnins.2020.610239", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a fully convolutional neural network (ConvNet), named RatLesNetv2,\nfor segmenting lesions in rodent magnetic resonance (MR) brain images.\nRatLesNetv2 architecture resembles an autoencoder and it incorporates residual\nblocks that facilitate its optimization. RatLesNetv2 is trained end to end on\nthree-dimensional images and it requires no preprocessing. We evaluated\nRatLesNetv2 on an exceptionally large dataset composed of 916 T2-weighted rat\nbrain MRI scans of 671 rats at nine different lesion stages that were used to\nstudy focal cerebral ischemia for drug development. In addition, we compared\nits performance with three other ConvNets specifically designed for medical\nimage segmentation. RatLesNetv2 obtained similar to higher Dice coefficient\nvalues than the other ConvNets and it produced much more realistic and compact\nsegmentations with notably fewer holes and lower Hausdorff distance. The Dice\nscores of RatLesNetv2 segmentations also exceeded inter-rater agreement of\nmanual segmentations. In conclusion, RatLesNetv2 could be used for automated\nlesion segmentation, reducing human workload and improving reproducibility.\nRatLesNetv2 is publicly available at https://github.com/jmlipman/RatLesNetv2.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 18:40:39 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 07:25:27 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 10:55:05 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 09:05:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Valverde", "Juan Miguel", ""], ["Shatillo", "Artem", ""], ["de Feo", "Riccardo", ""], ["Gr\u00f6hn", "Olli", ""], ["Sierra", "Alejandra", ""], ["Tohka", "Jussi", ""]]}, {"id": "2001.09174", "submitter": "Youbao Tang", "authors": "Vatsal Agarwal, Youbao Tang, Jing Xiao, Ronald M. Summers", "title": "Weakly Supervised Lesion Co-segmentation on CT Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion segmentation in medical imaging serves as an effective tool for\nassessing tumor sizes and monitoring changes in growth. However, not only is\nmanual lesion segmentation time-consuming, but it is also expensive and\nrequires expert radiologist knowledge. Therefore many hospitals rely on a loose\nsubstitute called response evaluation criteria in solid tumors (RECIST).\nAlthough these annotations are far from precise, they are widely used\nthroughout hospitals and are found in their picture archiving and communication\nsystems (PACS). Therefore, these annotations have the potential to serve as a\nrobust yet challenging means of weak supervision for training full lesion\nsegmentation models. In this work, we propose a weakly-supervised\nco-segmentation model that first generates pseudo-masks from the RECIST slices\nand uses these as training labels for an attention-based convolutional neural\nnetwork capable of segmenting common lesions from a pair of CT scans. To\nvalidate and test the model, we utilize the DeepLesion dataset, an extensive\nCT-scan lesion dataset that contains 32,735 PACS bookmarked images. Extensive\nexperimental results demonstrate the efficacy of our co-segmentation approach\nfor lesion segmentation with a mean Dice coefficient of 90.3%.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 19:39:33 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Agarwal", "Vatsal", ""], ["Tang", "Youbao", ""], ["Xiao", "Jing", ""], ["Summers", "Ronald M.", ""]]}, {"id": "2001.09181", "submitter": "Ziran Wang", "authors": "Zhensong Wei, Yu Jiang, Xishun Liao, Xuewei Qi, Ziran Wang, Guoyuan\n  Wu, Peng Hao, Matthew Barth", "title": "End-to-End Vision-Based Adaptive Cruise Control (ACC) Using Deep\n  Reinforcement Learning", "comments": "This manuscript was presented at 99th Transportation Research Board\n  Annual Meeting in Washington D.C., Jan 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presented a deep reinforcement learning method named Double Deep\nQ-networks to design an end-to-end vision-based adaptive cruise control (ACC)\nsystem. A simulation environment of a highway scene was set up in Unity, which\nis a game engine that provided both physical models of vehicles and feature\ndata for training and testing. Well-designed reward functions associated with\nthe following distance and throttle/brake force were implemented in the\nreinforcement learning model for both internal combustion engine (ICE) vehicles\nand electric vehicles (EV) to perform adaptive cruise control. The gap\nstatistics and total energy consumption are evaluated for different vehicle\ntypes to explore the relationship between reward functions and powertrain\ncharacteristics. Compared with the traditional radar-based ACC systems or\nhuman-in-the-loop simulation, the proposed vision-based ACC system can generate\neither a better gap regulated trajectory or a smoother speed trajectory\ndepending on the preset reward function. The proposed system can be well\nadaptive to different speed trajectories of the preceding vehicle and operated\nin real-time.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 20:02:50 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Wei", "Zhensong", ""], ["Jiang", "Yu", ""], ["Liao", "Xishun", ""], ["Qi", "Xuewei", ""], ["Wang", "Ziran", ""], ["Wu", "Guoyuan", ""], ["Hao", "Peng", ""], ["Barth", "Matthew", ""]]}, {"id": "2001.09189", "submitter": "Michael Jones", "authors": "Bharathkumar Ramachandra, Michael J. Jones, Ranga Raju Vatsavai", "title": "Learning a distance function with a Siamese network to localize\n  anomalies in videos", "comments": "accepted to WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a new approach to localize anomalies in surveillance\nvideo. The main novelty is the idea of using a Siamese convolutional neural\nnetwork (CNN) to learn a distance function between a pair of video patches\n(spatio-temporal regions of video). The learned distance function, which is not\nspecific to the target video, is used to measure the distance between each\nvideo patch in the testing video and the video patches found in normal training\nvideo. If a testing video patch is not similar to any normal video patch then\nit must be anomalous. We compare our approach to previously published\nalgorithms using 4 evaluation measures and 3 challenging target benchmark\ndatasets. Experiments show that our approach either surpasses or performs\ncomparably to current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 20:47:05 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Ramachandra", "Bharathkumar", ""], ["Jones", "Michael J.", ""], ["Vatsavai", "Ranga Raju", ""]]}, {"id": "2001.09193", "submitter": "Anjany Kumar Sekuboyina", "authors": "Anjany Sekuboyina, Malek E. Husseini, Amirhossein Bayat, Maximilian\n  L\\\"offler, Hans Liebl, Hongwei Li, Giles Tetteh, Jan Kuka\\v{c}ka, Christian\n  Payer, Darko \\v{S}tern, Martin Urschler, Maodong Chen, Dalong Cheng, Nikolas\n  Lessmann, Yujin Hu, Tianfu Wang, Dong Yang, Daguang Xu, Felix Ambellan, Tamaz\n  Amiranashvili, Moritz Ehlke, Hans Lamecker, Sebastian Lehnert, Marilia Lirio,\n  Nicol\\'as P\\'erez de Olaguer, Heiko Ramm, Manish Sahu, Alexander Tack, Stefan\n  Zachow, Tao Jiang, Xinjun Ma, Christoph Angerman, Xin Wang, Kevin Brown,\n  Matthias Wolf, Alexandre Kirszenberg, \\'Elodie Puybareau, Di Chen, Yiwei Bai,\n  Brandon H. Rapazzo, Timyoas Yeah, Amber Zhang, Shangliang Xu, Feng Hou,\n  Zhiqiang He, Chan Zeng, Zheng Xiangshang, Xu Liming, Tucker J. Netherton,\n  Raymond P. Mumme, Laurence E. Court, Zixun Huang, Chenhang He, Li-Wen Wang,\n  Sai Ho Ling, L\\^e Duy Hu\\`ynh, Nicolas Boutry, Roman Jakubicek, Jiri Chmelik,\n  Supriti Mulay, Mohanasankar Sivaprakasam, Johannes C. Paetzold, Suprosanna\n  Shit, Ivan Ezhov, Benedikt Wiestler, Ben Glocker, Alexander Valentinitsch,\n  Markus Rempfler, Bj\\\"orn H. Menze and Jan S. Kirschke", "title": "VerSe: A Vertebrae Labelling and Segmentation Benchmark for\n  Multi-detector CT Images", "comments": "Preprint for the VerSe 2019 and 2020 challenge report. Under review\n  at Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Vertebral labelling and segmentation are two fundamental tasks in an\nautomated spine processing pipeline. Reliable and accurate processing of spine\nimages is expected to benefit clinical decision-support systems for diagnosis,\nsurgery planning, and population-based analysis on spine and bone health.\nHowever, designing automated algorithms for spine processing is challenging\npredominantly due to considerable variations in anatomy and acquisition\nprotocols and due to a severe shortage of publicly available data. Addressing\nthese limitations, the Large Scale Vertebrae Segmentation Challenge (VerSe) was\norganised in conjunction with the International Conference on Medical Image\nComputing and Computer Assisted Intervention (MICCAI) in 2019 and 2020, with a\ncall for algorithms towards labelling and segmentation of vertebrae. Two\ndatasets containing a total of 374 multi-detector CT scans from 355 patients\nwere prepared and 4505 vertebrae have individually been annotated at\nvoxel-level by a human-machine hybrid algorithm (https://osf.io/nqjyw/,\nhttps://osf.io/t98fz/). A total of 25 algorithms were benchmarked on these\ndatasets. In this work, we present the the results of this evaluation and\nfurther investigate the performance-variation at vertebra-level, scan-level,\nand at different fields-of-view. We also evaluate the generalisability of the\napproaches to an implicit domain shift in data by evaluating the top performing\nalgorithms of one challenge iteration on data from the other iteration. The\nprincipal takeaway from VerSe: the performance of an algorithm in labelling and\nsegmenting a spine scan hinges on its ability to correctly identify vertebrae\nin cases of rare anatomical variations. The content and code concerning VerSe\ncan be accessed at: https://github.com/anjany/verse.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 21:09:18 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 16:41:14 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 10:36:03 GMT"}, {"version": "v4", "created": "Mon, 22 Mar 2021 16:58:59 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sekuboyina", "Anjany", ""], ["Husseini", "Malek E.", ""], ["Bayat", "Amirhossein", ""], ["L\u00f6ffler", "Maximilian", ""], ["Liebl", "Hans", ""], ["Li", "Hongwei", ""], ["Tetteh", "Giles", ""], ["Kuka\u010dka", "Jan", ""], ["Payer", "Christian", ""], ["\u0160tern", "Darko", ""], ["Urschler", "Martin", ""], ["Chen", "Maodong", ""], ["Cheng", "Dalong", ""], ["Lessmann", "Nikolas", ""], ["Hu", "Yujin", ""], ["Wang", "Tianfu", ""], ["Yang", "Dong", ""], ["Xu", "Daguang", ""], ["Ambellan", "Felix", ""], ["Amiranashvili", "Tamaz", ""], ["Ehlke", "Moritz", ""], ["Lamecker", "Hans", ""], ["Lehnert", "Sebastian", ""], ["Lirio", "Marilia", ""], ["de Olaguer", "Nicol\u00e1s P\u00e9rez", ""], ["Ramm", "Heiko", ""], ["Sahu", "Manish", ""], ["Tack", "Alexander", ""], ["Zachow", "Stefan", ""], ["Jiang", "Tao", ""], ["Ma", "Xinjun", ""], ["Angerman", "Christoph", ""], ["Wang", "Xin", ""], ["Brown", "Kevin", ""], ["Wolf", "Matthias", ""], ["Kirszenberg", "Alexandre", ""], ["Puybareau", "\u00c9lodie", ""], ["Chen", "Di", ""], ["Bai", "Yiwei", ""], ["Rapazzo", "Brandon H.", ""], ["Yeah", "Timyoas", ""], ["Zhang", "Amber", ""], ["Xu", "Shangliang", ""], ["Hou", "Feng", ""], ["He", "Zhiqiang", ""], ["Zeng", "Chan", ""], ["Xiangshang", "Zheng", ""], ["Liming", "Xu", ""], ["Netherton", "Tucker J.", ""], ["Mumme", "Raymond P.", ""], ["Court", "Laurence E.", ""], ["Huang", "Zixun", ""], ["He", "Chenhang", ""], ["Wang", "Li-Wen", ""], ["Ling", "Sai Ho", ""], ["Huynh", "L\u00ea Duy", ""], ["Boutry", "Nicolas", ""], ["Jakubicek", "Roman", ""], ["Chmelik", "Jiri", ""], ["Mulay", "Supriti", ""], ["Sivaprakasam", "Mohanasankar", ""], ["Paetzold", "Johannes C.", ""], ["Shit", "Suprosanna", ""], ["Ezhov", "Ivan", ""], ["Wiestler", "Benedikt", ""], ["Glocker", "Ben", ""], ["Valentinitsch", "Alexander", ""], ["Rempfler", "Markus", ""], ["Menze", "Bj\u00f6rn H.", ""], ["Kirschke", "Jan S.", ""]]}, {"id": "2001.09203", "submitter": "Erez Yahalomi", "authors": "Erez Yahalomi", "title": "Modular network for high accuracy object detection", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel modular object detection convolutional neural network that\nsignificantly improves the accuracy of object detection. The network consists\nof two stages in a hierarchical structure. The first stage is a network that\ndetects general classes. The second stage consists of separate networks to\nrefine the classification and localization of each of the general classes\nobjects. Compared to a state of the art object detection networks the\nclassification error in the modular network is improved by approximately 3-5\ntimes, from 12% to 2.5 %-4.5%. This network is easy to implement and has a 0.94\nmAP. The network architecture can be a platform to improve the accuracy of\nwidespread state of the art object detection networks and other kinds of deep\nlearning networks. We show that a deep learning network initialized by transfer\nlearning becomes more accurate as the number of classes it later trained to\ndetect becomes smaller.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 21:38:22 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 15:50:45 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 19:03:31 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yahalomi", "Erez", ""]]}, {"id": "2001.09220", "submitter": "Wei Wang", "authors": "Wei Wang, Shibo Zhou, Jingxi Li, Xiaohua Li, Junsong Yuan, Zhanpeng\n  Jin", "title": "Temporal Pulses Driven Spiking Neural Network for Fast Object\n  Recognition in Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate real-time object recognition from sensory data has long been a\ncrucial and challenging task for autonomous driving. Even though deep neural\nnetworks (DNNs) have been successfully applied in this area, most existing\nmethods still heavily rely on the pre-processing of the pulse signals derived\nfrom LiDAR sensors, and therefore introduce additional computational overhead\nand considerable latency. In this paper, we propose an approach to address the\nobject recognition problem directly with raw temporal pulses utilizing the\nspiking neural network (SNN). Being evaluated on various datasets (including\nSim LiDAR, KITTI and DVS-barrel) derived from LiDAR and dynamic vision sensor\n(DVS), our proposed method has shown comparable performance as the\nstate-of-the-art methods, while achieving remarkable time efficiency. It\nhighlights the SNN's great potentials in autonomous driving and related\napplications. To the best of our knowledge, this is the first attempt to use\nSNN to directly perform object recognition on raw temporal pulses.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 22:58:55 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Wang", "Wei", ""], ["Zhou", "Shibo", ""], ["Li", "Jingxi", ""], ["Li", "Xiaohua", ""], ["Yuan", "Junsong", ""], ["Jin", "Zhanpeng", ""]]}, {"id": "2001.09252", "submitter": "Yanwei Pang", "authors": "Jin Xie and Yanwei Pang and Hisham Cholakkal and Rao Muhammad Anwer\n  and Fahad Shahbaz Khan and Ling Shao", "title": "PSC-Net: Learning Part Spatial Co-occurrence for Occluded Pedestrian\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting pedestrians, especially under heavy occlusions, is a challenging\ncomputer vision problem with numerous real-world applications. This paper\nintroduces a novel approach, termed as PSC-Net, for occluded pedestrian\ndetection. The proposed PSC-Net contains a dedicated module that is designed to\nexplicitly capture both inter and intra-part co-occurrence information of\ndifferent pedestrian body parts through a Graph Convolutional Network (GCN).\nBoth inter and intra-part co-occurrence information contribute towards\nimproving the feature representation for handling varying level of occlusions,\nranging from partial to severe occlusions. Our PSC-Net exploits the topological\nstructure of pedestrian and does not require part-based annotations or\nadditional visible bounding-box (VBB) information to learn part spatial\nco-occurrence. Comprehensive experiments are performed on two challenging\ndatasets: CityPersons and Caltech datasets. The proposed PSC-Net achieves\nstate-of-the-art detection performance on both. On the heavy occluded\n(\\textbf{HO}) set of CityPerosns test set, our PSC-Net obtains an absolute gain\nof 4.0\\% in terms of log-average miss rate over the state-of-the-art with same\nbackbone, input scale and without using additional VBB supervision. Further,\nPSC-Net improves the state-of-the-art from 37.9 to 34.8 in terms of log-average\nmiss rate on Caltech (\\textbf{HO}) test set.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 02:03:17 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 13:19:15 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Xie", "Jin", ""], ["Pang", "Yanwei", ""], ["Cholakkal", "Hisham", ""], ["Anwer", "Rao Muhammad", ""], ["Khan", "Fahad Shahbaz", ""], ["Shao", "Ling", ""]]}, {"id": "2001.09257", "submitter": "Praveen Narayanan", "authors": "Nikita Jaipuria, Shubh Gupta, Praveen Narayanan, Vidya N. Murali", "title": "On the Role of Receptive Field in Unsupervised Sim-to-Real Image\n  Translation", "comments": "Machine Learning for Autonomous Driving Workshop at the 33rd\n  Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are now widely used for\nphoto-realistic image synthesis. In applications where a simulated image needs\nto be translated into a realistic image (sim-to-real), GANs trained on unpaired\ndata from the two domains are susceptible to failure in semantic content\nretention as the image is translated from one domain to the other. This failure\nmode is more pronounced in cases where the real data lacks content diversity,\nresulting in a content \\emph{mismatch} between the two domains - a situation\noften encountered in real-world deployment. In this paper, we investigate the\nrole of the discriminator's receptive field in GANs for unsupervised\nimage-to-image translation with mismatched data, and study its effect on\nsemantic content retention. Experiments with the discriminator architecture of\na state-of-the-art coupled Variational Auto-Encoder (VAE) - GAN model on\ndiverse, mismatched datasets show that the discriminator receptive field is\ndirectly correlated with semantic content discrepancy of the generated image.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 03:02:12 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Jaipuria", "Nikita", ""], ["Gupta", "Shubh", ""], ["Narayanan", "Praveen", ""], ["Murali", "Vidya N.", ""]]}, {"id": "2001.09284", "submitter": "Zhaokang Chen", "authors": "Zhaokang Chen and Bertram E. Shi", "title": "GEDDnet: A Network for Gaze Estimation with Dilation and Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance-based gaze estimation from RGB images provides relatively\nunconstrained gaze tracking from commonly available hardware. The accuracy of\nsubject-independent models is limited partly by small intra-subject and large\ninter-subject variations in appearance, and partly by a latent\nsubject-dependent bias. To improve estimation accuracy, we propose to use\ndilated-convolutions in a deep convolutional neural network to capture subtle\nchanges in the eye images, and a novel gaze decomposition method that\ndecomposes the gaze angle into the sum of a subject-independent gaze estimate\nfrom the image and a subject-dependent bias. To further reduce estimation\nerror, we propose a calibration method that estimates the bias from a few\nimages taken as the subject gazes at only a few or even just a single gaze\ntarget. This significantly redues calibration time and complexity. Experiments\non four datasets, including a new dataset we collected containing large\nvariations in head pose and face location, indicate that even without\ncalibration the estimator already outperforms state-of-the-art methods by more\nthan 6.3%. The proposed calibration method is robust to the location of\ncalibration target and reduces estimation error significantly (up to 35.6%),\nachieving state-of-the-art performance with much less calibration data than\nrequired by previously proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 09:30:06 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Chen", "Zhaokang", ""], ["Shi", "Bertram E.", ""]]}, {"id": "2001.09308", "submitter": "Zhenfang Chen", "authors": "Zhenfang Chen, Lin Ma, Wenhan Luo, Peng Tang, Kwan-Yee K. Wong", "title": "Look Closer to Ground Better: Weakly-Supervised Temporal Grounding of\n  Sentence in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of weakly-supervised temporal grounding\nof sentence in video. Specifically, given an untrimmed video and a query\nsentence, our goal is to localize a temporal segment in the video that\nsemantically corresponds to the query sentence, with no reliance on any\ntemporal annotation during training. We propose a two-stage model to tackle\nthis problem in a coarse-to-fine manner. In the coarse stage, we first generate\na set of fixed-length temporal proposals using multi-scale sliding windows, and\nmatch their visual features against the sentence features to identify the\nbest-matched proposal as a coarse grounding result. In the fine stage, we\nperform a fine-grained matching between the visual features of the frames in\nthe best-matched proposal and the sentence features to locate the precise frame\nboundary of the fine grounding result. Comprehensive experiments on the\nActivityNet Captions dataset and the Charades-STA dataset demonstrate that our\ntwo-stage model achieves compelling performance.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 13:07:43 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Chen", "Zhenfang", ""], ["Ma", "Lin", ""], ["Luo", "Wenhan", ""], ["Tang", "Peng", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "2001.09313", "submitter": "Hongwei Li", "authors": "Hongwei Li, Timo Loehr, Anjany Sekuboyina, Jianguo Zhang, Benedikt\n  Wiestler, and Bjoern Menze", "title": "Domain Adaptive Medical Image Segmentation via Adversarial Learning of\n  Disease-Specific Spatial Patterns", "comments": "submitted to a journal and under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, the heterogeneity of multi-centre data impedes the\napplicability of deep learning-based methods and results in significant\nperformance degradation when applying models in an unseen data domain, e.g. a\nnew centreor a new scanner. In this paper, we propose an unsupervised domain\nadaptation framework for boosting image segmentation performance across\nmultiple domains without using any manual annotations from the new target\ndomains, but by re-calibrating the networks on few images from the target\ndomain. To achieve this, we enforce architectures to be adaptive to new data by\nrejecting improbable segmentation patterns and implicitly learning through\nsemantic and boundary information, thus to capture disease-specific spatial\npatterns in an adversarial optimization. The adaptation process needs\ncontinuous monitoring, however, as we cannot assume the presence of\nground-truth masks for the target domain, we propose two new metrics to monitor\nthe adaptation process, and strategies to train the segmentation algorithm in a\nstable fashion. We build upon well-established 2D and 3D architectures and\nperform extensive experiments on three cross-centre brain lesion segmentation\ntasks, involving multicentre public and in-house datasets. We demonstrate that\nrecalibrating the deep networks on a few unlabeled images from the target\ndomain improves the segmentation accuracy significantly.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 13:48:02 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 07:04:55 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2020 12:28:09 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Li", "Hongwei", ""], ["Loehr", "Timo", ""], ["Sekuboyina", "Anjany", ""], ["Zhang", "Jianguo", ""], ["Wiestler", "Benedikt", ""], ["Menze", "Bjoern", ""]]}, {"id": "2001.09322", "submitter": "Jun Li", "authors": "Dengsheng Chen and Jun Li and Zheng Wang and Kai Xu", "title": "Learning Canonical Shape Space for Category-Level 6D Object Pose and\n  Size Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to category-level 6D object pose and size\nestimation. To tackle intra-class shape variations, we learn canonical shape\nspace (CASS), a unified representation for a large variety of instances of a\ncertain object category. In particular, CASS is modeled as the latent space of\na deep generative model of canonical 3D shapes with normalized pose. We train a\nvariational auto-encoder (VAE) for generating 3D point clouds in the canonical\nspace from an RGBD image. The VAE is trained in a cross-category fashion,\nexploiting the publicly available large 3D shape repositories. Since the 3D\npoint cloud is generated in normalized pose (with actual size), the encoder of\nthe VAE learns view-factorized RGBD embedding. It maps an RGBD image in\narbitrary view into a pose-independent 3D shape representation. Object pose is\nthen estimated via contrasting it with a pose-dependent feature of the input\nRGBD extracted with a separate deep neural networks. We integrate the learning\nof CASS and pose and size estimation into an end-to-end trainable network,\nachieving the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 14:16:17 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 08:06:40 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chen", "Dengsheng", ""], ["Li", "Jun", ""], ["Wang", "Zheng", ""], ["Xu", "Kai", ""]]}, {"id": "2001.09371", "submitter": "Nils Gessert", "authors": "Nils Gessert and Alexander Schlaefer", "title": "Learning Preference-Based Similarities from Face Images using Siamese\n  Multi-Task CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online dating has become a common occurrence over the last few decades. A key\nchallenge for online dating platforms is to determine suitable matches for\ntheir users. A lot of dating services rely on self-reported user traits and\npreferences for matching. At the same time, some services largely rely on user\nimages and thus initial visual preference. Especially for the latter approach,\nprevious research has attempted to capture users' visual preferences for\nautomatic match recommendation. These approaches are mostly based on the\nassumption that physical attraction is the key factor for relationship\nformation and personal preferences, interests, and attitude are largely\nneglected. Deep learning approaches have shown that a variety of properties can\nbe predicted from human faces to some degree, including age, health and even\npersonality traits. Therefore, we investigate the feasibility of bridging\nimage-based matching and matching with personal interests, preferences, and\nattitude. We approach the problem in a supervised manner by predicting\nsimilarity scores between two users based on images of their faces only. The\nground-truth for the similarity matching scores is determined by a test that\naims to capture users' preferences, interests, and attitude that are relevant\nfor forming romantic relationships. The images are processed by a Siamese\nMulti-Task deep learning architecture. We find a statistically significant\ncorrelation between predicted and target similarity scores. Thus, our results\nindicate that learning similarities in terms of interests, preferences, and\nattitude from face images appears to be feasible to some degree.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 23:08:12 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Gessert", "Nils", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "2001.09389", "submitter": "Gang Wang", "authors": "Gang Wang", "title": "Scene Text Recognition With Finer Grid Rectification", "comments": "6pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene Text Recognition is a challenging problem because of irregular styles\nand various distortions. This paper proposed an end-to-end trainable model\nconsists of a finer rectification module and a bidirectional attentional\nrecognition network(Firbarn). The rectification module adopts finer grid to\nrectify the distorted input image and the bidirectional decoder contains only\none decoding layer instead of two separated one. Firbarn can be trained in a\nweak supervised way, only requiring the scene text images and the corresponding\nword labels. With the flexible rectification and the novel bidirectional\ndecoder, the results of extensive evaluation on the standard benchmarks show\nFirbarn outperforms previous works, especially on irregular datasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 02:40:11 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Wang", "Gang", ""]]}, {"id": "2001.09414", "submitter": "Zheng Wang", "authors": "Di Hu, Zheng Wang, Haoyi Xiong, Dong Wang, Feiping Nie, Dejing Dou", "title": "Curriculum Audiovisual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associating sound and its producer in complex audiovisual scene is a\nchallenging task, especially when we are lack of annotated training data. In\nthis paper, we present a flexible audiovisual model that introduces a\nsoft-clustering module as the audio and visual content detector, and regards\nthe pervasive property of audiovisual concurrency as the latent supervision for\ninferring the correlation among detected contents. To ease the difficulty of\naudiovisual learning, we propose a novel curriculum learning strategy that\ntrains the model from simple to complex scene. We show that such ordered\nlearning procedure rewards the model the merits of easy training and fast\nconvergence. Meanwhile, our audiovisual model can also provide effective\nunimodal representation and cross-modal alignment performance. We further\ndeploy the well-trained model into practical audiovisual sound localization and\nseparation task. We show that our localization model significantly outperforms\nexisting methods, based on which we show comparable performance in sound\nseparation without referring external visual supervision. Our video demo can be\nfound at https://youtu.be/kuClfGG0cFU.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 07:08:47 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Hu", "Di", ""], ["Wang", "Zheng", ""], ["Xiong", "Haoyi", ""], ["Wang", "Dong", ""], ["Nie", "Feiping", ""], ["Dou", "Dejing", ""]]}, {"id": "2001.09417", "submitter": "Binglin Li", "authors": "Binglin Li, Mohammad Akbari, Jie Liang, Yang Wang", "title": "Deep Learning-based Image Compression with Trellis Coded Quantization", "comments": "Accepted in Data Compression Conference (DCC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently many works attempt to develop image compression models based on deep\nlearning architectures, where the uniform scalar quantizer (SQ) is commonly\napplied to the feature maps between the encoder and decoder. In this paper, we\npropose to incorporate trellis coded quantizer (TCQ) into a deep learning based\nimage compression framework. A soft-to-hard strategy is applied to allow for\nback propagation during training. We develop a simple image compression model\nthat consists of three subnetworks (encoder, decoder and entropy estimation),\nand optimize all of the components in an end-to-end manner. We experiment on\ntwo high resolution image datasets and both show that our model can achieve\nsuperior performance at low bit rates. We also show the comparisons between TCQ\nand SQ based on our proposed baseline model and demonstrate the advantage of\nTCQ.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 08:00:04 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Li", "Binglin", ""], ["Akbari", "Mohammad", ""], ["Liang", "Jie", ""], ["Wang", "Yang", ""]]}, {"id": "2001.09424", "submitter": "Matteo Fraschini", "authors": "Matteo Demuru and Matteo Fraschini", "title": "EEG fingerprinting: subject specific signature based on the aperiodic\n  component of power spectrum", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last few years, there has been growing interest in the effects\ninduced by individual variability on activation patterns and brain\nconnectivity. The practical implications of individual variability is of basic\nrelevance for both group level and subject level studies. The\nElectroencephalogram (EEG), still represents one of the most used recording\ntechniques to investigate a wide range of brain related features. In this work,\nwe aim to estimate the effect of individual variability on a set of very simple\nand easily interpretable features extracted from the EEG power spectra. In\nparticular, in an identification scenario, we investigated how the aperiodic\n(1/f background) component of the EEG power spectra can accurately identify\nsubjects from a large EEG dataset. The results of this study show that the\naperiodic component of the EEG signal is characterized by strong\nsubject-specific properties, that this feature is consistent across different\nexperimental conditions (eyes-open and eyes-closed) and outperforms the\ncanonically-defined frequency bands. These findings suggest that the simple\nfeatures (slope and offset) extracted from the aperiodic component of the EEG\nsignal are sensitive to individual traits and may help to characterize and make\ninferences at single subject level.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 09:04:26 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Demuru", "Matteo", ""], ["Fraschini", "Matteo", ""]]}, {"id": "2001.09425", "submitter": "Caiyi Xu", "authors": "Shengjie Li, Caiyi Xu, Jianping Xing, Yafei Ning, Yonghong Chen", "title": "SDOD:Real-time Segmenting and Detecting 3D Object by Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing instance segmentation methods only focus on improving\nperformance and are not suitable for real-time scenes such as autonomous\ndriving. This paper proposes a real-time framework that segmenting and\ndetecting 3D objects by depth. The framework is composed of two parallel\nbranches: one for instance segmentation and another for object detection. We\ndiscretize the objects' depth into depth categories and transform the instance\nsegmentation task into a pixel-level classification task. The Mask branch\npredicts pixel-level depth categories, and the 3D branch indicates\ninstance-level depth categories. We produce an instance mask by assigning\npixels which have the same depth categories to each instance. In addition, to\nsolve the imbalance between mask labels and 3D labels in the KITTI dataset, we\nintroduce a coarse mask generated by the auto-annotation model to increase\nsamples. Experiments on the challenging KITTI dataset show that our approach\noutperforms LklNet about 1.8 times on the speed of segmentation and 3D\ndetection.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 09:06:18 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 05:12:43 GMT"}, {"version": "v3", "created": "Sat, 24 Oct 2020 08:59:00 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Li", "Shengjie", ""], ["Xu", "Caiyi", ""], ["Xing", "Jianping", ""], ["Ning", "Yafei", ""], ["Chen", "Yonghong", ""]]}, {"id": "2001.09464", "submitter": "Frank Emmert-Streib", "authors": "Frank Emmert-Streib, Olli Yli-Harja, and Matthias Dehmer", "title": "Explainable Artificial Intelligence and Machine Learning: A reality\n  rooted perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are used to the availability of big data generated in nearly all fields of\nscience as a consequence of technological progress. However, the analysis of\nsuch data possess vast challenges. One of these relates to the explainability\nof artificial intelligence (AI) or machine learning methods. Currently, many of\nsuch methods are non-transparent with respect to their working mechanism and\nfor this reason are called black box models, most notably deep learning\nmethods. However, it has been realized that this constitutes severe problems\nfor a number of fields including the health sciences and criminal justice and\narguments have been brought forward in favor of an explainable AI. In this\npaper, we do not assume the usual perspective presenting explainable AI as it\nshould be, but rather we provide a discussion what explainable AI can be. The\ndifference is that we do not present wishful thinking but reality grounded\nproperties in relation to a scientific theory beyond physics.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 15:09:45 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Emmert-Streib", "Frank", ""], ["Yli-Harja", "Olli", ""], ["Dehmer", "Matthias", ""]]}, {"id": "2001.09501", "submitter": "Darvin Yi", "authors": "Darvin Yi, Endre Gr{\\o}vik, Michael Iv, Elizabeth Tong, Greg\n  Zaharchuk, Daniel Rubin", "title": "Brain Metastasis Segmentation Network Trained with Robustness to\n  Annotations with Multiple False Negatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has proven to be an essential tool for medical image analysis.\nHowever, the need for accurately labeled input data, often requiring time- and\nlabor-intensive annotation by experts, is a major limitation to the use of deep\nlearning. One solution to this challenge is to allow for use of coarse or noisy\nlabels, which could permit more efficient and scalable labeling of images. In\nthis work, we develop a lopsided loss function based on entropy regularization\nthat assumes the existence of a nontrivial false negative rate in the target\nannotations. Starting with a carefully annotated brain metastasis lesion\ndataset, we simulate data with false negatives by (1) randomly censoring the\nannotated lesions and (2) systematically censoring the smallest lesions. The\nlatter better models true physician error because smaller lesions are harder to\nnotice than the larger ones. Even with a simulated false negative rate as high\nas 50%, applying our loss function to randomly censored data preserves maximum\nsensitivity at 97% of the baseline with uncensored training data, compared to\njust 10% for a standard loss function. For the size-based censorship,\nperformance is restored from 17% with the current standard to 88% with our\nlopsided bootstrap loss. Our work will enable more efficient scaling of the\nimage labeling process, in parallel with other approaches on creating more\nefficient user interfaces and tools for annotation.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 19:23:07 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Yi", "Darvin", ""], ["Gr\u00f8vik", "Endre", ""], ["Iv", "Michael", ""], ["Tong", "Elizabeth", ""], ["Zaharchuk", "Greg", ""], ["Rubin", "Daniel", ""]]}, {"id": "2001.09518", "submitter": "Aysegul Dundar", "authors": "Aysegul Dundar, Kevin J. Shih, Animesh Garg, Robert Pottorf, Andrew\n  Tao, Bryan Catanzaro", "title": "Unsupervised Disentanglement of Pose, Appearance and Background from\n  Images and Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised landmark learning is the task of learning semantic keypoint-like\nrepresentations without the use of expensive input keypoint-level annotations.\nA popular approach is to factorize an image into a pose and appearance data\nstream, then to reconstruct the image from the factorized components. The pose\nrepresentation should capture a set of consistent and tightly localized\nlandmarks in order to facilitate reconstruction of the input image. Ultimately,\nwe wish for our learned landmarks to focus on the foreground object of\ninterest. However, the reconstruction task of the entire image forces the model\nto allocate landmarks to model the background. This work explores the effects\nof factorizing the reconstruction task into separate foreground and background\nreconstructions, conditioning only the foreground reconstruction on the\nunsupervised landmarks. Our experiments demonstrate that the proposed\nfactorization results in landmarks that are focused on the foreground object of\ninterest. Furthermore, the rendered background quality is also improved, as the\nbackground rendering pipeline no longer requires the ill-suited landmarks to\nmodel its pose and appearance. We demonstrate this improvement in the context\nof the video-prediction task.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 20:59:47 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Dundar", "Aysegul", ""], ["Shih", "Kevin J.", ""], ["Garg", "Animesh", ""], ["Pottorf", "Robert", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2001.09521", "submitter": "Pierre-Henri Conze", "authors": "Pierre-Henri Conze, Ali Emre Kavur, Emilie Cornec-Le Gall, Naciye\n  Sinem Gezer, Yannick Le Meur, M. Alper Selver and Fran\\c{c}ois Rousseau", "title": "Abdominal multi-organ segmentation with cascaded convolutional and\n  adversarial deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective : Abdominal anatomy segmentation is crucial for numerous\napplications from computer-assisted diagnosis to image-guided surgery. In this\ncontext, we address fully-automated multi-organ segmentation from abdominal CT\nand MR images using deep learning. Methods: The proposed model extends standard\nconditional generative adversarial networks. Additionally to the discriminator\nwhich enforces the model to create realistic organ delineations, it embeds\ncascaded partially pre-trained convolutional encoder-decoders as generator.\nEncoder fine-tuning from a large amount of non-medical images alleviates data\nscarcity limitations. The network is trained end-to-end to benefit from\nsimultaneous multi-level segmentation refinements using auto-context. Results :\nEmployed for healthy liver, kidneys and spleen segmentation, our pipeline\nprovides promising results by outperforming state-of-the-art encoder-decoder\nschemes. Followed for the Combined Healthy Abdominal Organ Segmentation (CHAOS)\nchallenge organized in conjunction with the IEEE International Symposium on\nBiomedical Imaging 2019, it gave us the first rank for three competition\ncategories: liver CT, liver MR and multi-organ MR segmentation. Conclusion :\nCombining cascaded convolutional and adversarial networks strengthens the\nability of deep learning pipelines to automatically delineate multiple\nabdominal organs, with good generalization capability. Significance : The\ncomprehensive evaluation provided suggests that better guidance could be\nachieved to help clinicians in abdominal image interpretation and clinical\ndecision making.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 21:28:04 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Conze", "Pierre-Henri", ""], ["Kavur", "Ali Emre", ""], ["Gall", "Emilie Cornec-Le", ""], ["Gezer", "Naciye Sinem", ""], ["Meur", "Yannick Le", ""], ["Selver", "M. Alper", ""], ["Rousseau", "Fran\u00e7ois", ""]]}, {"id": "2001.09526", "submitter": "Weimin Zhou", "authors": "Weimin Zhou, Mark A. Anastasio", "title": "Markov-Chain Monte Carlo Approximation of the Ideal Observer using\n  Generative Adversarial Networks", "comments": "SPIE Medical Imaging 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ideal Observer (IO) performance has been advocated when optimizing\nmedical imaging systems for signal detection tasks. However, analytical\ncomputation of the IO test statistic is generally intractable. To approximate\nthe IO test statistic, sampling-based methods that employ Markov-Chain Monte\nCarlo (MCMC) techniques have been developed. However, current applications of\nMCMC techniques have been limited to several object models such as a lumpy\nobject model and a binary texture model, and it remains unclear how MCMC\nmethods can be implemented with other more sophisticated object models. Deep\nlearning methods that employ generative adversarial networks (GANs) hold great\npromise to learn stochastic object models (SOMs) from image data. In this\nstudy, we described a method to approximate the IO by applying MCMC techniques\nto SOMs learned by use of GANs. The proposed method can be employed with\narbitrary object models that can be learned by use of GANs, thereby the domain\nof applicability of MCMC techniques for approximating the IO performance is\nextended. In this study, both signal-known-exactly (SKE) and\nsignal-known-statistically (SKS) binary signal detection tasks are considered.\nThe IO performance computed by the proposed method is compared to that computed\nby the conventional MCMC method. The advantages of the proposed method are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 21:51:08 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Zhou", "Weimin", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "2001.09528", "submitter": "Alberto Olmo", "authors": "Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda,\n  Subbarao Kambhampati", "title": "Imperfect ImaGANation: Implications of GANs Exacerbating Biases on\n  Facial Data Augmentation and Snapchat Selfie Lenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that popular Generative Adversarial Networks (GANs)\nexacerbate biases along the axes of gender and skin tone when given a skewed\ndistribution of face-shots. While practitioners celebrate synthetic data\ngeneration using GANs as an economical way to augment data for training\ndata-hungry machine learning models, it is unclear whether they recognize the\nperils of such techniques when applied to real world datasets biased along\nlatent dimensions. Specifically, we show that (1) traditional GANs further skew\nthe distribution of a dataset consisting of engineering faculty headshots,\ngenerating minority modes less often and of worse quality and (2)\nimage-to-image translation (conditional) GANs also exacerbate biases by\nlightening skin color of non-white faces and transforming female facial\nfeatures to be masculine when generating faces of engineering professors. Thus,\nour study is meant to serve as a cautionary tale.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 21:57:26 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 23:13:23 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 02:13:46 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Jain", "Niharika", ""], ["Olmo", "Alberto", ""], ["Sengupta", "Sailik", ""], ["Manikonda", "Lydia", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "2001.09531", "submitter": "M\\'elisande Teng", "authors": "Gautier Cosne, Adrien Juraver, M\\'elisande Teng, Victor Schmidt, Vahe\n  Vardanyan, Alexandra Luccioni and Yoshua Bengio", "title": "Using Simulated Data to Generate Images of Climate Change", "comments": "Proceeding ML-IRL workshop at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative adversarial networks (GANs) used in domain adaptation tasks have\nthe ability to generate images that are both realistic and personalized,\ntransforming an input image while maintaining its identifiable characteristics.\nHowever, they often require a large quantity of training data to produce\nhigh-quality images in a robust way, which limits their usability in cases when\naccess to data is limited. In our paper, we explore the potential of using\nimages from a simulated 3D environment to improve a domain adaptation task\ncarried out by the MUNIT architecture, aiming to use the resulting images to\nraise awareness of the potential future impacts of climate change.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 22:19:13 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Cosne", "Gautier", ""], ["Juraver", "Adrien", ""], ["Teng", "M\u00e9lisande", ""], ["Schmidt", "Victor", ""], ["Vardanyan", "Vahe", ""], ["Luccioni", "Alexandra", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2001.09535", "submitter": "Nishant Kumar", "authors": "Nishant Kumar, Nico Hoffmann, Matthias Kirsch and Stefan Gumhold", "title": "Visualisation of Medical Image Fusion and Translation for Accurate\n  Diagnosis of High Grade Gliomas", "comments": "5 pages, 3 figures, IEEE International Symposium on Biomedical\n  Imaging (IEEE ISBI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The medical image fusion combines two or more modalities into a single view\nwhile medical image translation synthesizes new images and assists in data\naugmentation. Together, these methods help in faster diagnosis of high grade\nmalignant gliomas. However, they might be untrustworthy due to which\nneurosurgeons demand a robust visualisation tool to verify the reliability of\nthe fusion and translation results before they make pre-operative surgical\ndecisions. In this paper, we propose a novel approach to compute a confidence\nheat map between the source-target image pair by estimating the information\ntransfer from the source to the target image using the joint probability\ndistribution of the two images. We evaluate several fusion and translation\nmethods using our visualisation procedure and showcase its robustness in\nenabling neurosurgeons to make finer clinical decisions.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 22:49:14 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 14:33:13 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 13:35:39 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Kumar", "Nishant", ""], ["Hoffmann", "Nico", ""], ["Kirsch", "Matthias", ""], ["Gumhold", "Stefan", ""]]}, {"id": "2001.09540", "submitter": "Mennatullah Siam M.S.", "authors": "Mennatullah Siam, Naren Doraiswamy, Boris N. Oreshkin, Hengshuai Yao\n  and Martin Jagersand", "title": "Weakly Supervised Few-shot Object Segmentation using Co-Attention with\n  Visual and Semantic Embeddings", "comments": "Accepted to IJCAI'20. The first three authors listed contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Significant progress has been made recently in developing few-shot object\nsegmentation methods. Learning is shown to be successful in few-shot\nsegmentation settings, using pixel-level, scribbles and bounding box\nsupervision. This paper takes another approach, i.e., only requiring\nimage-level label for few-shot object segmentation. We propose a novel\nmulti-modal interaction module for few-shot object segmentation that utilizes a\nco-attention mechanism using both visual and word embedding. Our model using\nimage-level labels achieves 4.8% improvement over previously proposed\nimage-level few-shot object segmentation. It also outperforms state-of-the-art\nmethods that use weak bounding box supervision on PASCAL-5i. Our results show\nthat few-shot segmentation benefits from utilizing word embeddings, and that we\nare able to perform few-shot segmentation using stacked joint visual semantic\nprocessing with weak image-level labels. We further propose a novel setup,\nTemporal Object Segmentation for Few-shot Learning (TOSFL) for videos. TOSFL\ncan be used on a variety of public video data such as Youtube-VOS, as\ndemonstrated in both instance-level and category-level TOSFL experiments.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 23:56:26 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 23:26:40 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 17:08:51 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Siam", "Mennatullah", ""], ["Doraiswamy", "Naren", ""], ["Oreshkin", "Boris N.", ""], ["Yao", "Hengshuai", ""], ["Jagersand", "Martin", ""]]}, {"id": "2001.09545", "submitter": "Chiranjib Sur", "authors": "Chiranjib Sur", "title": "aiTPR: Attribute Interaction-Tensor Product Representation for Image\n  Caption", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region visual features enhance the generative capability of the machines\nbased on features, however they lack proper interaction attentional perceptions\nand thus ends up with biased or uncorrelated sentences or pieces of\nmisinformation. In this work, we propose Attribute Interaction-Tensor Product\nRepresentation (aiTPR) which is a convenient way of gathering more information\nthrough orthogonal combination and learning the interactions as physical\nentities (tensors) and improving the captions. Compared to previous works,\nwhere features are added up to undefined feature spaces, TPR helps in\nmaintaining sanity in combinations and orthogonality helps in defining familiar\nspaces. We have introduced a new concept layer that defines the objects and\nalso their interactions that can play a crucial role in determination of\ndifferent descriptions. The interaction portions have contributed heavily for\nbetter caption quality and has out-performed different previous works on this\ndomain and MSCOCO dataset. We introduced, for the first time, the notion of\ncombining regional image features and abstracted interaction likelihood\nembedding for image captioning.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 00:19:41 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Sur", "Chiranjib", ""]]}, {"id": "2001.09556", "submitter": "Yao Xue", "authors": "Yao Xue, Siming Liu, Yonghui Li, Xueming Qian", "title": "Crowd Scene Analysis by Output Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd scene analysis receives growing attention due to its wide applications.\nGrasping the accurate crowd location (rather than merely crowd count) is\nimportant for spatially identifying high-risk regions in congested scenes. In\nthis paper, we propose a Compressed Sensing based Output Encoding (CSOE)\nscheme, which casts detecting pixel coordinates of small objects into a task of\nsignal regression in encoding signal space. CSOE helps to boost localization\nperformance in circumstances where targets are highly crowded without huge\nscale variation. In addition, proper receptive field sizes are crucial for\ncrowd analysis due to human size variations. We create Multiple Dilated\nConvolution Branches (MDCB) that offers a set of different receptive field\nsizes, to improve localization accuracy when objects sizes change drastically\nin an image. Also, we develop an Adaptive Receptive Field Weighting (ARFW)\nmodule, which further deals with scale variation issue by adaptively\nemphasizing informative channels that have proper receptive field size.\nExperiments demonstrate the effectiveness of the proposed method, which\nachieves state-of-the-art performance across four mainstream datasets,\nespecially achieves excellent results in highly crowded scenes. More\nimportantly, experiments support our insights that it is crucial to tackle\ntarget size variation issue in crowd analysis task, and casting crowd\nlocalization as regression in encoding signal space is quite effective for\ncrowd analysis.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 01:34:08 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Xue", "Yao", ""], ["Liu", "Siming", ""], ["Li", "Yonghui", ""], ["Qian", "Xueming", ""]]}, {"id": "2001.09578", "submitter": "Andrew Lensen", "authors": "Andrew Lensen, Bing Xue, Mengjie Zhang", "title": "Genetic Programming for Evolving a Front of Interpretable Models for\n  Data Visualisation", "comments": "Accepted by IEEE Transactions on Cybernetics, 2020", "journal-ref": null, "doi": "10.1109/TCYB.2020.2970198", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualisation is a key tool in data mining for understanding big\ndatasets. Many visualisation methods have been proposed, including the\nwell-regarded state-of-the-art method t-Distributed Stochastic Neighbour\nEmbedding. However, the most powerful visualisation methods have a significant\nlimitation: the manner in which they create their visualisation from the\noriginal features of the dataset is completely opaque. Many domains require an\nunderstanding of the data in terms of the original features; there is hence a\nneed for powerful visualisation methods which use understandable models. In\nthis work, we propose a genetic programming approach named GPtSNE for evolving\ninterpretable mappings from a dataset to highquality visualisations. A\nmulti-objective approach is designed that produces a variety of visualisations\nin a single run which give different trade-offs between visual quality and\nmodel complexity. Testing against baseline methods on a variety of datasets\nshows the clear potential of GP-tSNE to allow deeper insight into data than\nthat provided by existing visualisation methods. We further highlight the\nbenefits of a multi-objective approach through an in-depth analysis of a\ncandidate front, which shows how multiple models can\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 04:03:19 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Lensen", "Andrew", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "2001.09598", "submitter": "Yihao Huang", "authors": "Yihao Huang, Felix Juefei-Xu, Run Wang, Qing Guo, Xiaofei Xie, Lei Ma,\n  Jianwen Li, Weikai Miao, Yang Liu, Geguang Pu", "title": "FakeLocator: Robust Localization of GAN-Based Face Manipulations", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, full face synthesis and partial face manipulation by virtue of the\ngenerative adversarial networks (GANs) have raised wide public concerns. In the\nmulti-media forensics area, detecting and ultimately locating the image forgery\nhave become imperative. We investigated the architecture of existing GAN-based\nface manipulation methods and observed that the imperfection of upsampling\nmethods therewithin could be served as an important asset for GAN-synthesized\nfake images detection and forgery localization. Based on this basic\nobservation, we have proposed a novel approach to obtain high localization\naccuracy, at full resolution, on manipulated facial images. To the best of our\nknowledge, this is the very first attempt to solve the GAN-based fake\nlocalization problem with a gray-scale fakeness prediction map that preserves\nmore information of fake regions. To improve the universality of FakeLocator\nacross multifarious facial attributes, we introduce an attention mechanism to\nguide the training of the model. Experimental results on the CelebA and FFHQ\ndatabases with seven different state-of-the-art GAN-based face generation\nmethods show the effectiveness of our method. Compared with the baseline, our\nmethod performs two times better on various metrics. Moreover, the proposed\nmethod is robust against various real-world facial image degradations such as\nJPEG compression, low-resolution, noise, and blur.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 06:15:01 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 10:53:18 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 12:39:14 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Huang", "Yihao", ""], ["Juefei-Xu", "Felix", ""], ["Wang", "Run", ""], ["Guo", "Qing", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Li", "Jianwen", ""], ["Miao", "Weikai", ""], ["Liu", "Yang", ""], ["Pu", "Geguang", ""]]}, {"id": "2001.09610", "submitter": "Ibrahim Yilmaz", "authors": "Ibrahim Yilmaz", "title": "Practical Fast Gradient Sign Attack against Mammographic Image\n  Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) has been a topic of major research for many\nyears. Especially, with the emergence of deep neural network (DNN), these\nstudies have been tremendously successful. Today machines are capable of making\nfaster, more accurate decision than human. Thanks to the great development of\nmachine learning (ML) techniques, ML have been used many different fields such\nas education, medicine, malware detection, autonomous car etc. In spite of\nhaving this degree of interest and much successful research, ML models are\nstill vulnerable to adversarial attacks. Attackers can manipulate clean data in\norder to fool the ML classifiers to achieve their desire target. For instance;\na benign sample can be modified as a malicious sample or a malicious one can be\naltered as benign while this modification can not be recognized by human\nobserver. This can lead to many financial losses, or serious injuries, even\ndeaths. The motivation behind this paper is that we emphasize this issue and\nwant to raise awareness. Therefore, the security gap of mammographic image\nclassifier against adversarial attack is demonstrated. We use mamographic\nimages to train our model then evaluate our model performance in terms of\naccuracy. Later on, we poison original dataset and generate adversarial samples\nthat missclassified by the model. We then using structural similarity index\n(SSIM) analyze similarity between clean images and adversarial images. Finally,\nwe show how successful we are to misuse by using different poisoning factors.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 07:37:07 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Yilmaz", "Ibrahim", ""]]}, {"id": "2001.09614", "submitter": "Haifeng Li", "authors": "Jie Chen, Haozhe Huang, Jian Peng, Jiawei Zhu, Li Chen, Wenbo Li,\n  Binyu Sun, Haifeng Li", "title": "Convolution Neural Network Architecture Learning for Remote Sensing\n  Scene Classification", "comments": "10 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing image scene classification is a fundamental but challenging\ntask in understanding remote sensing images. Recently, deep learning-based\nmethods, especially convolutional neural network-based (CNN-based) methods have\nshown enormous potential to understand remote sensing images. CNN-based methods\nmeet with success by utilizing features learned from data rather than features\ndesigned manually. The feature-learning procedure of CNN largely depends on the\narchitecture of CNN. However, most of the architectures of CNN used for remote\nsensing scene classification are still designed by hand which demands a\nconsiderable amount of architecture engineering skills and domain knowledge,\nand it may not play CNN's maximum potential on a special dataset. In this\npaper, we proposed an automatically architecture learning procedure for remote\nsensing scene classification. We designed a parameters space in which every set\nof parameters represents a certain architecture of CNN (i.e., some parameters\nrepresent the type of operators used in the architecture such as convolution,\npooling, no connection or identity, and the others represent the way how these\noperators connect). To discover the optimal set of parameters for a given\ndataset, we introduced a learning strategy which can allow efficient search in\nthe architecture space by means of gradient descent. An architecture generator\nfinally maps the set of parameters into the CNN used in our experiments.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 07:42:46 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Chen", "Jie", ""], ["Huang", "Haozhe", ""], ["Peng", "Jian", ""], ["Zhu", "Jiawei", ""], ["Chen", "Li", ""], ["Li", "Wenbo", ""], ["Sun", "Binyu", ""], ["Li", "Haifeng", ""]]}, {"id": "2001.09650", "submitter": "Oshri Halimi", "authors": "Oshri Halimi, Ido Imanuel, Or Litany, Giovanni Trappolini, Emanuele\n  Rodol\\`a, Leonidas Guibas, Ron Kimmel", "title": "The Whole Is Greater Than the Sum of Its Nonrigid Parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to Aristotle, a philosopher in Ancient Greece, \"the whole is\ngreater than the sum of its parts\". This observation was adopted to explain\nhuman perception by the Gestalt psychology school of thought in the twentieth\ncentury. Here, we claim that observing part of an object which was previously\nacquired as a whole, one could deal with both partial matching and shape\ncompletion in a holistic manner. More specifically, given the geometry of a\nfull, articulated object in a given pose, as well as a partial scan of the same\nobject in a different pose, we address the problem of matching the part to the\nwhole while simultaneously reconstructing the new pose from its partial\nobservation. Our approach is data-driven, and takes the form of a Siamese\nautoencoder without the requirement of a consistent vertex labeling at\ninference time; as such, it can be used on unorganized point clouds as well as\non triangle meshes. We demonstrate the practical effectiveness of our model in\nthe applications of single-view deformable shape completion and dense shape\ncorrespondence, both on synthetic and real-world geometric data, where we\noutperform prior work on these tasks by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 09:48:01 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Halimi", "Oshri", ""], ["Imanuel", "Ido", ""], ["Litany", "Or", ""], ["Trappolini", "Giovanni", ""], ["Rodol\u00e0", "Emanuele", ""], ["Guibas", "Leonidas", ""], ["Kimmel", "Ron", ""]]}, {"id": "2001.09671", "submitter": "Sadaf Gulshad", "authors": "Sadaf Gulshad and Arnold Smeulders", "title": "Explaining with Counter Visual Attributes and Examples", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.07416,\n  arXiv:1904.08279", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to explain the decisions of neural networks by\nutilizing multimodal information. That is counter-intuitive attributes and\ncounter visual examples which appear when perturbed samples are introduced.\nDifferent from previous work on interpreting decisions using saliency maps,\ntext, or visual patches we propose to use attributes and counter-attributes,\nand examples and counter-examples as part of the visual explanations. When\nhumans explain visual decisions they tend to do so by providing attributes and\nexamples. Hence, inspired by the way of human explanations in this paper we\nprovide attribute-based and example-based explanations. Moreover, humans also\ntend to explain their visual decisions by adding counter-attributes and\ncounter-examples to explain what is not seen. We introduce directed\nperturbations in the examples to observe which attribute values change when\nclassifying the examples into the counter classes. This delivers intuitive\ncounter-attributes and counter-examples. Our experiments with both coarse and\nfine-grained datasets show that attributes provide discriminating and\nhuman-understandable intuitive and counter-intuitive explanations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 10:28:47 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Gulshad", "Sadaf", ""], ["Smeulders", "Arnold", ""]]}, {"id": "2001.09678", "submitter": "Zhao Sun", "authors": "Qiwei Xie, Qian Long, Liming Zhang, Zhao Sun", "title": "A Robust Real-Time Computing-based Environment Sensing System for\n  Intelligent Vehicle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For intelligent vehicles, sensing the 3D environment is the first but crucial\nstep. In this paper, we build a real-time advanced driver assistance system\nbased on a low-power mobile platform. The system is a real-time multi-scheme\nintegrated innovation system, which combines stereo matching algorithm with\nmachine learning based obstacle detection approach and takes advantage of the\ndistributed computing technology of a mobile platform with GPU and CPUs. First\nof all, a multi-scale fast MPV (Multi-Path-Viterbi) stereo matching algorithm\nis proposed, which can generate robust and accurate disparity map. Then a\nmachine learning, which is based on fusion technology of monocular and\nbinocular, is applied to detect the obstacles. We also advance an automatic\nfast calibration mechanism based on Zhang's calibration method. Finally, the\ndistributed computing and reasonable data flow programming are applied to\nensure the operational efficiency of the system. The experimental results show\nthat the system can achieve robust and accurate real-time environment\nperception for intelligent vehicles, which can be directly used in the\ncommercial real-time intelligent driving applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 10:47:50 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Xie", "Qiwei", ""], ["Long", "Qian", ""], ["Zhang", "Liming", ""], ["Sun", "Zhao", ""]]}, {"id": "2001.09691", "submitter": "Jonathan Munro", "authors": "Jonathan Munro and Dima Damen", "title": "Multi-Modal Domain Adaptation for Fine-Grained Action Recognition", "comments": "Accepted to CVPR 2020 for an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained action recognition datasets exhibit environmental bias, where\nmultiple video sequences are captured from a limited number of environments.\nTraining a model in one environment and deploying in another results in a drop\nin performance due to an unavoidable domain shift. Unsupervised Domain\nAdaptation (UDA) approaches have frequently utilised adversarial training\nbetween the source and target domains. However, these approaches have not\nexplored the multi-modal nature of video within each domain. In this work we\nexploit the correspondence of modalities as a self-supervised alignment\napproach for UDA in addition to adversarial alignment.\n  We test our approach on three kitchens from our large-scale dataset,\nEPIC-Kitchens, using two modalities commonly employed for action recognition:\nRGB and Optical Flow. We show that multi-modal self-supervision alone improves\nthe performance over source-only training by 2.4% on average. We then combine\nadversarial training with multi-modal self-supervision, showing that our\napproach outperforms other UDA methods by 3%.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 11:06:06 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 16:16:01 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Munro", "Jonathan", ""], ["Damen", "Dima", ""]]}, {"id": "2001.09703", "submitter": "Varsha Balakrishnan", "authors": "Varsha Balakrishnan", "title": "Unconstrained Biometric Recognition: Summary of Recent SOCIA Lab.\n  Research", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The development of biometric recognition solutions able to work in visual\nsurveillance conditions, i.e., in unconstrained data acquisition conditions and\nunder covert protocols has been motivating growing efforts from the research\ncommunity. Among the various laboratories, schools and research institutes\nconcerned about this problem, the SOCIA: Soft Computing and Image Analysis\nLab., of the University of Beira Interior, Portugal, has been among the most\nactive in pursuing disruptive solutions for obtaining such extremely ambitious\nkind of automata. This report summarises the research works published by\nelements of the SOCIA Lab. in the last decade in the scope of biometric\nrecognition in unconstrained conditions. The idea is that it can be used as\nbasis for someone wishing to entering in this research topic.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 11:38:16 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 10:08:48 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Balakrishnan", "Varsha", ""]]}, {"id": "2001.09730", "submitter": "Si Miao", "authors": "Si Miao, Yongxin Zhu", "title": "Handling noise in image deblurring via joint learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, many blind deblurring methods assume blurred images are noise-free\nand perform unsatisfactorily on the blurry images with noise. Unfortunately,\nnoise is quite common in real scenes. A straightforward solution is to denoise\nimages before deblurring them. However, even state-of-the-art denoisers cannot\nguarantee to remove noise entirely. Slight residual noise in the denoised\nimages could cause significant artifacts in the deblurring stage. To tackle\nthis problem, we propose a cascaded framework consisting of a denoiser\nsubnetwork and a deblurring subnetwork. In contrast to previous methods, we\ntrain the two subnetworks jointly. Joint learning reduces the effect of the\nresidual noise after denoising on deblurring, hence improves the robustness of\ndeblurring to heavy noise. Moreover, our method is also helpful for blur kernel\nestimation. Experiments on the CelebA dataset and the GOPRO dataset show that\nour method performs favorably against several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 12:59:52 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Miao", "Si", ""], ["Zhu", "Yongxin", ""]]}, {"id": "2001.09865", "submitter": "Nilanjan Ray", "authors": "Abhishek Nan, Matthew Tennant, Uriel Rubin and Nilanjan Ray", "title": "DRMIME: Differentiable Mutual Information and Matrix Exponential for\n  Multi-Resolution Image Registration", "comments": "Software: https://github.com/abnan/DRMIME", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we present a novel unsupervised image registration algorithm.\nIt is differentiable end-to-end and can be used for both multi-modal and\nmono-modal registration. This is done using mutual information (MI) as a\nmetric. The novelty here is that rather than using traditional ways of\napproximating MI, we use a neural estimator called MINE and supplement it with\nmatrix exponential for transformation matrix computation. This leads to\nimproved results as compared to the standard algorithms available\nout-of-the-box in state-of-the-art image registration toolboxes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 15:38:46 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Nan", "Abhishek", ""], ["Tennant", "Matthew", ""], ["Rubin", "Uriel", ""], ["Ray", "Nilanjan", ""]]}, {"id": "2001.09908", "submitter": "Chang Ye", "authors": "Chang Ye, Ahmed Khalifa, Philip Bontrager, Julian Togelius", "title": "Rotation, Translation, and Cropping for Zero-Shot Generalization", "comments": "IEEE Conference on Games 2020 Full Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) has shown impressive performance on domains\nwith visual inputs, in particular various games. However, the agent is usually\ntrained on a fixed environment, e.g. a fixed number of levels. A growing mass\nof evidence suggests that these trained models fail to generalize to even\nslight variations of the environments they were trained on. This paper advances\nthe hypothesis that the lack of generalization is partly due to the input\nrepresentation, and explores how rotation, cropping and translation could\nincrease generality. We show that a cropped, translated and rotated observation\ncan get better generalization on unseen levels of two-dimensional arcade games\nfrom the GVGAI framework. The generality of the agents is evaluated on both\nhuman-designed and procedurally generated levels.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 16:56:05 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 18:42:57 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 03:18:52 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Ye", "Chang", ""], ["Khalifa", "Ahmed", ""], ["Bontrager", "Philip", ""], ["Togelius", "Julian", ""]]}, {"id": "2001.09912", "submitter": "Sudhakar Kumawat", "authors": "Sudhakar Kumawat, Shanmuganathan Raman", "title": "Depthwise-STFT based separable Convolutional Neural Networks", "comments": "Accepted at ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new convolutional layer called Depthwise-STFT\nSeparable layer that can serve as an alternative to the standard depthwise\nseparable convolutional layer. The construction of the proposed layer is\ninspired by the fact that the Fourier coefficients can accurately represent\nimportant features such as edges in an image. It utilizes the Fourier\ncoefficients computed (channelwise) in the 2D local neighborhood (e.g., 3x3) of\neach position of the input map to obtain the feature maps. The Fourier\ncoefficients are computed using 2D Short Term Fourier Transform (STFT) at\nmultiple fixed low frequency points in the 2D local neighborhood at each\nposition. These feature maps at different frequency points are then linearly\ncombined using trainable pointwise (1x1) convolutions. We show that the\nproposed layer outperforms the standard depthwise separable layer-based models\non the CIFAR-10 and CIFAR-100 image classification datasets with reduced\nspace-time complexity.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 17:07:08 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Kumawat", "Sudhakar", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2001.09947", "submitter": "Sheela Ramanna", "authors": "Sheela Ramanna and Cenker Sengoz and Scott Kehler and Dat Pham", "title": "Near real-time map building with multi-class image set labelling and\n  classification of road conditions using convolutional neural networks", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weather is an important factor affecting transportation and road safety. In\nthis paper, we leverage state-of-the-art convolutional neural networks in\nlabelling images taken by street and highway cameras located across across\nNorth America. Road camera snapshots were used in experiments with multiple\ndeep learning frameworks to classify images by road condition. The training\ndata for these experiments used images labelled as dry, wet, snow/ice, poor,\nand offline. The experiments tested different configurations of six\nconvolutional neural networks (VGG-16, ResNet50, Xception, InceptionResNetV2,\nEfficientNet-B0 and EfficientNet-B4) to assess their suitability to this\nproblem. The precision, accuracy, and recall were measured for each framework\nconfiguration. In addition, the training sets were varied both in overall size\nand by size of individual classes. The final training set included 47,000\nimages labelled using the five aforementioned classes. The EfficientNet-B4\nframework was found to be most suitable to this problem, achieving validation\naccuracy of 90.6%, although EfficientNet-B0 achieved an accuracy of 90.3% with\nhalf the execution time. It was observed that VGG-16 with transfer learning\nproved to be very useful for data acquisition and pseudo-labelling with limited\nhardware resources, throughout this project. The EfficientNet-B4 framework was\nthen placed into a real-time production environment, where images could be\nclassified in real-time on an ongoing basis. The classified images were then\nused to construct a map showing real-time road conditions at various camera\nlocations across North America. The choice of these frameworks and our analysis\ntake into account unique requirements of real-time map building functions. A\ndetailed analysis of the process of semi-automated dataset labelling using\nthese frameworks is also presented in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 18:07:40 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Ramanna", "Sheela", ""], ["Sengoz", "Cenker", ""], ["Kehler", "Scott", ""], ["Pham", "Dat", ""]]}, {"id": "2001.10061", "submitter": "Micha{\\l} Byra", "authors": "Michal Byra, Piotr Jarosik, Katarzyna Dobruch-Sobczak, Ziemowit\n  Klimonda, Hanna Piotrzkowska-Wroblewska, Jerzy Litniewski, Andrzej Nowicki", "title": "Breast mass segmentation based on ultrasonic entropy maps and attention\n  gated U-Net", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep learning based approach to breast mass segmentation\nin ultrasound (US) imaging. In comparison to commonly applied segmentation\nmethods, which use US images, our approach is based on quantitative entropy\nparametric maps. To segment the breast masses we utilized an attention gated\nU-Net convolutional neural network. US images and entropy maps were generated\nbased on raw US signals collected from 269 breast masses. The segmentation\nnetworks were developed separately using US image and entropy maps, and\nevaluated on a test set of 81 breast masses. The attention U-Net trained based\non entropy maps achieved average Dice score of 0.60 (median 0.71), while for\nthe model trained using US images we obtained average Dice score of 0.53\n(median 0.59). Our work presents the feasibility of using quantitative US\nparametric maps for the breast mass segmentation. The obtained results suggest\nthat US parametric maps, which provide the information about local tissue\nscattering properties, might be more suitable for the development of breast\nmass segmentation methods than regular US images.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 20:17:10 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Byra", "Michal", ""], ["Jarosik", "Piotr", ""], ["Dobruch-Sobczak", "Katarzyna", ""], ["Klimonda", "Ziemowit", ""], ["Piotrzkowska-Wroblewska", "Hanna", ""], ["Litniewski", "Jerzy", ""], ["Nowicki", "Andrzej", ""]]}, {"id": "2001.10063", "submitter": "Caio Cesar Viana Da Silva", "authors": "Caio C. V. da Silva, Keiller Nogueira, Hugo N. Oliveira, Jefersson A.\n  dos Santos", "title": "Towards Open-Set Semantic Segmentation of Aerial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical and more recently deep computer vision methods are optimized for\nvisible spectrum images, commonly encoded in grayscale or RGB colorspaces\nacquired from smartphones or cameras. A more uncommon source of images\nexploited in the remote sensing field are satellite and aerial images. However,\nthe development of pattern recognition approaches for these data is relatively\nrecent, mainly due to the limited availability of this type of images, as until\nrecently they were used exclusively for military purposes. Access to aerial\nimagery, including spectral information, has been increasing mainly due to the\nlow cost of drones, cheapening of imaging satellite launch costs, and novel\npublic datasets. Usually remote sensing applications employ computer vision\ntechniques strictly modeled for classification tasks in closed set scenarios.\nHowever, real-world tasks rarely fit into closed set contexts, frequently\npresenting previously unknown classes, characterizing them as open set\nscenarios. Focusing on this problem, this is the first paper to study and\ndevelop semantic segmentation techniques for open set scenarios applied to\nremote sensing images. The main contributions of this paper are: 1) a\ndiscussion of related works in open set semantic segmentation, showing evidence\nthat these techniques can be adapted for open set remote sensing tasks; 2) the\ndevelopment and evaluation of a novel approach for open set semantic\nsegmentation. Our method yielded competitive results when compared to closed\nset methods for the same dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 20:19:57 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["da Silva", "Caio C. V.", ""], ["Nogueira", "Keiller", ""], ["Oliveira", "Hugo N.", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "2001.10072", "submitter": "Lance Rice", "authors": "Lance Rice, Samual Tate, David Farynyk, Joshua Sun, Greg Chism, Daniel\n  Charbonneau, Thomas Fasciano, Anna Dornhaus, and Min C. Shin", "title": "ABCTracker: an easy-to-use, cloud-based application for tracking\n  multiple objects", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual multi-object tracking has the potential to accelerate many forms of\nquantitative analyses, especially in research communities investigating the\nmotion, behavior, or social interactions within groups of animals. Despite its\npotential for increasing analysis throughput, complications related to\naccessibility, adaptability, accuracy, or scalable application arise with\nexisting tracking systems. Several iterations of prototyping and testing have\nled us to a multi-object tracking system -- ABCTracker -- that is: accessible\nin both system as well as technical knowledge requirements, easily adaptable to\nnew videos, and capable of producing accurate tracking data through a mixture\nof automatic and semi-automatic tracking features.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 20:39:56 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 14:51:56 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Rice", "Lance", ""], ["Tate", "Samual", ""], ["Farynyk", "David", ""], ["Sun", "Joshua", ""], ["Chism", "Greg", ""], ["Charbonneau", "Daniel", ""], ["Fasciano", "Thomas", ""], ["Dornhaus", "Anna", ""], ["Shin", "Min C.", ""]]}, {"id": "2001.10090", "submitter": "Chaoyang Wang", "authors": "Chaoyang Wang and Chen-Hsuan Lin and Simon Lucey", "title": "Deep NRSfM++: Towards Unsupervised 2D-3D Lifting in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recovery of 3D shape and pose from 2D landmarks stemming from a large\nensemble of images can be viewed as a non-rigid structure from motion (NRSfM)\nproblem. Classical NRSfM approaches, however, are problematic as they rely on\nheuristic priors on the 3D structure (e.g. low rank) that do not scale well to\nlarge datasets. Learning-based methods are showing the potential to reconstruct\na much broader set of 3D structures than classical methods -- dramatically\nexpanding the importance of NRSfM to atemporal unsupervised 2D to 3D lifting.\nHitherto, these learning approaches have not been able to effectively model\nperspective cameras or handle missing/occluded points -- limiting their\napplicability to in-the-wild datasets. In this paper, we present a generalized\nstrategy for improving learning-based NRSfM methods to tackle the above issues.\nOur approach, Deep NRSfM++, achieves state-of-the-art performance across\nnumerous large-scale benchmarks, outperforming both classical and\nlearning-based 2D-3D lifting methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 21:14:07 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 02:18:27 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Chaoyang", ""], ["Lin", "Chen-Hsuan", ""], ["Lucey", "Simon", ""]]}, {"id": "2001.10111", "submitter": "Augusto Valente", "authors": "Augusto C. Valente, Cristina Wada, Deangela Neves, Deangeli Neves,\n  F\\'abio V. M. Perez, Guilherme A. S. Megeto, Marcos H. Cascone, Otavio Gomes,\n  Qian Lin", "title": "Print Defect Mapping with Semantic Segmentation", "comments": "Accepted in WACV 2020. 8 pages + references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient automated print defect mapping is valuable to the printing industry\nsince such defects directly influence customer-perceived printer quality and\nmanually mapping them is cost-ineffective. Conventional methods consist of\ncomplicated and hand-crafted feature engineering techniques, usually targeting\nonly one type of defect. In this paper, we propose the first end-to-end\nframework to map print defects at pixel level, adopting an approach based on\nsemantic segmentation. Our framework uses Convolutional Neural Networks,\nspecifically DeepLab-v3+, and achieves promising results in the identification\nof defects in printed images. We use synthetic training data by simulating two\ntypes of print defects and a print-scan effect with image processing and\ncomputer graphic techniques. Compared with conventional methods, our framework\nis versatile, allowing two inference strategies, one being near real-time and\nproviding coarser results, and the other focusing on offline processing with\nmore fine-grained detection. Our model is evaluated on a dataset of real\nprinted images.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 22:40:09 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Valente", "Augusto C.", ""], ["Wada", "Cristina", ""], ["Neves", "Deangela", ""], ["Neves", "Deangeli", ""], ["Perez", "F\u00e1bio V. M.", ""], ["Megeto", "Guilherme A. S.", ""], ["Cascone", "Marcos H.", ""], ["Gomes", "Otavio", ""], ["Lin", "Qian", ""]]}, {"id": "2001.10117", "submitter": "Matthew Pitropov", "authors": "Matthew Pitropov, Danson Garcia, Jason Rebello, Michael Smart, Carlos\n  Wang, Krzysztof Czarnecki, Steven Waslander", "title": "Canadian Adverse Driving Conditions Dataset", "comments": null, "journal-ref": null, "doi": "10.1177/0278364920979368", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Canadian Adverse Driving Conditions (CADC) dataset was collected with the\nAutonomoose autonomous vehicle platform, based on a modified Lincoln MKZ. The\ndataset, collected during winter within the Region of Waterloo, Canada, is the\nfirst autonomous vehicle dataset that focuses on adverse driving conditions\nspecifically. It contains 7,000 frames collected through a variety of winter\nweather conditions of annotated data from 8 cameras (Ximea MQ013CG-E2), Lidar\n(VLP-32C) and a GNSS+INS system (Novatel OEM638). The sensors are time\nsynchronized and calibrated with the intrinsic and extrinsic calibrations\nincluded in the dataset. Lidar frame annotations that represent ground truth\nfor 3D object detection and tracking have been provided by Scale AI.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 23:21:38 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 15:24:01 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 17:23:40 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Pitropov", "Matthew", ""], ["Garcia", "Danson", ""], ["Rebello", "Jason", ""], ["Smart", "Michael", ""], ["Wang", "Carlos", ""], ["Czarnecki", "Krzysztof", ""], ["Waslander", "Steven", ""]]}, {"id": "2001.10155", "submitter": "Junyu Chen", "authors": "Junyu Chen, Eric C. Frey", "title": "Medical Image Segmentation via Unsupervised Convolutional Neural Network", "comments": "In Medical Imaging with Deep Learning (2020)", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/XrbnSCv4LU", "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the majority of the learning-based segmentation methods, a large quantity\nof high-quality training data is required. In this paper, we present a novel\nlearning-based segmentation model that could be trained semi- or un-\nsupervised. Specifically, in the unsupervised setting, we parameterize the\nActive contour without edges (ACWE) framework via a convolutional neural\nnetwork (ConvNet), and optimize the parameters of the ConvNet using a\nself-supervised method. In another setting (semi-supervised), the auxiliary\nsegmentation ground truth is used during training. We show that the method\nprovides fast and high-quality bone segmentation in the context of\nsingle-photon emission computed tomography (SPECT) image.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 03:56:42 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 19:29:41 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 03:35:21 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 16:09:09 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Chen", "Junyu", ""], ["Frey", "Eric C.", ""]]}, {"id": "2001.10159", "submitter": "Junwen Luo", "authors": "Junwen Luo and Jiaoyan Chen", "title": "An Internal Clock Based Space-time Neural Network for Motion Speed\n  Recognition", "comments": "To appear in Neuro-inspired Computational Elements Workshop (NICE\n  20). March 26-28,2020 Heidelberg, Germany, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we present a novel internal clock based space-time neural\nnetwork for motion speed recognition. The developed system has a spike train\nencoder, a Spiking Neural Network (SNN) with internal clocking behaviors, a\npattern transformation block and a Network Dynamic Dependent Plasticity (NDDP)\nlearning block. The core principle is that the developed SNN will automatically\ntune its network pattern frequency (internal clock frequency) to recognize\nhuman motions in a speed domain. We employed both cartoons and real-world\nvideos as training benchmarks, results demonstrate that our system can not only\nrecognize motions with considerable speed differences (e.g. run, walk, jump,\nwonder(think) and standstill), but also motions with subtle speed gaps such as\nrun and fast walk. The inference accuracy can be up to 83.3% (cartoon videos)\nand 75% (real-world videos). Meanwhile, the system only requires six video\ndatasets in the learning stage and with up to 42 training trials. Hardware\nperformance estimation indicates that the training time is 0.84-4.35s and power\nconsumption is 33.26-201mW (based on an ARM Cortex M4 processor). Therefore,\nour system takes unique learning advantages of the requirement of the small\ndataset, quick learning and low power performance, which shows great potentials\nfor edge or scalable AI-based applications.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 04:07:33 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Luo", "Junwen", ""], ["Chen", "Jiaoyan", ""]]}, {"id": "2001.10188", "submitter": "Muhammad Shahzad", "authors": "Muhammad Shahzad, Arif Iqbal Umar, Muazzam A. Khan, Syed Hamad\n  Shirazi, Zakir Khan, and Waqas Yousaf", "title": "Robust Method for Semantic Segmentation of Whole-Slide Blood Cell\n  Microscopic Image", "comments": "13 pages, 13 figures", "journal-ref": "Volume 2020, Article ID 4015323, 13 pages", "doi": "10.1155/2020/4015323", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous works on segmentation of SEM (scanning electron microscope) blood\ncell image ignore the semantic segmentation approach of whole-slide blood cell\nsegmentation. In the proposed work, we address the problem of whole-slide blood\ncell segmentation using the semantic segmentation approach. We design a novel\nconvolutional encoder-decoder framework along with VGG-16 as the pixel-level\nfeature extraction model. -e proposed framework comprises 3 main steps: First,\nall the original images along with manually generated ground truth masks of\neach blood cell type are passed through the preprocessing stage. In the\npreprocessing stage, pixel-level labeling, RGB to grayscale conversion of\nmasked image and pixel fusing, and unity mask generation are performed. After\nthat, VGG16 is loaded into the system, which acts as a pretrained pixel-level\nfeature extraction model. In the third step, the training process is initiated\non the proposed model. We have evaluated our network performance on three\nevaluation metrics. We obtained outstanding results with respect to classwise,\nas well as global and mean accuracies. Our system achieved classwise accuracies\nof 97.45%, 93.34%, and 85.11% for RBCs, WBCs, and platelets, respectively,\nwhile global and mean accuracies remain 97.18% and 91.96%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 06:32:21 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Shahzad", "Muhammad", ""], ["Umar", "Arif Iqbal", ""], ["Khan", "Muazzam A.", ""], ["Shirazi", "Syed Hamad", ""], ["Khan", "Zakir", ""], ["Yousaf", "Waqas", ""]]}, {"id": "2001.10220", "submitter": "Ozan \\c{C}atal", "authors": "Ozan \\c{C}atal, Lawrence De Mol, Tim Verbelen and Bart Dhoedt", "title": "Learning to Catch Piglets in Flight", "comments": "Fast Neural Perception and Learning for Intelligent Vehicles and\n  Robotics workshop at IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Catching objects in-flight is an outstanding challenge in robotics. In this\npaper, we present a closed-loop control system fusing data from two sensor\nmodalities: an RGB-D camera and a radar. To develop and test our method, we\nstart with an easy to identify object: a stuffed Piglet. We implement and\ncompare two approaches to detect and track the object, and to predict the\ninterception point. A baseline model uses colour filtering for locating the\nthrown object in the environment, while the interception point is predicted\nusing a least squares regression over the physical ballistic trajectory\nequations. A deep learning based method uses artificial neural networks for\nboth object detection and interception point prediction. We show that we are\nable to successfully catch Piglet in 80% of the cases with our deep learning\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 09:13:17 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["\u00c7atal", "Ozan", ""], ["De Mol", "Lawrence", ""], ["Verbelen", "Tim", ""], ["Dhoedt", "Bart", ""]]}, {"id": "2001.10223", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Javier\n  Ortega-Garcia", "title": "BioTouchPass2: Touchscreen Password Biometrics Using Time-Aligned\n  Recurrent Neural Networks", "comments": null, "journal-ref": "IEEE Transactions on Information Forensics and Security, 2020", "doi": "10.1109/TIFS.2020.2973832", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passwords are still used on a daily basis for all kind of applications.\nHowever, they are not secure enough by themselves in many cases. This work\nenhances password scenarios through two-factor authentication asking the users\nto draw each character of the password instead of typing them as usual. The\nmain contributions of this study are as follows: i) We present the novel\nMobileTouchDB public database, acquired in an unsupervised mobile scenario with\nno restrictions in terms of position, posture, and devices. This database\ncontains more than 64K on-line character samples performed by 217 users, with\n94 different smartphone models, and up to 6 acquisition sessions. ii) We\nperform a complete analysis of the proposed approach considering both\ntraditional authentication systems such as Dynamic Time Warping (DTW) and novel\napproaches based on Recurrent Neural Networks (RNNs). In addition, we present a\nnovel approach named Time-Aligned Recurrent Neural Networks (TA-RNNs). This\napproach combines the potential of DTW and RNNs to train more robust systems\nagainst attacks.\n  A complete analysis of the proposed approach is carried out using both\nMobileTouchDB and e-BioDigitDB databases. Our proposed TA-RNN system\noutperforms the state of the art, achieving a final 2.38% Equal Error Rate,\nusing just a 4-digit password and one training sample per character. These\nresults encourage the deployment of our proposed approach in comparison with\ntraditional typed-based password systems where the attack would have 100%\nsuccess rate under the same impostor scenario.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 09:25:06 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""], ["Ortega-Garcia", "Javier", ""]]}, {"id": "2001.10238", "submitter": "Antoine Plumerault", "authors": "Antoine Plumerault, Herv\\'e Le Borgne, C\\'eline Hudelot", "title": "Controlling generative models with continuous factors of variations", "comments": "Accepted as a poster presentation at the International Conference for\n  Learning Representations (ICLR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep generative models are able to provide photo-realistic images as\nwell as visual or textual content embeddings useful to address various tasks of\ncomputer vision and natural language processing. Their usefulness is\nnevertheless often limited by the lack of control over the generative process\nor the poor understanding of the learned representation. To overcome these\nmajor issues, very recent work has shown the interest of studying the semantics\nof the latent space of generative models. In this paper, we propose to advance\non the interpretability of the latent space of generative models by introducing\na new method to find meaningful directions in the latent space of any\ngenerative model along which we can move to control precisely specific\nproperties of the generated image like the position or scale of the object in\nthe image. Our method does not require human annotations and is particularly\nwell suited for the search of directions encoding simple transformations of the\ngenerated image, such as translation, zoom or color variations. We demonstrate\nthe effectiveness of our method qualitatively and quantitatively, both for GANs\nand variational auto-encoders.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 10:04:04 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Plumerault", "Antoine", ""], ["Borgne", "Herv\u00e9 Le", ""], ["Hudelot", "C\u00e9line", ""]]}, {"id": "2001.10291", "submitter": "Meng Chang", "authors": "Meng Chang, Qi Li, Huajun Feng, Zhihai Xu", "title": "Spatial-Adaptive Network for Single Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works have shown that convolutional neural networks can achieve good\nperformance in image denoising tasks. However, limited by the local rigid\nconvolutional operation, these methods lead to oversmoothing artifacts. A\ndeeper network structure could alleviate these problems, but more computational\noverhead is needed. In this paper, we propose a novel spatial-adaptive\ndenoising network (SADNet) for efficient single image blind noise removal. To\nadapt to changes in spatial textures and edges, we design a residual\nspatial-adaptive block. Deformable convolution is introduced to sample the\nspatially correlated features for weighting. An encoder-decoder structure with\na context block is introduced to capture multiscale information. With noise\nremoval from the coarse to fine, a high-quality noisefree image can be\nobtained. We apply our method to both synthetic and real noisy image datasets.\nThe experimental results demonstrate that our method can surpass the\nstate-of-the-art denoising methods both quantitatively and visually.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 12:24:17 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 02:30:55 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Chang", "Meng", ""], ["Li", "Qi", ""], ["Feng", "Huajun", ""], ["Xu", "Zhihai", ""]]}, {"id": "2001.10331", "submitter": "Ilia Petrov", "authors": "Konstantin Sofiiuk, Ilia Petrov, Olga Barinova and Anton Konushin", "title": "f-BRS: Rethinking Backpropagating Refinement for Interactive\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become a mainstream approach to interactive\nsegmentation. As we show in our experiments, while for some images a trained\nnetwork provides accurate segmentation result with just a few clicks, for some\nunknown objects it cannot achieve satisfactory result even with a large amount\nof user input. Recently proposed backpropagating refinement (BRS) scheme\nintroduces an optimization problem for interactive segmentation that results in\nsignificantly better performance for the hard cases. At the same time, BRS\nrequires running forward and backward pass through a deep network several times\nthat leads to significantly increased computational budget per click compared\nto other methods. We propose f-BRS (feature backpropagating refinement scheme)\nthat solves an optimization problem with respect to auxiliary variables instead\nof the network inputs, and requires running forward and backward pass just for\na small part of a network. Experiments on GrabCut, Berkeley, DAVIS and SBD\ndatasets set new state-of-the-art at an order of magnitude lower time per click\ncompared to original BRS. The code and trained models are available at\nhttps://github.com/saic-vul/fbrs_interactive_segmentation .\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 14:10:46 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 08:03:20 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 11:52:26 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Sofiiuk", "Konstantin", ""], ["Petrov", "Ilia", ""], ["Barinova", "Olga", ""], ["Konushin", "Anton", ""]]}, {"id": "2001.10335", "submitter": "Georgios Leontidis", "authors": "Mamatha Thota, Stefanos Kollias, Mark Swainson, Georgios Leontidis", "title": "Multi-Source Deep Domain Adaptation for Quality Control in Retail Food\n  Packaging", "comments": "8 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retail food packaging contains information which informs choice and can be\nvital to consumer health, including product name, ingredients list, nutritional\ninformation, allergens, preparation guidelines, pack weight, storage and shelf\nlife information (use-by / best before dates). The presence and accuracy of\nsuch information is critical to ensure a detailed understanding of the product\nand to reduce the potential for health risks. Consequently, erroneous or\nillegible labeling has the potential to be highly detrimental to consumers and\nmany other stakeholders in the supply chain. In this paper, a multi-source deep\nlearning-based domain adaptation system is proposed and tested to identify and\nverify the presence and legibility of use-by date information from food\npackaging photos taken as part of the validation process as the products pass\nalong the food production line. This was achieved by improving the\ngeneralization of the techniques via making use of multi-source datasets in\norder to extract domain-invariant representations for all domains and aligning\ndistribution of all pairs of source and target domains in a common feature\nspace, along with the class boundaries. The proposed system performed very well\nin the conducted experiments, for automating the verification process and\nreducing labeling errors that could otherwise threaten public health and\ncontravene legal requirements for food packaging information and accuracy.\nComprehensive experiments on our food packaging datasets demonstrate that the\nproposed multi-source deep domain adaptation method significantly improves the\nclassification accuracy and therefore has great potential for application and\nbeneficial impact in food manufacturing control systems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 14:16:58 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Thota", "Mamatha", ""], ["Kollias", "Stefanos", ""], ["Swainson", "Mark", ""], ["Leontidis", "Georgios", ""]]}, {"id": "2001.10388", "submitter": "Bonifaz Stuhr", "authors": "Bonifaz Stuhr and J\\\"urgen Brauer", "title": "CSNNs: Unsupervised, Backpropagation-free Convolutional Neural Networks\n  for Representation Learning", "comments": "18 pages,18 figures, Author's extended version of the paper. Final\n  version presented at 18th IEEE International Conference on Machine Learning\n  and Applications (ICMLA). Boca Raton, Florida / USA. 2019", "journal-ref": null, "doi": "10.1109/ICMLA.2019.00265", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work combines Convolutional Neural Networks (CNNs), clustering via\nSelf-Organizing Maps (SOMs) and Hebbian Learning to propose the building blocks\nof Convolutional Self-Organizing Neural Networks (CSNNs), which learn\nrepresentations in an unsupervised and Backpropagation-free manner. Our\napproach replaces the learning of traditional convolutional layers from CNNs\nwith the competitive learning procedure of SOMs and simultaneously learns local\nmasks between those layers with separate Hebbian-like learning rules to\novercome the problem of disentangling factors of variation when filters are\nlearned through clustering. We investigate the learned representation by\ndesigning two simple models with our building blocks, achieving comparable\nperformance to many methods which use Backpropagation, while we reach\ncomparable performance on Cifar10 and give baseline performances on Cifar100,\nTiny ImageNet and a small subset of ImageNet for Backpropagation-free methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 14:57:39 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 10:47:08 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Stuhr", "Bonifaz", ""], ["Brauer", "J\u00fcrgen", ""]]}, {"id": "2001.10420", "submitter": "Gustavo De Rosa", "authors": "Gustavo Henrique de Rosa, Jo\\~ao Paulo Papa, Alexandre Xavier Falc\\~ao", "title": "OPFython: A Python-Inspired Optimum-Path Forest Classifier", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning techniques have been paramount throughout the last years,\nbeing applied in a wide range of tasks, such as classification, object\nrecognition, person identification, and image segmentation. Nevertheless,\nconventional classification algorithms, e.g., Logistic Regression, Decision\nTrees, and Bayesian classifiers, might lack complexity and diversity, not\nsuitable when dealing with real-world data. A recent graph-inspired classifier,\nknown as the Optimum-Path Forest, has proven to be a state-of-the-art\ntechnique, comparable to Support Vector Machines and even surpassing it in some\ntasks. This paper proposes a Python-based Optimum-Path Forest framework,\ndenoted as OPFython, where all of its functions and classes are based upon the\noriginal C language implementation. Additionally, as OPFython is a Python-based\nlibrary, it provides a more friendly environment and a faster prototyping\nworkspace than the C language.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 15:46:19 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 14:06:47 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["de Rosa", "Gustavo Henrique", ""], ["Papa", "Jo\u00e3o Paulo", ""], ["Falc\u00e3o", "Alexandre Xavier", ""]]}, {"id": "2001.10422", "submitter": "Arber Zela", "authors": "Arber Zela, Julien Siems, Frank Hutter", "title": "NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural\n  Architecture Search", "comments": "In: International Conference on Learning Representations (ICLR 2020);\n  19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot neural architecture search (NAS) has played a crucial role in making\nNAS methods computationally feasible in practice. Nevertheless, there is still\na lack of understanding on how these weight-sharing algorithms exactly work due\nto the many factors controlling the dynamics of the process. In order to allow\na scientific study of these components, we introduce a general framework for\none-shot NAS that can be instantiated to many recently-introduced variants and\nintroduce a general benchmarking framework that draws on the recent large-scale\ntabular benchmark NAS-Bench-101 for cheap anytime evaluations of one-shot NAS\nmethods. To showcase the framework, we compare several state-of-the-art\none-shot NAS methods, examine how sensitive they are to their hyperparameters\nand how they can be improved by tuning their hyperparameters, and compare their\nperformance to that of blackbox optimizers for NAS-Bench-101.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 15:50:22 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 22:48:28 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zela", "Arber", ""], ["Siems", "Julien", ""], ["Hutter", "Frank", ""]]}, {"id": "2001.10467", "submitter": "Tom\\'a\\v{s} Dlask", "authors": "Tom\\'a\\v{s} Dlask, Tom\\'a\\v{s} Werner", "title": "A Class of Linear Programs Solvable by Coordinate-Wise Minimization", "comments": "The final authenticated version is available online at\n  https://doi.org/10.1007/978-3-030-53552-0_8", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinate-wise minimization is a simple popular method for large-scale\noptimization. Unfortunately, for general (non-differentiable) convex problems\nit may not find global minima. We present a class of linear programs that\ncoordinate-wise minimization solves exactly. We show that dual LP relaxations\nof several well-known combinatorial optimization problems are in this class and\nthe method finds a global minimum with sufficient accuracy in reasonable\nruntimes. Moreover, for extensions of these problems that no longer are in this\nclass the method yields reasonably good suboptima. Though the presented LP\nrelaxations can be solved by more efficient methods (such as max-flow), our\nresults are theoretically non-trivial and can lead to new large-scale\noptimization algorithms in the future.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 17:14:47 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 10:46:21 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 09:24:05 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 15:51:02 GMT"}, {"version": "v5", "created": "Mon, 14 Sep 2020 10:47:55 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Dlask", "Tom\u00e1\u0161", ""], ["Werner", "Tom\u00e1\u0161", ""]]}, {"id": "2001.10472", "submitter": "Yiqun Wang", "authors": "Yiqun Wang, Jing Ren, Dong-Ming Yan, Jianwei Guo, Xiaopeng Zhang,\n  Peter Wonka", "title": "MGCN: Descriptor Learning using Multiscale GCNs", "comments": "Accepted to SIGGRAPH 2020. (15 pages, 15 figures, 12 tables,\n  low-resolution version)", "journal-ref": "ACM Transactions on Graphics (Proceedings of SIGGRAPH), Vol. 39,\n  No. 4, Article 122. Publication date: July 2020", "doi": "10.1145/3386569.3392443", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for computing descriptors for characterizing\npoints on three-dimensional surfaces. First, we present a new non-learned\nfeature that uses graph wavelets to decompose the Dirichlet energy on a\nsurface. We call this new feature wavelet energy decomposition signature\n(WEDS). Second, we propose a new multiscale graph convolutional network (MGCN)\nto transform a non-learned feature to a more discriminative descriptor. Our\nresults show that the new descriptor WEDS is more discriminative than the\ncurrent state-of-the-art non-learned descriptors and that the combination of\nWEDS and MGCN is better than the state-of-the-art learned descriptors. An\nimportant design criterion for our descriptor is the robustness to different\nsurface discretizations including triangulations with varying numbers of\nvertices. Our results demonstrate that previous graph convolutional networks\nsignificantly overfit to a particular resolution or even a particular\ntriangulation, but MGCN generalizes well to different surface discretizations.\nIn addition, MGCN is compatible with previous descriptors and it can also be\nused to improve the performance of other descriptors, such as the heat kernel\nsignature, the wave kernel signature, or the local point signature.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 17:25:14 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 21:57:40 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 10:18:28 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Wang", "Yiqun", ""], ["Ren", "Jing", ""], ["Yan", "Dong-Ming", ""], ["Guo", "Jianwei", ""], ["Zhang", "Xiaopeng", ""], ["Wonka", "Peter", ""]]}, {"id": "2001.10484", "submitter": "Seyed Mehdi Ayyoubzadeh", "authors": "Seyed Mehdi Ayyoubzadeh, Xiaolin Wu", "title": "Lossless Compression of Mosaic Images with Convolutional Neural Network\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a CNN-based predictive lossless compression scheme for raw color\nmosaic images of digital cameras. This specialized application problem was\npreviously understudied but it is now becoming increasingly important, because\nmodern CNN methods for image restoration tasks (e.g., superresolution, low\nlighting enhancement, deblurring), must operate on original raw mosaic images\nto obtain the best possible results. The key innovation of this paper is a\nhigh-order nonlinear CNN predictor of spatial-spectral mosaic patterns. The\ndeep learning prediction can model highly complex sample dependencies in\nspatial-spectral mosaic images more accurately and hence remove statistical\nredundancies more thoroughly than existing image predictors. Experiments show\nthat the proposed CNN predictor achieves unprecedented lossless compression\nperformance on camera raw images.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 17:41:31 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Ayyoubzadeh", "Seyed Mehdi", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2001.10503", "submitter": "Daniel Elton", "authors": "Daniel C. Elton, Veit Sandfort, Perry J. Pickhardt, and Ronald M.\n  Summers", "title": "Accurately identifying vertebral levels in large datasets", "comments": "Accepted for publication in Proceedings of SPIE 2020: Medical Imaging", "journal-ref": null, "doi": "10.1117/12.2551247", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vertebral levels of the spine provide a useful coordinate system when\nmaking measurements of plaque, muscle, fat, and bone mineral density. Correctly\nclassifying vertebral levels with high accuracy is challenging due to the\nsimilar appearance of each vertebra, the curvature of the spine, and the\npossibility of anomalies such as fractured vertebrae, implants, lumbarization\nof the sacrum, and sacralization of L5. The goal of this work is to develop a\nsystem that can accurately and robustly identify the L1 level in large\nheterogeneous datasets. The first approach we study is using a 3D U-Net to\nsegment the L1 vertebra directly using the entire scan volume to provide\ncontext. We also tested models for two class segmentation of L1 and T12 and a\nthree class segmentation of L1, T12 and the rib attached to T12. By increasing\nthe number of training examples to 249 scans using pseudo-segmentations from an\nin-house segmentation tool we were able to achieve 98% accuracy with respect to\nidentifying the L1 vertebra, with an average error of 4.5 mm in the\ncraniocaudal level. We next developed an algorithm which performs iterative\ninstance segmentation and classification of the entire spine with a 3D U-Net.\nWe found the instance based approach was able to yield better segmentations of\nnearly the entire spine, but had lower classification accuracy for L1.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 18:15:02 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Elton", "Daniel C.", ""], ["Sandfort", "Veit", ""], ["Pickhardt", "Perry J.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "2001.10504", "submitter": "Jaka \\v{S}ircelj", "authors": "Jaka \\v{S}ircelj, Tim Oblak, Klemen Grm, Uro\\v{s} Petkovi\\'c, Ale\\v{s}\n  Jakli\\v{c}, Peter Peer, Vitomir \\v{S}truc and Franc Solina", "title": "Segmentation and Recovery of Superquadric Models using Convolutional\n  Neural Networks", "comments": "8 pages, in Computer Vision Winter Workshop, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of representing 3D visual data with\nparameterized volumetric shape primitives. Specifically, we present a\n(two-stage) approach built around convolutional neural networks (CNNs) capable\nof segmenting complex depth scenes into the simpler geometric structures that\ncan be represented with superquadric models. In the first stage, our approach\nuses a Mask RCNN model to identify superquadric-like structures in depth scenes\nand then fits superquadric models to the segmented structures using a specially\ndesigned CNN regressor. Using our approach we are able to describe complex\nstructures with a small number of interpretable parameters. We evaluated the\nproposed approach on synthetic as well as real-world depth data and show that\nour solution does not only result in competitive performance in comparison to\nthe state-of-the-art, but is able to decompose scenes into a number of\nsuperquadric models at a fraction of the time required by competing approaches.\nWe make all data and models used in the paper available from\nhttps://lmi.fe.uni-lj.si/en/research/resources/sq-seg.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 18:17:48 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["\u0160ircelj", "Jaka", ""], ["Oblak", "Tim", ""], ["Grm", "Klemen", ""], ["Petkovi\u0107", "Uro\u0161", ""], ["Jakli\u010d", "Ale\u0161", ""], ["Peer", "Peter", ""], ["\u0160truc", "Vitomir", ""], ["Solina", "Franc", ""]]}, {"id": "2001.10528", "submitter": "Geoff Pleiss", "authors": "Geoff Pleiss, Tianyi Zhang, Ethan R. Elenberg, Kilian Q. Weinberger", "title": "Identifying Mislabeled Data using the Area Under the Margin Ranking", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all data in a typical training set help with generalization; some samples\ncan be overly ambiguous or outrightly mislabeled. This paper introduces a new\nmethod to identify such samples and mitigate their impact when training neural\nnetworks. At the heart of our algorithm is the Area Under the Margin (AUM)\nstatistic, which exploits differences in the training dynamics of clean and\nmislabeled samples. A simple procedure - adding an extra class populated with\npurposefully mislabeled threshold samples - learns a AUM upper bound that\nisolates mislabeled data. This approach consistently improves upon prior work\non synthetic and real-world datasets. On the WebVision50 classification task\nour method removes 17% of training data, yielding a 1.6% (absolute) improvement\nin test error. On CIFAR100 removing 13% of the data leads to a 1.2% drop in\nerror.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 18:59:03 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 18:41:30 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 18:55:17 GMT"}, {"version": "v4", "created": "Wed, 23 Dec 2020 14:01:54 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Pleiss", "Geoff", ""], ["Zhang", "Tianyi", ""], ["Elenberg", "Ethan R.", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "2001.10609", "submitter": "Caner Sahin", "authors": "Caner Sahin, Guillermo Garcia-Hernando, Juil Sock, Tae-Kyun Kim", "title": "A Review on Object Pose Recovery: from 3D Bounding Box Detectors to Full\n  6D Pose Estimators", "comments": "Accepted to the journal of Image and Vision Computing (IVC). arXiv\n  admin note: text overlap with arXiv:1903.04229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object pose recovery has gained increasing attention in the computer vision\nfield as it has become an important problem in rapidly evolving technological\nareas related to autonomous driving, robotics, and augmented reality. Existing\nreview-related studies have addressed the problem at visual level in 2D, going\nthrough the methods which produce 2D bounding boxes of objects of interest in\nRGB images. The 2D search space is enlarged either using the geometry\ninformation available in the 3D space along with RGB (Mono/Stereo) images, or\nutilizing depth data from LIDAR sensors and/or RGB-D cameras. 3D bounding box\ndetectors, producing category-level amodal 3D bounding boxes, are evaluated on\ngravity aligned images, while full 6D object pose estimators are mostly tested\nat instance-level on the images where the alignment constraint is removed.\nRecently, 6D object pose estimation is tackled at the level of categories. In\nthis paper, we present the first comprehensive and most recent review of the\nmethods on object pose recovery, from 3D bounding box detectors to full 6D pose\nestimators. The methods mathematically model the problem as a classification,\nregression, classification & regression, template matching, and point-pair\nfeature matching task. Based on this, a mathematical-model-based categorization\nof the methods is established. Datasets used for evaluating the methods are\ninvestigated with respect to the challenges, and evaluation metrics are\nstudied. Quantitative results of experiments in the literature are analyzed to\nshow which category of methods best performs across what types of challenges.\nThe analyses are further extended comparing two methods, which are our own\nimplementations, so that the outcomes from the public results are further\nsolidified. Current position of the field is summarized regarding object pose\nrecovery, and possible research directions are identified.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 22:05:09 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 11:59:38 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Sahin", "Caner", ""], ["Garcia-Hernando", "Guillermo", ""], ["Sock", "Juil", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2001.10619", "submitter": "Xiaofeng Yang", "authors": "Yang Lei, Yabo Fu, Tonghe Wang, Richard L.J. Qiu, Walter J. Curran,\n  Tian Liu, Xiaofeng Yang", "title": "Deep Learning in Multi-organ Segmentation", "comments": "37 pages, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a review of deep learning (DL) in multi-organ\nsegmentation. We summarized the latest DL-based methods for medical image\nsegmentation and applications. These methods were classified into six\ncategories according to their network design. For each category, we listed the\nsurveyed works, highlighted important contributions and identified specific\nchallenges. Following the detailed review of each category, we briefly\ndiscussed its achievements, shortcomings and future potentials. We provided a\ncomprehensive comparison among DL-based methods for thoracic and head & neck\nmultiorgan segmentation using benchmark datasets, including the 2017 AAPM\nThoracic Auto-segmentation Challenge datasets and 2015 MICCAI Head Neck\nAuto-Segmentation Challenge datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 22:11:44 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Lei", "Yang", ""], ["Fu", "Yabo", ""], ["Wang", "Tonghe", ""], ["Qiu", "Richard L. J.", ""], ["Curran", "Walter J.", ""], ["Liu", "Tian", ""], ["Yang", "Xiaofeng", ""]]}, {"id": "2001.10673", "submitter": "Shubham Sonawani", "authors": "Shubham Sonawani (1), Ryan Alimo (2), Renaud Detry (2), Daniel Jeong\n  (2), Andrew Hess (2), Heni Ben Amor (1) ((1) Interactive Robotics Laboratory,\n  Arizona State University, Tempe, AZ, 85281, USA, (2) Jet Propulsion\n  Laboratory, California Institute of Technology, Pasadena, CA, 91109, USA)", "title": "Assistive Relative Pose Estimation for On-orbit Assembly using\n  Convolutional Neural Networks", "comments": null, "journal-ref": "AIAA-Scitech 2020", "doi": "10.2514/6.2020-2096", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate real-time pose estimation of spacecraft or object in space is a key\ncapability necessary for on-orbit spacecraft servicing and assembly tasks. Pose\nestimation of objects in space is more challenging than for objects on Earth\ndue to space images containing widely varying illumination conditions, high\ncontrast, and poor resolution in addition to power and mass constraints. In\nthis paper, a convolutional neural network is leveraged to uniquely determine\nthe translation and rotation of an object of interest relative to the camera.\nThe main idea of using CNN model is to assist object tracker used in on space\nassembly tasks where only feature based method is always not sufficient. The\nsimulation framework designed for assembly task is used to generate dataset for\ntraining the modified CNN models and, then results of different models are\ncompared with measure of how accurately models are predicting the pose. Unlike\nmany current approaches for spacecraft or object in space pose estimation, the\nmodel does not rely on hand-crafted object-specific features which makes this\nmodel more robust and easier to apply to other types of spacecraft. It is shown\nthat the model performs comparable to the current feature-selection methods and\ncan therefore be used in conjunction with them to provide more reliable\nestimates.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 02:53:52 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 08:02:42 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Sonawani", "Shubham", ""], ["Alimo", "Ryan", ""], ["Detry", "Renaud", ""], ["Jeong", "Daniel", ""], ["Hess", "Andrew", ""], ["Amor", "Heni Ben", ""]]}, {"id": "2001.10685", "submitter": "Joseph Bullock", "authors": "Tomaz Logar, Joseph Bullock, Edoardo Nemni, Lars Bromley, John A.\n  Quinn, Miguel Luengo-Oroz", "title": "PulseSatellite: A tool using human-AI feedback loops for satellite image\n  analysis in humanitarian contexts", "comments": "2 pages, 2 figures", "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence, New\n  York, United States, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanitarian response to natural disasters and conflicts can be assisted by\nsatellite image analysis. In a humanitarian context, very specific satellite\nimage analysis tasks must be done accurately and in a timely manner to provide\noperational support. We present PulseSatellite, a collaborative satellite image\nanalysis tool which leverages neural network models that can be retrained\non-the fly and adapted to specific humanitarian contexts and geographies. We\npresent two case studies, in mapping shelters and floods respectively, that\nillustrate the capabilities of PulseSatellite.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 04:09:51 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Logar", "Tomaz", ""], ["Bullock", "Joseph", ""], ["Nemni", "Edoardo", ""], ["Bromley", "Lars", ""], ["Quinn", "John A.", ""], ["Luengo-Oroz", "Miguel", ""]]}, {"id": "2001.10692", "submitter": "Charles Ruizhongtai Qi", "authors": "Charles R. Qi, Xinlei Chen, Or Litany, Leonidas J. Guibas", "title": "ImVoteNet: Boosting 3D Object Detection in Point Clouds with Image Votes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection has seen quick progress thanks to advances in deep\nlearning on point clouds. A few recent works have even shown state-of-the-art\nperformance with just point clouds input (e.g. VoteNet). However, point cloud\ndata have inherent limitations. They are sparse, lack color information and\noften suffer from sensor noise. Images, on the other hand, have high resolution\nand rich texture. Thus they can complement the 3D geometry provided by point\nclouds. Yet how to effectively use image information to assist point cloud\nbased detection is still an open question. In this work, we build on top of\nVoteNet and propose a 3D detection architecture called ImVoteNet specialized\nfor RGB-D scenes. ImVoteNet is based on fusing 2D votes in images and 3D votes\nin point clouds. Compared to prior work on multi-modal detection, we explicitly\nextract both geometric and semantic features from the 2D images. We leverage\ncamera parameters to lift these features to 3D. To improve the synergy of 2D-3D\nfeature fusion, we also propose a multi-tower training scheme. We validate our\nmodel on the challenging SUN RGB-D dataset, advancing state-of-the-art results\nby 5.7 mAP. We also provide rich ablation studies to analyze the contribution\nof each design choice.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 05:09:28 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Qi", "Charles R.", ""], ["Chen", "Xinlei", ""], ["Litany", "Or", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2001.10695", "submitter": "Aydogan Ozcan", "authors": "Hongda Wang, Hatice Ceylan Koydemir, Yunzhe Qiu, Bijie Bai, Yibo\n  Zhang, Yiyin Jin, Sabiha Tok, Enis Cagatay Yilmaz, Esin Gumustekin, Yair\n  Rivenson, Aydogan Ozcan", "title": "Early-detection and classification of live bacteria using time-lapse\n  coherent imaging and deep learning", "comments": "24 pages, 6 figures", "journal-ref": "Light: Science & Applications (2020)", "doi": "10.1038/s41377-020-00358-9", "report-no": null, "categories": "physics.ins-det cs.CV physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational live bacteria detection system that periodically\ncaptures coherent microscopy images of bacterial growth inside a 60 mm diameter\nagar-plate and analyzes these time-lapsed holograms using deep neural networks\nfor rapid detection of bacterial growth and classification of the corresponding\nspecies. The performance of our system was demonstrated by rapid detection of\nEscherichia coli and total coliform bacteria (i.e., Klebsiella aerogenes and\nKlebsiella pneumoniae subsp. pneumoniae) in water samples. These results were\nconfirmed against gold-standard culture-based results, shortening the detection\ntime of bacterial growth by >12 h as compared to the Environmental Protection\nAgency (EPA)-approved analytical methods. Our experiments further confirmed\nthat this method successfully detects 90% of bacterial colonies within 7-10 h\n(and >95% within 12 h) with a precision of 99.2-100%, and correctly identifies\ntheir species in 7.6-12 h with 80% accuracy. Using pre-incubation of samples in\ngrowth media, our system achieved a limit of detection (LOD) of ~1 colony\nforming unit (CFU)/L within 9 h of total test time. This computational bacteria\ndetection and classification platform is highly cost-effective (~$0.6 per test)\nand high-throughput with a scanning speed of 24 cm2/min over the entire plate\nsurface, making it highly suitable for integration with the existing analytical\nmethods currently used for bacteria detection on agar plates. Powered by deep\nlearning, this automated and cost-effective live bacteria detection platform\ncan be transformative for a wide range of applications in microbiology by\nsignificantly reducing the detection time, also automating the identification\nof colonies, without labeling or the need for an expert.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 05:39:23 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Wang", "Hongda", ""], ["Koydemir", "Hatice Ceylan", ""], ["Qiu", "Yunzhe", ""], ["Bai", "Bijie", ""], ["Zhang", "Yibo", ""], ["Jin", "Yiyin", ""], ["Tok", "Sabiha", ""], ["Yilmaz", "Enis Cagatay", ""], ["Gumustekin", "Esin", ""], ["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2001.10709", "submitter": "Yu Liu", "authors": "Yu Liu, Jie Li, Xia Yuan, Chunxia Zhao, Roland Siegwart, Ian Reid,\n  Cesar Cadena", "title": "Depth Based Semantic Scene Completion with Position Importance Aware\n  Loss", "comments": "ICRA2020 In Conjuction With RAL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic Scene Completion (SSC) refers to the task of inferring the 3D\nsemantic segmentation of a scene while simultaneously completing the 3D shapes.\nWe propose PALNet, a novel hybrid network for SSC based on single depth. PALNet\nutilizes a two-stream network to extract both 2D and 3D features from\nmulti-stages using fine-grained depth information to efficiently captures the\ncontext, as well as the geometric cues of the scene. Current methods for SSC\ntreat all parts of the scene equally causing unnecessary attention to the\ninterior of objects. To address this problem, we propose Position Aware\nLoss(PA-Loss) which is position importance aware while training the network.\nSpecifically, PA-Loss considers Local Geometric Anisotropy to determine the\nimportance of different positions within the scene. It is beneficial for\nrecovering key details like the boundaries of objects and the corners of the\nscene. Comprehensive experiments on two benchmark datasets demonstrate the\neffectiveness of the proposed method and its superior performance. Models and\nVideo demo can be found at: https://github.com/UniLauX/PALNet.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 07:05:52 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 20:16:34 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Liu", "Yu", ""], ["Li", "Jie", ""], ["Yuan", "Xia", ""], ["Zhao", "Chunxia", ""], ["Siegwart", "Roland", ""], ["Reid", "Ian", ""], ["Cadena", "Cesar", ""]]}, {"id": "2001.10710", "submitter": "Souvik Kundu", "authors": "Souvik Kundu, Mahdi Nazemi, Massoud Pedram, Keith M. Chugg, Peter A.\n  Beerel", "title": "Pre-defined Sparsity for Low-Complexity Convolutional Neural Networks", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high energy cost of processing deep convolutional neural networks impedes\ntheir ubiquitous deployment in energy-constrained platforms such as embedded\nsystems and IoT devices. This work introduces convolutional layers with\npre-defined sparse 2D kernels that have support sets that repeat periodically\nwithin and across filters. Due to the efficient storage of our periodic sparse\nkernels, the parameter savings can translate into considerable improvements in\nenergy efficiency due to reduced DRAM accesses, thus promising significant\nimprovements in the trade-off between energy consumption and accuracy for both\ntraining and inference. To evaluate this approach, we performed experiments\nwith two widely accepted datasets, CIFAR-10 and Tiny ImageNet in sparse\nvariants of the ResNet18 and VGG16 architectures. Compared to baseline models,\nour proposed sparse variants require up to 82% fewer model parameters with\n5.6times fewer FLOPs with negligible loss in accuracy for ResNet18 on CIFAR-10.\nFor VGG16 trained on Tiny ImageNet, our approach requires 5.8times fewer FLOPs\nand up to 83.3% fewer model parameters with a drop in top-5 (top-1) accuracy of\nonly 1.2% (2.1%). We also compared the performance of our proposed\narchitectures with that of ShuffleNet andMobileNetV2. Using similar\nhyperparameters and FLOPs, our ResNet18 variants yield an average accuracy\nimprovement of 2.8%.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 07:10:56 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 19:26:39 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Kundu", "Souvik", ""], ["Nazemi", "Mahdi", ""], ["Pedram", "Massoud", ""], ["Chugg", "Keith M.", ""], ["Beerel", "Peter A.", ""]]}, {"id": "2001.10717", "submitter": "Michael Barrow", "authors": "Michael Barrow, Alice Chao, Qizhi He, Sonia Ramamoorthy, Claude Sirlin\n  and Ryan Kastner", "title": "Patient Specific Biomechanics Are Clinically Significant In Accurate\n  Computer Aided Surgical Image Guidance", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality is used in Image Guided surgery (AR IG) to fuse surgical\nlandmarks from preoperative images into a video overlay. Physical simulation is\nessential to maintaining accurate position of the landmarks as surgery\nprogresses and ensuring patient safety by avoiding accidental damage to vessels\netc. In liver procedures, AR IG simulation accuracy is hampered by an inability\nto model stiffness variations unique to the patients disease. We introduce a\nnovel method to account for patient specific stiffness variation based on\nMagnetic Resonance Elastography (MRE) data. To the best of our knowledge we are\nthe first to demonstrate the use of in-vivo biomechanical data for AR IG\nlandmark placement. In this early work, a comparative evaluation of our MRE\ndata driven simulation and the traditional method shows clinically significant\ndifferences in accuracy during landmark placement and motivates further animal\nmodel trials.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 08:11:07 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Barrow", "Michael", ""], ["Chao", "Alice", ""], ["He", "Qizhi", ""], ["Ramamoorthy", "Sonia", ""], ["Sirlin", "Claude", ""], ["Kastner", "Ryan", ""]]}, {"id": "2001.10773", "submitter": "Martin Humenberger", "authors": "Yohann Cabon, Naila Murray, Martin Humenberger", "title": "Virtual KITTI 2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an updated version of the well-known Virtual KITTI\ndataset which consists of 5 sequence clones from the KITTI tracking benchmark.\nIn addition, the dataset provides different variants of these sequences such as\nmodified weather conditions (e.g. fog, rain) or modified camera configurations\n(e.g. rotated by 15 degrees). For each sequence, we provide multiple sets of\nimages containing RGB, depth, class segmentation, instance segmentation, flow,\nand scene flow data. Camera parameters and poses as well as vehicle locations\nare available as well. In order to showcase some of the dataset's capabilities,\nwe ran multiple relevant experiments using state-of-the-art algorithms from the\nfield of autonomous driving. The dataset is available for download at\nhttps://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 12:13:20 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Cabon", "Yohann", ""], ["Murray", "Naila", ""], ["Humenberger", "Martin", ""]]}, {"id": "2001.10785", "submitter": "Elena Andreeva", "authors": "Elena Andreeva, Vladimir V. Arlazarov, Oleg Slavin, Aleksey Mishev", "title": "Comparison of scanned administrative document images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work the methods of comparison of digitized copies of administrative\ndocuments were considered. This problem arises, for example, when comparing two\ncopies of documents signed by two parties in order to find possible\nmodifications made by one party, in the banking sector at the conclusion of\ncontracts in paper form. The proposed method of document image comparison is\nbased on a combination of several ways of image comparison of words that are\ndescriptors of text feature points. Testing was conducted on public Payslip\nDataset (French). The results showed the high quality and the reliability of\nfinding differences in two images that are versions of the same document.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 12:51:06 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Andreeva", "Elena", ""], ["Arlazarov", "Vladimir V.", ""], ["Slavin", "Oleg", ""], ["Mishev", "Aleksey", ""]]}, {"id": "2001.10789", "submitter": "Dan Barnes", "authors": "Dan Barnes and Ingmar Posner", "title": "Under the Radar: Learning to Predict Robust Keypoints for Odometry\n  Estimation and Metric Localisation in Radar", "comments": "Video summary: https://youtu.be/L-PO7nxWpJU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a self-supervised framework for learning to detect robust\nkeypoints for odometry estimation and metric localisation in radar. By\nembedding a differentiable point-based motion estimator inside our\narchitecture, we learn keypoint locations, scores and descriptors from\nlocalisation error alone. This approach avoids imposing any assumption on what\nmakes a robust keypoint and crucially allows them to be optimised for our\napplication. Furthermore the architecture is sensor agnostic and can be applied\nto most modalities. We run experiments on 280km of real world driving from the\nOxford Radar RobotCar Dataset and improve on the state-of-the-art in\npoint-based radar odometry, reducing errors by up to 45% whilst running an\norder of magnitude faster, simultaneously solving metric loop closures.\nCombining these outputs, we provide a framework capable of full mapping and\nlocalisation with radar in urban environments.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 12:59:09 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 10:36:00 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 13:43:32 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Barnes", "Dan", ""], ["Posner", "Ingmar", ""]]}, {"id": "2001.10853", "submitter": "Chao Li", "authors": "Zihao Huang, Chao Li, Feng Duan, Qibin Zhao", "title": "H-OWAN: Multi-distorted Image Restoration with Tensor 1x1 Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a challenging task to restore images from their variants with combined\ndistortions. In the existing works, a promising strategy is to apply parallel\n\"operations\" to handle different types of distortion. However, in the feature\nfusion phase, a small number of operations would dominate the restoration\nresult due to the features' heterogeneity by different operations. To this end,\nwe introduce the tensor 1x1 convolutional layer by imposing high-order tensor\n(outer) product, by which we not only harmonize the heterogeneous features but\nalso take additional non-linearity into account. To avoid the unacceptable\nkernel size resulted from the tensor product, we construct the kernels with\ntensor network decomposition, which is able to convert the exponential growth\nof the dimension to linear growth. Armed with the new layer, we propose\nHigh-order OWAN for multi-distorted image restoration. In the numerical\nexperiments, the proposed net outperforms the previous state-of-the-art and\nshows promising performance even in more difficult tasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 14:18:16 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Huang", "Zihao", ""], ["Li", "Chao", ""], ["Duan", "Feng", ""], ["Zhao", "Qibin", ""]]}, {"id": "2001.10857", "submitter": "Sebastian Stabinger BSc MSc", "authors": "Sebastian Stabinger, Peer David, Justus Piater, and Antonio\n  Rodr\\'iguez-S\\'anchez", "title": "Evaluating the Progress of Deep Learning for Visual Relational Concepts", "comments": "Submitted to Journal of Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have become the state of the art method\nfor image classification in the last ten years. Despite the fact that they\nachieve superhuman classification accuracy on many popular datasets, they often\nperform much worse on more abstract image classification tasks. We will show\nthat these difficult tasks are linked to relational concepts from the field of\nconcept learning.\n  We will review deep learning research that is linked to this area, even if it\nwas not originally presented from this angle. Reviewing the current literature,\nwe will argue that used datasets lead to an overestimate of system performance\nby providing data in a pre-attended form, by overestimating the true\nvariability and complexity of the given tasks, and other shortcomings.\n  We will hypothesise that iterative processing of the input, together with\nattentional shifts, will be needed to efficiently and reliably solve relational\nreasoning tasks with deep learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 14:21:34 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 21:17:00 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Stabinger", "Sebastian", ""], ["David", "Peer", ""], ["Piater", "Justus", ""], ["Rodr\u00edguez-S\u00e1nchez", "Antonio", ""]]}, {"id": "2001.10883", "submitter": "Max Berrendorf", "authors": "Diana Davletshina, Valentyn Melnychuk, Viet Tran, Hitansh Singla, Max\n  Berrendorf, Evgeniy Faerman, Michael Fromm, and Matthias Schubert", "title": "Unsupervised Anomaly Detection for X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining labels for medical (image) data requires scarce and expensive\nexperts. Moreover, due to ambiguous symptoms, single images rarely suffice to\ncorrectly diagnose a medical condition. Instead, it often requires to take\nadditional background information such as the patient's medical history or test\nresults into account. Hence, instead of focusing on uninterpretable black-box\nsystems delivering an uncertain final diagnosis in an end-to-end-fashion, we\ninvestigate how unsupervised methods trained on images without anomalies can be\nused to assist doctors in evaluating X-ray images of hands. Our method\nincreases the efficiency of making a diagnosis and reduces the risk of missing\nimportant regions. Therefore, we adopt state-of-the-art approaches for\nunsupervised learning to detect anomalies and show how the outputs of these\nmethods can be explained. To reduce the effect of noise, which often can be\nmistaken for an anomaly, we introduce a powerful preprocessing pipeline. We\nprovide an extensive evaluation of different approaches and demonstrate\nempirically that even without labels it is possible to achieve satisfying\nresults on a real-world dataset of X-ray images of hands. We also evaluate the\nimportance of preprocessing and one of our main findings is that without it,\nmost of our approaches perform not better than random. To foster\nreproducibility and accelerate research we make our code publicly available at\nhttps://github.com/Valentyn1997/xray\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 15:14:56 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 12:26:37 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Davletshina", "Diana", ""], ["Melnychuk", "Valentyn", ""], ["Tran", "Viet", ""], ["Singla", "Hitansh", ""], ["Berrendorf", "Max", ""], ["Faerman", "Evgeniy", ""], ["Fromm", "Michael", ""], ["Schubert", "Matthias", ""]]}, {"id": "2001.10900", "submitter": "Roman Pflugfelder", "authors": "Roman Pflugfelder, Axel Weissenfeld, Julian Wagner", "title": "On Learning Vehicle Detection in Satellite Video", "comments": "accepted by Computer Vision Winter Workshop\n  (https://cvww2020.vicos.si)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle detection in aerial and satellite images is still challenging due to\ntheir tiny appearance in pixels compared to the overall size of remote sensing\nimagery. Classical methods of object detection very often fail in this scenario\ndue to violation of implicit assumptions made such as rich texture, small to\nmoderate ratios between image size and object size. Satellite video is a very\nnew modality which introduces temporal consistency as inductive bias.\nApproaches for vehicle detection in satellite video use either background\nsubtraction, frame differencing or subspace methods showing moderate\nperformance (0.26 - 0.82 $F_1$ score). This work proposes to apply recent work\non deep learning for wide-area motion imagery (WAMI) on satellite video. We\nshow in a first approach comparable results (0.84 $F_1$) on Planet's SkySat-1\nLasVegas video with room for further improvement.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 15:35:16 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Pflugfelder", "Roman", ""], ["Weissenfeld", "Axel", ""], ["Wagner", "Julian", ""]]}, {"id": "2001.10964", "submitter": "Arjun Punjabi", "authors": "Arjun Punjabi, Jonas Schmid, Aggelos K. Katsaggelos", "title": "Examining the Benefits of Capsule Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule networks are a recently developed class of neural networks that\npotentially address some of the deficiencies with traditional convolutional\nneural networks. By replacing the standard scalar activations with vectors, and\nby connecting the artificial neurons in a new way, capsule networks aim to be\nthe next great development for computer vision applications. However, in order\nto determine whether these networks truly operate differently than traditional\nnetworks, one must look at the differences in the capsule features. To this\nend, we perform several analyses with the purpose of elucidating capsule\nfeatures and determining whether they perform as described in the initial\npublication. First, we perform a deep visualization analysis to visually\ncompare capsule features and convolutional neural network features. Then, we\nlook at the ability for capsule features to encode information across the\nvector components and address what changes in the capsule architecture provides\nthe most benefit. Finally, we look at how well the capsule features are able to\nencode instantiation parameters of class objects via visual transformations.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 17:18:43 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Punjabi", "Arjun", ""], ["Schmid", "Jonas", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "2001.10966", "submitter": "Shervan Fekri-Ershad", "authors": "Shervan Fekri-Ershad", "title": "Developing a gender classification approach in human face images using\n  modified local binary patterns and tani-moto based nearest neighbor algorithm", "comments": "12 Pages, 5 Figures, 3 Tables, Main publisher is NADIA,", "journal-ref": "International Journal of Signal Processing, Image Processing and\n  Pattern Recognition, Vol. 12, No. 4 (2019), pp.1-12", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Human identification is a much attention problem in computer vision. Gender\nclassification plays an important role in human identification as preprocess\nstep. So far, various methods have been proposed to solve this problem.\nAbsolutely, classification accuracy is the main challenge for researchers in\ngender classification. But, some challenges such as rotation, gray scale\nvariations, pose, illumination changes may be occurred in smart phone image\ncapturing. In this respect, a multi step approach is proposed in this paper to\nclassify genders in human face images based on improved local binary patters\n(MLBP). LBP is a texture descriptor, which extract local contrast and local\nspatial structure information. Some issues such as noise sensitivity, rotation\nsensitivity and low discriminative features can be considered as disadvantages\nof the basic LBP. MLBP handle disadvantages using a new theory to categorize\nextracted binary patterns of basic LBP. The proposed approach includes two\nstages. First of all, a feature vector is extracted for human face images based\non MLBP. Next, non linear classifiers can be used to classify gender. In this\npaper nearest neighborhood classifier is evaluated based on Tani-Moto metric as\ndistance measure. In the result part, two databases, self-collected and ICPR\nare used as human face database. Results are compared by some state-ofthe-art\nalgorithms in this literature that shows the high quality of the proposed\napproach in terms of accuracy rate. Some of other main advantages of the\nproposed approach are rotation invariant, low noise sensitivity, size invariant\nand low computational complexity. The proposed approach decreases the\ncomputational complexity of smartphone applications because of reducing the\nnumber of database comparisons. It can also improve performance of the\nsynchronous applications in the smarphones because of memory and CPU usage\nreduction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 17:21:56 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Fekri-Ershad", "Shervan", ""]]}, {"id": "2001.10980", "submitter": "Jing Jiang", "authors": "Jing Jiang", "title": "Multimodal Story Generation on Plural Images", "comments": "This is an undergraduate project report. Completed Dec. 2019 at the\n  Cooper Union", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, text generation models take in a sequence of text as input,\nand iteratively generate the next most probable word using pre-trained\nparameters. In this work, we propose the architecture to use images instead of\ntext as the input of the text generation model, called StoryGen. In the\narchitecture, we design a Relational Text Data Generator algorithm that relates\ndifferent features from multiple images. The output samples from the model\ndemonstrate the ability to generate meaningful paragraphs of text containing\nthe extracted features from the input images. This is an undergraduate project\nreport. Completed Dec. 2019 at the Cooper Union.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 03:39:00 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 10:44:37 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Jiang", "Jing", ""]]}, {"id": "2001.11027", "submitter": "Volker Tresp", "authors": "Volker Tresp and Sahand Sharifzadeh and Dario Konopatzki and Yunpu Ma", "title": "The Tensor Brain: Semantic Decoding for Perception and Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse perception and memory, using mathematical models for knowledge\ngraphs and tensors, to gain insights into the corresponding functionalities of\nthe human mind. Our discussion is based on the concept of propositional\nsentences consisting of \\textit{subject-predicate-object} (SPO) triples for\nexpressing elementary facts. SPO sentences are the basis for most natural\nlanguages but might also be important for explicit perception and declarative\nmemories, as well as intra-brain communication and the ability to argue and\nreason. A set of SPO sentences can be described as a knowledge graph, which can\nbe transformed into an adjacency tensor. We introduce tensor models, where\nconcepts have dual representations as indices and associated embeddings, two\nconstructs we believe are essential for the understanding of implicit and\nexplicit perception and memory in the brain. We argue that a biological\nrealization of perception and memory imposes constraints on information\nprocessing. In particular, we propose that explicit perception and declarative\nmemories require a semantic decoder, which, in a simple realization, is based\non four layers: First, a sensory memory layer, as a buffer for sensory input,\nsecond, an index layer representing concepts, third, a memoryless\nrepresentation layer for the broadcasting of information ---the \"blackboard\",\nor the \"canvas\" of the brain--- and fourth, a working memory layer as a\nprocessing center and data buffer. We discuss the operations of the four layers\nand relate them to the global workspace theory. In a Bayesian brain\ninterpretation, semantic memory defines the prior for observable triple\nstatements. We propose that ---in evolution and during development--- semantic\nmemory, episodic memory, and natural language evolved as emergent properties in\nagents' process to gain a deeper understanding of sensory information.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 07:48:01 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 16:21:01 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 08:41:03 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Tresp", "Volker", ""], ["Sharifzadeh", "Sahand", ""], ["Konopatzki", "Dario", ""], ["Ma", "Yunpu", ""]]}, {"id": "2001.11055", "submitter": "Isaac Dunn", "authors": "Isaac Dunn, Laura Hanu, Hadrien Pouget, Daniel Kroening, Tom Melham", "title": "Evaluating Robustness to Context-Sensitive Feature Perturbations of\n  Different Granularities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We cannot guarantee that training datasets are representative of the\ndistribution of inputs that will be encountered during deployment. So we must\nhave confidence that our models do not over-rely on this assumption. To this\nend, we introduce a new method that identifies context-sensitive feature\nperturbations (e.g. shape, location, texture, colour) to the inputs of image\nclassifiers. We produce these changes by performing small adjustments to the\nactivation values of different layers of a trained generative neural network.\nPerturbing at layers earlier in the generator causes changes to coarser-grained\nfeatures; perturbations further on cause finer-grained changes. Unsurprisingly,\nwe find that state-of-the-art classifiers are not robust to any such changes.\nMore surprisingly, when it comes to coarse-grained feature changes, we find\nthat adversarial training against pixel-space perturbations is not just\nunhelpful: it is counterproductive.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 19:20:01 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 07:39:38 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 13:58:39 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Dunn", "Isaac", ""], ["Hanu", "Laura", ""], ["Pouget", "Hadrien", ""], ["Kroening", "Daniel", ""], ["Melham", "Tom", ""]]}, {"id": "2001.11064", "submitter": "Adil Kaan Akan", "authors": "Adil Kaan Akan, Mehmet Ali Genc, Fatos T. Yarman Vural", "title": "Just Noticeable Difference for Machines to Generate Adversarial Images", "comments": "5 pages, 4 figures, submitted to ICIP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One way of designing a robust machine learning algorithm is to generate\nauthentic adversarial images which can trick the algorithms as much as\npossible. In this study, we propose a new method to generate adversarial images\nwhich are very similar to true images, yet, these images are discriminated from\nthe original ones and are assigned into another category by the model. The\nproposed method is based on a popular concept of experimental psychology,\ncalled, Just Noticeable Difference. We define Just Noticeable Difference for a\nmachine learning model and generate a least perceptible difference for\nadversarial images which can trick a model. The suggested model iteratively\ndistorts a true image by gradient descent method until the machine learning\nalgorithm outputs a false label. Deep Neural Networks are trained for object\ndetection and classification tasks. The cost function includes regularization\nterms to generate just noticeably different adversarial images which can be\ndetected by the model. The adversarial images generated in this study looks\nmore natural compared to the output of state of the art adversarial image\ngenerators.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 19:42:35 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Akan", "Adil Kaan", ""], ["Genc", "Mehmet Ali", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "2001.11071", "submitter": "Ning Zhang", "authors": "Ning Zhang, Yu Cao, Benyuan Liu, and Yan Luo", "title": "3D Aggregated Faster R-CNN for General Lesion Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesions are damages and abnormalities in tissues of the human body. Many of\nthem can later turn into fatal diseases such as cancers. Detecting lesions are\nof great importance for early diagnosis and timely treatment. To this end,\nComputed Tomography (CT) scans often serve as the screening tool, allowing us\nto leverage the modern object detection techniques to detect the lesions.\nHowever, lesions in CT scans are often small and sparse. The local area of\nlesions can be very confusing, leading the region based classifier branch of\nFaster R-CNN easily fail. Therefore, most of the existing state-of-the-art\nsolutions train two types of heterogeneous networks (multi-phase) separately\nfor the candidate generation and the False Positive Reduction (FPR) purposes.\nIn this paper, we enforce an end-to-end 3D Aggregated Faster R-CNN solution by\nstacking an \"aggregated classifier branch\" on the backbone of RPN. This\nclassifier branch is equipped with Feature Aggregation and Local Magnification\nLayers to enhance the classifier branch. We demonstrate our model can achieve\nthe state of the art performance on both LUNA16 and DeepLesion dataset.\nEspecially, we achieve the best single-model FROC performance on LUNA16 with\nthe inference time being 4.2s per processed scan.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 19:57:35 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Zhang", "Ning", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""], ["Luo", "Yan", ""]]}, {"id": "2001.11077", "submitter": "Pawe{\\l} Ksieniewicz", "authors": "Pawe{\\l} Ksieniewicz, Pawe{\\l} Zyblewski", "title": "stream-learn -- open-source Python library for difficult data stream\n  batch analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  stream-learn is a Python package compatible with scikit-learn and developed\nfor the drifting and imbalanced data stream analysis. Its main component is a\nstream generator, which allows to produce a synthetic data stream that may\nincorporate each of the three main concept drift types (i.e. sudden, gradual\nand incremental drift) in their recurring or non-recurring versions. The\npackage allows conducting experiments following established evaluation\nmethodologies (i.e. Test-Then-Train and Prequential). In addition, estimators\nadapted for data stream classification have been implemented, including both\nsimple classifiers and state-of-art chunk-based and online classifier\nensembles. To improve computational efficiency, package utilises its own\nimplementations of prediction metrics for imbalanced binary classification\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 20:15:09 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Ksieniewicz", "Pawe\u0142", ""], ["Zyblewski", "Pawe\u0142", ""]]}, {"id": "2001.11091", "submitter": "Mohamad Ballout", "authors": "Mohamad Ballout, Mohammad Tuqan, Daniel Asmar, Elie Shammas, George\n  Sakr", "title": "The benefits of synthetic data for action categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the value of using synthetically produced videos as\ntraining data for neural networks used for action categorization. Motivated by\nthe fact that texture and background of a video play little to no significant\nroles in optical flow, we generated simplified texture-less and background-less\nvideos and utilized the synthetic data to train a Temporal Segment Network\n(TSN). The results demonstrated that augmenting TSN with simplified synthetic\ndata improved the original network accuracy (68.5%), achieving 71.8% on HMDB-51\nwhen adding 4,000 videos and 72.4% when adding 8,000 videos. Also, training\nusing simplified synthetic videos alone on 25 classes of UCF-101 achieved\n30.71% when trained on 2500 videos and 52.7% when trained on 5000 videos.\nFinally, results showed that when reducing the number of real videos of UCF-25\nto 10% and combining them with synthetic videos, the accuracy drops to only\n85.41%, compared to a drop to 77.4% when no synthetic data is added.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 17:23:02 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Ballout", "Mohamad", ""], ["Tuqan", "Mohammad", ""], ["Asmar", "Daniel", ""], ["Shammas", "Elie", ""], ["Sakr", "George", ""]]}, {"id": "2001.11092", "submitter": "Bei Wang", "authors": "Bei Wang and Jianping An", "title": "FIS-Nets: Full-image Supervised Networks for Monocular Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the importance of full-image supervision for monocular\ndepth estimation. We propose a semi-supervised architecture, which combines\nboth unsupervised framework of using image consistency and supervised framework\nof dense depth completion. The latter provides full-image depth as supervision\nfor the former. Ego-motion from navigation system is also embedded into the\nunsupervised framework as output supervision of an inner temporal transform\nnetwork, making monocular depth estimation better. In the evaluation, we show\nthat our proposed model outperforms other approaches on depth estimation.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 06:04:26 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Wang", "Bei", ""], ["An", "Jianping", ""]]}, {"id": "2001.11101", "submitter": "Zhecheng Wang", "authors": "Zhecheng Wang, Haoyuan Li, Ram Rajagopal", "title": "Urban2Vec: Incorporating Street View Imagery and POIs for Multi-Modal\n  Urban Neighborhood Embedding", "comments": "To appear in Proceedings of the Thirty-Fourth AAAI Conference on\n  Artificial Intelligence (AAAI-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding intrinsic patterns and predicting spatiotemporal\ncharacteristics of cities require a comprehensive representation of urban\nneighborhoods. Existing works relied on either inter- or intra-region\nconnectivities to generate neighborhood representations but failed to fully\nutilize the informative yet heterogeneous data within neighborhoods. In this\nwork, we propose Urban2Vec, an unsupervised multi-modal framework which\nincorporates both street view imagery and point-of-interest (POI) data to learn\nneighborhood embeddings. Specifically, we use a convolutional neural network to\nextract visual features from street view images while preserving geospatial\nsimilarity. Furthermore, we model each POI as a bag-of-words containing its\ncategory, rating, and review information. Analog to document embedding in\nnatural language processing, we establish the semantic similarity between\nneighborhood (\"document\") and the words from its surrounding POIs in the vector\nspace. By jointly encoding visual, textual, and geospatial information into the\nneighborhood representation, Urban2Vec can achieve performances better than\nbaseline models and comparable to fully-supervised methods in downstream\nprediction tasks. Extensive experiments on three U.S. metropolitan areas also\ndemonstrate the model interpretability, generalization capability, and its\nvalue in neighborhood similarity analysis.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 21:30:53 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Wang", "Zhecheng", ""], ["Li", "Haoyuan", ""], ["Rajagopal", "Ram", ""]]}, {"id": "2001.11120", "submitter": "Zhong Zhou", "authors": "Zhong Zhou, Isak Czeresnia Etinger, Florian Metze, Alexander\n  Hauptmann, Alexander Waibel", "title": "Gun Source and Muzzle Head Detection", "comments": "EI 2020", "journal-ref": "Electronic Imaging 2020.8 (2020): 187-1", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a surging need across the world for protection against gun violence.\nThere are three main areas that we have identified as challenging in research\nthat tries to curb gun violence: temporal location of gunshots, gun type\nprediction and gun source (shooter) detection. Our task is gun source detection\nand muzzle head detection, where the muzzle head is the round opening of the\nfiring end of the gun. We would like to locate the muzzle head of the gun in\nthe video visually, and identify who has fired the shot. In our formulation, we\nturn the problem of muzzle head detection into two sub-problems of human object\ndetection and gun smoke detection. Our assumption is that the muzzle head\ntypically lies between the gun smoke caused by the shot and the shooter. We\nhave interesting results both in bounding the shooter as well as detecting the\ngun smoke. In our experiments, we are successful in detecting the muzzle head\nby detecting the gun smoke and the shooter.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 22:41:56 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zhou", "Zhong", ""], ["Etinger", "Isak Czeresnia", ""], ["Metze", "Florian", ""], ["Hauptmann", "Alexander", ""], ["Waibel", "Alexander", ""]]}, {"id": "2001.11122", "submitter": "Rosaura VidalMata", "authors": "Rosaura G. VidalMata, Walter J. Scheirer, Anna Kukleva, David Cox,\n  Hilde Kuehne", "title": "Joint Visual-Temporal Embedding for Unsupervised Learning of Actions in\n  Untrimmed Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the structure of complex activities in untrimmed videos is a\nchallenging task in the area of action recognition. One problem here is that\nthis task usually requires a large amount of hand-annotated minute- or even\nhour-long video data, but annotating such data is very time consuming and can\nnot easily be automated or scaled. To address this problem, this paper proposes\nan approach for the unsupervised learning of actions in untrimmed video\nsequences based on a joint visual-temporal embedding space. To this end, we\ncombine a visual embedding based on a predictive U-Net architecture with a\ntemporal continuous function. The resulting representation space allows\ndetecting relevant action clusters based on their visual as well as their\ntemporal appearance. The proposed method is evaluated on three standard\nbenchmark datasets, Breakfast Actions, INRIA YouTube Instructional Videos, and\n50 Salads. We show that the proposed approach is able to provide a meaningful\nvisual and temporal embedding out of the visual cues present in contiguous\nvideo frames and is suitable for the task of unsupervised temporal segmentation\nof actions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 22:51:06 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 14:56:26 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 17:17:06 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["VidalMata", "Rosaura G.", ""], ["Scheirer", "Walter J.", ""], ["Kukleva", "Anna", ""], ["Cox", "David", ""], ["Kuehne", "Hilde", ""]]}, {"id": "2001.11137", "submitter": "Yigit Alparslan", "authors": "Yigit Alparslan, Ken Alparslan, Jeremy Keim-Shenk, Shweta Khade,\n  Rachel Greenstadt", "title": "Adversarial Attacks on Convolutional Neural Networks in Facial\n  Recognition Domain", "comments": "18 pages, 8 figures, fixed typos, replotted figures, restyled the\n  plots and tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Numerous recent studies have demonstrated how Deep Neural Network (DNN)\nclassifiers can be fooled by adversarial examples, in which an attacker adds\nperturbations to an original sample, causing the classifier to misclassify the\nsample. Adversarial attacks that render DNNs vulnerable in real life represent\na serious threat in autonomous vehicles, malware filters, or biometric\nauthentication systems. In this paper, we apply Fast Gradient Sign Method to\nintroduce perturbations to a facial image dataset and then test the output on a\ndifferent classifier that we trained ourselves, to analyze transferability of\nthis method. Next, we craft a variety of different black-box attack algorithms\non a facial image dataset assuming minimal adversarial knowledge, to further\nassess the robustness of DNNs in facial recognition. While experimenting with\ndifferent image distortion techniques, we focus on modifying single optimal\npixels by a large amount, or modifying all pixels by a smaller amount, or\ncombining these two attack approaches. While our single-pixel attacks achieved\nabout a 15% average decrease in classifier confidence level for the actual\nclass, the all-pixel attacks were more successful and achieved up to an 84%\naverage decrease in confidence, along with an 81.6% misclassification rate, in\nthe case of the attack that we tested with the highest levels of perturbation.\nEven with these high levels of perturbation, the face images remained\nidentifiable to a human. Understanding how these noised and perturbed images\nbaffle the classification algorithms can yield valuable advances in the\ntraining of DNNs against defense-aware adversarial attacks, as well as adaptive\nnoise reduction techniques. We hope our research may help to advance the study\nof adversarial attacks on DNNs and defensive mechanisms to counteract them,\nparticularly in the facial recognition domain.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 00:25:05 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 21:19:12 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 07:43:45 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Alparslan", "Yigit", ""], ["Alparslan", "Ken", ""], ["Keim-Shenk", "Jeremy", ""], ["Khade", "Shweta", ""], ["Greenstadt", "Rachel", ""]]}, {"id": "2001.11152", "submitter": "Ankur Singh", "authors": "Ankur Singh", "title": "Adversarial Incremental Learning", "comments": "I want my draft to be withdrawn from arXiv. I don't want to make the\n  idea public right now. I understand now that it can't be completely removed.\n  So at least if it can't be completely removed you can direct users to the\n  latest version which doesn't has the PDF and not the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning performs really well in a wide variety of tasks, it\nstill suffers from catastrophic forgetting -- the tendency of neural networks\nto forget previously learned information upon learning new tasks where previous\ndata is not available. Earlier methods of incremental learning tackle this\nproblem by either using a part of the old dataset, by generating exemplars or\nby using memory networks. Although, these methods have shown good results but\nusing exemplars or generating them, increases memory and computation\nrequirements. To solve these problems we propose an adversarial discriminator\nbased method that does not make use of old data at all while training on new\ntasks. We particularly tackle the class incremental learning problem in image\nclassification, where data is provided in a class-based sequential manner. For\nthis problem, the network is trained using an adversarial loss along with the\ntraditional cross-entropy loss. The cross-entropy loss helps the network\nprogressively learn new classes while the adversarial loss helps in preserving\ninformation about the existing classes. Using this approach, we are able to\noutperform other state-of-the-art methods on CIFAR-100, SVHN, and MNIST\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 02:25:35 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 17:30:22 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Singh", "Ankur", ""]]}, {"id": "2001.11175", "submitter": "Jongmin Yu", "authors": "Jongmin Yu, Duyong Kim, Younkwan Lee, and Moongu Jeon", "title": "Unsupervised Pixel-level Road Defect Detection via Adversarial\n  Image-to-Frequency Transform", "comments": "Submitted to IV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past few years, the performance of road defect detection has been\nremarkably improved thanks to advancements on various studies on computer\nvision and deep learning. Although a large-scale and well-annotated datasets\nenhance the performance of detecting road pavement defects to some extent, it\nis still challengeable to derive a model which can perform reliably for various\nroad conditions in practice, because it is intractable to construct a dataset\nconsidering diverse road conditions and defect patterns. To end this, we\npropose an unsupervised approach to detecting road defects, using Adversarial\nImage-to-Frequency Transform (AIFT). AIFT adopts the unsupervised manner and\nadversarial learning in deriving the defect detection model, so AIFT does not\nneed annotations for road pavement defects. We evaluate the efficiency of AIFT\nusing GAPs384 dataset, Cracktree200 dataset, CRACK500 dataset, and CFD dataset.\nThe experimental results demonstrate that the proposed approach detects various\nroad detects, and it outperforms existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 04:50:00 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 04:27:32 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Yu", "Jongmin", ""], ["Kim", "Duyong", ""], ["Lee", "Younkwan", ""], ["Jeon", "Moongu", ""]]}, {"id": "2001.11180", "submitter": "Jimuyang Zhang", "authors": "Jimuyang Zhang, Sanping Zhou, Xin Chang, Fangbin Wan, Jinjun Wang,\n  Yang Wu, Dong Huang", "title": "Multiple Object Tracking by Flowing and Fusing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of Multiple Object Tracking (MOT) approaches compute individual target\nfeatures for two subtasks: estimating target-wise motions and conducting\npair-wise Re-Identification (Re-ID). Because of the indefinite number of\ntargets among video frames, both subtasks are very difficult to scale up\nefficiently in end-to-end Deep Neural Networks (DNNs). In this paper, we design\nan end-to-end DNN tracking approach, Flow-Fuse-Tracker (FFT), that addresses\nthe above issues with two efficient techniques: target flowing and target\nfusing. Specifically, in target flowing, a FlowTracker DNN module learns the\nindefinite number of target-wise motions jointly from pixel-level optical\nflows. In target fusing, a FuseTracker DNN module refines and fuses targets\nproposed by FlowTracker and frame-wise object detection, instead of trusting\neither of the two inaccurate sources of target proposal. Because FlowTracker\ncan explore complex target-wise motion patterns and FuseTracker can refine and\nfuse targets from FlowTracker and detectors, our approach can achieve the\nstate-of-the-art results on several MOT benchmarks. As an online MOT approach,\nFFT produced the top MOTA of 46.3 on the 2DMOT15, 56.5 on the MOT16, and 56.5\non the MOT17 tracking benchmarks, surpassing all the online and offline methods\nin existing publications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 05:17:22 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Zhang", "Jimuyang", ""], ["Zhou", "Sanping", ""], ["Chang", "Xin", ""], ["Wan", "Fangbin", ""], ["Wang", "Jinjun", ""], ["Wu", "Yang", ""], ["Huang", "Dong", ""]]}, {"id": "2001.11190", "submitter": "Max Allan", "authors": "Max Allan, Satoshi Kondo, Sebastian Bodenstedt, Stefan Leger, Rahim\n  Kadkhodamohammadi, Imanol Luengo, Felix Fuentes, Evangello Flouty, Ahmed\n  Mohammed, Marius Pedersen, Avinash Kori, Varghese Alex, Ganapathy\n  Krishnamurthi, David Rauber, Robert Mendel, Christoph Palm, Sophia Bano,\n  Guinther Saibro, Chi-Sheng Shih, Hsun-An Chiang, Juntang Zhuang, Junlin Yang,\n  Vladimir Iglovikov, Anton Dobrenkii, Madhu Reddiboina, Anubhav Reddy,\n  Xingtong Liu, Cong Gao, Mathias Unberath, Myeonghyeon Kim, Chanho Kim,\n  Chaewon Kim, Hyejin Kim, Gyeongmin Lee, Ihsan Ullah, Miguel Luna, Sang Hyun\n  Park, Mahdi Azizian, Danail Stoyanov, Lena Maier-Hein, Stefanie Speidel", "title": "2018 Robotic Scene Segmentation Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In 2015 we began a sub-challenge at the EndoVis workshop at MICCAI in Munich\nusing endoscope images of ex-vivo tissue with automatically generated\nannotations from robot forward kinematics and instrument CAD models. However,\nthe limited background variation and simple motion rendered the dataset\nuninformative in learning about which techniques would be suitable for\nsegmentation in real surgery. In 2017, at the same workshop in Quebec we\nintroduced the robotic instrument segmentation dataset with 10 teams\nparticipating in the challenge to perform binary, articulating parts and type\nsegmentation of da Vinci instruments. This challenge included realistic\ninstrument motion and more complex porcine tissue as background and was widely\naddressed with modifications on U-Nets and other popular CNN architectures. In\n2018 we added to the complexity by introducing a set of anatomical objects and\nmedical devices to the segmented classes. To avoid over-complicating the\nchallenge, we continued with porcine data which is dramatically simpler than\nhuman tissue due to the lack of fatty tissue occluding many organs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 06:37:07 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 19:44:45 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 01:55:24 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Allan", "Max", ""], ["Kondo", "Satoshi", ""], ["Bodenstedt", "Sebastian", ""], ["Leger", "Stefan", ""], ["Kadkhodamohammadi", "Rahim", ""], ["Luengo", "Imanol", ""], ["Fuentes", "Felix", ""], ["Flouty", "Evangello", ""], ["Mohammed", "Ahmed", ""], ["Pedersen", "Marius", ""], ["Kori", "Avinash", ""], ["Alex", "Varghese", ""], ["Krishnamurthi", "Ganapathy", ""], ["Rauber", "David", ""], ["Mendel", "Robert", ""], ["Palm", "Christoph", ""], ["Bano", "Sophia", ""], ["Saibro", "Guinther", ""], ["Shih", "Chi-Sheng", ""], ["Chiang", "Hsun-An", ""], ["Zhuang", "Juntang", ""], ["Yang", "Junlin", ""], ["Iglovikov", "Vladimir", ""], ["Dobrenkii", "Anton", ""], ["Reddiboina", "Madhu", ""], ["Reddy", "Anubhav", ""], ["Liu", "Xingtong", ""], ["Gao", "Cong", ""], ["Unberath", "Mathias", ""], ["Kim", "Myeonghyeon", ""], ["Kim", "Chanho", ""], ["Kim", "Chaewon", ""], ["Kim", "Hyejin", ""], ["Lee", "Gyeongmin", ""], ["Ullah", "Ihsan", ""], ["Luna", "Miguel", ""], ["Park", "Sang Hyun", ""], ["Azizian", "Mahdi", ""], ["Stoyanov", "Danail", ""], ["Maier-Hein", "Lena", ""], ["Speidel", "Stefanie", ""]]}, {"id": "2001.11192", "submitter": "Pei Wang", "authors": "Xiuxian Xu, Pei Wang, Xiaozheng Gan, Yaxin Li, Li Zhang, Qing Zhang,\n  Mei Zhou, Yinghui Zhao, Xinwei Li", "title": "Automatic marker-free registration of tree point-cloud data based on\n  rotating projection", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-cloud data acquired using a terrestrial laser scanner (TLS) play an\nimportant role in digital forestry research. Multiple scans are generally used\nto overcome occlusion effects and obtain complete tree structural information.\nHowever, it is time-consuming and difficult to place artificial reflectors in a\nforest with complex terrain for marker-based registration, a process that\nreduces registration automation and efficiency. In this study, we propose an\nautomatic coarse-to-fine method for the registration of point-cloud data from\nmultiple scans of a single tree. In coarse registration, point clouds produced\nby each scan are projected onto a spherical surface to generate a series of\ntwo-dimensional (2D) images, which are used to estimate the initial positions\nof multiple scans. Corresponding feature-point pairs are then extracted from\nthese series of 2D images. In fine registration, point-cloud data slicing and\nfitting methods are used to extract corresponding central stem and branch\ncenters for use as tie points to calculate fine transformation parameters. To\nevaluate the accuracy of registration results, we propose a model of error\nevaluation via calculating the distances between center points from\ncorresponding branches in adjacent scans. For accurate evaluation, we conducted\nexperiments on two simulated trees and a real-world tree. Average registration\nerrors of the proposed method were 0.26m around on simulated tree point clouds,\nand 0.05m around on real-world tree point cloud.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 06:53:59 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Xu", "Xiuxian", ""], ["Wang", "Pei", ""], ["Gan", "Xiaozheng", ""], ["Li", "Yaxin", ""], ["Zhang", "Li", ""], ["Zhang", "Qing", ""], ["Zhou", "Mei", ""], ["Zhao", "Yinghui", ""], ["Li", "Xinwei", ""]]}, {"id": "2001.11194", "submitter": "Xinhan Di", "authors": "Yuli Zhang, Yeyang He, Shaowen Zhu, Xinhan Di", "title": "The Direction-Aware, Learnable, Additive Kernels and the Adversarial\n  Network for Deep Floor Plan Recognition", "comments": "deep learning, floor plan, computer vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for the recognition of elements in floor\nplan layouts. Besides of elements with common shapes, we aim to recognize\nelements with irregular shapes such as circular rooms and inclined walls.\nFurthermore, the reduction of noise in the semantic segmentation of the floor\nplan is on demand. To this end, we propose direction-aware, learnable, additive\nkernels in the application of both the context module and common convolutional\nblocks. We apply them for high performance of elements with both common and\nirregular shapes. Besides, an adversarial network with two discriminators is\nproposed to further improve the accuracy of the elements and to reduce the\nnoise of the semantic segmentation. Experimental results demonstrate the\nsuperiority and effectiveness of the proposed network over the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 07:10:39 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Zhang", "Yuli", ""], ["He", "Yeyang", ""], ["Zhu", "Shaowen", ""], ["Di", "Xinhan", ""]]}, {"id": "2001.11198", "submitter": "Jayasree Saha", "authors": "Jayasree Saha, Yuvraj Khanna, Jayanta Mukherjee", "title": "A CNN With Multi-scale Convolution for Hyperspectral Image\n  Classification using Target-Pixel-Orientation scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, CNN is a popular choice to handle the hyperspectral image\nclassification challenges. In spite of having such large spectral information\nin Hyper-Spectral Image(s) (HSI), it creates a curse of dimensionality. Also,\nlarge spatial variability of spectral signature adds more difficulty in\nclassification problem. Additionally, training a CNN in the end to end fashion\nwith scarced training examples is another challenging and interesting problem.\nIn this paper, a novel target-patch-orientation method is proposed to train a\nCNN based network. Also, we have introduced a hybrid of 3D-CNN and 2D-CNN based\nnetwork architecture to implement band reduction and feature extraction\nmethods, respectively. Experimental results show that our method outperforms\nthe accuracies reported in the existing state of the art methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 07:45:07 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 06:47:34 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 14:28:06 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Saha", "Jayasree", ""], ["Khanna", "Yuvraj", ""], ["Mukherjee", "Jayanta", ""]]}, {"id": "2001.11202", "submitter": "Can Taylan Sari", "authors": "C. T. Sari, C. Sokmensuer, and C. Gunduz-Demir", "title": "Image Embedded Segmentation: Uniting Supervised and Unsupervised\n  Objectives for Segmenting Histopathological Images", "comments": "This work has been submitted for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new regularization method to train a fully\nconvolutional network for semantic tissue segmentation in histopathological\nimages. This method relies on the benefit of unsupervised learning, in the form\nof image reconstruction, for network training. To this end, it puts forward an\nidea of defining a new embedding that allows uniting the main supervised task\nof semantic segmentation and an auxiliary unsupervised task of image\nreconstruction into a single one and proposes to learn this united task by a\nsingle generative model. This embedding generates an output image by\nsuperimposing an input image on its segmentation map. Then, the method learns\nto translate the input image to this embedded output image using a conditional\ngenerative adversarial network, which is known as quite effective for\nimage-to-image translations. This proposal is different than the existing\napproach that uses image reconstruction for the same regularization purpose.\nThe existing approach considers segmentation and image reconstruction as two\nseparate tasks in a multi-task network, defines their losses independently, and\ncombines them in a joint loss function. However, the definition of such a\nfunction requires externally determining right contributions of the supervised\nand unsupervised losses that yield balanced learning between the segmentation\nand image reconstruction tasks. The proposed approach provides an easier\nsolution to this problem by uniting these two tasks into a single one, which\nintrinsically combines their losses. We test our approach on three datasets of\nhistopathological images. Our experiments demonstrate that it leads to better\nsegmentation results in these datasets, compared to its counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 08:09:38 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 10:22:35 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 11:18:20 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Sari", "C. T.", ""], ["Sokmensuer", "C.", ""], ["Gunduz-Demir", "C.", ""]]}, {"id": "2001.11207", "submitter": "Jaedong Hwang", "authors": "Jaedong Hwang, Seohyun Kim, Jeany Son, Bohyung Han", "title": "Weakly Supervised Instance Segmentation by Deep Community Learning", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a weakly supervised instance segmentation algorithm based on deep\ncommunity learning with multiple tasks. This task is formulated as a\ncombination of weakly supervised object detection and semantic segmentation,\nwhere individual objects of the same class are identified and segmented\nseparately. We address this problem by designing a unified deep neural network\narchitecture, which has a positive feedback loop of object detection with\nbounding box regression, instance mask generation, instance segmentation, and\nfeature extraction. Each component of the network makes active interactions\nwith others to improve accuracy, and the end-to-end trainability of our model\nmakes our results more robust and reproducible. The proposed algorithm achieves\nstate-of-the-art performance in the weakly supervised setting without any\nadditional training such as Fast R-CNN and Mask R-CNN on the standard benchmark\ndataset. The implementation of our algorithm is available on the project\nwebpage: https://cv.snu.ac.kr/research/WSIS_CL.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 08:35:42 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 04:33:57 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 09:43:49 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Hwang", "Jaedong", ""], ["Kim", "Seohyun", ""], ["Son", "Jeany", ""], ["Han", "Bohyung", ""]]}, {"id": "2001.11243", "submitter": "Zhuoran Shen", "authors": "Yu Li (1), Zhuoran Shen (2), Ying Shan (1) ((1) Tencent PCG Applied\n  Research Center, (2) The University of Hong Kong)", "title": "Fast Video Object Segmentation using the Global Context Module", "comments": "To appear at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a real-time, high-quality semi-supervised video object\nsegmentation algorithm. Its accuracy is on par with the most accurate,\ntime-consuming online-learning model, while its speed is similar to the fastest\ntemplate-matching method with sub-optimal accuracy. The core component of the\nmodel is a novel global context module that effectively summarizes and\npropagates information through the entire video. Compared to previous\napproaches that only use one frame or a few frames to guide the segmentation of\nthe current frame, the global context module uses all past frames. Unlike the\nprevious state-of-the-art space-time memory network that caches a memory at\neach spatio-temporal position, the global context module uses a fixed-size\nfeature representation. Therefore, it uses constant memory regardless of the\nvideo length and costs substantially less memory and computation. With the\nnovel module, our model achieves top performance on standard benchmarks at a\nreal-time speed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 10:22:27 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 01:11:10 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Li", "Yu", ""], ["Shen", "Zhuoran", ""], ["Shan", "Ying", ""]]}, {"id": "2001.11248", "submitter": "Martin Mayr", "authors": "Martin Mayr, Mathis Hoffmann, Andreas Maier, Vincent Christlein", "title": "Weakly Supervised Segmentation of Cracks on Solar Cells using Normalized\n  Lp Norm", "comments": "ICIP'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photovoltaic is one of the most important renewable energy sources for\ndealing with world-wide steadily increasing energy consumption. This raises the\ndemand for fast and scalable automatic quality management during production and\noperation. However, the detection and segmentation of cracks on\nelectroluminescence (EL) images of mono- or polycrystalline solar modules is a\nchallenging task. In this work, we propose a weakly supervised learning\nstrategy that only uses image-level annotations to obtain a method that is\ncapable of segmenting cracks on EL images of solar cells. We use a modified\nResNet-50 to derive a segmentation from network activation maps. We use defect\nclassification as a surrogate task to train the network. To this end, we apply\nnormalized Lp normalization to aggregate the activation maps into single scores\nfor classification. In addition, we provide a study how different\nparameterizations of the normalized Lp layer affect the segmentation\nperformance. This approach shows promising results for the given task. However,\nwe think that the method has the potential to solve other weakly supervised\nsegmentation problems as well.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 10:51:25 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Mayr", "Martin", ""], ["Hoffmann", "Mathis", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2001.11267", "submitter": "Ehsan Yaghoubi", "authors": "Ehsan Yaghoubi, Diana Borza, Aruna Kumar, Hugo Proen\\c{c}a", "title": "Person Re-identification: Implicitly Defining the Receptive Fields of\n  Deep Learning Classification Frameworks", "comments": "Submitted to PRL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{receptive fields} of deep learning classification models determine\nthe regions of the input data that have the most significance for providing\ncorrect decisions. The primary way to learn such receptive fields is to train\nthe models upon masked data, which helps the networks to ignore any unwanted\nregions, but has two major drawbacks: 1) it often yields edge-sensitive\ndecision processes; and 2) augments the computational cost of the inference\nphase considerably. This paper describes a solution for implicitly driving the\ninference of the networks' receptive fields, by creating synthetic learning\ndata composed of interchanged segments that should be \\emph{apriori}\nimportant/irrelevant for the network decision. In practice, we use a\nsegmentation module to distinguish between the foreground\n(important)/background (irrelevant) parts of each learning instance, and\nrandomly swap segments between image pairs, while keeping the class label\nexclusively consistent with the label of the deemed important segments. This\nstrategy typically drives the networks to early convergence and appropriate\nsolutions, where the identity and clutter descriptions are not correlated.\nMoreover, this data augmentation solution has various interesting properties:\n1) it is parameter-free; 2) it fully preserves the label information; and, 3)\nit is compatible with the typical data augmentation techniques. In the\nempirical validation, we considered the person re-identification problem and\nevaluated the effectiveness of the proposed solution in the well-known\n\\emph{Richly Annotated Pedestrian} (RAP) dataset for two different settings\n(\\emph{upper-body} and \\emph{full-body}), observing highly competitive results\nover the state-of-the-art. Under a reproducible research paradigm, both the\ncode and the empirical evaluation protocol are available at\n\\url{https://github.com/Ehsan-Yaghoubi/reid-strong-baseline}.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 11:45:44 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 12:38:49 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 18:38:32 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2020 19:59:52 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Yaghoubi", "Ehsan", ""], ["Borza", "Diana", ""], ["Kumar", "Aruna", ""], ["Proen\u00e7a", "Hugo", ""]]}, {"id": "2001.11284", "submitter": "Rhydian Windsor Mr", "authors": "Rhydian Windsor and Amir Jamaludin", "title": "The Ladder Algorithm: Finding Repetitive Structures in Medical Images by\n  Induction", "comments": "5 pages, 4 figures, IEEE International Symposium on Biomedical\n  Imaging (ISBI) 2020. Presentation:\n  https://www.youtube.com/watch?v=khlBqpNGRnE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the Ladder Algorithm; a novel recurrent algorithm\nto detect repetitive structures in natural images with high accuracy using\nlittle training data.\n  We then demonstrate the algorithm on the task of extracting vertebrae from\nwhole spine magnetic resonance scans with only lumbar MR scans for training\ndata. It is shown to achieve high perforamance with 99.8% precision and recall,\nexceeding current state of the art approaches for lumbar vertebrae detection in\nT1 and T2 weighted scans. It also generalises without retraining to whole spine\nimages with minimal drop in accuracy, achieving 99.4% detection rate.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 12:40:31 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 09:24:25 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Windsor", "Rhydian", ""], ["Jamaludin", "Amir", ""]]}, {"id": "2001.11302", "submitter": "Jadab Kumar Pal Dr", "authors": "Jimut Bahan Pal", "title": "A Deeper Look into Hybrid Images", "comments": "A deeper analysis for creating Hybrid Images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $Hybrid$ $images$ was first introduced by Olivia et al., that produced static\nimages with two interpretations such that the images changes as a function of\nviewing distance. Hybrid images are built by studying human processing of\nmultiscale images and are motivated by masking studies in visual perception.\nThe first introduction of hybrid images showed that two images can be blend\ntogether with a high pass filter and a low pass filter in such a way that when\nthe blended image is viewed from a distance, the high pass filter fades away\nand the low pass filter becomes prominent. Our main aim here is to study and\nreview the original paper by changing and tweaking certain parameters to see\nhow they affect the quality of the blended image produced. We have used\nexhaustively different set of images and filters to see how they function and\nwhether this can be used in a real time system or not.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 13:25:14 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 15:54:20 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Pal", "Jimut Bahan", ""]]}, {"id": "2001.11366", "submitter": "Anna Bosman", "authors": "Mamuku Mokuwe, Michael Burke, Anna Sergeevna Bosman", "title": "Black-Box Saliency Map Generation Using Bayesian Optimisation", "comments": "Submitted to IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency maps are often used in computer vision to provide intuitive\ninterpretations of what input regions a model has used to produce a specific\nprediction. A number of approaches to saliency map generation are available,\nbut most require access to model parameters. This work proposes an approach for\nsaliency map generation for black-box models, where no access to model\nparameters is available, using a Bayesian optimisation sampling method. The\napproach aims to find the global salient image region responsible for a\nparticular (black-box) model's prediction. This is achieved by a sampling-based\napproach to model perturbations that seeks to localise salient regions of an\nimage to the black-box model. Results show that the proposed approach to\nsaliency map generation outperforms grid-based perturbation approaches, and\nperforms similarly to gradient-based approaches which require access to model\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 14:39:12 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Mokuwe", "Mamuku", ""], ["Burke", "Michael", ""], ["Bosman", "Anna Sergeevna", ""]]}, {"id": "2001.11394", "submitter": "Lichao Mou", "authors": "Lichao Mou, Yuansheng Hua, Pu Jin, Xiao Xiang Zhu", "title": "ERA: A Dataset and Deep Learning Benchmark for Event Recognition in\n  Aerial Videos", "comments": "IEEE Geoscience and Remote Sensing Magazine. Project page:\n  https://lcmou.github.io/ERA_Dataset/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the increasing use of unmanned aerial vehicles (UAVs), large\nvolumes of aerial videos have been produced. It is unrealistic for humans to\nscreen such big data and understand their contents. Hence methodological\nresearch on the automatic understanding of UAV videos is of paramount\nimportance. In this paper, we introduce a novel problem of event recognition in\nunconstrained aerial videos in the remote sensing community and present a\nlarge-scale, human-annotated dataset, named ERA (Event Recognition in Aerial\nvideos), consisting of 2,864 videos each with a label from 25 different classes\ncorresponding to an event unfolding 5 seconds. The ERA dataset is designed to\nhave a significant intra-class variation and inter-class similarity and\ncaptures dynamic events in various circumstances and at dramatically various\nscales. Moreover, to offer a benchmark for this task, we extensively validate\nexisting deep networks. We expect that the ERA dataset will facilitate further\nprogress in automatic aerial video comprehension. The website is\nhttps://lcmou.github.io/ERA_Dataset/\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 15:25:54 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 12:47:47 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 10:00:19 GMT"}, {"version": "v4", "created": "Thu, 25 Jun 2020 10:23:08 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Mou", "Lichao", ""], ["Hua", "Yuansheng", ""], ["Jin", "Pu", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2001.11469", "submitter": "Johannes Stegmaier", "authors": "Sourabh Bhide, Ralf Mikut, Maria Leptin, Johannes Stegmaier", "title": "Semi-Automatic Generation of Tight Binary Masks and Non-Convex\n  Isosurfaces for Quantitative Analysis of 3D Biological Samples", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.CB q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current in vivo microscopy allows us detailed spatiotemporal imaging (3D+t)\nof complete organisms and offers insights into their development on the\ncellular level. Even though the imaging speed and quality is steadily\nimproving, fully-automated segmentation and analysis methods are often not\naccurate enough. This is particularly true while imaging large samples (100um -\n1mm) and deep inside the specimen. Drosophila embryogenesis, widely used as a\ndevelopmental paradigm, presents an example for such a challenge, especially\nwhere cell outlines need to imaged - a general challenge in other systems as\nwell. To deal with the current bottleneck in analyzing quantitatively the 3D+t\nlight-sheet microscopy images of Drosophila embryos, we developed a collection\nof semi-automatic open-source tools. The presented methods include a\nsemi-automatic masking procedure, automatic projection of non-convex 3D\nisosurfaces to 2D representations as well as cell segmentation and tracking.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 17:36:42 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Bhide", "Sourabh", ""], ["Mikut", "Ralf", ""], ["Leptin", "Maria", ""], ["Stegmaier", "Johannes", ""]]}, {"id": "2001.11499", "submitter": "Jana \\v{C}avojsk\\'a", "authors": "Jana \\v{C}avojsk\\'a (1), Julian Petrasch (1), Nicolas J. Lehmann (1),\n  Agn\\`es Voisard (1), Peter B\\\"ottcher (2) ((1) Freie Universit\\\"at Berlin,\n  Institute of Computer Science, 14195 Berlin, Germany, (2) Freie Universit\\\"at\n  Berlin, Clinic for Small Animals, 14163 Berlin, Germany)", "title": "Estimating and abstracting the 3D structure of bones using neural\n  networks on X-ray (2D) images", "comments": "13 pages, 5 figures, 1 table, submitted to Communications Biology", "journal-ref": "Communications biology, 2020, 3(1), pp.1-13", "doi": "10.1038/s42003-020-1057-3", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep-learning based method for estimating the 3D\nstructure of a bone from a pair of 2D X-ray images. Our triplet loss-trained\nneural network selects the most closely matching 3D bone shape from a\npredefined set of shapes. Our predictions have an average root mean square\n(RMS) distance of 1.08 mm between the predicted and true shapes, making it more\naccurate than the average error achieved by eight other examined 3D bone\nreconstruction approaches. The prediction process that we use is fully\nautomated and unlike many competing approaches, it does not rely on any\nprevious knowledge about bone geometry. Additionally, our neural network can\ndetermine the identity of a bone based only on its X-ray image. It computes a\nlow-dimensional representation (\"embedding\") of each 2D X-ray image and\nhenceforth compares different X-ray images based only on their embeddings. An\nembedding holds enough information to uniquely identify the bone CT belonging\nto the input X-ray image with a 100% accuracy and can therefore serve as a kind\nof fingerprint for that bone. Possible applications include faster, image\ncontent-based bone database searches for forensic purposes.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 20:41:17 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["\u010cavojsk\u00e1", "Jana", ""], ["Petrasch", "Julian", ""], ["Lehmann", "Nicolas J.", ""], ["Voisard", "Agn\u00e8s", ""], ["B\u00f6ttcher", "Peter", ""]]}, {"id": "2001.11539", "submitter": "Jiangbo Yuan", "authors": "Jiangbo Yuan, Bing Wu, Wanying Ding, Qing Ping, and Zhendong Yu", "title": "Adversarial Code Learning for Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the \"adversarial code learning\" (ACL) module that improves\noverall image generation performance to several types of deep models. Instead\nof performing a posterior distribution modeling in the pixel spaces of\ngenerators, ACLs aim to jointly learn a latent code with another image\nencoder/inference net, with a prior noise as its input. We conduct the learning\nin an adversarial learning process, which bears a close resemblance to the\noriginal GAN but again shifts the learning from image spaces to prior and\nlatent code spaces. ACL is a portable module that brings up much more\nflexibility and possibilities in generative model designs. First, it allows\nflexibility to convert non-generative models like Autoencoders and standard\nclassification models to decent generative models. Second, it enhances existing\nGANs' performance by generating meaningful codes and images from any part of\nthe prior. We have incorporated our ACL module with the aforementioned\nframeworks and have performed experiments on synthetic, MNIST, CIFAR-10, and\nCelebA datasets. Our models have achieved significant improvements which\ndemonstrated the generality for image generation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 19:52:46 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Yuan", "Jiangbo", ""], ["Wu", "Bing", ""], ["Ding", "Wanying", ""], ["Ping", "Qing", ""], ["Yu", "Zhendong", ""]]}, {"id": "2001.11547", "submitter": "Lee Cooper", "authors": "Sanghoon Lee, Mohamed Amgad, Deepak R. Chittajallu, Matt McCormick,\n  Brian P Pollack, Habiba Elfandy, Hagar Hussein, David A Gutman, Lee AD Cooper", "title": "HistomicsML2.0: Fast interactive machine learning for whole slide\n  imaging data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting quantitative phenotypic information from whole-slide images\npresents significant challenges for investigators who are not experienced in\ndeveloping image analysis algorithms. We present new software that enables\nrapid learn-by-example training of machine learning classifiers for detection\nof histologic patterns in whole-slide imaging datasets. HistomicsML2.0 uses\nconvolutional networks to be readily adaptable to a variety of applications,\nprovides a web-based user interface, and is available as a software container\nto simplify deployment.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 20:10:26 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Lee", "Sanghoon", ""], ["Amgad", "Mohamed", ""], ["Chittajallu", "Deepak R.", ""], ["McCormick", "Matt", ""], ["Pollack", "Brian P", ""], ["Elfandy", "Habiba", ""], ["Hussein", "Hagar", ""], ["Gutman", "David A", ""], ["Cooper", "Lee AD", ""]]}, {"id": "2001.11561", "submitter": "Linwei Ye", "authors": "Linwei Ye, Zhi Liu, Yang Wang", "title": "Dual Convolutional LSTM Network for Referring Image Segmentation", "comments": "12 pages, accepted for publication in IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2020.2971171", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider referring image segmentation. It is a problem at the intersection\nof computer vision and natural language understanding. Given an input image and\na referring expression in the form of a natural language sentence, the goal is\nto segment the object of interest in the image referred by the linguistic\nquery. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to\ntackle this problem. Our model consists of an encoder network and a decoder\nnetwork, where ConvLSTM is used in both encoder and decoder networks to capture\nspatial and sequential information. The encoder network extracts visual and\nlinguistic features for each word in the expression sentence, and adopts an\nattention mechanism to focus on words that are more informative in the\nmultimodal interaction. The decoder network integrates the features generated\nby the encoder network at multiple levels as its input and produces the final\nprecise segmentation mask. Experimental results on four challenging datasets\ndemonstrate that the proposed network achieves superior segmentation\nperformance compared with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 20:40:18 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Ye", "Linwei", ""], ["Liu", "Zhi", ""], ["Wang", "Yang", ""]]}, {"id": "2001.11580", "submitter": "Sathyanarayanan Aakur", "authors": "Sathyanarayanan N. Aakur, Arunkumar Bagavathi", "title": "Unsupervised Gaze Prediction in Egocentric Videos by Energy-based\n  Surprise Modeling", "comments": "To appear at VISAP2021. More details:\n  https://saakur.github.io/Projects/GazePrediction/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric perception has grown rapidly with the advent of immersive\ncomputing devices. Human gaze prediction is an important problem in analyzing\negocentric videos and has primarily been tackled through either saliency-based\nmodeling or highly supervised learning. We quantitatively analyze the\ngeneralization capabilities of supervised, deep learning models on the\negocentric gaze prediction task on unseen, out-of-domain data. We find that\ntheir performance is highly dependent on the training data and is restricted to\nthe domains specified in the training annotations. In this work, we tackle the\nproblem of jointly predicting human gaze points and temporal segmentation of\negocentric videos without using any training data. We introduce an unsupervised\ncomputational model that draws inspiration from cognitive psychology models of\nevent perception. We use Grenander's pattern theory formalism to represent\nspatial-temporal features and model surprise as a mechanism to predict gaze\nfixation points. Extensive evaluation on two publicly available datasets - GTEA\nand GTEA+ datasets-shows that the proposed model can significantly outperform\nall unsupervised baselines and some supervised gaze prediction baselines.\nFinally, we show that the model can also temporally segment egocentric videos\nwith a performance comparable to more complex, fully supervised deep learning\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 21:52:38 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 06:15:53 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Aakur", "Sathyanarayanan N.", ""], ["Bagavathi", "Arunkumar", ""]]}, {"id": "2001.11584", "submitter": "Wenbo Dong", "authors": "Wenbo Dong, Pravakar Roy, Cheng Peng, Volkan Isler", "title": "Ellipse R-CNN: Learning to Infer Elliptical Object from Clustering and\n  Occlusion", "comments": "18 pages, 20 figures, 7 tables", "journal-ref": null, "doi": "10.1109/TIP.2021.3050673", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images of heavily occluded objects in cluttered scenes, such as fruit\nclusters in trees, are hard to segment. To further retrieve the 3D size and 6D\npose of each individual object in such cases, bounding boxes are not reliable\nfrom multiple views since only a little portion of the object's geometry is\ncaptured. We introduce the first CNN-based ellipse detector, called Ellipse\nR-CNN, to represent and infer occluded objects as ellipses. We first propose a\nrobust and compact ellipse regression based on the Mask R-CNN architecture for\nelliptical object detection. Our method can infer the parameters of multiple\nelliptical objects even they are occluded by other neighboring objects. For\nbetter occlusion handling, we exploit refined feature regions for the\nregression stage, and integrate the U-Net structure for learning different\nocclusion patterns to compute the final detection score. The correctness of\nellipse regression is validated through experiments performed on synthetic data\nof clustered ellipses. We further quantitatively and qualitatively demonstrate\nthat our approach outperforms the state-of-the-art model (i.e., Mask R-CNN\nfollowed by ellipse fitting) and its three variants on both synthetic and real\ndatasets of occluded and clustered elliptical objects.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 22:04:54 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 21:05:09 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Dong", "Wenbo", ""], ["Roy", "Pravakar", ""], ["Peng", "Cheng", ""], ["Isler", "Volkan", ""]]}, {"id": "2001.11597", "submitter": "Stuart Eiffert", "authors": "Stuart Eiffert, He Kong, Navid Pirmarzdashti and Salah Sukkarieh", "title": "Path Planning in Dynamic Environments using Generative RNNs and Monte\n  Carlo Tree Search", "comments": "Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2020, video available at https://youtu.be/vBPKiqtCYRU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art methods for robotic path planning in dynamic environments,\nsuch as crowds or traffic, rely on hand crafted motion models for agents. These\nmodels often do not reflect interactions of agents in real world scenarios. To\novercome this limitation, this paper proposes an integrated path planning\nframework using generative Recurrent Neural Networks within a Monte Carlo Tree\nSearch (MCTS). This approach uses a learnt model of social response to predict\ncrowd dynamics during planning across the action space. This extends our recent\nwork using generative RNNs to learn the relationship between planned robotic\nactions and the likely response of a crowd. We show that the proposed framework\ncan considerably improve motion prediction accuracy during interactions,\nallowing more effective path planning. The performance of our method is\ncompared in simulation with existing methods for collision avoidance in a crowd\nof pedestrians, demonstrating the ability to control future states of nearby\nindividuals. We also conduct preliminary real world tests to validate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 22:46:37 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Eiffert", "Stuart", ""], ["Kong", "He", ""], ["Pirmarzdashti", "Navid", ""], ["Sukkarieh", "Salah", ""]]}, {"id": "2001.11610", "submitter": "Akkas Uddin Haque", "authors": "Akkas Haque, Ahmed Elsaharti, Tarek Elderini, Mohamed Atef Elsaharty,\n  and Jeremiah Neubert", "title": "UAV Autonomous Localization using Macro-Features Matching with a CAD\n  Model", "comments": null, "journal-ref": "Sensors 2020, 20, 743", "doi": "10.3390/s20030743", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research in the field of autonomous Unmanned Aerial Vehicles (UAVs) has\nsignificantly advanced in recent years, mainly due to their relevance in a\nlarge variety of commercial, industrial, and military applications. However,\nUAV navigation in GPS-denied environments continues to be a challenging problem\nthat has been tackled in recent research through sensor-based approaches. This\npaper presents a novel offline, portable, real-time in-door UAV localization\ntechnique that relies on macro-feature detection and matching. The proposed\nsystem leverages the support of machine learning, traditional computer vision\ntechniques, and pre-existing knowledge of the environment. The main\ncontribution of this work is the real-time creation of a macro-feature\ndescription vector from the UAV captured images which are simultaneously\nmatched with an offline pre-existing vector from a Computer-Aided Design (CAD)\nmodel. This results in a quick UAV localization within the CAD model. The\neffectiveness and accuracy of the proposed system were evaluated through\nsimulations and experimental prototype implementation. Final results reveal the\nalgorithm's low computational burden as well as its ease of deployment in\nGPS-denied environments.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 23:49:15 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Haque", "Akkas", ""], ["Elsaharti", "Ahmed", ""], ["Elderini", "Tarek", ""], ["Elsaharty", "Mohamed Atef", ""], ["Neubert", "Jeremiah", ""]]}, {"id": "2001.11612", "submitter": "Jindong Gu", "authors": "Jindong Gu, Volker Tresp", "title": "Search for Better Students to Learn Distilled Knowledge", "comments": null, "journal-ref": "24th European Conference on Artificial Intelligence (ECAI), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation, as a model compression technique, has received great\nattention. The knowledge of a well-performed teacher is distilled to a student\nwith a small architecture. The architecture of the small student is often\nchosen to be similar to their teacher's, with fewer layers or fewer channels,\nor both. However, even with the same number of FLOPs or parameters, the\nstudents with different architecture can achieve different generalization\nability. The configuration of a student architecture requires intensive network\narchitecture engineering. In this work, instead of designing a good student\narchitecture manually, we propose to search for the optimal student\nautomatically. Based on L1-norm optimization, a subgraph from the teacher\nnetwork topology graph is selected as a student, the goal of which is to\nminimize the KL-divergence between student's and teacher's outputs. We verify\nthe proposal on CIFAR10 and CIFAR100 datasets. The empirical experiments show\nthat the learned student architecture achieves better performance than ones\nspecified manually. We also visualize and understand the architecture of the\nfound student.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 23:55:15 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Gu", "Jindong", ""], ["Tresp", "Volker", ""]]}, {"id": "2001.11639", "submitter": "Caroline Potts", "authors": "Caroline Potts, Ethem F. Can, Aysu Ezen-Can, Xiangqian Hu", "title": "ParkingSticker: A Real-World Object Detection Dataset", "comments": "8 pages, 8 figures; Updated authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new and challenging object detection dataset, ParkingSticker,\nwhich mimics the type of data available in industry problems more closely than\npopular existing datasets like PASCAL VOC. ParkingSticker contains 1,871 images\nthat come from a security camera's video footage. The objective is to identify\nparking stickers on cars approaching a gate that the security camera faces.\nBounding boxes are drawn around parking stickers in the images. The parking\nstickers are much smaller on average than the objects in other popular object\ndetection datasets; this makes ParkingSticker a challenging test for object\ndetection methods. This dataset also very realistically represents the data\navailable in many industry problems where a customer presents a few video\nframes and asks for a solution to a very difficult problem. Performance of\nvarious object detection pipelines using a YOLOv2 architecture are presented\nand indicate that identifying the parking stickers in ParkingSticker is\nchallenging yet feasible. We believe that this dataset will challenge\nresearchers to solve a real-world problem with real-world constraints such as\nnon-ideal camera positioning and small object-size-to-image-size ratios.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 03:01:20 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 15:07:29 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Potts", "Caroline", ""], ["Can", "Ethem F.", ""], ["Ezen-Can", "Aysu", ""], ["Hu", "Xiangqian", ""]]}, {"id": "2001.11657", "submitter": "Sijie Song", "authors": "Sijie Song, Jiaying Liu, Yanghao Li, Zongming Guo", "title": "Modality Compensation Network: Cross-Modal Adaptation for Action\n  Recognition", "comments": "Accepted by IEEE Trans. on Image Processing, 2020. Project page:\n  http://39.96.165.147/Projects/MCN_tip2020_ssj/MCN_tip_2020_ssj.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of RGB-D cameras, multi-modal video data have become more\navailable for human action recognition. One main challenge for this task lies\nin how to effectively leverage their complementary information. In this work,\nwe propose a Modality Compensation Network (MCN) to explore the relationships\nof different modalities, and boost the representations for human action\nrecognition. We regard RGB/optical flow videos as source modalities, skeletons\nas auxiliary modality. Our goal is to extract more discriminative features from\nsource modalities, with the help of auxiliary modality. Built on deep\nConvolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks,\nour model bridges data from source and auxiliary modalities by a modality\nadaptation block to achieve adaptive representation learning, that the network\nlearns to compensate for the loss of skeletons at test time and even at\ntraining time. We explore multiple adaptation schemes to narrow the distance\nbetween source and auxiliary modal distributions from different levels,\naccording to the alignment of source and auxiliary data in training. In\naddition, skeletons are only required in the training phase. Our model is able\nto improve the recognition performance with source data when testing.\nExperimental results reveal that MCN outperforms state-of-the-art approaches on\nfour widely-used action recognition benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 04:51:55 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Song", "Sijie", ""], ["Liu", "Jiaying", ""], ["Li", "Yanghao", ""], ["Guo", "Zongming", ""]]}, {"id": "2001.11658", "submitter": "ByungSoo Ko", "authors": "Geonmo Gu, Byungsoo Ko", "title": "Symmetrical Synthesis for Deep Metric Learning", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning aims to learn embeddings that contain semantic\nsimilarity information among data points. To learn better embeddings, methods\nto generate synthetic hard samples have been proposed. Existing methods of\nsynthetic hard sample generation are adopting autoencoders or generative\nadversarial networks, but this leads to more hyper-parameters, harder\noptimization, and slower training speed. In this paper, we address these\nproblems by proposing a novel method of synthetic hard sample generation called\nsymmetrical synthesis. Given two original feature points from the same class,\nthe proposed method firstly generates synthetic points with each other as an\naxis of symmetry. Secondly, it performs hard negative pair mining within the\noriginal and synthetic points to select a more informative negative pair for\ncomputing the metric learning loss. Our proposed method is hyper-parameter free\nand plug-and-play for existing metric learning losses without network\nmodification. We demonstrate the superiority of our proposed method over\nexisting methods for a variety of loss functions on clustering and image\nretrieval tasks. Our implementations is publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 04:56:47 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 08:13:08 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 06:17:18 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Gu", "Geonmo", ""], ["Ko", "Byungsoo", ""]]}, {"id": "2001.11673", "submitter": "Mehrdad Alizadeh", "authors": "Mehrdad Alizadeh, Barbara Di Eugenio", "title": "Augmenting Visual Question Answering with Semantic Frame Information in\n  a Multitask Learning Approach", "comments": "14th IEEE International Conference on SEMANTIC COMPUTING, 8 Pages,\n  February 2020, San Diego CA USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual Question Answering (VQA) concerns providing answers to Natural\nLanguage questions about images. Several deep neural network approaches have\nbeen proposed to model the task in an end-to-end fashion. Whereas the task is\ngrounded in visual processing, if the question focuses on events described by\nverbs, the language understanding component becomes crucial. Our hypothesis is\nthat models should be aware of verb semantics, as expressed via semantic role\nlabels, argument types, and/or frame elements. Unfortunately, no VQA dataset\nexists that includes verb semantic information. Our first contribution is a new\nVQA dataset (imSituVQA) that we built by taking advantage of the imSitu\nannotations. The imSitu dataset consists of images manually labeled with\nsemantic frame elements, mostly taken from FrameNet. Second, we propose a\nmultitask CNN-LSTM VQA model that learns to classify the answers as well as the\nsemantic frame elements. Our experiments show that semantic frame element\nclassification helps the VQA system avoid inconsistent responses and improves\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 06:31:39 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Alizadeh", "Mehrdad", ""], ["Di Eugenio", "Barbara", ""]]}, {"id": "2001.11684", "submitter": "Ben Talbot", "authors": "Ben Talbot, Feras Dayoub, Peter Corke, Gordon Wyeth", "title": "Robot Navigation in Unseen Spaces using an Abstract Map", "comments": "15 pages, published in IEEE Transactions on Cognitive and\n  Developmental Systems (http://doi.org/10.1109/TCDS.2020.2993855), see\n  https://btalb.github.io/abstract_map/ for access to software", "journal-ref": null, "doi": "10.1109/TCDS.2020.2993855", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human navigation in built environments depends on symbolic spatial\ninformation which has unrealised potential to enhance robot navigation\ncapabilities. Information sources such as labels, signs, maps, planners, spoken\ndirections, and navigational gestures communicate a wealth of spatial\ninformation to the navigators of built environments; a wealth of information\nthat robots typically ignore. We present a robot navigation system that uses\nthe same symbolic spatial information employed by humans to purposefully\nnavigate in unseen built environments with a level of performance comparable to\nhumans. The navigation system uses a novel data structure called the abstract\nmap to imagine malleable spatial models for unseen spaces from spatial symbols.\nSensorimotor perceptions from a robot are then employed to provide purposeful\nnavigation to symbolic goal locations in the unseen environment. We show how a\ndynamic system can be used to create malleable spatial models for the abstract\nmap, and provide an open source implementation to encourage future work in the\narea of symbolic navigation. Symbolic navigation performance of humans and a\nrobot is evaluated in a real-world built environment. The paper concludes with\na qualitative analysis of human navigation strategies, providing further\ninsights into how the symbolic navigation capabilities of robots in unseen\nbuilt environments can be improved in the future.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 07:40:44 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 05:31:34 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Talbot", "Ben", ""], ["Dayoub", "Feras", ""], ["Corke", "Peter", ""], ["Wyeth", "Gordon", ""]]}, {"id": "2001.11690", "submitter": "Yu Lu", "authors": "Yu Lu, Muyan Feng, Ming Wu, Chuang Zhang", "title": "C-DLinkNet: considering multi-level semantic features for human parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human parsing is an essential branch of semantic segmentation, which is a\nfine-grained semantic segmentation task to identify the constituent parts of\nhuman. The challenge of human parsing is to extract effective semantic features\nto resolve deformation and multi-scale variations. In this work, we proposed an\nend-to-end model called C-DLinkNet based on LinkNet, which contains a new\nmodule named Smooth Module to combine the multi-level features in Decoder part.\nC-DLinkNet is capable of producing competitive parsing performance compared\nwith the state-of-the-art methods with smaller input sizes and no additional\ninformation, i.e., achiving mIoU=53.05 on the validation set of LIP dataset.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 07:47:45 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 11:12:12 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Lu", "Yu", ""], ["Feng", "Muyan", ""], ["Wu", "Ming", ""], ["Zhang", "Chuang", ""]]}, {"id": "2001.11698", "submitter": "Zhaotao Wu", "authors": "Zhaotao Wu and Jia Wei and Wenguang Yuan and Jiabing Wang and Tolga\n  Tasdizen", "title": "Inter-slice image augmentation based on frame interpolation for boosting\n  medical image segmentation accuracy", "comments": null, "journal-ref": null, "doi": "10.3233/FAIA200314", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the idea of inter-slice image augmentation whereby the numbers\nof the medical images and the corresponding segmentation labels are increased\nbetween two consecutive images in order to boost medical image segmentation\naccuracy. Unlike conventional data augmentation methods in medical imaging,\nwhich only increase the number of training samples directly by adding new\nvirtual samples using simple parameterized transformations such as rotation,\nflipping, scaling, etc., we aim to augment data based on the relationship\nbetween two consecutive images, which increases not only the number but also\nthe information of training samples. For this purpose, we propose a\nframe-interpolation-based data augmentation method to generate intermediate\nmedical images and the corresponding segmentation labels between two\nconsecutive images. We train and test a supervised U-Net liver segmentation\nnetwork on SLIVER07 and CHAOS2019, respectively, with the augmented training\nsamples, and obtain segmentation scores exhibiting significant improvement\ncompared to the conventional augmentation methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 08:12:46 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Wu", "Zhaotao", ""], ["Wei", "Jia", ""], ["Yuan", "Wenguang", ""], ["Wang", "Jiabing", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "2001.11708", "submitter": "Liang Liao", "authors": "Liang Liao and Stephen John Maybank", "title": "Generalized Visual Information Analysis via Tensorial Algebra", "comments": "42 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.AC math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher order data is modeled using matrices whose entries are numerical\narrays of a fixed size. These arrays, called t-scalars, form a commutative ring\nunder the convolution product. Matrices with elements in the ring of t-scalars\nare referred to as t-matrices. The t-matrices can be scaled, added and\nmultiplied in the usual way. There are t-matrix generalizations of positive\nmatrices, orthogonal matrices and Hermitian symmetric matrices. With the\nt-matrix model, it is possible to generalize many well-known matrix algorithms.\nIn particular, the t-matrices are used to generalize the SVD (Singular Value\nDecomposition), HOSVD (High Order SVD), PCA (Principal Component Analysis),\n2DPCA (Two Dimensional PCA) and GCA (Grassmannian Component Analysis). The\ngeneralized t-matrix algorithms, namely TSVD, THOSVD,TPCA, T2DPCA and TGCA, are\napplied to low-rank approximation, reconstruction,and supervised classification\nof images. Experiments show that the t-matrix algorithms compare favorably with\nstandard matrix algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 08:47:35 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 10:58:56 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liao", "Liang", ""], ["Maybank", "Stephen John", ""]]}, {"id": "2001.11711", "submitter": "Esther Puyol-Anton Dr", "authors": "Esther Puyol Anton, Bram Ruijsink, Christian F. Baumgartner, Matthew\n  Sinclair, Ender Konukoglu, Reza Razavi, Andrew P. King", "title": "Automated quantification of myocardial tissue characteristics from\n  native T1 mapping using neural networks with Bayesian inference for\n  uncertainty-based quality-control", "comments": null, "journal-ref": null, "doi": "10.1186/s12968-020-00650-y", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tissue characterisation with CMR parametric mapping has the potential to\ndetect and quantify both focal and diffuse alterations in myocardial structure\nnot assessable by late gadolinium enhancement. Native T1 mapping in particular\nhas shown promise as a useful biomarker to support diagnostic, therapeutic and\nprognostic decision-making in ischaemic and non-ischaemic cardiomyopathies.\nConvolutional neural networks with Bayesian inference are a category of\nartificial neural networks which model the uncertainty of the network output.\nThis study presents an automated framework for tissue characterisation from\nnative ShMOLLI T1 mapping at 1.5T using a Probabilistic Hierarchical\nSegmentation (PHiSeg) network. In addition, we use the uncertainty information\nprovided by the PHiSeg network in a novel automated quality control (QC) step\nto identify uncertain T1 values. The PHiSeg network and QC were validated\nagainst manual analysis on a cohort of the UK Biobank containing healthy\nsubjects and chronic cardiomyopathy patients. We used the proposed method to\nobtain reference T1 ranges for the left ventricular myocardium in healthy\nsubjects as well as common clinical cardiac conditions. T1 values computed from\nautomatic and manual segmentations were highly correlated (r=0.97).\nBland-Altman analysis showed good agreement between the automated and manual\nmeasurements. The average Dice metric was 0.84 for the left ventricular\nmyocardium. The sensitivity of detection of erroneous outputs was 91%. Finally,\nT1 values were automatically derived from 14,683 CMR exams from the UK Biobank.\nThe proposed pipeline allows for automatic analysis of myocardial native T1\nmapping and includes a QC process to detect potentially erroneous results. T1\nreference values were presented for healthy subjects and common clinical\ncardiac conditions from the largest cohort to date using T1-mapping images.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 08:51:33 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Anton", "Esther Puyol", ""], ["Ruijsink", "Bram", ""], ["Baumgartner", "Christian F.", ""], ["Sinclair", "Matthew", ""], ["Konukoglu", "Ender", ""], ["Razavi", "Reza", ""], ["King", "Andrew P.", ""]]}, {"id": "2001.11715", "submitter": "Zhibo Liu", "authors": "Zhibo Liu, Feng Gao, Yizhou Wang", "title": "A Generative Adversarial Network for AI-Aided Chair Design", "comments": "6 pages, 5 figures, accepted at MIPR2019", "journal-ref": null, "doi": "10.1109/MIPR.2019.00098", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for improving human design of chairs. The goal of the\nmethod is generating enormous chair candidates in order to facilitate human\ndesigner by creating sketches and 3d models accordingly based on the generated\nchair design. It consists of an image synthesis module, which learns the\nunderlying distribution of training dataset, a super-resolution module, which\nimprove quality of generated image and human involvements. Finally, we manually\npick one of the generated candidates to create a real life chair for\nillustration.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 08:57:32 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Liu", "Zhibo", ""], ["Gao", "Feng", ""], ["Wang", "Yizhou", ""]]}, {"id": "2001.11737", "submitter": "Ilker Bozcan", "authors": "Ilker Bozcan and Erdal Kayacan", "title": "AU-AIR: A Multi-modal Unmanned Aerial Vehicle Dataset for Low Altitude\n  Traffic Surveillance", "comments": "7 pages, 8 figures, 3 tables; for the associated dataset, see\n  http://bozcani.github.io/auairdataset accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAVs) with mounted cameras have the advantage of\ncapturing aerial (bird-view) images. The availability of aerial visual data and\nthe recent advances in object detection algorithms led the computer vision\ncommunity to focus on object detection tasks on aerial images. As a result of\nthis, several aerial datasets have been introduced, including visual data with\nobject annotations. UAVs are used solely as flying-cameras in these datasets,\ndiscarding different data types regarding the flight (e.g., time, location,\ninternal sensors). In this work, we propose a multi-purpose aerial dataset\n(AU-AIR) that has multi-modal sensor data (i.e., visual, time, location,\naltitude, IMU, velocity) collected in real-world outdoor environments. The\nAU-AIR dataset includes meta-data for extracted frames (i.e., bounding box\nannotations for traffic-related object category) from recorded RGB videos.\nMoreover, we emphasize the differences between natural and aerial images in the\ncontext of object detection task. For this end, we train and test mobile object\ndetectors (including YOLOv3-Tiny and MobileNetv2-SSDLite) on the AU-AIR\ndataset, which are applicable for real-time object detection using on-board\ncomputers with UAVs. Since our dataset has diversity in recorded data types, it\ncontributes to filling the gap between computer vision and robotics. The\ndataset is available at https://bozcani.github.io/auairdataset.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 09:45:12 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 07:04:25 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Bozcan", "Ilker", ""], ["Kayacan", "Erdal", ""]]}, {"id": "2001.11759", "submitter": "Teguh Santoso Lembono", "authors": "Antonio Paolillo, Teguh Santoso Lembono, Sylvain Calinon", "title": "A memory of motion for visual predictive control tasks", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of efficiently achieving visual predictive\ncontrol tasks. To this end, a memory of motion, containing a set of\ntrajectories built off-line, is used for leveraging precomputation and dealing\nwith difficult visual tasks. Standard regression techniques, such as k-nearest\nneighbors and Gaussian process regression, are used to query the memory and\nprovide on-line a warm-start and a way point to the control optimization\nprocess. The proposed technique allows the control scheme to achieve high\nperformance and, at the same time, keep the computational time limited.\nSimulation and experimental results, carried out with a 7-axis manipulator,\nshow the effectiveness of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 10:45:04 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 16:10:12 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 09:51:18 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Paolillo", "Antonio", ""], ["Lembono", "Teguh Santoso", ""], ["Calinon", "Sylvain", ""]]}, {"id": "2001.11761", "submitter": "Milad Mozafari", "authors": "Milad Mozafari, Leila Reddy, Rufin VanRullen", "title": "Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN", "comments": "Accepted to IEEE IJCNN2020", "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9206960", "report-no": null, "categories": "cs.CV cs.HC eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding and reconstructing images from brain imaging data is a research area\nof high interest. Recent progress in deep generative neural networks has\nintroduced new opportunities to tackle this problem. Here, we employ a recently\nproposed large-scale bi-directional generative adversarial network, called\nBigBiGAN, to decode and reconstruct natural scenes from fMRI patterns. BigBiGAN\nconverts images into a 120-dimensional latent space which encodes class and\nattribute information together, and can also reconstruct images based on their\nlatent vectors. We computed a linear mapping between fMRI data, acquired over\nimages from 150 different categories of ImageNet, and their corresponding\nBigBiGAN latent vectors. Then, we applied this mapping to the fMRI activity\npatterns obtained from 50 new test images from 50 unseen categories in order to\nretrieve their latent vectors, and reconstruct the corresponding images.\nPairwise image decoding from the predicted latent vectors was highly accurate\n(84%). Moreover, qualitative and quantitative assessments revealed that the\nresulting image reconstructions were visually plausible, successfully captured\nmany attributes of the original images, and had high perceptual similarity with\nthe original content. This method establishes a new state-of-the-art for\nfMRI-based natural image reconstruction, and can be flexibly updated to take\ninto account any future improvements in generative models of natural scene\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 10:46:59 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 12:12:57 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 20:15:46 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Mozafari", "Milad", ""], ["Reddy", "Leila", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2001.11767", "submitter": "Johannes Hofmanninger", "authors": "Johannes Hofmanninger, Florian Prayer, Jeanny Pan, Sebastian Rohrich,\n  Helmut Prosch and Georg Langs", "title": "Automatic lung segmentation in routine imaging is primarily a data\n  diversity problem, not a methodology problem", "comments": "10 pages, 5 figures, 5 tables", "journal-ref": "Eur Radiol Exp 4, 50 (2020)", "doi": "10.1186/s41747-020-00173-2", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation of anatomical structures is a crucial step in image\nanalysis. For lung segmentation in computed tomography, a variety of approaches\nexist, involving sophisticated pipelines trained and validated on different\ndatasets. However, the clinical applicability of these approaches across\ndiseases remains limited. We compared four generic deep learning approaches\ntrained on various datasets and two readily available lung segmentation\nalgorithms. We performed evaluation on routine imaging data with more than six\ndifferent disease patterns and three published data sets. Using different deep\nlearning approaches, mean Dice similarity coefficients (DSCs) on test datasets\nvaried not over 0.02. When trained on a diverse routine dataset (n = 36) a\nstandard approach (U-net) yields a higher DSC (0.97 $\\pm$ 0.05) compared to\ntraining on public datasets such as Lung Tissue Research Consortium (0.94 $\\pm$\n0.13, p = 0.024) or Anatomy 3 (0.92 $\\pm$ 0.15, p = 0.001). Trained on routine\ndata (n = 231) covering multiple diseases, U-net compared to reference methods\nyields a DSC of 0.98 $\\pm$ 0.03 versus 0.94 $\\pm$ 0.12 (p = 0.024).\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 11:01:35 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 15:02:26 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Hofmanninger", "Johannes", ""], ["Prayer", "Florian", ""], ["Pan", "Jeanny", ""], ["Rohrich", "Sebastian", ""], ["Prosch", "Helmut", ""], ["Langs", "Georg", ""]]}, {"id": "2001.11782", "submitter": "Zhengxiong Jia", "authors": "Zhengxiong Jia and Xirong Li", "title": "iCap: Interactive Image Captioning with Predictive Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a brand new topic of interactive image captioning with\nhuman in the loop. Different from automated image captioning where a given test\nimage is the sole input in the inference stage, we have access to both the test\nimage and a sequence of (incomplete) user-input sentences in the interactive\nscenario. We formulate the problem as Visually Conditioned Sentence Completion\n(VCSC). For VCSC, we propose asynchronous bidirectional decoding for image\ncaption completion (ABD-Cap). With ABD-Cap as the core module, we build iCap, a\nweb-based interactive image captioning system capable of predicting new text\nwith respect to live input from a user. A number of experiments covering both\nautomated evaluations and real user studies show the viability of our\nproposals.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 11:33:12 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 08:36:31 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 04:15:55 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Jia", "Zhengxiong", ""], ["Li", "Xirong", ""]]}, {"id": "2001.11801", "submitter": "Allard Hendriksen", "authors": "Allard A. Hendriksen, Daniel M. Pelt and K. Joost Batenburg", "title": "Noise2Inverse: Self-supervised deep convolutional denoising for\n  tomography", "comments": "This paper appears in: IEEE Transactions on Computational Imaging On\n  page(s): 1320-1335 Print ISSN: 2333-9403 Online ISSN: 2333-9403 Digital\n  Object Identifier: 10.1109/TCI.2020.3019647", "journal-ref": null, "doi": "10.1109/TCI.2020.3019647", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a high-quality image from noisy indirect measurements is an\nimportant problem with many applications. For such inverse problems, supervised\ndeep convolutional neural network (CNN)-based denoising methods have shown\nstrong results, but the success of these supervised methods critically depends\non the availability of a high-quality training dataset of similar measurements.\nFor image denoising, methods are available that enable training without a\nseparate training dataset by assuming that the noise in two different pixels is\nuncorrelated. However, this assumption does not hold for inverse problems,\nresulting in artifacts in the denoised images produced by existing methods.\nHere, we propose Noise2Inverse, a deep CNN-based denoising method for linear\nimage reconstruction algorithms that does not require any additional clean or\nnoisy data. Training a CNN-based denoiser is enabled by exploiting the noise\nmodel to compute multiple statistically independent reconstructions. We develop\na theoretical framework which shows that such training indeed obtains a\ndenoising CNN, assuming the measured noise is element-wise independent and\nzero-mean. On simulated CT datasets, Noise2Inverse demonstrates an improvement\nin peak signal-to-noise ratio and structural similarity index compared to\nstate-of-the-art image denoising methods and conventional reconstruction\nmethods, such as Total-Variation Minimization. We also demonstrate that the\nmethod is able to significantly reduce noise in challenging real-world\nexperimental datasets.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 12:50:24 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 13:25:56 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 08:27:07 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Hendriksen", "Allard A.", ""], ["Pelt", "Daniel M.", ""], ["Batenburg", "K. Joost", ""]]}, {"id": "2001.11845", "submitter": "Seyed Hamid Rezatofighi", "authors": "Hamid Rezatofighi, Roman Kaskman, Farbod T. Motlagh, Qinfeng Shi,\n  Anton Milan, Daniel Cremers, Laura Leal-Taix\\'e, Ian Reid", "title": "Learn to Predict Sets Using Feed-Forward Neural Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.00613", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of set prediction using deep feed-forward\nneural networks. A set is a collection of elements which is invariant under\npermutation and the size of a set is not fixed in advance. Many real-world\nproblems, such as image tagging and object detection, have outputs that are\nnaturally expressed as sets of entities. This creates a challenge for\ntraditional deep neural networks which naturally deal with structured outputs\nsuch as vectors, matrices or tensors. We present a novel approach for learning\nto predict sets with unknown permutation and cardinality using deep neural\nnetworks. In our formulation we define a likelihood for a set distribution\nrepresented by a) two discrete distributions defining the set cardinally and\npermutation variables, and b) a joint distribution over set elements with a\nfixed cardinality. Depending on the problem under consideration, we define\ndifferent training models for set prediction using deep neural networks. We\ndemonstrate the validity of our set formulations on relevant vision problems\nsuch as: 1)multi-label image classification where we achieve state-of-the-art\nperformance on the PASCAL VOC and MS COCO datasets, 2) object detection, for\nwhich our formulation outperforms state-of-the-art detectors such as Faster\nR-CNN and YOLO v3, and 3) a complex CAPTCHA test, where we observe that,\nsurprisingly, our set-based network acquired the ability of mimicking\narithmetics without any rules being coded.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 01:52:07 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Rezatofighi", "Hamid", ""], ["Kaskman", "Roman", ""], ["Motlagh", "Farbod T.", ""], ["Shi", "Qinfeng", ""], ["Milan", "Anton", ""], ["Cremers", "Daniel", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Reid", "Ian", ""]]}, {"id": "2001.11869", "submitter": "Chuang Wang", "authors": "Chuang Wang, Ruimin Hu, Min Hu, Jiang Liu, Ting Ren, Shan He, Ming\n  Jiang, Jing Miao", "title": "Lossless Attention in Convolutional Networks for Facial Expression\n  Recognition in the Wild", "comments": "5 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike the constraint frontal face condition, faces in the wild have various\nunconstrained interference factors, such as complex illumination, changing\nperspective and various occlusions. Facial expressions recognition (FER) in the\nwild is a challenging task and existing methods can't perform well. However,\nfor occluded faces (containing occlusion caused by other objects and\nself-occlusion caused by head posture changes), the attention mechanism has the\nability to focus on the non-occluded regions automatically. In this paper, we\npropose a Lossless Attention Model (LLAM) for convolutional neural networks\n(CNN) to extract attention-aware features from faces. Our module avoids decay\ninformation in the process of generating attention maps by using the\ninformation of the previous layer and not reducing the dimensionality.\nSequentially, we adaptively refine the feature responses by fusing the\nattention map with the feature map. We participate in the seven basic\nexpression classification sub-challenges of FG-2020 Affective Behavior Analysis\nin-the-wild Challenge. And we validate our method on the Aff-Wild2 datasets\nreleased by the Challenge. The total accuracy (Accuracy) and the unweighted\nmean (F1) of our method on the validation set are 0.49 and 0.38 respectively,\nand the final result is 0.42 (0.67 F1-Score + 0.33 Accuracy).\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 14:38:35 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Wang", "Chuang", ""], ["Hu", "Ruimin", ""], ["Hu", "Min", ""], ["Liu", "Jiang", ""], ["Ren", "Ting", ""], ["He", "Shan", ""], ["Jiang", "Ming", ""], ["Miao", "Jing", ""]]}, {"id": "2001.11921", "submitter": "Hossein Adeli", "authors": "Gregory J. Zelinsky, Yupei Chen, Seoyoung Ahn, Hossein Adeli, Zhibo\n  Yang, Lihan Huang, Dimitrios Samaras, Minh Hoai", "title": "Predicting Goal-directed Attention Control Using Inverse-Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding how goal states control behavior is a question ripe for\ninterrogation by new methods from machine learning. These methods require large\nand labeled datasets to train models. To annotate a large-scale image dataset\nwith observed search fixations, we collected 16,184 fixations from people\nsearching for either microwaves or clocks in a dataset of 4,366 images\n(MS-COCO). We then used this behaviorally-annotated dataset and the machine\nlearning method of Inverse-Reinforcement Learning (IRL) to learn\ntarget-specific reward functions and policies for these two target goals.\nFinally, we used these learned policies to predict the fixations of 60 new\nbehavioral searchers (clock = 30, microwave = 30) in a disjoint test dataset of\nkitchen scenes depicting both a microwave and a clock (thus controlling for\ndifferences in low-level image contrast). We found that the IRL model predicted\nbehavioral search efficiency and fixation-density maps using multiple metrics.\nMoreover, reward maps from the IRL model revealed target-specific patterns that\nsuggest, not just attention guidance by target features, but also guidance by\nscene context (e.g., fixations along walls in the search of clocks). Using\nmachine learning and the psychologically-meaningful principle of reward, it is\npossible to learn the visual features used in goal-directed attention control.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 15:53:52 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Zelinsky", "Gregory J.", ""], ["Chen", "Yupei", ""], ["Ahn", "Seoyoung", ""], ["Adeli", "Hossein", ""], ["Yang", "Zhibo", ""], ["Huang", "Lihan", ""], ["Samaras", "Dimitrios", ""], ["Hoai", "Minh", ""]]}, {"id": "2001.11927", "submitter": "Richard Shaw", "authors": "Richard Shaw and Carole H. Sudre and Sebastien Ourselin and M. Jorge\n  Cardoso", "title": "A Heteroscedastic Uncertainty Model for Decoupling Sources of MRI Image\n  Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality control (QC) of medical images is essential to ensure that downstream\nanalyses such as segmentation can be performed successfully. Currently, QC is\npredominantly performed visually at significant time and operator cost. We aim\nto automate the process by formulating a probabilistic network that estimates\nuncertainty through a heteroscedastic noise model, hence providing a proxy\nmeasure of task-specific image quality that is learnt directly from the data.\nBy augmenting the training data with different types of simulated k-space\nartefacts, we propose a novel cascading CNN architecture based on a\nstudent-teacher framework to decouple sources of uncertainty related to\ndifferent k-space augmentations in an entirely self-supervised manner. This\nenables us to predict separate uncertainty quantities for the different types\nof data degradation. While the uncertainty measures reflect the presence and\nseverity of image artefacts, the network also provides the segmentation\npredictions given the quality of the data. We show models trained with\nsimulated artefacts provide informative measures of uncertainty on real-world\nimages and we validate our uncertainty predictions on problematic images\nidentified by human-raters.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 16:04:41 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Shaw", "Richard", ""], ["Sudre", "Carole H.", ""], ["Ourselin", "Sebastien", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "2001.11935", "submitter": "Chunping Qiu", "authors": "C. Qiu and M. Schmitt and C. Geiss and T. K. Chen and X. X. Zhu", "title": "A framework for large-scale mapping of human settlement extent from\n  Sentinel-2 images via fully convolutional neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.isprsjprs.2020.01.028", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human settlement extent (HSE) information is a valuable indicator of\nworld-wide urbanization as well as the resulting human pressure on the natural\nenvironment. Therefore, mapping HSE is critical for various environmental\nissues at local, regional, and even global scales. This paper presents a\ndeep-learning-based framework to automatically map HSE from multi-spectral\nSentinel-2 data using regionally available geo-products as training labels. A\nstraightforward, simple, yet effective fully convolutional network-based\narchitecture, Sen2HSE, is implemented as an example for semantic segmentation\nwithin the framework. The framework is validated against both manually labelled\nchecking points distributed evenly over the test areas, and the OpenStreetMap\nbuilding layer. The HSE mapping results were extensively compared to several\nbaseline products in order to thoroughly evaluate the effectiveness of the\nproposed HSE mapping framework. The HSE mapping power is consistently\ndemonstrated over 10 representative areas across the world. We also present one\nregional-scale and one country-wide HSE mapping example from our framework to\nshow the potential for upscaling. The results of this study contribute to the\ngeneralization of the applicability of CNN-based approaches for large-scale\nurban mapping to cases where no up-to-date and accurate ground truth is\navailable, as well as the subsequent monitor of global urbanization.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 16:23:34 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Qiu", "C.", ""], ["Schmitt", "M.", ""], ["Geiss", "C.", ""], ["Chen", "T. K.", ""], ["Zhu", "X. X.", ""]]}, {"id": "2001.11951", "submitter": "S.H.Shabbeer Basha", "authors": "S.H.Shabbeer Basha, Sravan Kumar Vinakota, Shiv Ram Dubey, Viswanath\n  Pulabaigari, Snehasis Mukherjee", "title": "AutoFCL: Automatically Tuning Fully Connected Layers for Handling Small\n  Dataset", "comments": "This paper is published in Neural Computing & Applications Journal", "journal-ref": null, "doi": "10.1007/s00521-020-05549-4", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) have evolved as popular machine\nlearning models for image classification during the past few years, due to\ntheir ability to learn the problem-specific features directly from the input\nimages. The success of deep learning models solicits architecture engineering\nrather than hand-engineering the features. However, designing state-of-the-art\nCNN for a given task remains a non-trivial and challenging task, especially\nwhen training data size is less. To address this phenomena, transfer learning\nhas been used as a popularly adopted technique. While transferring the learned\nknowledge from one task to another, fine-tuning with the target-dependent Fully\nConnected (FC) layers generally produces better results over the target task.\nIn this paper, the proposed AutoFCL model attempts to learn the structure of FC\nlayers of a CNN automatically using Bayesian optimization. To evaluate the\nperformance of the proposed AutoFCL, we utilize five pre-trained CNN models\nsuch as VGG-16, ResNet, DenseNet, MobileNet, and NASNetMobile. The experiments\nare conducted on three benchmark datasets, namely CalTech-101, Oxford-102\nFlowers, and UC Merced Land Use datasets. Fine-tuning the newly learned\n(target-dependent) FC layers leads to state-of-the-art performance, according\nto the experiments carried out in this research. The proposed AutoFCL method\noutperforms the existing methods over CalTech-101 and Oxford-102 Flowers\ndatasets by achieving the accuracy of 94.38% and 98.89%, respectively. However,\nour method achieves comparable performance on the UC Merced Land Use dataset\nwith 96.83% accuracy. The source codes of this research are available at\nhttps://github.com/shabbeersh/AutoFCL.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 08:39:00 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 05:35:49 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 11:36:39 GMT"}, {"version": "v4", "created": "Thu, 28 Jan 2021 17:05:06 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Basha", "S. H. Shabbeer", ""], ["Vinakota", "Sravan Kumar", ""], ["Dubey", "Shiv Ram", ""], ["Pulabaigari", "Viswanath", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "2001.11976", "submitter": "Sevegni Allognon", "authors": "Sevegni Odilon Clement Allognon, Alessandro L. Koerich, Alceu de S.\n  Britto Jr", "title": "Continuous Emotion Recognition via Deep Convolutional Autoencoder and\n  Support Vector Regressor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic facial expression recognition is an important research area in the\nemotion recognition and computer vision. Applications can be found in several\ndomains such as medical treatment, driver fatigue surveillance, sociable\nrobotics, and several other human-computer interaction systems. Therefore, it\nis crucial that the machine should be able to recognize the emotional state of\nthe user with high accuracy. In recent years, deep neural networks have been\nused with great success in recognizing emotions. In this paper, we present a\nnew model for continuous emotion recognition based on facial expression\nrecognition by using an unsupervised learning approach based on transfer\nlearning and autoencoders. The proposed approach also includes preprocessing\nand post-processing techniques which contribute favorably to improving the\nperformance of predicting the concordance correlation coefficient for arousal\nand valence dimensions. Experimental results for predicting spontaneous and\nnatural emotions on the RECOLA 2016 dataset have shown that the proposed\napproach based on visual information can achieve CCCs of 0.516 and 0.264 for\nvalence and arousal, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 17:47:16 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Allognon", "Sevegni Odilon Clement", ""], ["Koerich", "Alessandro L.", ""], ["Britto", "Alceu de S.", "Jr"]]}, {"id": "2001.12010", "submitter": "Jun-Jie Huang", "authors": "Jun-Jie Huang and Pier Luigi Dragotti", "title": "Learning Deep Analysis Dictionaries for Image Super-Resolution", "comments": "Accepted by IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2020.3036902", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent success of deep neural networks and the recent efforts\nto develop multi-layer dictionary models, we propose a Deep Analysis dictionary\nModel (DeepAM) which is optimized to address a specific regression task known\nas single image super-resolution. Contrary to other multi-layer dictionary\nmodels, our architecture contains L layers of analysis dictionary and\nsoft-thresholding operators to gradually extract high-level features and a\nlayer of synthesis dictionary which is designed to optimize the regression task\nat hand. In our approach, each analysis dictionary is partitioned into two\nsub-dictionaries: an Information Preserving Analysis Dictionary (IPAD) and a\nClustering Analysis Dictionary (CAD). The IPAD together with the corresponding\nsoft-thresholds is designed to pass the key information from the previous layer\nto the next layer, while the CAD together with the corresponding\nsoft-thresholding operator is designed to produce a sparse feature\nrepresentation of its input data that facilitates discrimination of key\nfeatures. DeepAM uses both supervised and unsupervised setup. Simulation\nresults show that the proposed deep analysis dictionary model achieves better\nperformance compared to a deep neural network that has the same structure and\nis optimized using back-propagation when training datasets are small.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 18:59:35 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 06:37:37 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Huang", "Jun-Jie", ""], ["Dragotti", "Pier Luigi", ""]]}]