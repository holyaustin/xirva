[{"id": "1409.0083", "submitter": "Mehrtash Harandi", "authors": "Mehrtash Harandi, Richard Hartley, Brian Lovell, Conrad Sanderson", "title": "Sparse Coding on Symmetric Positive Definite Manifolds using Bregman\n  Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces sparse coding and dictionary learning for Symmetric\nPositive Definite (SPD) matrices, which are often used in machine learning,\ncomputer vision and related areas. Unlike traditional sparse coding schemes\nthat work in vector spaces, in this paper we discuss how SPD matrices can be\ndescribed by sparse combination of dictionary atoms, where the atoms are also\nSPD matrices. We propose to seek sparse coding by embedding the space of SPD\nmatrices into Hilbert spaces through two types of Bregman matrix divergences.\nThis not only leads to an efficient way of performing sparse coding, but also\nan online and iterative scheme for dictionary learning. We apply the proposed\nmethods to several computer vision tasks where images are represented by region\ncovariance matrices. Our proposed algorithms outperform state-of-the-art\nmethods on a wide range of classification tasks, including face recognition,\naction recognition, material classification and texture categorization.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 04:46:43 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Harandi", "Mehrtash", ""], ["Hartley", "Richard", ""], ["Lovell", "Brian", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1409.0084", "submitter": "Mehrtash Harandi", "authors": "Mehrtash Harandi, Mathieu Salzmann", "title": "Kernel Coding: General Formulation and Special Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing images by compact codes has proven beneficial for many visual\nrecognition tasks. Most existing techniques, however, perform this coding step\ndirectly in image feature space, where the distributions of the different\nclasses are typically entangled. In contrast, here, we study the problem of\nperforming coding in a high-dimensional Hilbert space, where the classes are\nexpected to be more easily separable. To this end, we introduce a general\ncoding formulation that englobes the most popular techniques, such as bag of\nwords, sparse coding and locality-based coding, and show how this formulation\nand its special cases can be kernelized. Importantly, we address several\naspects of learning in our general formulation, such as kernel learning,\ndictionary learning and supervised kernel coding. Our experimental evaluation\non several visual recognition tasks demonstrates the benefits of performing\ncoding in Hilbert space, and in particular of jointly learning the kernel, the\ndictionary and the classifier.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 05:01:15 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Harandi", "Mehrtash", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1409.0177", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Jamie L. Hanson, Jieping Ye, Richard J. Davidson, Seth\n  D. Pollak", "title": "Persistent Homology in Sparse Regression and its Application to Brain\n  Morphometry", "comments": "submitted to IEEE Transactions on Medical Imaging", "journal-ref": "IEEE Transactions on Medical Imaging 2015 34:1928-1939", "doi": "10.1109/TMI.2015.2416271", "report-no": null, "categories": "stat.ME cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse systems are usually parameterized by a tuning parameter that\ndetermines the sparsity of the system. How to choose the right tuning parameter\nis a fundamental and difficult problem in learning the sparse system. In this\npaper, by treating the the tuning parameter as an additional dimension,\npersistent homological structures over the parameter space is introduced and\nexplored. The structures are then further exploited in speeding up the\ncomputation using the proposed soft-thresholding technique. The topological\nstructures are further used as multivariate features in the tensor-based\nmorphometry (TBM) in characterizing white matter alterations in children who\nhave experienced severe early life stress and maltreatment. These analyses\nreveal that stress-exposed children exhibit more diffuse anatomical\norganization across the whole white matter region.\n", "versions": [{"version": "v1", "created": "Sun, 31 Aug 2014 02:15:33 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 04:35:11 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Chung", "Moo K.", ""], ["Hanson", "Jamie L.", ""], ["Ye", "Jieping", ""], ["Davidson", "Richard J.", ""], ["Pollak", "Seth D.", ""]]}, {"id": "1409.0347", "submitter": "Chao Li", "authors": "Chao Li and Lili Guo and Andrzej Cichocki", "title": "Multi-tensor Completion for Estimating Missing Values in Video Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tensor-based data completion methods aim to solve image and video\nin-painting problems. But, all methods were only developed for a single\ndataset. In most of real applications, we can usually obtain more than one\ndataset to reflect one phenomenon, and all the datasets are mutually related in\nsome sense. Thus one question raised whether such the relationship can improve\nthe performance of data completion or not? In the paper, we proposed a novel\nand efficient method by exploiting the relationship among datasets for\nmulti-video data completion. Numerical results show that the proposed method\nsignificantly improve the performance of video in-painting, particularly in the\ncase of very high missing percentage.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 09:46:52 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Li", "Chao", ""], ["Guo", "Lili", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1409.0575", "submitter": "Olga Russakovsky", "authors": "Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and\n  Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya\n  Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei", "title": "ImageNet Large Scale Visual Recognition Challenge", "comments": "43 pages, 16 figures. v3 includes additional comparisons with PASCAL\n  VOC (per-category comparisons in Table 3, distribution of localization\n  difficulty in Fig 16), a list of queries used for obtaining object detection\n  images (Appendix C), and some additional references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ImageNet Large Scale Visual Recognition Challenge is a benchmark in\nobject category classification and detection on hundreds of object categories\nand millions of images. The challenge has been run annually from 2010 to\npresent, attracting participation from more than fifty institutions.\n  This paper describes the creation of this benchmark dataset and the advances\nin object recognition that have been possible as a result. We discuss the\nchallenges of collecting large-scale ground truth annotation, highlight key\nbreakthroughs in categorical object recognition, provide a detailed analysis of\nthe current state of the field of large-scale image classification and object\ndetection, and compare the state-of-the-art computer vision accuracy with human\naccuracy. We conclude with lessons learned in the five years of the challenge,\nand propose future directions and improvements.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 22:29:38 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 01:08:31 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 01:23:59 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Russakovsky", "Olga", ""], ["Deng", "Jia", ""], ["Su", "Hao", ""], ["Krause", "Jonathan", ""], ["Satheesh", "Sanjeev", ""], ["Ma", "Sean", ""], ["Huang", "Zhiheng", ""], ["Karpathy", "Andrej", ""], ["Khosla", "Aditya", ""], ["Bernstein", "Michael", ""], ["Berg", "Alexander C.", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1409.0602", "submitter": "Zhu Shizhan", "authors": "Shizhan Zhu, Cheng Li, Chen Change Loy, and Xiaoou Tang", "title": "Transferring Landmark Annotations for Cross-Dataset Face Alignment", "comments": "Shizhan Zhu and Cheng Li share equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Dataset bias is a well known problem in object recognition domain. This\nissue, nonetheless, is rarely explored in face alignment research. In this\nstudy, we show that dataset plays an integral part of face alignment\nperformance. Specifically, owing to face alignment dataset bias, training on\none database and testing on another or unseen domain would lead to poor\nperformance. Creating an unbiased dataset through combining various existing\ndatabases, however, is non-trivial as one has to exhaustively re-label the\nlandmarks for standardisation. In this work, we propose a simple and yet\neffective method to bridge the disparate annotation spaces between databases,\nmaking datasets fusion possible. We show extensive results on combining various\npopular databases (LFW, AFLW, LFPW, HELEN) for improved cross-dataset and\nunseen data alignment.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 03:36:55 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Zhu", "Shizhan", ""], ["Li", "Cheng", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1409.0685", "submitter": "Feiyun Zhu", "authors": "Feiyun Zhu, Ying Wang, Bin Fan, Gaofeng Meng and Chunhong Pan", "title": "Effective Spectral Unmixing via Robust Representation and Learning-based\n  Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral unmixing (HU) plays a fundamental role in a wide range of\nhyperspectral applications. It is still challenging due to the common presence\nof outlier channels and the large solution space. To address the above two\nissues, we propose a novel model by emphasizing both robust representation and\nlearning-based sparsity. Specifically, we apply the $\\ell_{2,1}$-norm to\nmeasure the representation error, preventing outlier channels from dominating\nour objective. In this way, the side effects of outlier channels are greatly\nrelieved. Besides, we observe that the mixed level of each pixel varies over\nimage grids. Based on this observation, we exploit a learning-based sparsity\nmethod to simultaneously learn the HU results and a sparse guidance map. Via\nthis guidance map, the sparsity constraint in the $\\ell_{p}\\!\\left(\\!0\\!<\\!\np\\!\\leq\\!1\\right)$-norm is adaptively imposed according to the learnt mixed\nlevel of each pixel. Compared with state-of-the-art methods, our model is\nbetter suited to the real situation, thus expected to achieve better HU\nresults. The resulted objective is highly non-convex and non-smooth, and so it\nis hard to optimize. As a profound theoretical contribution, we propose an\nefficient algorithm to solve it. Meanwhile, the convergence proof and the\ncomputational complexity analysis are systematically provided. Extensive\nevaluations verify that our method is highly promising for the HU task---it\nachieves very accurate guidance maps and much better HU results compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 12:36:53 GMT"}, {"version": "v2", "created": "Fri, 12 Sep 2014 04:13:37 GMT"}, {"version": "v3", "created": "Fri, 19 Sep 2014 08:10:20 GMT"}, {"version": "v4", "created": "Sat, 26 Aug 2017 17:39:00 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Zhu", "Feiyun", ""], ["Wang", "Ying", ""], ["Fan", "Bin", ""], ["Meng", "Gaofeng", ""], ["Pan", "Chunhong", ""]]}, {"id": "1409.0749", "submitter": "Vikas Verma", "authors": "Vikas Verma", "title": "Image Retrieval And Classification Using Local Feature Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content Based Image Retrieval(CBIR) is one of the important subfield in the\nfield of Information Retrieval. The goal of a CBIR algorithm is to retrieve\nsemantically similar images in response to a query image submitted by the end\nuser. CBIR is a hard problem because of the phenomenon known as $\\textit\n{semantic gap}$.\n  In this thesis, we aim at analyzing the performance of a CBIR system build\nusing local feature vectors and Intermediate Matching Kernel. We also propose a\nTwo-Step Matching process for reducing the response time of the CBIR systems.\nFurther, we develop a Meta-Learning framework for improving the retrieval\nperformance of these systems. Our results show that the Two-Step Matching\nprocess significantly reduces response time and the Meta-Learning Framework\nimproves the retrieval performance by more than two fold. We also analyze the\nperformance of various image classification systems that use different image\nrepresentations constructed from the local feature vectors.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 15:17:33 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Verma", "Vikas", ""]]}, {"id": "1409.0814", "submitter": "Swakkhar Shatabda", "authors": "Rezaul Karim, Mohd. Momin Al Aziz, Swakkhar Shatabda, M. Sohel Rahman,\n  Md. Abul Kashem Mia, Farhana Zaman and Salman Rakin", "title": "CoMOGrad and PHOG: From Computer Vision to Fast and Accurate Protein\n  Tertiary Structure Retrieval", "comments": "draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advancements in technology number of entries in the structural\ndatabase of proteins are increasing day by day. Methods for retrieving protein\ntertiary structures from this large database is the key to comparative analysis\nof structures which plays an important role to understand proteins and their\nfunction. In this paper, we present fast and accurate methods for the retrieval\nof proteins from a large database with tertiary structures similar to a query\nprotein. Our proposed methods borrow ideas from the field of computer vision.\nThe speed and accuracy of our methods comes from the two newly introduced\nfeatures, the co-occurrence matrix of the oriented gradient and pyramid\nhistogram of oriented gradient and from the use of Euclidean distance as the\ndistance measure. Experimental results clearly indicate the superiority of our\napproach in both running time and accuracy. Our method is readily available for\nuse from this website: http://research.buet.ac.bd:8080/Comograd/.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 18:26:50 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Karim", "Rezaul", ""], ["Aziz", "Mohd. Momin Al", ""], ["Shatabda", "Swakkhar", ""], ["Rahman", "M. Sohel", ""], ["Mia", "Md. Abul Kashem", ""], ["Zaman", "Farhana", ""], ["Rakin", "Salman", ""]]}, {"id": "1409.0908", "submitter": "Anh Tran", "authors": "Anh Tran, Jinyan Guan, Thanima Pilantanakitti, Paul Cohen", "title": "Action Recognition in the Frequency Domain", "comments": "Keywords: Artificial Intelligence, Computer Vision, Action\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a simple strategy for mitigating variability in\ntemporal data series by shifting focus onto long-term, frequency domain\nfeatures that are less susceptible to variability. We apply this method to the\nhuman action recognition task and demonstrate how working in the frequency\ndomain can yield good recognition features for commonly used optical flow and\narticulated pose features, which are highly sensitive to small differences in\nmotion, viewpoint, dynamic backgrounds, occlusion and other sources of\nvariability. We show how these frequency-based features can be used in\ncombination with a simple forest classifier to achieve good and robust results\non the popular KTH Actions dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 22:34:29 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Tran", "Anh", ""], ["Guan", "Jinyan", ""], ["Pilantanakitti", "Thanima", ""], ["Cohen", "Paul", ""]]}, {"id": "1409.0924", "submitter": "Ahmad Hassanat", "authors": "Ahmad Basheer Hassanat", "title": "Visual Passwords Using Automatic Lip Reading", "comments": null, "journal-ref": "International Journal of Sciences: Basic and Applied Research\n  (IJSBAR) (2014) Volume 13, No 1, pp 218-231", "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a visual passwords system to increase security. The\nsystem depends mainly on recognizing the speaker using the visual speech signal\nalone. The proposed scheme works in two stages: setting the visual password\nstage and the verification stage. At the setting stage the visual passwords\nsystem request the user to utter a selected password, a video recording of the\nuser face is captured, and processed by a special words-based VSR system which\nextracts a sequence of feature vectors. In the verification stage, the same\nprocedure is executed, the features will be sent to be compared with the stored\nvisual password. The proposed scheme has been evaluated using a video database\nof 20 different speakers (10 females and 10 males), and 15 more males in\nanother video database with different experiment sets. The evaluation has\nproved the system feasibility, with average error rate in the range of 7.63% to\n20.51% at the worst tested scenario, and therefore, has potential to be a\npractical approach with the support of other conventional authentication\nmethods such as the use of usernames and passwords.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 23:57:04 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Hassanat", "Ahmad Basheer", ""]]}, {"id": "1409.0925", "submitter": "Ahmad Hassanat", "authors": "Ahmad B. A. Hassanat", "title": "Bypassing Captcha By Machine A Proof For Passing The Turing Test", "comments": "European Scientific Journal May 2014 edition vol.10, No.15 ISSN:\n  1857-7881 (Print) e-ISSN 1857-7431", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the last ten years, CAPTCHAs have been widely used by websites to prevent\ntheir data being automatically updated by machines. By supposedly allowing only\nhumans to do so, CAPTCHAs take advantage of the reverse Turing test (TT),\nknowing that humans are more intelligent than machines. Generally, CAPTCHAs\nhave defeated machines, but things are changing rapidly as technology improves.\nHence, advanced research into optical character recognition (OCR) is overtaking\nattempts to strengthen CAPTCHAs against machine-based attacks. This paper\ninvestigates the immunity of CAPTCHA, which was built on the failure of the TT.\nWe show that some CAPTCHAs are easily broken using a simple OCR machine built\nfor the purpose of this study. By reviewing other techniques, we show that even\nmore difficult CAPTCHAs can be broken using advanced OCR machines. Current\nadvances in OCR should enable machines to pass the TT in the image recognition\ndomain, which is exactly where machines are seeking to overcome CAPTCHAs. We\nenhance traditional CAPTCHAs by employing not only characters, but also natural\nlanguage and multiple objects within the same CAPTCHA. The proposed CAPTCHAs\nmight be able to hold out against machines, at least until the advent of a\nmachine that passes the TT completely.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 00:05:28 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Hassanat", "Ahmad B. A.", ""]]}, {"id": "1409.0964", "submitter": "Liansheng Zhuang", "authors": "Liansheng Zhuang, Shenghua Gao, Jinhui Tang, Jingjing Wang, Zhouchen\n  Lin, Yi Ma", "title": "Constructing a Non-Negative Low Rank and Sparse Graph with Data-Adaptive\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at constructing a good graph for discovering intrinsic data\nstructures in a semi-supervised learning setting. Firstly, we propose to build\na non-negative low-rank and sparse (referred to as NNLRS) graph for the given\ndata representation. Specifically, the weights of edges in the graph are\nobtained by seeking a nonnegative low-rank and sparse matrix that represents\neach data sample as a linear combination of others. The so-obtained NNLRS-graph\ncan capture both the global mixture of subspaces structure (by the low\nrankness) and the locally linear structure (by the sparseness) of the data,\nhence is both generative and discriminative. Secondly, as good features are\nextremely important for constructing a good graph, we propose to learn the data\nembedding matrix and construct the graph jointly within one framework, which is\ntermed as NNLRS with embedded features (referred to as NNLRS-EF). Extensive\nexperiments on three publicly available datasets demonstrate that the proposed\nmethod outperforms the state-of-the-art graph construction method by a large\nmargin for both semi-supervised classification and discriminative analysis,\nwhich verifies the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 06:45:11 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Zhuang", "Liansheng", ""], ["Gao", "Shenghua", ""], ["Tang", "Jinhui", ""], ["Wang", "Jingjing", ""], ["Lin", "Zhouchen", ""], ["Ma", "Yi", ""]]}, {"id": "1409.1062", "submitter": "Fanhua Shang", "authors": "Fanhua Shang, Yuanyuan Liu, Hanghang Tong, James Cheng, Hong Cheng", "title": "Structured Low-Rank Matrix Factorization with Missing and Grossly\n  Corrupted Observations", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering low-rank and sparse matrices from incomplete or corrupted\nobservations is an important problem in machine learning, statistics,\nbioinformatics, computer vision, as well as signal and image processing. In\ntheory, this problem can be solved by the natural convex joint/mixed\nrelaxations (i.e., l_{1}-norm and trace norm) under certain conditions.\nHowever, all current provable algorithms suffer from superlinear per-iteration\ncost, which severely limits their applicability to large-scale problems. In\nthis paper, we propose a scalable, provable structured low-rank matrix\nfactorization method to recover low-rank and sparse matrices from missing and\ngrossly corrupted data, i.e., robust matrix completion (RMC) problems, or\nincomplete and grossly corrupted measurements, i.e., compressive principal\ncomponent pursuit (CPCP) problems. Specifically, we first present two\nsmall-scale matrix trace norm regularized bilinear structured factorization\nmodels for RMC and CPCP problems, in which repetitively calculating SVD of a\nlarge-scale matrix is replaced by updating two much smaller factor matrices.\nThen, we apply the alternating direction method of multipliers (ADMM) to\nefficiently solve the RMC problems. Finally, we provide the convergence\nanalysis of our algorithm, and extend it to address general CPCP problems.\nExperimental results verified both the efficiency and effectiveness of our\nmethod compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:36:25 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Tong", "Hanghang", ""], ["Cheng", "James", ""], ["Cheng", "Hong", ""]]}, {"id": "1409.1199", "submitter": "Stephen Plaza PhD", "authors": "Stephen M. Plaza", "title": "Focused Proofreading: Efficiently Extracting Connectomes from Segmented\n  EM Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying complex neural circuitry from electron microscopic (EM) images\nmay help unlock the mysteries of the brain. However, identifying this circuitry\nrequires time-consuming, manual tracing (proofreading) due to the size and\nintricacy of these image datasets, thus limiting state-of-the-art analysis to\nvery small brain regions. Potential avenues to improve scalability include\nautomatic image segmentation and crowd sourcing, but current efforts have had\nlimited success. In this paper, we propose a new strategy, focused\nproofreading, that works with automatic segmentation and aims to limit\nproofreading to the regions of a dataset that are most impactful to the\nresulting circuit. We then introduce a novel workflow, which exploits\nbiological information such as synapses, and apply it to a large dataset in the\nfly optic lobe. With our techniques, we achieve significant tracing speedups of\n3-5x without sacrificing the quality of the resulting circuit. Furthermore, our\nmethodology makes the task of proofreading much more accessible and hence\npotentially enhances the effectiveness of crowd sourcing.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 19:14:13 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Plaza", "Stephen M.", ""]]}, {"id": "1409.1411", "submitter": "Ahmad Hassanat", "authors": "Ahmad B. A. Hassanat", "title": "Visual Speech Recognition", "comments": "Speech and Language Technologies (Book), Prof. Ivo Ipsic (Ed.), ISBN:\n  978-953-307-322-4, InTech (2011)", "journal-ref": null, "doi": "10.5772/19361", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip reading is used to understand or interpret speech without hearing it, a\ntechnique especially mastered by people with hearing difficulties. The ability\nto lip read enables a person with a hearing impairment to communicate with\nothers and to engage in social activities, which otherwise would be difficult.\nRecent advances in the fields of computer vision, pattern recognition, and\nsignal processing has led to a growing interest in automating this challenging\ntask of lip reading. Indeed, automating the human ability to lip read, a\nprocess referred to as visual speech recognition (VSR) (or sometimes speech\nreading), could open the door for other novel related applications. VSR has\nreceived a great deal of attention in the last decade for its potential use in\napplications such as human-computer interaction (HCI), audio-visual speech\nrecognition (AVSR), speaker recognition, talking heads, sign language\nrecognition and video surveillance. Its main aim is to recognise spoken word(s)\nby using only the visual signal that is produced during speech. Hence, VSR\ndeals with the visual domain of speech and involves image processing,\nartificial intelligence, object detection, pattern recognition, statistical\nmodelling, etc.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 00:19:42 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Hassanat", "Ahmad B. A.", ""]]}, {"id": "1409.1484", "submitter": "Alejandro Betancourt", "authors": "Alejandro Betancourt, Pietro Morerio, Carlo S. Regazzoni, Matthias\n  Rauterberg", "title": "The Evolution of First Person Vision Methods: A Survey", "comments": "First Person Vision, Egocentric Vision, Wearable Devices, Smart\n  Glasses, Computer Vision, Video Analytics, Human-machine Interaction", "journal-ref": "Betancourt, A., Morerio, P., Regazzoni, C. S., & Rauterberg, M.\n  (2015). The Evolution of First Person Vision Methods: A Survey. IEEE\n  Transactions on Circuits and Systems for Video Technology,\n  doi:10.1109/TCSVT.2015.2409731", "doi": "10.1109/TCSVT.2015.2409731", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of new wearable technologies such as action cameras and\nsmart-glasses has increased the interest of computer vision scientists in the\nFirst Person perspective. Nowadays, this field is attracting attention and\ninvestments of companies aiming to develop commercial devices with First Person\nVision recording capabilities. Due to this interest, an increasing demand of\nmethods to process these videos, possibly in real-time, is expected. Current\napproaches present a particular combinations of different image features and\nquantitative methods to accomplish specific objectives like object detection,\nactivity recognition, user machine interaction and so on. This paper summarizes\nthe evolution of the state of the art in First Person Vision video analysis\nbetween 1997 and 2014, highlighting, among others, most commonly used features,\nmethods, challenges and opportunities within the field.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 16:38:43 GMT"}, {"version": "v2", "created": "Thu, 18 Sep 2014 16:05:03 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2015 10:00:17 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Betancourt", "Alejandro", ""], ["Morerio", "Pietro", ""], ["Regazzoni", "Carlo S.", ""], ["Rauterberg", "Matthias", ""]]}, {"id": "1409.1556", "submitter": "Karen Simonyan", "authors": "Karen Simonyan, Andrew Zisserman", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the effect of the convolutional network depth on\nits accuracy in the large-scale image recognition setting. Our main\ncontribution is a thorough evaluation of networks of increasing depth using an\narchitecture with very small (3x3) convolution filters, which shows that a\nsignificant improvement on the prior-art configurations can be achieved by\npushing the depth to 16-19 weight layers. These findings were the basis of our\nImageNet Challenge 2014 submission, where our team secured the first and the\nsecond places in the localisation and classification tracks respectively. We\nalso show that our representations generalise well to other datasets, where\nthey achieve state-of-the-art results. We have made our two best-performing\nConvNet models publicly available to facilitate further research on the use of\ndeep visual representations in computer vision.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 19:48:04 GMT"}, {"version": "v2", "created": "Mon, 15 Sep 2014 19:58:29 GMT"}, {"version": "v3", "created": "Tue, 18 Nov 2014 20:43:11 GMT"}, {"version": "v4", "created": "Fri, 19 Dec 2014 20:01:21 GMT"}, {"version": "v5", "created": "Tue, 23 Dec 2014 20:05:00 GMT"}, {"version": "v6", "created": "Fri, 10 Apr 2015 16:25:04 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Simonyan", "Karen", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1409.1789", "submitter": "Gary Huang", "authors": "Gary B. Huang and Stephen Plaza", "title": "Identifying Synapses Using Deep and Wide Multiscale Recursive Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a learning framework for identifying synapses using\na deep and wide multi-scale recursive (DAWMR) network, previously considered in\nimage segmentation applications. We apply this approach on electron microscopy\ndata from invertebrate fly brain tissue. By learning features directly from the\ndata, we are able to achieve considerable improvements over existing techniques\nthat rely on a small set of hand-designed features. We show that this system\ncan reduce the amount of manual annotation required, in both acquisition of\ntraining data as well as verification of inferred detections.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 13:36:51 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Huang", "Gary B.", ""], ["Plaza", "Stephen", ""]]}, {"id": "1409.1801", "submitter": "Stephen Plaza", "authors": "Stephen M. Plaza, Toufiq Parag, Gary B. Huang, Donald J. Olbris,\n  Mathew A. Saunders, Patricia K. Rivlin", "title": "Annotating Synapses in Large EM Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing neuronal circuits at the level of synapses is a central\nproblem in neuroscience and becoming a focus of the emerging field of\nconnectomics. To date, electron microscopy (EM) is the most proven technique\nfor identifying and quantifying synaptic connections. As advances in EM make\nacquiring larger datasets possible, subsequent manual synapse identification\n({\\em i.e.}, proofreading) for deciphering a connectome becomes a major time\nbottleneck. Here we introduce a large-scale, high-throughput, and\nsemi-automated methodology to efficiently identify synapses. We successfully\napplied our methodology to the Drosophila medulla optic lobe, annotating many\nmore synapses than previous connectome efforts. Our approaches are extensible\nand will make the often complicated process of synapse identification\naccessible to a wider-community of potential proofreaders.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 13:52:47 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 16:18:01 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Plaza", "Stephen M.", ""], ["Parag", "Toufiq", ""], ["Huang", "Gary B.", ""], ["Olbris", "Donald J.", ""], ["Saunders", "Mathew A.", ""], ["Rivlin", "Patricia K.", ""]]}, {"id": "1409.1892", "submitter": "Ting  Zhao", "authors": "Ting Zhao, Stephen M Plaza", "title": "Automatic Neuron Type Identification by Neurite Localization in the\n  Drosophila Medulla", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping the connectivity of neurons in the brain (i.e., connectomics) is a\nchallenging problem due to both the number of connections in even the smallest\norganisms and the nanometer resolution required to resolve them. Because of\nthis, previous connectomes contain only hundreds of neurons, such as in the\nC.elegans connectome. Recent technological advances will unlock the mysteries\nof increasingly large connectomes (or partial connectomes). However, the value\nof these maps is limited by our ability to reason with this data and understand\nany underlying motifs. To aid connectome analysis, we introduce algorithms to\ncluster similarly-shaped neurons, where 3D neuronal shapes are represented as\nskeletons. In particular, we propose a novel location-sensitive clustering\nalgorithm. We show clustering results on neurons reconstructed from the\nDrosophila medulla that show high-accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 18:03:03 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Zhao", "Ting", ""], ["Plaza", "Stephen M", ""]]}, {"id": "1409.2050", "submitter": "Stephen Czarnuch", "authors": "Stephen Czarnuch, Alex Mihailidis", "title": "Depth image hand tracking from an overhead perspective using partially\n  labeled, unbalanced data: Development and real-world testing", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": "10.3109/17483107.2015.1027304", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the development and evaluation of a hand tracking algorithm based\non single depth images captured from an overhead perspective for use in the\nCOACH prompting system. We train a random decision forest body part classifier\nusing approximately 5,000 manually labeled, unbalanced, partially labeled\ntraining images. The classifier represents a random subset of pixels in each\ndepth image with a learned probability density function across all trained body\nparts. A local mode-find approach is used to search for clusters present in the\nunderlying feature space sampled by the classified pixels. In each frame, body\npart positions are chosen as the mode with the highest confidence. User hand\npositions are translated into hand washing task actions based on proximity to\nenvironmental objects. We validate the performance of the classifier and task\naction proposals on a large set of approximately 24,000 manually labeled\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 19:30:34 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Czarnuch", "Stephen", ""], ["Mihailidis", "Alex", ""]]}, {"id": "1409.2080", "submitter": "Pierre Bellec", "authors": "P. Bellec and Y. Benhajali and F. Carbonell and C. Dansereau and G.\n  Albouy and M. Pelland and C. Craddock and O. Collignon and J. Doyon and E.\n  Stip and P. Orban", "title": "Multiscale statistical testing for connectome-wide association studies\n  in fMRI", "comments": "54 pages, 12 main figures, 1 main table, 10 supplementary figures, 1\n  supplementary table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Alterations in brain connectivity have been associated with a variety of\nclinical disorders using functional magnetic resonance imaging (fMRI). We\ninvestigated empirically how the number of brain parcels (or scale) impacted\nthe results of a mass univariate general linear model (GLM) on connectomes. The\nbrain parcels used as nodes in the connectome analysis were functionnally\ndefined by a group cluster analysis. We first validated that a classic\nBenjamini-Hochberg procedure with parametric GLM tests did control\nappropriately the false-discovery rate (FDR) at a given scale. We then observed\non realistic simulations that there was no substantial inflation of the FDR\nacross scales, as long as the FDR was controlled independently within each\nscale, and the presence of true associations could be established using an\nomnibus permutation test combining all scales. Second, we observed both on\nsimulations and on three real resting-state fMRI datasets (schizophrenia,\ncongenital blindness, motor practice) that the rate of discovery varied\nmarkedly as a function of scales, and was relatively higher for low scales,\nbelow 25. Despite the differences in discovery rate, the statistical maps\nderived at different scales were generally very consistent in the three real\ndatasets. Some seeds still showed effects better observed around 50,\nillustrating the potential benefits of multiscale analysis. On real data, the\nstatistical maps agreed well with the existing literature. Overall, our results\nsupport that the multiscale GLM connectome analysis with FDR is statistically\nvalid and can capture biologically meaningful effects in a variety of\nexperimental conditions.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 04:07:22 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 21:00:45 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Bellec", "P.", ""], ["Benhajali", "Y.", ""], ["Carbonell", "F.", ""], ["Dansereau", "C.", ""], ["Albouy", "G.", ""], ["Pelland", "M.", ""], ["Craddock", "C.", ""], ["Collignon", "O.", ""], ["Doyon", "J.", ""], ["Stip", "E.", ""], ["Orban", "P.", ""]]}, {"id": "1409.2104", "submitter": "Chunhua Shen", "authors": "Lei Luo, Chunhua Shen, Xinwang Liu, Chunyuan Zhang", "title": "A Computational Model of the Short-Cut Rule for 2D Shape Decomposition", "comments": "11 pages", "journal-ref": null, "doi": "10.1109/TIP.2014.2376188", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new 2D shape decomposition method based on the short-cut rule.\nThe short-cut rule originates from cognition research, and states that the\nhuman visual system prefers to partition an object into parts using the\nshortest possible cuts. We propose and implement a computational model for the\nshort-cut rule and apply it to the problem of shape decomposition. The model we\nproposed generates a set of cut hypotheses passing through the points on the\nsilhouette which represent the negative minima of curvature. We then show that\nmost part-cut hypotheses can be eliminated by analysis of local properties of\neach. Finally, the remaining hypotheses are evaluated in ascending length\norder, which guarantees that of any pair of conflicting cuts only the shortest\nwill be accepted. We demonstrate that, compared with state-of-the-art shape\ndecomposition methods, the proposed approach achieves decomposition results\nwhich better correspond to human intuition as revealed in psychological\nexperiments.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 10:43:36 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Luo", "Lei", ""], ["Shen", "Chunhua", ""], ["Liu", "Xinwang", ""], ["Zhang", "Chunyuan", ""]]}, {"id": "1409.2232", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang, Xuefeng Cui, Ge Yu, Lili Guo, Xin Gao", "title": "When coding meets ranking: A joint framework based on local learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding, which represents a data point as a sparse reconstruction code\nwith regard to a dictionary, has been a popular data representation method.\nMeanwhile, in database retrieval problems, learning the ranking scores from\ndata points plays an important role. Up to now, these two problems have always\nbeen considered separately, assuming that data coding and ranking are two\nindependent and irrelevant problems. However, is there any internal\nrelationship between sparse coding and ranking score learning? If yes, how to\nexplore and make use of this internal relationship? In this paper, we try to\nanswer these questions by developing the first joint sparse coding and ranking\nscore learning algorithm. To explore the local distribution in the sparse code\nspace, and also to bridge coding and ranking problems, we assume that in the\nneighborhood of each data point, the ranking scores can be approximated from\nthe corresponding sparse codes by a local linear function. By considering the\nlocal approximation error of ranking scores, the reconstruction error and\nsparsity of sparse coding, and the query information provided by the user, we\nconstruct a unified objective function for learning of sparse codes, the\ndictionary and ranking scores. We further develop an iterative algorithm to\nsolve this optimization problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 08:10:37 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 07:33:51 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wang", "Jim Jing-Yan", ""], ["Cui", "Xuefeng", ""], ["Yu", "Ge", ""], ["Guo", "Lili", ""], ["Gao", "Xin", ""]]}, {"id": "1409.2287", "submitter": "Andreas Damianou Mr", "authors": "Andreas C. Damianou, Michalis K. Titsias, Neil D. Lawrence", "title": "Variational Inference for Uncertainty on the Inputs of Gaussian Process\n  Models", "comments": "51 pages (of which 10 is Appendix), 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process latent variable model (GP-LVM) provides a flexible\napproach for non-linear dimensionality reduction that has been widely applied.\nHowever, the current approach for training GP-LVMs is based on maximum\nlikelihood, where the latent projection variables are maximized over rather\nthan integrated out. In this paper we present a Bayesian method for training\nGP-LVMs by introducing a non-standard variational inference framework that\nallows to approximately integrate out the latent variables and subsequently\ntrain a GP-LVM by maximizing an analytic lower bound on the exact marginal\nlikelihood. We apply this method for learning a GP-LVM from iid observations\nand for learning non-linear dynamical systems where the observations are\ntemporally correlated. We show that a benefit of the variational Bayesian\nprocedure is its robustness to overfitting and its ability to automatically\nselect the dimensionality of the nonlinear latent space. The resulting\nframework is generic, flexible and easy to extend for other purposes, such as\nGaussian process regression with uncertain inputs and semi-supervised Gaussian\nprocesses. We demonstrate our method on synthetic data and standard machine\nlearning benchmarks, as well as challenging real world datasets, including high\nresolution video data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 10:47:23 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Damianou", "Andreas C.", ""], ["Titsias", "Michalis K.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1409.2413", "submitter": "Rino Franco aina", "authors": "Franco Rino", "title": "Image processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Gabor filters can extract multi-orientation and multiscale features from face\nimages. Researchers have designed different ways to use the magnitude of the\nfiltered results for face recognition: Gabor Fisher classifier exploited only\nthe magnitude information of Gabor magnitude pictures (GMPs); Local Gabor\nBinary Pattern uses only the gradient information. In this paper, we regard\nGMPs as smooth surfaces. By completely describing the shape of GMPs, we get a\nface representation method called Gabor Surface Feature (GSF). First, we\ncompute the magnitude, 1st and 2nd derivatives of GMPs, then binarize them and\ntransform them into decimal values. Finally we construct joint histograms and\nuse subspace methods for classification. Experiments on FERET, ORL and FRGC\n1.0.4 database show the effectiveness of GSF.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 03:39:41 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Rino", "Franco", ""]]}, {"id": "1409.2465", "submitter": "Ives Rey-Otero", "authors": "Ives Rey-Otero, Mauricio Delbracio, Jean-Michel Morel", "title": "Comparing Feature Detectors: A bias in the repeatability criteria, and\n  how to correct it", "comments": "Fixed typo in affiliations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most computer vision application rely on algorithms finding local\ncorrespondences between different images. These algorithms detect and compare\nstable local invariant descriptors centered at scale-invariant keypoints.\nBecause of the importance of the problem, new keypoint detectors and\ndescriptors are constantly being proposed, each one claiming to perform better\n(or to be complementary) to the preceding ones. This raises the question of a\nfair comparison between very diverse methods. This evaluation has been mainly\nbased on a repeatability criterion of the keypoints under a series of image\nperturbations (blur, illumination, noise, rotations, homotheties, homographies,\netc). In this paper, we argue that the classic repeatability criterion is\nbiased towards algorithms producing redundant overlapped detections. To\ncompensate this bias, we propose a variant of the repeatability rate taking\ninto account the descriptors overlap. We apply this variant to revisit the\npopular benchmark by Mikolajczyk et al., on classic and new feature detectors.\nExperimental evidence shows that the hierarchy of these feature detectors is\nseverely disrupted by the amended comparator.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 19:02:01 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 08:12:35 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Rey-Otero", "Ives", ""], ["Delbracio", "Mauricio", ""], ["Morel", "Jean-Michel", ""]]}, {"id": "1409.2579", "submitter": "Gang Wu", "authors": "Ting-ting Feng, Gang Wu", "title": "A theoretical contribution to the fast implementation of null linear\n  discriminant analysis method using random matrix multiplication with scatter\n  matrices", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The null linear discriminant analysis method is a competitive approach for\ndimensionality reduction. The implementation of this method, however, is\ncomputationally expensive. Recently, a fast implementation of null linear\ndiscriminant analysis method using random matrix multiplication with scatter\nmatrices was proposed. However, if the random matrix is chosen arbitrarily, the\norientation matrix may be rank deficient, and some useful discriminant\ninformation will be lost. In this paper, we investigate how to choose the\nrandom matrix properly, such that the two criteria of the null LDA method are\nsatisfied theoretically. We give a necessary and sufficient condition to\nguarantee full column rank of the orientation matrix. Moreover, the geometric\ncharacterization of the condition is also described.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 11:46:40 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Feng", "Ting-ting", ""], ["Wu", "Gang", ""]]}, {"id": "1409.2702", "submitter": "Francesco Setti", "authors": "Francesco Setti, Chris Russell, Chiara Bassetti and Marco Cristani", "title": "F-formation Detection: Individuating Free-standing Conversational Groups\n  in Images", "comments": "32 pages, submitted to PLOS One", "journal-ref": null, "doi": "10.1371/journal.pone.0123783", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of groups of interacting people is a very interesting and useful\ntask in many modern technologies, with application fields spanning from\nvideo-surveillance to social robotics. In this paper we first furnish a\nrigorous definition of group considering the background of the social sciences:\nthis allows us to specify many kinds of group, so far neglected in the Computer\nVision literature. On top of this taxonomy, we present a detailed state of the\nart on the group detection algorithms. Then, as a main contribution, we present\na brand new method for the automatic detection of groups in still images, which\nis based on a graph-cuts framework for clustering individuals; in particular we\nare able to codify in a computational sense the sociological definition of\nF-formation, that is very useful to encode a group having only proxemic\ninformation: position and orientation of people. We call the proposed method\nGraph-Cuts for F-formation (GCFF). We show how GCFF definitely outperforms all\nthe state of the art methods in terms of different accuracy measures (some of\nthem are brand new), demonstrating also a strong robustness to noise and\nversatility in recognizing groups of various cardinality.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 11:54:45 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Setti", "Francesco", ""], ["Russell", "Chris", ""], ["Bassetti", "Chiara", ""], ["Cristani", "Marco", ""]]}, {"id": "1409.2800", "submitter": "Toufiq Parag", "authors": "Toufiq Parag", "title": "Enforcing Label and Intensity Consistency for IR Target Detection", "comments": "First appeared in OTCBVS 2011 \\cite{parag11otcbvs}. This manuscript\n  presents updated results and an extension", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study formulates the IR target detection as a binary classification\nproblem of each pixel. Each pixel is associated with a label which indicates\nwhether it is a target or background pixel. The optimal label set for all the\npixels of an image maximizes aposteriori distribution of label configuration\ngiven the pixel intensities. The posterior probability is factored into (or\nproportional to) a conditional likelihood of the intensity values and a prior\nprobability of label configuration. Each of these two probabilities are\ncomputed assuming a Markov Random Field (MRF) on both pixel intensities and\ntheir labels. In particular, this study enforces neighborhood dependency on\nboth intensity values, by a Simultaneous Auto Regressive (SAR) model, and on\nlabels, by an Auto-Logistic model. The parameters of these MRF models are\nlearned from labeled examples. During testing, an MRF inference technique,\nnamely Iterated Conditional Mode (ICM), produces the optimal label for each\npixel. The detection performance is further improved by incorporating temporal\ninformation through background subtraction. High performances on benchmark\ndatasets demonstrate effectiveness of this method for IR target detection.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 16:20:08 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Parag", "Toufiq", ""]]}, {"id": "1409.2821", "submitter": "Nasser Ghadiri", "authors": "Meysam Ghaffari, Nasser Ghadiri", "title": "Ambiguity-Driven Fuzzy C-Means Clustering: How to Detect Uncertain\n  Clustered Records", "comments": null, "journal-ref": null, "doi": "10.1007/s10489-016-0759-1", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a well-known clustering algorithm, Fuzzy C-Means (FCM) allows each input\nsample to belong to more than one cluster, providing more flexibility than\nnon-fuzzy clustering methods. However, the accuracy of FCM is subject to false\ndetections caused by noisy records, weak feature selection and low certainty of\nthe algorithm in some cases. The false detections are very important in some\ndecision-making application domains like network security and medical\ndiagnosis, where weak decisions based on such false detections may lead to\ncatastrophic outcomes. They are mainly emerged from making decisions about a\nsubset of records that do not provide enough evidence to make a good decision.\nIn this paper, we propose a method for detecting such ambiguous records in FCM\nby introducing a certainty factor to decrease invalid detections. This approach\nenables us to send the detected ambiguous records to another discrimination\nmethod for a deeper investigation, thus increasing the accuracy by lowering the\nerror rate. Most of the records are still processed quickly and with low error\nrate which prevents performance loss compared to similar hybrid methods.\nExperimental results of applying the proposed method on several datasets from\ndifferent domains show a significant decrease in error rate as well as improved\nsensitivity of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 17:17:48 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Ghaffari", "Meysam", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1409.2918", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "Quantum Edge Detection for Image Segmentation in Optical Environments", "comments": "31 pages, 20 figures, 5 tables. arXiv admin note: substantial text\n  overlap with arXiv:1406.5121, arXiv:1408.2427; and text overlap with\n  arXiv:quant-ph/0402085 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quantum edge detector for image segmentation in optical environments is\npresented in this work. A Boolean version of the same detector is presented\ntoo. The quantum version of the new edge detector works with computational\nbasis states, exclusively. This way, we can easily avoid the problem of quantum\nmeasurement retrieving the result of applying the new detector on the image.\nBesides, a new criterion and logic based on projections onto vertical axis of\nBloch's Sphere exclusively are presented too. This approach will allow us: 1) a\nsimpler development of logic quantum operations, where they will closer to\nthose used in the classical logic operations, 2) building simple and robust\nclassical-to-quantum and quantum-to-classical interfaces. Said so far is\nextended to quantum algorithms outside image processing too. In a special\nsection on metric and simulations, a new metric based on the comparison between\nthe classical and quantum versions algorithms for edge detection of images is\npresented. Notable differences between the results of classical and quantum\nversions of such algorithms (outside and inside of quantum computer,\nrespectively) show the existence of implementation problems involved in the\nexperiment, and that they have not been properly modeled for optical\nenvironments. However, although they are different, the quantum results are\nequally valid. The latter is clearly seen in the computer simulations\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 23:22:56 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1409.3024", "submitter": "Yasser Fouda Dr.", "authors": "Y. M. Fouda", "title": "One-Dimensional Vector based Pattern Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Template matching is a basic method in image analysis to extract useful\ninformation from images. In this paper, we suggest a new method for pattern\nmatching. Our method transform the template image from two dimensional image\ninto one dimensional vector. Also all sub-windows (same size of template) in\nthe reference image will transform into one dimensional vectors. The three\nsimilarity measures SAD, SSD, and Euclidean are used to compute the likeness\nbetween template and all sub-windows in the reference image to find the best\nmatch. The experimental results show the superior performance of the proposed\nmethod over the conventional methods on various template of different sizes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 11:18:08 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Fouda", "Y. M.", ""]]}, {"id": "1409.3505", "submitter": "Wanli Ouyang", "authors": "Wanli Ouyang, Ping Luo, Xingyu Zeng, Shi Qiu, Yonglong Tian, Hongsheng\n  Li, Shuo Yang, Zhe Wang, Yuanjun Xiong, Chen Qian, Zhenyao Zhu, Ruohui Wang,\n  Chen-Change Loy, Xiaogang Wang, Xiaoou Tang", "title": "DeepID-Net: multi-stage and deformable deep convolutional neural\n  networks for object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose multi-stage and deformable deep convolutional\nneural networks for object detection. This new deep learning object detection\ndiagram has innovations in multiple aspects. In the proposed new deep\narchitecture, a new deformation constrained pooling (def-pooling) layer models\nthe deformation of object parts with geometric constraint and penalty. With the\nproposed multi-stage training strategy, multiple classifiers are jointly\noptimized to process samples at different difficulty levels. A new pre-training\nstrategy is proposed to learn feature representations more suitable for the\nobject detection task and with good generalization capability. By changing the\nnet structures, training strategies, adding and removing some key components in\nthe detection pipeline, a set of models with large diversity are obtained,\nwhich significantly improves the effectiveness of modeling averaging. The\nproposed approach ranked \\#2 in ILSVRC 2014. It improves the mean averaged\nprecision obtained by RCNN, which is the state-of-the-art of object detection,\nfrom $31\\%$ to $45\\%$. Detailed component-wise analysis is also provided\nthrough extensive experimental evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 17:13:26 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Ouyang", "Wanli", ""], ["Luo", "Ping", ""], ["Zeng", "Xingyu", ""], ["Qiu", "Shi", ""], ["Tian", "Yonglong", ""], ["Li", "Hongsheng", ""], ["Yang", "Shuo", ""], ["Wang", "Zhe", ""], ["Xiong", "Yuanjun", ""], ["Qian", "Chen", ""], ["Zhu", "Zhenyao", ""], ["Wang", "Ruohui", ""], ["Loy", "Chen-Change", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1409.3660", "submitter": "Fei-Yun Zhu", "authors": "Feiyun Zhu, Bin Fan, Xinliang Zhu, Ying Wang, Shiming Xiang and\n  Chunhong Pan", "title": "10,000+ Times Accelerated Robust Subset Selection (ARSS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset selection from massive data with noised information is increasingly\npopular for various applications. This problem is still highly challenging as\ncurrent methods are generally slow in speed and sensitive to outliers. To\naddress the above two issues, we propose an accelerated robust subset selection\n(ARSS) method. Specifically in the subset selection area, this is the first\nattempt to employ the $\\ell_{p}(0<p\\leq1)$-norm based measure for the\nrepresentation loss, preventing large errors from dominating our objective. As\na result, the robustness against outlier elements is greatly enhanced.\nActually, data size is generally much larger than feature length, i.e. $N\\gg\nL$. Based on this observation, we propose a speedup solver (via ALM and\nequivalent derivations) to highly reduce the computational cost, theoretically\nfrom $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmark\ndatasets verify that our method not only outperforms state of the art methods,\nbut also runs 10,000+ times faster than the most related method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 07:18:17 GMT"}, {"version": "v2", "created": "Fri, 19 Sep 2014 02:49:19 GMT"}, {"version": "v3", "created": "Mon, 13 Oct 2014 07:58:57 GMT"}, {"version": "v4", "created": "Mon, 17 Nov 2014 14:39:31 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Zhu", "Feiyun", ""], ["Fan", "Bin", ""], ["Zhu", "Xinliang", ""], ["Wang", "Ying", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1409.3714", "submitter": "Han Wang", "authors": "Habib Ammari, Han Wang", "title": "Time-domain multiscale shape identification in electro-sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents premier and innovative time-domain multi-scale method for\nshape identification in electro-sensing using pulse-type signals. The method is\nbased on transform-invariant shape descriptors computed from filtered\npolarization tensors at multi-scales. The proposed algorithm enjoys a\nremarkable noise robustness even with far-field measurements at very limited\nangle of view. It opens a door for pulsed imaging using echolocation and\ninduction data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 12:14:19 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Ammari", "Habib", ""], ["Wang", "Han", ""]]}, {"id": "1409.3854", "submitter": "M. Emre Celebi", "authors": "M. Emre Celebi and Hassan A. Kingravi", "title": "Linear, Deterministic, and Order-Invariant Initialization Methods for\n  the K-Means Clustering Algorithm", "comments": "21 pages, 2 figures, 5 tables, Partitional Clustering Algorithms\n  (Springer, 2014). arXiv admin note: substantial text overlap with\n  arXiv:1304.7465, arXiv:1209.1960", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past five decades, k-means has become the clustering algorithm of\nchoice in many application domains primarily due to its simplicity, time/space\nefficiency, and invariance to the ordering of the data points. Unfortunately,\nthe algorithm's sensitivity to the initial selection of the cluster centers\nremains to be its most serious drawback. Numerous initialization methods have\nbeen proposed to address this drawback. Many of these methods, however, have\ntime complexity superlinear in the number of data points, which makes them\nimpractical for large data sets. On the other hand, linear methods are often\nrandom and/or sensitive to the ordering of the data points. These methods are\ngenerally unreliable in that the quality of their results is unpredictable.\nTherefore, it is common practice to perform multiple runs of such methods and\ntake the output of the run that produces the best results. Such a practice,\nhowever, greatly increases the computational requirements of the otherwise\nhighly efficient k-means algorithm. In this chapter, we investigate the\nempirical performance of six linear, deterministic (non-random), and\norder-invariant k-means initialization methods on a large and diverse\ncollection of data sets from the UCI Machine Learning Repository. The results\ndemonstrate that two relatively unknown hierarchical initialization methods due\nto Su and Dy outperform the remaining four methods with respect to two\nobjective effectiveness criteria. In addition, a recent method due to Erisoglu\net al. performs surprisingly poorly.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 20:11:36 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Celebi", "M. Emre", ""], ["Kingravi", "Hassan A.", ""]]}, {"id": "1409.3879", "submitter": "Qianli Liao", "authors": "Qianli Liao, Joel Z. Leibo, Tomaso Poggio", "title": "Unsupervised learning of clutter-resistant visual representations from\n  natural videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Populations of neurons in inferotemporal cortex (IT) maintain an explicit\ncode for object identity that also tolerates transformations of object\nappearance e.g., position, scale, viewing angle [1, 2, 3]. Though the learning\nrules are not known, recent results [4, 5, 6] suggest the operation of an\nunsupervised temporal-association-based method e.g., Foldiak's trace rule [7].\nSuch methods exploit the temporal continuity of the visual world by assuming\nthat visual experience over short timescales will tend to have invariant\nidentity content. Thus, by associating representations of frames from nearby\ntimes, a representation that tolerates whatever transformations occurred in the\nvideo may be achieved. Many previous studies verified that such rules can work\nin simple situations without background clutter, but the presence of visual\nclutter has remained problematic for this approach. Here we show that temporal\nassociation based on large class-specific filters (templates) avoids the\nproblem of clutter. Our system learns in an unsupervised way from natural\nvideos gathered from the internet, and is able to perform a difficult\nunconstrained face recognition task on natural images: Labeled Faces in the\nWild [8].\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 22:35:08 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 00:33:14 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Liao", "Qianli", ""], ["Leibo", "Joel Z.", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1409.3906", "submitter": "Jianjun Yang", "authors": "Ju Shen, Jianjun Yang, Sami Taha-abusneineh, Bryson Payne, Markus Hitz", "title": "Structure Preserving Large Imagery Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  With the explosive growth of web-based cameras and mobile devices, billions\nof photographs are uploaded to the internet. We can trivially collect a huge\nnumber of photo streams for various goals, such as image clustering, 3D scene\nreconstruction, and other big data applications. However, such tasks are not\neasy due to the fact the retrieved photos can have large variations in their\nview perspectives, resolutions, lighting, noises, and distortions.\nFur-thermore, with the occlusion of unexpected objects like people, vehicles,\nit is even more challenging to find feature correspondences and reconstruct\nre-alistic scenes. In this paper, we propose a structure-based image completion\nalgorithm for object removal that produces visually plausible content with\nconsistent structure and scene texture. We use an edge matching technique to\ninfer the potential structure of the unknown region. Driven by the estimated\nstructure, texture synthesis is performed automatically along the estimated\ncurves. We evaluate the proposed method on different types of images: from\nhighly structured indoor environment to natural scenes. Our experimental\nresults demonstrate satisfactory performance that can be potentially used for\nsubsequent big data processing, such as image localization, object retrieval,\nand scene reconstruction. Our experiments show that this approach achieves\nfavorable results that outperform existing state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 03:29:27 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Shen", "Ju", ""], ["Yang", "Jianjun", ""], ["Taha-abusneineh", "Sami", ""], ["Payne", "Bryson", ""], ["Hitz", "Markus", ""]]}, {"id": "1409.3913", "submitter": "Jae-Yeong Lee", "authors": "Jae-Yeong Lee and Wonpil Yu", "title": "Concurrent Tracking of Inliers and Outliers", "comments": "draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In object tracking, outlier is one of primary factors which degrade\nperformance of image-based tracking algorithms. In this respect, therefore,\nmost of the existing methods simply discard detected outliers and pay little or\nno attention to employing them as an important source of information for motion\nestimation. We consider outliers as important as inliers for object tracking\nand propose a motion estimation algorithm based on concurrent tracking of\ninliers and outliers. Our tracker makes use of pyramidal implementation of the\nLucas-Kanade tracker to estimate motion flows of inliers and outliers and final\ntarget motion is estimated robustly based on both of these information.\nExperimental results from challenging benchmark video sequences confirm\nenhanced tracking performance, showing highly stable target tracking under\nsevere occlusion compared with state-of-the-art algorithms. The proposed\nalgorithm runs at more than 100 frames per second even without using a hardware\naccelerator, which makes the proposed method more practical and portable.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 05:52:36 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Lee", "Jae-Yeong", ""], ["Yu", "Wonpil", ""]]}, {"id": "1409.3964", "submitter": "Loris Bazzani", "authors": "Loris Bazzani, Alessandro Bergamo, Dragomir Anguelov, Lorenzo\n  Torresani", "title": "Self-taught Object Localization with Deep Networks", "comments": "WACV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces self-taught object localization, a novel approach that\nleverages deep convolutional networks trained for whole-image recognition to\nlocalize objects in images without additional human supervision, i.e., without\nusing any ground-truth bounding boxes for training. The key idea is to analyze\nthe change in the recognition scores when artificially masking out different\nregions of the image. The masking out of a region that includes the object\ntypically causes a significant drop in recognition score. This idea is embedded\ninto an agglomerative clustering technique that generates self-taught\nlocalization hypotheses. Our object localization scheme outperforms existing\nproposal methods in both precision and recall for small number of subwindow\nproposals (e.g., on ILSVRC-2012 it produces a relative gain of 23.4% over the\nstate-of-the-art for top-1 hypothesis). Furthermore, our experiments show that\nthe annotations automatically-generated by our method can be used to train\nobject detectors yielding recognition results remarkably close to those\nobtained by training on manually-annotated bounding boxes.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 16:12:43 GMT"}, {"version": "v2", "created": "Mon, 24 Nov 2014 17:21:58 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 21:07:04 GMT"}, {"version": "v4", "created": "Mon, 4 May 2015 17:25:38 GMT"}, {"version": "v5", "created": "Sat, 5 Sep 2015 13:54:19 GMT"}, {"version": "v6", "created": "Tue, 8 Sep 2015 18:32:00 GMT"}, {"version": "v7", "created": "Tue, 2 Feb 2016 20:55:59 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Bazzani", "Loris", ""], ["Bergamo", "Alessandro", ""], ["Anguelov", "Dragomir", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1409.3970", "submitter": "Yin Zheng", "authors": "Yin Zheng, Yu-Jin Zhang, Hugo Larochelle", "title": "A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data", "comments": "24 pages, 10 figures. A version has been accepted by TPAMI on Aug\n  4th, 2015. Add footnote about how to train the model in practice in Section\n  5.1. arXiv admin note: substantial text overlap with arXiv:1305.5306", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2476802", "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling based on latent Dirichlet allocation (LDA) has been a\nframework of choice to deal with multimodal data, such as in image annotation\ntasks. Another popular approach to model the multimodal data is through deep\nneural networks, such as the deep Boltzmann machine (DBM). Recently, a new type\nof topic model called the Document Neural Autoregressive Distribution Estimator\n(DocNADE) was proposed and demonstrated state-of-the-art performance for text\ndocument modeling. In this work, we show how to successfully apply and extend\nthis model to multimodal data, such as simultaneous image classification and\nannotation. First, we propose SupDocNADE, a supervised extension of DocNADE,\nthat increases the discriminative power of the learned hidden topic features\nand show how to employ it to learn a joint representation from image visual\nwords, annotation words and class label information. We test our model on the\nLabelMe and UIUC-Sports data sets and show that it compares favorably to other\ntopic models. Second, we propose a deep extension of our model and provide an\nefficient way of training the deep model. Experimental results show that our\ndeep model outperforms its shallow version and reaches state-of-the-art\nperformance on the Multimedia Information Retrieval (MIR) Flickr data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 17:17:05 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 02:44:29 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2015 16:12:31 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Zheng", "Yin", ""], ["Zhang", "Yu-Jin", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1409.4014", "submitter": "Pichao Wang", "authors": "Pichao Wang, Wanqing Li, Philip Ogunbona, Zhimin Gao and Hanling Zhang", "title": "Mining Mid-level Features for Action Recognition Based on Effective\n  Skeleton Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, mid-level features have shown promising performance in computer\nvision. Mid-level features learned by incorporating class-level information are\npotentially more discriminative than traditional low-level local features. In\nthis paper, an effective method is proposed to extract mid-level features from\nKinect skeletons for 3D human action recognition. Firstly, the orientations of\nlimbs connected by two skeleton joints are computed and each orientation is\nencoded into one of the 27 states indicating the spatial relationship of the\njoints. Secondly, limbs are combined into parts and the limb's states are\nmapped into part states. Finally, frequent pattern mining is employed to mine\nthe most frequent and relevant (discriminative, representative and\nnon-redundant) states of parts in continuous several frames. These parts are\nreferred to as Frequent Local Parts or FLPs. The FLPs allow us to build\npowerful bag-of-FLP-based action representation. This new representation yields\nstate-of-the-art results on MSR DailyActivity3D and MSR ActionPairs3D.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 04:40:36 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Wanqing", ""], ["Ogunbona", "Philip", ""], ["Gao", "Zhimin", ""], ["Zhang", "Hanling", ""]]}, {"id": "1409.4043", "submitter": "M. C Hanumantharaju Raju", "authors": "M. C. Hanumantharaju, M. Ravishankar, and D. R. Rameshbabu", "title": "Design of Novel Algorithm and Architecture for Gaussian Based Color\n  Image Enhancement System for Real Time Applications", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": "10.1007/978-3-642-36321-4_56", "report-no": null, "categories": "cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of a new algorithm for Gaussian based\ncolor image enhancement system. The algorithm has been designed into\narchitecture suitable for FPGA/ASIC implementation. The color image enhancement\nis achieved by first convolving an original image with a Gaussian kernel since\nGaussian distribution is a point spread function which smoothen the image.\nFurther, logarithm-domain processing and gain/offset corrections are employed\nin order to enhance and translate pixels into the display range of 0 to 255.\nThe proposed algorithm not only provides better dynamic range compression and\ncolor rendition effect but also achieves color constancy in an image. The\ndesign exploits high degrees of pipelining and parallel processing to achieve\nreal time performance. The design has been realized by RTL compliant Verilog\ncoding and fits into a single FPGA with a gate count utilization of 321,804.\nThe proposed method is implemented using Xilinx Virtex-II Pro XC2VP40-7FF1148\nFPGA device and is capable of processing high resolution color motion pictures\nof sizes of up to 1600x1200 pixels at the real time video rate of 116 frames\nper second. This shows that the proposed design would work for not only still\nimages but also for high resolution video sequences.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 10:24:04 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Hanumantharaju", "M. C.", ""], ["Ravishankar", "M.", ""], ["Rameshbabu", "D. R.", ""]]}, {"id": "1409.4046", "submitter": "M. C Hanumantharaju Raju", "authors": "M. C Hanumantharaju, M. Ravishankar, D. R Rameshbabu, and V. N\n  Manjunath Aradhya", "title": "A New Framework for Retinex based Color Image Enhancement using Particle\n  Swarm Optimization", "comments": "24 pages, 12 figures", "journal-ref": null, "doi": "10.1504/IJSI.2014.060241", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for tuning the parameters of MultiScale Retinex (MSR) based\ncolor image enhancement algorithm using a popular optimization method, namely,\nParticle Swarm Optimization (PSO) is presented in this paper. The image\nenhancement using MSR scheme heavily depends on parameters such as Gaussian\nsurround space constant, number of scales, gain and offset etc. Selection of\nthese parameters, empirically and its application to MSR scheme to produce\ninevitable results are the major blemishes. The method presented here results\nin huge savings of computation time as well as improvement in the visual\nquality of an image, since the PSO exploited maximizes the MSR parameters. The\nobjective of PSO is to validate the visual quality of the enhanced image\niteratively using an effective objective criterion based on entropy and edge\ninformation of an image. The PSO method of parameter optimization of MSR scheme\nachieves a very good quality of reconstructed images, far better than that\npossible with the other existing methods. Finally, the quality of the enhanced\ncolor images obtained by the proposed method are evaluated using novel metric,\nnamely, Wavelet Energy (WE). The experimental results presented show that color\nimages enhanced using the proposed scheme are clearer, more vivid and\nefficient.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 10:56:12 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Hanumantharaju", "M. C", ""], ["Ravishankar", "M.", ""], ["Rameshbabu", "D. R", ""], ["Aradhya", "V. N Manjunath", ""]]}, {"id": "1409.4095", "submitter": "Jonathan Balzer", "authors": "Jonathan Balzer, Daniel Acevedo-Feliz, Stefano Soatto, Sebastian\n  H\\\"ofer, Markus Hadwiger, and J\\\"urgen Beyerer", "title": "Cavlectometry: Towards Holistic Reconstruction of Large Mirror Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method based on the deflectometry principle for the\nreconstruction of specular objects exhibiting significant size and geometric\ncomplexity. A key feature of our approach is the deployment of an Automatic\nVirtual Environment (CAVE) as pattern generator. To unfold the full power of\nthis extraordinary experimental setup, an optical encoding scheme is developed\nwhich accounts for the distinctive topology of the CAVE. Furthermore, we devise\nan algorithm for detecting the object of interest in raw deflectometric images.\nThe segmented foreground is used for single-view reconstruction, the background\nfor estimation of the camera pose, necessary for calibrating the sensor system.\nExperiments suggest a significant gain of coverage in single measurements\ncompared to previous methods. To facilitate research on specular surface\nreconstruction, we will make our data set publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 20:11:11 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Balzer", "Jonathan", ""], ["Acevedo-Feliz", "Daniel", ""], ["Soatto", "Stefano", ""], ["H\u00f6fer", "Sebastian", ""], ["Hadwiger", "Markus", ""], ["Beyerer", "J\u00fcrgen", ""]]}, {"id": "1409.4127", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su, Tzu-Hsuan Chiu, Chun-Yen Yeh, Hsin-Fu Huang, Winston H.\n  Hsu", "title": "Transfer Learning for Video Recognition with Scarce Training Data for\n  Deep Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained video recognition and Deep Convolution Network (DCN) are two\nactive topics in computer vision recently. In this work, we apply DCNs as\nframe-based recognizers for video recognition. Our preliminary studies,\nhowever, show that video corpora with complete ground truth are usually not\nlarge and diverse enough to learn a robust model. The networks trained directly\non the video data set suffer from significant overfitting and have poor\nrecognition rate on the test set. The same lack-of-training-sample problem\nlimits the usage of deep models on a wide range of computer vision problems\nwhere obtaining training data are difficult. To overcome the problem, we\nperform transfer learning from images to videos to utilize the knowledge in the\nweakly labeled image corpus for video recognition. The image corpus help to\nlearn important visual patterns for natural images, while these patterns are\nignored by models trained only on the video corpus. Therefore, the resultant\nnetworks have better generalizability and better recognition rate. We show that\nby means of transfer learning from image to video, we can learn a frame-based\nrecognizer with only 4k videos. Because the image corpus is weakly labeled, the\nentire learning process requires only 4k annotated instances, which is far less\nthan the million scale image data sets required by previous works. The same\napproach may be applied to other visual recognition tasks where only scarce\ntraining data is available, and it improves the applicability of DCNs in\nvarious computer vision problems. Our experiments also reveal the correlation\nbetween meta-parameters and the performance of DCNs, given the properties of\nthe target problem and data. These results lead to a heuristic for\nmeta-parameter selection for future researches, which does not rely on the time\nconsuming meta-parameter search.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 01:26:55 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 14:47:54 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Chiu", "Tzu-Hsuan", ""], ["Yeh", "Chun-Yen", ""], ["Huang", "Hsin-Fu", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1409.4139", "submitter": "Yue Wang", "authors": "Liang Zhao, Jianhua Xuan, and Yue Wang", "title": "A feasible roadmap for developing volumetric probability atlas of\n  localized prostate cancer", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical volumetric model, showing the probability map of localized\nprostate cancer within the host anatomical structure, has been developed from\n90 optically-imaged surgical specimens. This master model permits an accurate\ncharacterization of prostate cancer distribution patterns and an atlas-informed\nbiopsy sampling strategy. The model is constructed by mapping individual\nprostate models onto a site model, together with localized tumors. An accurate\nmulti-object non-rigid warping scheme is developed based on a mixture of\nprincipal-axis registrations. We report our evaluation and pilot studies on the\neffectiveness of the method and its application to optimizing needle biopsy\nstrategies.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 02:03:52 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Zhao", "Liang", ""], ["Xuan", "Jianhua", ""], ["Wang", "Yue", ""]]}, {"id": "1409.4205", "submitter": "Bruno Conejo", "authors": "B. Conejo, N. Komodakis, S. Leprince, J.P. Avouac", "title": "Speeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of\n  Pruning Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general and versatile framework that significantly speeds-up\ngraphical model optimization while maintaining an excellent solution accuracy.\nThe proposed approach relies on a multi-scale pruning scheme that is able to\nprogressively reduce the solution space by use of a novel strategy based on a\ncoarse-to-fine cascade of learnt classifiers. We thoroughly experiment with\nclassic computer vision related MRF problems, where our framework constantly\nyields a significant time speed-up (with respect to the most efficient\ninference methods) and obtains a more accurate solution than directly\noptimizing the MRF.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 10:53:04 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Conejo", "B.", ""], ["Komodakis", "N.", ""], ["Leprince", "S.", ""], ["Avouac", "J. P.", ""]]}, {"id": "1409.4271", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng, and M\\'ario A. T. Figueiredo", "title": "The Ordered Weighted $\\ell_1$ Norm: Atomic Formulation, Projections, and\n  Algorithms", "comments": "13 pages, 17 figures. The latest version of this paper was submitted\n  to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ordered weighted $\\ell_1$ norm (OWL) was recently proposed, with two\ndifferent motivations: its good statistical properties as a sparsity promoting\nregularizer; the fact that it generalizes the so-called {\\it octagonal\nshrinkage and clustering algorithm for regression} (OSCAR), which has the\nability to cluster/group regression variables that are highly correlated. This\npaper contains several contributions to the study and application of OWL\nregularization: the derivation of the atomic formulation of the OWL norm; the\nderivation of the dual of the OWL norm, based on its atomic formulation; a new\nand simpler derivation of the proximity operator of the OWL norm; an efficient\nscheme to compute the Euclidean projection onto an OWL ball; the instantiation\nof the conditional gradient (CG, also known as Frank-Wolfe) algorithm for\nlinear regression problems under OWL regularization; the instantiation of\naccelerated projected gradient algorithms for the same class of problems.\nFinally, a set of experiments give evidence that accelerated projected gradient\nalgorithms are considerably faster than CG, for the class of problems\nconsidered.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 14:22:34 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 14:01:09 GMT"}, {"version": "v3", "created": "Mon, 22 Sep 2014 14:08:29 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2015 16:07:39 GMT"}, {"version": "v5", "created": "Fri, 10 Apr 2015 13:21:46 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1409.4326", "submitter": "Jure \\v{Z}bontar", "authors": "Jure \\v{Z}bontar and Yann LeCun", "title": "Computing the Stereo Matching Cost with a Convolutional Neural Network", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR), June\n  2015", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298767", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for extracting depth information from a rectified image\npair. We train a convolutional neural network to predict how well two image\npatches match and use it to compute the stereo matching cost. The cost is\nrefined by cross-based cost aggregation and semiglobal matching, followed by a\nleft-right consistency check to eliminate errors in the occluded regions. Our\nstereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and\nis currently (August 2014) the top performing method on this dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 16:54:42 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 15:08:48 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["\u017dbontar", "Jure", ""], ["LeCun", "Yann", ""]]}, {"id": "1409.4327", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman and Kristen Grauman", "title": "Zero Shot Recognition with Unreliable Attributes", "comments": "NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In principle, zero-shot learning makes it possible to train a recognition\nmodel simply by specifying the category's attributes. For example, with\nclassifiers for generic attributes like \\emph{striped} and \\emph{four-legged},\none can construct a classifier for the zebra category by enumerating which\nproperties it possesses---even without providing zebra training images. In\npractice, however, the standard zero-shot paradigm suffers because attribute\npredictions in novel images are hard to get right. We propose a novel random\nforest approach to train zero-shot models that explicitly accounts for the\nunreliability of attribute predictions. By leveraging statistics about each\nattribute's error tendencies, our method obtains more robust discriminative\nmodels for the unseen classes. We further devise extensions to handle the\nfew-shot scenario and unreliable attribute descriptions. On three datasets, we\ndemonstrate the benefit for visual category learning with zero or few training\nexamples, a critical domain for rare categories or categories defined on the\nfly.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 16:56:07 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 19:33:17 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1409.4349", "submitter": "Yonathan Aflalo Dr", "authors": "Yonathan Aflalo, Haim Brezis, Ron Kimmel", "title": "On the optimality of shape and data representation in the spectral\n  domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A proof of the optimality of the eigenfunctions of the Laplace-Beltrami\noperator (LBO) in representing smooth functions on surfaces is provided and\nadapted to the field of applied shape and data analysis. It is based on the\nCourant-Fischer min-max principle adapted to our case. % The theorem we present\nsupports the new trend in geometry processing of treating geometric structures\nby using their projection onto the leading eigenfunctions of the decomposition\nof the LBO. Utilisation of this result can be used for constructing numerically\nefficient algorithms to process shapes in their spectrum. We review a couple of\napplications as possible practical usage cases of the proposed optimality\ncriteria. % We refer to a scale invariant metric, which is also invariant to\nbending of the manifold. This novel pseudo-metric allows constructing an LBO by\nwhich a scale invariant eigenspace on the surface is defined. We demonstrate\nthe efficiency of an intermediate metric, defined as an interpolation between\nthe scale invariant and the regular one, in representing geometric structures\nwhile capturing both coarse and fine details. Next, we review a numerical\nacceleration technique for classical scaling, a member of a family of\nflattening methods known as multidimensional scaling (MDS). There, the\noptimality is exploited to efficiently approximate all geodesic distances\nbetween pairs of points on a given surface, and thereby match and compare\nbetween almost isometric surfaces. Finally, we revisit the classical principal\ncomponent analysis (PCA) definition by coupling its variational form with a\nDirichlet energy on the data manifold. By pairing the PCA with the LBO we can\nhandle cases that go beyond the scope defined by the observation set that is\nhandled by regular PCA.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 17:50:26 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Aflalo", "Yonathan", ""], ["Brezis", "Haim", ""], ["Kimmel", "Ron", ""]]}, {"id": "1409.4469", "submitter": "Dmitri Nikonov", "authors": "Dmitri E. Nikonov, Ian A. Young, and George I. Bourianoff", "title": "Convolutional Networks for Image Processing by Coupled Oscillator Arrays", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.PS cond-mat.dis-nn cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A coupled oscillator array is shown to approximate convolutions with Gabor\nfilters for image processing tasks. Pixelated image fragments and filter\nfunctions are converted to voltages, differenced, and input into a\ncorresponding array of weakly coupled Voltage Controlled Oscillators (VCOs).\nThis is referred to as Frequency Shift Keying (FSK). Upon synchronization of\nthe array, the common node amplitude provides a metric for the degree of match\nbetween the image fragment and the filter function. The optimal oscillator\nparameters for synchronization are determined and favor a moderate value of the\nQ-factor.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 23:34:15 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Nikonov", "Dmitri E.", ""], ["Young", "Ian A.", ""], ["Bourianoff", "George I.", ""]]}, {"id": "1409.4481", "submitter": "Aniket Bera", "authors": "Aniket Bera, David Wolinski, Julien Pettr\\'e, Dinesh Manocha", "title": "Real-time Crowd Tracking using Parameter Optimized Mixture of Motion\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, real-time algorithm to track the trajectory of each\npedestrian in moderately dense crowded scenes. Our formulation is based on an\nadaptive particle-filtering scheme that uses a combination of various\nmulti-agent heterogeneous pedestrian simulation models. We automatically\ncompute the optimal parameters for each of these different models based on\nprior tracked data and use the best model as motion prior for our\nparticle-filter based tracking algorithm. We also use our \"mixture of motion\nmodels\" for adaptive particle selection and accelerate the performance of the\nonline tracking algorithm. The motion model parameter estimation is formulated\nas an optimization problem, and we use an approach that solves this\ncombinatorial optimization problem in a model independent manner and hence\nscalable to any multi-agent pedestrian motion model. We evaluate the\nperformance of our approach on different crowd video datasets and highlight the\nimprovement in accuracy over homogeneous motion models and a baseline\nmean-shift based tracker. In practice, our formulation can compute trajectories\nof tens of pedestrians on a multi-core desktop CPU in in real time and offer\nhigher accuracy as compared to prior real time pedestrian tracking algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 01:36:52 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Bera", "Aniket", ""], ["Wolinski", "David", ""], ["Pettr\u00e9", "Julien", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1409.4559", "submitter": "Redouan Kochiyne", "authors": "Redouan Korchiyne, Sidi Mohamed Farssi, Abderrahmane Sbihi, Rajaa\n  Touahni, Mustapha Tahiri Alaoui", "title": "A Combined Method Of Fractal And GLCM Features For MRI And CT Scan\n  Images Classification", "comments": "13 pages, 6 figures, Signal & Image Processing : An International\n  Journal(SIPIJ)Vol.5, No.4, August 2014", "journal-ref": null, "doi": "10.5121/sipij.2014.5409", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractal analysis has been shown to be useful in image processing for\ncharacterizing shape and gray-scale complexity. The fractal feature is a\ncompact descriptor used to give a numerical measure of the degree of\nirregularity of the medical images. This descriptor property does not give\nownership of the local image structure. In this paper, we present a combination\nof this parameter based on Box Counting with GLCM Features. This powerful\ncombination has proved good results especially in classification of medical\ntexture from MRI and CT Scan images of trabecular bone. This method has the\npotential to improve clinical diagnostics tests for osteoporosis pathologies.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 10:02:00 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Korchiyne", "Redouan", ""], ["Farssi", "Sidi Mohamed", ""], ["Sbihi", "Abderrahmane", ""], ["Touahni", "Rajaa", ""], ["Alaoui", "Mustapha Tahiri", ""]]}, {"id": "1409.4627", "submitter": "Petra Budikova", "authors": "Petra Budikova, Jan Botorek, Michal Batko, Pavel Zezula", "title": "DISA at ImageCLEF 2014 Revised: Search-based Image Annotation with DeCAF\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper constitutes an extension to the report on DISA-MU team\nparticipation in the ImageCLEF 2014 Scalable Concept Image Annotation Task as\npublished in [3]. Specifically, we introduce a new similarity search component\nthat was implemented into the system, report on the results achieved by\nutilizing this component, and analyze the influence of different similarity\nsearch parameters on the annotation quality.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 13:24:44 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Budikova", "Petra", ""], ["Botorek", "Jan", ""], ["Batko", "Michal", ""], ["Zezula", "Pavel", ""]]}, {"id": "1409.4689", "submitter": "Johannes Lederer", "authors": "Johannes Lederer and Sergio Guadarrama", "title": "Compute Less to Get More: Using ORC to Improve Sparse Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Filtering is a popular feature learning algorithm for image\nclassification pipelines. In this paper, we connect the performance of Sparse\nFiltering with spectral properties of the corresponding feature matrices. This\nconnection provides new insights into Sparse Filtering; in particular, it\nsuggests early stopping of Sparse Filtering. We therefore introduce the Optimal\nRoundness Criterion (ORC), a novel stopping criterion for Sparse Filtering. We\nshow that this stopping criterion is related with pre-processing procedures\nsuch as Statistical Whitening and demonstrate that it can make image\nclassification with Sparse Filtering considerably faster and more accurate.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 16:31:07 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 09:16:03 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Lederer", "Johannes", ""], ["Guadarrama", "Sergio", ""]]}, {"id": "1409.4842", "submitter": "Christian Szegedy", "authors": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n  Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke and Andrew Rabinovich", "title": "Going Deeper with Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep convolutional neural network architecture codenamed\n\"Inception\", which was responsible for setting the new state of the art for\nclassification and detection in the ImageNet Large-Scale Visual Recognition\nChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the\nimproved utilization of the computing resources inside the network. This was\nachieved by a carefully crafted design that allows for increasing the depth and\nwidth of the network while keeping the computational budget constant. To\noptimize quality, the architectural decisions were based on the Hebbian\nprinciple and the intuition of multi-scale processing. One particular\nincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22\nlayers deep network, the quality of which is assessed in the context of\nclassification and detection.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 01:03:11 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Szegedy", "Christian", ""], ["Liu", "Wei", ""], ["Jia", "Yangqing", ""], ["Sermanet", "Pierre", ""], ["Reed", "Scott", ""], ["Anguelov", "Dragomir", ""], ["Erhan", "Dumitru", ""], ["Vanhoucke", "Vincent", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1409.4958", "submitter": "Yi Wang phd", "authors": "Yi Wang", "title": "Tensity Research Based on the Information of Eye Movement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User's mental state is concerned gradually, during the interaction course of\nhuman robot. As the measurement and identification method of psychological\nstate, tension, has certain practical significance role. At presents there is\nno suitable method of measuring the tension. Firstly, sum up some availability\nof eye movement index. And then parameters extraction on eye movement\ncharacteristics of normal illumination is studied, including the location of\nthe face, eyes location, access to the pupil diameter, the eye pupil center\ncharacteristic parameters. And with the judgment of the tension in eye images,\nextract exact information of gaze direction. Finally, through the experiment to\nprove the proposed method is effective.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 11:52:36 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Wang", "Yi", ""]]}, {"id": "1409.4995", "submitter": "Xirong Li", "authors": "Xixi He, Xirong Li, Gang Yang, Jieping Xu, Qin Jin", "title": "Adaptive Tag Selection for Image Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all tags are relevant to an image, and the number of relevant tags is\nimage-dependent. Although many methods have been proposed for image\nauto-annotation, the question of how to determine the number of tags to be\nselected per image remains open. The main challenge is that for a large tag\nvocabulary, there is often a lack of ground truth data for acquiring optimal\ncutoff thresholds per tag. In contrast to previous works that pre-specify the\nnumber of tags to be selected, we propose in this paper adaptive tag selection.\nThe key insight is to divide the vocabulary into two disjoint subsets, namely a\nseen set consisting of tags having ground truth available for optimizing their\nthresholds and a novel set consisting of tags without any ground truth. Such a\ndivision allows us to estimate how many tags shall be selected from the novel\nset according to the tags that have been selected from the seen set. The\neffectiveness of the proposed method is justified by our participation in the\nImageCLEF 2014 image annotation task. On a set of 2,065 test images with ground\ntruth available for 207 tags, the benchmark evaluation shows that compared to\nthe popular top-$k$ strategy which obtains an F-score of 0.122, adaptive tag\nselection achieves a higher F-score of 0.223. Moreover, by treating the\nunderlying image annotation system as a black box, the new method can be used\nas an easy plug-in to boost the performance of existing systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 13:50:37 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["He", "Xixi", ""], ["Li", "Xirong", ""], ["Yang", "Gang", ""], ["Xu", "Jieping", ""], ["Jin", "Qin", ""]]}, {"id": "1409.5114", "submitter": "Shuxin Ouyang", "authors": "Shuxin Ouyang, Timothy Hospedales, Yi-Zhe Song, Xueming Li", "title": "A Survey on Heterogeneous Face Recognition: Sketch, Infra-red, 3D and\n  Low-resolution", "comments": "survey paper(35 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous face recognition (HFR) refers to matching face imagery across\ndifferent domains. It has received much interest from the research community as\na result of its profound implications in law enforcement. A wide variety of new\ninvariant features, cross-modality matching models and heterogeneous datasets\nbeing established in recent years. This survey provides a comprehensive review\nof established techniques and recent developments in HFR. Moreover, we offer a\ndetailed account of datasets and benchmarks commonly used for evaluation. We\nfinish by assessing the state of the field and discussing promising directions\nfor future research.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 19:55:34 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 13:23:30 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Ouyang", "Shuxin", ""], ["Hospedales", "Timothy", ""], ["Song", "Yi-Zhe", ""], ["Li", "Xueming", ""]]}, {"id": "1409.5185", "submitter": "Zhuowen Tu", "authors": "Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen\n  Tu", "title": "Deeply-Supervised Nets", "comments": "Patent disclosure, UCSD Docket No. SD2014-313, filed on May 22, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Our proposed deeply-supervised nets (DSN) method simultaneously minimizes\nclassification error while making the learning process of hidden layers direct\nand transparent. We make an attempt to boost the classification performance by\nstudying a new formulation in deep networks. Three aspects in convolutional\nneural networks (CNN) style architectures are being looked at: (1) transparency\nof the intermediate layers to the overall classification; (2)\ndiscriminativeness and robustness of learned features, especially in the early\nlayers; (3) effectiveness in training due to the presence of the exploding and\nvanishing gradients. We introduce \"companion objective\" to the individual\nhidden layers, in addition to the overall objective at the output layer (a\ndifferent strategy to layer-wise pre-training). We extend techniques from\nstochastic gradient methods to analyze our algorithm. The advantage of our\nmethod is evident and our experimental result on benchmark datasets shows\nsignificant performance gain over existing methods (e.g. all state-of-the-art\nresults on MNIST, CIFAR-10, CIFAR-100, and SVHN).\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 04:08:25 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 05:03:06 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lee", "Chen-Yu", ""], ["Xie", "Saining", ""], ["Gallagher", "Patrick", ""], ["Zhang", "Zhengyou", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1409.5188", "submitter": "Ruxin Wang", "authors": "Ruxin Wang, Congying Han, Yanping Wu, Tiande Guo", "title": "Fingerprint Classification Based on Depth Neural Network", "comments": "14 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint classification is an effective technique for reducing the\ncandidate numbers of fingerprints in the stage of matching in automatic\nfingerprint identification system (AFIS). In recent years, deep learning is an\nemerging technology which has achieved great success in many fields, such as\nimage processing, natural language processing and so on. In this paper, we only\nchoose the orientation field as the input feature and adopt a new method\n(stacked sparse autoencoders) based on depth neural network for fingerprint\nclassification. For the four-class problem, we achieve a classification of 93.1\npercent using the depth network structure which has three hidden layers (with\n1.8% rejection) in the NIST-DB4 database. And then we propose a novel method\nusing two classification probabilities for fuzzy classification which can\neffectively enhance the accuracy of classification. By only adjusting the\nprobability threshold, we get the accuracy of classification is 96.1% (setting\nthreshold is 0.85), 97.2% (setting threshold is 0.90) and 98.0% (setting\nthreshold is 0.95). Using the fuzzy method, we obtain higher accuracy than\nother methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 04:36:19 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Wang", "Ruxin", ""], ["Han", "Congying", ""], ["Wu", "Yanping", ""], ["Guo", "Tiande", ""]]}, {"id": "1409.5209", "submitter": "Chunhua Shen", "authors": "Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel", "title": "Pedestrian Detection with Spatially Pooled Features and Structured\n  Ensemble Learning", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many typical applications of object detection operate within a prescribed\nfalse-positive range. In this situation the performance of a detector should be\nassessed on the basis of the area under the ROC curve over that range, rather\nthan over the full curve, as the performance outside the range is irrelevant.\nThis measure is labelled as the partial area under the ROC curve (pAUC). We\npropose a novel ensemble learning method which achieves a maximal detection\nrate at a user-defined range of false positive rates by directly optimizing the\npartial AUC using structured learning.\n  In order to achieve a high object detection performance, we propose a new\napproach to extract low-level visual features based on spatial pooling.\nIncorporating spatial pooling improves the translational invariance and thus\nthe robustness of the detection process. Experimental results on both synthetic\nand real-world data sets demonstrate the effectiveness of our approach, and we\nshow that it is possible to train state-of-the-art pedestrian detectors using\nthe proposed structured ensemble learning method with spatially pooled\nfeatures. The result is the current best reported performance on the\nCaltech-USA pedestrian detection dataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 07:14:33 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2014 02:35:33 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2015 10:15:37 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Paisitkriangkrai", "Sakrapee", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1409.5230", "submitter": "Jingdong Wang", "authors": "Baoguang Shi and Xiang Bai and Wenyu Liu and Jingdong Wang", "title": "Deep Regression for Face Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep regression approach for face alignment. The\ndeep architecture consists of a global layer and multi-stage local layers. We\napply the back-propagation algorithm with the dropout strategy to jointly\noptimize the regression parameters. We show that the resulting deep regressor\ngradually and evenly approaches the true facial landmarks stage by stage,\navoiding the tendency to yield over-strong early stage regressors while\nover-weak later stage regressors. Experimental results show that our approach\nachieves the state-of-the-art\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 09:10:28 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Shi", "Baoguang", ""], ["Bai", "Xiang", ""], ["Liu", "Wenyu", ""], ["Wang", "Jingdong", ""]]}, {"id": "1409.5241", "submitter": "Basura Fernando", "authors": "Basura Fernando, Amaury Habrard, Marc Sebban and Tinne Tuytelaars", "title": "Subspace Alignment For Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new domain adaptation (DA) algorithm where the\nsource and target domains are represented by subspaces spanned by eigenvectors.\nOur method seeks a domain invariant feature space by learning a mapping\nfunction which aligns the source subspace with the target one. We show that the\nsolution of the corresponding optimization problem can be obtained in a simple\nclosed form, leading to an extremely fast algorithm. We present two approaches\nto determine the only hyper-parameter in our method corresponding to the size\nof the subspaces. In the first approach we tune the size of subspaces using a\ntheoretical bound on the stability of the obtained result. In the second\napproach, we use maximum likelihood estimation to determine the subspace size,\nwhich is particularly useful for high dimensional data. Apart from PCA, we\npropose a subspace creation method that outperform partial least squares (PLS)\nand linear discriminant analysis (LDA) in domain adaptation. We test our method\non various datasets and show that, despite its intrinsic simplicity, it\noutperforms state of the art DA methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 09:57:41 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 08:40:06 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Fernando", "Basura", ""], ["Habrard", "Amaury", ""], ["Sebban", "Marc", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1409.5400", "submitter": "Tobias Weyand", "authors": "Tobias Weyand and Bastian Leibe", "title": "Visual Landmark Recognition from Internet Photo Collections: A\n  Large-Scale Evaluation", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2015.02.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of a visual landmark recognition system is to identify photographed\nbuildings or objects in query photos and to provide the user with relevant\ninformation on them. With their increasing coverage of the world's landmark\nbuildings and objects, Internet photo collections are now being used as a\nsource for building such systems in a fully automatic fashion. This process\ntypically consists of three steps: clustering large amounts of images by the\nobjects they depict; determining object names from user-provided tags; and\nbuilding a robust, compact, and efficient recognition index. To this date,\nhowever, there is little empirical information on how well current approaches\nfor those steps perform in a large-scale open-set mining and recognition task.\nFurthermore, there is little empirical information on how recognition\nperformance varies for different types of landmark objects and where there is\nstill potential for improvement. With this paper, we intend to fill these gaps.\nUsing a dataset of 500k images from Paris, we analyze each component of the\nlandmark recognition pipeline in order to answer the following questions: How\nmany and what kinds of objects can be discovered automatically? How can we best\nuse the resulting image clusters to recognize the object in a query? How can\nthe object be efficiently represented in memory for recognition? How reliably\ncan semantic information be extracted? And finally: What are the limiting\nfactors in the resulting pipeline from query to semantics? We evaluate how\ndifferent choices of methods and parameters for the individual pipeline steps\naffect overall system performance and examine their effects for different query\ncategories such as buildings, paintings or sculptures.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 18:28:20 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Weyand", "Tobias", ""], ["Leibe", "Bastian", ""]]}, {"id": "1409.5403", "submitter": "Ross Girshick", "authors": "Ross Girshick, Forrest Iandola, Trevor Darrell, Jitendra Malik", "title": "Deformable Part Models are Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable part models (DPMs) and convolutional neural networks (CNNs) are\ntwo widely used tools for visual recognition. They are typically viewed as\ndistinct approaches: DPMs are graphical models (Markov random fields), while\nCNNs are \"black-box\" non-linear classifiers. In this paper, we show that a DPM\ncan be formulated as a CNN, thus providing a novel synthesis of the two ideas.\nOur construction involves unrolling the DPM inference algorithm and mapping\neach step to an equivalent (and at times novel) CNN layer. From this\nperspective, it becomes natural to replace the standard image features used in\nDPM with a learned feature extractor. We call the resulting model DeepPyramid\nDPM and experimentally validate it on PASCAL VOC. DeepPyramid DPM significantly\noutperforms DPMs based on histograms of oriented gradients features (HOG) and\nslightly outperforms a comparable version of the recently introduced R-CNN\ndetection system, while running an order of magnitude faster.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 18:34:10 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 18:44:14 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Girshick", "Ross", ""], ["Iandola", "Forrest", ""], ["Darrell", "Trevor", ""], ["Malik", "Jitendra", ""]]}, {"id": "1409.5729", "submitter": "Qi Wei", "authors": "Qi Wei, Jos\\'e Bioucas-Dias, Nicolas Dobigeon, and Jean-Yves Tourneret", "title": "Hyperspectral and Multispectral Image Fusion based on a Sparse\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a variational based approach to fusing hyperspectral and\nmultispectral images. The fusion process is formulated as an inverse problem\nwhose solution is the target image assumed to live in a much lower dimensional\nsubspace. A sparse regularization term is carefully designed, relying on a\ndecomposition of the scene on a set of dictionaries. The dictionary atoms and\nthe corresponding supports of active coding coefficients are learned from the\nobserved images. Then, conditionally on these dictionaries and supports, the\nfusion problem is solved via alternating optimization with respect to the\ntarget image (using the alternating direction method of multipliers) and the\ncoding coefficients. Simulation results demonstrate the efficiency of the\nproposed algorithm when compared with the state-of-the-art fusion methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 17:07:53 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Wei", "Qi", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1409.5763", "submitter": "Jin  Xu", "authors": "Jin Xu, Haibo He, Hong Man", "title": "Active Dictionary Learning in Sparse Representation Based Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation, which uses dictionary atoms to reconstruct input\nvectors, has been studied intensively in recent years. A proper dictionary is a\nkey for the success of sparse representation. In this paper, an active\ndictionary learning (ADL) method is introduced, in which classification error\nand reconstruction error are considered as the active learning criteria in\nselection of the atoms for dictionary construction. The learned dictionaries\nare caculated in sparse representation based classification (SRC). The\nclassification accuracy and reconstruction error are used to evaluate the\nproposed dictionary learning method. The performance of the proposed dictionary\nlearning method is compared with other methods, including unsupervised\ndictionary learning and whole-training-data dictionary. The experimental\nresults based on the UCI data sets and face data set demonstrate the efficiency\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 19:03:57 GMT"}, {"version": "v2", "created": "Sat, 27 Sep 2014 00:24:11 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Xu", "Jin", ""], ["He", "Haibo", ""], ["Man", "Hong", ""]]}, {"id": "1409.5786", "submitter": "Xi Peng", "authors": "Xi Peng, Rui Yan, Bo Zhao, Huajin Tang, Zhang Yi", "title": "Fast Low-rank Representation based Spatial Pyramid Matching for Image\n  Classification", "comments": "accepted into knowledge based systems, 2015", "journal-ref": "Knowledge based Systems, 2015, 90, p.14-22", "doi": "10.1016/j.knosys.2015.10.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial Pyramid Matching (SPM) and its variants have achieved a lot of\nsuccess in image classification. The main difference among them is their\nencoding schemes. For example, ScSPM incorporates Sparse Code (SC) instead of\nVector Quantization (VQ) into the framework of SPM. Although the methods\nachieve a higher recognition rate than the traditional SPM, they consume more\ntime to encode the local descriptors extracted from the image. In this paper,\nwe propose using Low Rank Representation (LRR) to encode the descriptors under\nthe framework of SPM. Different from SC, LRR considers the group effect among\ndata points instead of sparsity. Benefiting from this property, the proposed\nmethod (i.e., LrrSPM) can offer a better performance. To further improve the\ngeneralizability and robustness, we reformulate the rank-minimization problem\nas a truncated projection problem. Extensive experimental studies show that\nLrrSPM is more efficient than its counterparts (e.g., ScSPM) while achieving\ncompetitive recognition rates on nine image data sets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 13:35:34 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 14:12:58 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Peng", "Xi", ""], ["Yan", "Rui", ""], ["Zhao", "Bo", ""], ["Tang", "Huajin", ""], ["Yi", "Zhang", ""]]}, {"id": "1409.5957", "submitter": "Shahar Kovalsky", "authors": "Shahar Z. Kovalsky, Daniel Glasner, Ronen Basri", "title": "A Global Approach for Solving Edge-Matching Puzzles", "comments": null, "journal-ref": "SIAM J. Imaging Sciences, Vol. 8, Issue 2, 916--938, 2015", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider apictorial edge-matching puzzles, in which the goal is to arrange\na collection of puzzle pieces with colored edges so that the colors match along\nthe edges of adjacent pieces. We devise an algebraic representation for this\nproblem and provide conditions under which it exactly characterizes a puzzle.\nUsing the new representation, we recast the combinatorial, discrete problem of\nsolving puzzles as a global, polynomial system of equations with continuous\nvariables. We further propose new algorithms for generating approximate\nsolutions to the continuous problem by solving a sequence of convex\nrelaxations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 09:10:24 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 08:24:48 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Kovalsky", "Shahar Z.", ""], ["Glasner", "Daniel", ""], ["Basri", "Ronen", ""]]}, {"id": "1409.6041", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang", "title": "Domain Adaptive Neural Networks for Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple neural network model to deal with the domain adaptation\nproblem in object recognition. Our model incorporates the Maximum Mean\nDiscrepancy (MMD) measure as a regularization in the supervised learning to\nreduce the distribution mismatch between the source and target domains in the\nlatent space. From experiments, we demonstrate that the MMD regularization is\nan effective tool to provide good domain adaptation models on both SURF\nfeatures and raw image pixels of a particular image data set. We also show that\nour proposed model, preceded by the denoising auto-encoder pretraining,\nachieves better performance than recent benchmark models on the same data sets.\nThis work represents the first study of MMD measure in the context of neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 20:42:00 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1409.6045", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Analyzing sparse dictionaries for online learning with kernels", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many signal processing and machine learning methods share essentially the\nsame linear-in-the-parameter model, with as many parameters as available\nsamples as in kernel-based machines. Sparse approximation is essential in many\ndisciplines, with new challenges emerging in online learning with kernels. To\nthis end, several sparsity measures have been proposed in the literature to\nquantify sparse dictionaries and constructing relevant ones, the most prolific\nones being the distance, the approximation, the coherence and the Babel\nmeasures. In this paper, we analyze sparse dictionaries based on these\nmeasures. By conducting an eigenvalue analysis, we show that these sparsity\nmeasures share many properties, including the linear independence condition and\ninducing a well-posed optimization problem. Furthermore, we prove that there\nexists a quasi-isometry between the parameter (i.e., dual) space and the\ndictionary's induced feature space.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 21:46:19 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1409.6046", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Approximation errors of online sparsification criteria", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/TSP.2015.2442960", "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning frameworks, such as resource-allocating networks,\nkernel-based methods, Gaussian processes, and radial-basis-function networks,\nrequire a sparsification scheme in order to address the online learning\nparadigm. For this purpose, several online sparsification criteria have been\nproposed to restrict the model definition on a subset of samples. The most\nknown criterion is the (linear) approximation criterion, which discards any\nsample that can be well represented by the already contributing samples, an\noperation with excessive computational complexity. Several computationally\nefficient sparsification criteria have been introduced in the literature, such\nas the distance, the coherence and the Babel criteria. In this paper, we\nprovide a framework that connects these sparsification criteria to the issue of\napproximating samples, by deriving theoretical bounds on the approximation\nerrors. Moreover, we investigate the error of approximating any feature, by\nproposing upper-bounds on the approximation error for each of the\naforementioned sparsification criteria. Two classes of features are described\nin detail, the empirical mean and the principal axes in the kernel principal\ncomponent analysis.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 21:53:08 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1409.6070", "submitter": "Benjamin Graham", "authors": "Benjamin Graham", "title": "Spatially-sparse convolutional neural networks", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) perform well on problems such as\nhandwriting recognition and image classification. However, the performance of\nthe networks is often limited by budget and time constraints, particularly when\ntrying to train deep networks.\n  Motivated by the problem of online handwriting recognition, we developed a\nCNN for processing spatially-sparse inputs; a character drawn with a one-pixel\nwide pen on a high resolution grid looks like a sparse matrix. Taking advantage\nof the sparsity allowed us more efficiently to train and test large, deep CNNs.\nOn the CASIA-OLHWDB1.1 dataset containing 3755 character classes we get a test\nerror of 3.82%.\n  Although pictures are not sparse, they can be thought of as sparse by adding\npadding. Applying a deep convolutional network using sparsity has resulted in a\nsubstantial reduction in test error on the CIFAR small picture datasets: 6.28%\non CIFAR-10 and 24.30% for CIFAR-100.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 02:39:27 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Graham", "Benjamin", ""]]}, {"id": "1409.6080", "submitter": "Adway Mitra", "authors": "Adway Mitra, Soma Biswas, Chiranjib Bhattacharyya", "title": "Temporally Coherent Bayesian Models for Entity Discovery in Videos by\n  Tracklet Clustering", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A video can be represented as a sequence of tracklets, each spanning 10-20\nframes, and associated with one entity (eg. a person). The task of \\emph{Entity\nDiscovery} in videos can be naturally posed as tracklet clustering. We approach\nthis task by leveraging \\emph{Temporal Coherence}(TC): the fundamental property\nof videos that each tracklet is likely to be associated with the same entity as\nits temporal neighbors. Our major contributions are the first Bayesian\nnonparametric models for TC at tracklet-level. We extend Chinese Restaurant\nProcess (CRP) to propose TC-CRP, and further to Temporally Coherent Chinese\nRestaurant Franchise (TC-CRF) to jointly model short temporal segments. On the\ntask of discovering persons in TV serial videos without meta-data like scripts,\nthese methods show considerable improvement in cluster purity and person\ncoverage compared to state-of-the-art approaches to tracklet clustering. We\nrepresent entities with mixture components, and tracklets with vectors of very\ngeneric features, which can work for any type of entity (not necessarily\nperson). The proposed methods can perform online tracklet clustering on\nstreaming videos with little performance deterioration unlike existing\napproaches, and can automatically reject tracklets resulting from false\ndetections. Finally we discuss entity-driven video summarization- where some\ntemporal segments of the video are selected automatically based on the\ndiscovered entities.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 04:31:12 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 19:34:08 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Mitra", "Adway", ""], ["Biswas", "Soma", ""], ["Bhattacharyya", "Chiranjib", ""]]}, {"id": "1409.6155", "submitter": "Hei Law", "authors": "Cewu Lu, Hao Chen, Qifeng Chen, Hei Law, Yao Xiao and Chi-Keung Tang", "title": "1-HKUST: Object Detection in ILSVRC 2014", "comments": "3 pages; Author list fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Imagenet Large Scale Visual Recognition Challenge (ILSVRC) is the one of\nthe most important big data challenges to date. We participated in the object\ndetection track of ILSVRC 2014 and received the fourth place among the 38\nteams. We introduce in our object detection system a number of novel techniques\nin localization and recognition. For localization, initial candidate proposals\nare generated using selective search, and a novel bounding boxes regression\nmethod is used for better object localization. For recognition, to represent a\ncandidate proposal, we adopt three features, namely, RCNN feature, IFV feature,\nand DPM feature. Given these features, category-specific combination functions\nare learned to improve the object recognition rate. In addition, object context\nin the form of background priors and object interaction priors are learned and\napplied in our system. Our ILSVRC 2014 results are reported alongside with the\nresults of other participating teams.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 12:42:08 GMT"}, {"version": "v2", "created": "Wed, 24 Sep 2014 01:45:02 GMT"}, {"version": "v3", "created": "Sun, 5 Oct 2014 13:05:24 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Lu", "Cewu", ""], ["Chen", "Hao", ""], ["Chen", "Qifeng", ""], ["Law", "Hei", ""], ["Xiao", "Yao", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1409.6235", "submitter": "Shiry Ginosar", "authors": "Shiry Ginosar, Daniel Haas, Timothy Brown, and Jitendra Malik", "title": "Detecting People in Cubist Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the human visual system is surprisingly robust to extreme distortion\nwhen recognizing objects, most evaluations of computer object detection methods\nfocus only on robustness to natural form deformations such as people's pose\nchanges. To determine whether algorithms truly mirror the flexibility of human\nvision, they must be compared against human vision at its limits. For example,\nin Cubist abstract art, painted objects are distorted by object fragmentation\nand part-reorganization, to the point that human vision often fails to\nrecognize them. In this paper, we evaluate existing object detection methods on\nthese abstract renditions of objects, comparing human annotators to four\nstate-of-the-art object detectors on a corpus of Picasso paintings. Our results\ndemonstrate that while human perception significantly outperforms current\nmethods, human perception and part-based models exhibit a similarly graceful\ndegradation in object detection performance as the objects become increasingly\nabstract and fragmented, corroborating the theory of part-based object\nrepresentation in the brain.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 16:37:57 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Ginosar", "Shiry", ""], ["Haas", "Daniel", ""], ["Brown", "Timothy", ""], ["Malik", "Jitendra", ""]]}, {"id": "1409.6440", "submitter": "Rashid Khogali", "authors": "Rashid Khogali", "title": "A non-linear learning & classification algorithm that achieves full\n  training accuracy with stellar classification accuracy", "comments": "43 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast Non-linear and non-iterative learning and classification algorithm is\nsynthesized and validated. This algorithm named the \"Reverse Ripple\nEffect(R.R.E)\", achieves 100% learning accuracy but is computationally\nexpensive upon classification. The R.R.E is a (deterministic) algorithm that\nsuper imposes Gaussian weighted functions on training points. In this work, the\nR.R.E algorithm is compared against known learning and classification\ntechniques/algorithms such as: the Perceptron Criterion algorithm, Linear\nSupport Vector machines, the Linear Fisher Discriminant and a simple Neural\nNetwork. The classification accuracy of the R.R.E algorithm is evaluated using\nsimulations conducted in MATLAB. The R.R.E algorithm's behaviour is analyzed\nunder linearly and non-linearly separable data sets. For the comparison with\nthe Neural Network, the classical XOR problem is considered.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 08:17:07 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 17:46:07 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Khogali", "Rashid", ""]]}, {"id": "1409.6448", "submitter": "Bo Han", "authors": "Bo Han, Bo He, Tingting Sun, Mengmeng Ma, Amaury Lendasse", "title": "HSR: L1/2 Regularized Sparse Representation for Fast Face Recognition\n  using Hierarchical Feature Selection", "comments": "Submitted to IEEE Computational Intelligence Magazine in 09/2014", "journal-ref": null, "doi": "10.1007/s00521-015-1907-y", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for fast face recognition called\nL1/2 Regularized Sparse Representation using Hierarchical Feature Selection\n(HSR). By employing hierarchical feature selection, we can compress the scale\nand dimension of global dictionary, which directly contributes to the decrease\nof computational cost in sparse representation that our approach is strongly\nrooted in. It consists of Gabor wavelets and Extreme Learning Machine\nAuto-Encoder (ELM-AE) hierarchically. For Gabor wavelets part, local features\ncan be extracted at multiple scales and orientations to form Gabor-feature\nbased image, which in turn improves the recognition rate. Besides, in the\npresence of occluded face image, the scale of Gabor-feature based global\ndictionary can be compressed accordingly because redundancies exist in\nGabor-feature based occlusion dictionary. For ELM-AE part, the dimension of\nGabor-feature based global dictionary can be compressed because\nhigh-dimensional face images can be rapidly represented by low-dimensional\nfeature. By introducing L1/2 regularization, our approach can produce sparser\nand more robust representation compared to regularized Sparse Representation\nbased Classification (SRC), which also contributes to the decrease of the\ncomputational cost in sparse representation. In comparison with related work\nsuch as SRC and Gabor-feature based SRC (GSRC), experimental results on a\nvariety of face databases demonstrate the great advantage of our method for\ncomputational cost. Moreover, we also achieve approximate or even better\nrecognition rate.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 08:36:05 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Han", "Bo", ""], ["He", "Bo", ""], ["Sun", "Tingting", ""], ["Ma", "Mengmeng", ""], ["Lendasse", "Amaury", ""]]}, {"id": "1409.6498", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Anqi Qiu, Seongho Seo, Houri K. Vorperian", "title": "Unified Heat Kernel Regression for Diffusion, Kernel Smoothing and\n  Wavelets on Manifolds and Its Application to Mandible Growth Modeling in CT\n  Images", "comments": "Accepted in Medical Image Analysis", "journal-ref": "Medical Image Analysis 2015 22:63-76", "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel kernel regression framework for smoothing scalar surface\ndata using the Laplace-Beltrami eigenfunctions. Starting with the heat kernel\nconstructed from the eigenfunctions, we formulate a new bivariate kernel\nregression framework as a weighted eigenfunction expansion with the heat kernel\nas the weights. The new kernel regression is mathematically equivalent to\nisotropic heat diffusion, kernel smoothing and recently popular diffusion\nwavelets. Unlike many previous partial differential equation based approaches\ninvolving diffusion, our approach represents the solution of diffusion\nanalytically, reducing numerical inaccuracy and slow convergence. The numerical\nimplementation is validated on a unit sphere using spherical harmonics. As an\nillustration, we have applied the method in characterizing the localized growth\npattern of mandible surfaces obtained in CT images from subjects between ages 0\nand 20 years by regressing the length of displacement vectors with respect to\nthe template surface.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 11:45:19 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 11:21:50 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Chung", "Moo K.", ""], ["Qiu", "Anqi", ""], ["Seo", "Seongho", ""], ["Vorperian", "Houri K.", ""]]}, {"id": "1409.6689", "submitter": "Ahmad Hassanat", "authors": "Ahmad Basheer Hassanat", "title": "Visual Words for Automatic Lip-Reading", "comments": null, "journal-ref": "PhD thesis, the University of Buckingham, 2009", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip reading is used to understand or interpret speech without hearing it, a\ntechnique especially mastered by people with hearing difficulties. The ability\nto lip read enables a person with a hearing impairment to communicate with\nothers and to engage in social activities, which otherwise would be difficult.\nRecent advances in the fields of computer vision, pattern recognition, and\nsignal processing has led to a growing interest in automating this challenging\ntask of lip reading. Indeed, automating the human ability to lip read, a\nprocess referred to as visual speech recognition, could open the door for other\nnovel applications. This thesis investigates various issues faced by an\nautomated lip-reading system and proposes a novel \"visual words\" based approach\nto automatic lip reading. The proposed approach includes a novel automatic face\nlocalisation scheme and a lip localisation method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 21:58:02 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Hassanat", "Ahmad Basheer", ""]]}, {"id": "1409.6745", "submitter": "Ifeoma Nwogu", "authors": "Ifeoma Nwogu, Goker Erdogan, Ilker Yildirim, Robert Jacobs", "title": "A Concept Learning Approach to Multisensory Object Perception", "comments": "6 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a computational model of concept learning using Bayesian\ninference for a grammatically structured hypothesis space, and test the model\non multisensory (visual and haptics) recognition of 3D objects. The study is\nperformed on a set of artificially generated 3D objects known as fribbles,\nwhich are complex, multipart objects with categorical structures. The goal of\nthis work is to develop a working multisensory representational model that\nintegrates major themes on concepts and concepts learning from the cognitive\nscience literature. The model combines the representational power of a\nprobabilistic generative grammar with the inferential power of Bayesian\ninduction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 20:25:46 GMT"}], "update_date": "2014-09-25", "authors_parsed": [["Nwogu", "Ifeoma", ""], ["Erdogan", "Goker", ""], ["Yildirim", "Ilker", ""], ["Jacobs", "Robert", ""]]}, {"id": "1409.6813", "submitter": "Hossein Rahmani", "authors": "Hossein Rahmani, Arif Mahmood, Du Huynh, Ajmal Mian", "title": "Histogram of Oriented Principal Components for Cross-View Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques for 3D action recognition are sensitive to viewpoint\nvariations because they extract features from depth images which are viewpoint\ndependent. In contrast, we directly process pointclouds for cross-view action\nrecognition from unknown and unseen views. We propose the Histogram of Oriented\nPrincipal Components (HOPC) descriptor that is robust to noise, viewpoint,\nscale and action speed variations. At a 3D point, HOPC is computed by\nprojecting the three scaled eigenvectors of the pointcloud within its local\nspatio-temporal support volume onto the vertices of a regular dodecahedron.\nHOPC is also used for the detection of Spatio-Temporal Keypoints (STK) in 3D\npointcloud sequences so that view-invariant STK descriptors (or Local HOPC\ndescriptors) at these key locations only are used for action recognition. We\nalso propose a global descriptor computed from the normalized spatio-temporal\ndistribution of STKs in 4-D, which we refer to as STK-D. We have evaluated the\nperformance of our proposed descriptors against nine existing techniques on two\ncross-view and three single-view human action recognition datasets. The\nExperimental results show that our techniques provide significant improvement\nover state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 03:57:49 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 05:12:27 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Rahmani", "Hossein", ""], ["Mahmood", "Arif", ""], ["Huynh", "Du", ""], ["Mian", "Ajmal", ""]]}, {"id": "1409.6838", "submitter": "Dacheng Tao", "authors": "Ruxin Wang, Dacheng Tao", "title": "Recent Progress in Image Deblurring", "comments": "53 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper comprehensively reviews the recent development of image\ndeblurring, including non-blind/blind, spatially invariant/variant deblurring\ntechniques. Indeed, these techniques share the same objective of inferring a\nlatent sharp image from one or several corresponding blurry images, while the\nblind deblurring techniques are also required to derive an accurate blur\nkernel. Considering the critical role of image restoration in modern imaging\nsystems to provide high-quality images under complex environments such as\nmotion, undesirable lighting conditions, and imperfect system components, image\ndeblurring has attracted growing attention in recent years. From the viewpoint\nof how to handle the ill-posedness which is a crucial issue in deblurring\ntasks, existing methods can be grouped into five categories: Bayesian inference\nframework, variational methods, sparse representation-based methods,\nhomography-based modeling, and region-based methods. In spite of achieving a\ncertain level of development, image deblurring, especially the blind case, is\nlimited in its success by complex application conditions which make the blur\nkernel hard to obtain and be spatially variant. We provide a holistic\nunderstanding and deep insight into image deblurring in this review. An\nanalysis of the empirical evidence for representative methods, practical\nissues, as well as a discussion of promising future directions are also\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 06:08:35 GMT"}], "update_date": "2014-09-25", "authors_parsed": [["Wang", "Ruxin", ""], ["Tao", "Dacheng", ""]]}, {"id": "1409.6911", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Xiangyang Xue", "title": "Do More Dropouts in Pool5 Feature Maps for Better Object Detection", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have gained great success in image\nclassification and object detection. In these fields, the outputs of all layers\nof CNNs are usually considered as a high dimensional feature vector extracted\nfrom an input image and the correspondence between finer level feature vectors\nand concepts that the input image contains is all-important. However, fewer\nstudies focus on this deserving issue. On considering the correspondence, we\npropose a novel approach which generates an edited version for each original\nCNN feature vector by applying the maximum entropy principle to abandon\nparticular vectors. These selected vectors correspond to the unfriendly\nconcepts in each image category. The classifier trained from merged feature\nsets can significantly improve model generalization of individual categories\nwhen training data is limited. The experimental results for\nclassification-based object detection on canonical datasets including VOC 2007\n(60.1%), 2010 (56.4%) and 2012 (56.3%) show obvious improvement in mean average\nprecision (mAP) with simple linear support vector machines.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 11:50:48 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 12:27:23 GMT"}, {"version": "v3", "created": "Tue, 18 Nov 2014 17:34:22 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1409.7164", "submitter": "Xiang Bai", "authors": "Zhuotun Zhu, Xinggang Wang, Song Bai, Cong Yao, Xiang Bai", "title": "Deep Learning Representation using Autoencoder for 3D Shape Retrieval", "comments": "6 pages, 7 figures, 2014ICSPAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of how to build a deep learning representation for 3D\nshape. Deep learning has shown to be very effective in variety of visual\napplications, such as image classification and object detection. However, it\nhas not been successfully applied to 3D shape recognition. This is because 3D\nshape has complex structure in 3D space and there are limited number of 3D\nshapes for feature learning. To address these problems, we project 3D shapes\ninto 2D space and use autoencoder for feature learning on the 2D images. High\naccuracy 3D shape retrieval performance is obtained by aggregating the features\nlearned on 2D images. In addition, we show the proposed deep learning feature\nis complementary to conventional local image descriptors. By combing the global\ndeep learning representation and the local descriptor representation, our\nmethod can obtain the state-of-the-art performance on 3D shape retrieval\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 06:27:28 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Zhu", "Zhuotun", ""], ["Wang", "Xinggang", ""], ["Bai", "Song", ""], ["Yao", "Cong", ""], ["Bai", "Xiang", ""]]}, {"id": "1409.7272", "submitter": "Ulrich Stern", "authors": "Ulrich Stern and Chung-Hui Yang", "title": "Ctrax extensions for tracking in difficult lighting conditions", "comments": null, "journal-ref": "Scientific Reports 5 (2015), 10432", "doi": "10.1038/srep10432", "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fly tracking software Ctrax by Branson et al. is popular for positional\ntracking of animals both within and beyond the fly community. Ctrax was not\ndesigned to handle tracking in difficult lighting conditions with strong\nshadows or recurring \"on\"/\"off\" changes in lighting - a condition that will\nlikely become increasingly common due to the advent of red-shifted\nchannelrhodopsin. We describe Ctrax extensions we developed that address this\nproblem. The extensions enabled good tracking accuracy in three types of\ndifficult lighting conditions in our lab. Our technique handling shadows relies\non \"single animal tracking\"; the other techniques should be widely applicable.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 14:35:27 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Stern", "Ulrich", ""], ["Yang", "Chung-Hui", ""]]}, {"id": "1409.7307", "submitter": "YuGei Gan", "authors": "Yufei Gan, Tong Zhuo, Chu He", "title": "Image Classification with A Deep Network Model based on Compressive\n  Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To simplify the parameter of the deep learning network, a cascaded\ncompressive sensing model \"CSNet\" is implemented for image classification.\nFirstly, we use cascaded compressive sensing network to learn feature from the\ndata. Secondly, CSNet generates the feature by binary hashing and block-wise\nhistograms. Finally, a linear SVM classifier is used to classify these\nfeatures. The experiments on the MNIST dataset indicate that higher\nclassification accuracy can be obtained by this algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 15:52:05 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Gan", "Yufei", ""], ["Zhuo", "Tong", ""], ["He", "Chu", ""]]}, {"id": "1409.7313", "submitter": "YuGei Gan", "authors": "Yufei Gan, Teng Yang, Chu He", "title": "A Deep Graph Embedding Network Model for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new deep learning network \"GENet\", it combines\nthe multi-layer network architec- ture and graph embedding framework. Firstly,\nwe use simplest unsupervised learning PCA/LDA as first layer to generate the\nlow- level feature. Secondly, many cascaded dimensionality reduction layers\nbased on graph embedding framework are applied to GENet. Finally, a linear SVM\nclassifier is used to classify dimension-reduced features. The experiments\nindicate that higher classification accuracy can be obtained by this algorithm\non the CMU-PIE, ORL, Extended Yale B dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 16:14:18 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Gan", "Yufei", ""], ["Yang", "Teng", ""], ["He", "Chu", ""]]}, {"id": "1409.7450", "submitter": "Jing Qin", "authors": "Jing Qin and Weihong Guo", "title": "Two-stage Geometric Information Guided Image Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressive sensing, it is challenging to reconstruct image of high\nquality from very few noisy linear projections. Existing methods mostly work\nwell on piecewise constant images but not so well on piecewise smooth images\nsuch as natural images, medical images that contain a lot of details. We\npropose a two-stage method called GeoCS to recover images with rich geometric\ninformation from very limited amount of noisy measurements. The method adopts\nthe shearlet transform that is mathematically proven to be optimal in sparsely\nrepresenting images containing anisotropic features such as edges, corners,\nspikes etc. It also uses the weighted total variation (TV) sparsity with\nspatially variant weights to preserve sharp edges but to reduce the staircase\neffects of TV. Geometric information extracted from the results of stage I\nserves as an initial prior for stage II which alternates image reconstruction\nand geometric information update in a mutually beneficial way. GeoCS has been\ntested on incomplete spectral Fourier samples. It is applicable to other types\nof measurements as well. Experimental results on various complicated images\nshow that GeoCS is efficient and generates high-quality images.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 00:49:04 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 22:17:35 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Qin", "Jing", ""], ["Guo", "Weihong", ""]]}, {"id": "1409.7474", "submitter": "Zhongbin Li", "authors": "Zhongbin Li, Wenzhong Shi, Qunming Wang, and Zelang Miao", "title": "Extracting man-made objects from remote sensing images via fast level\n  set evolutions", "comments": "This paper includes 31 pages and 12 figures", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, Vol.53(2),\n  pp.883-899, 2015", "doi": "10.1109/TGRS.2015.2454251", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object extraction from remote sensing images has long been an intensive\nresearch topic in the field of surveying and mapping. Most existing methods are\ndevoted to handling just one type of object and little attention has been paid\nto improving the computational efficiency. In recent years, level set evolution\n(LSE) has been shown to be very promising for object extraction in the\ncommunity of image processing and computer vision because it can handle\ntopological changes automatically while achieving high accuracy. However, the\napplication of state-of-the-art LSEs is compromised by laborious parameter\ntuning and expensive computation. In this paper, we proposed two fast LSEs for\nman-made object extraction from high spatial resolution remote sensing images.\nThe traditional mean curvature-based regularization term is replaced by a\nGaussian kernel and it is mathematically sound to do that. Thus a larger time\nstep can be used in the numerical scheme to expedite the proposed LSEs. In\ncontrast to existing methods, the proposed LSEs are significantly faster. Most\nimportantly, they involve much fewer parameters while achieving better\nperformance. The advantages of the proposed LSEs over other state-of-the-art\napproaches have been verified by a range of experiments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 06:17:23 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Li", "Zhongbin", ""], ["Shi", "Wenzhong", ""], ["Wang", "Qunming", ""], ["Miao", "Zelang", ""]]}, {"id": "1409.7480", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Ahmed Elgammal", "title": "Generalized Twin Gaussian Processes using Sharma-Mittal Divergence", "comments": "This work got accepted for Publication in the Machine Learning\n  Journal 2015. The work is scheduled for presentation at ECML-PKDD 2015\n  journal track papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a growing interest in mutual information measures due to their\nwide range of applications in Machine Learning and Computer Vision. In this\npaper, we present a generalized structured regression framework based on\nShama-Mittal divergence, a relative entropy measure, which is introduced to the\nMachine Learning community in this work. Sharma-Mittal (SM) divergence is a\ngeneralized mutual information measure for the widely used R\\'enyi, Tsallis,\nBhattacharyya, and Kullback-Leibler (KL) relative entropies. Specifically, we\nstudy Sharma-Mittal divergence as a cost function in the context of the Twin\nGaussian Processes (TGP)~\\citep{Bo:2010}, which generalizes over the\nKL-divergence without computational penalty. We show interesting properties of\nSharma-Mittal TGP (SMTGP) through a theoretical analysis, which covers missing\ninsights in the traditional TGP formulation. However, we generalize this theory\nbased on SM-divergence instead of KL-divergence which is a special case.\nExperimentally, we evaluated the proposed SMTGP framework on several datasets.\nThe results show that SMTGP reaches better predictions than KL-based TGP, since\nit offers a bigger class of models through its parameters that we learn from\nthe data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 06:46:38 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 13:32:50 GMT"}, {"version": "v3", "created": "Fri, 3 Oct 2014 03:54:41 GMT"}, {"version": "v4", "created": "Mon, 6 Oct 2014 03:47:51 GMT"}, {"version": "v5", "created": "Mon, 1 Jun 2015 06:30:29 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1409.7556", "submitter": "Basura Fernando", "authors": "Basura Fernando, Tatiana Tommasi, Tinne Tuytelaars", "title": "Location Recognition Over Large Time Lags", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Would it be possible to automatically associate ancient pictures to modern\nones and create fancy cultural heritage city maps? We introduce here the task\nof recognizing the location depicted in an old photo given modern annotated\nimages collected from the Internet. We present an extensive analysis on\ndifferent features, looking for the most discriminative and most robust to the\nimage variability induced by large time lags. Moreover, we show that the\ndescribed task benefits from domain adaptation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 12:36:54 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2015 15:58:17 GMT"}, {"version": "v3", "created": "Tue, 26 May 2015 03:14:19 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Fernando", "Basura", ""], ["Tommasi", "Tatiana", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1409.7618", "submitter": "Wenhan Luo", "authors": "Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu,\n  Xiaowei Zhao, Tae-Kyun Kim", "title": "Multiple Object Tracking: A Literature Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple Object Tracking (MOT) is an important computer vision problem which\nhas gained increasing attention due to its academic and commercial potential.\nAlthough different kinds of approaches have been proposed to tackle this\nproblem, it still remains challenging due to factors like abrupt appearance\nchanges and severe object occlusions. In this work, we contribute the first\ncomprehensive and most recent review on this problem. We inspect the recent\nadvances in various aspects and propose some interesting directions for future\nresearch. To the best of our knowledge, there has not been any extensive review\non this topic in the community. We endeavor to provide a thorough review on the\ndevelopment of this problem in recent decades. The main contributions of this\nreview are fourfold: 1) Key aspects in a multiple object tracking system,\nincluding formulation, categorization, key principles, evaluation of an MOT are\ndiscussed. 2) Instead of enumerating individual works, we discuss existing\napproaches according to various aspects, in each of which methods are divided\ninto different groups and each group is discussed in detail for the principles,\nadvances and drawbacks. 3) We examine experiments of existing publications and\nsummarize results on popular datasets to provide quantitative comparisons. We\nalso point to some interesting discoveries by analyzing these results. 4) We\nprovide a discussion about issues of MOT research, as well as some interesting\ndirections which could possibly become potential research effort in the future.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 16:15:32 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 05:24:28 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 15:13:28 GMT"}, {"version": "v4", "created": "Mon, 22 May 2017 03:45:08 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Luo", "Wenhan", ""], ["Xing", "Junliang", ""], ["Milan", "Anton", ""], ["Zhang", "Xiaoqin", ""], ["Liu", "Wei", ""], ["Zhao", "Xiaowei", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1409.7686", "submitter": "Matthias K\\\"ummerer", "authors": "Matthias K\\\"ummerer, Thomas Wallis, Matthias Bethge", "title": "How close are we to understanding image-based saliency?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the set of the many complex factors driving gaze placement, the\nproperities of an image that are associated with fixations under free viewing\nconditions have been studied extensively. There is a general impression that\nthe field is close to understanding this particular association. Here we frame\nsaliency models probabilistically as point processes, allowing the calculation\nof log-likelihoods and bringing saliency evaluation into the domain of\ninformation. We compared the information gain of state-of-the-art models to a\ngold standard and find that only one third of the explainable spatial\ninformation is captured. We additionally provide a principled method to show\nwhere and how models fail to capture information in the fixations. Thus,\ncontrary to previous assertions, purely spatial saliency remains a significant\nchallenge.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 19:59:44 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["K\u00fcmmerer", "Matthias", ""], ["Wallis", "Thomas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1409.7787", "submitter": "Marco Crocco", "authors": "Marco Crocco, Marco Cristani, Andrea Trucco and Vittorio Murino", "title": "Audio Surveillance: a Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite surveillance systems are becoming increasingly ubiquitous in our\nliving environment, automated surveillance, currently based on video sensory\nmodality and machine intelligence, lacks most of the time the robustness and\nreliability required in several real applications. To tackle this issue, audio\nsensory devices have been taken into account, both alone or in combination with\nvideo, giving birth, in the last decade, to a considerable amount of research.\nIn this paper audio-based automated surveillance methods are organized into a\ncomprehensive survey: a general taxonomy, inspired by the more widespread video\nsurveillance field, is proposed in order to systematically describe the methods\ncovering background subtraction, event classification, object tracking and\nsituation analysis. For each of these tasks, all the significant works are\nreviewed, detailing their pros and cons and the context for which they have\nbeen proposed. Moreover, a specific section is devoted to audio features,\ndiscussing their expressiveness and their employment in the above described\ntasks. Differently, from other surveys on audio processing and analysis, the\npresent one is specifically targeted to automated surveillance, highlighting\nthe target applications of each described methods and providing the reader\ntables and schemes useful to retrieve the most suited algorithms for a specific\nrequirement.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 09:47:16 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Crocco", "Marco", ""], ["Cristani", "Marco", ""], ["Trucco", "Andrea", ""], ["Murino", "Vittorio", ""]]}, {"id": "1409.7794", "submitter": "Steven C.H. Hoi", "authors": "Yue Wu, Steven C. H. Hoi, Tao Mei, Nenghai Yu", "title": "Large-scale Online Feature Selection for Ultra-high Dimensional Sparse\n  Data", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection with large-scale high-dimensional data is important yet\nvery challenging in machine learning and data mining. Online feature selection\nis a promising new paradigm that is more efficient and scalable than batch\nfeature section methods, but the existing online approaches usually fall short\nin their inferior efficacy as compared with batch approaches. In this paper, we\npresent a novel second-order online feature selection scheme that is simple yet\neffective, very fast and extremely scalable to deal with large-scale ultra-high\ndimensional sparse data streams. The basic idea is to improve the existing\nfirst-order online feature selection methods by exploiting second-order\ninformation for choosing the subset of important features with high confidence\nweights. However, unlike many second-order learning methods that often suffer\nfrom extra high computational cost, we devise a novel smart algorithm for\nsecond-order online feature selection using a MaxHeap-based approach, which is\nnot only more effective than the existing first-order approaches, but also\nsignificantly more efficient and scalable for large-scale feature selection\nwith ultra-high dimensional sparse data, as validated from our extensive\nexperiments. Impressively, on a billion-scale synthetic dataset (1-billion\ndimensions, 1-billion nonzero features, and 1-million samples), our new\nalgorithm took only 8 minutes on a single PC, which is orders of magnitudes\nfaster than traditional batch approaches. \\url{http://arxiv.org/abs/1409.7794}\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 10:58:09 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 12:49:16 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 14:21:21 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Wu", "Yue", ""], ["Hoi", "Steven C. H.", ""], ["Mei", "Tao", ""], ["Yu", "Nenghai", ""]]}, {"id": "1409.7818", "submitter": "Shervin Minaee", "authors": "Shervin Minaee and AmirAli Abdolrashidi", "title": "On The Power of Joint Wavelet-DCT Features for Multispectral Palmprint\n  Recognition", "comments": "Asilomar Conference on Signals, Systems and Computers, IEEE, 2015,\n  (to Appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric-based identification has drawn a lot of attention in the recent\nyears. Among all biometrics, palmprint is known to possess a rich set of\nfeatures. In this paper we have proposed to use DCT-based features in parallel\nwith wavelet-based ones for palmprint identification. PCA is applied to the\nfeatures to reduce their dimensionality and the majority voting algorithm is\nused to perform classification. The features introduced here result in a\nnear-perfectly accurate identification. This method is tested on a well-known\nmultispectral palmprint database and an accuracy rate of 99.97-100\\% is\nachieved, outperforming all previous methods in similar conditions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 15:45:51 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 00:13:39 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "AmirAli", ""]]}, {"id": "1409.7935", "submitter": "Lior Shamir", "authors": "Evan Kuminski, Joe George, John Wallin, Lior Shamir", "title": "Combining human and machine learning for morphological analysis of\n  galaxy images", "comments": "PASP, accepted", "journal-ref": null, "doi": "10.1086/678977", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing importance of digital sky surveys collecting many millions of\ngalaxy images has reinforced the need for robust methods that can perform\nmorphological analysis of large galaxy image databases. Citizen science\ninitiatives such as Galaxy Zoo showed that large datasets of galaxy images can\nbe analyzed effectively by non-scientist volunteers, but since databases\ngenerated by robotic telescopes grow much faster than the processing power of\nany group of citizen scientists, it is clear that computer analysis is\nrequired. Here we propose to use citizen science data for training machine\nlearning systems, and show experimental results demonstrating that machine\nlearning systems can be trained with citizen science data. Our findings show\nthat the performance of machine learning depends on the quality of the data,\nwhich can be improved by using samples that have a high degree of agreement\nbetween the citizen scientists. The source code of the method is publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 17:47:35 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kuminski", "Evan", ""], ["George", "Joe", ""], ["Wallin", "John", ""], ["Shamir", "Lior", ""]]}, {"id": "1409.7963", "submitter": "Arjun Jain", "authors": "Arjun Jain, Jonathan Tompson, Yann LeCun and Christoph Bregler", "title": "MoDeep: A Deep Learning Framework Using Motion Features for Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel and efficient method for articulated human\npose estimation in videos using a convolutional network architecture, which\nincorporates both color and motion features. We propose a new human body pose\ndataset, FLIC-motion, that extends the FLIC dataset with additional motion\nfeatures. We apply our architecture to this dataset and report significantly\nbetter performance than current state-of-the-art pose detection systems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 21:32:15 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Jain", "Arjun", ""], ["Tompson", "Jonathan", ""], ["LeCun", "Yann", ""], ["Bregler", "Christoph", ""]]}, {"id": "1409.8230", "submitter": "Adrian Barbu", "authors": "Josue Anaya, Adrian Barbu", "title": "RENOIR - A Dataset for Real Low-Light Image Noise Reduction", "comments": "27 pages, 11 figures", "journal-ref": "Journal of Visual Communication and Image Representation 51, No.\n  2, 144-154, 2018", "doi": "10.1016/j.jvcir.2018.01.012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising algorithms are evaluated using images corrupted by artificial\nnoise, which may lead to incorrect conclusions about their performances on real\nnoise. In this paper we introduce a dataset of color images corrupted by\nnatural noise due to low-light conditions, together with spatially and\nintensity-aligned low noise images of the same scenes. We also introduce a\nmethod for estimating the true noise level in our images, since even the low\nnoise images contain small amounts of noise. We evaluate the accuracy of our\nnoise estimation method on real and artificial noise, and investigate the\nPoisson-Gaussian noise model. Finally, we use our dataset to evaluate six\ndenoising algorithms: Active Random Field, BM3D, Bilevel-MRF, Multi-Layer\nPerceptron, and two versions of NL-means. We show that while the Multi-Layer\nPerceptron, Bilevel-MRF, and NL-means with soft threshold outperform BM3D on\ngray images with synthetic noise, they lag behind on our dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 18:38:52 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 17:43:06 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 17:11:16 GMT"}, {"version": "v4", "created": "Mon, 20 Jun 2016 15:12:13 GMT"}, {"version": "v5", "created": "Tue, 21 Jun 2016 10:38:40 GMT"}, {"version": "v6", "created": "Tue, 9 Aug 2016 21:23:51 GMT"}, {"version": "v7", "created": "Tue, 18 Oct 2016 13:59:43 GMT"}, {"version": "v8", "created": "Tue, 13 Dec 2016 21:36:38 GMT"}, {"version": "v9", "created": "Mon, 8 May 2017 22:24:44 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Anaya", "Josue", ""], ["Barbu", "Adrian", ""]]}, {"id": "1409.8403", "submitter": "Zeynep Akata", "authors": "Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, Bernt Schiele", "title": "Evaluation of Output Embeddings for Fine-Grained Image Classification", "comments": "@inproceedings {ARWLS15, title = {Evaluation of Output Embeddings for\n  Fine-Grained Image Classification}, booktitle = {IEEE Computer Vision and\n  Pattern Recognition}, year = {2015}, author = {Zeynep Akata and Scott Reed\n  and Daniel Walter and Honglak Lee and Bernt Schiele} }", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298911", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification has advanced significantly in recent years with the\navailability of large-scale image sets. However, fine-grained classification\nremains a major challenge due to the annotation cost of large numbers of\nfine-grained categories. This project shows that compelling classification\nperformance can be achieved on such categories even without labeled training\ndata. Given image and class embeddings, we learn a compatibility function such\nthat matching embeddings are assigned a higher score than mismatching ones;\nzero-shot classification of an image proceeds by finding the label yielding the\nhighest joint compatibility score. We use state-of-the-art image features and\nfocus on different supervised attributes and unsupervised output embeddings\neither derived from hierarchies or learned from unlabeled text corpora. We\nestablish a substantially improved state-of-the-art on the Animals with\nAttributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate\nthat purely unsupervised output embeddings (learned from Wikipedia and improved\nwith fine-grained text) achieve compelling results, even outperforming the\nprevious supervised state-of-the-art. By combining different output embeddings,\nwe further improve results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 06:49:53 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 09:00:48 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Akata", "Zeynep", ""], ["Reed", "Scott", ""], ["Walter", "Daniel", ""], ["Lee", "Honglak", ""], ["Schiele", "Bernt", ""]]}, {"id": "1409.8500", "submitter": "Radu Horaud P", "authors": "Antoine Deleforge, Florence Forbes, Sileye Ba and Radu Horaud", "title": "Hyper-Spectral Image Analysis with Partially-Latent Regression and\n  Spatial Markov Dependencies", "comments": "12 pages, 4 figures, 3 tables", "journal-ref": "IEEE Journal on Selected Topics in Signal Processing, volume 9,\n  number 6, 1037-1048, 2015", "doi": "10.1109/JSTSP.2015.2416677", "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper-spectral data can be analyzed to recover physical properties at large\nplanetary scales. This involves resolving inverse problems which can be\naddressed within machine learning, with the advantage that, once a relationship\nbetween physical parameters and spectra has been established in a data-driven\nfashion, the learned relationship can be used to estimate physical parameters\nfor new hyper-spectral observations. Within this framework, we propose a\nspatially-constrained and partially-latent regression method which maps\nhigh-dimensional inputs (hyper-spectral images) onto low-dimensional responses\n(physical parameters such as the local chemical composition of the soil). The\nproposed regression model comprises two key features. Firstly, it combines a\nGaussian mixture of locally-linear mappings (GLLiM) with a partially-latent\nresponse model. While the former makes high-dimensional regression tractable,\nthe latter enables to deal with physical parameters that cannot be observed or,\nmore generally, with data contaminated by experimental artifacts that cannot be\nexplained with noise models. Secondly, spatial constraints are introduced in\nthe model through a Markov random field (MRF) prior which provides a spatial\nstructure to the Gaussian-mixture hidden variables. Experiments conducted on a\ndatabase composed of remotely sensed observations collected from the Mars\nplanet by the Mars Express orbiter demonstrate the effectiveness of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:59:01 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 16:09:52 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Deleforge", "Antoine", ""], ["Forbes", "Florence", ""], ["Ba", "Sileye", ""], ["Horaud", "Radu", ""]]}]