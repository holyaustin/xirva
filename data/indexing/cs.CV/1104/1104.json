[{"id": "1104.0579", "submitter": "Michael Lew", "authors": "Ye Ji", "title": "Image Retrieval Method Using Top-surf Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents the results and details of a content-based image\nretrieval project using the Top-surf descriptor. The experimental results are\npreliminary, however, it shows the capability of deducing objects from parts of\nthe objects or from the objects that are similar. This paper uses a dataset\nconsisting of 1200 images of which 800 images are equally divided into 8\ncategories, namely airplane, beach, motorbike, forest, elephants, horses, bus\nand building, while the other 400 images are randomly picked from the Internet.\nThe best results achieved are from building category.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2011 14:14:47 GMT"}], "update_date": "2011-04-05", "authors_parsed": [["Ji", "Ye", ""]]}, {"id": "1104.0582", "submitter": "Michael Lew", "authors": "Ran Tao", "title": "Visual Concept Detection and Real Time Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bag-of-words model is implemented and tried on 10-class visual concept\ndetection problem. The experimental results show that \"DURF+ERT+SVM\"\noutperforms \"SIFT+ERT+SVM\" both in detection performance and computation\nefficiency. Besides, combining DURF and SIFT results in even better detection\nperformance. Real-time object detection using SIFT and RANSAC is also tried on\nsimple objects, e.g. drink can, and good result is achieved.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2011 14:18:51 GMT"}], "update_date": "2011-04-05", "authors_parsed": [["Tao", "Ran", ""]]}, {"id": "1104.0654", "submitter": "Ehsan Elhamifar", "authors": "Ehsan Elhamifar and Rene Vidal", "title": "Block-Sparse Recovery via Convex Optimization", "comments": "IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2012.2196694", "report-no": null, "categories": "math.OC cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dictionary that consists of multiple blocks and a signal that lives\nin the range space of only a few blocks, we study the problem of finding a\nblock-sparse representation of the signal, i.e., a representation that uses the\nminimum number of blocks. Motivated by signal/image processing and computer\nvision applications, such as face recognition, we consider the block-sparse\nrecovery problem in the case where the number of atoms in each block is\narbitrary, possibly much larger than the dimension of the underlying subspace.\nTo find a block-sparse representation of a signal, we propose two classes of\nnon-convex optimization programs, which aim to minimize the number of nonzero\ncoefficient blocks and the number of nonzero reconstructed vectors from the\nblocks, respectively. Since both classes of problems are NP-hard, we propose\nconvex relaxations and derive conditions under which each class of the convex\nprograms is equivalent to the original non-convex formulation. Our conditions\ndepend on the notions of mutual and cumulative subspace coherence of a\ndictionary, which are natural generalizations of existing notions of mutual and\ncumulative coherence. We evaluate the performance of the proposed convex\nprograms through simulations as well as real experiments on face recognition.\nWe show that treating the face recognition problem as a block-sparse recovery\nproblem improves the state-of-the-art results by 10% with only 25% of the\ntraining data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2011 19:14:03 GMT"}, {"version": "v2", "created": "Sat, 16 Apr 2011 01:54:12 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2012 23:38:01 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Elhamifar", "Ehsan", ""], ["Vidal", "Rene", ""]]}, {"id": "1104.1237", "submitter": "Soumen Bag", "authors": "Soumen Bag, Soumen Barik, Prithwiraj Sen, Gautam Sanyal", "title": "A Statistical Nonparametric Approach of Face Recognition: Combination of\n  Eigenface & Modified k-Means Clustering", "comments": "7 pages, 2 figures. In proceedings of the Second International\n  Conference on Information Processing (ICIP), pp. 198-204, Bangalore, India,\n  2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions convey non-verbal cues, which play an important role in\ninterpersonal relations. Automatic recognition of human face based on facial\nexpression can be an important component of natural human-machine interface. It\nmay also be used in behavioural science. Although human can recognize the face\npractically without any effort, but reliable face recognition by machine is a\nchallenge. This paper presents a new approach for recognizing the face of a\nperson considering the expressions of the same human face at different\ninstances of time. This methodology is developed combining Eigenface method for\nfeature extraction and modified k-Means clustering for identification of the\nhuman face. This method endowed the face recognition without using the\nconventional distance measure classifiers. Simulation results show that\nproposed face recognition using perception of k-Means clustering is useful for\nface images with different facial expressions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2011 03:17:08 GMT"}], "update_date": "2011-04-08", "authors_parsed": [["Bag", "Soumen", ""], ["Barik", "Soumen", ""], ["Sen", "Prithwiraj", ""], ["Sanyal", "Gautam", ""]]}, {"id": "1104.1472", "submitter": "X.P.  Xu", "authors": "Xiaopeng Xu, Xiaochun Zhang", "title": "Gaussian Affine Feature Detector", "comments": "A paper about two dimension image signal detection, including\n  position, length, width, height, orentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed to get image features' geometric information. Using\nGaussian as an input signal, a theoretical optimal solution to calculate\nfeature's affine shape is proposed. Based on analytic result of a feature\nmodel, the method is different from conventional iterative approaches. From the\nmodel, feature's parameters such as position, orientation, background\nluminance, contrast, area and aspect ratio can be extracted. Tested with\nsynthesized and benchmark data, the method achieves or outperforms existing\napproaches in term of accuracy, speed and stability. The method can detect\nsmall, long or thin objects precisely, and works well under general conditions,\nsuch as for low contrast, blurred or noisy images.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2011 03:15:43 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Xu", "Xiaopeng", ""], ["Zhang", "Xiaochun", ""]]}, {"id": "1104.1485", "submitter": "Arijit  Laha Ph.D.", "authors": "Arijit Laha and J. Das", "title": "Fuzzy Rules and Evidence Theory for Satellite Image Analysis", "comments": "5 pages, International Conference on Advances in Pattern Recognition\n  2003 (ICAPR03)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of a fuzzy rule based classifier is proposed. The performance of the\nclassifier for multispectral satellite image classification is improved using\nDempster- Shafer theory of evidence that exploits information of the\nneighboring pixels. The classifiers are tested rigorously with two known images\nand their performance are found to be better than the results available in the\nliterature. We also demonstrate the improvement of performance while using D-S\ntheory along with fuzzy rule based classifiers over the basic fuzzy rule based\nclassifiers for all the test cases.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2011 05:18:15 GMT"}], "update_date": "2011-04-11", "authors_parsed": [["Laha", "Arijit", ""], ["Das", "J.", ""]]}, {"id": "1104.1550", "submitter": "Khaled Masmoudi Mr.", "authors": "Khaled Masmoudi, Marc Antonini, and Pierre Kornprobst", "title": "A bio-inspired image coder with temporal scalability", "comments": "12 pages; Advanced Concepts for Intelligent Vision Systems (ACIVS\n  2011)", "journal-ref": null, "doi": "10.1007/978-3-642-23687-7_41", "report-no": null, "categories": "cs.CV cs.IT cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel bio-inspired and dynamic coding scheme for static images.\nOur coder aims at reproducing the main steps of the visual stimulus processing\nin the mammalian retina taking into account its time behavior. The main novelty\nof this work is to show how to exploit the time behavior of the retina cells to\nensure, in a simple way, scalability and bit allocation. To do so, our main\nsource of inspiration will be the biologically plausible retina model called\nVirtual Retina. Following a similar structure, our model has two stages. The\nfirst stage is an image transform which is performed by the outer layers in the\nretina. Here it is modelled by filtering the image with a bank of difference of\nGaussians with time-delays. The second stage is a time-dependent\nanalog-to-digital conversion which is performed by the inner layers in the\nretina. Thanks to its conception, our coder enables scalability and bit\nallocation across time. Also, our decoded images do not show annoying artefacts\nsuch as ringing and block effects. As a whole, this article shows how to\ncapture the main properties of a biological system, here the retina, in order\nto design a new efficient coder.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2011 11:30:49 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2011 10:43:39 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Masmoudi", "Khaled", ""], ["Antonini", "Marc", ""], ["Kornprobst", "Pierre", ""]]}, {"id": "1104.1556", "submitter": "Jan Klein", "authors": "Jan Klein, Sebastiano Barbieri, Miriam H.A. Bauer, Christopher Nimsky,\n  Horst K. Hahn", "title": "Benchmarking the Quality of Diffusion-Weighted Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method that allows for measuring the quality of\ndiffusion-weighted MR images dependent on the image resolution and the image\nnoise. For this purpose, we introduce a new thresholding technique so that\nnoise and the signal can automatically be estimated from a single data set.\nThus, no user interaction as well as no double acquisition technique, which\nrequires a time-consuming proper geometrical registration, is needed. As a\ncoarser image resolution or slice thickness leads to a higher signal-to-noise\nratio (SNR), our benchmark determines a resolution-independent quality measure\nso that images with different resolutions can be adequately compared. To\nevaluate our method, a set of diffusion-weighted images from different vendors\nis used. It is shown that the quality can efficiently be determined and that\nthe automatically computed SNR is comparable to the SNR which is measured\nmanually in a manually selected region of interest.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2011 12:03:16 GMT"}, {"version": "v2", "created": "Fri, 6 May 2011 15:17:54 GMT"}, {"version": "v3", "created": "Mon, 9 May 2011 08:47:23 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Klein", "Jan", ""], ["Barbieri", "Sebastiano", ""], ["Bauer", "Miriam H. A.", ""], ["Nimsky", "Christopher", ""], ["Hahn", "Horst K.", ""]]}, {"id": "1104.1892", "submitter": "Kallam Suresh", "authors": "K. Suresh", "title": "\"Improved FCM algorithm for Clustering on Web Usage Mining\"", "comments": "ISSN(Online):1694-0814.\n  http://www.ijcsi.org/papers/IJCSI-8-1-42-45.pdf", "journal-ref": "IJCSI International Journal of Computer Sciencec Issues, Vol.8\n  Issue 1, January 2011, p42-46", "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present clustering method is very sensitive to the initial\ncenter values, requirements on the data set too high, and cannot handle noisy\ndata the proposal method is using information entropy to initialize the cluster\ncenters and introduce weighting parameters to adjust the location of cluster\ncenters and noise problems.The navigation datasets which are sequential in\nnature, Clustering web data is finding the groups which share common interests\nand behavior by analyzing the data collected in the web servers, this improves\nclustering on web data efficiently using improved fuzzy c-means(FCM)\nclustering. Web usage mining is the application of data mining techniques to\nweb log data repositories. It is used in finding the user access patterns from\nweb access log. Web data Clusters are formed using on MSNBC web navigation\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 09:38:47 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Suresh", "K.", ""]]}, {"id": "1104.1945", "submitter": "Malakappa Shirdhonkar", "authors": "M. S. Shirdhonkar, Manesh B.Kokare", "title": "Off-Line Handwritten Signature Retrieval using Curvelet Transforms", "comments": "(IJCSIS) International Journal of Computer Science and Information\n  Security, Vol.8, No.8, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, a new method for offline handwritten signature retrieval is\nbased on curvelet transform is proposed. Many applications in image processing\nrequire similarity retrieval of an image from a large collection of images. In\nsuch cases, image indexing becomes important for efficient organization and\nretrieval of images. This paper addresses this issue in the context of a\ndatabase of handwritten signature images and describes a system for similarity\nretrieval. The proposed system uses a curvelet based texture features\nextraction. The performance of the system has been tested with an image\ndatabase of 180 signatures. The results obtained indicate that the proposed\nsystem is able to identify signatures with great with accuracy even when a part\nof a signature is missing.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 13:37:08 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Shirdhonkar", "M. S.", ""], ["Kokare", "Manesh B.", ""]]}, {"id": "1104.2059", "submitter": "Michael Lew", "authors": "Kwie Min Wong", "title": "Template-based matching using weight maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template matching is one of the most prevalent pattern recognition methods\nworldwide. It has found uses in most visual concept detection fields. In this\nwork, we investigate methods for improving template matching by adjusting the\nweights of different regions of the template. We compare several weight maps\nand test the methods using the FERET face test set in the context of human eye\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 20:32:54 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Wong", "Kwie Min", ""]]}, {"id": "1104.2069", "submitter": "Michael Lew", "authors": "Alwin de Rooij", "title": "GEOMIR2K9 - A Similar Scene Finder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of the GEOMIR2K9 project is to create a software program that\nis able to find similar scenic images clustered by geographical location and\nsorted by similarity based only on their visual content. The user should be\nable to input a query image, based on this given query image the program should\nfind relevant visual content and present this to the user in a meaningful way.\nTechnically the goal for the GEOMIR2K9 project is twofold. The first of these\ntwo goals is to create a basic low level visual information retrieval system.\nThis includes feature extraction, post processing of the feature data and\nclassification/ clustering based on similarity with a strong focus on scenic\nimages. The second goal of this project is to provide the user with a novel and\nsuitable interface and visualization method so that the user may interact with\nthe retrieved images in a natural and meaningful way.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 21:17:28 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["de Rooij", "Alwin", ""]]}, {"id": "1104.2171", "submitter": "Sibel Tari", "authors": "Sibel Tari and Murat Genctav", "title": "From a Modified Ambrosio-Tortorelli to a Randomized Part Hierarchy Tree", "comments": "Scale Space and Variational Methods 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the possibility of coding parts, features that are higher\nlevel than boundaries, using a modified AT field after augmenting the\ninteraction term of the AT energy with a non-local term and weakening the\nseparation into boundary/not-boundary phases. The iteratively extracted parts\nusing the level curves with double point singularities are organized as a\nproper binary tree. Inconsistencies due to non-generic configurations for level\ncurves as well as due to visual changes such as occlusion are successfully\nhandled once the tree is endowed with a probabilistic structure. The work is a\nstep in establishing the AT function as a bridge between low and high level\nvisual processing.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 11:20:06 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Tari", "Sibel", ""], ["Genctav", "Murat", ""]]}, {"id": "1104.2175", "submitter": "Sibel Tari", "authors": "Sibel Tari", "title": "Extracting Parts of 2D Shapes Using Local and Global Interactions\n  Simultaneously", "comments": "invited book chapter, Handbook of Pattern Recognition and Computer\n  Vision, 4th edition, C. Chen (ed) The presented surface is also related to\n  Ambrosio-Tortorelli phase field", "journal-ref": "Handbook of Pattern Recognition and Computer Vision, 4th edition,\n  C. Chen (ed), Dec 2009", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception research provides strong evidence in favor of part based\nrepresentation of shapes in human visual system. Despite considerable\ndifferences among different theories in terms of how part boundaries are found,\nthere is substantial agreement on that the process depends on many local and\nglobal geometric factors. This poses an important challenge from the\ncomputational point of view. In the first part of the chapter, I present a\nnovel decomposition method by taking both local and global interactions within\nthe shape domain into account. At the top of the partitioning hierarchy, the\nshape gets split into two parts capturing, respectively, the gross structure\nand the peripheral structure. The gross structure may be conceived as the least\ndeformable part of the shape which remains stable under visual transformations.\nThe peripheral structure includes limbs, protrusions, and boundary texture.\nSuch a separation is in accord with the behavior of the artists who start with\na gross shape and enrich it with details. The method is particularly\ninteresting from the computational point of view as it does not resort to any\ngeometric notions (e.g. curvature, convexity) explicitly. In the second part of\nthe chapter, I relate the new method to PDE based shape representation schemes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 11:33:20 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Tari", "Sibel", ""]]}, {"id": "1104.2285", "submitter": "Abhishek Das", "authors": "Abhishek Das, Avijit Kar, Debasis Bhattacharyya", "title": "Elimination of Specular reflection and Identification of ROI: The First\n  Step in Automated Detection of Cervical Cancer using Digital Colposcopy", "comments": "IEEE Imaging Systems and Techniques, 2011, Print ISBN:\n  978-1-61284-894-5, pages 237 - 241", "journal-ref": "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5962218, 2011", "doi": "10.1109/IST.2011.5962218", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cervical Cancer is one of the most common forms of cancer in women worldwide.\nMost cases of cervical cancer can be prevented through screening programs aimed\nat detecting precancerous lesions. During Digital Colposcopy, Specular\nReflections (SR) appear as bright spots heavily saturated with white light.\nThese occur due to the presence of moisture on the uneven cervix surface, which\nact like mirrors reflecting light from the illumination source. Apart from\ncamouflaging the actual features, the SR also affects subsequent segmentation\nroutines and hence must be removed. Our novel technique eliminates the SR and\nmakes the colposcopic images (cervigram) ready for segmentation algorithms. The\ncervix region occupies about half of the cervigram image. Other parts of the\nimage contain irrelevant information, such as equipment, frames, text and\nnon-cervix tissues. This irrelevant information can confuse automatic\nidentification of the tissues within the cervix. The first step is, therefore,\nfocusing on the cervical borders, so that we have a geometric boundary on the\nrelevant image area. We have proposed a type of modified kmeans clustering\nalgorithm to evaluate the region of interest.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 17:49:19 GMT"}, {"version": "v2", "created": "Mon, 30 May 2011 03:21:47 GMT"}, {"version": "v3", "created": "Thu, 11 Aug 2011 09:17:05 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Das", "Abhishek", ""], ["Kar", "Avijit", ""], ["Bhattacharyya", "Debasis", ""]]}, {"id": "1104.2580", "submitter": "Diego Rother", "authors": "Diego Rother, Simon Sch\\\"utz, Ren\\'e Vidal", "title": "Hypothesize and Bound: A Computational Focus of Attention Mechanism for\n  Simultaneous N-D Segmentation, Pose Estimation and Classification Using Shape\n  Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the ever increasing bandwidth of the visual information available to\nmany intelligent systems, it is becoming essential to endow them with a sense\nof what is worthwhile their attention and what can be safely disregarded. This\narticle presents a general mathematical framework to efficiently allocate the\navailable computational resources to process the parts of the input that are\nrelevant to solve a given perceptual problem. By this we mean to find the\nhypothesis H (i.e., the state of the world) that maximizes a function L(H),\nrepresenting how well each hypothesis \"explains\" the input. Given the large\nbandwidth of the sensory input, fully evaluating L(H) for each hypothesis H is\ncomputationally infeasible (e.g., because it would imply checking a large\nnumber of pixels). To address this problem we propose a mathematical framework\nwith two key ingredients. The first one is a Bounding Mechanism (BM) to compute\nlower and upper bounds of L(H), for a given computational budget. These bounds\nare much cheaper to compute than L(H) itself, can be refined at any time by\nincreasing the budget allocated to a hypothesis, and are frequently enough to\ndiscard a hypothesis. To compute these bounds, we develop a novel theory of\nshapes and shape priors. The second ingredient is a Focus of Attention\nMechanism (FoAM) to select which hypothesis' bounds should be refined next,\nwith the goal of discarding non-optimal hypotheses with the least amount of\ncomputation. The proposed framework: 1) is very efficient since most hypotheses\nare discarded with minimal computation; 2) is parallelizable; 3) is guaranteed\nto find the globally optimal hypothesis; and 4) its running time depends on the\nproblem at hand, not on the bandwidth of the input. We instantiate the proposed\nframework for the problem of simultaneously estimating the class, pose, and a\nnoiseless version of a 2D shape in a 2D image.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 18:59:52 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2011 19:31:24 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Rother", "Diego", ""], ["Sch\u00fctz", "Simon", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1104.2745", "submitter": "Sibel Tari", "authors": "Cagri Aslan and Sibel Tari", "title": "An Axis-Based Representation for Recognition", "comments": null, "journal-ref": "ICCV(2005) 1339-1346", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new axis-based shape representation scheme along with a\nmatching framework to address the problem of generic shape recognition. The\nmain idea is to define the relative spatial arrangement of local symmetry axes\nand their metric properties in a shape centered coordinate frame. The resulting\ndescriptions are invariant to scale, rotation, small changes in viewpoint and\narticulations. Symmetry points are extracted from a surface whose level curves\nroughly mimic the motion by curvature. By increasing the amount of smoothing on\nthe evolving curve, only those symmetry axes that correspond to the most\nprominent parts of a shape are extracted. The representation does not suffer\nfrom the common instability problems of the traditional connected skeletons. It\ncaptures the perceptual qualities of shapes well. Therefore finding the\nsimilarities and the differences among shapes becomes easier. The matching\nprocess gives highly successful results on a diverse database of 2D shapes.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 12:52:40 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Aslan", "Cagri", ""], ["Tari", "Sibel", ""]]}, {"id": "1104.2751", "submitter": "Sibel Tari", "authors": "C. Aslan, A. Erdem, E. Erdem, S. Tari", "title": "Disconnected Skeleton: Shape at its Absolute Scale", "comments": "The work excluding {\\S}V and {\\S}VI has first appeared in 2005 ICCV:\n  Aslan, C., Tari, S.: An Axis-Based Representation for Recognition. In\n  ICCV(2005) 1339- 1346.; Aslan, C., : Disconnected Skeletons for Shape\n  Recognition. Masters thesis, Department of Computer Engineering, Middle East\n  Technical University, May 2005", "journal-ref": "T-PAMI vol. 30 no. 12, pp. 2188-2203, 2008", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new skeletal representation along with a matching framework to\naddress the deformable shape recognition problem. The disconnectedness arises\nas a result of excessive regularization that we use to describe a shape at an\nattainably coarse scale. Our motivation is to rely on the stable properties of\nthe shape instead of inaccurately measured secondary details. The new\nrepresentation does not suffer from the common instability problems of\ntraditional connected skeletons, and the matching process gives quite\nsuccessful results on a diverse database of 2D shapes. An important difference\nof our approach from the conventional use of the skeleton is that we replace\nthe local coordinate frame with a global Euclidean frame supported by\nadditional mechanisms to handle articulations and local boundary deformations.\nAs a result, we can produce descriptions that are sensitive to any combination\nof changes in scale, position, orientation and articulation, as well as\ninvariant ones.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 13:02:43 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Aslan", "C.", ""], ["Erdem", "A.", ""], ["Erdem", "E.", ""], ["Tari", "S.", ""]]}, {"id": "1104.3513", "submitter": "Kodge B. G.", "authors": "B. G. Kodge, P. S. Hiremath", "title": "An Effect of Spatial Filtering in Visualization of Coronary Arteries\n  Imaging", "comments": "06 Pages, 18 figures", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT), Vol. 2, No. 1, 2009, PP: 35-40", "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, coronary angiography is the well known standard for the diagnosis\nof coronary artery disease. Conventional coronary angiography is an invasive\nprocedure with a small, yet inherent risk of myocardial infarction, stroke,\npotential arrhythmias, and death. Other noninvasive diagnostic tools, such as\nelectrocardiography, echocardiography, and nuclear imaging are now widely\navailable but are limited by their inability to directly visualize and quantify\ncoronary artery stenoses and predict the stability of plaques. Coronary\nmagnetic resonance angiography (MRA) is a technique that allows visualization\nof the coronary arteries by noninvasive means; however, it has not yet reached\na stage where it can be used in routine clinical practice. Although coronary\nMRA is a potentially useful diagnostic tool, it has limitations. Further\nresearch should focus on improving the diagnostic resolution and accuracy of\ncoronary MRA. This paper will helps to cardiologists to take the clear look of\nspatial filtered imaging of coronary arteries.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 12:35:47 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Kodge", "B. G.", ""], ["Hiremath", "P. S.", ""]]}, {"id": "1104.3571", "submitter": "Kodge B. G.", "authors": "B. G. Kodge, P. S. Hiremath", "title": "Visualization techniques for data mining of Latur district satellite\n  imagery", "comments": "This paper has been withdrawn by the author", "journal-ref": "Advances in Computational Research, Volume 2, Issue 1, 2010,\n  pp-21-24", "doi": null, "report-no": null, "categories": "cs.CE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a new visualization tool for classification of satellite\nimagery. Visualization of feature space allows exploration of patterns in the\nimage data and insight into the classification process and related uncertainty.\nVisual Data Mining provides added value to image classifications as the user\ncan be involved in the classification process providing increased confidence in\nand understanding of the results. In this study, we present a prototype\nvisualization tool for visual data mining (VDM) of satellite imagery. The\nvisualization tool is showcased in a classification study of highresolution\nimageries of Latur district in Maharashtra state of India.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 12:35:11 GMT"}, {"version": "v2", "created": "Fri, 11 May 2012 05:42:42 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Kodge", "B. G.", ""], ["Hiremath", "P. S.", ""]]}, {"id": "1104.3742", "submitter": "Fillipe  Souza", "authors": "Fillipe Souza, Eduardo Valle, Guillermo Ch\\'avez and Arnaldo Ara\\'ujo", "title": "Hue Histograms to Spatiotemporal Local Features for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent developments in spatiotemporal local features for action\nrecognition in video sequences, local color information has so far been\nignored. However, color has been proved an important element to the success of\nautomated recognition of objects and scenes. In this paper we extend the\nspace-time interest point descriptor STIP to take into account the color\ninformation on the features' neighborhood. We compare the performance of our\ncolor-aware version of STIP (which we have called HueSTIP) with the original\none.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 13:36:15 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Souza", "Fillipe", ""], ["Valle", "Eduardo", ""], ["Ch\u00e1vez", "Guillermo", ""], ["Ara\u00fajo", "Arnaldo", ""]]}, {"id": "1104.4168", "submitter": "Eraldo  Ribeiro", "authors": "Wei Liu and Eraldo Ribeiro", "title": "A Meshless Method for Variational Nonrigid 2-D Shape Registration", "comments": "60 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for nonrigid registration of 2-D geometric shapes. Our\ncontribution is twofold. First, we extend the classic chamfer-matching energy\nto a variational functional. Secondly, we introduce a meshless deformation\nmodel that can handle significant high-curvature deformations. We represent 2-D\nshapes implicitly using distance transforms, and registration error is defined\nbased on the shape contours' mutual distances. In addition, we model global\nshape deformation as an approximation blended from local deformation fields\nusing partition-of-unity. The global deformation field is regularized by\npenalizing inconsistencies between local fields. The representation can be made\nadaptive to shape's contour, leading to registration that is both flexible and\nefficient. Finally, registration is achieved by minimizing a variational\nchamfer-energy functional combined with the consistency regularizer. We\ndemonstrate the effectiveness of our method on a number of experiments.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 04:48:41 GMT"}], "update_date": "2011-04-22", "authors_parsed": [["Liu", "Wei", ""], ["Ribeiro", "Eraldo", ""]]}, {"id": "1104.4295", "submitter": "Oleg Pianykh", "authors": "Oleg S. Pianykh", "title": "Improving digital signal interpolation: L2-optimal kernels with\n  kernel-invariant interpolation speed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpolation is responsible for digital signal resampling and can\nsignificantly degrade the original signal quality if not done properly. For\nmany years, optimal interpolation algorithms were sought within constrained\nclasses of interpolation kernel functions. We derive a new family of\nunconstrained L2-optimal interpolation kernels, and compare their properties to\nthe previously known. Although digital images are used to illustrate this work,\nour L2-optimal kernels can be applied to interpolate any digital signals.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 15:45:52 GMT"}], "update_date": "2011-04-23", "authors_parsed": [["Pianykh", "Oleg S.", ""]]}, {"id": "1104.4298", "submitter": "Carsten Gottschlich", "authors": "Carsten Gottschlich", "title": "Curved Gabor Filters for Fingerprint Image Enhancement", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, vol. 21, no. 4, pp.\n  2220-2227, Apr. 2012", "doi": "10.1109/TIP.2011.2170696", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gabor filters play an important role in many application areas for the\nenhancement of various types of images and the extraction of Gabor features.\nFor the purpose of enhancing curved structures in noisy images, we introduce\ncurved Gabor filters which locally adapt their shape to the direction of flow.\nThese curved Gabor filters enable the choice of filter parameters which\nincrease the smoothing power without creating artifacts in the enhanced image.\nIn this paper, curved Gabor filters are applied to the curved ridge and valley\nstructure of low-quality fingerprint images. First, we combine two orientation\nfield estimation methods in order to obtain a more robust estimation for very\nnoisy images. Next, curved regions are constructed by following the respective\nlocal orientation and they are used for estimating the local ridge frequency.\nLastly, curved Gabor filters are defined based on curved regions and they are\napplied for the enhancement of low-quality fingerprint images. Experimental\nresults on the FVC2004 databases show improvements of this approach in\ncomparison to state-of-the-art enhancement methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 15:52:39 GMT"}, {"version": "v2", "created": "Fri, 25 Jul 2014 08:10:45 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Gottschlich", "Carsten", ""]]}, {"id": "1104.4376", "submitter": "Vikram Krishnamurthy", "authors": "Alex Wang and Vikram Krishnamurthy and Bhashyam Balaji", "title": "Intent Inference and Syntactic Tracking with GMTI Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional target tracking systems, human operators use the estimated\ntarget tracks to make higher level inference of the target behaviour/intent.\nThis paper develops syntactic filtering algorithms that assist human operators\nby extracting spatial patterns from target tracks to identify\nsuspicious/anomalous spatial trajectories. The targets' spatial trajectories\nare modeled by a stochastic context free grammar (SCFG) and a switched mode\nstate space model. Bayesian filtering algorithms for stochastic context free\ngrammars are presented for extracting the syntactic structure and illustrated\nfor a ground moving target indicator (GMTI) radar example. The performance of\nthe algorithms is tested with the experimental data collected using DRDC\nOttawa's X-band Wideband Experimental Airborne Radar (XWEAR).\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2011 02:27:45 GMT"}], "update_date": "2011-04-25", "authors_parsed": [["Wang", "Alex", ""], ["Krishnamurthy", "Vikram", ""], ["Balaji", "Bhashyam", ""]]}, {"id": "1104.4385", "submitter": "Nikhil Rao", "authors": "Nikhil S Rao, Robert D. Nowak, Stephen J. Wright and Nick G. Kingsbury", "title": "Convex Approaches to Model Wavelet Sparsity Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical dependencies among wavelet coefficients are commonly represented\nby graphical models such as hidden Markov trees(HMTs). However, in linear\ninverse problems such as deconvolution, tomography, and compressed sensing, the\npresence of a sensing or observation matrix produces a linear mixing of the\nsimple Markovian dependency structure. This leads to reconstruction problems\nthat are non-convex optimizations. Past work has dealt with this issue by\nresorting to greedy or suboptimal iterative reconstruction methods. In this\npaper, we propose new modeling approaches based on group-sparsity penalties\nthat leads to convex optimizations that can be solved exactly and efficiently.\nWe show that the methods we develop perform significantly better in\ndeconvolution and compressed sensing applications, while being as\ncomputationally efficient as standard coefficient-wise approaches such as\nlasso.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2011 04:35:29 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Rao", "Nikhil S", ""], ["Nowak", "Robert D.", ""], ["Wright", "Stephen J.", ""], ["Kingsbury", "Nick G.", ""]]}, {"id": "1104.4704", "submitter": "Chunhua Shen", "authors": "Chunhua Shen, Junae Kim, Lei Wang, Anton van den Hengel", "title": "Positive Semidefinite Metric Learning Using Boosting-like Algorithms", "comments": "30 pages, appearing in Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of many machine learning and pattern recognition methods relies\nheavily upon the identification of an appropriate distance metric on the input\ndata. It is often beneficial to learn such a metric from the input training\ndata, instead of using a default one such as the Euclidean distance. In this\nwork, we propose a boosting-based technique, termed BoostMetric, for learning a\nquadratic Mahalanobis distance metric. Learning a valid Mahalanobis distance\nmetric requires enforcing the constraint that the matrix parameter to the\nmetric remains positive definite. Semidefinite programming is often used to\nenforce this constraint, but does not scale well and easy to implement.\nBoostMetric is instead based on the observation that any positive semidefinite\nmatrix can be decomposed into a linear combination of trace-one rank-one\nmatrices. BoostMetric thus uses rank-one positive semidefinite matrices as weak\nlearners within an efficient and scalable boosting-based learning process. The\nresulting methods are easy to implement, efficient, and can accommodate various\ntypes of constraints. We extend traditional boosting algorithms in that its\nweak learner is a positive semidefinite matrix with trace and rank being one\nrather than a classifier or regressor. Experiments on various datasets\ndemonstrate that the proposed algorithms compare favorably to those\nstate-of-the-art methods in terms of classification accuracy and running time.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2011 10:38:03 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2012 05:56:40 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Shen", "Chunhua", ""], ["Kim", "Junae", ""], ["Wang", "Lei", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1104.4723", "submitter": "Lucas Bueno", "authors": "Lucas Moutinho Bueno, Eduardo Valle, Ricardo da Silva Torres", "title": "Bayesian approach for near-duplicate image detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a bayesian approach for near-duplicate image\ndetection, and investigate how different probabilistic models affect the\nperformance obtained. The task of identifying an image whose metadata are\nmissing is often demanded for a myriad of applications: metadata retrieval in\ncultural institutions, detection of copyright violations, investigation of\nlatent cross-links in archives and libraries, duplicate elimination in storage\nmanagement, etc. The majority of current solutions are based either on voting\nalgorithms, which are very precise, but expensive; either on the use of visual\ndictionaries, which are efficient, but less precise. Our approach, uses local\ndescriptors in a novel way, which by a careful application of decision theory,\nallows a very fine control of the compromise between precision and efficiency.\nIn addition, the method attains a great compromise between those two axes, with\nmore than 99% accuracy with less than 10 database operations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2011 14:04:16 GMT"}], "update_date": "2011-04-26", "authors_parsed": [["Bueno", "Lucas Moutinho", ""], ["Valle", "Eduardo", ""], ["Torres", "Ricardo da Silva", ""]]}, {"id": "1104.4989", "submitter": "Abhishek Das", "authors": "Abhishek Das, Avijit Kar, Debasis Bhattacharyya", "title": "Preprocessing: A Step in Automating Early Detection of Cervical Cancer", "comments": "wrong conference name mentioned (This paper has been withdrawn)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 18:38:01 GMT"}, {"version": "v2", "created": "Sun, 22 May 2011 15:55:36 GMT"}, {"version": "v3", "created": "Wed, 25 May 2011 18:14:30 GMT"}, {"version": "v4", "created": "Fri, 27 May 2011 02:18:55 GMT"}, {"version": "v5", "created": "Mon, 30 May 2011 03:22:05 GMT"}, {"version": "v6", "created": "Thu, 11 Aug 2011 09:00:58 GMT"}], "update_date": "2011-08-12", "authors_parsed": [["Das", "Abhishek", ""], ["Kar", "Avijit", ""], ["Bhattacharyya", "Debasis", ""]]}, {"id": "1104.5284", "submitter": "Antonio Junior Antonio junior", "authors": "Antonio da Luz, Eduardo Valle, Arnaldo Araujo", "title": "Content-Based Spam Filtering on Video Sharing Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we are concerned with the detection of spam in video sharing\nsocial networks. Specifically, we investigate how much visual content-based\nanalysis can aid in detecting spam in videos. This is a very challenging task,\nbecause of the high-level semantic concepts involved; of the assorted nature of\nsocial networks, preventing the use of constrained a priori information; and,\nwhat is paramount, of the context dependent nature of spam. Content filtering\nfor social networks is an increasingly demanded task: due to their popularity,\nthe number of abuses also tends to increase, annoying the user base and\ndisrupting their services. We systematically evaluate several approaches for\nprocessing the visual information: using static and dynamic (motionaware)\nfeatures, with and without considering the context, and with or without latent\nsemantic analysis (LSA). Our experiments show that LSA is helpful, but taking\nthe context into consideration is paramount. The whole scheme shows good\nresults, showing the feasibility of the concept.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 03:16:42 GMT"}], "update_date": "2011-04-29", "authors_parsed": [["da Luz", "Antonio", ""], ["Valle", "Eduardo", ""], ["Araujo", "Arnaldo", ""]]}, {"id": "1104.5304", "submitter": "Gael Varoquaux", "authors": "Vincent Michel (LNAO, INRIA Saclay - Ile de France), Alexandre\n  Gramfort (LNAO, INRIA Saclay - Ile de France), Ga\\\"el Varoquaux (LNAO, INRIA\n  Saclay - Ile de France), Evelyn Eger, Christine Keribin (INRIA Saclay - Ile\n  de France, LM-Orsay), Bertrand Thirion (LNAO, INRIA Saclay - Ile de France)", "title": "A supervised clustering approach for fMRI-based inference of brain\n  states", "comments": null, "journal-ref": "Pattern Recognition (2011)", "doi": "10.1016/j.patcog.2011.04.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that combines signals from many brain regions observed in\nfunctional Magnetic Resonance Imaging (fMRI) to predict the subject's behavior\nduring a scanning session. Such predictions suffer from the huge number of\nbrain regions sampled on the voxel grid of standard fMRI data sets: the curse\nof dimensionality. Dimensionality reduction is thus needed, but it is often\nperformed using a univariate feature selection procedure, that handles neither\nthe spatial structure of the images, nor the multivariate nature of the signal.\nBy introducing a hierarchical clustering of the brain volume that incorporates\nconnectivity constraints, we reduce the span of the possible spatial\nconfigurations to a single tree of nested regions tailored to the signal. We\nthen prune the tree in a supervised setting, hence the name supervised\nclustering, in order to extract a parcellation (division of the volume) such\nthat parcel-based signal averages best predict the target information.\nDimensionality reduction is thus achieved by feature agglomeration, and the\nconstructed features now provide a multi-scale representation of the signal.\nComparisons with reference methods on both simulated and real data show that\nour approach yields higher prediction accuracy than standard voxel-based\napproaches. Moreover, the method infers an explicit weighting of the regions\ninvolved in the regression or classification task.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 06:12:45 GMT"}], "update_date": "2011-04-29", "authors_parsed": [["Michel", "Vincent", "", "LNAO, INRIA Saclay - Ile de France"], ["Gramfort", "Alexandre", "", "LNAO, INRIA Saclay - Ile de France"], ["Varoquaux", "Ga\u00ebl", "", "LNAO, INRIA\n  Saclay - Ile de France"], ["Eger", "Evelyn", "", "INRIA Saclay - Ile\n  de France, LM-Orsay"], ["Keribin", "Christine", "", "INRIA Saclay - Ile\n  de France, LM-Orsay"], ["Thirion", "Bertrand", "", "LNAO, INRIA Saclay - Ile de France"]]}]