[{"id": "1709.00029", "submitter": "Patrick Helber", "authors": "Patrick Helber, Benjamin Bischke, Andreas Dengel, Damian Borth", "title": "EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and\n  Land Cover Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the challenge of land use and land cover\nclassification using Sentinel-2 satellite images. The Sentinel-2 satellite\nimages are openly and freely accessible provided in the Earth observation\nprogram Copernicus. We present a novel dataset based on Sentinel-2 satellite\nimages covering 13 spectral bands and consisting out of 10 classes with in\ntotal 27,000 labeled and geo-referenced images. We provide benchmarks for this\nnovel dataset with its spectral bands using state-of-the-art deep Convolutional\nNeural Network (CNNs). With the proposed novel dataset, we achieved an overall\nclassification accuracy of 98.57%. The resulting classification system opens a\ngate towards a number of Earth observation applications. We demonstrate how\nthis classification system can be used for detecting land use and land cover\nchanges and how it can assist in improving geographical maps. The\ngeo-referenced dataset EuroSAT is made publicly available at\nhttps://github.com/phelber/eurosat.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 18:19:10 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 09:51:18 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Helber", "Patrick", ""], ["Bischke", "Benjamin", ""], ["Dengel", "Andreas", ""], ["Borth", "Damian", ""]]}, {"id": "1709.00042", "submitter": "Jie Zhang", "authors": "Jie Zhang, Qingyang Li, Richard J. Caselli, Jieping Ye, Yalin Wang", "title": "Multi-task Dictionary Learning based Convolutional Neural Network for\n  Computer aided Diagnosis with Longitudinal Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic image-based diagnosis and prognosis of neurodegenerative diseases\non longitudinal data has drawn great interest from computer vision researchers.\nThe current state-of-the-art models for many image classification tasks are\nbased on the Convolutional Neural Networks (CNN). However, a key challenge in\napplying CNN to biological problems is that the available labeled training\nsamples are very limited. Another issue for CNN to be applied in computer aided\ndiagnosis applications is that to achieve better diagnosis and prognosis\naccuracy, one usually has to deal with the longitudinal dataset, i.e., the\ndataset of images scanned at different time points. Here we argue that an\nenhanced CNN model with transfer learning for the joint analysis of tasks from\nmultiple time points or regions of interests may have a potential to improve\nthe accuracy of computer aided diagnosis. To reach this goal, we innovate a CNN\nbased deep learning multi-task dictionary learning framework to address the\nabove challenges. Firstly, we pre-train CNN on the ImageNet dataset and\ntransfer the knowledge from the pre-trained model to the medical imaging\nprogression representation, generating the features for different tasks. Then,\nwe propose a novel unsupervised learning method, termed Multi-task Stochastic\nCoordinate Coding (MSCC), for learning different tasks by using shared and\nindividual dictionaries and generating the sparse features required to predict\nthe future cognitive clinical scores. We apply our new model in a publicly\navailable neuroimaging cohort to predict clinical measures with two different\nfeature sets and compare them with seven other state-of-the-art methods. The\nexperimental results show our proposed method achieved superior results.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 18:58:59 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Zhang", "Jie", ""], ["Li", "Qingyang", ""], ["Caselli", "Richard J.", ""], ["Ye", "Jieping", ""], ["Wang", "Yalin", ""]]}, {"id": "1709.00069", "submitter": "Varun Jampani", "authors": "Varun Jampani", "title": "Learning Inference Models for Computer Vision", "comments": "PhD Dissertation", "journal-ref": null, "doi": "10.15496/publikation-17888", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision can be understood as the ability to perform inference on\nimage data. Breakthroughs in computer vision technology are often marked by\nadvances in inference techniques. This thesis proposes novel inference schemes\nand demonstrates applications in computer vision. We propose inference\ntechniques for both generative and discriminative vision models. The use of\ngenerative models in vision is often hampered by the difficulty of posterior\ninference. We propose techniques for improving inference in MCMC sampling and\nmessage-passing inference. Our inference strategy is to learn separate\ndiscriminative models that assist Bayesian inference in a generative model.\nExperiments on a range of generative models show that the proposed techniques\naccelerate the inference process and/or converge to better solutions. A main\ncomplication in the design of discriminative models is the inclusion of prior\nknowledge. We concentrate on CNN models and propose a generalization of\nstandard spatial convolutions to bilateral convolutions. We generalize the\nexisting use of bilateral filters and then propose new neural network\narchitectures with learnable bilateral filters, which we call `Bilateral Neural\nNetworks'. Experiments demonstrate the use of the bilateral networks on a wide\nrange of image and video tasks and datasets. In summary, we propose techniques\nfor better inference in several vision models ranging from inverse graphics to\nfreely parameterized neural networks. In generative models, our inference\ntechniques alleviate some of the crucial hurdles in Bayesian posterior\ninference, paving new ways for the use of model based machine learning in\nvision. In discriminative CNN models, the proposed filter generalizations aid\nin the design of new neural network architectures that can handle sparse\nhigh-dimensional data as well as provide a way to incorporate prior knowledge\ninto CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 20:33:06 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Jampani", "Varun", ""]]}, {"id": "1709.00072", "submitter": "Akbar Saadat", "authors": "Akbar Saadat", "title": "Exact Blur Measure Outperforms Conventional Learned Features for Depth\n  Finding", "comments": "5 pages, Submitted to SSPD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image analysis methods that are based on exact blur values are faced with the\ncomputational complexities due to blur measurement error. This atmosphere\nencourages scholars to look for handcrafted and learned features for finding\ndepth from a single image. This paper introduces a novel exact realization for\nblur measures on digital images and implements it on a new measure of defocus\nGaussian blur at edge points in Depth From Defocus (DFD) methods with the\npotential to change this atmosphere. The experiments on real images indicate\nsuperiority of the proposed measure in error performance over conventional\nlearned features in the state-of the-art single image based depth estimation\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 20:41:19 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Saadat", "Akbar", ""]]}, {"id": "1709.00106", "submitter": "Brendt Wohlberg", "authors": "Jialin Liu, Cristina Garcia-Cardona, Brendt Wohlberg, Wotao Yin", "title": "First and Second Order Methods for Online Convolutional Dictionary\n  Learning", "comments": null, "journal-ref": "SIAM J. Imaging Sci., 11(2), 1589-1628, 2018", "doi": "10.1137/17M1145689", "report-no": null, "categories": "cs.LG cs.CV eess.IV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional sparse representations are a form of sparse representation with\na structured, translation invariant dictionary. Most convolutional dictionary\nlearning algorithms to date operate in batch mode, requiring simultaneous\naccess to all training images during the learning process, which results in\nvery high memory usage and severely limits the training data that can be used.\nVery recently, however, a number of authors have considered the design of\nonline convolutional dictionary learning algorithms that offer far better\nscaling of memory and computational cost with training set size than batch\nmethods. This paper extends our prior work, improving a number of aspects of\nour previous algorithm; proposing an entirely new one, with better performance,\nand that supports the inclusion of a spatial mask for learning from incomplete\ndata; and providing a rigorous theoretical analysis of these methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 23:19:02 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 17:18:20 GMT"}, {"version": "v3", "created": "Sat, 16 Jun 2018 19:21:10 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Liu", "Jialin", ""], ["Garcia-Cardona", "Cristina", ""], ["Wohlberg", "Brendt", ""], ["Yin", "Wotao", ""]]}, {"id": "1709.00138", "submitter": "Pan He", "authors": "Pan He, Weilin Huang, Tong He, Qile Zhu, Yu Qiao, Xiaolin Li", "title": "Single Shot Text Detector with Regional Attention", "comments": "To appear in IEEE International Conference on Computer Vision (ICCV),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel single-shot text detector that directly outputs word-level\nbounding boxes in a natural image. We propose an attention mechanism which\nroughly identifies text regions via an automatically learned attentional map.\nThis substantially suppresses background interference in the convolutional\nfeatures, which is the key to producing accurate inference of words,\nparticularly at extremely small sizes. This results in a single model that\nessentially works in a coarse-to-fine manner. It departs from recent FCN- based\ntext detectors which cascade multiple FCN models to achieve an accurate\nprediction. Furthermore, we develop a hierarchical inception module which\nefficiently aggregates multi-scale inception features. This enhances local\ndetails, and also encodes strong context information, allow- ing the detector\nto work reliably on multi-scale and multi- orientation text with single-scale\nimages. Our text detector achieves an F-measure of 77% on the ICDAR 2015 bench-\nmark, advancing the state-of-the-art results in [18, 28]. Demo is available at:\nhttp://sstd.whuang.org/.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 02:42:57 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["He", "Pan", ""], ["Huang", "Weilin", ""], ["He", "Tong", ""], ["Zhu", "Qile", ""], ["Qiao", "Yu", ""], ["Li", "Xiaolin", ""]]}, {"id": "1709.00141", "submitter": "Martin Lukac", "authors": "Martin Lukac, Aigerim Bazarbayeva and Michitaka Kameyama", "title": "Context Based Visual Content Verification", "comments": "6 pages, 6 Figures, Published in Proceedings of the Information and\n  Digital Technology Conference, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the intermediary visual content verification method based on\nmulti-level co-occurrences is studied. The co-occurrence statistics are in\ngeneral used to determine relational properties between objects based on\ninformation collected from data. As such these measures are heavily subject to\nrelative number of occurrences and give only limited amount of accuracy when\npredicting objects in real world. In order to improve the accuracy of this\nmethod in the verification task, we include the context information such as\nlocation, type of environment etc. In order to train our model we provide new\nannotated dataset the Advanced Attribute VOC (AAVOC) that contains additional\nproperties of the image. We show that the usage of context greatly improve the\naccuracy of verification with up to 16% improvement.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 02:52:46 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Lukac", "Martin", ""], ["Bazarbayeva", "Aigerim", ""], ["Kameyama", "Michitaka", ""]]}, {"id": "1709.00158", "submitter": "Vahid Jalili", "authors": "Vahid Jalili", "title": "Reasoning with shapes: profiting cognitive susceptibilities to infer\n  linear mapping transformations between shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual information plays an indispensable role in our daily interactions with\nenvironment. Such information is manipulated for a wide range of purposes\nspanning from basic object and material perception to complex gesture\ninterpretations. There have been novel studies in cognitive science for\nin-depth understanding of visual information manipulation, which lead to answer\nquestions such as: how we infer 2D/3D motion from a sequence of 2D images? how\nwe understand a motion from a single image frame? how we see forest avoiding\ntrees?\n  Leveraging on congruence, linear mapping transformation determination between\na set of shapes facilitate motion perception. Present study methodizes recent\ndiscoveries of human cognitive ability for scene understanding. The proposed\nmethod processes images hierarchically, that is an iterative analysis of scene\nabstractions using a rapidly converging heuristic iterative method. The method\nhierarchically abstracts images; the abstractions are represented in polar\ncoordinate system, and any two consecutive abstractions have incremental level\nof details. The method then creates a graph of approximated linear mapping\ntransformations based on circular shift permutations of hierarchical\nabstractions. The graph is then traversed in best-first fashion to find best\nlinear mapping transformation. The accuracy of the proposed method is assessed\nusing normal, noisy, and deformed images. Additionally, the present study\ndeduces (i) the possibility of determining optimal mapping linear\ntransformations in logarithmic iterations with respect to the precision of\nresults, and (ii) computational cost is independent from the resolution of\ninput shapes.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 05:08:19 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Jalili", "Vahid", ""]]}, {"id": "1709.00179", "submitter": "Ryuhei Hamaguchi", "authors": "Ryuhei Hamaguchi, Aito Fujita, Keisuke Nemoto, Tomoyuki Imaizumi,\n  Shuhei Hikosaka", "title": "Effective Use of Dilated Convolutions for Segmenting Small Object\n  Instances in Remote Sensing Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to recent advances in CNNs, solid improvements have been made in\nsemantic segmentation of high resolution remote sensing imagery. However, most\nof the previous works have not fully taken into account the specific\ndifficulties that exist in remote sensing tasks. One of such difficulties is\nthat objects are small and crowded in remote sensing imagery. To tackle with\nthis challenging task we have proposed a novel architecture called local\nfeature extraction (LFE) module attached on top of dilated front-end module.\nThe LFE module is based on our findings that aggressively increasing dilation\nfactors fails to aggregate local features due to sparsity of the kernel, and\ndetrimental to small objects. The proposed LFE module solves this problem by\naggregating local features with decreasing dilation factor. We tested our\nnetwork on three remote sensing datasets and acquired remarkably good results\nfor all datasets especially for small objects.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 07:20:01 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Hamaguchi", "Ryuhei", ""], ["Fujita", "Aito", ""], ["Nemoto", "Keisuke", ""], ["Imaizumi", "Tomoyuki", ""], ["Hikosaka", "Shuhei", ""]]}, {"id": "1709.00192", "submitter": "Chang Yi", "authors": "Yi Chang, Luxin Yan, Houzhang Fang, Sheng Zhong, Zhijun Zhang", "title": "Weighted Low-rank Tensor Recovery for Hyperspectral Image Restoration", "comments": "22 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging, providing abundant spatial and spectral information\nsimultaneously, has attracted a lot of interest in recent years. Unfortunately,\ndue to the hardware limitations, the hyperspectral image (HSI) is vulnerable to\nvarious degradations, such noises (random noise, HSI denoising), blurs\n(Gaussian and uniform blur, HSI deblurring), and down-sampled (both spectral\nand spatial downsample, HSI super-resolution). Previous HSI restoration methods\nare designed for one specific task only. Besides, most of them start from the\n1-D vector or 2-D matrix models and cannot fully exploit the structurally\nspectral-spatial correlation in 3-D HSI. To overcome these limitations, in this\nwork, we propose a unified low-rank tensor recovery model for comprehensive HSI\nrestoration tasks, in which non-local similarity between spectral-spatial cubic\nand spectral correlation are simultaneously captured by 3-order tensors.\nFurther, to improve the capability and flexibility, we formulate it as a\nweighted low-rank tensor recovery (WLRTR) model by treating the singular values\ndifferently, and study its analytical solution. We also consider the exclusive\nstripe noise in HSI as the gross error by extending WLRTR to robust principal\ncomponent analysis (WLRTR-RPCA). Extensive experiments demonstrate the proposed\nWLRTR models consistently outperform state-of-the-arts in typical low level\nvision HSI tasks, including denoising, destriping, deblurring and\nsuper-resolution.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 07:58:34 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Chang", "Yi", ""], ["Yan", "Luxin", ""], ["Fang", "Houzhang", ""], ["Zhong", "Sheng", ""], ["Zhang", "Zhijun", ""]]}, {"id": "1709.00201", "submitter": "Shihao Sun", "authors": "Ruirui Li, Wenjie Liu, Lei Yang, Shihao Sun, Wei Hu, Fan Zhang, Wei Li", "title": "DeepUNet: A Deep Fully Convolutional Network for Pixel-level Sea-Land\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a fundamental research in remote sensing image\nprocessing. Because of the complex maritime environment, the sea-land\nsegmentation is a challenging task. Although the neural network has achieved\nexcellent performance in semantic segmentation in the last years, there are a\nfew of works using CNN for sea-land segmentation and the results could be\nfurther improved. This paper proposes a novel deep convolution neural network\nnamed DeepUNet. Like the U-Net, its structure has a contracting path and an\nexpansive path to get high resolution output. But differently, the DeepUNet\nuses DownBlocks instead of convolution layers in the contracting path and uses\nUpBlock in the expansive path. The two novel blocks bring two new connections\nthat are U-connection and Plus connection. They are promoted to get more\nprecise segmentation results. To verify our network architecture, we made a new\nchallenging sea-land dataset and compare the DeepUNet on it with the SegNet and\nthe U-Net. Experimental results show that DeepUNet achieved good performance\ncompared with other architectures, especially in high-resolution remote sensing\nimagery.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 08:54:12 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Li", "Ruirui", ""], ["Liu", "Wenjie", ""], ["Yang", "Lei", ""], ["Sun", "Shihao", ""], ["Hu", "Wei", ""], ["Zhang", "Fan", ""], ["Li", "Wei", ""]]}, {"id": "1709.00235", "submitter": "Xiaowei Zhang", "authors": "Xiaowei Zhang, Li Cheng, Bo Li, Hai-Miao Hu", "title": "Too Far to See? Not Really! --- Pedestrian Detection with Scale-aware\n  Localization Policy", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2818018", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major bottleneck of pedestrian detection lies on the sharp performance\ndeterioration in the presence of small-size pedestrians that are relatively far\nfrom the camera. Motivated by the observation that pedestrians of disparate\nspatial scales exhibit distinct visual appearances, we propose in this paper an\nactive pedestrian detector that explicitly operates over multiple-layer\nneuronal representations of the input still image. More specifically,\nconvolutional neural nets such as ResNet and faster R-CNNs are exploited to\nprovide a rich and discriminative hierarchy of feature representations as well\nas initial pedestrian proposals. Here each pedestrian observation of distinct\nsize could be best characterized in terms of the ResNet feature representation\nat a certain layer of the hierarchy; Meanwhile, initial pedestrian proposals\nare attained by faster R-CNNs techniques, i.e. region proposal network and\nfollow-up region of interesting pooling layer employed right after the specific\nResNet convolutional layer of interest, to produce joint predictions on the\nbounding-box proposals' locations and categories (i.e. pedestrian or not). This\nis engaged as input to our active detector where for each initial pedestrian\nproposal, a sequence of coordinate transformation actions is carried out to\ndetermine its proper x-y 2D location and layer of feature representation, or\neventually terminated as being background. Empirically our approach is\ndemonstrated to produce overall lower detection errors on widely-used\nbenchmarks, and it works particularly well with far-scale pedestrians. For\nexample, compared with 60.51% log-average miss rate of the state-of-the-art\nMS-CNN for far-scale pedestrians (those below 80 pixels in bounding-box height)\nof the Caltech benchmark, the miss rate of our approach is 41.85%, with a\nnotable reduction of 18.68%.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 10:32:09 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Zhang", "Xiaowei", ""], ["Cheng", "Li", ""], ["Li", "Bo", ""], ["Hu", "Hai-Miao", ""]]}, {"id": "1709.00265", "submitter": "Aitor Alvarez-Gila", "authors": "Aitor Alvarez-Gila, Joost van de Weijer, Estibaliz Garrote", "title": "Adversarial Networks for Spatial Context-Aware Spectral Image\n  Reconstruction from RGB", "comments": "Accepted at IEEE ICCVW 2017 - \"Physics Based Vision meets Deep\n  Learning\" Workshop (PBDL 2017). Added train-test splits and updated results", "journal-ref": null, "doi": "10.1109/ICCVW.2017.64", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral signal reconstruction aims at recovering the original spectral\ninput that produced a certain trichromatic (RGB) response from a capturing\ndevice or observer. Given the heavily underconstrained, non-linear nature of\nthe problem, traditional techniques leverage different statistical properties\nof the spectral signal in order to build informative priors from real world\nobject reflectances for constructing such RGB to spectral signal mapping.\nHowever, most of them treat each sample independently, and thus do not benefit\nfrom the contextual information that the spatial dimensions can provide. We\npose hyperspectral natural image reconstruction as an image to image mapping\nlearning problem, and apply a conditional generative adversarial framework to\nhelp capture spatial semantics. This is the first time Convolutional Neural\nNetworks -and, particularly, Generative Adversarial Networks- are used to solve\nthis task. Quantitative evaluation shows a Root Mean Squared Error (RMSE) drop\nof 33.2% and a Relative RMSE drop of 54.0% on the ICVL natural hyperspectral\nimage dataset.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 12:00:51 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 14:48:14 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Alvarez-Gila", "Aitor", ""], ["van de Weijer", "Joost", ""], ["Garrote", "Estibaliz", ""]]}, {"id": "1709.00300", "submitter": "Yu Wang", "authors": "Yu Wang, Jixing Xu, Aohan Wu, Mantian Li, Yang He, Jinghe Hu, Weipeng\n  P. Yan", "title": "Telepath: Understanding Users from a Human Vision Perspective in\n  Large-Scale Recommender Systems", "comments": "8 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing an e-commerce recommender system that serves hundreds of millions\nof active users is a daunting challenge. From a human vision perspective,\nthere're two key factors that affect users' behaviors: items' attractiveness\nand their matching degree with users' interests. This paper proposes Telepath,\na vision-based bionic recommender system model, which understands users from\nsuch perspective. Telepath is a combination of a convolutional neural network\n(CNN), a recurrent neural network (RNN) and deep neural networks (DNNs). Its\nCNN subnetwork simulates the human vision system to extract key visual signals\nof items' attractiveness and generate corresponding activations. Its RNN and\nDNN subnetworks simulate cerebral cortex to understand users' interest based on\nthe activations generated from browsed items. In practice, the Telepath model\nhas been launched to JD's recommender system and advertising system. For one of\nthe major item recommendation blocks on the JD app, click-through rate (CTR),\ngross merchandise value (GMV) and orders have increased 1.59%, 8.16% and 8.71%\nrespectively. For several major ads publishers of JD demand-side platform, CTR,\nGMV and return on investment have increased 6.58%, 61.72% and 65.57%\nrespectively by the first launch, and further increased 2.95%, 41.75% and\n41.37% respectively by the second launch.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 13:29:36 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 16:06:57 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Wang", "Yu", ""], ["Xu", "Jixing", ""], ["Wu", "Aohan", ""], ["Li", "Mantian", ""], ["He", "Yang", ""], ["Hu", "Jinghe", ""], ["Yan", "Weipeng P.", ""]]}, {"id": "1709.00308", "submitter": "Chee Seng Chan", "authors": "John E. Ball, Derek T. Anderson, Chee Seng Chan", "title": "A Comprehensive Survey of Deep Learning in Remote Sensing: Theories,\n  Tools and Challenges for the Community", "comments": "64 pages, 411 references. To appear in Journal of Applied Remote\n  Sensing", "journal-ref": "J. Appl. Remote Sens. 11(4) (2017) 042609", "doi": "10.1117/1.JRS.11.042609", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning (DL), a re-branding of neural networks (NNs),\nhas risen to the top in numerous areas, namely computer vision (CV), speech\nrecognition, natural language processing, etc. Whereas remote sensing (RS)\npossesses a number of unique challenges, primarily related to sensors and\napplications, inevitably RS draws from many of the same theories as CV; e.g.,\nstatistics, fusion, and machine learning, to name a few. This means that the RS\ncommunity should be aware of, if not at the leading edge of, of advancements\nlike DL. Herein, we provide the most comprehensive survey of state-of-the-art\nRS DL research. We also review recent new developments in the DL field that can\nbe used in DL for RS. Namely, we focus on theories, tools and challenges for\nthe RS community. Specifically, we focus on unsolved challenges and\nopportunities as it relates to (i) inadequate data sets, (ii)\nhuman-understandable solutions for modelling physical phenomena, (iii) Big\nData, (iv) non-traditional heterogeneous data sources, (v) DL architectures and\nlearning algorithms for spectral, spatial and temporal data, (vi) transfer\nlearning, (vii) an improved theoretical understanding of DL systems, (viii)\nhigh barriers to entry, and (ix) training and optimizing the DL.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 13:40:35 GMT"}, {"version": "v2", "created": "Sun, 24 Sep 2017 06:11:30 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Ball", "John E.", ""], ["Anderson", "Derek T.", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1709.00340", "submitter": "Yuxin Peng", "authors": "Xiangteng He and Yuxin Peng", "title": "Fine-grained Visual-textual Representation Learning", "comments": "12 pages, accepted by IEEE Transactions on Circuits and Systems for\n  Video Technology (TCSVT)", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2892802", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual categorization is to recognize hundreds of subcategories\nbelonging to the same basic-level category, which is a highly challenging task\ndue to the quite subtle and local visual distinctions among similar\nsubcategories. Most existing methods generally learn part detectors to discover\ndiscriminative regions for better categorization performance. However, not all\nparts are beneficial and indispensable for visual categorization, and the\nsetting of part detector number heavily relies on prior knowledge as well as\nexperimental validation. As is known to all, when we describe the object of an\nimage via textual descriptions, we mainly focus on the pivotal characteristics,\nand rarely pay attention to common characteristics as well as the background\nareas. This is an involuntary transfer from human visual attention to textual\nattention, which leads to the fact that textual attention tells us how many and\nwhich parts are discriminative and significant to categorization. So textual\nattention could help us to discover visual attention in image. Inspired by\nthis, we propose a fine-grained visual-textual representation learning (VTRL)\napproach, and its main contributions are: (1) Fine-grained visual-textual\npattern mining devotes to discovering discriminative visual-textual pairwise\ninformation for boosting categorization performance through jointly modeling\nvision and text with generative adversarial networks (GANs), which\nautomatically and adaptively discovers discriminative parts. (2) Visual-textual\nrepresentation learning jointly combines visual and textual information, which\npreserves the intra-modality and inter-modality information to generate\ncomplementary fine-grained representation, as well as further improves\ncategorization performance.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 12:41:55 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 12:34:34 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 11:30:58 GMT"}, {"version": "v4", "created": "Wed, 20 Feb 2019 13:37:21 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["He", "Xiangteng", ""], ["Peng", "Yuxin", ""]]}, {"id": "1709.00382", "submitter": "Guotai Wang", "authors": "Guotai Wang, Wenqi Li, Sebastien Ourselin, and Tom Vercauteren", "title": "Automatic Brain Tumor Segmentation using Cascaded Anisotropic\n  Convolutional Neural Networks", "comments": "12 pages, 5 figures. MICCAI Brats Challenge 2017", "journal-ref": null, "doi": "10.1007/978-3-319-75238-9_16", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cascade of fully convolutional neural networks is proposed to segment\nmulti-modal Magnetic Resonance (MR) images with brain tumor into background and\nthree hierarchical regions: whole tumor, tumor core and enhancing tumor core.\nThe cascade is designed to decompose the multi-class segmentation problem into\na sequence of three binary segmentation problems according to the subregion\nhierarchy. The whole tumor is segmented in the first step and the bounding box\nof the result is used for the tumor core segmentation in the second step. The\nenhancing tumor core is then segmented based on the bounding box of the tumor\ncore segmentation result. Our networks consist of multiple layers of\nanisotropic and dilated convolution filters, and they are combined with\nmulti-view fusion to reduce false positives. Residual connections and\nmulti-scale predictions are employed in these networks to boost the\nsegmentation performance. Experiments with BraTS 2017 validation set show that\nthe proposed method achieved average Dice scores of 0.7859, 0.9050, 0.8378 for\nenhancing tumor core, whole tumor and tumor core, respectively. The\ncorresponding values for BraTS 2017 testing set were 0.7831, 0.8739, and\n0.7748, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 16:11:34 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 09:15:00 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Wang", "Guotai", ""], ["Li", "Wenqi", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1709.00408", "submitter": "Rajesh Menon", "authors": "Ganghun Kim, Stefan Kapetanovic, Rachael Palmer, Rajesh Menon", "title": "Lensless-camera based machine learning for image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) has been widely applied to image classification. Here,\nwe extend this application to data generated by a camera comprised of only a\nstandard CMOS image sensor with no lens. We first created a database of\nlensless images of handwritten digits. Then, we trained a ML algorithm on this\ndataset. Finally, we demonstrated that the trained ML algorithm is able to\nclassify the digits with accuracy as high as 99% for 2 digits. Our approach\nclearly demonstrates the potential for non-human cameras in machine-based\ndecision-making scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 23:42:29 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Kim", "Ganghun", ""], ["Kapetanovic", "Stefan", ""], ["Palmer", "Rachael", ""], ["Menon", "Rajesh", ""]]}, {"id": "1709.00443", "submitter": "Stavros Petridis", "authors": "Stavros Petridis, Yujiang Wang, Zuwei Li, Maja Pantic", "title": "End-to-End Multi-View Lipreading", "comments": "Accepted to BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-frontal lip views contain useful information which can be used to enhance\nthe performance of frontal view lipreading. However, the vast majority of\nrecent lipreading works, including the deep learning approaches which\nsignificantly outperform traditional approaches, have focused on frontal mouth\nimages. As a consequence, research on joint learning of visual features and\nspeech classification from multiple views is limited. In this work, we present\nan end-to-end multi-view lipreading system based on Bidirectional Long-Short\nMemory (BLSTM) networks. To the best of our knowledge, this is the first model\nwhich simultaneously learns to extract features directly from the pixels and\nperforms visual speech classification from multiple views and also achieves\nstate-of-the-art performance. The model consists of multiple identical streams,\none for each view, which extract features directly from different poses of\nmouth images. The temporal dynamics in each stream/view are modelled by a BLSTM\nand the fusion of multiple streams/views takes place via another BLSTM. An\nabsolute average improvement of 3% and 3.8% over the frontal view performance\nis reported on the OuluVS2 database when the best two (frontal and profile) and\nthree views (frontal, profile, 45) are combined, respectively. The best\nthree-view model results in a 10.5% absolute improvement over the current\nmulti-view state-of-the-art performance on OuluVS2, without using external\ndatabases for training, achieving a maximum classification accuracy of 96.9%.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 18:51:53 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Petridis", "Stavros", ""], ["Wang", "Yujiang", ""], ["Li", "Zuwei", ""], ["Pantic", "Maja", ""]]}, {"id": "1709.00483", "submitter": "Tao Sun", "authors": "Tao Sun, Hao Jiang, Lizhi Cheng, Wei Zhu", "title": "Iteratively Linearized Reweighted Alternating Direction Method of\n  Multipliers for a Class of Nonconvex Problems", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2868269", "report-no": null, "categories": "cs.NA cs.CV math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider solving a class of nonconvex and nonsmooth\nproblems frequently appearing in signal processing and machine learning\nresearch. The traditional alternating direction method of multipliers\nencounters troubles in both mathematics and computations in solving the\nnonconvex and nonsmooth subproblem. In view of this, we propose a reweighted\nalternating direction method of multipliers. In this algorithm, all subproblems\nare convex and easy to solve. We also provide several guarantees for the\nconvergence and prove that the algorithm globally converges to a critical point\nof an auxiliary function with the help of the Kurdyka-{\\L}ojasiewicz property.\nSeveral numerical results are presented to demonstrate the efficiency of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 21:20:30 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 04:03:33 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 04:15:28 GMT"}, {"version": "v4", "created": "Tue, 20 Mar 2018 00:52:41 GMT"}, {"version": "v5", "created": "Thu, 22 Mar 2018 23:26:33 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Sun", "Tao", ""], ["Jiang", "Hao", ""], ["Cheng", "Lizhi", ""], ["Zhu", "Wei", ""]]}, {"id": "1709.00505", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman, Ruohan Gao, and Kristen Grauman", "title": "ShapeCodes: Self-Supervised Feature Learning by Lifting Views to\n  Viewgrids", "comments": "To appear at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an unsupervised feature learning approach that embeds 3D shape\ninformation into a single-view image representation. The main idea is a\nself-supervised training objective that, given only a single 2D image, requires\nall unseen views of the object to be predictable from learned features. We\nimplement this idea as an encoder-decoder convolutional neural network. The\nnetwork maps an input image of an unknown category and unknown viewpoint to a\nlatent space, from which a deconvolutional decoder can best \"lift\" the image to\nits complete viewgrid showing the object from all viewing angles. Our\nclass-agnostic training procedure encourages the representation to capture\nfundamental shape primitives and semantic regularities in a data-driven\nmanner---without manual semantic labels. Our results on two widely-used shape\ndatasets show 1) our approach successfully learns to perform \"mental rotation\"\neven for objects unseen during training, and 2) the learned latent space is a\npowerful representation for object recognition, outperforming several existing\nunsupervised feature learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 23:15:28 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 03:34:11 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 04:17:28 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 03:02:06 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Gao", "Ruohan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1709.00507", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman and Kristen Grauman", "title": "Learning to Look Around: Intelligently Exploring Unseen Environments for\n  Unknown Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to implicitly assume access to intelligently captured inputs\n(e.g., photos from a human photographer), yet autonomously capturing good\nobservations is itself a major challenge. We address the problem of learning to\nlook around: if a visual agent has the ability to voluntarily acquire new views\nto observe its environment, how can it learn efficient exploratory behaviors to\nacquire informative observations? We propose a reinforcement learning solution,\nwhere the agent is rewarded for actions that reduce its uncertainty about the\nunobserved portions of its environment. Based on this principle, we develop a\nrecurrent neural network-based approach to perform active completion of\npanoramic natural scenes and 3D object shapes. Crucially, the learned policies\nare not tied to any recognition task nor to the particular semantic content\nseen during training. As a result, 1) the learned \"look around\" behavior is\nrelevant even for new tasks in unseen environments, and 2) training data\nacquisition involves no manual labeling. Through tests in diverse settings, we\ndemonstrate that our approach learns useful generic policies that transfer to\nnew unseen tasks and environments. Completion episodes are shown at\nhttps://goo.gl/BgWX3W.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 23:34:29 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 17:11:10 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1709.00513", "submitter": "Zheng Xu", "authors": "Zheng Xu, Yen-Chang Hsu, Jiawei Huang", "title": "Training Shallow and Thin Networks for Acceleration via Knowledge\n  Distillation with Conditional Adversarial Networks", "comments": "Shorter version will appear at ICLR workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest on accelerating neural networks for real-time\napplications. We study the student-teacher strategy, in which a small and fast\nstudent network is trained with the auxiliary information learned from a large\nand accurate teacher network. We propose to use conditional adversarial\nnetworks to learn the loss function to transfer knowledge from teacher to\nstudent. The proposed method is particularly effective for relatively small\nstudent networks. Moreover, experimental results show the effect of network\nsize when the modern networks are used as student. We empirically study the\ntrade-off between inference time and classification accuracy, and provide\nsuggestions on choosing a proper student network.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 01:03:08 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 18:42:13 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Xu", "Zheng", ""], ["Hsu", "Yen-Chang", ""], ["Huang", "Jiawei", ""]]}, {"id": "1709.00516", "submitter": "Yichi Gu", "authors": "Yichi Gu, Qisheng Wu, Jing Li, Kai Cheng", "title": "Gaussian Filter in CRF Based Semantic Segmentation", "comments": "11 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence is making great changes in academy and industry with\nthe fast development of deep learning, which is a branch of machine learning\nand statistical learning. Fully convolutional network [1] is the standard model\nfor semantic segmentation. Conditional random fields coded as CNN [2] or RNN\n[3] and connected with FCN has been successfully applied in object detection\n[4]. In this paper, we introduce a multi-resolution neural network for FCN and\napply Gaussian filter to the extended CRF kernel neighborhood and the label\nimage to reduce the oscillating effect of CRF neural network segmentation, thus\nachieve higher precision and faster training speed.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 01:38:34 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Gu", "Yichi", ""], ["Wu", "Qisheng", ""], ["Li", "Jing", ""], ["Cheng", "Kai", ""]]}, {"id": "1709.00531", "submitter": "Yuhang Wu", "authors": "Yuhang Wu and Ioannis A. Kakadiaris", "title": "Facial 3D Model Registration Under Occlusions With SensiblePoints-based\n  Reinforced Hypothesis Refinement", "comments": "Accepted in International Joint Conference on Biometrics (IJCB) 2017", "journal-ref": "2017 IEEE International Joint Conference on Biometrics", "doi": "10.1109/BTAS.2017.8272734", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registering a 3D facial model to a 2D image under occlusion is difficult.\nFirst, not all of the detected facial landmarks are accurate under occlusions.\nSecond, the number of reliable landmarks may not be enough to constrain the\nproblem. We propose a method to synthesize additional points (SensiblePoints)\nto create pose hypotheses. The visual clues extracted from the fiducial points,\nnon-fiducial points, and facial contour are jointly employed to verify the\nhypotheses. We define a reward function to measure whether the projected dense\n3D model is well-aligned with the confidence maps generated by two fully\nconvolutional networks, and use the function to train recurrent policy networks\nto move the SensiblePoints. The same reward function is employed in testing to\nselect the best hypothesis from a candidate pool of hypotheses. Experimentation\ndemonstrates that the proposed approach is very promising in solving the facial\nmodel registration problem under occlusion.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 04:00:29 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wu", "Yuhang", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1709.00536", "submitter": "Ronald Yu", "authors": "Ronald Yu, Shunsuke Saito, Haoxiang Li, Duygu Ceylan, Hao Li", "title": "Learning Dense Facial Correspondences in Unconstrained Images", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a minimalistic but effective neural network that computes dense\nfacial correspondences in highly unconstrained RGB images. Our network learns a\nper-pixel flow and a matchability mask between 2D input photographs of a person\nand the projection of a textured 3D face model. To train such a network, we\ngenerate a massive dataset of synthetic faces with dense labels using\nrenderings of a morphable face model with variations in pose, expressions,\nlighting, and occlusions. We found that a training refinement using real\nphotographs is required to drastically improve the ability to handle real\nimages. When combined with a facial detection and 3D face fitting step, we show\nthat our approach outperforms the state-of-the-art face alignment methods in\nterms of accuracy and speed. By directly estimating dense correspondences, we\ndo not rely on the full visibility of sparse facial landmarks and are not\nlimited to the model space of regression-based approaches. We also assess our\nmethod on video frames and demonstrate successful per-frame processing under\nextreme pose variations, occlusions, and lighting conditions. Compared to\nexisting 3D facial tracking techniques, our fitting does not rely on previous\nframes or frontal facial initialization and is robust to imperfect face\ndetections.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 06:02:26 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Yu", "Ronald", ""], ["Saito", "Shunsuke", ""], ["Li", "Haoxiang", ""], ["Ceylan", "Duygu", ""], ["Li", "Hao", ""]]}, {"id": "1709.00572", "submitter": "C\\u{a}t\\u{a}lina Cangea", "authors": "C\\u{a}t\\u{a}lina Cangea, Petar Veli\\v{c}kovi\\'c, Pietro Li\\`o", "title": "XFlow: Cross-modal Deep Neural Networks for Audiovisual Classification", "comments": "Accepted at the IEEE ICDL-EPIROB 2017 Workshop on Computational\n  Models for Crossmodal Learning (CMCML), 4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there have been numerous developments towards solving\nmultimodal tasks, aiming to learn a stronger representation than through a\nsingle modality. Certain aspects of the data can be particularly useful in this\ncase - for example, correlations in the space or time domain across modalities\n- but should be wisely exploited in order to benefit from their full predictive\npotential. We propose two deep learning architectures with multimodal\ncross-connections that allow for dataflow between several feature extractors\n(XFlow). Our models derive more interpretable features and achieve better\nperformances than models which do not exchange representations, usefully\nexploiting correlations between audio and visual data, which have a different\ndimensionality and are nontrivially exchangeable. Our work improves on existing\nmultimodal deep learning algorithms in two essential ways: (1) it presents a\nnovel method for performing cross-modality (before features are learned from\nindividual modalities) and (2) extends the previously proposed\ncross-connections which only transfer information between streams that process\ncompatible data. Illustrating some of the representations learned by the\nconnections, we analyse their contribution to the increase in discrimination\nability and reveal their compatibility with a lip-reading network intermediate\nrepresentation. We provide the research community with Digits, a new dataset\nconsisting of three data types extracted from videos of people saying the\ndigits 0-9. Results show that both cross-modal architectures outperform their\nbaselines (by up to 11.5%) when evaluated on the AVletters, CUAVE and Digits\ndatasets, achieving state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 12:43:59 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 21:43:42 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Cangea", "C\u0103t\u0103lina", ""], ["Veli\u010dkovi\u0107", "Petar", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "1709.00584", "submitter": "Mark Anastasio", "authors": "Brendan Kelly, Thomas P. Matthews, Mark A. Anastasio", "title": "Deep Learning-Guided Image Reconstruction from Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach to incorporate deep learning within an iterative image\nreconstruction framework to reconstruct images from severely incomplete\nmeasurement data is presented. Specifically, we utilize a convolutional neural\nnetwork (CNN) as a quasi-projection operator within a least squares\nminimization procedure. The CNN is trained to encode high level information\nabout the class of images being imaged; this information is utilized to\nmitigate artifacts in intermediate images produced by use of an iterative\nmethod. The structure of the method was inspired by the proximal gradient\ndescent method, where the proximal operator is replaced by a deep CNN and the\ngradient descent step is generalized by use of a linear reconstruction\noperator. It is demonstrated that this approach improves image quality for\nseveral cases of limited-view image reconstruction and that using a CNN in an\niterative method increases performance compared to conventional image\nreconstruction approaches. We test our method on several limited-view image\nreconstruction problems. Qualitative and quantitative results demonstrate\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 14:15:24 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Kelly", "Brendan", ""], ["Matthews", "Thomas P.", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "1709.00643", "submitter": "Qifeng Chen", "authors": "Qifeng Chen, Jia Xu, Vladlen Koltun", "title": "Fast Image Processing with Fully-Convolutional Networks", "comments": "Published at the International Conference on Computer Vision (ICCV\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to accelerating a wide variety of image processing\noperators. Our approach uses a fully-convolutional network that is trained on\ninput-output pairs that demonstrate the operator's action. After training, the\noriginal operator need not be run at all. The trained network operates at full\nresolution and runs in constant time. We investigate the effect of network\narchitecture on approximation accuracy, runtime, and memory footprint, and\nidentify a specific architecture that balances these considerations. We\nevaluate the presented approach on ten advanced image processing operators,\nincluding multiple variational models, multiscale tone and detail manipulation,\nphotographic style transfer, nonlocal dehazing, and nonphotorealistic\nstylization. All operators are approximated by the same model. Experiments\ndemonstrate that the presented approach is significantly more accurate than\nprior approximation schemes. It increases approximation accuracy as measured by\nPSNR across the evaluated operators by 8.5 dB on the MIT-Adobe dataset (from\n27.5 to 36 dB) and reduces DSSIM by a multiplicative factor of 3 compared to\nthe most accurate prior approximation scheme, while being the fastest. We show\nthat our models generalize across datasets and across resolutions, and\ninvestigate a number of extensions of the presented approach. The results are\nshown in the supplementary video at https://youtu.be/eQyfHgLx8Dc\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 22:38:13 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Chen", "Qifeng", ""], ["Xu", "Jia", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1709.00649", "submitter": "Sebastian Wagner-Carena", "authors": "Max Hopkins, Michael Mitzenmacher, and Sebastian Wagner-Carena", "title": "Simulated Annealing for JPEG Quantization", "comments": "Appendix not included in arXiv version due to size restrictions. For\n  full paper go to:\n  http://www.eecs.harvard.edu/~michaelm/SimAnneal/PAPER/simulated-annealing-jpeg.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JPEG is one of the most widely used image formats, but in some ways remains\nsurprisingly unoptimized, perhaps because some natural optimizations would go\noutside the standard that defines JPEG. We show how to improve JPEG compression\nin a standard-compliant, backward-compatible manner, by finding improved\ndefault quantization tables. We describe a simulated annealing technique that\nhas allowed us to find several quantization tables that perform better than the\nindustry standard, in terms of both compressed size and image fidelity.\nSpecifically, we derive tables that reduce the FSIM error by over 10% while\nimproving compression by over 20% at quality level 95 in our tests; we also\nprovide similar results for other quality levels. While we acknowledge our\napproach can in some images lead to visible artifacts under large\nmagnification, we believe use of these quantization tables, or additional\ntables that could be found using our methodology, would significantly reduce\nJPEG file sizes with improved overall image quality.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 01:10:18 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Hopkins", "Max", ""], ["Mitzenmacher", "Michael", ""], ["Wagner-Carena", "Sebastian", ""]]}, {"id": "1709.00657", "submitter": "Guangcan Liu", "authors": "Yang Li, Guangcan Liu, Shengyong Chen", "title": "Detection of Moving Object in Dynamic Background Using Gaussian\n  Max-Pooling and Segmentation Constrained RPCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its efficiency and stability, Robust Principal Component Analysis\n(RPCA) has been emerging as a promising tool for moving object detection.\nUnfortunately, existing RPCA based methods assume static or quasi-static\nbackground, and thereby they may have trouble in coping with the background\nscenes that exhibit a persistent dynamic behavior. In this work, we shall\nintroduce two techniques to fill in the gap. First, instead of using the raw\npixel-value as features that are brittle in the presence of dynamic background,\nwe devise a so-called Gaussian max-pooling operator to estimate a\n\"stable-value\" for each pixel. Those stable-values are robust to various\nbackground changes and can therefore distinguish effectively the foreground\nobjects from the background. Then, to obtain more accurate results, we further\npropose a Segmentation Constrained RPCA (SC-RPCA) model, which incorporates the\ntemporal and spatial continuity in images into RPCA. The inference process of\nSC-RPCA is a group sparsity constrained nuclear norm minimization problem,\nwhich is convex and easy to solve. Experimental results on seven videos from\nthe CDCNET 2014 database show the superior performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 03:38:58 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Li", "Yang", ""], ["Liu", "Guangcan", ""], ["Chen", "Shengyong", ""]]}, {"id": "1709.00663", "submitter": "Ashish Mishra", "authors": "Ashish Mishra, M Shiva Krishna Reddy, Anurag Mittal and Hema A Murthy", "title": "A Generative Model For Zero Shot Learning Using Conditional Variational\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Zero shot learning in Image Classification refers to the setting where images\nfrom some novel classes are absent in the training data but other information\nsuch as natural language descriptions or attribute vectors of the classes are\navailable. This setting is important in the real world since one may not be\nable to obtain images of all the possible classes at training. While previous\napproaches have tried to model the relationship between the class attribute\nspace and the image space via some kind of a transfer function in order to\nmodel the image space correspondingly to an unseen class, we take a different\napproach and try to generate the samples from the given attributes, using a\nconditional variational autoencoder, and use the generated samples for\nclassification of the unseen classes. By extensive testing on four benchmark\ndatasets, we show that our model outperforms the state of the art, particularly\nin the more realistic generalized setting, where the training classes can also\nappear at the test time along with the novel classes.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 04:17:27 GMT"}, {"version": "v2", "created": "Sat, 27 Jan 2018 13:30:42 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Mishra", "Ashish", ""], ["Reddy", "M Shiva Krishna", ""], ["Mittal", "Anurag", ""], ["Murthy", "Hema A", ""]]}, {"id": "1709.00672", "submitter": "Gaurav Pandey", "authors": "Gaurav Pandey and Ambedkar Dukkipati", "title": "Unsupervised feature learning with discriminative encoder", "comments": "10 pages, 4 figures, International Conference on Data Mining, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep discriminative models have achieved extraordinary\nperformance on supervised learning tasks, significantly outperforming their\ngenerative counterparts. However, their success relies on the presence of a\nlarge amount of labeled data. How can one use the same discriminative models\nfor learning useful features in the absence of labels? We address this question\nin this paper, by jointly modeling the distribution of data and latent features\nin a manner that explicitly assigns zero probability to unobserved data. Rather\nthan maximizing the marginal probability of observed data, we maximize the\njoint probability of the data and the latent features using a two step EM-like\nprocedure. To prevent the model from overfitting to our initial selection of\nlatent features, we use adversarial regularization. Depending on the task, we\nallow the latent features to be one-hot or real-valued vectors and define a\nsuitable prior on the features. For instance, one-hot features correspond to\nclass labels and are directly used for the unsupervised and semi-supervised\nclassification task, whereas real-valued feature vectors are fed as input to\nsimple classifiers for auxiliary supervised discrimination tasks. The proposed\nmodel, which we dub discriminative encoder (or DisCoder), is flexible in the\ntype of latent features that it can capture. The proposed model achieves\nstate-of-the-art performance on several challenging tasks.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 06:40:35 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Pandey", "Gaurav", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1709.00725", "submitter": "Shadrokh Samavi", "authors": "Maryam Karimi, Najmeh Soltanian, Shadrokh Samavi, Nader Karimi,\n  S.M.Reza Soroushmehr, Kayvan Najarian", "title": "Blind Stereo Image Quality Assessment Inspired by Brain Sensory-Motor\n  Fusion", "comments": "11 pages, 13 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of 3D and stereo imaging is rapidly increasing. Compression,\ntransmission, and processing could degrade the quality of stereo images.\nQuality assessment of such images is different than their 2D counterparts.\nMetrics that represent 3D perception by human visual system (HVS) are expected\nto assess stereoscopic quality more accurately. In this paper, inspired by\nbrain sensory/motor fusion process, two stereo images are fused together. Then\nfrom every fused image two synthesized images are extracted. Effects of\ndifferent distortions on statistical distributions of the synthesized images\nare shown. Based on the observed statistical changes, features are extracted\nfrom these synthesized images. These features can reveal type and severity of\ndistortions. Then, a stacked neural network model is proposed, which learns the\nextracted features and accurately evaluates the quality of stereo images. This\nmodel is tested on 3D images of popular databases. Experimental results show\nthe superiority of this method over state of the art stereo image quality\nassessment approaches\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 14:50:48 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Karimi", "Maryam", ""], ["Soltanian", "Najmeh", ""], ["Samavi", "Shadrokh", ""], ["Karimi", "Nader", ""], ["Soroushmehr", "S. M. Reza", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1709.00726", "submitter": "Vandit Gajjar", "authors": "Vandit Gajjar, Ayesha Gurnani and Yash Khandhediya", "title": "Human Detection and Tracking for Video Surveillance A Cognitive Science\n  Approach", "comments": "ICCV 2017 Venice, Italy Pages 5 Figures 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With crimes on the rise all around the world, video surveillance is becoming\nmore important day by day. Due to the lack of human resources to monitor this\nincreasing number of cameras manually new computer vision algorithms to perform\nlower and higher level tasks are being developed. We have developed a new\nmethod incorporating the most acclaimed Histograms of Oriented Gradients the\ntheory of Visual Saliency and the saliency prediction model Deep Multi Level\nNetwork to detect human beings in video sequences. Furthermore we implemented\nthe k Means algorithm to cluster the HOG feature vectors of the positively\ndetected windows and determined the path followed by a person in the video. We\nachieved a detection precision of 83.11% and a recall of 41.27%. We obtained\nthese results 76.866 times faster than classification on normal images.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 14:52:54 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Gajjar", "Vandit", ""], ["Gurnani", "Ayesha", ""], ["Khandhediya", "Yash", ""]]}, {"id": "1709.00727", "submitter": "Vandit Gajjar J", "authors": "Vandit Gajjar, Viraj Mavani, Ayesha Gurnani", "title": "Hand Gesture Real Time Paint Tool - Box", "comments": "This paper needs a proper writing and experiments need to be\n  implemented, thus we are withdrawing this submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With current development universally in computing, now a days user\ninteraction approaches with mouse, keyboard, touch-pens etc. are not\nsufficient. Directly using of hands or hand gestures as an input device is a\nmethod to attract people with providing the applications, through Machine\nLearning and Computer Vision. Human-computer interaction application in which\nyou can simply draw different shapes, fill the colors, moving the folder from\none place to another place and rotating your image with rotating your hand\ngesture all this will be without touching your device only. In this paper\nMachine Learning based hand gestures recognition is presented, with the use of\nComputer Vision different types of gesture applications have been created.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 14:53:05 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 16:59:47 GMT"}, {"version": "v3", "created": "Sun, 18 Mar 2018 05:10:35 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Gajjar", "Vandit", ""], ["Mavani", "Viraj", ""], ["Gurnani", "Ayesha", ""]]}, {"id": "1709.00751", "submitter": "Seung Hyun Son", "authors": "Yeongjin Oh, Seunghyun Son, Gyumin Sim", "title": "Sushi Dish - Object detection and classification from real images", "comments": "6 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conveyor belt sushi restaurants, billing is a burdened job because one has\nto manually count the number of dishes and identify the color of them to\ncalculate the price. In a busy situation, there can be a mistake that customers\nare overcharged or under-charged. To deal with this problem, we developed a\nmethod that automatically identifies the color of dishes and calculate the\ntotal price using real images. Our method consists of ellipse fitting and\nconvol-utional neural network. It achieves ellipse detection precision 85% and\nrecall 96% and classification accuracy 92%.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 18:02:01 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 16:18:10 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Oh", "Yeongjin", ""], ["Son", "Seunghyun", ""], ["Sim", "Gyumin", ""]]}, {"id": "1709.00753", "submitter": "Tran Minh Quan", "authors": "Tran Minh Quan, Thanh Nguyen-Duc and Won-Ki Jeong", "title": "Compressed Sensing MRI Reconstruction using a Generative Adversarial\n  Network with a Cyclic Loss", "comments": "submitted to IEEE Transactions on Medical Imaging", "journal-ref": "IEEE Trans. Med. Imaging 37(6): 1488-1497 (2018)", "doi": "10.1109/TMI.2018.2820120", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Compressed Sensing MRI (CS-MRI) has provided theoretical foundations upon\nwhich the time-consuming MRI acquisition process can be accelerated. However,\nit primarily relies on iterative numerical solvers which still hinders their\nadaptation in time-critical applications. In addition, recent advances in deep\nneural networks have shown their potential in computer vision and image\nprocessing, but their adaptation to MRI reconstruction is still in an early\nstage. In this paper, we propose a novel deep learning-based generative\nadversarial model, RefineGAN, for fast and accurate CS-MRI reconstruction. The\nproposed model is a variant of fully-residual convolutional autoencoder and\ngenerative adversarial networks (GANs), specifically designed for CS-MRI\nformulation; it employs deeper generator and discriminator networks with cyclic\ndata consistency loss for faithful interpolation in the given under-sampled\nk-space data. In addition, our solution leverages a chained network to further\nenhance the reconstruction quality. RefineGAN is fast and accurate -- the\nreconstruction process is extremely rapid, as low as tens of milliseconds for\nreconstruction of a 256x256 image, because it is one-way deployment on a\nfeed-forward network, and the image quality is superior even for extremely low\nsampling rate (as low as 10%) due to the data-driven nature of the method. We\ndemonstrate that RefineGAN outperforms the state-of-the-art CS-MRI methods by a\nlarge margin in terms of both running time and image quality via evaluation\nusing several open-source MRI databases.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 18:08:14 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 15:39:43 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Quan", "Tran Minh", ""], ["Nguyen-Duc", "Thanh", ""], ["Jeong", "Won-Ki", ""]]}, {"id": "1709.00786", "submitter": "Daisuke Komura", "authors": "Daisuke Komura and Shumpei Ishikawa", "title": "Machine learning methods for histopathological image analysis", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.csbj.2018.01.001", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abundant accumulation of digital histopathological images has led to the\nincreased demand for their analysis, such as computer-aided diagnosis using\nmachine learning techniques. However, digital pathological images and related\ntasks have some issues to be considered. In this mini-review, we introduce the\napplication of digital pathological image analysis using machine learning\nalgorithms, address some problems specific to such analysis, and propose\npossible solutions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 02:13:15 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 01:35:37 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Komura", "Daisuke", ""], ["Ishikawa", "Shumpei", ""]]}, {"id": "1709.00799", "submitter": "Hongming Li", "authors": "Hongming Li, Yong Fan", "title": "Non-rigid image registration using fully convolutional networks with\n  deep self-supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel non-rigid image registration algorithm that is built upon\nfully convolutional networks (FCNs) to optimize and learn spatial\ntransformations between pairs of images to be registered. Different from most\nexisting deep learning based image registration methods that learn spatial\ntransformations from training data with known corresponding spatial\ntransformations, our method directly estimates spatial transformations between\npairs of images by maximizing an image-wise similarity metric between fixed and\ndeformed moving images, similar to conventional image registration algorithms.\nAt the same time, our method also learns FCNs for encoding the spatial\ntransformations at the same spatial resolution of images to be registered,\nrather than learning coarse-grained spatial transformation information. The\nimage registration is implemented in a multi-resolution image registration\nframework to jointly optimize and learn spatial transformations and FCNs at\ndifferent resolutions with deep self-supervision through typical feedforward\nand backpropagation computation. Since our method simultaneously optimizes and\nlearns spatial transformations for the image registration, our method can be\ndirectly used to register a pair of images, and the registration of a set of\nimages is also a training procedure for FCNs so that the trained FCNs can be\ndirectly adopted to register new images by feedforward computation of the\nlearned FCNs without any optimization. The proposed method has been evaluated\nfor registering 3D structural brain magnetic resonance (MR) images and obtained\nbetter performance than state-of-the-art image registration algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 03:22:20 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Li", "Hongming", ""], ["Fan", "Yong", ""]]}, {"id": "1709.00835", "submitter": "Kang Zhu", "authors": "Kang Zhu, Yujia Xue, Qiang Fu, Sing Bing Kang, Xilin Chen, and Jingyi\n  Yu", "title": "Hyperspectral Light Field Stereo Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe how scene depth can be extracted using a\nhyperspectral light field capture (H-LF) system. Our H-LF system consists of a\n5 x 6 array of cameras, with each camera sampling a different narrow band in\nthe visible spectrum. There are two parts to extracting scene depth. The first\npart is our novel cross-spectral pairwise matching technique, which involves a\nnew spectral-invariant feature descriptor and its companion matching metric we\ncall bidirectional weighted normalized cross correlation (BWNCC). The second\npart, namely, H-LF stereo matching, uses a combination of spectral-dependent\ncorrespondence and defocus cues that rely on BWNCC. These two new cost terms\nare integrated into a Markov Random Field (MRF) for disparity estimation.\nExperiments on synthetic and real H-LF data show that our approach can produce\nhigh-quality disparity maps. We also show that these results can be used to\nproduce the complete plenoptic cube in addition to synthesizing all-focus and\ndefocused color images under different sensor spectral responses.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 06:36:55 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zhu", "Kang", ""], ["Xue", "Yujia", ""], ["Fu", "Qiang", ""], ["Kang", "Sing Bing", ""], ["Chen", "Xilin", ""], ["Yu", "Jingyi", ""]]}, {"id": "1709.00846", "submitter": "Alexander Wendel", "authors": "Alexander Wendel and James Underwood", "title": "Extrinsic Parameter Calibration for Line Scanning Cameras on Ground\n  Vehicles with Navigation Systems Using a Calibration Pattern", "comments": "Published in MDPI Sensors, 30 October 2017", "journal-ref": "Sensors 2017, 17, 2491", "doi": "10.3390/s17112491", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Line scanning cameras, which capture only a single line of pixels, have been\nincreasingly used in ground based mobile or robotic platforms. In applications\nwhere it is advantageous to directly georeference the camera data to world\ncoordinates, an accurate estimate of the camera's 6D pose is required. This\npaper focuses on the common case where a mobile platform is equipped with a\nrigidly mounted line scanning camera, whose pose is unknown, and a navigation\nsystem providing vehicle body pose estimates. We propose a novel method that\nestimates the camera's pose relative to the navigation system. The approach\ninvolves imaging and manually labelling a calibration pattern with distinctly\nidentifiable points, triangulating these points from camera and navigation\nsystem data and reprojecting them in order to compute a likelihood, which is\nmaximised to estimate the 6D camera pose. Additionally, a Markov Chain Monte\nCarlo (MCMC) algorithm is used to estimate the uncertainty of the offset.\nTested on two different platforms, the method was able to estimate the pose to\nwithin 0.06 m / 1.05$^{\\circ}$ and 0.18 m / 2.39$^{\\circ}$. We also propose\nseveral approaches to displaying and interpreting the 6D results in a human\nreadable way.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 07:46:43 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 00:56:14 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 06:26:47 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Wendel", "Alexander", ""], ["Underwood", "James", ""]]}, {"id": "1709.00849", "submitter": "Manik Goyal", "authors": "Manik Goyal, Param Rajpura, Hristo Bojinov, Ravi Hegde", "title": "Dataset Augmentation with Synthetic Images Improves Semantic\n  Segmentation", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": "10.1007/978-981-13-0020-2_31", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Deep Convolutional Neural Networks trained with strong pixel-level\nannotations have significantly pushed the performance in semantic segmentation,\nannotation efforts required for the creation of training data remains a\nroadblock for further improvements. We show that augmentation of the weakly\nannotated training dataset with synthetic images minimizes both the annotation\nefforts and also the cost of capturing images with sufficient variety.\nEvaluation on the PASCAL 2012 validation dataset shows an increase in mean IOU\nfrom 52.80% to 55.47% by adding just 100 synthetic images per object class. Our\napproach is thus a promising solution to the problems of annotation and dataset\ncollection.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 07:58:59 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 11:39:33 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 11:19:09 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Goyal", "Manik", ""], ["Rajpura", "Param", ""], ["Bojinov", "Hristo", ""], ["Hegde", "Ravi", ""]]}, {"id": "1709.00907", "submitter": "Burhan Gulbahar", "authors": "Burhan Gulbahar and Gorkem Memisoglu", "title": "CSSTag: Optical Nanoscale Radar and Particle Tracking for In-Body and\n  Microfluidic Systems with Vibrating Graphene and Resonance Energy Transfer", "comments": "13 double column pages, 9 figures, 1 table", "journal-ref": "IEEE Transactions on NanoBioscience, vol. 16, no. 8, pp. 905--916,\n  2017", "doi": "10.1109/TNB.2017.2785226", "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single particle tracking systems monitor cellular processes with great\naccuracy in nano-biological systems. The emissions of the fluorescent molecules\nare detected with cameras or photodetectors. However, state-of-the-art imaging\nsystems have challenges in the detection capability, collection and analysis of\nimaging data, penetration depth and complicated set-ups. In this article, a\n\\textit{signaling based nanoscale acousto-optic radar and microfluidic particle\ntracking system} is proposed based on the theoretical design providing\nnanoscale optical modulator with vibrating F{\\\"{o}}rster resonance energy\ntransfer (VFRET) and vibrating CdSe/ZnS quantum dots (QDs) on graphene\nresonators. The modulator structure combines the significant advantages of\ngraphene membranes having wideband resonance frequencies with QDs having broad\nabsorption spectrum and tunable properties. The solution denoted by chirp\nspread spectrum (CSS) Tag (\\textit{CSSTag}) utilizes classical radar target\ntracking approaches in nanoscale environments based on the capability to\ngenerate CSS sequences to identify different bio-particles. Numerical and\nMonte-Carlo simulations are realized showing the significant performance for\nmultiple particle tracking (MPT) with a modulator of $10 \\, \\mu$m $\\times$ $10\n\\, \\mu$m $\\times$ $10 \\, \\mu$m dimension and several picograms of weight,\nsignal to noise ratio (SNR) in the range $-7$ dB to $10$ dB and high speed\ntracking capability for microfluidic and in-body environments.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 11:59:58 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Gulbahar", "Burhan", ""], ["Memisoglu", "Gorkem", ""]]}, {"id": "1709.00930", "submitter": "Yuchao Dai Dr.", "authors": "Yiran Zhong, Yuchao Dai, Hongdong Li", "title": "Self-Supervised Learning for Stereo Matching with Self-Improving Ability", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exiting deep-learning based dense stereo matching methods often rely on\nground-truth disparity maps as the training signals, which are however not\nalways available in many situations. In this paper, we design a simple\nconvolutional neural network architecture that is able to learn to compute\ndense disparity maps directly from the stereo inputs. Training is performed in\nan end-to-end fashion without the need of ground-truth disparity maps. The idea\nis to use image warping error (instead of disparity-map residuals) as the loss\nfunction to drive the learning process, aiming to find a depth-map that\nminimizes the warping error. While this is a simple concept well-known in\nstereo matching, to make it work in a deep-learning framework, many non-trivial\nchallenges must be overcome, and in this work we provide effective solutions.\nOur network is self-adaptive to different unseen imageries as well as to\ndifferent camera settings. Experiments on KITTI and Middlebury stereo benchmark\ndatasets show that our method outperforms many state-of-the-art stereo matching\nmethods with a margin, and at the same time significantly faster.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 12:56:18 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zhong", "Yiran", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1709.00938", "submitter": "Mario Valerio Giuffrida", "authors": "Mario Valerio Giuffrida, Hanno Scharr, Sotirios A Tsaftaris", "title": "ARIGAN: Synthetic Arabidopsis Plants using Generative Adversarial\n  Network", "comments": "8 pages, 6 figures, 1 table, ICCV CVPPP Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an increasing interest in image-based plant\nphenotyping, applying state-of-the-art machine learning approaches to tackle\nchallenging problems, such as leaf segmentation (a multi-instance problem) and\ncounting. Most of these algorithms need labelled data to learn a model for the\ntask at hand. Despite the recent release of a few plant phenotyping datasets,\nlarge annotated plant image datasets for the purpose of training deep learning\nalgorithms are lacking. One common approach to alleviate the lack of training\ndata is dataset augmentation. Herein, we propose an alternative solution to\ndataset augmentation for plant phenotyping, creating artificial images of\nplants using generative neural networks. We propose the Arabidopsis Rosette\nImage Generator (through) Adversarial Network: a deep convolutional network\nthat is able to generate synthetic rosette-shaped plants, inspired by DCGAN (a\nrecent adversarial network model using convolutional layers). Specifically, we\ntrained the network using A1, A2, and A4 of the CVPPP 2017 LCC dataset,\ncontaining Arabidopsis Thaliana plants. We show that our model is able to\ngenerate realistic 128x128 colour images of plants. We train our network\nconditioning on leaf count, such that it is possible to generate plants with a\ngiven number of leaves suitable, among others, for training regression based\nmodels. We propose a new Ax dataset of artificial plants images, obtained by\nour ARIGAN. We evaluate this new dataset using a state-of-the-art leaf counting\nalgorithm, showing that the testing error is reduced when Ax is used as part of\nthe training data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 13:12:58 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Giuffrida", "Mario Valerio", ""], ["Scharr", "Hanno", ""], ["Tsaftaris", "Sotirios A", ""]]}, {"id": "1709.00962", "submitter": "Guillaume Heusch", "authors": "Guillaume Heusch, Andr\\'e Anjos, S\\'ebastien Marcel", "title": "A Reproducible Study on Remote Heart Rate Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of reproducible research in remote\nphotoplethysmography (rPPG). Most of the work published in this domain is\nassessed on privately-owned databases, making it difficult to evaluate proposed\nalgorithms in a standard and principled manner. As a consequence, we present a\nnew, publicly available database containing a relatively large number of\nsubjects recorded under two different lighting conditions. Also, three\nstate-of-the-art rPPG algorithms from the literature were selected, implemented\nand released as open source free software. After a thorough, unbiased\nexperimental evaluation in various settings, it is shown that none of the\nselected algorithms is precise enough to be used in a real-world scenario.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 13:55:27 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Heusch", "Guillaume", ""], ["Anjos", "Andr\u00e9", ""], ["Marcel", "S\u00e9bastien", ""]]}, {"id": "1709.00965", "submitter": "Jens Grubert", "authors": "Daniel Schneider and Jens Grubert", "title": "Feasibility of Corneal Imaging for Handheld Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones are a popular device class for mobile Augmented Reality but\nsuffer from a limited input space. Around-device interaction techniques aim at\nextending this input space using various sensing modalities. In this paper we\npresent our work towards extending the input area of mobile devices using\nfront-facing device-centered cameras that capture reflections in the cornea. As\ncurrent generation mobile devices lack high resolution front-facing cameras, we\nstudy the feasibility of around-device interaction using corneal reflective\nimaging based on a high resolution camera. We present a workflow, a technical\nprototype and a feasibility evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 14:00:59 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Schneider", "Daniel", ""], ["Grubert", "Jens", ""]]}, {"id": "1709.00966", "submitter": "Jens Grubert", "authors": "Daniel Schneider and Jens Grubert", "title": "Towards Around-Device Interaction using Corneal Imaging", "comments": null, "journal-ref": null, "doi": "10.1145/3132272.3134127", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Around-device interaction techniques aim at extending the input space using\nvarious sensing modalities on mobile and wearable devices. In this paper, we\npresent our work towards extending the input area of mobile devices using\nfront-facing device-centered cameras that capture reflections in the human eye.\nAs current generation mobile devices lack high resolution front-facing cameras\nwe study the feasibility of around-device interaction using corneal reflective\nimaging based on a high resolution camera. We present a workflow, a technical\nprototype and an evaluation, including a migration path from high resolution to\nlow resolution imagers. Our study indicates, that under optimal conditions a\nspatial sensing resolution of 5 cm in the vicinity of a mobile phone is\npossible.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 14:01:45 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Schneider", "Daniel", ""], ["Grubert", "Jens", ""]]}, {"id": "1709.01041", "submitter": "Marc Masana Castrillo", "authors": "Marc Masana, Joost van de Weijer, Luis Herranz, Andrew D. Bagdanov,\n  Jose M Alvarez", "title": "Domain-adaptive deep network compression", "comments": "Accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks trained on large datasets can be easily transferred to\nnew domains with far fewer labeled examples by a process called fine-tuning.\nThis has the advantage that representations learned in the large source domain\ncan be exploited on smaller target domains. However, networks designed to be\noptimal for the source task are often prohibitively large for the target task.\nIn this work we address the compression of networks after domain transfer.\n  We focus on compression algorithms based on low-rank matrix decomposition.\nExisting methods base compression solely on learned network weights and ignore\nthe statistics of network activations. We show that domain transfer leads to\nlarge shifts in network activations and that it is desirable to take this into\naccount when compressing. We demonstrate that considering activation statistics\nwhen compressing weights leads to a rank-constrained regression problem with a\nclosed-form solution. Because our method takes into account the target domain,\nit can more optimally remove the redundancy in the weights. Experiments show\nthat our Domain Adaptive Low Rank (DALR) method significantly outperforms\nexisting low-rank compression techniques. With our approach, the fc6 layer of\nVGG19 can be compressed more than 4x more than using truncated SVD alone --\nwith only a minor or no loss in accuracy. When applied to domain-transferred\nnetworks it allows for compression down to only 5-20% of the original number of\nparameters with only a minor drop in performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 16:52:46 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 10:26:38 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Masana", "Marc", ""], ["van de Weijer", "Joost", ""], ["Herranz", "Luis", ""], ["Bagdanov", "Andrew D.", ""], ["Alvarez", "Jose M", ""]]}, {"id": "1709.01057", "submitter": "Aabhas Majumdar", "authors": "Aabhas Majumdar, Raghav Mehta and Jayanthi Sivaswamy", "title": "To Learn or Not to Learn Features for Deformable Registration?", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature-based registration has been popular with a variety of features\nranging from voxel intensity to Self-Similarity Context (SSC). In this paper,\nwe examine the question on how features learnt using various Deep Learning (DL)\nframeworks can be used for deformable registration and whether this feature\nlearning is necessary or not. We investigate the use of features learned by\ndifferent DL methods in the current state-of-the-art discrete registration\nframework and analyze its performance on 2 publicly available datasets. We draw\ninsights into the type of DL framework useful for feature learning and the\nimpact, if any, of the complexity of different DL models and brain parcellation\nmethods on the performance of discrete registration. Our results indicate that\nthe registration performance with DL features and SSC are comparable and stable\nacross datasets whereas this does not hold for low level features.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 17:54:15 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 08:19:06 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 03:13:48 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Majumdar", "Aabhas", ""], ["Mehta", "Raghav", ""], ["Sivaswamy", "Jayanthi", ""]]}, {"id": "1709.01062", "submitter": "Mark Tygert", "authors": "Cinna Wu, Mark Tygert, and Yann LeCun", "title": "A hierarchical loss and its problems when classifying non-hierarchically", "comments": "19 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failing to distinguish between a sheepdog and a skyscraper should be worse\nand penalized more than failing to distinguish between a sheepdog and a poodle;\nafter all, sheepdogs and poodles are both breeds of dogs. However, existing\nmetrics of failure (so-called \"loss\" or \"win\") used in textual or visual\nclassification/recognition via neural networks seldom leverage a-priori\ninformation, such as a sheepdog being more similar to a poodle than to a\nskyscraper. We define a metric that, inter alia, can penalize failure to\ndistinguish between a sheepdog and a skyscraper more than failure to\ndistinguish between a sheepdog and a poodle. Unlike previously employed\npossibilities, this metric is based on an ultrametric tree associated with any\ngiven tree organization into a semantically meaningful hierarchy of a\nclassifier's classes. An ultrametric tree is a tree with a so-called\nultrametric distance metric such that all leaves are at the same distance from\nthe root. Unfortunately, extensive numerical experiments indicate that the\nstandard practice of training neural networks via stochastic gradient descent\nwith random starting points often drives down the hierarchical loss nearly as\nmuch when minimizing the standard cross-entropy loss as when trying to minimize\nthe hierarchical loss directly. Thus, this hierarchical loss is unreliable as\nan objective for plain, randomly started stochastic gradient descent to\nminimize; the main value of the hierarchical loss may be merely as a meaningful\nmetric of success of a classifier.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 23:46:59 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 20:38:32 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wu", "Cinna", ""], ["Tygert", "Mark", ""], ["LeCun", "Yann", ""]]}, {"id": "1709.01077", "submitter": "Guy Rosman", "authors": "Guy Rosman, John W. Fisher III and Daniela Rus", "title": "A Nonparametric Model for Multimodal Collaborative Activities\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ego-centric data streams provide a unique opportunity to reason about joint\nbehavior by pooling data across individuals. This is especially evident in\nurban environments teeming with human activities, but which suffer from\nincomplete and noisy data. Collaborative human activities exhibit common\nspatial, temporal, and visual characteristics facilitating inference across\nindividuals from multiple sensory modalities as we explore in this paper from\nthe perspective of meetings. We propose a new Bayesian nonparametric model that\nenables us to efficiently pool video and GPS data towards collaborative\nactivities analysis from multiple individuals. We demonstrate the utility of\nthis model for inference tasks such as activity detection, classification, and\nsummarization. We further demonstrate how spatio-temporal structure embedded in\nour model enables better understanding of partial and noisy observations such\nas localization and face detections based on social interactions. We show\nresults on both synthetic experiments and a new dataset of egocentric video and\nnoisy GPS data from multiple individuals.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 17:14:47 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Rosman", "Guy", ""], ["Fisher", "John W.", "III"], ["Rus", "Daniela", ""]]}, {"id": "1709.01118", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, Luc\n  Van Gool", "title": "WESPE: Weakly Supervised Photo Enhancer for Digital Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-end and compact mobile cameras demonstrate limited photo quality mainly\ndue to space, hardware and budget constraints. In this work, we propose a deep\nlearning solution that translates photos taken by cameras with limited\ncapabilities into DSLR-quality photos automatically. We tackle this problem by\nintroducing a weakly supervised photo enhancer (WESPE) - a novel image-to-image\nGenerative Adversarial Network-based architecture. The proposed model is\ntrained by under weak supervision: unlike previous works, there is no need for\nstrong supervision in the form of a large annotated dataset of aligned\noriginal/enhanced photo pairs. The sole requirement is two distinct datasets:\none from the source camera, and one composed of arbitrary high-quality images\nthat can be generally crawled from the Internet - the visual content they\nexhibit may be unrelated. Hence, our solution is repeatable for any camera:\ncollecting the data and training can be achieved in a couple of hours. In this\nwork, we emphasize on extensive evaluation of obtained results. Besides\nstandard objective metrics and subjective user study, we train a virtual rater\nin the form of a separate CNN that mimics human raters on Flickr data and use\nthis network to get reference scores for both original and enhanced photos. Our\nexperiments on the DPED, KITTI and Cityscapes datasets as well as pictures from\nseveral generations of smartphones demonstrate that WESPE produces comparable\nor improved qualitative results with state-of-the-art strongly supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 18:59:06 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 18:51:55 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ignatov", "Andrey", ""], ["Kobyshev", "Nikolay", ""], ["Timofte", "Radu", ""], ["Vanhoey", "Kenneth", ""], ["Van Gool", "Luc", ""]]}, {"id": "1709.01134", "submitter": "Asit Mishra", "authors": "Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook and Debbie Marr", "title": "WRPN: Wide Reduced-Precision Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For computer vision applications, prior works have shown the efficacy of\nreducing numeric precision of model parameters (network weights) in deep neural\nnetworks. Activation maps, however, occupy a large memory footprint during both\nthe training and inference step when using mini-batches of inputs. One way to\nreduce this large memory footprint is to reduce the precision of activations.\nHowever, past works have shown that reducing the precision of activations hurts\nmodel accuracy. We study schemes to train networks from scratch using\nreduced-precision activations without hurting accuracy. We reduce the precision\nof activation maps (along with model parameters) and increase the number of\nfilter maps in a layer, and find that this scheme matches or surpasses the\naccuracy of the baseline full-precision network. As a result, one can\nsignificantly improve the execution efficiency (e.g. reduce dynamic memory\nfootprint, memory bandwidth and computational energy) and speed up the training\nand inference process with appropriate hardware support. We call our scheme\nWRPN - wide reduced-precision networks. We report results and show that WRPN\nscheme is better than previously reported accuracies on ILSVRC-12 dataset while\nbeing computationally less expensive compared to previously reported\nreduced-precision networks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 19:56:48 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Mishra", "Asit", ""], ["Nurvitadhi", "Eriko", ""], ["Cook", "Jeffrey J", ""], ["Marr", "Debbie", ""]]}, {"id": "1709.01140", "submitter": "Yizhe Zhu", "authors": "Yizhe Zhu, Ahmed Elgammal", "title": "A Multilayer-Based Framework for Online Background Subtraction with\n  Freely Moving Cameras", "comments": "Accepted by ICCV'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponentially increasing use of moving platforms for video capture\nintroduces the urgent need to develop the general background subtraction\nalgorithms with the capability to deal with the moving background. In this\npaper, we propose a multilayer-based framework for online background\nsubtraction for videos captured by moving cameras. Unlike the previous\ntreatments of the problem, the proposed method is not restricted to binary\nsegmentation of background and foreground, but formulates it as a multi-label\nsegmentation problem by modeling multiple foreground objects in different\nlayers when they appear simultaneously in the scene. We assign an independent\nprocessing layer to each foreground object, as well as the background, where\nboth motion and appearance models are estimated, and a probability map is\ninferred using a Bayesian filtering framework. Finally, Multi-label Graph-cut\non Markov Random Field is employed to perform pixel-wise labeling. Extensive\nevaluation results show that the proposed method outperforms state-of-the-art\nmethods on challenging video sequences.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 20:06:51 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Zhu", "Yizhe", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1709.01148", "submitter": "Yizhe Zhu", "authors": "Mohamed Elhoseiny, Yizhe Zhu, Han Zhang, Ahmed Elgammal", "title": "Link the head to the \"beak\": Zero Shot Learning from Noisy Text\n  Description at Part Precision", "comments": "Accepted by CVPR'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study learning visual classifiers from unstructured text\ndescriptions at part precision with no training images. We propose a learning\nframework that is able to connect text terms to its relevant parts and suppress\nconnections to non-visual text terms without any part-text annotations. For\ninstance, this learning process enables terms like \"beak\" to be sparsely linked\nto the visual representation of parts like head, while reduces the effect of\nnon-visual terms like \"migrate\" on classifier prediction. Images are encoded by\na part-based CNN that detect bird parts and learn part-specific representation.\nPart-based visual classifiers are predicted from text descriptions of unseen\nvisual classifiers to facilitate classification without training images (also\nknown as zero-shot recognition). We performed our experiments on CUBirds 2011\ndataset and improves the state-of-the-art text-based zero-shot recognition\nresults from 34.7\\% to 43.6\\%. We also created large scale benchmarks on North\nAmerican Bird Images augmented with text descriptions, where we also show that\nour approach outperforms existing methods. Our code, data, and models are\npublically available.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 20:36:14 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Zhu", "Yizhe", ""], ["Zhang", "Han", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1709.01182", "submitter": "Carlos Thomaz", "authors": "Carlos E. Thomaz, Vagner Amaral, Gilson A. Giraldi, Duncan F. Gillies,\n  and Daniel Rueckert", "title": "Is human face processing a feature- or pattern-based task? Evidence\n  using a unified computational method driven by eye movements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on human face processing using eye movements has provided evidence\nthat we recognize face images successfully focusing our visual attention on a\nfew inner facial regions, mainly on the eyes, nose and mouth. To understand how\nwe accomplish this process of coding high-dimensional faces so efficiently,\nthis paper proposes and implements a multivariate extraction method that\ncombines face images variance with human spatial attention maps modeled as\nfeature- and pattern-based information sources. It is based on a unified\nmultidimensional representation of the well-known face-space concept. The\nspatial attention maps are summary statistics of the eye-tracking fixations of\na number of participants and trials to frontal and well-framed face images\nduring separate gender and facial expression recognition tasks. Our\nexperimental results carried out on publicly available face databases have\nindicated that we might emulate the human extraction system as a pattern-based\ncomputational method rather than a feature-based one to properly explain the\nproficiency of the human system in recognizing visual face information.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 22:29:56 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Thomaz", "Carlos E.", ""], ["Amaral", "Vagner", ""], ["Giraldi", "Gilson A.", ""], ["Gillies", "Duncan F.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1709.01212", "submitter": "Yang Wang", "authors": "Yang Wang, Lin Wu", "title": "Multi-View Spectral Clustering via Structured Low-Rank Matrix\n  Factorization", "comments": "Accepted to appear at IEEE Trans on Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2777489", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view data clustering attracts more attention than their single view\ncounterparts due to the fact that leveraging multiple independent and\ncomplementary information from multi-view feature spaces outperforms the single\none. Multi-view Spectral Clustering aims at yielding the data partition\nagreement over their local manifold structures by seeking\neigenvalue-eigenvector decompositions. However, as we observed, such classical\nparadigm still suffers from (1) overlooking the flexible local manifold\nstructure, caused by (2) enforcing the low-rank data correlation agreement\namong all views; worse still, (3) LRR is not intuitively flexible to capture\nthe latent data clustering structures. In this paper, we present the structured\nLRR by factorizing into the latent low-dimensional data-cluster\nrepresentations, which characterize the data clustering structure for each\nview. Upon such representation, (b) the laplacian regularizer is imposed to be\ncapable of preserving the flexible local manifold structure for each view. (c)\nWe present an iterative multi-view agreement strategy by minimizing the\ndivergence objective among all factorized latent data-cluster representations\nduring each iteration of optimization process, where such latent representation\nfrom each view serves to regulate those from other views, such intuitive\nprocess iteratively coordinates all views to be agreeable. (d) We remark that\nsuch data-cluster representation can flexibly encode the data clustering\nstructure from any view with adaptive input cluster number. To this end, (e) a\nnovel non-convex objective function is proposed via the efficient alternating\nminimization strategy. The complexity analysis are also presented. The\nextensive experiments conducted against the real-world multi-view datasets\ndemonstrate the superiority over state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 01:54:43 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 12:33:23 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 00:33:05 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Wang", "Yang", ""], ["Wu", "Lin", ""]]}, {"id": "1709.01215", "submitter": "Chunyuan Li", "authors": "Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo\n  Henao, Lawrence Carin", "title": "ALICE: Towards Understanding Adversarial Learning for Joint Distribution\n  Matching", "comments": "NIPS 2017 (22 pages); short version (9 pages):\n  http://people.duke.edu/~cl319/doc/papers/nips_2017_alice.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the non-identifiability issues associated with bidirectional\nadversarial training for joint distribution matching. Within a framework of\nconditional entropy, we propose both adversarial and non-adversarial approaches\nto learn desirable matched joint distributions for unsupervised and supervised\ntasks. We unify a broad family of adversarial models as joint distribution\nmatching problems. Our approach stabilizes learning of unsupervised\nbidirectional adversarial learning methods. Further, we introduce an extension\nfor semi-supervised learning tasks. Theoretical results are validated in\nsynthetic data and real-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 02:18:06 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 03:58:52 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Li", "Chunyuan", ""], ["Liu", "Hao", ""], ["Chen", "Changyou", ""], ["Pu", "Yunchen", ""], ["Chen", "Liqun", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1709.01220", "submitter": "Zhiwu Lu", "authors": "Yulei Niu, Zhiwu Lu, Ji-Rong Wen, Tao Xiang, and Shih-Fu Chang", "title": "Multi-Modal Multi-Scale Deep Learning for Large-Scale Image Annotation", "comments": "Submited to IEEE TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image annotation aims to annotate a given image with a variable number of\nclass labels corresponding to diverse visual concepts. In this paper, we\naddress two main issues in large-scale image annotation: 1) how to learn a rich\nfeature representation suitable for predicting a diverse set of visual concepts\nranging from object, scene to abstract concept; 2) how to annotate an image\nwith the optimal number of class labels. To address the first issue, we propose\na novel multi-scale deep model for extracting rich and discriminative features\ncapable of representing a wide range of visual concepts. Specifically, a novel\ntwo-branch deep neural network architecture is proposed which comprises a very\ndeep main network branch and a companion feature fusion network branch designed\nfor fusing the multi-scale features computed from the main branch. The deep\nmodel is also made multi-modal by taking noisy user-provided tags as model\ninput to complement the image input. For tackling the second issue, we\nintroduce a label quantity prediction auxiliary task to the main label\nprediction task to explicitly estimate the optimal label number for a given\nimage. Extensive experiments are carried out on two large-scale image\nannotation benchmark datasets and the results show that our method\nsignificantly outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 02:50:45 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 01:35:38 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Niu", "Yulei", ""], ["Lu", "Zhiwu", ""], ["Wen", "Ji-Rong", ""], ["Xiang", "Tao", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1709.01237", "submitter": "Hariprasad Kannan", "authors": "Hariprasad Kannan, Nikos Komodakis, Nikos Paragios", "title": "Newton-type Methods for Inference in Higher-Order Markov Random Fields", "comments": "10 pages, 3 figures, 3 tables, CVPR 2017", "journal-ref": "Poster at IEEE International Conference on Computer Vision and\n  Pattern Recognition 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear programming relaxations are central to {\\sc map} inference in discrete\nMarkov Random Fields. The ability to properly solve the Lagrangian dual is a\ncritical component of such methods. In this paper, we study the benefit of\nusing Newton-type methods to solve the Lagrangian dual of a smooth version of\nthe problem. We investigate their ability to achieve superior convergence\nbehavior and to better handle the ill-conditioned nature of the formulation, as\ncompared to first order methods. We show that it is indeed possible to\nefficiently apply a trust region Newton method for a broad range of {\\sc map}\ninference problems. In this paper we propose a provably convergent and\nefficient framework that includes (i) excellent compromise between\ncomputational complexity and precision concerning the Hessian matrix\nconstruction, (ii) a damping strategy that aids efficient optimization, (iii) a\ntruncation strategy coupled with a generic pre-conditioner for Conjugate\nGradients, (iv) efficient sum-product computation for sparse clique potentials.\nResults for higher-order Markov Random Fields demonstrate the potential of this\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 04:55:47 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Kannan", "Hariprasad", ""], ["Komodakis", "Nikos", ""], ["Paragios", "Nikos", ""]]}, {"id": "1709.01295", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, Isht Dwivedi, Abhijat Biswas, Sahil\n  Manocha, R. Venkatesh Babu", "title": "SketchParse : Towards Rich Descriptions for Poorly Drawn Sketches using\n  Multi-Task Hierarchical Deep Networks", "comments": "A shorter version of this submission was accepted at ACM Multimedia\n  (ACMMM) 2017. Code, annotated datasets and pre-trained models available at\n  https://github.com/val-iisc/sketch-parse", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to semantically interpret hand-drawn line sketches, although very\nchallenging, can pave way for novel applications in multimedia. We propose\nSketchParse, the first deep-network architecture for fully automatic parsing of\nfreehand object sketches. SketchParse is configured as a two-level fully\nconvolutional network. The first level contains shared layers common to all\nobject categories. The second level contains a number of expert sub-networks.\nEach expert specializes in parsing sketches from object categories which\ncontain structurally similar parts. Effectively, the two-level configuration\nenables our architecture to scale up efficiently as additional categories are\nadded. We introduce a router layer which (i) relays sketch features from shared\nlayers to the correct expert (ii) eliminates the need to manually specify\nobject category during inference. To bypass laborious part-level annotation, we\nsketchify photos from semantic object-part image datasets and use them for\ntraining. Our architecture also incorporates object pose prediction as a novel\nauxiliary task which boosts overall performance while providing supplementary\ninformation regarding the sketch. We demonstrate SketchParse's abilities (i) on\ntwo challenging large-scale sketch datasets (ii) in parsing unseen,\nsemantically related object categories (iii) in improving fine-grained\nsketch-based image retrieval. As a novel application, we also outline how\nSketchParse's output can be used to generate caption-style descriptions for\nhand-drawn sketches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 09:10:59 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Dwivedi", "Isht", ""], ["Biswas", "Abhijat", ""], ["Manocha", "Sahil", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1709.01305", "submitter": "Jianfeng Dong", "authors": "Jianfeng Dong, Xirong Li, Duanqing Xu", "title": "Cross-Media Similarity Evaluation for Web Image Retrieval in the Wild", "comments": "14 pages, 10 figures, accepted by IEEE Transactions on Multimedia\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to retrieve unlabeled images by textual queries, cross-media\nsimilarity computation is a key ingredient. Although novel methods are\ncontinuously introduced, little has been done to evaluate these methods\ntogether with large-scale query log analysis. Consequently, how far have these\nmethods brought us in answering real-user queries is unclear. Given baseline\nmethods that compute cross-media similarity using relatively simple text/image\nmatching, how much progress have advanced models made is also unclear. This\npaper takes a pragmatic approach to answering the two questions. Queries are\nautomatically categorized according to the proposed query visualness measure,\nand later connected to the evaluation of multiple cross-media similarity models\non three test sets. Such a connection reveals that the success of the\nstate-of-the-art is mainly attributed to their good performance on\nvisual-oriented queries, while these queries account for only a small part of\nreal-user queries. To quantify the current progress, we propose a simple\ntext2image method, representing a novel test query by a set of images selected\nfrom large-scale query log. Consequently, computing cross-media similarity\nbetween the test query and a given image boils down to comparing the visual\nsimilarity between the given image and the selected images. Image retrieval\nexperiments on the challenging Clickture dataset show that the proposed\ntext2image compares favorably to recent deep learning based alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 09:38:32 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 06:09:12 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Dong", "Jianfeng", ""], ["Li", "Xirong", ""], ["Xu", "Duanqing", ""]]}, {"id": "1709.01353", "submitter": "Noa Garcia", "authors": "Noa Garcia and George Vogiatzis", "title": "Learning Non-Metric Visual Similarity for Image Retrieval", "comments": "Image and Vision Computing (2019)", "journal-ref": null, "doi": "10.1016/j.imavis.2019.01.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring visual similarity between two or more instances within a data\ndistribution is a fundamental task in image retrieval. Theoretically,\nnon-metric distances are able to generate a more complex and accurate\nsimilarity model than metric distances, provided that the non-linear data\ndistribution is precisely captured by the system. In this work, we explore\nneural networks models for learning a non-metric similarity function for\ninstance search. We argue that non-metric similarity functions based on neural\nnetworks can build a better model of human visual perception than standard\nmetric distances. As our proposed similarity function is differentiable, we\nexplore a real end-to-end trainable approach for image retrieval, i.e. we learn\nthe weights from the input image pixels to the final similarity score.\nExperimental evaluation shows that non-metric similarity networks are able to\nlearn visual similarities between images and improve performance on top of\nstate-of-the-art image representations, boosting results in standard image\nretrieval datasets with respect standard metric distances.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 12:39:30 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 02:06:00 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Garcia", "Noa", ""], ["Vogiatzis", "George", ""]]}, {"id": "1709.01355", "submitter": "Fergal Cotter", "authors": "Fergal Cotter and Nick Kingsbury", "title": "Visualizing and Improving Scattering Networks", "comments": "To Appear in the 27th IEEE International Workshop on Machine Learning\n  For Signal Processing (MLSP) 2017. 6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scattering Transforms (or ScatterNets) introduced by Mallat are a promising\nstart into creating a well-defined feature extractor to use for pattern\nrecognition and image classification tasks. They are of particular interest due\nto their architectural similarity to Convolutional Neural Networks (CNNs),\nwhile requiring no parameter learning and still performing very well\n(particularly in constrained classification tasks).\n  In this paper we visualize what the deeper layers of a ScatterNet are\nsensitive to using a 'DeScatterNet'. We show that the higher orders of\nScatterNets are sensitive to complex, edge-like patterns (checker-boards and\nrippled edges). These complex patterns may be useful for texture\nclassification, but are quite dissimilar from the patterns visualized in second\nand third layers of Convolutional Neural Networks (CNNs) - the current state of\nthe art Image Classifiers. We propose that this may be the source of the\ncurrent gaps in performance between ScatterNets and CNNs (83% vs 93% on\nCIFAR-10 for ScatterNet+SVM vs ResNet). We then use these visualization tools\nto propose possible enhancements to the ScatterNet design, which show they have\nthe power to extract features more closely resembling CNNs, while still being\nwell-defined and having the invariance properties fundamental to ScatterNets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 12:41:05 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Cotter", "Fergal", ""], ["Kingsbury", "Nick", ""]]}, {"id": "1709.01357", "submitter": "Maryam Khanian", "authors": "Maryam Khanian, Ali Sharifi Boroujerdi, Michael Breu{\\ss}", "title": "Photometric stereo for strong specular highlights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photometric stereo (PS) is a fundamental technique in computer vision known\nto produce 3-D shape with high accuracy. The setting of PS is defined by using\nseveral input images of a static scene taken from one and the same camera\nposition but under varying illumination. The vast majority of studies in this\n3-D reconstruction method assume orthographic projection for the camera model.\nIn addition, they mainly consider the Lambertian reflectance model as the way\nthat light scatters at surfaces. So, providing reliable PS results from real\nworld objects still remains a challenging task. We address 3-D reconstruction\nby PS using a more realistic set of assumptions combining for the first time\nthe complete Blinn-Phong reflectance model and perspective projection. To this\nend, we will compare two different methods of incorporating the perspective\nprojection into our model. Experiments are performed on both synthetic and real\nworld images. Note that our real-world experiments do not benefit from\nlaboratory conditions. The results show the high potential of our method even\nfor complex real world applications such as medical endoscopy images which may\ninclude high amounts of specular highlights.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 12:44:32 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Khanian", "Maryam", ""], ["Boroujerdi", "Ali Sharifi", ""], ["Breu\u00df", "Michael", ""]]}, {"id": "1709.01362", "submitter": "Jianfeng Dong", "authors": "Jianfeng Dong, Xirong Li, Cees G. M. Snoek", "title": "Predicting Visual Features from Text for Image and Video Caption\n  Retrieval", "comments": "Accepted by Transaction on Multimedia. Code is available at\n  https://github.com/danieljf24/w2vv", "journal-ref": null, "doi": "10.1109/TMM.2018.2832602", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives to find amidst a set of sentences the one best describing\nthe content of a given image or video. Different from existing works, which\nrely on a joint subspace for their image and video caption retrieval, we\npropose to do so in a visual space exclusively. Apart from this conceptual\nnovelty, we contribute \\emph{Word2VisualVec}, a deep neural network\narchitecture that learns to predict a visual feature representation from\ntextual input. Example captions are encoded into a textual embedding based on\nmulti-scale sentence vectorization and further transferred into a deep visual\nfeature of choice via a simple multi-layer perceptron. We further generalize\nWord2VisualVec for video caption retrieval, by predicting from text both 3-D\nconvolutional neural network features as well as a visual-audio representation.\nExperiments on Flickr8k, Flickr30k, the Microsoft Video Description dataset and\nthe very recent NIST TrecVid challenge for video caption retrieval detail\nWord2VisualVec's properties, its benefit over textual embeddings, the potential\nfor multimodal query composition and its state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 12:55:44 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 12:32:19 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2018 12:01:20 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Dong", "Jianfeng", ""], ["Li", "Xirong", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1709.01421", "submitter": "Konstantin Sozykin", "authors": "Konstantin Sozykin, Stanislav Protasov, Adil Khan, Rasheed Hussain,\n  Jooyoung Lee", "title": "Multi-label Class-imbalanced Action Recognition in Hockey Videos via 3D\n  Convolutional Neural Networks", "comments": "Accepted to IEEE/ACIS SNPD 2018, 6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic analysis of the video is one of most complex problems in the fields\nof computer vision and machine learning. A significant part of this research\ndeals with (human) activity recognition (HAR) since humans, and the activities\nthat they perform, generate most of the video semantics. Video-based HAR has\napplications in various domains, but one of the most important and challenging\nis HAR in sports videos. Some of the major issues include high inter- and\nintra-class variations, large class imbalance, the presence of both group\nactions and single player actions, and recognizing simultaneous actions, i.e.,\nthe multi-label learning problem. Keeping in mind these challenges and the\nrecent success of CNNs in solving various computer vision problems, in this\nwork, we implement a 3D CNN based multi-label deep HAR system for multi-label\nclass-imbalanced action recognition in hockey videos. We test our system for\ntwo different scenarios: an ensemble of $k$ binary networks vs. a single\n$k$-output network, on a publicly available dataset. We also compare our\nresults with the system that was originally designed for the chosen dataset.\nExperimental results show that the proposed approach performs better than the\nexisting solution.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 14:44:20 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 15:23:28 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Sozykin", "Konstantin", ""], ["Protasov", "Stanislav", ""], ["Khan", "Adil", ""], ["Hussain", "Rasheed", ""], ["Lee", "Jooyoung", ""]]}, {"id": "1709.01424", "submitter": "Maedeh Aghaei", "authors": "Maedeh Aghaei, Mariella Dimiccoli, Cristian Canton Ferrer, Petia\n  Radeva", "title": "Towards social pattern characterization in egocentric photo-streams", "comments": "42 pages, 14 figures. Submitted to Elsevier, Computer Vision and\n  Image Understanding (Under Review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the increasingly popular trend of social interaction analysis in\negocentric vision, this manuscript presents a comprehensive study for automatic\nsocial pattern characterization of a wearable photo-camera user, by relying on\nthe visual analysis of egocentric photo-streams. The proposed framework\nconsists of three major steps. The first step is to detect social interactions\nof the user where the impact of several social signals on the task is explored.\nThe detected social events are inspected in the second step for categorization\ninto different social meetings. These two steps act at event-level where each\npotential social event is modeled as a multi-dimensional time-series, whose\ndimensions correspond to a set of relevant features for each task, and LSTM is\nemployed to classify the time-series. The last step of the framework is to\ncharacterize social patterns, which is essentially to infer the diversity and\nfrequency of the social relations of the user through discovery of recurrences\nof the same people across the whole set of social events of the user.\nExperimental evaluation over a dataset acquired by 9 users demonstrates\npromising results on the task of social pattern characterization from\negocentric photo-streams.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 14:50:00 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 16:02:18 GMT"}, {"version": "v3", "created": "Tue, 9 Jan 2018 11:14:53 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Aghaei", "Maedeh", ""], ["Dimiccoli", "Mariella", ""], ["Ferrer", "Cristian Canton", ""], ["Radeva", "Petia", ""]]}, {"id": "1709.01442", "submitter": "Yaojie Liu", "authors": "Yaojie Liu, Amin Jourabloo, William Ren, Xiaoming Liu", "title": "Dense Face Alignment", "comments": "To appear in ICCV 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment is a classic problem in the computer vision field. Previous\nworks mostly focus on sparse alignment with a limited number of facial landmark\npoints, i.e., facial landmark detection. In this paper, for the first time, we\naim at providing a very dense 3D alignment for large-pose face images. To\nachieve this, we train a CNN to estimate the 3D face shape, which not only\naligns limited facial landmarks but also fits face contours and SIFT feature\npoints. Moreover, we also address the bottleneck of training CNN with multiple\ndatasets, due to different landmark markups on different datasets, such as 5,\n34, 68. Experimental results show our method not only provides high-quality,\ndense 3D face fitting but also outperforms the state-of-the-art facial landmark\ndetection methods on the challenging datasets. Our model can run at real time\nduring testing.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:14:32 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Liu", "Yaojie", ""], ["Jourabloo", "Amin", ""], ["Ren", "William", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1709.01450", "submitter": "Grant Van Horn", "authors": "Grant Van Horn, Pietro Perona", "title": "The Devil is in the Tails: Fine-grained Classification in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is long-tailed. What does this mean for computer vision and visual\nrecognition? The main two implications are (1) the number of categories we need\nto consider in applications can be very large, and (2) the number of training\nexamples for most categories can be very small. Current visual recognition\nalgorithms have achieved excellent classification accuracy. However, they\nrequire many training examples to reach peak performance, which suggests that\nlong-tailed distributions will not be dealt with well. We analyze this question\nin the context of eBird, a large fine-grained classification dataset, and a\nstate-of-the-art deep network classification algorithm. We find that (a) peak\nclassification performance on well-represented categories is excellent, (b)\ngiven enough data, classification performance suffers only minimally from an\nincrease in the number of classes, (c) classification performance decays\nprecipitously as the number of training examples decreases, (d) surprisingly,\ntransfer learning is virtually absent in current methods. Our findings suggest\nthat our community should come to grips with the question of long tails.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:26:47 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Van Horn", "Grant", ""], ["Perona", "Pietro", ""]]}, {"id": "1709.01459", "submitter": "David Joseph Tan", "authors": "David Joseph Tan and Nassir Navab and Federico Tombari", "title": "6D Object Pose Estimation with Depth Images: A Seamless Approach for\n  Robotic Interaction and Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To determine the 3D orientation and 3D location of objects in the\nsurroundings of a camera mounted on a robot or mobile device, we developed two\npowerful algorithms in object detection and temporal tracking that are combined\nseamlessly for robotic perception and interaction as well as Augmented Reality\n(AR). A separate evaluation of, respectively, the object detection and the\ntemporal tracker demonstrates the important stride in research as well as the\nimpact on industrial robotic applications and AR. When evaluated on a standard\ndataset, the detector produced the highest f1-score with a large margin while\nthe tracker generated the best accuracy at a very low latency of approximately\n2 ms per frame with one CPU core: both algorithms outperforming the state of\nthe art. When combined, we achieve a powerful framework that is robust to\nhandle multiple instances of the same object under occlusion and clutter while\nattaining real-time performance. Aiming at stepping beyond the simple scenarios\nused by current systems, often constrained by having a single object in absence\nof clutter, averting to touch the object to prevent close-range partial\nocclusion, selecting brightly colored objects to easily segment them\nindividually or assuming that the object has simple geometric structure, we\ndemonstrate the capacity to handle challenging cases under clutter, partial\nocclusion and varying lighting conditions with objects of different shapes and\nsizes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:38:26 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Tan", "David Joseph", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1709.01467", "submitter": "Jo\\~ao Carvalho", "authors": "Jo\\~ao Carvalho, Manuel Marques, Jo\\~ao P. Costeira", "title": "Subspace Segmentation by Successive Approximations: A Method for\n  Low-Rank and High-Rank Data with Missing Entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to reconstruct and cluster incomplete high-dimensional\ndata lying in a union of low-dimensional subspaces. Exploring the sparse\nrepresentation model, we jointly estimate the missing data while imposing the\nintrinsic subspace structure. Since we have a non-convex problem, we propose an\niterative method to reconstruct the data and provide a sparse similarity\naffinity matrix. This method is robust to initialization and achieves greater\nreconstruction accuracy than current methods, which dramatically improves\nclustering performance. Extensive experiments with synthetic and real data show\nthat our approach leads to significant improvements in the reconstruction and\nsegmentation, outperforming current state of the art for both low and high-rank\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 15:58:30 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Carvalho", "Jo\u00e3o", ""], ["Marques", "Manuel", ""], ["Costeira", "Jo\u00e3o P.", ""]]}, {"id": "1709.01472", "submitter": "Mario Valerio Giuffrida", "authors": "Andrei Dobrescu, Mario Valerio Giuffrida, Sotirios A Tsaftaris", "title": "Leveraging multiple datasets for deep leaf counting", "comments": "8 pages, 3 figures, 3 tables", "journal-ref": "CVPPP workshop 2017, ICCV", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of leaves a plant has is one of the key traits (phenotypes)\ndescribing its development and growth. Here, we propose an automated, deep\nlearning based approach for counting leaves in model rosette plants. While\nstate-of-the-art results on leaf counting with deep learning methods have\nrecently been reported, they obtain the count as a result of leaf segmentation\nand thus require per-leaf (instance) segmentation to train the models (a rather\nstrong annotation). Instead, our method treats leaf counting as a direct\nregression problem and thus only requires as annotation the total leaf count\nper plant. We argue that combining different datasets when training a deep\nneural network is beneficial and improves the results of the proposed approach.\nWe evaluate our method on the CVPPP 2017 Leaf Counting Challenge dataset, which\ncontains images of Arabidopsis and tobacco plants. Experimental results show\nthat the proposed method significantly outperforms the winner of the previous\nCVPPP challenge, improving the results by a minimum of ~50% on each of the test\ndatasets, and can achieve this performance without knowing the experimental\norigin of the data (i.e. in the wild setting of the challenge). We also compare\nthe counting accuracy of our model with that of per leaf segmentation\nalgorithms, achieving a 20% decrease in mean absolute difference in count\n(|DiC|).\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 16:09:18 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Dobrescu", "Andrei", ""], ["Giuffrida", "Mario Valerio", ""], ["Tsaftaris", "Sotirios A", ""]]}, {"id": "1709.01476", "submitter": "Jan Zacharias", "authors": "Daniel Sonntag, Michael Barz, Jan Zacharias, Sven Stauden, Vahid\n  Rahmani, \\'Aron F\\'othi, Andr\\'as L\\H{o}rincz", "title": "Fine-tuning deep CNN models on specific MS COCO categories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fine-tuning of a deep convolutional neural network (CNN) is often desired.\nThis paper provides an overview of our publicly available py-faster-rcnn-ft\nsoftware library that can be used to fine-tune the VGG_CNN_M_1024 model on\ncustom subsets of the Microsoft Common Objects in Context (MS COCO) dataset.\nFor example, we improved the procedure so that the user does not have to look\nfor suitable image files in the dataset by hand which can then be used in the\ndemo program. Our implementation randomly selects images that contain at least\none object of the categories on which the model is fine-tuned.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 16:22:28 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Sonntag", "Daniel", ""], ["Barz", "Michael", ""], ["Zacharias", "Jan", ""], ["Stauden", "Sven", ""], ["Rahmani", "Vahid", ""], ["F\u00f3thi", "\u00c1ron", ""], ["L\u0151rincz", "Andr\u00e1s", ""]]}, {"id": "1709.01500", "submitter": "Oscar Mendez", "authors": "Oscar Mendez, Simon Hadfield, Nicolas Pugeault, Richard Bowden", "title": "SeDAR - Semantic Detection and Ranging: Humans can localise without\n  LiDAR, can robots?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does a person work out their location using a floorplan? It is probably\nsafe to say that we do not explicitly measure depths to every visible surface\nand try to match them against different pose estimates in the floorplan. And\nyet, this is exactly how most robotic scan-matching algorithms operate.\nSimilarly, we do not extrude the 2D geometry present in the floorplan into 3D\nand try to align it to the real-world. And yet, this is how most vision-based\napproaches localise.\n  Humans do the exact opposite. Instead of depth, we use high level semantic\ncues. Instead of extruding the floorplan up into the third dimension, we\ncollapse the 3D world into a 2D representation. Evidence of this is that many\nof the floorplans we use in everyday life are not accurate, opting instead for\nhigh levels of discriminative landmarks.\n  In this work, we use this insight to present a global localisation approach\nthat relies solely on the semantic labels present in the floorplan and\nextracted from RGB images. While our approach is able to use range measurements\nif available, we demonstrate that they are unnecessary as we can achieve\nresults comparable to state-of-the-art without them.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 17:35:07 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 09:02:29 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Mendez", "Oscar", ""], ["Hadfield", "Simon", ""], ["Pugeault", "Nicolas", ""], ["Bowden", "Richard", ""]]}, {"id": "1709.01507", "submitter": "Gang Sun", "authors": "Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu", "title": "Squeeze-and-Excitation Networks", "comments": "journal version of the CVPR 2018 paper, accepted by TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The central building block of convolutional neural networks (CNNs) is the\nconvolution operator, which enables networks to construct informative features\nby fusing both spatial and channel-wise information within local receptive\nfields at each layer. A broad range of prior research has investigated the\nspatial component of this relationship, seeking to strengthen the\nrepresentational power of a CNN by enhancing the quality of spatial encodings\nthroughout its feature hierarchy. In this work, we focus instead on the channel\nrelationship and propose a novel architectural unit, which we term the\n\"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise\nfeature responses by explicitly modelling interdependencies between channels.\nWe show that these blocks can be stacked together to form SENet architectures\nthat generalise extremely effectively across different datasets. We further\ndemonstrate that SE blocks bring significant improvements in performance for\nexisting state-of-the-art CNNs at slight additional computational cost.\nSqueeze-and-Excitation Networks formed the foundation of our ILSVRC 2017\nclassification submission which won first place and reduced the top-5 error to\n2.251%, surpassing the winning entry of 2016 by a relative improvement of ~25%.\nModels and code are available at https://github.com/hujie-frank/SENet.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 17:42:13 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 17:21:25 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 19:40:36 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 05:32:17 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Hu", "Jie", ""], ["Shen", "Li", ""], ["Albanie", "Samuel", ""], ["Sun", "Gang", ""], ["Wu", "Enhua", ""]]}, {"id": "1709.01574", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Graham W Taylor, Alexander Wong", "title": "Opening the Black Box of Financial AI with CLEAR-Trade: A CLass-Enhanced\n  Attentive Response Approach for Explaining and Visualizing Deep\n  Learning-Driven Stock Market Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been shown to outperform traditional machine learning\nalgorithms across a wide range of problem domains. However, current deep\nlearning algorithms have been criticized as uninterpretable \"black-boxes\" which\ncannot explain their decision making processes. This is a major shortcoming\nthat prevents the widespread application of deep learning to domains with\nregulatory processes such as finance. As such, industries such as finance have\nto rely on traditional models like decision trees that are much more\ninterpretable but less effective than deep learning for complex problems. In\nthis paper, we propose CLEAR-Trade, a novel financial AI visualization\nframework for deep learning-driven stock market prediction that mitigates the\ninterpretability issue of deep learning methods. In particular, CLEAR-Trade\nprovides a effective way to visualize and explain decisions made by deep stock\nmarket prediction models. We show the efficacy of CLEAR-Trade in enhancing the\ninterpretability of stock market prediction by conducting experiments based on\nS&P 500 stock index prediction. The results demonstrate that CLEAR-Trade can\nprovide significant insight into the decision-making process of deep\nlearning-driven financial models, particularly for regulatory processes, thus\nimproving their potential uptake in the financial industry.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 19:56:36 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Kumar", "Devinder", ""], ["Taylor", "Graham W", ""], ["Wong", "Alexander", ""]]}, {"id": "1709.01591", "submitter": "Sina Honari", "authors": "Sina Honari, Pavlo Molchanov, Stephen Tyree, Pascal Vincent,\n  Christopher Pal, Jan Kautz", "title": "Improving Landmark Localization with Semi-Supervised Learning", "comments": "Published as a conference paper in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two techniques to improve landmark localization in images from\npartially annotated datasets. Our primary goal is to leverage the common\nsituation where precise landmark locations are only provided for a small data\nsubset, but where class labels for classification or regression tasks related\nto the landmarks are more abundantly available. First, we propose the framework\nof sequential multitasking and explore it here through an architecture for\nlandmark localization where training with class labels acts as an auxiliary\nsignal to guide the landmark localization on unlabeled data. A key aspect of\nour approach is that errors can be backpropagated through a complete landmark\nlocalization model. Second, we propose and explore an unsupervised learning\ntechnique for landmark localization based on having a model predict equivariant\nlandmarks with respect to transformations applied to the image. We show that\nthese techniques, improve landmark prediction considerably and can learn\neffective detectors even when only a small fraction of the dataset has landmark\nlabels. We present results on two toy datasets and four real datasets, with\nhands and faces, and report new state-of-the-art on two datasets in the wild,\ne.g. with only 5\\% of labeled images we outperform previous state-of-the-art\ntrained on the AFLW dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 20:52:23 GMT"}, {"version": "v2", "created": "Sun, 24 Sep 2017 15:39:37 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 00:04:05 GMT"}, {"version": "v4", "created": "Sat, 24 Feb 2018 18:19:28 GMT"}, {"version": "v5", "created": "Tue, 27 Mar 2018 04:01:24 GMT"}, {"version": "v6", "created": "Thu, 24 May 2018 17:23:32 GMT"}, {"version": "v7", "created": "Sun, 28 Oct 2018 15:05:52 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Honari", "Sina", ""], ["Molchanov", "Pavlo", ""], ["Tyree", "Stephen", ""], ["Vincent", "Pascal", ""], ["Pal", "Christopher", ""], ["Kautz", "Jan", ""]]}, {"id": "1709.01599", "submitter": "Hongming Li", "authors": "Hongming Li, Mohamad Habes, Yong Fan", "title": "Deep Ordinal Ranking for Multi-Category Diagnosis of Alzheimer's Disease\n  using Hippocampal MRI data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing effort in brain image analysis has been dedicated to early\ndiagnosis of Alzheimer's disease (AD) based on neuroimaging data. Most existing\nstudies have been focusing on binary classification problems, e.g.,\ndistinguishing AD patients from normal control (NC) elderly or mild cognitive\nimpairment (MCI) individuals from NC elderly. However, identifying individuals\nwith AD and MCI, especially MCI individuals who will convert to AD (progressive\nMCI, pMCI), in a single setting, is needed to achieve the goal of early\ndiagnosis of AD. In this paper, we propose a deep ordinal ranking model for\ndistinguishing NC, stable MCI (sMCI), pMCI, and AD at an individual subject\nlevel, taking into account the inherent ordinal severity of brain degeneration\ncaused by normal aging, MCI, and AD, rather than formulating the classification\nas a multi-category classification problem. The proposed deep ordinal ranking\nmodel focuses on the hippocampal morphology of individuals and learns\ninformative and discriminative features automatically. Experiment results based\non a large cohort of individuals from the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) indicate that the proposed method can achieve better\nperformance than traditional multi-category classification techniques using\nshape and radiomics features from structural magnetic resonance imaging (MRI)\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 21:29:39 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 15:26:38 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Li", "Hongming", ""], ["Habes", "Mohamad", ""], ["Fan", "Yong", ""]]}, {"id": "1709.01602", "submitter": "Islem Rekik", "authors": "Samya Amiri, Mohamed Ali Mahjoub, Islem Rekik", "title": "Dynamic Multiscale Tree Learning Using Ensemble Strong Classifiers for\n  Multi-label Segmentation of Medical Images with Lesions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dynamic multiscale tree (DMT) architecture that learns how to\nleverage the strengths of different existing classifiers for supervised\nmulti-label image segmentation. Unlike previous works that simply aggregate or\ncascade classifiers for addressing image segmentation and labeling tasks, we\npropose to embed strong classifiers into a tree structure that allows\nbi-directional flow of information between its classifier nodes to gradually\nimprove their performances. Our DMT is a generic classification model that\ninherently embeds different cascades of classifiers while enhancing learning\ntransfer between them to boost up their classification accuracies.\nSpecifically, each node in our DMT can nest a Structured Random Forest (SRF)\nclassifier or a Bayesian Network (BN) classifier. The proposed SRF-BN DMT\narchitecture has several appealing properties. First, while SRF operates at a\npatch-level (regular image region), BN operates at the super-pixel level\n(irregular image region), thereby enabling the DMT to integrate multi-level\nimage knowledge in the learning process. Second, although BN is powerful in\nmodeling dependencies between image elements (superpixels, edges) and their\nfeatures, the learning of its structure and parameters is challenging. On the\nother hand, SRF may fail to accurately detect very irregular object boundaries.\nThe proposed DMT robustly overcomes these limitations for both classifiers\nthrough the ascending and descending flow of contextual information between\neach parent node and its children nodes. Third, we train DMT using different\nscales, where we progressively decrease the patch and superpixel sizes as we go\ndeeper along the tree edges nearing its leaf nodes. Last, DMT demonstrates its\noutperformance in comparison to several state-of-the-art segmentation methods\nfor multi-labeling of brain images with gliomas.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 21:41:58 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Amiri", "Samya", ""], ["Mahjoub", "Mohamed Ali", ""], ["Rekik", "Islem", ""]]}, {"id": "1709.01618", "submitter": "Chris Tensmeyer", "authors": "Chris Tensmeyer, Brian Davis, Curtis Wigington, Iain Lee, Bill Barrett", "title": "PageNet: Page Boundary Extraction in Historical Handwritten Documents", "comments": "HIP 2017 (in submission)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When digitizing a document into an image, it is common to include a\nsurrounding border region to visually indicate that the entire document is\npresent in the image. However, this border should be removed prior to automated\nprocessing. In this work, we present a deep learning based system, PageNet,\nwhich identifies the main page region in an image in order to segment content\nfrom both textual and non-textual border noise. In PageNet, a Fully\nConvolutional Network obtains a pixel-wise segmentation which is post-processed\ninto the output quadrilateral region. We evaluate PageNet on 4 collections of\nhistorical handwritten documents and obtain over 94% mean intersection over\nunion on all datasets and approach human performance on 2 of these collections.\nAdditionally, we show that PageNet can segment documents that are overlayed on\ntop of other documents.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 22:54:49 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Tensmeyer", "Chris", ""], ["Davis", "Brian", ""], ["Wigington", "Curtis", ""], ["Lee", "Iain", ""], ["Barrett", "Bill", ""]]}, {"id": "1709.01625", "submitter": "Payman Yadollahpour", "authors": "Payman Yadollahpour", "title": "Exploring and Exploiting Diversity for Image Segmentation", "comments": "PhD Thesis. For overall document size considerations the results in\n  this appendix section have been moved to\n  http://ttic.uchicago.edu/~pyadolla/papers/thesis.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation is an important computer vision task that is\ndifficult because it consists of both recognition and segmentation. The task is\noften cast as a structured output problem on an exponentially large\noutput-space, which is typically modeled by a discrete probabilistic model. The\nbest segmentation is found by inferring the Maximum a-Posteriori (MAP) solution\nover the output distribution defined by the model. Due to limitations in\noptimization, the model cannot be arbitrarily complex. This leads to a\ntrade-off: devise a more accurate model that incorporates rich high-order\ninteractions between image elements at the cost of inaccurate and possibly\nintractable optimization OR leverage a tractable model which produces less\naccurate MAP solutions but may contain high quality solutions as other modes of\nits output distribution.\n  This thesis investigates the latter and presents a two stage approach to\nsemantic segmentation. In the first stage a tractable segmentation model\noutputs a set of high probability segmentations from the underlying\ndistribution that are not just minor perturbations of each other. Critically\nthe output of this stage is a diverse set of plausible solutions and not just a\nsingle one. In the second stage, a discriminatively trained re-ranking model\nselects the best segmentation from this set. The re-ranking stage can use much\nmore complex features than what could be tractably used in the segmentation\nmodel, allowing a better exploration of the solution space than simply\nreturning the MAP solution. The formulation is agnostic to the underlying\nsegmentation model (e.g. CRF, CNN, etc.) and optimization algorithm, which\nmakes it applicable to a wide range of models and inference methods. Evaluation\nof the approach on a number of semantic image segmentation benchmark datasets\nhighlight its superiority over inferring the MAP solution.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 23:30:11 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Yadollahpour", "Payman", ""]]}, {"id": "1709.01630", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, and Jianbo Shi", "title": "Using Cross-Model EgoSupervision to Learn Cooperative Basketball\n  Intention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a first-person method for cooperative basketball intention\nprediction: we predict with whom the camera wearer will cooperate in the near\nfuture from unlabeled first-person images. This is a challenging task that\nrequires inferring the camera wearer's visual attention, and decoding the\nsocial cues of other players. Our key observation is that a first-person view\nprovides strong cues to infer the camera wearer's momentary visual attention,\nand his/her intentions. We exploit this observation by proposing a new\ncross-model EgoSupervision learning scheme that allows us to predict with whom\nthe camera wearer will cooperate in the near future, without using manually\nlabeled intention labels. Our cross-model EgoSupervision operates by\ntransforming the outputs of a pretrained pose-estimation network, into pseudo\nground truth labels, which are then used as a supervisory signal to train a new\nnetwork for a cooperative intention task. We evaluate our method, and show that\nit achieves similar or even better accuracy than the fully supervised methods\ndo.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 23:49:51 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Bertasius", "Gedas", ""], ["Shi", "Jianbo", ""]]}, {"id": "1709.01643", "submitter": "Alexander Ratner", "authors": "Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hussain, Jared\n  Dunnmon, Christopher R\\'e", "title": "Learning to Compose Domain-Specific Transformations for Data\n  Augmentation", "comments": "To appear at Neural Information Processing Systems (NIPS) 2017", "journal-ref": "Advances in Neural Information Processing Systems 30, 2017,\n  3236--3246", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a ubiquitous technique for increasing the size of\nlabeled training sets by leveraging task-specific data transformations that\npreserve class labels. While it is often easy for domain experts to specify\nindividual transformations, constructing and tuning the more sophisticated\ncompositions typically needed to achieve state-of-the-art results is a\ntime-consuming manual task in practice. We propose a method for automating this\nprocess by learning a generative sequence model over user-specified\ntransformation functions using a generative adversarial approach. Our method\ncan make use of arbitrary, non-deterministic transformation functions, is\nrobust to misspecified user input, and is trained on unlabeled data. The\nlearned transformation model can then be used to perform data augmentation for\nany end discriminative model. In our experiments, we show the efficacy of our\napproach on both image and text datasets, achieving improvements of 4.0\naccuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task,\nand 3.4 accuracy points when using domain-specific transformation operations on\na medical imaging dataset as compared to standard heuristic augmentation\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 01:17:31 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 18:09:15 GMT"}, {"version": "v3", "created": "Sat, 30 Sep 2017 04:27:53 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Ratner", "Alexander J.", ""], ["Ehrenberg", "Henry R.", ""], ["Hussain", "Zeshan", ""], ["Dunnmon", "Jared", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1709.01664", "submitter": "Buket Barkana", "authors": "Zakariya Qawaqneh, Arafat Abu Mallouh, Buket D. Barkana", "title": "Deep Convolutional Neural Network for Age Estimation based on VGG-Face\n  Model", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic age estimation from real-world and unconstrained face images is\nrapidly gaining importance. In our proposed work, a deep CNN model that was\ntrained on a database for face recognition task is used to estimate the age\ninformation on the Adience database. This paper has three significant\ncontributions in this field. (1) This work proves that a CNN model, which was\ntrained for face recognition task, can be utilized for age estimation to\nimprove performance; (2) Over fitting problem can be overcome by employing a\npretrained CNN on a large database for face recognition task; (3) Not only the\nnumber of training images and the number subjects in a training database effect\nthe performance of the age estimation model, but also the pre-training task of\nthe employed CNN determines the performance of the model.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 03:37:12 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Qawaqneh", "Zakariya", ""], ["Mallouh", "Arafat Abu", ""], ["Barkana", "Buket D.", ""]]}, {"id": "1709.01686", "submitter": "Surat Teerapittayanon", "authors": "Surat Teerapittayanon, Bradley McDanel, H.T. Kung", "title": "BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are state of the art methods for many learning tasks due\nto their ability to extract increasingly better features at each network layer.\nHowever, the improved performance of additional layers in a deep network comes\nat the cost of added latency and energy usage in feedforward inference. As\nnetworks continue to get deeper and larger, these costs become more prohibitive\nfor real-time and energy-sensitive applications. To address this issue, we\npresent BranchyNet, a novel deep network architecture that is augmented with\nadditional side branch classifiers. The architecture allows prediction results\nfor a large portion of test samples to exit the network early via these\nbranches when samples can already be inferred with high confidence. BranchyNet\nexploits the observation that features learned at an early layer of a network\nmay often be sufficient for the classification of many data points. For more\ndifficult samples, which are expected less frequently, BranchyNet will use\nfurther or all network layers to provide the best likelihood of correct\nprediction. We study the BranchyNet architecture using several well-known\nnetworks (LeNet, AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that\nit can both improve accuracy and significantly reduce the inference time of the\nnetwork.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 06:30:51 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Teerapittayanon", "Surat", ""], ["McDanel", "Bradley", ""], ["Kung", "H. T.", ""]]}, {"id": "1709.01688", "submitter": "Alexandr Rassadin G.", "authors": "Alexandr G. Rassadin, Alexey S. Gruzdev, Andrey V. Savchenko", "title": "Group-level Emotion Recognition using Transfer Learning from Face\n  Identification", "comments": "5 pages, 3 figures, accepted for publication at ICMI17 (EmotiW Grand\n  Challenge)", "journal-ref": "Proceedings of the 19th ACM International Conference on Multimodal\n  Interaction (ICMI), 2017, pp. 544-548", "doi": "10.1145/3136755.3143007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our algorithmic approach, which was used for\nsubmissions in the fifth Emotion Recognition in the Wild (EmotiW 2017)\ngroup-level emotion recognition sub-challenge. We extracted feature vectors of\ndetected faces using the Convolutional Neural Network trained for face\nidentification task, rather than traditional pre-training on emotion\nrecognition problems. In the final pipeline an ensemble of Random Forest\nclassifiers was learned to predict emotion score using available training set.\nIn case when the faces have not been detected, one member of our ensemble\nextracts features from the whole image. During our experimental study, the\nproposed approach showed the lowest error rate when compared to other explored\ntechniques. In particular, we achieved 75.4% accuracy on the validation data,\nwhich is 20% higher than the handcrafted feature-based baseline. The source\ncode using Keras framework is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 06:47:23 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 13:31:40 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 19:33:12 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Rassadin", "Alexandr G.", ""], ["Gruzdev", "Alexey S.", ""], ["Savchenko", "Andrey V.", ""]]}, {"id": "1709.01695", "submitter": "Jacopo Cavazza", "authors": "Jacopo Cavazza, Pietro Morerio and Vittorio Murino", "title": "A Compact Kernel Approximation for 3D Action Recognition", "comments": "Best paper award special mention at the 19th edition of the GIRPR\n  International Conference on Image Analysis and Processing (ICIAP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D action recognition was shown to benefit from a covariance representation\nof the input data (joint 3D positions). A kernel machine feed with such feature\nis an effective paradigm for 3D action recognition, yielding state-of-the-art\nresults. Yet, the whole framework is affected by the well-known scalability\nissue. In fact, in general, the kernel function has to be evaluated for all\npairs of instances inducing a Gram matrix whose complexity is quadratic in the\nnumber of samples. In this work we reduce such complexity to be linear by\nproposing a novel and explicit feature map to approximate the kernel function.\nThis allows to train a linear classifier with an explicit feature encoding,\nwhich implicitly implements a Log-Euclidean machine in a scalable fashion. Not\nonly we prove that the proposed approximation is unbiased, but also we work out\nan explicit strong bound for its variance, attesting a theoretical superiority\nof our approach with respect to existing ones. Experimentally, we verify that\nour representation provides a compact encoding and outperforms other\napproximation schemes on a number of publicly available benchmark datasets for\n3D action recognition.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 07:02:58 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 11:05:35 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Cavazza", "Jacopo", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""]]}, {"id": "1709.01710", "submitter": "Marina Ljubenovi\\'c", "authors": "Marina Ljubenovi\\'c and M\\'ario A. T. Figueiredo", "title": "Blind image deblurring using class-adapted image priors", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring (BID) is an ill-posed inverse problem, usually\naddressed by imposing prior knowledge on the (unknown) image and on the\nblurring filter. Most of the work on BID has focused on natural images, using\nimage priors based on statistical properties of generic natural images.\nHowever, in many applications, it is known that the image being recovered\nbelongs to some specific class (e.g., text, face, fingerprints), and exploiting\nthis knowledge allows obtaining more accurate priors. In this work, we propose\na method where a Gaussian mixture model (GMM) is used to learn a class-adapted\nprior, by training on a dataset of clean images of that class. Experiments show\nthe competitiveness of the proposed method in terms of restoration quality when\ndealing with images containing text, faces, or fingerprints. Additionally,\nexperiments show that the proposed method is able to handle text images at high\nnoise levels, outperforming state-of-the-art methods specifically designed for\nBID of text images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 08:20:10 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ljubenovi\u0107", "Marina", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1709.01722", "submitter": "Devis Tuia", "authors": "Nicolas Rey, Michele Volpi, St\\'ephane Joost, Devis Tuia", "title": "Detecting animals in African Savanna with UAVs and the crowds", "comments": null, "journal-ref": "Remote Sensing of Environment, 200, pp. 341-351, 2017", "doi": "10.1016/j.rse.2017.08.026", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAVs) offer new opportunities for wildlife\nmonitoring, with several advantages over traditional field-based methods. They\nhave readily been used to count birds, marine mammals and large herbivores in\ndifferent environments, tasks which are routinely performed through manual\ncounting in large collections of images. In this paper, we propose a\nsemi-automatic system able to detect large mammals in semi-arid Savanna. It\nrelies on an animal-detection system based on machine learning, trained with\ncrowd-sourced annotations provided by volunteers who manually interpreted\nsub-decimeter resolution color images. The system achieves a high recall rate\nand a human operator can then eliminate false detections with limited effort.\nOur system provides good perspectives for the development of data-driven\nmanagement practices in wildlife conservation. It shows that the detection of\nlarge mammals in semi-arid Savanna can be approached by processing data\nprovided by standard RGB cameras mounted on affordable fixed wings UAVs.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 08:45:09 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Rey", "Nicolas", ""], ["Volpi", "Michele", ""], ["Joost", "St\u00e9phane", ""], ["Tuia", "Devis", ""]]}, {"id": "1709.01727", "submitter": "Fei Yin", "authors": "Fei Yin, Yi-Chao Wu, Xu-Yao Zhang, Cheng-Lin Liu", "title": "Scene Text Recognition with Sliding Convolutional Character Models", "comments": "10 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition has attracted great interests from the computer vision\nand pattern recognition community in recent years. State-of-the-art methods use\nconcolutional neural networks (CNNs), recurrent neural networks with long\nshort-term memory (RNN-LSTM) or the combination of them. In this paper, we\ninvestigate the intrinsic characteristics of text recognition, and inspired by\nhuman cognition mechanisms in reading texts, we propose a scene text\nrecognition method with character models on convolutional feature map. The\nmethod simultaneously detects and recognizes characters by sliding the text\nline image with character models, which are learned end-to-end on text line\nimages labeled with text transcripts. The character classifier outputs on the\nsliding windows are normalized and decoded with Connectionist Temporal\nClassification (CTC) based algorithm. Compared to previous methods, our method\nhas a number of appealing properties: (1) It avoids the difficulty of character\nsegmentation which hinders the performance of segmentation-based recognition\nmethods; (2) The model can be trained simply and efficiently because it avoids\ngradient vanishing/exploding in training RNN-LSTM based models; (3) It bases on\ncharacter models trained free of lexicon, and can recognize unknown words. (4)\nThe recognition process is highly parallel and enables fast recognition. Our\nexperiments on several challenging English and Chinese benchmarks, including\nthe IIIT-5K, SVT, ICDAR03/13 and TRW15 datasets, demonstrate that the proposed\nmethod yields superior or comparable performance to state-of-the-art methods\nwhile the model size is relatively small.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 09:01:53 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Yin", "Fei", ""], ["Wu", "Yi-Chao", ""], ["Zhang", "Xu-Yao", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1709.01779", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues and Francisco Pereira", "title": "Deep learning from crowds", "comments": "10 pages, The Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, deep learning has revolutionized the field of\nmachine learning by dramatically improving the state-of-the-art in various\ndomains. However, as the size of supervised artificial neural networks grows,\ntypically so does the need for larger labeled datasets. Recently, crowdsourcing\nhas established itself as an efficient and cost-effective solution for labeling\nlarge sets of data in a scalable manner, but it often requires aggregating\nlabels from multiple noisy contributors with different levels of expertise. In\nthis paper, we address the problem of learning deep neural networks from\ncrowds. We begin by describing an EM algorithm for jointly learning the\nparameters of the network and the reliabilities of the annotators. Then, a\nnovel general-purpose crowd layer is proposed, which allows us to train deep\nneural networks end-to-end, directly from the noisy labels of multiple\nannotators, using only backpropagation. We empirically show that the proposed\napproach is able to internally capture the reliability and biases of different\nannotators and achieve new state-of-the-art results for various crowdsourced\ndatasets across different settings, namely classification, regression and\nsequence labeling.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:41:19 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 12:30:12 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Rodrigues", "Filipe", ""], ["Pereira", "Francisco", ""]]}, {"id": "1709.01782", "submitter": "Ekta Vats", "authors": "Ekta Vats, Anders Hast and Prashant Singh", "title": "Automatic Document Image Binarization using Bayesian Optimization", "comments": null, "journal-ref": "4th International Workshop on Historical Document Imaging and\n  Processing (HIP2017). ACM, New York, NY, USA, 89-94", "doi": "10.1145/3151509.3151520", "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document image binarization is often a challenging task due to various forms\nof degradation. Although there exist several binarization techniques in\nliterature, the binarized image is typically sensitive to control parameter\nsettings of the employed technique. This paper presents an automatic document\nimage binarization algorithm to segment the text from heavily degraded document\nimages. The proposed technique uses a two band-pass filtering approach for\nbackground noise removal, and Bayesian optimization for automatic\nhyperparameter selection for optimal results. The effectiveness of the proposed\nbinarization technique is empirically demonstrated on the Document Image\nBinarization Competition (DIBCO) and the Handwritten Document Image\nBinarization Competition (H-DIBCO) datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:47:31 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 08:25:26 GMT"}, {"version": "v3", "created": "Sat, 21 Oct 2017 11:20:24 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Vats", "Ekta", ""], ["Hast", "Anders", ""], ["Singh", "Prashant", ""]]}, {"id": "1709.01784", "submitter": "Xin Ji", "authors": "Xin Ji, Wei Wang, Meihui Zhang, Yang Yang", "title": "Cross-Domain Image Retrieval with Attention Modeling", "comments": "8 pages with an extra reference page", "journal-ref": "2017 ACM Multimedia Conference", "doi": "10.1145/3123266.3123429", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the proliferation of e-commerce websites and the ubiquitousness of smart\nphones, cross-domain image retrieval using images taken by smart phones as\nqueries to search products on e-commerce websites is emerging as a popular\napplication. One challenge of this task is to locate the attention of both the\nquery and database images. In particular, database images, e.g. of fashion\nproducts, on e-commerce websites are typically displayed with other\naccessories, and the images taken by users contain noisy background and large\nvariations in orientation and lighting. Consequently, their attention is\ndifficult to locate. In this paper, we exploit the rich tag information\navailable on the e-commerce websites to locate the attention of database\nimages. For query images, we use each candidate image in the database as the\ncontext to locate the query attention. Novel deep convolutional neural network\narchitectures, namely TagYNet and CtxYNet, are proposed to learn the attention\nweights and then extract effective representations of the images. Experimental\nresults on public datasets confirm that our approaches have significant\nimprovement over the existing methods in terms of the retrieval accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:49:46 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ji", "Xin", ""], ["Wang", "Wei", ""], ["Zhang", "Meihui", ""], ["Yang", "Yang", ""]]}, {"id": "1709.01788", "submitter": "Ekta Vats", "authors": "Anders Hast and Ekta Vats", "title": "Radial Line Fourier Descriptor for Historical Handwritten Text\n  Representation", "comments": "under review", "journal-ref": null, "doi": "10.24132/JWSCG.2018.26.1.4", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of historical handwritten manuscripts is a daunting\ntask due to paper degradation over time. Recognition-free retrieval or word\nspotting is popularly used for information retrieval and digitization of the\nhistorical handwritten documents. However, the performance of word spotting\nalgorithms depends heavily on feature detection and representation methods.\nAlthough there exist popular feature descriptors such as Scale Invariant\nFeature Transform (SIFT) and Speeded Up Robust Features (SURF), the invariant\nproperties of these descriptors amplify the noise in the degraded document\nimages, rendering them more sensitive to noise and complex characteristics of\nhistorical manuscripts. Therefore, an efficient and relaxed feature descriptor\nis required as handwritten words across different documents are indeed similar,\nbut not identical. This paper introduces a Radial Line Fourier (RLF) descriptor\nfor handwritten word representation, with a short feature vector of 32\ndimensions. A segmentation-free and training-free handwritten word spotting\nmethod is studied herein that relies on the proposed RLF descriptor, takes into\naccount different keypoint representations and uses a simple\npreconditioner-based feature matching algorithm. The effectiveness of the RLF\ndescriptor for segmentation-free handwritten word spotting is empirically\nevaluated on well-known historical handwritten datasets using standard\nevaluation measures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:56:34 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 11:39:39 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 14:30:47 GMT"}, {"version": "v4", "created": "Tue, 20 Mar 2018 15:23:08 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Hast", "Anders", ""], ["Vats", "Ekta", ""]]}, {"id": "1709.01809", "submitter": "Harshit Gupta", "authors": "Harshit Gupta, Kyong Hwan Jin, Ha Q. Nguyen, Michael T. McCann, and\n  Michael Unser", "title": "CNN-Based Projected Gradient Descent for Consistent Image Reconstruction", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp.\n  1440-1453, June 2018", "doi": "10.1109/TMI.2018.2832656", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for image reconstruction which replaces the projector\nin a projected gradient descent (PGD) with a convolutional neural network\n(CNN). CNNs trained as high-dimensional (image-to-image) regressors have\nrecently been used to efficiently solve inverse problems in imaging. However,\nthese approaches lack a feedback mechanism to enforce that the reconstructed\nimage is consistent with the measurements. This is crucial for inverse\nproblems, and more so in biomedical imaging, where the reconstructions are used\nfor diagnosis. In our scheme, the gradient descent enforces measurement\nconsistency, while the CNN recursively projects the solution closer to the\nspace of desired reconstruction images. We provide a formal framework to ensure\nthat the classical PGD converges to a local minimizer of a non-convex\nconstrained least-squares problem. When the projector is replaced with a CNN,\nwe propose a relaxed PGD, which always converges. Finally, we propose a simple\nscheme to train a CNN to act like a projector. Our experiments on sparse view\nComputed Tomography (CT) reconstruction for both noiseless and noisy\nmeasurements show an improvement over the total-variation (TV) method and a\nrecent CNN-based technique.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 12:55:56 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Gupta", "Harshit", ""], ["Jin", "Kyong Hwan", ""], ["Nguyen", "Ha Q.", ""], ["McCann", "Michael T.", ""], ["Unser", "Michael", ""]]}, {"id": "1709.01813", "submitter": "Sophie Crommelinck", "authors": "Sophie Crommelinck, Michael Ying Yang, Mila Koeva, Markus Gerke, Rohan\n  Bennett, George Vosselman", "title": "Towards Automated Cadastral Boundary Delineation from UAV Data", "comments": "Report on current state (August 2017) of PhD work of first author.\n  Further info: https://its4land.com/automate-it-wp5/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles (UAV) are evolving as an alternative tool to acquire\nland tenure data. UAVs can capture geospatial data at high quality and\nresolution in a cost-effective, transparent and flexible manner, from which\nvisible land parcel boundaries, i.e., cadastral boundaries are delineable. This\ndelineation is to no extent automated, even though physical objects\nautomatically retrievable through image analysis methods mark a large portion\nof cadastral boundaries. This study proposes (i) a workflow that automatically\nextracts candidate cadastral boundaries from UAV orthoimages and (ii) a tool\nfor their semi-automatic processing to delineate final cadastral boundaries.\nThe workflow consists of two state-of-the-art computer vision methods, namely\ngPb contour detection and SLIC superpixels that are transferred to remote\nsensing in this study. The tool combines the two methods, allows a\nsemi-automatic final delineation and is implemented as a publicly available\nQGIS plugin. The approach does not yet aim to provide a comparable alternative\nto manual cadastral mapping procedures. However, the methodological development\nof the tool towards this goal is developed in this paper. A study with 13\nvolunteers investigates the design and implementation of the approach and\ngathers initial qualitative as well as quantitate results. The study revealed\npoints for improvement, which are prioritized based on the study results and\nwhich will be addressed in future work.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 13:24:41 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Crommelinck", "Sophie", ""], ["Yang", "Michael Ying", ""], ["Koeva", "Mila", ""], ["Gerke", "Markus", ""], ["Bennett", "Rohan", ""], ["Vosselman", "George", ""]]}, {"id": "1709.01829", "submitter": "Yi Zhu", "authors": "Yi Zhu, Yanzhao Zhou, Qixiang Ye, Qiang Qiu and Jianbin Jiao", "title": "Soft Proposal Networks for Weakly Supervised Object Localization", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object localization remains challenging, where only image\nlabels instead of bounding boxes are available during training. Object proposal\nis an effective component in localization, but often computationally expensive\nand incapable of joint optimization with some of the remaining modules. In this\npaper, to the best of our knowledge, we for the first time integrate weakly\nsupervised object proposal into convolutional neural networks (CNNs) in an\nend-to-end learning manner. We design a network component, Soft Proposal (SP),\nto be plugged into any standard convolutional architecture to introduce the\nnearly cost-free object proposal, orders of magnitude faster than\nstate-of-the-art methods. In the SP-augmented CNNs, referred to as Soft\nProposal Networks (SPNs), iteratively evolved object proposals are generated\nbased on the deep feature maps then projected back, and further jointly\noptimized with network parameters, with image-level supervision only. Through\nthe unified learning process, SPNs learn better object-centric filters,\ndiscover more discriminative visual evidence, and suppress background\ninterference, significantly boosting both weakly supervised object localization\nand classification performance. We report the best results on popular\nbenchmarks, including PASCAL VOC, MS COCO, and ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 14:11:59 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Zhu", "Yi", ""], ["Zhou", "Yanzhao", ""], ["Ye", "Qixiang", ""], ["Qiu", "Qiang", ""], ["Jiao", "Jianbin", ""]]}, {"id": "1709.01841", "submitter": "Qi Wei", "authors": "Qi Wei, Kai Fan, Lawrence Carin, Katherine A. Heller", "title": "An inner-loop free solution to inverse problems using deep neural\n  networks", "comments": null, "journal-ref": "2017 Conference on Neural Information Processing Systems (NIPS)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method that uses deep learning techniques to accelerate the\npopular alternating direction method of multipliers (ADMM) solution for inverse\nproblems. The ADMM updates consist of a proximity operator, a least squares\nregression that includes a big matrix inversion, and an explicit solution for\nupdating the dual variables. Typically, inner loops are required to solve the\nfirst two sub-minimization problems due to the intractability of the prior and\nthe matrix inversion. To avoid such drawbacks or limitations, we propose an\ninner-loop free update rule with two pre-trained deep convolutional\narchitectures. More specifically, we learn a conditional denoising auto-encoder\nwhich imposes an implicit data-dependent prior/regularization on ground-truth\nin the first sub-minimization problem. This design follows an empirical\nBayesian strategy, leading to so-called amortized inference. For matrix\ninversion in the second sub-problem, we learn a convolutional neural network to\napproximate the matrix inversion, i.e., the inverse mapping is learned by\nfeeding the input through the learned forward network. Note that training this\nneural network does not require ground-truth or measurements, i.e., it is\ndata-independent. Extensive experiments on both synthetic data and real\ndatasets demonstrate the efficiency and accuracy of the proposed method\ncompared with the conventional ADMM solution using inner loops for solving\ninverse problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 14:41:33 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 04:08:58 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 23:00:28 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Wei", "Qi", ""], ["Fan", "Kai", ""], ["Carin", "Lawrence", ""], ["Heller", "Katherine A.", ""]]}, {"id": "1709.01870", "submitter": "Sunrita Poddar", "authors": "Sunrita Poddar, Mathews Jacob", "title": "Clustering of Data with Missing Entries using Non-convex Fusion\n  Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of missing entries in data often creates challenges for pattern\nrecognition algorithms. Traditional algorithms for clustering data assume that\nall the feature values are known for every data point. We propose a method to\ncluster data in the presence of missing information. Unlike conventional\nclustering techniques where every feature is known for each point, our\nalgorithm can handle cases where a few feature values are unknown for every\npoint. For this more challenging problem, we provide theoretical guarantees for\nclustering using a $\\ell_0$ fusion penalty based optimization problem.\nFurthermore, we propose an algorithm to solve a relaxation of this problem\nusing saturating non-convex fusion penalties. It is observed that this\nalgorithm produces solutions that degrade gradually with an increase in the\nfraction of missing feature values. We demonstrate the utility of the proposed\nmethod using a simulated dataset, the Wine dataset and also an under-sampled\ncardiac MRI dataset. It is shown that the proposed method is a promising\nclustering technique for datasets with large fractions of missing entries.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 15:59:57 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Poddar", "Sunrita", ""], ["Jacob", "Mathews", ""]]}, {"id": "1709.01872", "submitter": "Tejpal Virdi", "authors": "John T. Guibas, Tejpal S. Virdi, Peter S. Li", "title": "Synthetic Medical Images from Dual Generative Adversarial Networks", "comments": "First two authors contributed equally. Accepted to NIPS 2017\n  Workshops on Medical Imaging and Machine Learning for Health", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently there is strong interest in data-driven approaches to medical image\nclassification. However, medical imaging data is scarce, expensive, and fraught\nwith legal concerns regarding patient privacy. Typical consent forms only allow\nfor patient data to be used in medical journals or education, meaning the\nmajority of medical data is inaccessible for general public research. We\npropose a novel, two-stage pipeline for generating synthetic medical images\nfrom a pair of generative adversarial networks, tested in practice on retinal\nfundi images. We develop a hierarchical generation process to divide the\ncomplex image generation task into two parts: geometry and photorealism. We\nhope researchers will use our pipeline to bring private medical data into the\npublic domain, sparking growth in imaging tasks that have previously relied on\nthe hand-tuning of models. We have begun this initiative through the\ndevelopment of SynthMed, an online repository for synthetic medical images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 16:07:30 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 05:01:18 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 20:32:51 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Guibas", "John T.", ""], ["Virdi", "Tejpal S.", ""], ["Li", "Peter S.", ""]]}, {"id": "1709.01889", "submitter": "Carlos Esteves", "authors": "Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, Kostas\n  Daniilidis", "title": "Polar Transformer Networks", "comments": "Accepted as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are inherently equivariant to\ntranslation. Efforts to embed other forms of equivariance have concentrated\nsolely on rotation. We expand the notion of equivariance in CNNs through the\nPolar Transformer Network (PTN). PTN combines ideas from the Spatial\nTransformer Network (STN) and canonical coordinate representations. The result\nis a network invariant to translation and equivariant to both rotation and\nscale. PTN is trained end-to-end and composed of three distinct stages: a polar\norigin predictor, the newly introduced polar transformer module and a\nclassifier. PTN achieves state-of-the-art on rotated MNIST and the newly\nintroduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and\nperturbing digits with translation, rotation and scaling. The ideas of PTN are\nextensible to 3D which we demonstrate through the Cylindrical Transformer\nNetwork.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 16:43:59 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 21:47:37 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 06:19:51 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Esteves", "Carlos", ""], ["Allen-Blanchette", "Christine", ""], ["Zhou", "Xiaowei", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1709.01921", "submitter": "Surat Teerapittayanon", "authors": "Surat Teerapittayanon, Bradley McDanel and H.T. Kung", "title": "Distributed Deep Neural Networks over the Cloud, the Edge and End\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose distributed deep neural networks (DDNNs) over distributed\ncomputing hierarchies, consisting of the cloud, the edge (fog) and end devices.\nWhile being able to accommodate inference of a deep neural network (DNN) in the\ncloud, a DDNN also allows fast and localized inference using shallow portions\nof the neural network at the edge and end devices. When supported by a scalable\ndistributed computing hierarchy, a DDNN can scale up in neural network size and\nscale out in geographical span. Due to its distributed nature, DDNNs enhance\nsensor fusion, system fault tolerance and data privacy for DNN applications. In\nimplementing a DDNN, we map sections of a DNN onto a distributed computing\nhierarchy. By jointly training these sections, we minimize communication and\nresource usage for devices and maximize usefulness of extracted features which\nare utilized in the cloud. The resulting system has built-in support for\nautomatic sensor fusion and fault tolerance. As a proof of concept, we show a\nDDNN can exploit geographical diversity of sensors to improve object\nrecognition accuracy and reduce communication cost. In our experiment, compared\nwith the traditional method of offloading raw sensor data to be processed in\nthe cloud, DDNN locally processes most sensor data on end devices while\nachieving high accuracy and is able to reduce the communication cost by a\nfactor of over 20x.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 04:00:18 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Teerapittayanon", "Surat", ""], ["McDanel", "Bradley", ""], ["Kung", "H. T.", ""]]}, {"id": "1709.01922", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, Gy\\\"orgy Fazekas, Kyunghyun Cho and Mark Sandler", "title": "A Comparison of Audio Signal Preprocessing Methods for Deep Neural\n  Networks on Music Tagging", "comments": "5 pages. EUSIPCO 2018 camera-ready. arXiv:1706.02361 does not have\n  the overlapped part with this submission anymore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we empirically investigate the effect of audio preprocessing\non music tagging with deep neural networks. We perform comprehensive\nexperiments involving audio preprocessing using different time-frequency\nrepresentations, logarithmic magnitude compression, frequency weighting, and\nscaling. We show that many commonly used input preprocessing techniques are\nredundant except magnitude compression.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 12:44:01 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 13:12:55 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 13:21:38 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "Gy\u00f6rgy", ""], ["Cho", "Kyunghyun", ""], ["Sandler", "Mark", ""]]}, {"id": "1709.01956", "submitter": "Yang He", "authors": "Yang He, Margret Keuper, Bernt Schiele, Mario Fritz", "title": "Learning Dilation Factors for Semantic Segmentation of Street Scenes", "comments": "GCPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual information is crucial for semantic segmentation. However, finding\nthe optimal trade-off between keeping desired fine details and at the same time\nproviding sufficiently large receptive fields is non trivial. This is even more\nso, when objects or classes present in an image significantly vary in size.\nDilated convolutions have proven valuable for semantic segmentation, because\nthey allow to increase the size of the receptive field without sacrificing\nimage resolution. However, in current state-of-the-art methods, dilation\nparameters are hand-tuned and fixed. In this paper, we present an approach for\nlearning dilation parameters adaptively per channel, consistently improving\nsemantic segmentation results on street-scene datasets like Cityscapes and\nCamvid.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 18:19:10 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["He", "Yang", ""], ["Keuper", "Margret", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1709.01993", "submitter": "Hao Zhou", "authors": "Hao Zhou and Jin Sun and Yaser Yacoob and David W. Jacobs", "title": "Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lighting estimation from face images is an important task and has\napplications in many areas such as image editing, intrinsic image\ndecomposition, and image forgery detection. We propose to train a deep\nConvolutional Neural Network (CNN) to regress lighting parameters from a single\nface image. Lacking massive ground truth lighting labels for face images in the\nwild, we use an existing method to estimate lighting parameters, which are\ntreated as ground truth with unknown noises. To alleviate the effect of such\nnoises, we utilize the idea of Generative Adversarial Networks (GAN) and\npropose a Label Denoising Adversarial Network (LDAN) to make use of synthetic\ndata with accurate ground truth to help train a deep CNN for lighting\nregression on real face images. Experiments show that our network outperforms\nexisting methods in producing consistent lighting parameters of different faces\nunder similar lighting conditions. Moreover, our method is 100,000 times faster\nin execution time than prior optimization-based lighting estimation approaches.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 20:46:17 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Zhou", "Hao", ""], ["Sun", "Jin", ""], ["Yacoob", "Yaser", ""], ["Jacobs", "David W.", ""]]}, {"id": "1709.02016", "submitter": "Ronald Salloum", "authors": "Ronald Salloum, Yuzhuo Ren and C.-C. Jay Kuo", "title": "Image Splicing Localization Using A Multi-Task Fully Convolutional\n  Network (MFCN)", "comments": "This manuscript was submitted for publication", "journal-ref": "Journal of Visual Communication and Image Representation, Volume\n  51, February 2018, Pages 201-209", "doi": "10.1016/j.jvcir.2018.01.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a technique that utilizes a fully convolutional\nnetwork (FCN) to localize image splicing attacks. We first evaluated a\nsingle-task FCN (SFCN) trained only on the surface label. Although the SFCN is\nshown to provide superior performance over existing methods, it still provides\na coarse localization output in certain cases. Therefore, we propose the use of\na multi-task FCN (MFCN) that utilizes two output branches for multi-task\nlearning. One branch is used to learn the surface label, while the other branch\nis used to learn the edge or boundary of the spliced region. We trained the\nnetworks using the CASIA v2.0 dataset, and tested the trained models on the\nCASIA v1.0, Columbia Uncompressed, Carvalho, and the DARPA/NIST Nimble\nChallenge 2016 SCI datasets. Experiments show that the SFCN and MFCN outperform\nexisting splicing localization algorithms, and that the MFCN can achieve finer\nlocalization than the SFCN.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 22:23:05 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Salloum", "Ronald", ""], ["Ren", "Yuzhuo", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1709.02033", "submitter": "Chuong Nguyen", "authors": "Chuong Nguyen, Matt Adcock, Stuart Anderson, David Lovell, Nicole\n  Fisher, John La Salle", "title": "Towards high-throughput 3D insect capture for species discovery and\n  diagnostics", "comments": "2 pages, 1 figure, for BigDig workshop at 2017 eScience conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitisation of natural history collections not only preserves precious\ninformation about biological diversity, it also enables us to share, analyse,\nannotate and compare specimens to gain new insights. High-resolution,\nfull-colour 3D capture of biological specimens yields color and geometry\ninformation complementary to other techniques (e.g., 2D capture, electron\nscanning and micro computed tomography). However 3D colour capture of small\nspecimens is slow for reasons including specimen handling, the narrow depth of\nfield of high magnification optics, and the large number of images required to\nresolve complex shapes of specimens. In this paper, we outline techniques to\naccelerate 3D image capture, including using a desktop robotic arm to automate\nthe insect handling process; using a calibrated pan-tilt rig to avoid attaching\ncalibration targets to specimens; using light field cameras to capture images\nat an extended depth of field in one shot; and using 3D Web and mixed reality\ntools to facilitate the annotation, distribution and visualisation of 3D\ndigital models.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 00:31:03 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Nguyen", "Chuong", ""], ["Adcock", "Matt", ""], ["Anderson", "Stuart", ""], ["Lovell", "David", ""], ["Fisher", "Nicole", ""], ["La Salle", "John", ""]]}, {"id": "1709.02039", "submitter": "Chuong Nguyen", "authors": "Chuong V. Nguyen, David R. Lovell, Matt Adcock, John La Salle", "title": "Capturing natural-colour 3D models of insects for species discovery", "comments": "24 pages, 17 figures, PLOS ONE journal", "journal-ref": "published 2014", "doi": "10.1371/journal.pone.0094346", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collections of biological specimens are fundamental to scientific\nunderstanding and characterization of natural diversity. This paper presents a\nsystem for liberating useful information from physical collections by bringing\nspecimens into the digital domain so they can be more readily shared, analyzed,\nannotated and compared. It focuses on insects and is strongly motivated by the\ndesire to accelerate and augment current practices in insect taxonomy which\npredominantly use text, 2D diagrams and images to describe and characterize\nspecies. While these traditional kinds of descriptions are informative and\nuseful, they cannot cover insect specimens \"from all angles\" and precious\nspecimens are still exchanged between researchers and collections for this\nreason. Furthermore, insects can be complex in structure and pose many\nchallenges to computer vision systems. We present a new prototype for a\npractical, cost-effective system of off-the-shelf components to acquire\nnatural-colour 3D models of insects from around 3mm to 30mm in length. Colour\nimages are captured from different angles and focal depths using a digital\nsingle lens reflex (DSLR) camera rig and two-axis turntable. These 2D images\nare processed into 3D reconstructions using software based on a visual hull\nalgorithm. The resulting models are compact (around 10 megabytes), afford\nexcellent optical resolution, and can be readily embedded into documents and\nweb pages, as well as viewed on mobile devices. The system is portable, safe,\nrelatively affordable, and complements the sort of volumetric data that can be\nacquired by computed tomography. This system provides a new way to augment the\ndescription and documentation of insect species holotypes, reducing the need to\nhandle or ship specimens. It opens up new opportunities to collect data for\nresearch, education, art, entertainment, biodiversity assessment and\nbiosecurity control.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 01:23:01 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Nguyen", "Chuong V.", ""], ["Lovell", "David R.", ""], ["Adcock", "Matt", ""], ["La Salle", "John", ""]]}, {"id": "1709.02043", "submitter": "Alexander Wong", "authors": "Audrey Chung, Mohammad Javad Shafiee, Paul Fieguth, and Alexander Wong", "title": "The Mating Rituals of Deep Neural Networks: Learning Compact Feature\n  Representations through Sexual Evolutionary Synthesis", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary deep intelligence was recently proposed as a method for\nachieving highly efficient deep neural network architectures over successive\ngenerations. Drawing inspiration from nature, we propose the incorporation of\nsexual evolutionary synthesis. Rather than the current asexual synthesis of\nnetworks, we aim to produce more compact feature representations by\nsynthesizing more diverse and generalizable offspring networks in subsequent\ngenerations via the combination of two parent networks. Experimental results\nwere obtained using the MNIST and CIFAR-10 datasets, and showed improved\narchitectural efficiency and comparable testing accuracy relative to the\nbaseline asexual evolutionary neural networks. In particular, the network\nsynthesized via sexual evolutionary synthesis for MNIST had approximately\ndouble the architectural efficiency (cluster efficiency of 34.29X and synaptic\nefficiency of 258.37X) in comparison to the network synthesized via asexual\nevolutionary synthesis, with both networks achieving a testing accuracy of\n~97%.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 01:43:19 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Chung", "Audrey", ""], ["Shafiee", "Mohammad Javad", ""], ["Fieguth", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "1709.02054", "submitter": "Zhanzhan Cheng", "authors": "Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu and\n  Shuigeng Zhou", "title": "Focusing Attention: Towards Accurate Text Recognition in Natural Images", "comments": "Revise the description of IC15 datasets (1811 samples)", "journal-ref": null, "doi": "10.1109/ICCV.2017.543", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Scene text recognition has been a hot research topic in computer vision due\nto its various applications. The state of the art is the attention-based\nencoder-decoder framework that learns the mapping between input images and\noutput sequences in a purely data-driven way. However, we observe that existing\nattention-based methods perform poorly on complicated and/or low-quality\nimages. One major reason is that existing methods cannot get accurate\nalignments between feature areas and targets for such images. We call this\nphenomenon \"attention drift\". To tackle this problem, in this paper we propose\nthe FAN (the abbreviation of Focusing Attention Network) method that employs a\nfocusing attention mechanism to automatically draw back the drifted attention.\nFAN consists of two major components: an attention network (AN) that is\nresponsible for recognizing character targets as in the existing methods, and a\nfocusing network (FN) that is responsible for adjusting attention by evaluating\nwhether AN pays attention properly on the target areas in the images.\nFurthermore, different from the existing methods, we adopt a ResNet-based\nnetwork to enrich deep representations of scene text images. Extensive\nexperiments on various benchmarks, including the IIIT5k, SVT and ICDAR\ndatasets, show that the FAN method substantially outperforms the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 02:58:01 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 01:54:04 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 04:54:55 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Cheng", "Zhanzhan", ""], ["Bai", "Fan", ""], ["Xu", "Yunlu", ""], ["Zheng", "Gang", ""], ["Pu", "Shiliang", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "1709.02073", "submitter": "Lei Xiang", "authors": "Lei Xiang, Qian Wang, Xiyao Jin, Dong Nie, Yu Qiao, Dinggang Shen", "title": "Deep Embedding Convolutional Neural Network for Synthesizing CT Image\n  from T1-Weighted MR Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, more and more attention is drawn to the field of medical image\nsynthesis across modalities. Among them, the synthesis of computed tomography\n(CT) image from T1-weighted magnetic resonance (MR) image is of great\nimportance, although the mapping between them is highly complex due to large\ngaps of appearances of the two modalities. In this work, we aim to tackle this\nMR-to-CT synthesis by a novel deep embedding convolutional neural network\n(DECNN). Specifically, we generate the feature maps from MR images, and then\ntransform these feature maps forward through convolutional layers in the\nnetwork. We can further compute a tentative CT synthesis from the midway of the\nflow of feature maps, and then embed this tentative CT synthesis back to the\nfeature maps. This embedding operation results in better feature maps, which\nare further transformed forward in DECNN. After repeat-ing this embedding\nprocedure for several times in the network, we can eventually synthesize a\nfinal CT image in the end of the DECNN. We have validated our proposed method\non both brain and prostate datasets, by also compar-ing with the\nstate-of-the-art methods. Experimental results suggest that our DECNN (with\nrepeated embedding op-erations) demonstrates its superior performances, in\nterms of both the perceptive quality of the synthesized CT image and the\nrun-time cost for synthesizing a CT image.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 05:20:26 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 04:06:13 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Xiang", "Lei", ""], ["Wang", "Qian", ""], ["Jin", "Xiyao", ""], ["Nie", "Dong", ""], ["Qiao", "Yu", ""], ["Shen", "Dinggang", ""]]}, {"id": "1709.02081", "submitter": "Ha Tran Hong Phan", "authors": "Ha Tran Hong Phan, Ashnil Kumar, David Feng, Michael Fulham, Jinman\n  Kim", "title": "An unsupervised long short-term memory neural network for event\n  detection in cell videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic unsupervised cell event detection and classification\nmethod, which expands convolutional Long Short-Term Memory (LSTM) neural\nnetworks, for cellular events in cell video sequences. Cells in images that are\ncaptured from various biomedical applications usually have different shapes and\nmotility, which pose difficulties for the automated event detection in cell\nvideos. Current methods to detect cellular events are based on supervised\nmachine learning and rely on tedious manual annotation from investigators with\nspecific expertise. So that our LSTM network could be trained in an\nunsupervised manner, we designed it with a branched structure where one branch\nlearns the frequent, regular appearance and movements of objects and the second\nlearns the stochastic events, which occur rarely and without warning in a cell\nvideo sequence. We tested our network on a publicly available dataset of\ndensely packed stem cell phase-contrast microscopy images undergoing cell\ndivision. This dataset is considered to be more challenging that a dataset with\nsparse cells. We compared our method to several published supervised methods\nevaluated on the same dataset and to a supervised LSTM method with a similar\ndesign and configuration to our unsupervised method. We used an F1-score, which\nis a balanced measure for both precision and recall. Our results show that our\nunsupervised method has a higher or similar F1-score when compared to two fully\nsupervised methods that are based on Hidden Conditional Random Fields (HCRF),\nand has comparable accuracy with the current best supervised HCRF-based method.\nOur method was generalizable as after being trained on one video it could be\napplied to videos where the cells were in different conditions. The accuracy of\nour unsupervised method approached that of its supervised counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 05:58:23 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Phan", "Ha Tran Hong", ""], ["Kumar", "Ashnil", ""], ["Feng", "David", ""], ["Fulham", "Michael", ""], ["Kim", "Jinman", ""]]}, {"id": "1709.02123", "submitter": "Zhizhong Li", "authors": "Zhizhong Li and Dahua Lin", "title": "Integrating Specialized Classifiers Based on Continuous Time Markov\n  Chain", "comments": "Published at IJCAI-17, typo fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized classifiers, namely those dedicated to a subset of classes, are\noften adopted in real-world recognition systems. However, integrating such\nclassifiers is nontrivial. Existing methods, e.g. weighted average, usually\nimplicitly assume that all constituents of an ensemble cover the same set of\nclasses. Such methods can produce misleading predictions when used to combine\nspecialized classifiers. This work explores a novel approach. Instead of\ncombining predictions from individual classifiers directly, it first decomposes\nthe predictions into sets of pairwise preferences, treating them as transition\nchannels between classes, and thereon constructs a continuous-time Markov\nchain, and use the equilibrium distribution of this chain as the final\nprediction. This way allows us to form a coherent picture over all specialized\npredictions. On large public datasets, the proposed method obtains considerable\nimprovement compared to mainstream ensemble methods, especially when the\nclassifier coverage is highly unbalanced.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 07:56:30 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Li", "Zhizhong", ""], ["Lin", "Dahua", ""]]}, {"id": "1709.02142", "submitter": "Anders Glent Buch", "authors": "Anders Glent Buch, Lilita Kiforenko, Dirk Kraft", "title": "Rotational Subgroup Voting and Pose Clustering for Robust 3D Object\n  Recognition", "comments": "Accepted for International Conference on Computer Vision (ICCV), 2017", "journal-ref": "2017 IEEE International Conference on Computer Vision (ICCV)", "doi": "10.1109/iccv.2017.443", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is possible to associate a highly constrained subset of relative 6 DoF\nposes between two 3D shapes, as long as the local surface orientation, the\nnormal vector, is available at every surface point. Local shape features can be\nused to find putative point correspondences between the models due to their\nability to handle noisy and incomplete data. However, this correspondence set\nis usually contaminated by outliers in practical scenarios, which has led to\nmany past contributions based on robust detectors such as the Hough transform\nor RANSAC. The key insight of our work is that a single correspondence between\noriented points on the two models is constrained to cast votes in a 1 DoF\nrotational subgroup of the full group of poses, SE(3). Kernel density\nestimation allows combining the set of votes efficiently to determine a full 6\nDoF candidate pose between the models. This modal pose with the highest density\nis stable under challenging conditions, such as noise, clutter, and occlusions,\nand provides the output estimate of our method.\n  We first analyze the robustness of our method in relation to noise and show\nthat it handles high outlier rates much better than RANSAC for the task of 6\nDoF pose estimation. We then apply our method to four state of the art data\nsets for 3D object recognition that contain occluded and cluttered scenes. Our\nmethod achieves perfect recall on two LIDAR data sets and outperforms competing\nmethods on two RGB-D data sets, thus setting a new standard for general 3D\nobject recognition using point cloud data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 08:54:20 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Buch", "Anders Glent", ""], ["Kiforenko", "Lilita", ""], ["Kraft", "Dirk", ""]]}, {"id": "1709.02150", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro", "title": "Improving Sonar Image Patch Matching via Deep Learning", "comments": "Author version", "journal-ref": "Proceedings of the European Conference on Mobile Robotics 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching sonar images with high accuracy has been a problem for a long time,\nas sonar images are inherently hard to model due to reflections, noise and\nviewpoint dependence. Autonomous Underwater Vehicles require good sonar image\nmatching capabilities for tasks such as tracking, simultaneous localization and\nmapping (SLAM) and some cases of object detection/recognition. We propose the\nuse of Convolutional Neural Networks (CNN) to learn a matching function that\ncan be trained from labeled sonar data, after pre-processing to generate\nmatching and non-matching pairs. In a dataset of 39K training pairs, we obtain\n0.91 Area under the ROC Curve (AUC) for a CNN that outputs a binary\nclassification matching decision, and 0.89 AUC for another CNN that outputs a\nmatching score. In comparison, classical keypoint matching methods like SIFT,\nSURF, ORB and AKAZE obtain AUC 0.61 to 0.68. Alternative learning methods\nobtain similar results, with a Random Forest Classifier obtaining AUC 0.79, and\na Support Vector Machine resulting in AUC 0.66.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 09:25:58 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Valdenegro-Toro", "Matias", ""]]}, {"id": "1709.02153", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro", "title": "Real-time convolutional networks for sonar image classification in\n  low-power embedded systems", "comments": "Author version", "journal-ref": "ESANN 2017 proceedings", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have impressive classification performance, but this\ncomes at the expense of significant computational resources at inference time.\nAutonomous Underwater Vehicles use low-power embedded systems for sonar image\nperception, and cannot execute large neural networks in real-time. We propose\nthe use of max-pooling aggressively, and we demonstrate it with a Fire-based\nmodule and a new Tiny module that includes max-pooling in each module. By\nstacking them we build networks that achieve the same accuracy as bigger ones,\nwhile reducing the number of parameters and considerably increasing\ncomputational performance. Our networks can classify a 96x96 sonar image with\n98.8 - 99.7 accuracy on only 41 to 61 milliseconds on a Raspberry Pi 2, which\ncorresponds to speedups of 28.6 - 19.7.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 09:36:06 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Valdenegro-Toro", "Matias", ""]]}, {"id": "1709.02228", "submitter": "Yao Tang", "authors": "Yao Tang, Fei Gao, Jufu Feng, Yuhang Liu", "title": "FingerNet: An Unified Deep Network for Fingerprint Minutiae Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minutiae extraction is of critical importance in automated fingerprint\nrecognition. Previous works on rolled/slap fingerprints failed on latent\nfingerprints due to noisy ridge patterns and complex background noises. In this\npaper, we propose a new way to design deep convolutional network combining\ndomain knowledge and the representation ability of deep learning. In terms of\norientation estimation, segmentation, enhancement and minutiae extraction,\nseveral typical traditional methods performed well on rolled/slap fingerprints\nare transformed into convolutional manners and integrated as an unified plain\nnetwork. We demonstrate that this pipeline is equivalent to a shallow network\nwith fixed weights. The network is then expanded to enhance its representation\nability and the weights are released to learn complex background variance from\ndata, while preserving end-to-end differentiability. Experimental results on\nNIST SD27 latent database and FVC 2004 slap database demonstrate that the\nproposed algorithm outperforms the state-of-the-art minutiae extraction\nalgorithms. Code is made publicly available at:\nhttps://github.com/felixTY/FingerNet.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 13:31:38 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Tang", "Yao", ""], ["Gao", "Fei", ""], ["Feng", "Jufu", ""], ["Liu", "Yuhang", ""]]}, {"id": "1709.02235", "submitter": "Shahar Tsiper Mr.", "authors": "Shahar Tsiper, Or Dicker, Idan Kaizerman, Zeev Zohar, Mordechai Segev\n  and Yonina C. Eldar", "title": "Sparsity-Based Super Resolution for SEM Images", "comments": "Final publication available at ACS Nano Letters", "journal-ref": null, "doi": "10.1021/acs.nanolett.7b02091", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scanning electron microscope (SEM) produces an image of a sample by\nscanning it with a focused beam of electrons. The electrons interact with the\natoms in the sample, which emit secondary electrons that contain information\nabout the surface topography and composition. The sample is scanned by the\nelectron beam point by point, until an image of the surface is formed. Since\nits invention in 1942, SEMs have become paramount in the discovery and\nunderstanding of the nanometer world, and today it is extensively used for both\nresearch and in industry. In principle, SEMs can achieve resolution better than\none nanometer. However, for many applications, working at sub-nanometer\nresolution implies an exceedingly large number of scanning points. For exactly\nthis reason, the SEM diagnostics of microelectronic chips is performed either\nat high resolution (HR) over a small area or at low resolution (LR) while\ncapturing a larger portion of the chip. Here, we employ sparse coding and\ndictionary learning to algorithmically enhance LR SEM images of microelectronic\nchips up to the level of the HR images acquired by slow SEM scans, while\nconsiderably reducing the noise. Our methodology consists of two steps: an\noffline stage of learning a joint dictionary from a sequence of LR and HR\nimages of the same region in the chip, followed by a fast-online\nsuper-resolution step where the resolution of a new LR image is enhanced. We\nprovide several examples with typical chips used in the microelectronics\nindustry, as well as a statistical study on arbitrary images with\ncharacteristic structural features. Conceptually, our method works well when\nthe images have similar characteristics. This work demonstrates that employing\nsparsity concepts can greatly improve the performance of SEM, thereby\nconsiderably increasing the scanning throughput without compromising on\nanalysis quality and resolution.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 16:23:43 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Tsiper", "Shahar", ""], ["Dicker", "Or", ""], ["Kaizerman", "Idan", ""], ["Zohar", "Zeev", ""], ["Segev", "Mordechai", ""], ["Eldar", "Yonina C.", ""]]}, {"id": "1709.02236", "submitter": "Andrea Gigli", "authors": "Andrea Gigli, Arjan Gijsberts, Valentina Gregori, Matteo Cognolato,\n  Manfredo Atzori, Barbara Caputo", "title": "Visual Cues to Improve Myoelectric Control of Upper Limb Prostheses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instability of myoelectric signals over time complicates their use to\ncontrol highly articulated prostheses. To address this problem, studies have\ntried to combine surface electromyography with modalities that are less\naffected by the amputation and environment, such as accelerometry or gaze\ninformation. In the latter case, the hypothesis is that a subject looks at the\nobject he or she intends to manipulate and that knowing this object's\naffordances allows to constrain the set of possible grasps. In this paper, we\ndevelop an automated way to detect stable fixations and show that gaze\ninformation is indeed helpful in predicting hand movements. In our multimodal\napproach, we automatically detect stable gazes and segment an object of\ninterest around the subject's fixation in the visual frame. The patch extracted\naround this object is subsequently fed through an off-the-shelf deep\nconvolutional neural network to obtain a high level feature representation,\nwhich is then combined with traditional surface electromyography in the\nclassification stage. Tests have been performed on a dataset acquired from five\nintact subjects who performed ten types of grasps on various objects as well as\nin a functional setting. They show that the addition of gaze information\nincreases the classification accuracy considerably. Further analysis\ndemonstrates that this improvement is consistent for all grasps and\nconcentrated during the movement onset and offset.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 16:59:19 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Gigli", "Andrea", ""], ["Gijsberts", "Arjan", ""], ["Gregori", "Valentina", ""], ["Cognolato", "Matteo", ""], ["Atzori", "Manfredo", ""], ["Caputo", "Barbara", ""]]}, {"id": "1709.02243", "submitter": "Sultan Khan Daud", "authors": "Sultan Daud Khan, Muhammad Saqib, Michael Blumenstein", "title": "Towards a Dedicated Computer Vision Tool set for Crowd Simulation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the population of world is increasing, and even more concentrated in urban\nareas, ensuring public safety is becoming a taunting job for security personnel\nand crowd managers. Mass events like sports, festivals, concerts, political\ngatherings attract thousand of people in a constraint environment,therefore\nadequate safety measures should be adopted. Despite safety measures, crowd\ndisasters still occur frequently. Understanding underlying dynamics and\nbehavior of crowd is becoming areas of interest for most of computer\nscientists. In recent years, researchers developed several models for\nunderstanding crowd dynamics. These models should be properly calibrated and\nvalidated by means of data acquired in the field. In this paper, we developed a\ncomputer vision tool set that can be helpful not only in initializing the crowd\nsimulation models but can also validate the simulation results. The main\nfeatures of proposed tool set are: (1) Crowd flow segmentation and crowd\ncounting, (2) Identifying source/sink location for understanding crowd\nbehavior, (3) Group detection and tracking in crowds.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 04:02:36 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Khan", "Sultan Daud", ""], ["Saqib", "Muhammad", ""], ["Blumenstein", "Michael", ""]]}, {"id": "1709.02245", "submitter": "Nour Eldeen Khalifa PhD", "authors": "Nour Eldeen M. Khalifa, Mohamed Hamed N. Taha, Aboul Ella Hassanien,\n  I. M. Selim", "title": "Deep Galaxy: Classification of Galaxies based on Deep Convolutional\n  Neural Networks", "comments": "4 pages, 6 figures, 2 tables, Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a deep convolutional neural network architecture for galaxies\nclassification is presented. The galaxy can be classified based on its features\ninto main three categories Elliptical, Spiral, and Irregular. The proposed deep\ngalaxies architecture consists of 8 layers, one main convolutional layer for\nfeatures extraction with 96 filters, followed by two principles fully connected\nlayers for classification. It is trained over 1356 images and achieved 97.272%\nin testing accuracy. A comparative result is made and the testing accuracy was\ncompared with other related works. The proposed architecture outperformed other\nrelated works in terms of testing accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 11:14:43 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Khalifa", "Nour Eldeen M.", ""], ["Taha", "Mohamed Hamed N.", ""], ["Hassanien", "Aboul Ella", ""], ["Selim", "I. M.", ""]]}, {"id": "1709.02246", "submitter": "Wenye He", "authors": "Wenye He", "title": "A Survey of Efficient Regression of General-Activity Human Poses from\n  Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comprehensive review on regression-based method for\nhuman pose estimation. The problem of human pose estimation has been\nintensively studied and enabled many application from entertainment to\ntraining. Traditional methods often rely on color image only which cannot\ncompletely ambiguity of joint 3D position, especially in the complex context.\nWith the popularity of depth sensors, the precision of 3D estimation has\nsignificant improvement. In this paper, we give a detailed analysis of\nstate-of-the-art on human pose estimation, including depth image based and\nRGB-D based approaches. The experimental results demonstrate their advantages\nand limitation for different scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 14:32:08 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["He", "Wenye", ""]]}, {"id": "1709.02247", "submitter": "Saed Khawaldeh", "authors": "Saed Khawaldeh, Tajwar Abrar Aleef, Usama Pervaiz, Vu Hoang Minh and\n  Yeman Brhane Hagos", "title": "Complete End-To-End Low Cost Solution To a 3D Scanning System with\n  Integrated Turntable", "comments": "22 pages", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT), 9, 39-55 (2017)", "doi": "10.5121/ijcsit.2017.9404", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction is a technique used in computer vision which has a wide\nrange of applications in areas like object recognition, city modelling, virtual\nreality, physical simulations, video games and special effects. Previously, to\nperform a 3D reconstruction, specialized hardwares were required. Such systems\nwere often very expensive and was only available for industrial or research\npurpose. With the rise of the availability of high-quality low cost 3D sensors,\nit is now possible to design inexpensive complete 3D scanning systems. The\nobjective of this work was to design an acquisition and processing system that\ncan perform 3D scanning and reconstruction of objects seamlessly. In addition,\nthe goal of this work also included making the 3D scanning process fully\nautomated by building and integrating a turntable alongside the software. This\nmeans the user can perform a full 3D scan only by a press of a few buttons from\nour dedicated graphical user interface. Three main steps were followed to go\nfrom acquisition of point clouds to the finished reconstructed 3D model. First,\nour system acquires point cloud data of a person/object using inexpensive\ncamera sensor. Second, align and convert the acquired point cloud data into a\nwatertight mesh of good quality. Third, export the reconstructed model to a 3D\nprinter to obtain a proper 3D print of the model.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 13:40:23 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Khawaldeh", "Saed", ""], ["Aleef", "Tajwar Abrar", ""], ["Pervaiz", "Usama", ""], ["Minh", "Vu Hoang", ""], ["Hagos", "Yeman Brhane", ""]]}, {"id": "1709.02249", "submitter": "Sungjoon Choi", "authors": "Sungjoon Choi, Kyungjae Lee, Sungbin Lim, Songhwai Oh", "title": "Uncertainty-Aware Learning from Demonstration using Mixture Density\n  Networks with Sampling-Free Variance Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an uncertainty-aware learning from demonstration\nmethod by presenting a novel uncertainty estimation method utilizing a mixture\ndensity network appropriate for modeling complex and noisy human behaviors. The\nproposed uncertainty acquisition can be done with a single forward path without\nMonte Carlo sampling and is suitable for real-time robotics applications. The\nproperties of the proposed uncertainty measure are analyzed through three\ndifferent synthetic examples, absence of data, heavy measurement noise, and\ncomposition of functions scenarios. We show that each case can be distinguished\nusing the proposed uncertainty measure and presented an uncertainty-aware\nlearn- ing from demonstration method of an autonomous driving using this\nproperty. The proposed uncertainty-aware learning from demonstration method\noutperforms other compared methods in terms of safety using a complex\nreal-world driving dataset.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 18:57:54 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 06:11:36 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Choi", "Sungjoon", ""], ["Lee", "Kyungjae", ""], ["Lim", "Sungbin", ""], ["Oh", "Songhwai", ""]]}, {"id": "1709.02250", "submitter": "Syed Anwar", "authors": "Syed Muhammad Anwar, Muhammad Majid, Adnan Qayyum, Muhammad Awais,\n  Majdi Alnowami, Muhammad Khurram Khan", "title": "Medical Image Analysis using Convolutional Neural Networks: A Review", "comments": null, "journal-ref": "Journal of Medical Systems (2018)", "doi": "10.1007/s10916-018-1088-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The science of solving clinical problems by analyzing images generated in\nclinical practice is known as medical image analysis. The aim is to extract\ninformation in an effective and efficient manner for improved clinical\ndiagnosis. The recent advances in the field of biomedical engineering has made\nmedical image analysis one of the top research and development area. One of the\nreason for this advancement is the application of machine learning techniques\nfor the analysis of medical images. Deep learning is successfully used as a\ntool for machine learning, where a neural network is capable of automatically\nlearning features. This is in contrast to those methods where traditionally\nhand crafted features are used. The selection and calculation of these features\nis a challenging task. Among deep learning techniques, deep convolutional\nnetworks are actively used for the purpose of medical image analysis. This\ninclude application areas such as segmentation, abnormality detection, disease\nclassification, computer aided diagnosis and retrieval. In this study, a\ncomprehensive review of the current state-of-the-art in medical image analysis\nusing deep convolutional networks is presented. The challenges and potential of\nthese techniques are also highlighted.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 08:37:28 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 05:57:58 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Anwar", "Syed Muhammad", ""], ["Majid", "Muhammad", ""], ["Qayyum", "Adnan", ""], ["Awais", "Muhammad", ""], ["Alnowami", "Majdi", ""], ["Khan", "Muhammad Khurram", ""]]}, {"id": "1709.02251", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Qin Jin", "title": "Multi-modal Conditional Attention Fusion for Dimensional Emotion\n  Prediction", "comments": "Appeared at ACM Multimedia 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous dimensional emotion prediction is a challenging task where the\nfusion of various modalities usually achieves state-of-the-art performance such\nas early fusion or late fusion. In this paper, we propose a novel multi-modal\nfusion strategy named conditional attention fusion, which can dynamically pay\nattention to different modalities at each time step. Long-short term memory\nrecurrent neural networks (LSTM-RNN) is applied as the basic uni-modality model\nto capture long time dependencies. The weights assigned to different modalities\nare automatically decided by the current input features and recent history\ninformation rather than being fixed at any kinds of situation. Our experimental\nresults on a benchmark dataset AVEC2015 show the effectiveness of our method\nwhich outperforms several common fusion strategies for valence prediction.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 12:05:47 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Chen", "Shizhe", ""], ["Jin", "Qin", ""]]}, {"id": "1709.02252", "submitter": "Carlos Alberto Lara-Alvarez", "authors": "Carlos Lara-Alvarez, Tania Reyes", "title": "A Geometric Approach to Harmonic Color Palette Design", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of finding harmonic colors, this problem has many\napplications, from fashion to industrial design. In order to solve this problem\nwe consider that colors follow normal distributions in tone (chroma and\nlightness) and hue. The proposed approach relies in the CIE standard for\nrepresenting colors and evaluate proximity. Other approaches to this problem\nuse a set of rules. Experimental results show that lines with specific\nparameters angles of inclination, and distance from the reference point are\npreferred over others, and that uncertain line patterns outperform non-linear\npatterns.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 15:18:29 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Lara-Alvarez", "Carlos", ""], ["Reyes", "Tania", ""]]}, {"id": "1709.02253", "submitter": "Faxian Cao", "authors": "Faxian Cao, Zhijing Yang, Jinchang Ren, Mengying Jiang, Wing-Kuen Ling", "title": "Linear vs Nonlinear Extreme Learning Machine for Spectral-Spatial\n  Classification of Hyperspectral Image", "comments": "13 pages,8 figures,3 tables,article", "journal-ref": "Sensors,17,2017,2603", "doi": "10.3390/s17112603", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a new machine learning approach, extreme learning machine (ELM) has\nreceived wide attentions due to its good performances. However, when directly\napplied to the hyperspectral image (HSI) classification, the recognition rate\nis too low. This is because ELM does not use the spatial information which is\nvery important for HSI classification. In view of this, this paper proposes a\nnew framework for spectral-spatial classification of HSI by combining ELM with\nloopy belief propagation (LBP). The original ELM is linear, and the nonlinear\nELMs (or Kernel ELMs) are the improvement of linear ELM (LELM). However, based\non lots of experiments and analysis, we found out that the LELM is a better\nchoice than nonlinear ELM for spectral-spatial classification of HSI.\nFurthermore, we exploit the marginal probability distribution that uses the\nwhole information in the HSI and learn such distribution using the LBP. The\nproposed method not only maintain the fast speed of ELM, but also greatly\nimproves the accuracy of classification. The experimental results in the\nwell-known HSI data sets, Indian Pines and Pavia University, demonstrate the\ngood performances of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 06:53:02 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 02:16:40 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Cao", "Faxian", ""], ["Yang", "Zhijing", ""], ["Ren", "Jinchang", ""], ["Jiang", "Mengying", ""], ["Ling", "Wing-Kuen", ""]]}, {"id": "1709.02255", "submitter": "Yipeng Hu", "authors": "Yipeng Hu, Eli Gibson, Tom Vercauteren, Hashim U. Ahmed, Mark\n  Emberton, Caroline M. Moore, J. Alison Noble, Dean C. Barratt", "title": "Intraoperative Organ Motion Models with an Ensemble of Conditional\n  Generative Adversarial Networks", "comments": "Accepted to MICCAI 2017", "journal-ref": null, "doi": "10.1007/978-3-319-66185-8_42", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe how a patient-specific, ultrasound-probe-induced\nprostate motion model can be directly generated from a single preoperative MR\nimage. Our motion model allows for sampling from the conditional distribution\nof dense displacement fields, is encoded by a generative neural network\nconditioned on a medical image, and accepts random noise as additional input.\nThe generative network is trained by a minimax optimisation with a second\ndiscriminative neural network, tasked to distinguish generated samples from\ntraining motion data. In this work, we propose that 1) jointly optimising a\nthird conditioning neural network that pre-processes the input image, can\neffectively extract patient-specific features for conditioning; and 2)\ncombining multiple generative models trained separately with heuristically\npre-disjointed training data sets can adequately mitigate the problem of mode\ncollapse. Trained with diagnostic T2-weighted MR images from 143 real patients\nand 73,216 3D dense displacement fields from finite element simulations of\nintraoperative prostate motion due to transrectal ultrasound probe pressure,\nthe proposed models produced physically-plausible patient-specific motion of\nprostate glands. The ability to capture biomechanically simulated motion was\nevaluated using two errors representing generalisability and specificity of the\nmodel. The median values, calculated from a 10-fold cross-validation, were\n2.8+/-0.3 mm and 1.7+/-0.1 mm, respectively. We conclude that the introduced\napproach demonstrates the feasibility of applying state-of-the-art machine\nlearning algorithms to generate organ motion models from patient images, and\nshows significant promise for future research.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 21:14:39 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Hu", "Yipeng", ""], ["Gibson", "Eli", ""], ["Vercauteren", "Tom", ""], ["Ahmed", "Hashim U.", ""], ["Emberton", "Mark", ""], ["Moore", "Caroline M.", ""], ["Noble", "J. Alison", ""], ["Barratt", "Dean C.", ""]]}, {"id": "1709.02260", "submitter": "Surat Teerapittayanon", "authors": "Bradley McDanel, Surat Teerapittayanon, H.T. Kung", "title": "Embedded Binarized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study embedded Binarized Neural Networks (eBNNs) with the aim of allowing\ncurrent binarized neural networks (BNNs) in the literature to perform\nfeedforward inference efficiently on small embedded devices. We focus on\nminimizing the required memory footprint, given that these devices often have\nmemory as small as tens of kilobytes (KB). Beyond minimizing the memory\nrequired to store weights, as in a BNN, we show that it is essential to\nminimize the memory used for temporaries which hold intermediate results\nbetween layers in feedforward inference. To accomplish this, eBNN reorders the\ncomputation of inference while preserving the original BNN structure, and uses\njust a single floating-point temporary for the entire neural network. All\nintermediate results from a layer are stored as binary values, as opposed to\nfloating-points used in current BNN implementations, leading to a 32x reduction\nin required temporary space. We provide empirical evidence that our proposed\neBNN approach allows efficient inference (10s of ms) on devices with severely\nlimited memory (10s of KB). For example, eBNN achieves 95\\% accuracy on the\nMNIST dataset running on an Intel Curie with only 15 KB of usable memory with\nan inference runtime of under 50 ms per sample. To ease the development of\napplications in embedded contexts, we make our source code available that\nallows users to train and discover eBNN models for a learning task at hand,\nwhich fit within the memory constraint of the target device.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 06:45:33 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["McDanel", "Bradley", ""], ["Teerapittayanon", "Surat", ""], ["Kung", "H. T.", ""]]}, {"id": "1709.02270", "submitter": "Shadrokh Samavi", "authors": "Zohreh HosseinKhani, Mohsen Hajabdollahi, Nader Karimi, Reza\n  Soroushmehr, Shahram Shirani, Kayvan Najarian, Shadrokh Samavi", "title": "Adaptive Real-Time Removal of Impulse Noise in Medical Images", "comments": "9 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is an important factor that degrades the quality of medical images.\nImpulse noise is a common noise, which is caused by malfunctioning of sensor\nelements or errors in the transmission of images. In medical images due to\npresence of white foreground and black background, many pixels have intensities\nsimilar to impulse noise and distinction between noisy and regular pixels is\ndifficult. In software techniques, the accuracy of the noise removal is more\nimportant than the algorithm's complexity. But for hardware implementation\nhaving a low complexity algorithm with an acceptable accuracy is essential. In\nthis paper a low complexity de-noising method is proposed that removes the\nnoise by local analysis of the image blocks. The proposed method distinguishes\nnon-noisy pixels that have noise-like intensities. All steps are designed to\nhave low hardware complexity. Simulation results show that for different\nmagnetic resonance images, the proposed method removes impulse noise with an\nacceptable accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 14:37:20 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 09:06:38 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["HosseinKhani", "Zohreh", ""], ["Hajabdollahi", "Mohsen", ""], ["Karimi", "Nader", ""], ["Soroushmehr", "Reza", ""], ["Shirani", "Shahram", ""], ["Najarian", "Kayvan", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1709.02285", "submitter": "Darius Burschka", "authors": "Darius Burschka", "title": "Monocular Navigation in Large Scale Dynamic Environments", "comments": "2017 British Machine Vision Conference, London (BMVC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a processing technique for a robust reconstruction of motion\nproperties for single points in large scale, dynamic environments. We assume\nthat the acquisition camera is moving and that there are other independently\nmoving agents in a large environment, like road scenarios. The separation of\ndirection and magnitude of the reconstructed motion allows for robust\nreconstruction of the dynamic state of the objects in situations, where\nconventional binocular systems fail due to a small signal (disparity) from the\nimages due to a constant detection error, and where structure from motion\napproaches fail due to unobserved motion of other agents between the camera\nframes.\n  We present the mathematical framework and the sensitivity analysis for the\nresulting system.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 14:46:45 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Burschka", "Darius", ""]]}, {"id": "1709.02287", "submitter": "Patricia Binder", "authors": "Patricia Binder, Michael Muma, Abdelhak M. Zoubir", "title": "Gravitational Clustering: A Simple, Robust and Adaptive Approach for\n  Distributed Networks", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed signal processing for wireless sensor networks enables that\ndifferent devices cooperate to solve different signal processing tasks. A\ncrucial first step is to answer the question: who observes what? Recently,\nseveral distributed algorithms have been proposed, which frame the\nsignal/object labelling problem in terms of cluster analysis after extracting\nsource-specific features, however, the number of clusters is assumed to be\nknown. We propose a new method called Gravitational Clustering (GC) to\nadaptively estimate the time-varying number of clusters based on a set of\nfeature vectors. The key idea is to exploit the physical principle of\ngravitational force between mass units: streaming-in feature vectors are\nconsidered as mass units of fixed position in the feature space, around which\nmobile mass units are injected at each time instant. The cluster enumeration\nexploits the fact that the highest attraction on the mobile mass units is\nexerted by regions with a high density of feature vectors, i.e., gravitational\nclusters. By sharing estimates among neighboring nodes via a\ndiffusion-adaptation scheme, cooperative and distributed cluster enumeration is\nachieved. Numerical experiments concerning robustness against outliers,\nconvergence and computational complexity are conducted. The application in a\ndistributed cooperative multi-view camera network illustrates the applicability\nto real-world problems.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 12:05:17 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Binder", "Patricia", ""], ["Muma", "Michael", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "1709.02371", "submitter": "Deqing Sun", "authors": "Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz", "title": "PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume", "comments": "CVPR 2018 camera ready version (with github link to Caffe and PyTorch\n  code)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a compact but effective CNN model for optical flow, called\nPWC-Net. PWC-Net has been designed according to simple and well-established\nprinciples: pyramidal processing, warping, and the use of a cost volume. Cast\nin a learnable feature pyramid, PWC-Net uses the cur- rent optical flow\nestimate to warp the CNN features of the second image. It then uses the warped\nfeatures and features of the first image to construct a cost volume, which is\nprocessed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in\nsize and easier to train than the recent FlowNet2 model. Moreover, it\noutperforms all published optical flow methods on the MPI Sintel final pass and\nKITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436)\nimages. Our models are available on https://github.com/NVlabs/PWC-Net.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 17:47:59 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 15:52:03 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 20:34:58 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Sun", "Deqing", ""], ["Yang", "Xiaodong", ""], ["Liu", "Ming-Yu", ""], ["Kautz", "Jan", ""]]}, {"id": "1709.02373", "submitter": "Salaheddin Alakkari", "authors": "Salaheddin Alakkari and John Dingliana", "title": "Adaptive PCA for Time-Varying Data", "comments": "Typos fixed, uncited references removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an online adaptive PCA algorithm that is able to\ncompute the full dimensional eigenspace per new time-step of sequential data.\nThe algorithm is based on a one-step update rule that considers all second\norder correlations between previous samples and the new time-step. Our\nalgorithm has O(n) complexity per new time-step in its deterministic mode and\nO(1) complexity per new time-step in its stochastic mode. We test our algorithm\non a number of time-varying datasets of different physical phenomena. Explained\nvariance curves indicate that our technique provides an excellent approximation\nto the original eigenspace computed using standard PCA in batch mode. In\naddition, our experiments show that the stochastic mode, despite its much lower\ncomputational complexity, converges to the same eigenspace computed using the\ndeterministic mode.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 17:49:47 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 15:55:44 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Alakkari", "Salaheddin", ""], ["Dingliana", "John", ""]]}, {"id": "1709.02458", "submitter": "SouYoung Jin", "authors": "SouYoung Jin, Hang Su, Chris Stauffer, Erik Learned-Miller", "title": "End-to-end Face Detection and Cast Grouping in Movies Using\n  Erd\\H{o}s-R\\'{e}nyi Clustering", "comments": "to appear in ICCV 2017 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end system for detecting and clustering faces by\nidentity in full-length movies. Unlike works that start with a predefined set\nof detected faces, we consider the end-to-end problem of detection and\nclustering together. We make three separate contributions. First, we combine a\nstate-of-the-art face detector with a generic tracker to extract high quality\nface tracklets. We then introduce a novel clustering method, motivated by the\nclassic graph theory results of Erd\\H{o}s and R\\'enyi. It is based on the\nobservations that large clusters can be fully connected by joining just a small\nfraction of their point pairs, while just a single connection between two\ndifferent people can lead to poor clustering results. This suggests clustering\nusing a verification system with very few false positives but perhaps moderate\nrecall. We introduce a novel verification method, rank-1 counts verification,\nthat has this property, and use it in a link-based clustering scheme. Finally,\nwe define a novel end-to-end detection and clustering evaluation metric\nallowing us to assess the accuracy of the entire end-to-end system. We present\nstate-of-the-art results on multiple video data sets and also on standard face\ndatabases.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 21:22:26 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Jin", "SouYoung", ""], ["Su", "Hang", ""], ["Stauffer", "Chris", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1709.02463", "submitter": "Ayan Kumar Bhunia", "authors": "Prithaj Banerjee, Ayan Kumar Bhunia, Avirup Bhattacharyya, Partha\n  Pratim Roy, Subrahmanyam Murala", "title": "Local Neighborhood Intensity Pattern: A new texture feature descriptor\n  for image retrieval", "comments": "Expert Systems with Applications(Elsevier)", "journal-ref": null, "doi": "10.1016/j.eswa.2018.06.044", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new texture descriptor based on the local neighborhood\nintensity difference is proposed for content based image retrieval (CBIR). For\ncomputation of texture features like Local Binary Pattern (LBP), the center\npixel in a 3*3 window of an image is compared with all the remaining neighbors,\none pixel at a time to generate a binary bit pattern. It ignores the effect of\nthe adjacent neighbors of a particular pixel for its binary encoding and also\nfor texture description. The proposed method is based on the concept that\nneighbors of a particular pixel hold a significant amount of texture\ninformation that can be considered for efficient texture representation for\nCBIR. Taking this into account, we develop a new texture descriptor, named as\nLocal Neighborhood Intensity Pattern (LNIP) which considers the relative\nintensity difference between a particular pixel and the center pixel by\nconsidering its adjacent neighbors and generate a sign and a magnitude pattern.\nSince sign and magnitude patterns hold complementary information to each other,\nthese two patterns are concatenated into a single feature descriptor to\ngenerate a more concrete and useful feature descriptor. The proposed descriptor\nhas been tested for image retrieval on four databases, including three texture\nimage databases - Brodatz texture image database, MIT VisTex database and\nSalzburg texture database and one face database AT&T face database. The\nprecision and recall values observed on these databases are compared with some\nstate-of-art local patterns. The proposed method showed a significant\nimprovement over many other existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 21:56:32 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 18:25:08 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 05:49:52 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Banerjee", "Prithaj", ""], ["Bhunia", "Ayan Kumar", ""], ["Bhattacharyya", "Avirup", ""], ["Roy", "Partha Pratim", ""], ["Murala", "Subrahmanyam", ""]]}, {"id": "1709.02476", "submitter": "Timnit Gebru", "authors": "Timnit Gebru, Judy Hoffman, Li Fei-Fei", "title": "Fine-grained Recognition in the Wild: A Multi-Task Domain Adaptation\n  Approach", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While fine-grained object recognition is an important problem in computer\nvision, current models are unlikely to accurately classify objects in the wild.\nThese fully supervised models need additional annotated images to classify\nobjects in every new scenario, a task that is infeasible. However, sources such\nas e-commerce websites and field guides provide annotated images for many\nclasses. In this work, we study fine-grained domain adaptation as a step\ntowards overcoming the dataset shift between easily acquired annotated images\nand the real world. Adaptation has not been studied in the fine-grained setting\nwhere annotations such as attributes could be used to increase performance. Our\nwork uses an attribute based multi-task adaptation loss to increase accuracy\nfrom a baseline of 4.1% to 19.1% in the semi-supervised adaptation case. Prior\ndo- main adaptation works have been benchmarked on small datasets such as [46]\nwith a total of 795 images for some domains, or simplistic datasets such as\n[41] consisting of digits. We perform experiments on a subset of a new\nchallenging fine-grained dataset consisting of 1,095,021 images of 2, 657 car\ncategories drawn from e-commerce web- sites and Google Street View.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 22:31:45 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Gebru", "Timnit", ""], ["Hoffman", "Judy", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1709.02480", "submitter": "Timnit Gebru", "authors": "Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Li\n  Fei-Fei", "title": "Fine-Grained Car Detection for Visual Census Estimation", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted socioeconomic policies require an accurate understanding of a\ncountry's demographic makeup. To that end, the United States spends more than 1\nbillion dollars a year gathering census data such as race, gender, education,\noccupation and unemployment rates. Compared to the traditional method of\ncollecting surveys across many years which is costly and labor intensive,\ndata-driven, machine learning driven approaches are cheaper and faster--with\nthe potential ability to detect trends in close to real time. In this work, we\nleverage the ubiquity of Google Street View images and develop a computer\nvision pipeline to predict income, per capita carbon emission, crime rates and\nother city attributes from a single source of publicly available visual data.\nWe first detect cars in 50 million images across 200 of the largest US cities\nand train a model to predict demographic attributes using the detected cars. To\nfacilitate our work, we have collected the largest and most challenging\nfine-grained dataset reported to date consisting of over 2600 classes of cars\ncomprised of images from Google Street View and other web sources, classified\nby car experts to account for even the most subtle of visual differences. We\nuse this data to construct the largest scale fine-grained detection system\nreported to date. Our prediction results correlate well with ground truth\nincome data (r=0.82), Massachusetts department of vehicle registration, and\nsources investigating crime rates, income segregation, per capita carbon\nemission, and other market research. Finally, we learn interesting\nrelationships between cars and neighborhoods allowing us to perform the first\nlarge scale sociological analysis of cities using computer vision techniques.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 22:56:21 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Gebru", "Timnit", ""], ["Krause", "Jonathan", ""], ["Wang", "Yilun", ""], ["Chen", "Duyun", ""], ["Deng", "Jia", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1709.02482", "submitter": "Timnit Gebru", "authors": "Timnit Gebru, Jonathan Krause, Jia Deng, Li Fei-Fei", "title": "Scalable Annotation of Fine-Grained Categories Without Experts", "comments": "CHI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a crowdsourcing workflow to collect image annotations for visually\nsimilar synthetic categories without requiring experts. In animals, there is a\ndirect link between taxonomy and visual similarity: e.g. a collie (type of dog)\nlooks more similar to other collies (e.g. smooth collie) than a greyhound\n(another type of dog). However, in synthetic categories such as cars, objects\nwith similar taxonomy can have very different appearance: e.g. a 2011 Ford\nF-150 Supercrew-HD looks the same as a 2011 Ford F-150 Supercrew-LL but very\ndifferent from a 2011 Ford F-150 Supercrew-SVT. We introduce a graph based\ncrowdsourcing algorithm to automatically group visually indistinguishable\nobjects together. Using our workflow, we label 712,430 images by ~1,000 Amazon\nMechanical Turk workers; resulting in the largest fine-grained visual dataset\nreported to date with 2,657 categories of cars annotated at 1/20th the cost of\nhiring experts.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 23:08:26 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Gebru", "Timnit", ""], ["Krause", "Jonathan", ""], ["Deng", "Jia", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1709.02495", "submitter": "Ali Mahdi", "authors": "Ali Mahdi, Jun Qin", "title": "DeepFeat: A Bottom Up and Top Down Saliency Model Based on Deep Features\n  of Convolutional Neural Nets", "comments": "9 pages, 7 figures, submitted to IEEE transactions on cognitive\n  developmental systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep feature based saliency model (DeepFeat) is developed to leverage the\nunderstanding of the prediction of human fixations. Traditional saliency models\noften predict the human visual attention relying on few level image cues.\nAlthough such models predict fixations on a variety of image complexities,\ntheir approaches are limited to the incorporated features. In this study, we\naim to provide an intuitive interpretation of convolu- tional neural network\ndeep features by combining low and high level visual factors. We exploit four\nevaluation metrics to evaluate the correspondence between the proposed\nframework and the ground-truth fixations. The key findings of the results\ndemon- strate that the DeepFeat algorithm, incorporation of bottom up and top\ndown saliency maps, outperforms the individual bottom up and top down approach.\nMoreover, in comparison to nine 9 state-of-the-art saliency models, our\nproposed DeepFeat model achieves satisfactory performance based on all four\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 01:15:16 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Mahdi", "Ali", ""], ["Qin", "Jun", ""]]}, {"id": "1709.02508", "submitter": "Pan Ji", "authors": "Pan Ji, Tong Zhang, Hongdong Li, Mathieu Salzmann, Ian Reid", "title": "Deep Subspace Clustering Networks", "comments": "Accepted to NIPS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep neural network architecture for unsupervised subspace\nclustering. This architecture is built upon deep auto-encoders, which\nnon-linearly map the input data into a latent space. Our key idea is to\nintroduce a novel self-expressive layer between the encoder and the decoder to\nmimic the \"self-expressiveness\" property that has proven effective in\ntraditional subspace clustering. Being differentiable, our new self-expressive\nlayer provides a simple but effective way to learn pairwise affinities between\nall data points through a standard back-propagation procedure. Being nonlinear,\nour neural-network based method is able to cluster data points having complex\n(often nonlinear) structures. We further propose pre-training and fine-tuning\nstrategies that let us effectively learn the parameters of our subspace\nclustering networks. Our experiments show that the proposed method\nsignificantly outperforms the state-of-the-art unsupervised subspace clustering\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 02:22:22 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Ji", "Pan", ""], ["Zhang", "Tong", ""], ["Li", "Hongdong", ""], ["Salzmann", "Mathieu", ""], ["Reid", "Ian", ""]]}, {"id": "1709.02517", "submitter": "Faxian Cao", "authors": "Faxian Cao, Zhijing Yang, Jinchang Ren, Wing-Kuen Ling", "title": "Extreme Sparse Multinomial Logistic Regression: A Fast and Robust\n  Framework for Hyperspectral Image Classification", "comments": "14 pages,7 figures,4 tables", "journal-ref": "Remote sensing,9,2017,1255", "doi": "10.3390/rs9121255", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although the sparse multinomial logistic regression (SMLR) has provided a\nuseful tool for sparse classification, it suffers from inefficacy in dealing\nwith high dimensional features and manually set initial regressor values. This\nhas significantly constrained its applications for hyperspectral image (HSI)\nclassification. In order to tackle these two drawbacks, an extreme sparse\nmultinomial logistic regression (ESMLR) is proposed for effective\nclassification of HSI. First, the HSI dataset is projected to a new feature\nspace with randomly generated weight and bias. Second, an optimization model is\nestablished by the Lagrange multiplier method and the dual principle to\nautomatically determine a good initial regressor for SMLR via minimizing the\ntraining error and the regressor value. Furthermore, the extended\nmulti-attribute profiles (EMAPs) are utilized for extracting both the spectral\nand spatial features. A combinational linear multiple features learning (MFL)\nmethod is proposed to further enhance the features extracted by ESMLR and\nEMAPs. Finally, the logistic regression via the variable splitting and the\naugmented Lagrangian (LORSAL) is adopted in the proposed framework for reducing\nthe computational time. Experiments are conducted on two well-known HSI\ndatasets, namely the Indian Pines dataset and the Pavia University dataset,\nwhich have shown the fast and robust performance of the proposed ESMLR\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 03:16:52 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 07:32:27 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Cao", "Faxian", ""], ["Yang", "Zhijing", ""], ["Ren", "Jinchang", ""], ["Ling", "Wing-Kuen", ""]]}, {"id": "1709.02549", "submitter": "Saeed Vahidian", "authors": "Yasaman Ettefagh, Mohammad Hossein Moghaddam, Saeed Vahidian", "title": "A Novel Low-Complexity Framework in Ultra-Wideband Imaging for Breast\n  Cancer Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research work, a novel framework is pro- posed as an efficient\nsuccessor to traditional imaging methods for breast cancer detection in order\nto decrease the computational complexity. In this framework, the breast is\ndevided into seg- ments in an iterative process and in each iteration, the one\nhaving the most probability of containing tumor with lowest possible resolution\nis selected by using suitable decision metrics. After finding the smallest\ntumor-containing segment, the resolution is increased in the detected\ntumor-containing segment, leaving the other parts of the breast image with low\nresolution. Our framework is applied on the most common used beamforming\ntechniques, such as delay and sum (DAS) and delay multiply and sum (DMAS) and\naccording to simulation results, our framework can decrease the computational\ncomplexity significantly for both DAS and DMAS without imposing any degradation\non accuracy of basic algorithms. The amount of complexity reduction can be\ndetermined manually or automatically based on two proposed methods that are\ndescribed in this framework.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 05:45:15 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Ettefagh", "Yasaman", ""], ["Moghaddam", "Mohammad Hossein", ""], ["Vahidian", "Saeed", ""]]}, {"id": "1709.02554", "submitter": "Sachin Mehta", "authors": "Sachin Mehta, Ezgi Mercan, Jamen Bartlett, Donald Weaver, Joann\n  Elmore, and Linda Shapiro", "title": "Learning to Segment Breast Biopsy Whole Slide Images", "comments": "Added more WSI images in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We trained and applied an encoder-decoder model to semantically segment\nbreast biopsy images into biologically meaningful tissue labels. Since\nconventional encoder-decoder networks cannot be applied directly on large\nbiopsy images and the different sized structures in biopsies present novel\nchallenges, we propose four modifications: (1) an input-aware encoding block to\ncompensate for information loss, (2) a new dense connection pattern between\nencoder and decoder, (3) dense and sparse decoders to combine multi-level\nfeatures, (4) a multi-resolution network that fuses the results of\nencoder-decoders run on different resolutions. Our model outperforms a\nfeature-based approach and conventional encoder-decoders from the literature.\nWe use semantic segmentations produced with our model in an automated diagnosis\ntask and obtain higher accuracies than a baseline approach that employs an SVM\nfor feature-based segmentation, both using the same segmentation-based\ndiagnostic features.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 06:20:23 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 22:22:06 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Mehta", "Sachin", ""], ["Mercan", "Ezgi", ""], ["Bartlett", "Jamen", ""], ["Weaver", "Donald", ""], ["Elmore", "Joann", ""], ["Shapiro", "Linda", ""]]}, {"id": "1709.02565", "submitter": "M. Hossein Eybposh", "authors": "M. Hossein Eybposh, Mohammad Haghir Ebrahim-Abadi, Mohammad\n  Jalilpour-Monesi, and Seyed Saman Saboksayr", "title": "Segmentation and Classification of Cine-MR Images Using Fully\n  Convolutional Networks and Handcrafted Features", "comments": "9 pages, 3 figures, 2 tables. Was accepted to the ACDC challenge,\n  MICCAI 2017 (not attended)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional cine-MRI is of crucial importance for assessing the cardiac\nfunction. Features that describe the anatomy and function of cardiac structures\n(e.g. Left Ventricle (LV), Right Ventricle (RV), and Myocardium(MC)) are known\nto have significant diagnostic value and can be computed from 3D cine-MR\nimages. However, these features require precise segmentation of cardiac\nstructures. Among the fully automated segmentation methods, Fully Convolutional\nNetworks (FCN) with Skip Connections have shown robustness in medical\nsegmentation problems. In this study, we develop a complete pipeline for\nclassification of subjects with cardiac conditions based on 3D cine-MRI. For\nthe segmentation task, we develop a 2D FCN and introduce Parallel Paths (PP) as\na way to exploit the 3D information of the cine-MR image. For the\nclassification task, 125 features were extracted from the segmented structures,\ndescribing their anatomy and function. Next, a two-stage pipeline for feature\nselection using the LASSO method is developed. A subset of 20 features is\nselected for classification. Each subject is classified using an ensemble of\nLogistic Regression, Multi-Layer Perceptron, and Support Vector Machine\nclassifiers through majority voting. The Dice Coefficient for segmentation was\n0.95+-0.03, 0.89+-0.13, and 0.90+-0.03 for LV, RV, and MC respectively. The\n8-fold cross validation accuracy for the classification task was 95.05% and\n92.77% based on ground truth and the proposed methods segmentations\nrespectively. The results show that the PPs increase the segmentation accuracy,\nby exploiting the spatial relations. Moreover, the classification algorithm and\nthe features showed discriminability while keeping the sensitivity to\nsegmentation error as low as possible.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 06:56:01 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 07:54:23 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Eybposh", "M. Hossein", ""], ["Ebrahim-Abadi", "Mohammad Haghir", ""], ["Jalilpour-Monesi", "Mohammad", ""], ["Saboksayr", "Seyed Saman", ""]]}, {"id": "1709.02600", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro", "title": "Objectness Scoring and Detection Proposals in Forward-Looking Sonar\n  Images with Convolutional Neural Networks", "comments": "Author version", "journal-ref": "Proceedings of ANNPR 2016", "doi": "10.1007/978-3-319-46182-3_18", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forward-looking sonar can capture high resolution images of underwater\nscenes, but their interpretation is complex. Generic object detection in such\nimages has not been solved, specially in cases of small and unknown objects. In\ncomparison, detection proposal algorithms have produced top performing object\ndetectors in real-world color images. In this work we develop a Convolutional\nNeural Network that can reliably score objectness of image windows in\nforward-looking sonar images and by thresholding objectness, we generate\ndetection proposals. In our dataset of marine garbage objects, we obtain 94%\nrecall, generating around 60 proposals per image. The biggest strength of our\nmethod is that it can generalize to previously unseen objects. We show this by\ndetecting chain links, walls and a wrench without previous training in such\nobjects. We strongly believe our method can be used for class-independent\nobject detection, with many real-world applications such as chain following and\nmine detection.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 09:11:32 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Valdenegro-Toro", "Matias", ""]]}, {"id": "1709.02601", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro", "title": "Best Practices in Convolutional Networks for Forward-Looking Sonar Image\n  Recognition", "comments": "Author version; IEEE/MTS Oceans 2017 Aberdeen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have revolutionized perception for color\nimages, and their application to sonar images has also obtained good results.\nBut in general CNNs are difficult to train without a large dataset, need manual\ntuning of a considerable number of hyperparameters, and require many careful\ndecisions by a designer. In this work, we evaluate three common decisions that\nneed to be made by a CNN designer, namely the performance of transfer learning,\nthe effect of object/image size and the relation between training set size. We\nevaluate three CNN models, namely one based on LeNet, and two based on the Fire\nmodule from SqueezeNet. Our findings are: Transfer learning with an SVM works\nvery well, even when the train and transfer sets have no classes in common, and\nhigh classification performance can be obtained even when the target dataset is\nsmall. The ADAM optimizer combined with Batch Normalization can make a high\naccuracy CNN classifier, even with small image sizes (16 pixels). At least 50\nsamples per class are required to obtain $90\\%$ test accuracy, and using\nDropout with a small dataset helps improve performance, but Batch Normalization\nis better when a large dataset is available.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 09:14:25 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Valdenegro-Toro", "Matias", ""]]}, {"id": "1709.02635", "submitter": "Ramanpreet Pahwa Singh", "authors": "Ramanpreet Singh Pahwa, Minh N. Do, Tian Tsong Ng, Binh-Son Hua", "title": "Calibration of depth cameras using denoised depth images", "comments": "5 pages, 3 figures, conference", "journal-ref": "2014 IEEE International Conference on Image Processing (ICIP),\n  Paris, 2014, pp. 3459-3463", "doi": "10.1109/ICIP.2014.7025702", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth sensing devices have created various new applications in scientific and\ncommercial research with the advent of Microsoft Kinect and PMD (Photon Mixing\nDevice) cameras. Most of these applications require the depth cameras to be\npre-calibrated. However, traditional calibration methods using a checkerboard\ndo not work very well for depth cameras due to the low image resolution. In\nthis paper, we propose a depth calibration scheme which excels in estimating\ncamera calibration parameters when only a handful of corners and calibration\nimages are available. We exploit the noise properties of PMD devices to denoise\ndepth measurements and perform camera calibration using the denoised depth as\nan additional set of measurements. Our synthetic and real experiments show that\nour depth denoising and depth based calibration scheme provides significantly\nbetter results than traditional calibration methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 10:34:11 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Pahwa", "Ramanpreet Singh", ""], ["Do", "Minh N.", ""], ["Ng", "Tian Tsong", ""], ["Hua", "Binh-Son", ""]]}, {"id": "1709.02641", "submitter": "Longhao Yuan", "authors": "Longhao Yuan, Qibin Zhao and Jianting Cao", "title": "Completion of High Order Tensor Data with Missing Entries via\n  Tensor-train Decomposition", "comments": "8 pages, ICONIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at the completion problem of high order tensor data\nwith missing entries. The existing tensor factorization and completion methods\nsuffer from the curse of dimensionality when the order of tensor N>>3. To\novercome this problem, we propose an efficient algorithm called TT-WOPT\n(Tensor-train Weighted OPTimization) to find the latent core tensors of tensor\ndata and recover the missing entries. Tensor-train decomposition, which has the\npowerful representation ability with linear scalability to tensor order, is\nemployed in our algorithm. The experimental results on synthetic data and\nnatural image completion demonstrate that our method significantly outperforms\nthe other related methods. Especially when the missing rate of data is very\nhigh, e.g., 85% to 99%, our algorithm can achieve much better performance than\nother state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 10:48:56 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 12:36:41 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Yuan", "Longhao", ""], ["Zhao", "Qibin", ""], ["Cao", "Jianting", ""]]}, {"id": "1709.02653", "submitter": "Ramanpreet Pahwa Singh", "authors": "Ramanpreet Singh Pahwa, Jiangbo Lu, Nianjuan Jiang, Tian Tsong Ng,\n  Minh N. Do", "title": "Locating 3D Object Proposals: A Depth-Based Online Approach", "comments": "14 pages, 12 figures, journal", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  2016", "doi": "10.1109/TCSVT.2016.2616143", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2D object proposals, quickly detected regions in an image that likely contain\nan object of interest, are an effective approach for improving the\ncomputational efficiency and accuracy of object detection in color images. In\nthis work, we propose a novel online method that generates 3D object proposals\nin a RGB-D video sequence. Our main observation is that depth images provide\nimportant information about the geometry of the scene. Diverging from the\ntraditional goal of 2D object proposals to provide a high recall (lots of 2D\nbounding boxes near potential objects), we aim for precise 3D proposals. We\nleverage on depth information per frame and multi-view scene information to\nobtain accurate 3D object proposals. Using efficient but robust registration\nenables us to combine multiple frames of a scene in near real time and generate\n3D bounding boxes for potential 3D regions of interest. Using standard metrics,\nsuch as Precision-Recall curves and F-measure, we show that the proposed\napproach is significantly more accurate than the current state-of-the-art\ntechniques. Our online approach can be integrated into SLAM based video\nprocessing for quick 3D object localization. Our method takes less than a\nsecond in MATLAB on the UW-RGBD scene dataset on a single thread CPU and thus,\nhas potential to be used in low-power chips in Unmanned Aerial Vehicles (UAVs),\nquadcopters, and drones.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 11:24:32 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Pahwa", "Ramanpreet Singh", ""], ["Lu", "Jiangbo", ""], ["Jiang", "Nianjuan", ""], ["Ng", "Tian Tsong", ""], ["Do", "Minh N.", ""]]}, {"id": "1709.02684", "submitter": "Jonathan George", "authors": "Jonathan K. George, Cesare Soci, Volker J. Sorger", "title": "Identifying Mirror Symmetry Density with Delay in Spiking Neural\n  Networks", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to rapidly identify symmetry and anti-symmetry is an essential\nattribute of intelligence. Symmetry perception is a central process in human\nvision and may be key to human 3D visualization. While previous work in\nunderstanding neuron symmetry perception has concentrated on the neuron as an\nintegrator, here we show how the coincidence detecting property of the spiking\nneuron can be used to reveal symmetry density in spatial data. We develop a\nmethod for synchronizing symmetry-identifying spiking artificial neural\nnetworks to enable layering and feedback in the network. We show a method for\nbuilding a network capable of identifying symmetry density between sets of data\nand present a digital logic implementation demonstrating an 8x8\nleaky-integrate-and-fire symmetry detector in a field programmable gate array.\nOur results show that the efficiencies of spiking neural networks can be\nharnessed to rapidly identify symmetry in spatial data with applications in\nimage processing, 3D computer vision, and robotics.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 17:18:16 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["George", "Jonathan K.", ""], ["Soci", "Cesare", ""], ["Sorger", "Volker J.", ""]]}, {"id": "1709.02700", "submitter": "Lee Friedman", "authors": "Evgeny Abdulin, Lee Friedman and Oleg V. Komogortsev", "title": "Method to Detect Eye Position Noise from Video-Oculography when\n  Detection of Pupil or Corneal Reflection Position Fails", "comments": "9 figures, 20 pages, pseudocode and Matlab code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present software to detect noise in eye position signals from video-based\neye-tracking systems that depend on accurate pupil and corneal reflection\nposition estimation. When such systems transiently fail to properly detect the\npupil or the corneal reflection due to occlusion from eyelids, eye lashes or\nvarious shadows, the estimated gaze position is false. This produces an\nartifactual signal in the position trace that is rapidly, irregularly\noscillating between true and false gaze positions. We refer to this noise as\nRIONEPS (Rapid Irregularly Oscillating Noise of the Eye Position Signal). Our\nmethod for detecting these periods automatically is based on an estimate of the\nrelative inefficiency of the eye position signal. We look for RIONEPS in the\nhorizontal and vertical traces separately, and although we typically use it\noffline, it is suitable to adaptation for real time use. This method requires a\nthreshold to be set, and although we provide some guidance, thresholds will\nhave to be estimated empirically.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 13:39:14 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Abdulin", "Evgeny", ""], ["Friedman", "Lee", ""], ["Komogortsev", "Oleg V.", ""]]}, {"id": "1709.02741", "submitter": "Shadrokh Samavi", "authors": "Hamid R. Fazlali, Nader Karimi, S.M. Reza Soroushmehr, Shahram\n  Shirani, Brahmajee.K. Nallamothu, Kevin R. Ward, Shadrokh Samavi, Kayvan\n  Najarian", "title": "Vessel Segmentation and Catheter Detection in X-Ray Angiograms Using\n  Superpixels", "comments": "14 Pages, 19 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary artery disease (CAD) is the leading causes of death around the\nworld. One of the most common imaging methods for diagnosing this disease is\nX-ray angiography. Diagnosing using these images is usually challenging due to\nnon-uniform illumination, low contrast, presence of other body tissues,\npresence of catheter etc. These challenges make the diagnoses task of\ncardiologists tougher and more prone to misdiagnosis. In this paper we propose\na new automated framework for coronary arteries segmentation, catheter\ndetection and center-line extraction in x-ray angiography images. Our proposed\nsegmentation method is based on superpixels. In this method at first three\ndifferent superpixel scales are exploited and a measure for vesselness\nprobability of each superpixel is determined. A majority voting is used for\nobtaining an initial segmentation map from these three superpixel scales. This\ninitial segmentation is refined by finding the orthogonal line on each ridge\npixel of vessel region. In this framework we use our catheter detection and\ntracking method which detects the catheter by finding its ridge in the first\nframe and traces in other frames by fitting a second order polynomial on it.\nAlso we use the image ridges for extracting the coronary arteries centerlines.\nWe evaluated our method qualitatively and quantitatively on two different\nchallenging datasets and compared it with one of the previous well-known\ncoronary arteries segmentation methods. Our method could detect the catheter\nand reduced the false positive rate in addition to achieving better\nsegmentation results. The evaluation results prove that our method performs\nbetter in a much shorter time.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 15:22:36 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Fazlali", "Hamid R.", ""], ["Karimi", "Nader", ""], ["Soroushmehr", "S. M. Reza", ""], ["Shirani", "Shahram", ""], ["Nallamothu", "Brahmajee. K.", ""], ["Ward", "Kevin R.", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1709.02764", "submitter": "Lorenz Berger", "authors": "Lorenz Berger, Eoin Hyde, M. Jorge Cardoso, Sebastien Ourselin", "title": "An Adaptive Sampling Scheme to Efficiently Train Fully Convolutional\n  Networks for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have shown excellent performance in\nobject recognition tasks and dense classification problems such as semantic\nsegmentation. However, training deep neural networks on large and sparse\ndatasets is still challenging and can require large amounts of computation and\nmemory. In this work, we address the task of performing semantic segmentation\non large data sets, such as three-dimensional medical images. We propose an\nadaptive sampling scheme that uses a-posterior error maps, generated throughout\ntraining, to focus sampling on difficult regions, resulting in improved\nlearning. Our contribution is threefold: 1) We give a detailed description of\nthe proposed sampling algorithm to speed up and improve learning performance on\nlarge images. We propose a deep dual path CNN that captures information at fine\nand coarse scales, resulting in a network with a large field of view and high\nresolution outputs. We show that our method is able to attain new\nstate-of-the-art results on the VISCERAL Anatomy benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 16:17:55 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 10:55:33 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 12:51:53 GMT"}, {"version": "v4", "created": "Fri, 22 Dec 2017 14:16:07 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Berger", "Lorenz", ""], ["Hyde", "Eoin", ""], ["Cardoso", "M. Jorge", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "1709.02779", "submitter": "Vedran Dunjko", "authors": "Vedran Dunjko and Hans J. Briegel", "title": "Machine learning \\& artificial intelligence in the quantum domain", "comments": "Review paper. 106 pages. 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum information technologies, and intelligent learning systems, are both\nemergent technologies that will likely have a transforming impact on our\nsociety. The respective underlying fields of research -- quantum information\n(QI) versus machine learning (ML) and artificial intelligence (AI) -- have\ntheir own specific challenges, which have hitherto been investigated largely\nindependently. However, in a growing body of recent work, researchers have been\nprobing the question to what extent these fields can learn and benefit from\neach other. QML explores the interaction between quantum computing and ML,\ninvestigating how results and techniques from one field can be used to solve\nthe problems of the other. Recently, we have witnessed breakthroughs in both\ndirections of influence. For instance, quantum computing is finding a vital\napplication in providing speed-ups in ML, critical in our \"big data\" world.\nConversely, ML already permeates cutting-edge technologies, and may become\ninstrumental in advanced quantum technologies. Aside from quantum speed-up in\ndata analysis, or classical ML optimization used in quantum experiments,\nquantum enhancements have also been demonstrated for interactive learning,\nhighlighting the potential of quantum-enhanced learning agents. Finally, works\nexploring the use of AI for the very design of quantum experiments, and for\nperforming parts of genuine research autonomously, have reported their first\nsuccesses. Beyond the topics of mutual enhancement, researchers have also\nbroached the fundamental issue of quantum generalizations of ML/AI concepts.\nThis deals with questions of the very meaning of learning and intelligence in a\nworld that is described by quantum mechanics. In this review, we describe the\nmain ideas, recent developments, and progress in a broad spectrum of research\ninvestigating machine learning and artificial intelligence in the quantum\ndomain.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 16:53:24 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Dunjko", "Vedran", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1709.02780", "submitter": "Alejandro Cartas", "authors": "Alejandro Cartas, Mariella Dimiccoli and Petia Radeva", "title": "Detecting Hands in Egocentric Videos: Towards Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, there has been a growing interest in analyzing human daily\nactivities from data collected by wearable cameras. Since the hands are\ninvolved in a vast set of daily tasks, detecting hands in egocentric images is\nan important step towards the recognition of a variety of egocentric actions.\nHowever, besides extreme illumination changes in egocentric images, hand\ndetection is not a trivial task because of the intrinsic large variability of\nhand appearance. We propose a hand detector that exploits skin modeling for\nfast hand proposal generation and Convolutional Neural Networks for hand\nrecognition. We tested our method on UNIGE-HANDS dataset and we showed that the\nproposed approach achieves competitive hand detection results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 17:08:15 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Cartas", "Alejandro", ""], ["Dimiccoli", "Mariella", ""], ["Radeva", "Petia", ""]]}, {"id": "1709.02848", "submitter": "Wuming Zhang", "authors": "Wuming Zhang, Zhixin Shu, Dimitris Samaras, Liming Chen", "title": "Improving Heterogeneous Face Recognition with Conditional Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous face recognition between color image and depth image is a much\ndesired capacity for real world applications where shape information is looked\nupon as merely involved in gallery. In this paper, we propose a cross-modal\ndeep learning method as an effective and efficient workaround for this\nchallenge. Specifically, we begin with learning two convolutional neural\nnetworks (CNNs) to extract 2D and 2.5D face features individually. Once\ntrained, they can serve as pre-trained models for another two-way CNN which\nexplores the correlated part between color and depth for heterogeneous\nmatching. Compared with most conventional cross-modal approaches, our method\nadditionally conducts accurate depth image reconstruction from single color\nimage with Conditional Generative Adversarial Nets (cGAN), and further enhances\nthe recognition performance by fusing multi-modal matching results. Through\nboth qualitative and quantitative experiments on benchmark FRGC 2D/3D face\ndatabase, we demonstrate that the proposed pipeline outperforms\nstate-of-the-art performance on heterogeneous face recognition and ensures a\ndrastically efficient on-line stage.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 20:09:50 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 18:01:12 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Zhang", "Wuming", ""], ["Shu", "Zhixin", ""], ["Samaras", "Dimitris", ""], ["Chen", "Liming", ""]]}, {"id": "1709.02896", "submitter": "Yanwei Pang", "authors": "Yanwei Pang, Bo Zhou, and Feiping Nie", "title": "Simultaneously Learning Neighborship and Projection Matrix for\n  Supervised Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicitly or implicitly, most of dimensionality reduction methods need to\ndetermine which samples are neighbors and the similarity between the neighbors\nin the original highdimensional space. The projection matrix is then learned on\nthe assumption that the neighborhood information (e.g., the similarity) is\nknown and fixed prior to learning. However, it is difficult to precisely\nmeasure the intrinsic similarity of samples in high-dimensional space because\nof the curse of dimensionality. Consequently, the neighbors selected according\nto such similarity might and the projection matrix obtained according to such\nsimilarity and neighbors are not optimal in the sense of classification and\ngeneralization. To overcome the drawbacks, in this paper we propose to let the\nsimilarity and neighbors be variables and model them in low-dimensional space.\nBoth the optimal similarity and projection matrix are obtained by minimizing a\nunified objective function. Nonnegative and sum-to-one constraints on the\nsimilarity are adopted. Instead of empirically setting the regularization\nparameter, we treat it as a variable to be optimized. It is interesting that\nthe optimal regularization parameter is adaptive to the neighbors in\nlow-dimensional space and has intuitive meaning. Experimental results on the\nYALE B, COIL-100, and MNIST datasets demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 02:44:18 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Pang", "Yanwei", ""], ["Zhou", "Bo", ""], ["Nie", "Feiping", ""]]}, {"id": "1709.02898", "submitter": "Zhang Qiang", "authors": "Qiang Zhang, Qiangqiang Yuan, Jie Li, Zhen Yang, Xiaoshuang Ma", "title": "Learning a Dilated Residual Network for SAR Image Despeckling", "comments": "18 pages, 13 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, to break the limit of the traditional linear models for\nsynthetic aperture radar (SAR) image despeckling, we propose a novel deep\nlearning approach by learning a non-linear end-to-end mapping between the noisy\nand clean SAR images with a dilated residual network (SAR-DRN). SAR-DRN is\nbased on dilated convolutions, which can both enlarge the receptive field and\nmaintain the filter size and layer depth with a lightweight structure. In\naddition, skip connections and residual learning strategy are added to the\ndespeckling model to maintain the image details and reduce the vanishing\ngradient problem. Compared with the traditional despeckling methods, the\nproposed method shows superior performance over the state-of-the-art methods on\nboth quantitative and visual assessments, especially for strong speckle noise.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 03:22:26 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 07:27:31 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 03:06:32 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Zhang", "Qiang", ""], ["Yuan", "Qiangqiang", ""], ["Li", "Jie", ""], ["Yang", "Zhen", ""], ["Ma", "Xiaoshuang", ""]]}, {"id": "1709.02908", "submitter": "Haodong Li", "authors": "Bolin Chen, Haodong Li, Weiqi Luo", "title": "Image Processing Operations Identification via Convolutional Neural\n  Network", "comments": null, "journal-ref": "Sci. China Inf. Sci. 63, 139109 (2020)", "doi": "10.1007/s11432-018-9492-6", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, image forensics has attracted more and more attention, and\nmany forensic methods have been proposed for identifying image processing\noperations. Up to now, most existing methods are based on hand crafted\nfeatures, and just one specific operation is considered in their methods. In\nmany forensic scenarios, however, multiple classification for various image\nprocessing operations is more practical. Besides, it is difficult to obtain\neffective features by hand for some image processing operations. In this paper,\ntherefore, we propose a new convolutional neural network (CNN) based method to\nadaptively learn discriminative features for identifying typical image\nprocessing operations. We carefully design the high pass filter bank to get the\nimage residuals of the input image, the channel expansion layer to mix up the\nresulting residuals, the pooling layers, and the activation functions employed\nin our method. The extensive results show that the proposed method can\noutperform the currently best method based on hand crafted features and three\nrelated methods based on CNN for image steganalysis and/or forensics, achieving\nthe state-of-the-art results. Furthermore, we provide more supplementary\nresults to show the rationality and robustness of the proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 04:34:48 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chen", "Bolin", ""], ["Li", "Haodong", ""], ["Luo", "Weiqi", ""]]}, {"id": "1709.02920", "submitter": "Ramanarayan Mohanty", "authors": "Ramanarayan Mohanty, S L Happy, Aurobinda Routray", "title": "Graph Scaling Cut with L1-Norm for Classification of Hyperspectral\n  Images", "comments": "European Signal Processing Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an L1 normalized graph based dimensionality\nreduction method for Hyperspectral images, called as L1-Scaling Cut (L1-SC).\nThe underlying idea of this method is to generate the optimal projection matrix\nby retaining the original distribution of the data. Though L2-norm is generally\npreferred for computation, it is sensitive to noise and outliers. However,\nL1-norm is robust to them. Therefore, we obtain the optimal projection matrix\nby maximizing the ratio of between-class dispersion to within-class dispersion\nusing L1-norm. Furthermore, an iterative algorithm is described to solve the\noptimization problem. The experimental results of the HSI classification\nconfirm the effectiveness of the proposed L1-SC method on both noisy and\nnoiseless data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 06:51:23 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Mohanty", "Ramanarayan", ""], ["Happy", "S L", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1709.02926", "submitter": "Mingwei Cao", "authors": "Mingwei Cao, Ming Yang, Chunxiang Wang, Yeqiang Qian, Bing Wang", "title": "Joint Calibration of Panoramic Camera and Lidar Based on Supervised\n  Learning", "comments": "in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view of contemporary panoramic camera-laser scanner system, the\ntraditional calibration method is not suitable for panoramic cameras whose\nimaging model is extremely nonlinear. The method based on statistical\noptimization has the disadvantage that the requirement of the number of laser\nscanner's channels is relatively high. Calibration equipments with extreme\naccuracy for panoramic camera-laser scanner system are costly. Facing all these\nin the calibration of panoramic camera-laser scanner system, a method based on\nsupervised learning is proposed. Firstly, corresponding feature points of\npanoramic images and point clouds are gained to generate the training dataset\nby designing a round calibration object. Furthermore, the traditional\ncalibration problem is transformed into a multiple nonlinear regression\noptimization problem by designing a supervised learning network with\npreprocessing of the panoramic imaging model. Back propagation algorithm is\nutilized to regress the rotation and translation matrix with high accuracy.\nExperimental results show that this method can quickly regress the calibration\nparameters and the accuracy is better than the traditional calibration method\nand the method based on statistical optimization. The calibration accuracy of\nthis method is really high, and it is more highly-automated.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 08:08:36 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 08:19:47 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Cao", "Mingwei", ""], ["Yang", "Ming", ""], ["Wang", "Chunxiang", ""], ["Qian", "Yeqiang", ""], ["Wang", "Bing", ""]]}, {"id": "1709.02929", "submitter": "Chong Wang", "authors": "Chong Wang and Xipeng Lan and Yangang Zhang", "title": "Model Distillation with Knowledge Transfer from Face Classification to\n  Alignment and Verification", "comments": "10 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation is a potential solution for model compression. The\nidea is to make a small student network imitate the target of a large teacher\nnetwork, then the student network can be competitive to the teacher one. Most\nprevious studies focus on model distillation in the classification task, where\nthey propose different architects and initializations for the student network.\nHowever, only the classification task is not enough, and other related tasks\nsuch as regression and retrieval are barely considered. To solve the problem,\nin this paper, we take face recognition as a breaking point and propose model\ndistillation with knowledge transfer from face classification to alignment and\nverification. By selecting appropriate initializations and targets in the\nknowledge transfer, the distillation can be easier in non-classification tasks.\nExperiments on the CelebA and CASIA-WebFace datasets demonstrate that the\nstudent network can be competitive to the teacher one in alignment and\nverification, and even surpasses the teacher network under specific compression\nrates. In addition, to achieve stronger knowledge transfer, we also use a\ncommon initialization trick to improve the distillation performance of\nclassification. Evaluations on the CASIA-Webface and large-scale MS-Celeb-1M\ndatasets show the effectiveness of this simple trick.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 08:30:18 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 07:51:29 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Wang", "Chong", ""], ["Lan", "Xipeng", ""], ["Zhang", "Yangang", ""]]}, {"id": "1709.02939", "submitter": "Vahid Moosavi", "authors": "Vahid Moosavi", "title": "Urban morphology meets deep learning: Exploring urban forms in one\n  million cities, town and villages across the planet", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study of urban form is an important area of research in urban planning/design\nthat contributes to our understanding of how cities function and evolve.\nHowever, classical approaches are based on very limited observations and\ninconsistent methods. As an alternative, availability of massive urban data\ncollections such as Open Street Map from the one hand and the recent\nadvancements in machine learning methods such as deep learning techniques on\nthe other have opened up new possibilities to automatically investigate urban\nforms at the global scale. In this work for the first time, by collecting a\nlarge data set of street networks in more than one million cities, towns and\nvillages all over the world, we trained a deep convolutional auto-encoder, that\nautomatically learns the hierarchical structures of urban forms and represents\nthem via dense and comparable vectors. We showed how the learned urban vectors\ncould be used for different investigations. Using the learned urban vectors,\none is able to easily find and compare similar urban forms all over the world,\nconsidering their overall spatial structure and other factors such as\norientation, graphical structure, and density and partial deformations. Further\ncluster analysis reveals the distribution of the main patterns of urban forms\nall over the planet.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 10:06:48 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 10:30:20 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Moosavi", "Vahid", ""]]}, {"id": "1709.02940", "submitter": "Chong Wang", "authors": "Chong Wang and Xue Zhang and Xipeng Lan", "title": "How to Train Triplet Networks with 100K Identities?", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training triplet networks with large-scale data is challenging in face\nrecognition. Due to the number of possible triplets explodes with the number of\nsamples, previous studies adopt the online hard negative mining(OHNM) to handle\nit. However, as the number of identities becomes extremely large, the training\nwill suffer from bad local minima because effective hard triplets are difficult\nto be found. To solve the problem, in this paper, we propose training triplet\nnetworks with subspace learning, which splits the space of all identities into\nsubspaces consisting of only similar identities. Combined with the batch OHNM,\nhard triplets can be found much easier. Experiments on the large-scale\nMS-Celeb-1M challenge with 100K identities demonstrate that the proposed method\ncan largely improve the performance. In addition, to deal with heavy noise and\nlarge-scale retrieval, we also make some efforts on robust noise removing and\nefficient image retrieval, which are used jointly with the subspace learning to\nobtain the state-of-the-art performance on the MS-Celeb-1M competition (without\nexternal data in Challenge1).\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 10:18:41 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Wang", "Chong", ""], ["Zhang", "Xue", ""], ["Lan", "Xipeng", ""]]}, {"id": "1709.02967", "submitter": "James Brown", "authors": "Andrew Beers, Ken Chang, James Brown, Emmett Sartor, CP Mammen,\n  Elizabeth Gerstner, Bruce Rosen, Jayashree Kalpathy-Cramer", "title": "Sequential 3D U-Nets for Biologically-Informed Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has quickly become the weapon of choice for brain lesion\nsegmentation. However, few existing algorithms pre-configure any biological\ncontext of their chosen segmentation tissues, and instead rely on the neural\nnetwork's optimizer to develop such associations de novo. We present a novel\nmethod for applying deep neural networks to the problem of glioma tissue\nsegmentation that takes into account the structured nature of gliomas -\nedematous tissue surrounding mutually-exclusive regions of enhancing and\nnon-enhancing tumor. We trained multiple deep neural networks with a 3D U-Net\narchitecture in a tree structure to create segmentations for edema,\nnon-enhancing tumor, and enhancing tumor regions. Specifically, training was\nconfigured such that the whole tumor region including edema was predicted\nfirst, and its output segmentation was fed as input into separate models to\npredict enhancing and non-enhancing tumor. Our method was trained and evaluated\non the publicly available BraTS dataset, achieving Dice scores of 0.882, 0.732,\nand 0.730 for whole tumor, enhancing tumor and tumor core respectively.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 15:57:51 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Beers", "Andrew", ""], ["Chang", "Ken", ""], ["Brown", "James", ""], ["Sartor", "Emmett", ""], ["Mammen", "CP", ""], ["Gerstner", "Elizabeth", ""], ["Rosen", "Bruce", ""], ["Kalpathy-Cramer", "Jayashree", ""]]}, {"id": "1709.02974", "submitter": "Jan Funke", "authors": "Jan Funke and Fabian David Tschopp and William Grisaitis and Arlo\n  Sheridan and Chandan Singh and Stephan Saalfeld and Srinivas C. Turaga", "title": "Large Scale Image Segmentation with Structured Loss based Deep Learning\n  for Connectome Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2018.2835450", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method combining affinity prediction with region agglomeration,\nwhich improves significantly upon the state of the art of neuron segmentation\nfrom electron microscopy (EM) in accuracy and scalability. Our method consists\nof a 3D U-NET, trained to predict affinities between voxels, followed by\niterative region agglomeration. We train using a structured loss based on\nMALIS, encouraging topologically correct segmentations obtained from affinity\nthresholding. Our extension consists of two parts: First, we present a\nquasi-linear method to compute the loss gradient, improving over the original\nquadratic algorithm. Second, we compute the gradient in two separate passes to\navoid spurious gradient contributions in early training stages. Our predictions\nare accurate enough that simple learning-free percentile-based agglomeration\noutperforms more involved methods used earlier on inferior predictions. We\npresent results on three diverse EM datasets, achieving relative improvements\nover previous results of 27%, 15%, and 250%. Our findings suggest that a single\nmethod can be applied to both nearly isotropic block-face EM data and\nanisotropic serial sectioned EM data. The runtime of our method scales linearly\nwith the size of the volume and achieves a throughput of about 2.6 seconds per\nmegavoxel, qualifying our method for the processing of very large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 16:25:52 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 23:32:42 GMT"}, {"version": "v3", "created": "Sun, 24 Sep 2017 16:16:36 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 09:21:55 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Funke", "Jan", ""], ["Tschopp", "Fabian David", ""], ["Grisaitis", "William", ""], ["Sheridan", "Arlo", ""], ["Singh", "Chandan", ""], ["Saalfeld", "Stephan", ""], ["Turaga", "Srinivas C.", ""]]}, {"id": "1709.02993", "submitter": "Saeed Ranjbar Alvar", "authors": "Saeed Ranjbar Alvar, Hyomin Choi and Ivan V. Bajic", "title": "Can you tell a face from a HEVC bitstream?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and video analytics are being increasingly used on a massive scale. Not\nonly is the amount of data growing, but the complexity of the data processing\npipelines is also increasing, thereby exacerbating the problem. It is becoming\nincreasingly important to save computational resources wherever possible. We\nfocus on one of the poster problems of visual analytics -- face detection --\nand approach the issue of reducing the computation by asking: Is it possible to\ndetect a face without full image reconstruction from the High Efficiency Video\nCoding (HEVC) bitstream? We demonstrate that this is indeed possible, with\naccuracy comparable to conventional face detection, by training a Convolutional\nNeural Network on the output of the HEVC entropy decoder.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 18:43:52 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Alvar", "Saeed Ranjbar", ""], ["Choi", "Hyomin", ""], ["Bajic", "Ivan V.", ""]]}, {"id": "1709.02995", "submitter": "Ying Lu", "authors": "Ying Lu, Liming Chen, Alexandre Saidi", "title": "Optimal Transport for Deep Joint Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a Deep Neural Network (DNN) from scratch requires a large amount of\nlabeled data. For a classification task where only small amount of training\ndata is available, a common solution is to perform fine-tuning on a DNN which\nis pre-trained with related source data. This consecutive training process is\ntime consuming and does not consider explicitly the relatedness between\ndifferent source and target tasks.\n  In this paper, we propose a novel method to jointly fine-tune a Deep Neural\nNetwork with source data and target data. By adding an Optimal Transport loss\n(OT loss) between source and target classifier predictions as a constraint on\nthe source classifier, the proposed Joint Transfer Learning Network (JTLN) can\neffectively learn useful knowledge for target classification from source data.\nFurthermore, by using different kind of metric as cost matrix for the OT loss,\nJTLN can incorporate different prior knowledge about the relatedness between\ntarget categories and source categories.\n  We carried out experiments with JTLN based on Alexnet on image classification\ndatasets and the results verify the effectiveness of the proposed JTLN in\ncomparison with standard consecutive fine-tuning. This Joint Transfer Learning\nwith OT loss is general and can also be applied to other kind of Neural\nNetworks.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 18:56:05 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Lu", "Ying", ""], ["Chen", "Liming", ""], ["Saidi", "Alexandre", ""]]}, {"id": "1709.03009", "submitter": "Lee Clement", "authors": "Lee Clement and Jonathan Kelly", "title": "How to Train a CAT: Learning Canonical Appearance Transformations for\n  Direct Visual Localization Under Illumination Change", "comments": "In IEEE Robotics and Automation Letters (RA-L) and presented at the\n  IEEE International Conference on Robotics and Automation (ICRA'18), Brisbane,\n  Australia, May 21-25, 2018", "journal-ref": null, "doi": "10.1109/LRA.2018.2799741", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct visual localization has recently enjoyed a resurgence in popularity\nwith the increasing availability of cheap mobile computing power. The\ncompetitive accuracy and robustness of these algorithms compared to\nstate-of-the-art feature-based methods, as well as their natural ability to\nyield dense maps, makes them an appealing choice for a variety of mobile\nrobotics applications. However, direct methods remain brittle in the face of\nappearance change due to their underlying assumption of photometric\nconsistency, which is commonly violated in practice. In this paper, we propose\nto mitigate this problem by training deep convolutional encoder-decoder models\nto transform images of a scene such that they correspond to a previously-seen\ncanonical appearance. We validate our method in multiple environments and\nillumination conditions using high-fidelity synthetic RGB-D datasets, and\nintegrate the trained models into a direct visual localization pipeline,\nyielding improvements in visual odometry (VO) accuracy through time-varying\nillumination conditions, as well as improved metric relocalization performance\nunder illumination change, where conventional methods normally fail. We further\nprovide a preliminary investigation of transfer learning from synthetic to real\nenvironments in a localization context. An open-source implementation of our\nmethod using PyTorch is available at https://github.com/utiasSTARS/cat-net.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 21:58:12 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 16:55:21 GMT"}, {"version": "v3", "created": "Wed, 17 Jan 2018 01:51:10 GMT"}, {"version": "v4", "created": "Mon, 10 Sep 2018 01:32:39 GMT"}, {"version": "v5", "created": "Tue, 11 Sep 2018 13:52:34 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Clement", "Lee", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1709.03028", "submitter": "Mohammadhassan Izadyyazdanabadi", "authors": "Mohammadhassan Izadyyazdanabadi, Evgenii Belykh, Michael Mooney,\n  Nikolay Martirosyan, Jennifer Eschbacher, Peter Nakaji, Mark C. Preul and\n  Yezhou Yang", "title": "Convolutional Neural Networks: Ensemble Modeling, Fine-Tuning and\n  Unsupervised Semantic Localization for Intraoperative CLE Images", "comments": "The related work was updated", "journal-ref": null, "doi": "10.1016/j.jvcir.2018.04.004", "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confocal laser endomicroscopy (CLE) is an advanced optical fluorescence\ntechnology undergoing assessment for applications in brain tumor surgery.\nDespite its promising potential, interpreting the unfamiliar gray tone images\nof fluorescent stains can be difficult. Many of the CLE images can be distorted\nby motion, extremely low or high fluorescence signal, or obscured by red blood\ncell accumulation, and these can be interpreted as nondiagnostic. However, just\none neat CLE image might suffice for intraoperative diagnosis of the tumor.\nWhile manual examination of thousands of nondiagnostic images during surgery\nwould be impractical, this creates an opportunity for a model to select\ndiagnostic images for the pathologists or surgeon's review. In this study, we\nsought to develop a deep learning model to automatically detect the diagnostic\nimages using a manually annotated dataset, and we employed a patient-based\nnested cross-validation approach to explore generalizability of the model. We\nexplored various training regimes: deep training, shallow fine-tuning, and deep\nfine-tuning. Further, we investigated the effect of ensemble modeling by\ncombining the top-5 single models crafted in the development phase. We\nlocalized histological features from diagnostic CLE images by visualization of\nshallow and deep neural activations. Our inter-rater experiment results\nconfirmed that our ensemble of deeply fine-tuned models achieved higher\nagreement with the ground truth than the other observers. With the speed and\nprecision of the proposed method (110 images/second; 85% on the gold standard\ntest subset), it has potential to be integrated into the operative workflow in\nthe brain tumor surgery.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 02:33:35 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 18:07:26 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Izadyyazdanabadi", "Mohammadhassan", ""], ["Belykh", "Evgenii", ""], ["Mooney", "Michael", ""], ["Martirosyan", "Nikolay", ""], ["Eschbacher", "Jennifer", ""], ["Nakaji", "Peter", ""], ["Preul", "Mark C.", ""], ["Yang", "Yezhou", ""]]}, {"id": "1709.03030", "submitter": "Xiaodong Feng", "authors": "Xiaodong Feng, Zhiwei Tang, Sen Wu", "title": "Robust Sparse Coding via Self-Paced Learning", "comments": "submitted to AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding (SC) is attracting more and more attention due to its\ncomprehensive theoretical studies and its excellent performance in many signal\nprocessing applications. However, most existing sparse coding algorithms are\nnonconvex and are thus prone to becoming stuck into bad local minima,\nespecially when there are outliers and noisy data. To enhance the learning\nrobustness, in this paper, we propose a unified framework named Self-Paced\nSparse Coding (SPSC), which gradually include matrix elements into SC learning\nfrom easy to complex. We also generalize the self-paced learning schema into\ndifferent levels of dynamic selection on samples, features and elements\nrespectively. Experimental results on real-world data demonstrate the efficacy\nof the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 03:15:48 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Feng", "Xiaodong", ""], ["Tang", "Zhiwei", ""], ["Wu", "Sen", ""]]}, {"id": "1709.03086", "submitter": "Sibel Tari", "authors": "Asli Genctav, Sibel Tari", "title": "A Product Shape Congruity Measure via Entropy in Shape Scale Space", "comments": "Proceedings of EUSIPCO 2017 Satellite Workshops, Corresponding\n  Workshop: Creative Design and Advanced Manufacturing: An emerging application\n  area for Signals and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product shape is one of the factors that trigger preference decisions of\ncustomers. Congruity of shape elements and deformation of shape from the\nprototype are two factors that are found to influence aesthetic response, hence\npreference. We propose a measure to indirectly quantify congruity of different\nparts of the shape and the degree to which the parts deviate from a sphere,\ni.e. our choice of the prototype, without explicitly defining parts and their\nrelations. The basic signals and systems concept that we use is the entropy.\nOur measure attains its lowest value for a volume enclosed by a sphere. On one\nhand, deformations from the prototype cause an increase in the measure. On the\nother hand, as deformations create congruent parts, our measure decreases due\nto the attained harmony. Our preliminary experimental results are consistent\nwith our expectations.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 11:51:08 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Genctav", "Asli", ""], ["Tari", "Sibel", ""]]}, {"id": "1709.03124", "submitter": "Di Claudio Elio D.", "authors": "Elio D. Di Claudio, Giovanni Jacovitti", "title": "A Detail Based Method for Linear Full Reference Image Quality Prediction", "comments": "15 pages, 9 figures. Copyright notice: The paper has been accepted\n  for publication on the IEEE Trans. on Image Processing on 19/09/2017 and the\n  copyright has been transferred to the IEEE", "journal-ref": "IEEE Transactions on Image Processing, vol. 27, no. 1, pp.\n  179-193, Jan. 2018", "doi": "10.1109/TIP.2017.2757139", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel Full Reference method is proposed for image quality\nassessment, using the combination of two separate metrics to measure the\nperceptually distinct impact of detail losses and of spurious details. To this\npurpose, the gradient of the impaired image is locally decomposed as a\npredicted version of the original gradient, plus a gradient residual. It is\nassumed that the detail attenuation identifies the detail loss, whereas the\ngradient residuals describe the spurious details. It turns out that the\nperceptual impact of detail losses is roughly linear with the loss of the\npositional Fisher information, while the perceptual impact of the spurious\ndetails is roughly proportional to a logarithmic measure of the signal to\nresidual ratio. The affine combination of these two metrics forms a new index\nstrongly correlated with the empirical Differential Mean Opinion Score (DMOS)\nfor a significant class of image impairments, as verified for three independent\npopular databases. The method allowed alignment and merging of DMOS data coming\nfrom these different databases to a common DMOS scale by affine\ntransformations. Unexpectedly, the DMOS scale setting is possible by the\nanalysis of a single image affected by additive noise.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 16:28:24 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 09:13:27 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 18:32:08 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Di Claudio", "Elio D.", ""], ["Jacovitti", "Giovanni", ""]]}, {"id": "1709.03126", "submitter": "Bowen Cheng", "authors": "Bowen Cheng, Zhangyang Wang, Zhaobin Zhang, Zhu Li, Ding Liu, Jianchao\n  Yang, Shuai Huang, Thomas S. Huang", "title": "Robust Emotion Recognition from Low Quality and Low Bit Rate Video: A\n  Deep Learning Approach", "comments": "Accepted by the Seventh International Conference on Affective\n  Computing and Intelligent Interaction (ACII2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition from facial expressions is tremendously useful,\nespecially when coupled with smart devices and wireless multimedia\napplications. However, the inadequate network bandwidth often limits the\nspatial resolution of the transmitted video, which will heavily degrade the\nrecognition reliability. We develop a novel framework to achieve robust emotion\nrecognition from low bit rate video. While video frames are downsampled at the\nencoder side, the decoder is embedded with a deep network model for joint\nsuper-resolution (SR) and recognition. Notably, we propose a novel max-mix\ntraining strategy, leading to a single \"One-for-All\" model that is remarkably\nrobust to a vast range of downsampling factors. That makes our framework well\nadapted for the varied bandwidths in real transmission scenarios, without\nhampering scalability or efficiency. The proposed framework is evaluated on the\nAVEC 2016 benchmark, and demonstrates significantly improved stand-alone\nrecognition performance, as well as rate-distortion (R-D) performance, than\neither directly recognizing from LR frames, or separating SR and recognition.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 16:31:56 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Cheng", "Bowen", ""], ["Wang", "Zhangyang", ""], ["Zhang", "Zhaobin", ""], ["Li", "Zhu", ""], ["Liu", "Ding", ""], ["Yang", "Jianchao", ""], ["Huang", "Shuai", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1709.03128", "submitter": "Jonathan Kelly", "authors": "Valentin Peretroukhin and Jonathan Kelly", "title": "DPC-Net: Deep Pose Correction for Visual Localization", "comments": "In IEEE Robotics and Automation Letters (RA-L) and presented at the\n  IEEE International Conference on Robotics and Automation (ICRA'18), Brisbane,\n  Australia, May 21-25, 2018", "journal-ref": null, "doi": "10.1109/LRA.2017.2778765", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to fuse the power of deep networks with the\ncomputational efficiency of geometric and probabilistic localization\nalgorithms. In contrast to other methods that completely replace a classical\nvisual estimator with a deep network, we propose an approach that uses a\nconvolutional neural network to learn difficult-to-model corrections to the\nestimator from ground-truth training data. To this end, we derive a novel loss\nfunction for learning SE(3) corrections based on a matrix Lie groups approach,\nwith a natural formulation for balancing translation and rotation errors. We\nuse this loss to train a Deep Pose Correction network (DPC-Net) that predicts\ncorrections for a particular estimator, sensor and environment. Using the KITTI\nodometry dataset, we demonstrate significant improvements to the accuracy of a\ncomputationally-efficient sparse stereo visual odometry pipeline, that render\nit as accurate as a modern computationally-intensive dense estimator. Further,\nwe show how DPC-Net can be used to mitigate the effect of poorly calibrated\nlens distortion parameters.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 16:35:55 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 01:28:32 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 01:36:08 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Peretroukhin", "Valentin", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1709.03138", "submitter": "Florian Piewak", "authors": "Florian Piewak", "title": "Fully Convolutional Neural Networks for Dynamic Object Detection in Grid\n  Maps (Masters Thesis)", "comments": "This is the masters thesis of Florian Piewak. A shorter version of\n  this thesis was accepted at IV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important parts of environment perception is the detection of\nobstacles in the surrounding of the vehicle. To achieve that, several sensors\nlike radars, LiDARs and cameras are installed in autonomous vehicles. The\nproduced sensor data is fused to a general representation of the surrounding.\nIn this thesis the dynamic occupancy grid map approach of Nuss et al. is used\nwhile three goals are achieved. First, the approach of Nuss et al. to\ndistinguish between moving and non-moving obstacles is improved by using Fully\nConvolutional Neural Networks to create a class prediction for each grid cell.\nFor this purpose, the network is initialized with public pre-trained network\nmodels and the training is executed with a semi-automatic generated dataset.\nThe second goal is to provide orientation information for each detected moving\nobstacle. This could improve tracking algorithms, which are based on the\ndynamic occupancy grid map. The orientation extraction based on the\nConvolutional Neural Network shows a better performance in comparison to an\norientation extraction directly over the velocity information of the dynamic\noccupancy grid map. A general problem of developing machine learning approaches\nlike Neural Networks is the number of labeled data, which can always be\nincreased. For this reason, the last goal is to evaluate a semi-supervised\nlearning algorithm, to generate automatically more labeled data. The result of\nthis evaluation shows that the automated labeled data does not improve the\nperformance of the Convolutional Neural Network. All in all, the best results\nare combined to compare the detection against the approach of Nuss et al. [36]\nand a relative improvement of 34.8% is reached.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 17:06:23 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Piewak", "Florian", ""]]}, {"id": "1709.03139", "submitter": "Florian Piewak", "authors": "Florian Piewak, Timo Rehfeld, Michael Weber, J. Marius Z\\\"ollner", "title": "Fully Convolutional Neural Networks for Dynamic Object Detection in Grid\n  Maps", "comments": "This is a shorter version of the masters thesis of Florian Piewak and\n  it was accapted at IV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid maps are widely used in robotics to represent obstacles in the\nenvironment and differentiating dynamic objects from static infrastructure is\nessential for many practical applications. In this work, we present a methods\nthat uses a deep convolutional neural network (CNN) to infer whether grid cells\nare covering a moving object or not. Compared to tracking approaches, that use\ne.g. a particle filter to estimate grid cell velocities and then make a\ndecision for individual grid cells based on this estimate, our approach uses\nthe entire grid map as input image for a CNN that inspects a larger area around\neach cell and thus takes the structural appearance in the grid map into account\nto make a decision. Compared to our reference method, our concept yields a\nperformance increase from 83.9% to 97.2%. A runtime optimized version of our\napproach yields similar improvements with an execution time of just 10\nmilliseconds.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 17:06:48 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Piewak", "Florian", ""], ["Rehfeld", "Timo", ""], ["Weber", "Michael", ""], ["Z\u00f6llner", "J. Marius", ""]]}, {"id": "1709.03170", "submitter": "Wenye He", "authors": "Wenye He", "title": "An Iterative Regression Approach for Face Pose Estimation from RGB\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a iterative optimization method, explicit shape\nregression, for face pose detection and localization. The regression function\nis learnt to find out the entire facial shape and minimize the alignment\nerrors. A cascaded learning framework is employed to enhance shape constraint\nduring detection. A combination of a two-level boosted regression, shape\nindexed features and a correlation-based feature selection method is used to\nimprove the performance. In this paper, we have explain the advantage of ESR\nfor deformable object like face pose estimation and reveal its generic\napplications of the method. In the experiment, we compare the results with\ndifferent work and demonstrate the accuracy and robustness in different\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 20:34:24 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["He", "Wenye", ""]]}, {"id": "1709.03196", "submitter": "Evgeniya Ustinova", "authors": "E. Ustinova, V. Lempitsky", "title": "Deep multi-frame face super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face verification and recognition problems have seen rapid progress in recent\nyears, however recognition from small size images remains a challenging task\nthat is inherently intertwined with the task of face super-resolution. Tackling\nthis problem using multiple frames is an attractive idea, yet requires solving\nthe alignment problem that is also challenging for low-resolution faces. Here\nwe present a holistic system for multi-frame recognition, alignment, and\nsuperresolution of faces. Our neural network architecture restores the central\nframe of each input sequence additionally taking into account a number of\nadjacent frames and making use of sub-pixel movements. We present our results\nusing the popular dataset for video face recognition (YouTube Faces). We show a\nnotable improvement of identification score compared to several baselines\nincluding the one based on single-image super-resolution.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 23:16:26 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 19:49:34 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Ustinova", "E.", ""], ["Lempitsky", "V.", ""]]}, {"id": "1709.03199", "submitter": "Duc Toan Bui", "authors": "Toan Duc Bui, Jitae Shin, and Taesup Moon", "title": "3D Densely Convolutional Networks for Volumetric Segmentation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the isointense stage, the accurate volumetric image segmentation is a\nchallenging task due to the low contrast between tissues. In this paper, we\npropose a novel very deep network architecture based on a densely convolutional\nnetwork for volumetric brain segmentation. The proposed network architecture\nprovides a dense connection between layers that aims to improve the information\nflow in the network. By concatenating features map of fine and coarse dense\nblocks, it allows capturing multi-scale contextual information. Experimental\nresults demonstrate significant advantages of the proposed method over existing\nmethods, in terms of both segmentation accuracy and parameter efficiency in\nMICCAI grand challenge on 6-month infant brain MRI segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 00:02:48 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 18:35:02 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Bui", "Toan Duc", ""], ["Shin", "Jitae", ""], ["Moon", "Taesup", ""]]}, {"id": "1709.03209", "submitter": "Rohun Tripathi", "authors": "Rohun Tripathi, Aman Gill, Riccha Tripati", "title": "Recurrent neural networks based Indic word-wise script identification\n  using character-wise training", "comments": "Version accepted at ICPRS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel methodology of Indic handwritten script\nrecognition using Recurrent Neural Networks and addresses the problem of script\nrecognition in poor data scenarios, such as when only character level online\ndata is available. It is based on the hypothesis that curves of online\ncharacter data comprise sufficient information for prediction at the word\nlevel. Online character data is used to train RNNs using BLSTM architecture\nwhich are then used to make predictions of online word level data. These\nprediction results on the test set are at par with prediction results of models\ntrained with online word data, while the training of the character level model\nis much less data intensive and takes much less time. Performance for\nbinary-script models and then 5 Indic script models are reported, along with\ncomparison with HMM models.The system is extended for offline data prediction.\nRaw offline data lacks the temporal information available in online data and\nrequired for prediction using models trained with online data. To overcome\nthis, stroke recovery is implemented and the strokes are utilized for\npredicting using the online character level models. The performance on\ncharacter and word level offline data is reported.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 01:35:22 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 15:01:36 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Tripathi", "Rohun", ""], ["Gill", "Aman", ""], ["Tripati", "Riccha", ""]]}, {"id": "1709.03272", "submitter": "Yuchen Dai", "authors": "Yuchen Dai, Zheng Huang, Yuting Gao, Youxuan Xu, Kai Chen, Jie Guo,\n  Weidong Qiu", "title": "Fused Text Segmentation Networks for Multi-oriented Scene Text Detection", "comments": "Accepted by ICPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel end-end framework for multi-oriented\nscene text detection from an instance-aware semantic segmentation perspective.\nWe present Fused Text Segmentation Networks, which combine multi-level features\nduring the feature extracting as text instance may rely on finer feature\nexpression compared to general objects. It detects and segments the text\ninstance jointly and simultaneously, leveraging merits from both semantic\nsegmentation task and region proposal based object detection task. Not\ninvolving any extra pipelines, our approach surpasses the current state of the\nart on multi-oriented scene text detection benchmarks: ICDAR2015 Incidental\nScene Text and MSRA-TD500 reaching Hmean 84.1% and 82.0% respectively. Morever,\nwe report a baseline on total-text containing curved text which suggests\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 07:25:37 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 08:24:30 GMT"}, {"version": "v3", "created": "Mon, 25 Dec 2017 13:45:49 GMT"}, {"version": "v4", "created": "Mon, 7 May 2018 06:05:52 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Dai", "Yuchen", ""], ["Huang", "Zheng", ""], ["Gao", "Yuting", ""], ["Xu", "Youxuan", ""], ["Chen", "Kai", ""], ["Guo", "Jie", ""], ["Qiu", "Weidong", ""]]}, {"id": "1709.03329", "submitter": "Inkyu Sa", "authors": "Inkyu Sa, Zetao Chen, Marija Popovic, Raghav Khanna, Frank Liebisch,\n  Juan Nieto, Roland Siegwart", "title": "weedNet: Dense Semantic Weed Classification Using Multispectral Images\n  and MAV for Smart Farming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective weed treatment is a critical step in autonomous crop management as\nrelated to crop health and yield. However, a key challenge is reliable, and\naccurate weed detection to minimize damage to surrounding plants. In this\npaper, we present an approach for dense semantic weed classification with\nmultispectral images collected by a micro aerial vehicle (MAV). We use the\nrecently developed encoder-decoder cascaded Convolutional Neural Network (CNN),\nSegnet, that infers dense semantic classes while allowing any number of input\nimage channels and class balancing with our sugar beet and weed datasets. To\nobtain training datasets, we established an experimental field with varying\nherbicide levels resulting in field plots containing only either crop or weed,\nenabling us to use the Normalized Difference Vegetation Index (NDVI) as a\ndistinguishable feature for automatic ground truth generation. We train 6\nmodels with different numbers of input channels and condition (fine-tune) it to\nachieve about 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification\nmetrics. For model deployment, an embedded GPU system (Jetson TX2) is tested\nfor MAV integration. Dataset used in this paper is released to support the\ncommunity and future work.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 10:59:01 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Sa", "Inkyu", ""], ["Chen", "Zetao", ""], ["Popovic", "Marija", ""], ["Khanna", "Raghav", ""], ["Liebisch", "Frank", ""], ["Nieto", "Juan", ""], ["Siegwart", "Roland", ""]]}, {"id": "1709.03376", "submitter": "Jiuxiang Gu Mr", "authors": "Jiuxiang Gu, Jianfei Cai, Gang Wang, Tsuhan Chen", "title": "Stack-Captioning: Coarse-to-Fine Learning for Image Captioning", "comments": "AAAI-2018, Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing image captioning approaches typically train a one-stage sentence\ndecoder, which is difficult to generate rich fine-grained descriptions. On the\nother hand, multi-stage image caption model is hard to train due to the\nvanishing gradient problem. In this paper, we propose a coarse-to-fine\nmulti-stage prediction framework for image captioning, composed of multiple\ndecoders each of which operates on the output of the previous stage, producing\nincreasingly refined image descriptions. Our proposed learning approach\naddresses the difficulty of vanishing gradients during training by providing a\nlearning objective function that enforces intermediate supervisions.\nParticularly, we optimize our model with a reinforcement learning approach\nwhich utilizes the output of each intermediate decoder's test-time inference\nalgorithm as well as the output of its preceding decoder to normalize the\nrewards, which simultaneously solves the well-known exposure bias problem and\nthe loss-evaluation mismatch problem. We extensively evaluate the proposed\napproach on MSCOCO and show that our approach can achieve the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 13:44:30 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 10:04:21 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 09:31:38 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Gu", "Jiuxiang", ""], ["Cai", "Jianfei", ""], ["Wang", "Gang", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1709.03395", "submitter": "Aravind Vasudevan", "authors": "Andrew Anderson, Aravind Vasudevan, Cormac Keane and David Gregg", "title": "Low-memory GEMM-based convolution algorithms for deep neural networks", "comments": "13 pages, 16 figures and 3 tables. arXiv admin note: text overlap\n  with arXiv:1704.04428", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) require very large amounts of computation both\nfor training and for inference when deployed in the field. A common approach to\nimplementing DNNs is to recast the most computationally expensive operations as\ngeneral matrix multiplication (GEMM). However, as we demonstrate in this paper,\nthere are a great many different ways to express DNN convolution operations\nusing GEMM. Although different approaches all perform the same number of\noperations, the size of temporary data structures differs significantly.\nConvolution of an input matrix with dimensions $C \\times H \\times W$, requires\n$O(K^2CHW)$ additional space using the classical im2col approach. More recently\nmemory-efficient approaches requiring just $O(KCHW)$ auxiliary space have been\nproposed.\n  We present two novel GEMM-based algorithms that require just $O(MHW)$ and\n$O(KW)$ additional space respectively, where $M$ is the number of channels in\nthe result of the convolution. These algorithms dramatically reduce the space\noverhead of DNN convolution, making it much more suitable for memory-limited\nembedded systems. Experimental evaluation shows that our low-memory algorithms\nare just as fast as the best patch-building approaches despite requiring just a\nfraction of the amount of additional memory. Our low-memory algorithms have\nexcellent data locality which gives them a further edge over patch-building\nalgorithms when multiple cores are used. As a result, our low memory algorithms\noften outperform the best patch-building algorithms using multiple threads.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 06:32:33 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Anderson", "Andrew", ""], ["Vasudevan", "Aravind", ""], ["Keane", "Cormac", ""], ["Gregg", "David", ""]]}, {"id": "1709.03399", "submitter": "Chris Bleakley", "authors": "Paul W. Connolly, Guenole C. Silvestre and Chris J. Bleakley", "title": "Automated Identification of Trampoline Skills Using Computer Vision\n  Extracted Pose Estimation", "comments": "8 pages, Irish Machine Vision and Image Processing Conference (IMVIP)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method to identify trampoline skills using a single video camera is\nproposed herein. Conventional computer vision techniques are used for\nidentification, estimation, and tracking of the gymnast's body in a video\nrecording of the routine. For each frame, an open source convolutional neural\nnetwork is used to estimate the pose of the athlete's body. Body orientation\nand joint angle estimates are extracted from these pose estimates. The\ntrajectories of these angle estimates over time are compared with those of\nlabelled reference skills. A nearest neighbour classifier utilising a mean\nsquared error distance metric is used to identify the skill performed. A\ndataset containing 714 skill examples with 20 distinct skills performed by\nadult male and female gymnasts was recorded and used for evaluation of the\nsystem. The system was found to achieve a skill identification accuracy of\n80.7% for the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 14:23:51 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Connolly", "Paul W.", ""], ["Silvestre", "Guenole C.", ""], ["Bleakley", "Chris J.", ""]]}, {"id": "1709.03409", "submitter": "Filip Radenovi\\'c", "authors": "Filip Radenovi\\'c, Giorgos Tolias, Ond\\v{r}ej Chum", "title": "Deep Shape Matching", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We cast shape matching as metric learning with convolutional networks. We\nbreak the end-to-end process of image representation into two parts. Firstly,\nwell established efficient methods are chosen to turn the images into edge\nmaps. Secondly, the network is trained with edge maps of landmark images, which\nare automatically obtained by a structure-from-motion pipeline. The learned\nrepresentation is evaluated on a range of different tasks, providing\nimprovements on challenging cases of domain generalization, generic\nsketch-based image retrieval or its fine-grained counterpart. In contrast to\nother methods that learn a different model per task, object category, or\ndomain, we use the same network throughout all our experiments, achieving\nstate-of-the-art results in multiple benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 14:33:42 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 20:11:12 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Radenovi\u0107", "Filip", ""], ["Tolias", "Giorgos", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1709.03410", "submitter": "Shray Bansal", "authors": "Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa and Byron Boots", "title": "One-Shot Learning for Semantic Segmentation", "comments": "To appear in the proceedings of the British Machine Vision Conference\n  (BMVC) 2017. The code is available at https://github.com/lzzcd001/OSLSM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-shot learning methods for image classification support learning from\nsparse data. We extend these techniques to support dense semantic image\nsegmentation. Specifically, we train a network that, given a small set of\nannotated images, produces parameters for a Fully Convolutional Network (FCN).\nWe use this FCN to perform dense pixel-level prediction on a test image for the\nnew semantic class. Our architecture shows a 25% relative meanIoU improvement\ncompared to the best baseline methods for one-shot segmentation on unseen\nclasses in the PASCAL VOC 2012 dataset and is at least 3 times faster.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 14:34:58 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Shaban", "Amirreza", ""], ["Bansal", "Shray", ""], ["Liu", "Zhen", ""], ["Essa", "Irfan", ""], ["Boots", "Byron", ""]]}, {"id": "1709.03439", "submitter": "Hansang Lee", "authors": "Han S. Lee, Alex A. Agarwal, Junmo Kim", "title": "Why Do Deep Neural Networks Still Not Recognize These Images?: A\n  Qualitative Analysis on Failure Cases of ImageNet Classification", "comments": "Poster presented at CVPR 2017 Scene Understanding Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent decade, ImageNet has become the most notable and powerful\nbenchmark database in computer vision and machine learning community. As\nImageNet has emerged as a representative benchmark for evaluating the\nperformance of novel deep learning models, its evaluation tends to include only\nquantitative measures such as error rate, rather than qualitative analysis.\nThus, there are few studies that analyze the failure cases of deep learning\nmodels in ImageNet, though there are numerous works analyzing the networks\nthemselves and visualizing them. In this abstract, we qualitatively analyze the\nfailure cases of ImageNet classification results from recent deep learning\nmodel, and categorize these cases according to the certain image patterns.\nThrough this failure analysis, we believe that it can be discovered what the\nfinal challenges are in ImageNet database, which the current deep learning\nmodel is still vulnerable to.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 15:35:05 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Lee", "Han S.", ""], ["Agarwal", "Alex A.", ""], ["Kim", "Junmo", ""]]}, {"id": "1709.03450", "submitter": "Mario Amrehn", "authors": "Mario Amrehn, Sven Gaube, Mathias Unberath, Frank Schebesch, Tim Horz,\n  Maddalena Strumia, Stefan Steidl, Markus Kowarschik, Andreas Maier", "title": "UI-Net: Interactive Artificial Neural Networks for Iterative Image\n  Segmentation Based on a User Model", "comments": "This work is submitted to the 2017 Eurographics Workshop on Visual\n  Computing for Biology and Medicine", "journal-ref": "Eurographics Workshop on Visual Computing for Biology and Medicine\n  (2017) 143-147", "doi": "10.2312/vcbm.20171248", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For complex segmentation tasks, fully automatic systems are inherently\nlimited in their achievable accuracy for extracting relevant objects.\nEspecially in cases where only few data sets need to be processed for a highly\naccurate result, semi-automatic segmentation techniques exhibit a clear benefit\nfor the user. One area of application is medical image processing during an\nintervention for a single patient. We propose a learning-based cooperative\nsegmentation approach which includes the computing entity as well as the user\ninto the task. Our system builds upon a state-of-the-art fully convolutional\nartificial neural network (FCN) as well as an active user model for training.\nDuring the segmentation process, a user of the trained system can iteratively\nadd additional hints in form of pictorial scribbles as seed points into the FCN\nsystem to achieve an interactive and precise segmentation result. The\nsegmentation quality of interactive FCNs is evaluated. Iterative FCN approaches\ncan yield superior results compared to networks without the user input channel\ncomponent, due to a consistent improvement in segmentation quality after each\ninteraction.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 15:50:24 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Amrehn", "Mario", ""], ["Gaube", "Sven", ""], ["Unberath", "Mathias", ""], ["Schebesch", "Frank", ""], ["Horz", "Tim", ""], ["Strumia", "Maddalena", ""], ["Steidl", "Stefan", ""], ["Kowarschik", "Markus", ""], ["Maier", "Andreas", ""]]}, {"id": "1709.03456", "submitter": "Jawad Tayyub", "authors": "Jawad Tayyub, Majd Hawasly, David C. Hogg and Anthony G. Cohn", "title": "CLAD: A Complex and Long Activities Dataset with Rich Crowdsourced\n  Annotations", "comments": null, "journal-ref": null, "doi": "10.5518/249", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel activity dataset which exhibits real-life and\ndiverse scenarios of complex, temporally-extended human activities and actions.\nThe dataset presents a set of videos of actors performing everyday activities\nin a natural and unscripted manner. The dataset was recorded using a static\nKinect 2 sensor which is commonly used on many robotic platforms. The dataset\ncomprises of RGB-D images, point cloud data, automatically generated skeleton\ntracks in addition to crowdsourced annotations. Furthermore, we also describe\nthe methodology used to acquire annotations through crowdsourcing. Finally some\nactivity recognition benchmarks are presented using current state-of-the-art\ntechniques. We believe that this dataset is particularly suitable as a testbed\nfor activity recognition research but it can also be applicable for other\ncommon tasks in robotics/computer vision research such as object detection and\nhuman skeleton tracking.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 16:01:17 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 16:52:04 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Tayyub", "Jawad", ""], ["Hawasly", "Majd", ""], ["Hogg", "David C.", ""], ["Cohn", "Anthony G.", ""]]}, {"id": "1709.03481", "submitter": "Shubham Pachori", "authors": "Sainandan Ramakrishnan, Shubham Pachori. Aalok Gangopadhyay,\n  Shanmuganathan Raman", "title": "Deep Generative Filter for Motion Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing blur caused by camera shake in images has always been a challenging\nproblem in computer vision literature due to its ill-posed nature. Motion blur\ncaused due to the relative motion between the camera and the object in 3D space\ninduces a spatially varying blurring effect over the entire image. In this\npaper, we propose a novel deep filter based on Generative Adversarial Network\n(GAN) architecture integrated with global skip connection and dense\narchitecture in order to tackle this problem. Our model, while bypassing the\nprocess of blur kernel estimation, significantly reduces the test time which is\nnecessary for practical applications. The experiments on the benchmark datasets\nprove the effectiveness of the proposed method which outperforms the\nstate-of-the-art blind deblurring algorithms both quantitatively and\nqualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 17:18:26 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Ramakrishnan", "Sainandan", ""], ["Gangopadhyay", "Shubham Pachori. Aalok", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1709.03485", "submitter": "Eli Gibson", "authors": "Eli Gibson, Wenqi Li, Carole Sudre, Lucas Fidon, Dzhoshkun I. Shakir,\n  Guotai Wang, Zach Eaton-Rosen, Robert Gray, Tom Doel, Yipeng Hu, Tom Whyntie,\n  Parashkev Nachev, Marc Modat, Dean C. Barratt, S\\'ebastien Ourselin, M. Jorge\n  Cardoso and Tom Vercauteren", "title": "NiftyNet: a deep-learning platform for medical imaging", "comments": "Wenqi Li and Eli Gibson contributed equally to this work. M. Jorge\n  Cardoso and Tom Vercauteren contributed equally to this work. 26 pages, 6\n  figures; Update includes additional applications, updated author list and\n  formatting for journal submission", "journal-ref": null, "doi": "10.1016/j.cmpb.2018.01.025", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image analysis and computer-assisted intervention problems are\nincreasingly being addressed with deep-learning-based solutions. Established\ndeep-learning platforms are flexible but do not provide specific functionality\nfor medical image analysis and adapting them for this application requires\nsubstantial implementation effort. Thus, there has been substantial duplication\nof effort and incompatible infrastructure developed across many research\ngroups. This work presents the open-source NiftyNet platform for deep learning\nin medical imaging. The ambition of NiftyNet is to accelerate and simplify the\ndevelopment of these solutions, and to provide a common mechanism for\ndisseminating research outputs for the community to use, adapt and build upon.\n  NiftyNet provides a modular deep-learning pipeline for a range of medical\nimaging applications including segmentation, regression, image generation and\nrepresentation learning applications. Components of the NiftyNet pipeline\nincluding data loading, data augmentation, network architectures, loss\nfunctions and evaluation metrics are tailored to, and take advantage of, the\nidiosyncracies of medical image analysis and computer-assisted intervention.\nNiftyNet is built on TensorFlow and supports TensorBoard visualization of 2D\nand 3D images and computational graphs by default.\n  We present 3 illustrative medical image analysis applications built using\nNiftyNet: (1) segmentation of multiple abdominal organs from computed\ntomography; (2) image regression to predict computed tomography attenuation\nmaps from brain magnetic resonance images; and (3) generation of simulated\nultrasound images for specified anatomical poses.\n  NiftyNet enables researchers to rapidly develop and distribute deep learning\nsolutions for segmentation, regression, image generation and representation\nlearning applications, or extend the platform to new applications.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 17:42:10 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 13:46:31 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Gibson", "Eli", ""], ["Li", "Wenqi", ""], ["Sudre", "Carole", ""], ["Fidon", "Lucas", ""], ["Shakir", "Dzhoshkun I.", ""], ["Wang", "Guotai", ""], ["Eaton-Rosen", "Zach", ""], ["Gray", "Robert", ""], ["Doel", "Tom", ""], ["Hu", "Yipeng", ""], ["Whyntie", "Tom", ""], ["Nachev", "Parashkev", ""], ["Modat", "Marc", ""], ["Barratt", "Dean C.", ""], ["Ourselin", "S\u00e9bastien", ""], ["Cardoso", "M. Jorge", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1709.03524", "submitter": "Syed Ammar Abbas", "authors": "Syed Ammar Abbas, Sibt ul Hussain", "title": "Recovering Homography from Camera Captured Documents using Convolutional\n  Neural Networks", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing perspective distortion from hand held camera captured document\nimages is one of the primitive tasks in document analysis, but unfortunately,\nno such method exists that can reliably remove the perspective distortion from\ndocument images automatically. In this paper, we propose a convolutional neural\nnetwork based method for recovering homography from hand-held camera captured\ndocuments.\n  Our proposed method works independent of document's underlying content and is\ntrained end-to-end in a fully automatic way. Specifically, this paper makes\nfollowing three contributions: Firstly, we introduce a large scale synthetic\ndataset for recovering homography from documents images captured under\ndifferent geometric and photometric transformations; secondly, we show that a\ngeneric convolutional neural network based architecture can be successfully\nused for regressing the corners positions of documents captured under wild\nsettings; thirdly, we show that L1 loss can be reliably used for corners\nregression. Our proposed method gives state-of-the-art performance on the\ntested datasets, and has potential to become an integral part of document\nanalysis pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 18:08:58 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Abbas", "Syed Ammar", ""], ["Hussain", "Sibt ul", ""]]}, {"id": "1709.03548", "submitter": "Teresa Brooks", "authors": "Teresa Nicole Brooks", "title": "Exploring Geometric Property Thresholds For Filtering Non-Text Regions\n  In A Connected Component Based Text Detection Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated text detection is a difficult computer vision task. In order to\naccurately detect and identity text in an image or video, two major problems\nmust be addressed. The primary problem is implementing a robust and reliable\nmethod for distinguishing text vs non-text regions in images and videos. Part\nof the difficulty stems from the almost unlimited combinations of fonts,\nlighting conditions, distortions, and other variations that can be found in\nimages and videos. This paper explores key properties of two popular and proven\nmethods for implementing text detection; maximum stable external regions (MSER)\nand stroke width variation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 19:05:53 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Brooks", "Teresa Nicole", ""]]}, {"id": "1709.03553", "submitter": "Ding Zhao", "authors": "Wenshuo Wang and Ding Zhao", "title": "Extracting Traffic Primitives Directly from Naturalistically Logged Data\n  for Self-Driving Applications", "comments": "7 pages, 8 figures, 2 tables, ICRA 2018", "journal-ref": null, "doi": "10.1109/LRA.2018.2794604", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing an automated vehicle, that can handle complicated driving\nscenarios and appropriately interact with other road users, requires the\nability to semantically learn and understand driving environment, oftentimes,\nbased on analyzing massive amounts of naturalistic driving data. An important\nparadigm that allows automated vehicles to both learn from human drivers and\ngain insights is understanding the principal compositions of the entire\ntraffic, termed as traffic primitives. However, the exploding data growth\npresents a great challenge in extracting primitives from high-dimensional\ntime-series traffic data with various types of road users engaged. Therefore,\nautomatically extracting primitives is becoming one of the cost-efficient ways\nto help autonomous vehicles understand and predict the complex traffic\nscenarios. In addition, the extracted primitives from raw data should 1) be\nappropriate for automated driving applications and also 2) be easily used to\ngenerate new traffic scenarios. However, existing literature does not provide a\nmethod to automatically learn these primitives from large-scale traffic data.\nThe contribution of this paper has two manifolds. The first one is that we\nproposed a new framework to generate new traffic scenarios from a handful of\nlimited traffic data. The second one is that we introduce a nonparametric\nBayesian learning method -- a sticky hierarchical Dirichlet process hidden\nMarkov model -- to automatically extract primitives from multidimensional\ntraffic data without prior knowledge of the primitive settings. The developed\nmethod is then validated using one day of naturalistic driving data. Experiment\nresults show that the nonparametric Bayesian learning method is able to extract\nprimitives from traffic scenarios where both the binary and continuous events\ncoexist.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 19:14:42 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 18:09:46 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 03:05:45 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Wang", "Wenshuo", ""], ["Zhao", "Ding", ""]]}, {"id": "1709.03572", "submitter": "Samuel Murray", "authors": "Samuel Murray", "title": "Real-Time Multiple Object Tracking - A Study on the Importance of Speed", "comments": "Master Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we implement a multiple object tracker, following the\ntracking-by-detection paradigm, as an extension of an existing method. It works\nby modelling the movement of objects by solving the filtering problem, and\nassociating detections with predicted new locations in new frames using the\nHungarian algorithm. Three different similarity measures are used, which use\nthe location and shape of the bounding boxes. Compared to other trackers on the\nMOTChallenge leaderboard, our method, referred to as C++SORT, is the fastest\nnon-anonymous submission, while also achieving decent score on other metrics.\nBy running our model on the Okutama-Action dataset, sampled at different\nframe-rates, we show that the performance is greatly reduced when running the\nmodel - including detecting objects - in real-time. In most metrics, the score\nis reduced by 50%, but in certain cases as much as 90%. We argue that this\nindicates that other, slower methods could not be used for tracking in\nreal-time, but that more research is required specifically on this.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 20:16:22 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 16:26:37 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Murray", "Samuel", ""]]}, {"id": "1709.03582", "submitter": "Valentin Khrulkov", "authors": "Valentin Khrulkov and Ivan Oseledets", "title": "Art of singular vectors and universal adversarial perturbations", "comments": "Submitted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been\nattracting a lot of attention in recent studies. It has been shown that for\nmany state of the art DNNs performing image classification there exist\nuniversal adversarial perturbations --- image-agnostic perturbations mere\naddition of which to natural images with high probability leads to their\nmisclassification. In this work we propose a new algorithm for constructing\nsuch universal perturbations. Our approach is based on computing the so-called\n$(p, q)$-singular vectors of the Jacobian matrices of hidden layers of a\nnetwork. Resulting perturbations present interesting visual patterns, and by\nusing only 64 images we were able to construct universal perturbations with\nmore than 60 \\% fooling rate on the dataset consisting of 50000 images. We also\ninvestigate a correlation between the maximal singular value of the Jacobian\nmatrix and the fooling rate of the corresponding singular vector, and show that\nthe constructed perturbations generalize across networks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 20:22:37 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 03:09:07 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Khrulkov", "Valentin", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1709.03588", "submitter": "Foteini Fotopoulou", "authors": "Foteini Fotopoulou (1) and George Economou (2) ((1) Department of\n  Computer Engineering and Informatics, University of Patras, Greece, (2)\n  Department of Physics, University of Patras, Greece)", "title": "On the definition of Shape Parts: a Dominant Sets Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper a novel graph-based approach to the shape decomposition\nproblem is addressed. The shape is appropriately transformed into a visibility\ngraph enriched with local neighborhood information. A two-step diffusion\nprocess is then applied to the visibility graph that efficiently enhances the\ninformation provided, thus leading to a more robust and meaningful graph\nconstruction. Inspired by the notion of a clique as a strict cluster\ndefinition, the dominant sets algorithm is invoked, slightly modified to\ncomport with the specific problem of defining shape parts. The cluster\ncohesiveness and a node participation vector are two important outputs of the\nproposed graph partitioning method. Opposed to most of the existing techniques,\nthe final number of the clusters is determined automatically, by estimating the\ncluster cohesiveness on a random network generation process. Experimental\nresults on several shape databases show the effectiveness of our framework for\ngraph based shape decomposition.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 20:40:49 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Fotopoulou", "Foteini", ""], ["Economou", "George", ""]]}, {"id": "1709.03612", "submitter": "Qizhu Li", "authors": "Qizhu Li, Anurag Arnab and Philip H.S. Torr", "title": "Holistic, Instance-Level Human Parsing", "comments": "Poster at BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object parsing -- the task of decomposing an object into its semantic parts\n-- has traditionally been formulated as a category-level segmentation problem.\nConsequently, when there are multiple objects in an image, current methods\ncannot count the number of objects in the scene, nor can they determine which\npart belongs to which object. We address this problem by segmenting the parts\nof objects at an instance-level, such that each pixel in the image is assigned\na part label, as well as the identity of the object it belongs to. Moreover, we\nshow how this approach benefits us in obtaining segmentations at coarser\ngranularities as well. Our proposed network is trained end-to-end given\ndetections, and begins with a category-level segmentation module. Thereafter, a\ndifferentiable Conditional Random Field, defined over a variable number of\ninstances for every input image, reasons about the identity of each part by\nassociating it with a human detection. In contrast to other approaches, our\nmethod can handle the varying number of people in each image and our holistic\nnetwork produces state-of-the-art results in instance-level part and human\nsegmentation, together with competitive results in category-level part\nsegmentation, all achieved by a single forward-pass through our neural network.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 22:06:34 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Li", "Qizhu", ""], ["Arnab", "Anurag", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1709.03654", "submitter": "Yi Li", "authors": "Yi Li, Lingxiao Song, Xiang Wu, Ran He, and Tieniu Tan", "title": "Anti-Makeup: Learning A Bi-Level Adversarial Network for\n  Makeup-Invariant Face Verification", "comments": "The paper is accepted by AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Makeup is widely used to improve facial attractiveness and is well accepted\nby the public. However, different makeup styles will result in significant\nfacial appearance changes. It remains a challenging problem to match makeup and\nnon-makeup face images. This paper proposes a learning from generation approach\nfor makeup-invariant face verification by introducing a bi-level adversarial\nnetwork (BLAN). To alleviate the negative effects from makeup, we first\ngenerate non-makeup images from makeup ones, and then use the synthesized\nnon-makeup images for further verification. Two adversarial networks in BLAN\nare integrated in an end-to-end deep network, with the one on pixel level for\nreconstructing appealing facial images and the other on feature level for\npreserving identity information. These two networks jointly reduce the sensing\ngap between makeup and non-makeup images. Moreover, we make the generator well\nconstrained by incorporating multiple perceptual losses. Experimental results\non three benchmark makeup face datasets demonstrate that our method achieves\nstate-of-the-art verification accuracy across makeup status and can produce\nphoto-realistic non-makeup face images.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 02:08:40 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 13:04:41 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Li", "Yi", ""], ["Song", "Lingxiao", ""], ["Wu", "Xiang", ""], ["He", "Ran", ""], ["Tan", "Tieniu", ""]]}, {"id": "1709.03655", "submitter": "Jiagang Zhu", "authors": "Jiagang Zhu, Wei Zou, Zheng Zhu", "title": "Learning Gating ConvNet for Two-Stream based Methods in Action\n  Recognition", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the two-stream style methods in action recognition, fusing the two\nstreams' predictions is always by the weighted averaging scheme. This fusion\nmethod with fixed weights lacks of pertinence to different action videos and\nalways needs trial and error on the validation set. In order to enhance the\nadaptability of two-stream ConvNets and improve its performance, an end-to-end\ntrainable gated fusion method, namely gating ConvNet, for the two-stream\nConvNets is proposed in this paper based on the MoE (Mixture of Experts)\ntheory. The gating ConvNet takes the combination of feature maps from the same\nlayer of the spatial and the temporal nets as input and adopts ReLU (Rectified\nLinear Unit) as the gating output activation function. To reduce the\nover-fitting of gating ConvNet caused by the redundancy of parameters, a new\nmulti-task learning method is designed, which jointly learns the gating fusion\nweights for the two streams and learns the gating ConvNet for action\nclassification. With our gated fusion method and multi-task learning approach,\na high accuracy of 94.5% is achieved on the dataset UCF101.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 02:09:04 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 09:13:31 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Zhu", "Jiagang", ""], ["Zou", "Wei", ""], ["Zhu", "Zheng", ""]]}, {"id": "1709.03656", "submitter": "Nan Xu", "authors": "Nan Xu, Yanqing Guo, Jiujun Wang, Xiangyang Luo, and Ran He", "title": "Joint Adaptive Neighbours and Metric Learning for Multi-view Subspace\n  Clustering", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the existence of various views or representations in many real-world\ndata, multi-view learning has drawn much attention recently. Multi-view\nspectral clustering methods based on similarity matrixes or graphs are pretty\npopular. Generally, these algorithms learn informative graphs by directly\nutilizing original data. However, in the real-world applications, original data\noften contain noises and outliers that lead to unreliable graphs. In addition,\ndifferent views may have different contributions to data clustering. In this\npaper, a novel Multiview Subspace Clustering method unifying Adaptive\nneighbours and Metric learning (MSCAM), is proposed to address the above\nproblems. In this method, we use the subspace representations of different\nviews to adaptively learn a consensus similarity matrix, uncovering the\nsubspace structure and avoiding noisy nature of original data. For all views,\nwe also learn different Mahalanobis matrixes that parameterize the squared\ndistances and consider the contributions of different views. Further, we\nconstrain the graph constructed by the similarity matrix to have exact c (c is\nthe number of clusters) connected components. An iterative algorithm is\ndeveloped to solve this optimization problem. Moreover, experiments on a\nsynthetic dataset and different real-world datasets demonstrate the\neffectiveness of MSCAM.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 02:09:29 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Xu", "Nan", ""], ["Guo", "Yanqing", ""], ["Wang", "Jiujun", ""], ["Luo", "Xiangyang", ""], ["He", "Ran", ""]]}, {"id": "1709.03675", "submitter": "Ran He", "authors": "Lingxiao Song and Man Zhang and Xiang Wu and Ran He", "title": "Adversarial Discriminative Heterogeneous Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gap between sensing patterns of different face modalities remains a\nchallenging problem in heterogeneous face recognition (HFR). This paper\nproposes an adversarial discriminative feature learning framework to close the\nsensing gap via adversarial learning on both raw-pixel space and compact\nfeature space. This framework integrates cross-spectral face hallucination and\ndiscriminative feature learning into an end-to-end adversarial network. In the\npixel space, we make use of generative adversarial networks to perform\ncross-spectral face hallucination. An elaborate two-path model is introduced to\nalleviate the lack of paired images, which gives consideration to both global\nstructures and local textures. In the feature space, an adversarial loss and a\nhigh-order variance discrepancy loss are employed to measure the global and\nlocal discrepancy between two heterogeneous distributions respectively. These\ntwo losses enhance domain-invariant feature learning and modality independent\nnoise removing. Experimental results on three NIR-VIS databases show that our\nproposed approach outperforms state-of-the-art HFR methods, without requiring\nof complex network or large-scale training dataset.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 03:22:45 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Song", "Lingxiao", ""], ["Zhang", "Man", ""], ["Wu", "Xiang", ""], ["He", "Ran", ""]]}, {"id": "1709.03688", "submitter": "Soheil Kolouri", "authors": "Soheil Kolouri, Mohammad Rostami, Yuri Owechko, Kyungnam Kim", "title": "Joint Dictionaries for Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic approach toward zero-shot learning (ZSL) is to map the input domain\nto a set of semantically meaningful attributes that could be used later on to\nclassify unseen classes of data (e.g. visual data). In this paper, we propose\nto learn a visual feature dictionary that has semantically meaningful atoms.\nSuch dictionary is learned via joint dictionary learning for the visual domain\nand the attribute domain, while enforcing the same sparse coding for both\ndictionaries. Our novel attribute aware formulation provides an algorithmic\nsolution to the domain shift/hubness problem in ZSL. Upon learning the joint\ndictionaries, images from unseen classes can be mapped into the attribute space\nby finding the attribute aware joint sparse representation using solely the\nvisual data. We demonstrate that our approach provides superior or comparable\nperformance to that of the state of the art on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 04:41:33 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Kolouri", "Soheil", ""], ["Rostami", "Mohammad", ""], ["Owechko", "Yuri", ""], ["Kim", "Kyungnam", ""]]}, {"id": "1709.03697", "submitter": "Victor Stamatescu", "authors": "Victor Stamatescu, Peter Barsznica, Manjung Kim, Kin K. Liu, Mark\n  McKenzie, Will Meakin, Gwilyn Saunders, Sebastien C. Wong and Russell S. A.\n  Brinkworth", "title": "Automatic Ground Truths: Projected Image Annotations for Omnidirectional\n  Vision", "comments": "2017 International Conference on Digital Image Computing: Techniques\n  and Applications (DICTA), Page 1 - 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel data set made up of omnidirectional video of multiple\nobjects whose centroid positions are annotated automatically. Omnidirectional\nvision is an active field of research focused on the use of spherical imagery\nin video analysis and scene understanding, involving tasks such as object\ndetection, tracking and recognition. Our goal is to provide a large and\nconsistently annotated video data set that can be used to train and evaluate\nnew algorithms for these tasks. Here we describe the experimental setup and\nsoftware environment used to capture and map the 3D ground truth positions of\nmultiple objects into the image. Furthermore, we estimate the expected\nsystematic error on the mapped positions. In addition to final data products,\nwe release publicly the software tools and raw data necessary to re-calibrate\nthe camera and/or redo this mapping. The software also provides a simple\nframework for comparing the results of standard image annotation tools or\nvisual tracking systems against our mapped ground truth annotations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 05:38:42 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Stamatescu", "Victor", ""], ["Barsznica", "Peter", ""], ["Kim", "Manjung", ""], ["Liu", "Kin K.", ""], ["McKenzie", "Mark", ""], ["Meakin", "Will", ""], ["Saunders", "Gwilyn", ""], ["Wong", "Sebastien C.", ""], ["Brinkworth", "Russell S. A.", ""]]}, {"id": "1709.03698", "submitter": "Bo Chang", "authors": "Bo Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert and\n  Elliot Holtham", "title": "Reversible Architectures for Arbitrarily Deep Residual Neural Networks", "comments": "Accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep residual networks have been successfully applied in many\ncomputer vision and natural language processing tasks, pushing the\nstate-of-the-art performance with deeper and wider architectures. In this work,\nwe interpret deep residual networks as ordinary differential equations (ODEs),\nwhich have long been studied in mathematics and physics with rich theoretical\nand empirical success. From this interpretation, we develop a theoretical\nframework on stability and reversibility of deep neural networks, and derive\nthree reversible neural network architectures that can go arbitrarily deep in\ntheory. The reversibility property allows a memory-efficient implementation,\nwhich does not need to store the activations for most hidden layers. Together\nwith the stability of our architectures, this enables training deeper networks\nusing only modest computational resources. We provide both theoretical analyses\nand empirical results. Experimental results demonstrate the efficacy of our\narchitectures against several strong baselines on CIFAR-10, CIFAR-100 and\nSTL-10 with superior or on-par state-of-the-art performance. Furthermore, we\nshow our architectures yield superior results when trained using fewer training\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 05:41:13 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 22:10:57 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Chang", "Bo", ""], ["Meng", "Lili", ""], ["Haber", "Eldad", ""], ["Ruthotto", "Lars", ""], ["Begert", "David", ""], ["Holtham", "Elliot", ""]]}, {"id": "1709.03708", "submitter": "Yusuke Matsui", "authors": "Yusuke Matsui, Keisuke Ogaki, Toshihiko Yamasaki, Kiyoharu Aizawa", "title": "PQk-means: Billion-scale Clustering for Product-quantized Codes", "comments": "To appear in ACMMM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is a fundamental operation in data analysis. For handling\nlarge-scale data, the standard k-means clustering method is not only slow, but\nalso memory-inefficient. We propose an efficient clustering method for\nbillion-scale feature vectors, called PQk-means. By first compressing input\nvectors into short product-quantized (PQ) codes, PQk-means achieves fast and\nmemory-efficient clustering, even for high-dimensional vectors. Similar to\nk-means, PQk-means repeats the assignment and update steps, both of which can\nbe performed in the PQ-code domain. Experimental results show that even\nshort-length (32 bit) PQ-codes can produce competitive results compared with\nk-means. This result is of practical importance for clustering in\nmemory-restricted environments. Using the proposed PQk-means scheme, the\nclustering of one billion 128D SIFT features with K = 10^5 is achieved within\n14 hours, using just 32 GB of memory consumption on a single computer.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 07:00:18 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Matsui", "Yusuke", ""], ["Ogaki", "Keisuke", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1709.03717", "submitter": "Ehsan Arbabi", "authors": "Ehsan Arbabi, Mohammad Shabani and Ali Yarigholi", "title": "A low cost non-wearable gaze detection system based on infrared image\n  processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human eye gaze detection plays an important role in various fields, including\nhuman-computer interaction, virtual reality and cognitive science. Although\ndifferent relatively accurate systems of eye tracking and gaze detection exist,\nthey are usually either too expensive to be bought for low cost applications or\ntoo complex to be implemented easily. In this article, we propose a\nnon-wearable system for eye tracking and gaze detection with low complexity and\ncost. The proposed system provides a medium accuracy which makes it suitable\nfor general applications in which low cost and easy implementation is more\nimportant than achieving very precise gaze detection. The proposed method\nincludes pupil and marker detection using infrared image processing, and gaze\nevaluation using an interpolation-based strategy. The interpolation-based\nstrategy exploits the positions of the detected pupils and markers in a target\ncaptured image and also in some previously captured training images for\nestimating the position of a point that the user is gazing at. The proposed\nsystem has been evaluated by three users in two different lighting conditions.\nThe experimental results show that the accuracy of this low cost system can be\nbetween 90% and 100% for finding major gazing directions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 07:46:05 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Arbabi", "Ehsan", ""], ["Shabani", "Mohammad", ""], ["Yarigholi", "Ali", ""]]}, {"id": "1709.03739", "submitter": "Tadashi Matsuo", "authors": "Tadashi Matsuo, Nobutaka Shimada", "title": "Construction of Latent Descriptor Space and Inference Model of\n  Hand-Object Interactions", "comments": null, "journal-ref": "IEICE Trans. on Info. and Sys., Vol.E100-D, No.6, pp.1350-1359,\n  2017", "doi": "10.1587/transinf.2016EDP7410", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance-based generic object recognition is a challenging problem because\nall possible appearances of objects cannot be registered, especially as new\nobjects are produced every day. Function of objects, however, has a\ncomparatively small number of prototypes. Therefore, function-based\nclassification of new objects could be a valuable tool for generic object\nrecognition. Object functions are closely related to hand-object interactions\nduring handling of a functional object; i.e., how the hand approaches the\nobject, which parts of the object and contact the hand, and the shape of the\nhand during interaction. Hand-object interactions are helpful for modeling\nobject functions. However, it is difficult to assign discrete labels to\ninteractions because an object shape and grasping hand-postures intrinsically\nhave continuous variations. To describe these interactions, we propose the\ninteraction descriptor space which is acquired from unlabeled appearances of\nhuman hand-object interactions. By using interaction descriptors, we can\nnumerically describe the relation between an object's appearance and its\npossible interaction with the hand. The model infers the quantitative state of\nthe interaction from the object image alone. It also identifies the parts of\nobjects designed for hand interactions such as grips and handles. We\ndemonstrate that the proposed method can unsupervisedly generate interaction\ndescriptors that make clusters corresponding to interaction types. And also we\ndemonstrate that the model can infer possible hand-object interactions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 08:34:23 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Matsuo", "Tadashi", ""], ["Shimada", "Nobutaka", ""]]}, {"id": "1709.03749", "submitter": "Siavash Arjomand Bigdeli", "authors": "Siavash Arjomand Bigdeli, Meiguang Jin, Paolo Favaro, Matthias Zwicker", "title": "Deep Mean-Shift Priors for Image Restoration", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a natural image prior that directly represents a\nGaussian-smoothed version of the natural image distribution. We include our\nprior in a formulation of image restoration as a Bayes estimator that also\nallows us to solve noise-blind image restoration problems. We show that the\ngradient of our prior corresponds to the mean-shift vector on the natural image\ndistribution. In addition, we learn the mean-shift vector field using denoising\nautoencoders, and use it in a gradient descent approach to perform Bayes risk\nminimization. We demonstrate competitive results for noise-blind deblurring,\nsuper-resolution, and demosaicing.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 09:11:24 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 13:13:53 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Bigdeli", "Siavash Arjomand", ""], ["Jin", "Meiguang", ""], ["Favaro", "Paolo", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1709.03754", "submitter": "Tadashi Matsuo", "authors": "Tadashi Matsuo, Hiroya Fukuhara, Nobutaka Shimada", "title": "Transform Invariant Auto-encoder", "comments": "6 pages, 17 figures, to be published in IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The auto-encoder method is a type of dimensionality reduction method. A\nmapping from a vector to a descriptor that represents essential information can\nbe automatically generated from a set of vectors without any supervising\ninformation. However, an image and its spatially shifted version are encoded\ninto different descriptors by an existing ordinary auto-encoder because each\ndescriptor includes a spatial subpattern and its position. To generate a\ndescriptor representing a spatial subpattern in an image, we need to normalize\nits spatial position in the images prior to training an ordinary auto-encoder;\nhowever, such a normalization is generally difficult for images without obvious\nstandard positions. We propose a transform invariant auto-encoder and an\ninference model of transform parameters. By the proposed method, we can\nseparate an input into a transform invariant descriptor and transform\nparameters. The proposed method can be applied to various auto-encoders without\nrequiring any special modules or labeled training samples. By applying it to\nshift transforms, we can achieve a shift invariant auto-encoder that can\nextract a typical spatial subpattern independent of its relative position in a\nwindow. In addition, we can achieve a model that can infer shift parameters\nrequired to restore the input from the typical subpattern. As an example of the\nproposed method, we demonstrate that a descriptor generated by a shift\ninvariant auto-encoder can represent a typical spatial subpattern. In addition,\nwe demonstrate the imitation of a human hand by a robot hand as an example of a\nregression based on spatial subpatterns.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 09:19:34 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Matsuo", "Tadashi", ""], ["Fukuhara", "Hiroya", ""], ["Shimada", "Nobutaka", ""]]}, {"id": "1709.03763", "submitter": "Robert Maier", "authors": "Robert Maier, Raphael Schaller, Daniel Cremers", "title": "Efficient Online Surface Correction for Real-time Large-Scale 3D\n  Reconstruction", "comments": "British Machine Vision Conference (BMVC), London, September 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for large-scale 3D reconstruction from RGB-D sensors\nusually reduce drift in camera tracking by globally optimizing the estimated\ncamera poses in real-time without simultaneously updating the reconstructed\nsurface on pose changes. We propose an efficient on-the-fly surface correction\nmethod for globally consistent dense 3D reconstruction of large-scale scenes.\nOur approach uses a dense Visual RGB-D SLAM system that estimates the camera\nmotion in real-time on a CPU and refines it in a global pose graph\noptimization. Consecutive RGB-D frames are locally fused into keyframes, which\nare incorporated into a sparse voxel hashed Signed Distance Field (SDF) on the\nGPU. On pose graph updates, the SDF volume is corrected on-the-fly using a\nnovel keyframe re-integration strategy with reduced GPU-host streaming. We\ndemonstrate in an extensive quantitative evaluation that our method is up to\n93% more runtime efficient compared to the state-of-the-art and requires\nsignificantly less memory, with only negligible loss of surface quality.\nOverall, our system requires only a single GPU and allows for real-time surface\ncorrection of large environments.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 09:44:23 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Maier", "Robert", ""], ["Schaller", "Raphael", ""], ["Cremers", "Daniel", ""]]}, {"id": "1709.03792", "submitter": "Faxian Cao", "authors": "Faxian Cao, Zhijing Yang, Jinchang Ren, Wing-Kuen Ling", "title": "Sparse Representation Based Augmented Multinomial Logistic Extreme\n  Learning Machine with Weighted Composite Features for Spectral Spatial\n  Hyperspectral Image Classification", "comments": "16 pages, 6 figuers and 4 tables", "journal-ref": null, "doi": "10.1109/TGRS.2018.2828601", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although extreme learning machine (ELM) has been successfully applied to a\nnumber of pattern recognition problems, it fails to pro-vide sufficient good\nresults in hyperspectral image (HSI) classification due to two main drawbacks.\nThe first is due to the random weights and bias of ELM, which may lead to\nill-posed problems. The second is the lack of spatial information for\nclassification. To tackle these two problems, in this paper, we propose a new\nframework for ELM based spectral-spatial classification of HSI, where\nprobabilistic modelling with sparse representation and weighted composite\nfeatures (WCF) are employed respectively to derive the op-timized output\nweights and extract spatial features. First, the ELM is represented as a\nconcave logarithmic likelihood function under statistical modelling using the\nmaximum a posteriori (MAP). Second, the sparse representation is applied to the\nLaplacian prior to effi-ciently determine a logarithmic posterior with a unique\nmaximum in order to solve the ill-posed problem of ELM. The variable splitting\nand the augmented Lagrangian are subsequently used to further reduce the\ncomputation complexity of the proposed algorithm and it has been proven a more\nefficient method for speed improvement. Third, the spatial information is\nextracted using the weighted compo-site features (WCFs) to construct the\nspectral-spatial classification framework. In addition, the lower bound of the\nproposed method is derived by a rigorous mathematical proof. Experimental\nresults on two publicly available HSI data sets demonstrate that the proposed\nmethodology outperforms ELM and a number of state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 11:39:51 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 12:25:21 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Cao", "Faxian", ""], ["Yang", "Zhijing", ""], ["Ren", "Jinchang", ""], ["Ling", "Wing-Kuen", ""]]}, {"id": "1709.03806", "submitter": "Hansang Lee", "authors": "Han S. Lee, Heechul Jung, Alex A. Agarwal, Junmo Kim", "title": "Can Deep Neural Networks Match the Related Objects?: A Survey on\n  ImageNet-trained Classification Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have shown the state-of-the-art level of\nperformances in wide range of complicated tasks. In recent years, the studies\nhave been actively conducted to analyze the black box characteristics of DNNs\nand to grasp the learning behaviours, tendency, and limitations of DNNs. In\nthis paper, we investigate the limitation of DNNs in image classification task\nand verify it with the method inspired by cognitive psychology. Through\nanalyzing the failure cases of ImageNet classification task, we hypothesize\nthat the DNNs do not sufficiently learn to associate related classes of\nobjects. To verify how DNNs understand the relatedness between object classes,\nwe conducted experiments on the image database provided in cognitive\npsychology. We applied the ImageNet-trained DNNs to the database consisting of\npairs of related and unrelated object images to compare the feature\nsimilarities and determine whether the pairs match each other. In the\nexperiments, we observed that the DNNs show limited performance in determining\nrelatedness between object classes. In addition, the DNNs present somewhat\nimproved performance in discovering relatedness based on similarity, but they\nperform weaker in discovering relatedness based on association. Through these\nexperiments, a novel analysis of learning behaviour of DNNs is provided and the\nlimitation which needs to be overcome is suggested.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 12:35:47 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Lee", "Han S.", ""], ["Jung", "Heechul", ""], ["Agarwal", "Alex A.", ""], ["Kim", "Junmo", ""]]}, {"id": "1709.03820", "submitter": "Massimiliano Patacchiola", "authors": "Luca Surace, Massimiliano Patacchiola, Elena Battini S\\\"onmez, William\n  Spataro, Angelo Cangelosi", "title": "Emotion Recognition in the Wild using Deep Neural Networks and Bayesian\n  Classifiers", "comments": "accepted by the Fifth Emotion Recognition in the Wild (EmotiW)\n  Challenge 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group emotion recognition in the wild is a challenging problem, due to the\nunstructured environments in which everyday life pictures are taken. Some of\nthe obstacles for an effective classification are occlusions, variable lighting\nconditions, and image quality. In this work we present a solution based on a\nnovel combination of deep neural networks and Bayesian classifiers. The neural\nnetwork works on a bottom-up approach, analyzing emotions expressed by isolated\nfaces. The Bayesian classifier estimates a global emotion integrating top-down\nfeatures obtained through a scene descriptor. In order to validate the system\nwe tested the framework on the dataset released for the Emotion Recognition in\nthe Wild Challenge 2017. Our method achieved an accuracy of 64.68% on the test\nset, significantly outperforming the 53.62% competition baseline.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 13:09:22 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Surace", "Luca", ""], ["Patacchiola", "Massimiliano", ""], ["S\u00f6nmez", "Elena Battini", ""], ["Spataro", "William", ""], ["Cangelosi", "Angelo", ""]]}, {"id": "1709.03842", "submitter": "Hui Ding", "authors": "Hui Ding, Kumar Sricharan, Rama Chellappa", "title": "ExprGAN: Facial Expression Editing with Controllable Expression\n  Intensity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression editing is a challenging task as it needs a high-level\nsemantic understanding of the input face image. In conventional methods, either\npaired training data is required or the synthetic face resolution is low.\nMoreover, only the categories of facial expression can be changed. To address\nthese limitations, we propose an Expression Generative Adversarial Network\n(ExprGAN) for photo-realistic facial expression editing with controllable\nexpression intensity. An expression controller module is specially designed to\nlearn an expressive and compact expression code in addition to the\nencoder-decoder network. This novel architecture enables the expression\nintensity to be continuously adjusted from low to high. We further show that\nour ExprGAN can be applied for other tasks, such as expression transfer, image\nretrieval, and data augmentation for training improved face expression\nrecognition models. To tackle the small size of the training database, an\neffective incremental learning scheme is proposed. Quantitative and qualitative\nevaluations on the widely used Oulu-CASIA dataset demonstrate the effectiveness\nof ExprGAN.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 13:51:54 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 13:05:23 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Ding", "Hui", ""], ["Sricharan", "Kumar", ""], ["Chellappa", "Rama", ""]]}, {"id": "1709.03851", "submitter": "Hui Ding", "authors": "Hui Ding, Hao Zhou, Shaohua Kevin Zhou, Rama Chellappa", "title": "A Deep Cascade Network for Unaligned Face Attribute Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans focus attention on different face regions when recognizing face\nattributes. Most existing face attribute classification methods use the whole\nimage as input. Moreover, some of these methods rely on fiducial landmarks to\nprovide defined face parts. In this paper, we propose a cascade network that\nsimultaneously learns to localize face regions specific to attributes and\nperforms attribute classification without alignment. First, a weakly-supervised\nface region localization network is designed to automatically detect regions\n(or parts) specific to attributes. Then multiple part-based networks and a\nwhole-image-based network are separately constructed and combined together by\nthe region switch layer and attribute relation layer for final attribute\nclassification. A multi-net learning method and hint-based model compression is\nfurther proposed to get an effective localization model and a compact\nclassification model, respectively. Our approach achieves significantly better\nperformance than state-of-the-art methods on unaligned CelebA dataset, reducing\nthe classification error by 30.9%.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 14:05:14 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 13:01:25 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Ding", "Hui", ""], ["Zhou", "Hao", ""], ["Zhou", "Shaohua Kevin", ""], ["Chellappa", "Rama", ""]]}, {"id": "1709.03872", "submitter": "Xihua Li", "authors": "Xihua Li", "title": "Improving precision and recall of face recognition in SIPP with\n  combination of modified mean search and LSH", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although face recognition has been improved much as the development of Deep\nNeural Networks, SIPP(Single Image Per Person) problem in face recognition has\nnot been better solved, especially in practical applications where searching\nover complicated database. In this paper, a combination of modified mean search\nand LSH method would be introduced orderly to improve the precision and recall\nof SIPP face recognition without retrain of the DNN model. First, a modified\nSVD based augmentation method would be introduced to get more intra-class\nvariations even for person with only one image. Second, an unique rule based\ncombination of modified mean search and LSH method was proposed the first time\nto help get the most similar personID in a complicated dataset, and some\ntheoretical explaining followed. Third, we would like to emphasize, no need to\nretrain of the DNN model and would easy to be extended without much efforts. We\ndo some practical testing in competition of Msceleb challenge-2 2017 which was\nhold by Microsoft Research, great improvement of coverage from 13.39% to\n19.25%, 29.94%, 42.11%, 47.52% at precision 99%(P99) would be shown latter,\ncoverage reach 94.2% and 100% at precision 97%(P97) and 95%(P95) respectively.\nAs far as we known, this is the only paper who do not fine-tuning on\ncompetition dataset and ranked top-10. A similar test on CASIA WebFace dataset\nalso demonstrated the same improvements on both precision and recall.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 11:42:28 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 16:33:02 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Li", "Xihua", ""]]}, {"id": "1709.03917", "submitter": "Jiawang Bian", "authors": "JiaWang Bian, Le Zhang, Yun Liu, Wen-Yan Lin, Ming-Ming Cheng and Ian\n  D. Reid", "title": "Image Matching: An Application-oriented Benchmark", "comments": "We made a significant change and re-submitted as \"MatchBench: An\n  Evaluation of Feature Matchers\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matching approaches have been widely used in computer vision\napplications in which the image-level matching performance of matchers is\ncritical. However, it has not been well investigated by previous works which\nplace more emphases on evaluating local features. To this end, we present a\nuniform benchmark with novel evaluation metrics and a large-scale dataset for\nevaluating the overall performance of image matching methods. The proposed\nmetrics are application-oriented as they emphasize application requirements for\nmatchers. The dataset contains two portions for benchmarking video frame\nmatching and unordered image matching separately, where each portion consists\nof real-world image sequences and each sequence has a specific attribute.\nSubsequently, we carry out a comprehensive performance evaluation of different\nstate-of-the-art methods and conduct in-depth analyses regarding various\naspects such as application requirements, matching types, and data diversity.\nMoreover, we shed light on how to choose appropriate approaches for different\napplications based on empirical results and analyses. Conclusions in this\nbenchmark can be used as general guidelines to design practical matching\nsystems and also advocate potential future research directions in this field.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 15:41:34 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 06:28:10 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 03:04:36 GMT"}, {"version": "v4", "created": "Tue, 7 Aug 2018 08:00:22 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Bian", "JiaWang", ""], ["Zhang", "Le", ""], ["Liu", "Yun", ""], ["Lin", "Wen-Yan", ""], ["Cheng", "Ming-Ming", ""], ["Reid", "Ian D.", ""]]}, {"id": "1709.03919", "submitter": "Boyi Li", "authors": "Boyi Li and Xiulian Peng and Zhangyang Wang and Jizheng Xu and Dan\n  Feng", "title": "End-to-End United Video Dehazing and Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of CNN-based image dehazing has revealed the\neffectiveness of end-to-end modeling. However, extending the idea to end-to-end\nvideo dehazing has not been explored yet. In this paper, we propose an\nEnd-to-End Video Dehazing Network (EVD-Net), to exploit the temporal\nconsistency between consecutive video frames. A thorough study has been\nconducted over a number of structure options, to identify the best temporal\nfusion strategy. Furthermore, we build an End-to-End United Video Dehazing and\nDetection Network(EVDD-Net), which concatenates and jointly trains EVD-Net with\na video object detection model. The resulting augmented end-to-end pipeline has\ndemonstrated much more stable and accurate detection results in hazy video.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 15:43:28 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Li", "Boyi", ""], ["Peng", "Xiulian", ""], ["Wang", "Zhangyang", ""], ["Xu", "Jizheng", ""], ["Feng", "Dan", ""]]}, {"id": "1709.03966", "submitter": "Ty Nguyen", "authors": "Ty Nguyen, Steven W. Chen, Shreyas S. Shivakumar, Camillo J. Taylor,\n  Vijay Kumar", "title": "Unsupervised Deep Homography: A Fast and Robust Homography Estimation\n  Model", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homography estimation between multiple aerial images can provide relative\npose estimation for collaborative autonomous exploration and monitoring. The\nusage on a robotic system requires a fast and robust homography estimation\nalgorithm. In this study, we propose an unsupervised learning algorithm that\ntrains a Deep Convolutional Neural Network to estimate planar homographies. We\ncompare the proposed algorithm to traditional feature-based and direct methods,\nas well as a corresponding supervised learning algorithm. Our empirical results\ndemonstrate that compared to traditional approaches, the unsupervised algorithm\nachieves faster inference speed, while maintaining comparable or better\naccuracy and robustness to illumination variation. In addition, on both a\nsynthetic dataset and representative real-world aerial dataset, our\nunsupervised method has superior adaptability and performance compared to the\nsupervised deep learning method.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 17:30:58 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 00:56:14 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 04:41:07 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Nguyen", "Ty", ""], ["Chen", "Steven W.", ""], ["Shivakumar", "Shreyas S.", ""], ["Taylor", "Camillo J.", ""], ["Kumar", "Vijay", ""]]}, {"id": "1709.03979", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xin Yuan, Bihan Wen, Jiantao Zhou, Jiachao Zhang, Ce Zhu", "title": "A Benchmark for Sparse Coding: When Group Sparsity Meets Rank\n  Minimization", "comments": "arXiv admin note: text overlap with arXiv:1611.08983", "journal-ref": "IEEE Transaction on Image Processing 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse coding has achieved a great success in various image processing tasks.\nHowever, a benchmark to measure the sparsity of image patch/group is missing\nsince sparse coding is essentially an NP-hard problem. This work attempts to\nfill the gap from the perspective of rank minimization. More details please see\nthe manuscript....\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 05:34:19 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 06:47:50 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2019 16:09:37 GMT"}, {"version": "v4", "created": "Mon, 3 Feb 2020 15:05:46 GMT"}, {"version": "v5", "created": "Wed, 5 Feb 2020 01:46:29 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Yuan", "Xin", ""], ["Wen", "Bihan", ""], ["Zhou", "Jiantao", ""], ["Zhang", "Jiachao", ""], ["Zhu", "Ce", ""]]}, {"id": "1709.04056", "submitter": "Paulo Cavalin", "authors": "Paulo R. Cavalin, Marcelo N. Kapp, Luiz S. Oliveira", "title": "Multi-scale Forest Species Recognition Systems for Reduced Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on cost reduction methods for forest species recognition\nsystems. Current state-of-the-art shows that the accuracy of these systems have\nincreased considerably in the past years, but the cost in time to perform the\nrecognition of input samples has also increased proportionally. For this\nreason, in this work we focus on investigating methods for cost reduction\nlocally (at either feature extraction or classification level individually) and\nglobally (at both levels combined), and evaluate two main aspects: 1) the\nimpact in cost reduction, given the proposed measures for it; and 2) the impact\nin recognition accuracy. The experimental evaluation conducted on two forest\nspecies datasets demonstrated that, with global cost reduction, the cost of the\nsystem can be reduced to less than 1/20 and recognition rates that are better\nthan those of the original system can be achieved.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 20:49:11 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Cavalin", "Paulo R.", ""], ["Kapp", "Marcelo N.", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1709.04060", "submitter": "Yaman Umuroglu", "authors": "Yaman Umuroglu and Magnus Jahre", "title": "Streamlined Deployment for Quantized Neural Networks", "comments": "Presented at the International Workshop on Highly Efficient Neural\n  Networks Design (HENND) co-located with CASES'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running Deep Neural Network (DNN) models on devices with limited\ncomputational capability is a challenge due to large compute and memory\nrequirements. Quantized Neural Networks (QNNs) have emerged as a potential\nsolution to this problem, promising to offer most of the DNN accuracy benefits\nwith much lower computational cost. However, harvesting these benefits on\nexisting mobile CPUs is a challenge since operations on highly quantized\ndatatypes are not natively supported in most instruction set architectures\n(ISAs). In this work, we first describe a streamlining flow to convert all QNN\ninference operations to integer ones. Afterwards, we provide techniques based\non processing one bit position at a time (bit-serial) to show how QNNs can be\nefficiently deployed using common bitwise operations. We demonstrate the\npotential of QNNs on mobile CPUs with microbenchmarks and on a quantized\nAlexNet, which is 3.5x faster than an optimized 8-bit baseline. Our bit-serial\nmatrix multiplication library is available on GitHub at https://git.io/vhshn\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 21:14:57 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 14:20:28 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Umuroglu", "Yaman", ""], ["Jahre", "Magnus", ""]]}, {"id": "1709.04093", "submitter": "Seyed Hamid Rezatofighi", "authors": "S. Hamid Rezatofighi, Anton Milan, Qinfeng Shi, Anthony Dick, Ian Reid", "title": "Joint Learning of Set Cardinality and State Distribution", "comments": "Accepted in AAAI 2018. arXiv admin note: text overlap with\n  arXiv:1611.08998", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for learning to predict sets using deep learning.\nIn recent years, deep neural networks have shown remarkable results in computer\nvision, natural language processing and other related problems. Despite their\nsuccess, traditional architectures suffer from a serious limitation in that\nthey are built to deal with structured input and output data, i.e. vectors or\nmatrices. Many real-world problems, however, are naturally described as sets,\nrather than vectors. Existing techniques that allow for sequential data, such\nas recurrent neural networks, typically heavily depend on the input and output\norder and do not guarantee a valid solution. Here, we derive in a principled\nway, a mathematical formulation for set prediction where the output is\npermutation invariant. In particular, our approach jointly learns both the\ncardinality and the state distribution of the target set. We demonstrate the\nvalidity of our method on the task of multi-label image classification and\nachieve a new state of the art on the PASCAL VOC and MS COCO datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 00:33:50 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:29:33 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Rezatofighi", "S. Hamid", ""], ["Milan", "Anton", ""], ["Shi", "Qinfeng", ""], ["Dick", "Anthony", ""], ["Reid", "Ian", ""]]}, {"id": "1709.04108", "submitter": "Ehsan Mohammady Ardehaly", "authors": "Ehsan Mohammady Ardehaly, Aron Culotta", "title": "Co-training for Demographic Classification Using Deep Learning from\n  Label Proportions", "comments": null, "journal-ref": null, "doi": "10.1109/ICDMW.2017.144", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have recently produced state-of-the-art accuracy in\nmany classification tasks, but this success is typically dependent on access to\nmany annotated training examples. For domains without such data, an attractive\nalternative is to train models with light, or distant supervision. In this\npaper, we introduce a deep neural network for the Learning from Label\nProportion (LLP) setting, in which the training data consist of bags of\nunlabeled instances with associated label distributions for each bag. We\nintroduce a new regularization layer, Batch Averager, that can be appended to\nthe last layer of any deep neural network to convert it from supervised\nlearning to LLP. This layer can be implemented readily with existing deep\nlearning packages. To further support domains in which the data consist of two\nconditionally independent feature views (e.g. image and text), we propose a\nco-training algorithm that iteratively generates pseudo bags and refits the\ndeep LLP model to improve classification accuracy. We demonstrate our models on\ndemographic attribute classification (gender and race/ethnicity), which has\nmany applications in social media analysis, public health, and marketing. We\nconduct experiments to predict demographics of Twitter users based on their\ntweets and profile image, without requiring any user-level annotations for\ntraining. We find that the deep LLP approach outperforms baselines for both\ntext and image features separately. Additionally, we find that co-training\nalgorithm improves image and text classification by 4% and 8% absolute F1,\nrespectively. Finally, an ensemble of text and image classifiers further\nimproves the absolute F1 measure by 4% on average.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 02:06:19 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Ardehaly", "Ehsan Mohammady", ""], ["Culotta", "Aron", ""]]}, {"id": "1709.04111", "submitter": "Falong Shen", "authors": "Falong Shen, Shuicheng Yan and Gang Zeng", "title": "Meta Networks for Neural Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new method to get the specified network parameters\nthrough one time feed-forward propagation of the meta networks and explore the\napplication to neural style transfer. Recent works on style transfer typically\nneed to train image transformation networks for every new style, and the style\nis encoded in the network parameters by enormous iterations of stochastic\ngradient descent. To tackle these issues, we build a meta network which takes\nin the style image and produces a corresponding image transformations network\ndirectly. Compared with optimization-based methods for every style, our meta\nnetworks can handle an arbitrary new style within $19ms$ seconds on one modern\nGPU card. The fast image transformation network generated by our meta network\nis only 449KB, which is capable of real-time executing on a mobile device. We\nalso investigate the manifold of the style transfer networks by operating the\nhidden features from meta networks. Experiments have well validated the\neffectiveness of our method. Code and trained models has been released\nhttps://github.com/FalongShen/styletransfer.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 02:18:39 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Shen", "Falong", ""], ["Yan", "Shuicheng", ""], ["Zeng", "Gang", ""]]}, {"id": "1709.04121", "submitter": "Yajing Chen", "authors": "Yajing Chen, Shikui Tu, Yuqi Yi and Lei Xu", "title": "Sketch-pix2seq: a Model to Generate Sketches of Multiple Categories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch is an important media for human to communicate ideas, which reflects\nthe superiority of human intelligence. Studies on sketch can be roughly\nsummarized into recognition and generation. Existing models on image\nrecognition failed to obtain satisfying performance on sketch classification.\nBut for sketch generation, a recent study proposed a sequence-to-sequence\nvariational-auto-encoder (VAE) model called sketch-rnn which was able to\ngenerate sketches based on human inputs. The model achieved amazing results\nwhen asked to learn one category of object, such as an animal or a vehicle.\nHowever, the performance dropped when multiple categories were fed into the\nmodel. Here, we proposed a model called sketch-pix2seq which could learn and\ndraw multiple categories of sketches. Two modifications were made to improve\nthe sketch-rnn model: one is to replace the bidirectional recurrent neural\nnetwork (BRNN) encoder with a convolutional neural network(CNN); the other is\nto remove the Kullback-Leibler divergence from the objective function of VAE.\nExperimental results showed that models with CNN encoders outperformed those\nwith RNN encoders in generating human-style sketches. Visualization of the\nlatent space illustrated that the removal of KL-divergence made the encoder\nlearn a posterior of latent space that reflected the features of different\ncategories. Moreover, the combination of CNN encoder and removal of\nKL-divergence, i.e., the sketch-pix2seq model, had better performance in\nlearning and generating sketches of multiple categories and showed promising\nresults in creativity tasks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 03:22:27 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Chen", "Yajing", ""], ["Tu", "Shikui", ""], ["Yi", "Yuqi", ""], ["Xu", "Lei", ""]]}, {"id": "1709.04295", "submitter": "Huaxiong Ding", "authors": "Huaxiong Ding, Liming Chen", "title": "Densely tracking sequences of 3D face scans", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D face dense tracking aims to find dense inter-frame correspondences in a\nsequence of 3D face scans and constitutes a powerful tool for many face\nanalysis tasks, e.g., 3D dynamic facial expression analysis. The majority of\nthe existing methods just fit a 3D face surface or model to a 3D target surface\nwithout considering temporal information between frames. In this paper, we\npropose a novel method for densely tracking sequences of 3D face scans, which\nex- tends the non-rigid ICP algorithm by adding a novel specific criterion for\ntemporal information. A novel fitting framework is presented for automatically\ntracking a full sequence of 3D face scans. The results of experiments carried\nout on the BU4D-FE database are promising, showing that the proposed algorithm\noutperforms state-of-the-art algorithms for 3D face dense tracking.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 12:49:30 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Ding", "Huaxiong", ""], ["Chen", "Liming", ""]]}, {"id": "1709.04299", "submitter": "Jens Grubert", "authors": "Jens Grubert, Yuta Itoh, Kenneth Moser and J. Edward Swan II", "title": "A Survey of Calibration Methods for Optical See-Through Head-Mounted\n  Displays", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2017.2754257", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical see-through head-mounted displays (OST HMDs) are a major output\nmedium for Augmented Reality, which have seen significant growth in popularity\nand usage among the general public due to the growing release of\nconsumer-oriented models, such as the Microsoft Hololens. Unlike Virtual\nReality headsets, OST HMDs inherently support the addition of\ncomputer-generated graphics directly into the light path between a user's eyes\nand their view of the physical world. As with most Augmented and Virtual\nReality systems, the physical position of an OST HMD is typically determined by\nan external or embedded 6-Degree-of-Freedom tracking system. However, in order\nto properly render virtual objects, which are perceived as spatially aligned\nwith the physical environment, it is also necessary to accurately measure the\nposition of the user's eyes within the tracking system's coordinate frame. For\nover 20 years, researchers have proposed various calibration methods to\ndetermine this needed eye position. However, to date, there has not been a\ncomprehensive overview of these procedures and their requirements. Hence, this\npaper surveys the field of calibration methods for OST HMDs. Specifically, it\nprovides insights into the fundamentals of calibration techniques, and presents\nan overview of both manual and automatic approaches, as well as evaluation\nmethods and metrics. Finally, it also identifies opportunities for future\nresearch. % relative to the tracking coordinate system, and, hence, its\nposition in 3D space.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 12:55:45 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Grubert", "Jens", ""], ["Itoh", "Yuta", ""], ["Moser", "Kenneth", ""], ["Swan", "J. Edward", "II"]]}, {"id": "1709.04303", "submitter": "Yunze Gao", "authors": "Yunze Gao (1 and 2), Yingying Chen (1 and 2), Jinqiao Wang (1 and 2),\n  Hanqing Lu (1 and 2) ((1) National Lab of Pattern Recognition, Institute of\n  Automation, Chinese Academy of Sciences, (2) University of Chinese Academy of\n  Sciences)", "title": "Reading Scene Text with Attention Convolutional Sequence Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reading text in the wild is a challenging task in the field of computer\nvision. Existing approaches mainly adopted Connectionist Temporal\nClassification (CTC) or Attention models based on Recurrent Neural Network\n(RNN), which is computationally expensive and hard to train. In this paper, we\npresent an end-to-end Attention Convolutional Network for scene text\nrecognition. Firstly, instead of RNN, we adopt the stacked convolutional layers\nto effectively capture the contextual dependencies of the input sequence, which\nis characterized by lower computational complexity and easier parallel\ncomputation. Compared to the chain structure of recurrent networks, the\nConvolutional Neural Network (CNN) provides a natural way to capture long-term\ndependencies between elements, which is 9 times faster than Bidirectional Long\nShort-Term Memory (BLSTM). Furthermore, in order to enhance the representation\nof foreground text and suppress the background noise, we incorporate the\nresidual attention modules into a small densely connected network to improve\nthe discriminability of CNN features. We validate the performance of our\napproach on the standard benchmarks, including the Street View Text, IIIT5K and\nICDAR datasets. As a result, state-of-the-art or highly-competitive performance\nand efficiency show the superiority of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 12:57:47 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Gao", "Yunze", "", "1 and 2"], ["Chen", "Yingying", "", "1 and 2"], ["Wang", "Jinqiao", "", "1 and 2"], ["Lu", "Hanqing", "", "1 and 2"]]}, {"id": "1709.04329", "submitter": "Longhui Wei", "authors": "Longhui Wei, Shiliang Zhang, Hantao Yao, Wen Gao, Qi Tian", "title": "GLAD: Global-Local-Alignment Descriptor for Pedestrian Retrieval", "comments": "Accepted by ACM MM2017, 9 pages, 5 figures", "journal-ref": null, "doi": "10.1145/3123266.3123279", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The huge variance of human pose and the misalignment of detected human images\nsignificantly increase the difficulty of person Re-Identification (Re-ID).\nMoreover, efficient Re-ID systems are required to cope with the massive visual\ndata being produced by video surveillance systems. Targeting to solve these\nproblems, this work proposes a Global-Local-Alignment Descriptor (GLAD) and an\nefficient indexing and retrieval framework, respectively. GLAD explicitly\nleverages the local and global cues in human body to generate a discriminative\nand robust representation. It consists of part extraction and descriptor\nlearning modules, where several part regions are first detected and then deep\nneural networks are designed for representation learning on both the local and\nglobal regions. A hierarchical indexing and retrieval framework is designed to\neliminate the huge redundancy in the gallery set, and accelerate the online\nRe-ID procedure. Extensive experimental results show GLAD achieves competitive\naccuracy compared to the state-of-the-art methods. Our retrieval framework\nsignificantly accelerates the online Re-ID procedure without loss of accuracy.\nTherefore, this work has potential to work better on person Re-ID tasks in real\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 13:44:46 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Wei", "Longhui", ""], ["Zhang", "Shiliang", ""], ["Yao", "Hantao", ""], ["Gao", "Wen", ""], ["Tian", "Qi", ""]]}, {"id": "1709.04343", "submitter": "Stavros Petridis", "authors": "Stavros Petridis, Yujiang Wang, Zuwei Li, Maja Pantic", "title": "End-to-End Audiovisual Fusion with LSTMs", "comments": "Accepted to AVSP 2017. arXiv admin note: substantial text overlap\n  with arXiv:1709.00443 and text overlap with arXiv:1701.05847", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several end-to-end deep learning approaches have been recently presented\nwhich simultaneously extract visual features from the input images and perform\nvisual speech classification. However, research on jointly extracting audio and\nvisual features and performing classification is very limited. In this work, we\npresent an end-to-end audiovisual model based on Bidirectional Long Short-Term\nMemory (BLSTM) networks. To the best of our knowledge, this is the first\naudiovisual fusion model which simultaneously learns to extract features\ndirectly from the pixels and spectrograms and perform classification of speech\nand nonlinguistic vocalisations. The model consists of multiple identical\nstreams, one for each modality, which extract features directly from mouth\nregions and spectrograms. The temporal dynamics in each stream/modality are\nmodeled by a BLSTM and the fusion of multiple streams/modalities takes place\nvia another BLSTM. An absolute improvement of 1.9% in the mean F1 of 4\nnonlingusitic vocalisations over audio-only classification is reported on the\nAVIC database. At the same time, the proposed end-to-end audiovisual fusion\nsystem improves the state-of-the-art performance on the AVIC database leading\nto a 9.7% absolute increase in the mean F1 measure. We also perform audiovisual\nspeech recognition experiments on the OuluVS2 database using different views of\nthe mouth, frontal to profile. The proposed audiovisual system significantly\noutperforms the audio-only model for all views when the acoustic noise is high.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 15:23:55 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Petridis", "Stavros", ""], ["Wang", "Yujiang", ""], ["Li", "Zuwei", ""], ["Pantic", "Maja", ""]]}, {"id": "1709.04344", "submitter": "Lixue Zhuang", "authors": "Lixue Zhuang, Yi Xu, Bingbing Ni, Hongteng Xu", "title": "Flexible Network Binarization with Layer-wise Priority", "comments": "More experiments on image classification are planned", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to effectively approximate real-valued parameters with binary codes plays\na central role in neural network binarization. In this work, we reveal an\nimportant fact that binarizing different layers has a widely-varied effect on\nthe compression ratio of network and the loss of performance. Based on this\nfact, we propose a novel and flexible neural network binarization method by\nintroducing the concept of layer-wise priority which binarizes parameters in\ninverse order of their layer depth. In each training step, our method selects a\nspecific network layer, minimizes the discrepancy between the original\nreal-valued weights and its binary approximations, and fine-tunes the whole\nnetwork accordingly. During the iteration of the above process, it is\nsignificant that we can flexibly decide whether to binarize the remaining\nfloating layers or not and explore a trade-off between the loss of performance\nand the compression ratio of model. The resulting binary network is applied for\nefficient pedestrian detection. Extensive experimental results on several\nbenchmarks show that under the same compression ratio, our method achieves much\nlower miss rate and faster detection speed than the state-of-the-art neural\nnetwork binarization method.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 14:14:15 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 17:55:32 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 06:14:44 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Zhuang", "Lixue", ""], ["Xu", "Yi", ""], ["Ni", "Bingbing", ""], ["Xu", "Hongteng", ""]]}, {"id": "1709.04347", "submitter": "Hongyang Li", "authors": "Hongyang Li, Yu Liu, Wanli Ouyang and Xiaogang Wang", "title": "Zoom Out-and-In Network with Map Attention Decision for Region Proposal\n  and Object Detection", "comments": "Accepted by IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a zoom-out-and-in network for generating object\nproposals. A key observation is that it is difficult to classify anchors of\ndifferent sizes with the same set of features. Anchors of different sizes\nshould be placed accordingly based on different depth within a network: smaller\nboxes on high-resolution layers with a smaller stride while larger boxes on\nlow-resolution counterparts with a larger stride. Inspired by the conv/deconv\nstructure, we fully leverage the low-level local details and high-level\nregional semantics from two feature map streams, which are complimentary to\neach other, to identify the objectness in an image. A map attention decision\n(MAD) unit is further proposed to aggressively search for neuron activations\namong two streams and attend the most contributive ones on the feature learning\nof the final loss. The unit serves as a decisionmaker to adaptively activate\nmaps along certain channels with the solely purpose of optimizing the overall\ntraining loss. One advantage of MAD is that the learned weights enforced on\neach feature channel is predicted on-the-fly based on the input context, which\nis more suitable than the fixed enforcement of a convolutional kernel.\nExperimental results on three datasets, including PASCAL VOC 2007, ImageNet\nDET, MS COCO, demonstrate the effectiveness of our proposed algorithm over\nother state-of-the-arts, in terms of average recall (AR) for region proposal\nand average precision (AP) for object detection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 14:18:45 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 09:59:28 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Li", "Hongyang", ""], ["Liu", "Yu", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1709.04393", "submitter": "Kazem Qazanfari", "authors": "Roohollah Aslanzadeh, Kazem Qazanfari, Mohammad Rahmati", "title": "An Efficient Evolutionary Based Method For Image Segmentation", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to present a new efficient image segmentation\nmethod based on evolutionary computation which is a model inspired from human\nbehavior. Based on this model, a four layer process for image segmentation is\nproposed using the split/merge approach. In the first layer, an image is split\ninto numerous regions using the watershed algorithm. In the second layer, a\nco-evolutionary process is applied to form centers of finals segments by\nmerging similar primary regions. In the third layer, a meta-heuristic process\nuses two operators to connect the residual regions to their corresponding\ndetermined centers. In the final layer, an evolutionary algorithm is used to\ncombine the resulted similar and neighbor regions. Different layers of the\nalgorithm are totally independent, therefore for certain applications a\nspecific layer can be changed without constraint of changing other layers. Some\nproperties of this algorithm like the flexibility of its method, the ability to\nuse different feature vectors for segmentation (grayscale, color, texture,\netc), the ability to control uniformity and the number of final segments using\nfree parameters and also maintaining small regions, makes it possible to apply\nthe algorithm to different applications. Moreover, the independence of each\nregion from other regions in the second layer, and the independence of centers\nin the third layer, makes parallel implementation possible. As a result the\nalgorithm speed will increase. The presented algorithm was tested on a standard\ndataset (BSDS 300) of images, and the region boundaries were compared with\ndifferent people segmentation contours. Results show the efficiency of the\nalgorithm and its improvement to similar methods. As an instance, in 70% of\ntested images, results are better than ACT algorithm, besides in 100% of tested\nimages, we had better results in comparison with VSP algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 16:00:01 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 18:06:26 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Aslanzadeh", "Roohollah", ""], ["Qazanfari", "Kazem", ""], ["Rahmati", "Mohammad", ""]]}, {"id": "1709.04396", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, Gy\\\"orgy Fazekas, Kyunghyun Cho and Mark Sandler", "title": "A Tutorial on Deep Learning for Music Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following their success in Computer Vision and other areas, deep learning\ntechniques have recently become widely adopted in Music Information Retrieval\n(MIR) research. However, the majority of works aim to adopt and assess methods\nthat have been shown to be effective in other domains, while there is still a\ngreat need for more original research focusing on music primarily and utilising\nmusical knowledge and insight. The goal of this paper is to boost the interest\nof beginners by providing a comprehensive tutorial and reducing the barriers to\nentry into deep learning for MIR. We lay out the basic principles and review\nprominent works in this hard to navigate the field. We then outline the network\nstructures that have been successful in MIR problems and facilitate the\nselection of building blocks for the problems at hand. Finally, guidelines for\nnew tasks and some advanced topics in deep learning are discussed to stimulate\nnew research in this fascinating field.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 16:05:51 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 06:29:55 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "Gy\u00f6rgy", ""], ["Cho", "Kyunghyun", ""], ["Sandler", "Mark", ""]]}, {"id": "1709.04411", "submitter": "Julian Yarkony", "authors": "Shaofei Wang, Konrad Kording, Julian Yarkony", "title": "Exploiting skeletal structure in computer vision annotation with Benders\n  decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many annotation problems in computer vision can be phrased as integer linear\nprograms (ILPs). The use of standard industrial solvers does not to exploit the\nunderlying structure of such problems eg, the skeleton in pose estimation. The\nleveraging of the underlying structure in conjunction with industrial solvers\npromises increases in both speed and accuracy. Such structure can be exploited\nusing Bender's decomposition, a technique from operations research, that solves\ncomplex ILPs or mixed integer linear programs by decomposing them into\nsub-problems that communicate via a master problem. The intuition is that\nconditioned on a small subset of the variables the solution to the remaining\nvariables can be computed easily by taking advantage of properties of the ILP\nconstraint matrix such as block structure. In this paper we apply Benders\ndecomposition to a typical problem in computer vision where we have many\nsub-ILPs (eg, partitioning of detections, body-parts) coupled to a master ILP\n(eg, constructing skeletons). Dividing inference problems into a master problem\nand sub-problems motivates the development of a plethora of novel models, and\ninference approaches for the field of computer vision.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 16:43:07 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Wang", "Shaofei", ""], ["Kording", "Konrad", ""], ["Yarkony", "Julian", ""]]}, {"id": "1709.04427", "submitter": "Gang Cao Dr.", "authors": "Gang Cao, Lihui Huang, Huawei Tian, Xianglin Huang, Yongbin Wang,\n  Ruicong Zhi", "title": "Contrast Enhancement of Brightness-Distorted Images by Improved Adaptive\n  Gamma Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an efficient image contrast enhancement (CE) tool, adaptive gamma\ncorrection (AGC) was previously proposed by relating gamma parameter with\ncumulative distribution function (CDF) of the pixel gray levels within an\nimage. ACG deals well with most dimmed images, but fails for globally bright\nimages and the dimmed images with local bright regions. Such two categories of\nbrightness-distorted images are universal in real scenarios, such as improper\nexposure and white object regions. In order to attenuate such deficiencies,\nhere we propose an improved AGC algorithm. The novel strategy of negative\nimages is used to realize CE of the bright images, and the gamma correction\nmodulated by truncated CDF is employed to enhance the dimmed ones. As such,\nlocal over-enhancement and structure distortion can be alleviated. Both\nqualitative and quantitative experimental results show that our proposed method\nyields consistently good CE results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 17:17:01 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Cao", "Gang", ""], ["Huang", "Lihui", ""], ["Tian", "Huawei", ""], ["Huang", "Xianglin", ""], ["Wang", "Yongbin", ""], ["Zhi", "Ruicong", ""]]}, {"id": "1709.04447", "submitter": "Arunesh Sinha", "authors": "Linh Nguyen, Sky Wang, Arunesh Sinha", "title": "A Learning and Masking Approach to Secure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been shown to be vulnerable against\nadversarial examples, which are data points cleverly constructed to fool the\nclassifier. Such attacks can be devastating in practice, especially as DNNs are\nbeing applied to ever increasing critical tasks like image recognition in\nautonomous driving. In this paper, we introduce a new perspective on the\nproblem. We do so by first defining robustness of a classifier to adversarial\nexploitation. Next, we show that the problem of adversarial example generation\ncan be posed as learning problem. We also categorize attacks in literature into\nhigh and low perturbation attacks; well-known attacks like fast-gradient sign\nmethod (FGSM) and our attack produce higher perturbation adversarial examples\nwhile the more potent but computationally inefficient Carlini-Wagner (CW)\nattack is low perturbation. Next, we show that the dual approach of the attack\nlearning problem can be used as a defensive technique that is effective against\nhigh perturbation attacks. Finally, we show that a classifier masking method\nachieved by adding noise to the a neural network's logit output protects\nagainst low distortion attacks such as the CW attack. We also show that both\nour learning and masking defense can work simultaneously to protect against\nmultiple attacks. We demonstrate the efficacy of our techniques by\nexperimenting with the MNIST and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 17:44:35 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 23:08:36 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 04:52:45 GMT"}, {"version": "v4", "created": "Sat, 27 Jan 2018 04:08:56 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Nguyen", "Linh", ""], ["Wang", "Sky", ""], ["Sinha", "Arunesh", ""]]}, {"id": "1709.04496", "submitter": "Lisa Koch", "authors": "Christian F. Baumgartner, Lisa M. Koch, Marc Pollefeys, Ender\n  Konukoglu", "title": "An Exploration of 2D and 3D Deep Learning Techniques for Cardiac MR\n  Image Segmentation", "comments": "to appear in STACOM 2017 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of the heart is an important step towards evaluating\ncardiac function. In this paper, we present a fully automated framework for\nsegmentation of the left (LV) and right (RV) ventricular cavities and the\nmyocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and\n3D convolutional neural network architectures for this task. We investigate the\nsuitability of various state-of-the art 2D and 3D convolutional neural network\narchitectures, as well as slight modifications thereof, for this task.\nExperiments were performed on the ACDC 2017 challenge training dataset\ncomprising cardiac MR images of 100 patients, where manual reference\nsegmentations were made available for end-diastolic (ED) and end-systolic (ES)\nframes. We find that processing the images in a slice-by-slice fashion using 2D\nnetworks is beneficial due to a relatively large slice thickness. However, the\nexact network architecture only plays a minor role. We report mean Dice\ncoefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectively\nwith an average evaluation time of 1.1 seconds per volume on a modern GPU.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 18:36:48 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 08:09:31 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Baumgartner", "Christian F.", ""], ["Koch", "Lisa M.", ""], ["Pollefeys", "Marc", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1709.04518", "submitter": "Lingxi Xie", "authors": "Qihang Yu, Lingxi Xie, Yan Wang, Yuyin Zhou, Elliot K. Fishman, Alan\n  L. Yuille", "title": "Recurrent Saliency Transformation Network: Incorporating Multi-Stage\n  Visual Cues for Small Organ Segmentation", "comments": "Accepted to CVPR 2018 (10 pages, 6 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim at segmenting small organs (e.g., the pancreas) from abdominal CT\nscans. As the target often occupies a relatively small region in the input\nimage, deep neural networks can be easily confused by the complex and variable\nbackground. To alleviate this, researchers proposed a coarse-to-fine approach,\nwhich used prediction from the first (coarse) stage to indicate a smaller input\nregion for the second (fine) stage. Despite its effectiveness, this algorithm\ndealt with two stages individually, which lacked optimizing a global energy\nfunction, and limited its ability to incorporate multi-stage visual cues.\nMissing contextual information led to unsatisfying convergence in iterations,\nand that the fine stage sometimes produced even lower segmentation accuracy\nthan the coarse stage.\n  This paper presents a Recurrent Saliency Transformation Network. The key\ninnovation is a saliency transformation module, which repeatedly converts the\nsegmentation probability map from the previous iteration as spatial weights and\napplies these weights to the current iteration. This brings us two-fold\nbenefits. In training, it allows joint optimization over the deep networks\ndealing with different input scales. In testing, it propagates multi-stage\nvisual information throughout iterations to improve segmentation accuracy.\nExperiments in the NIH pancreas segmentation dataset demonstrate the\nstate-of-the-art accuracy, which outperforms the previous best by an average of\nover 2%. Much higher accuracies are also reported on several small organs in a\nlarger dataset collected by ourselves. In addition, our approach enjoys better\nconvergence properties, making it more efficient and reliable in practice.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 19:54:56 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 13:25:58 GMT"}, {"version": "v3", "created": "Sat, 18 Nov 2017 21:12:22 GMT"}, {"version": "v4", "created": "Sun, 8 Apr 2018 01:14:34 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Yu", "Qihang", ""], ["Xie", "Lingxi", ""], ["Wang", "Yan", ""], ["Zhou", "Yuyin", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1709.04550", "submitter": "Jinhui Yu", "authors": "Jinhui Yu, Kailin Wu, Kang Zhang, Xianjun Sam Zheng", "title": "A Computational Model of Afterimages based on Simultaneous and\n  Successive Contrasts", "comments": "10 pages, 6 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Negative afterimage appears in our vision when we shift our gaze from an over\nstimulated original image to a new area with a uniform color. The colors of\nnegative afterimages differ from the old stimulating colors in the original\nimage when the color in the new area is either neutral or chromatic. The\ninteraction between stimulating colors in the test and inducing field in the\noriginal image changes our color perception due to simultaneous contrast, and\nthe interaction between changed colors perceived in the previously-viewed field\nand the color in the currently-viewed field also affects our perception of\ncolors in negative afterimages due to successive contrast. Based on these\nobservations we propose a computational model to estimate colors of negative\nafterimages in more general cases where the original stimulating color in the\ntest field is chromatic, and the original stimulating color in the inducing\nfield and the new stimulating color can be either neutral or chromatic. We\nvalidate our model with human experiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 21:54:58 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Yu", "Jinhui", ""], ["Wu", "Kailin", ""], ["Zhang", "Kang", ""], ["Zheng", "Xianjun Sam", ""]]}, {"id": "1709.04577", "submitter": "Zhishuai Zhang", "authors": "Zhishuai Zhang, Cihang Xie, Jianyu Wang, Lingxi Xie, Alan L. Yuille", "title": "DeepVoting: A Robust and Explainable Deep Network for Semantic Part\n  Detection under Partial Occlusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the task of detecting semantic parts of an object,\ne.g., a wheel of a car, under partial occlusion. We propose that all models\nshould be trained without seeing occlusions while being able to transfer the\nlearned knowledge to deal with occlusions. This setting alleviates the\ndifficulty in collecting an exponentially large dataset to cover occlusion\npatterns and is more essential. In this scenario, the proposal-based deep\nnetworks, like RCNN-series, often produce unsatisfactory results, because both\nthe proposal extraction and classification stages may be confused by the\nirrelevant occluders. To address this, [25] proposed a voting mechanism that\ncombines multiple local visual cues to detect semantic parts. The semantic\nparts can still be detected even though some visual cues are missing due to\nocclusions. However, this method is manually-designed, thus is hard to be\noptimized in an end-to-end manner.\n  In this paper, we present DeepVoting, which incorporates the robustness shown\nby [25] into a deep network, so that the whole pipeline can be jointly\noptimized. Specifically, it adds two layers after the intermediate features of\na deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts the\nevidence of local visual cues, and the second layer performs a voting mechanism\nby utilizing the spatial relationship between visual cues and semantic parts.\nWe also propose an improved version DeepVoting+ by learning visual cues from\ncontext outside objects. In experiments, DeepVoting achieves significantly\nbetter performance than several baseline methods, including Faster-RCNN, for\nsemantic part detection under occlusion. In addition, DeepVoting enjoys\nexplainability as the detection results can be diagnosed via looking up the\nvoting cues.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 01:37:43 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:56:07 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Zhang", "Zhishuai", ""], ["Xie", "Cihang", ""], ["Wang", "Jianyu", ""], ["Xie", "Lingxi", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1709.04583", "submitter": "Gang Cao", "authors": "Gang Cao, Huawei Tian, Lifang Yu, Xianglin Huang, Yongbin Wang", "title": "Acceleration of Histogram-Based Contrast Enhancement via Selective\n  Downsampling", "comments": "accepted by IET Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework to accelerate the universal\nhistogram-based image contrast enhancement (CE) algorithms. Both spatial and\ngray-level selective down- sampling of digital images are adopted to decrease\ncomputational cost, while the visual quality of enhanced images is still\npreserved and without apparent degradation. Mapping function calibration is\nnovelly proposed to reconstruct the pixel mapping on the gray levels missed by\ndownsampling. As two case studies, accelerations of histogram equalization (HE)\nand the state-of-the-art global CE algorithm, i.e., spatial mutual information\nand PageRank (SMIRANK), are presented detailedly. Both quantitative and\nqualitative assessment results have verified the effectiveness of our proposed\nCE acceleration framework. In typical tests, computational efficiencies of HE\nand SMIRANK have been speeded up by about 3.9 and 13.5 times, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 01:50:27 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 08:45:10 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Cao", "Gang", ""], ["Tian", "Huawei", ""], ["Yu", "Lifang", ""], ["Huang", "Xianglin", ""], ["Wang", "Yongbin", ""]]}, {"id": "1709.04595", "submitter": "Debang Li", "authors": "Debang Li, Huikai Wu, Junge Zhang, Kaiqi Huang", "title": "A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image cropping aims at improving the aesthetic quality of images by adjusting\ntheir composition. Most weakly supervised cropping methods (without bounding\nbox supervision) rely on the sliding window mechanism. The sliding window\nmechanism requires fixed aspect ratios and limits the cropping region with\narbitrary size. Moreover, the sliding window method usually produces tens of\nthousands of windows on the input image which is very time-consuming. Motivated\nby these challenges, we firstly formulate the aesthetic image cropping as a\nsequential decision-making process and propose a weakly supervised Aesthetics\nAware Reinforcement Learning (A2-RL) framework to address this problem.\nParticularly, the proposed method develops an aesthetics aware reward function\nwhich especially benefits image cropping. Similar to human's decision making,\nwe use a comprehensive state representation including both the current\nobservation and the historical experience. We train the agent using the\nactor-critic architecture in an end-to-end manner. The agent is evaluated on\nseveral popular unseen cropping datasets. Experiment results show that our\nmethod achieves the state-of-the-art performance with much fewer candidate\nwindows and much less time compared with previous weakly supervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 02:35:37 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 03:49:31 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 15:49:14 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Li", "Debang", ""], ["Wu", "Huikai", ""], ["Zhang", "Junge", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1709.04609", "submitter": "Yi-Hsuan Tsai", "authors": "Jingchun Cheng, Sifei Liu, Yi-Hsuan Tsai, Wei-Chih Hung, Shalini De\n  Mello, Jinwei Gu, Jan Kautz, Shengjin Wang, Ming-Hsuan Yang", "title": "Learning to Segment Instances in Videos with Spatial Propagation Network", "comments": "CVPR 2017 Workshop on DAVIS Challenge. Code is available at\n  http://github.com/JingchunCheng/Seg-with-SPN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning-based framework for instance-level object\nsegmentation. Our method mainly consists of three steps. First, We train a\ngeneric model based on ResNet-101 for foreground/background segmentations.\nSecond, based on this generic model, we fine-tune it to learn instance-level\nmodels and segment individual objects by using augmented object annotations in\nfirst frames of test videos. To distinguish different instances in the same\nvideo, we compute a pixel-level score map for each object from these\ninstance-level models. Each score map indicates the objectness likelihood and\nis only computed within the foreground mask obtained in the first step. To\nfurther refine this per frame score map, we learn a spatial propagation\nnetwork. This network aims to learn how to propagate a coarse segmentation mask\nspatially based on the pairwise similarities in each frame. In addition, we\napply a filter on the refined score map that aims to recognize the best\nconnected region using spatial and temporal consistencies in the video.\nFinally, we decide the instance-level object segmentation in each video by\ncomparing score maps of different instances.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 04:15:49 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Cheng", "Jingchun", ""], ["Liu", "Sifei", ""], ["Tsai", "Yi-Hsuan", ""], ["Hung", "Wei-Chih", ""], ["De Mello", "Shalini", ""], ["Gu", "Jinwei", ""], ["Kautz", "Jan", ""], ["Wang", "Shengjin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1709.04625", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Cuong Duc Dao, Modar Alfadly, C. Huck Yang, Bernard\n  Ghanem", "title": "Robustness Analysis of Visual QA Models by Basic Questions", "comments": "Accepted by CVPR 2018 VQA Challenge and Visual Dialog Workshop.\n  (Acknowledgement updating)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) models should have both high robustness and\naccuracy. Unfortunately, most of the current VQA research only focuses on\naccuracy because there is a lack of proper methods to measure the robustness of\nVQA models. There are two main modules in our algorithm. Given a natural\nlanguage question about an image, the first module takes the question as input\nand then outputs the ranked basic questions, with similarity scores, of the\nmain given question. The second module takes the main question, image and these\nbasic questions as input and then outputs the text-based answer of the main\nquestion about the given image. We claim that a robust VQA model is one, whose\nperformance is not changed much when related basic questions as also made\navailable to it as input. We formulate the basic questions generation problem\nas a LASSO optimization, and also propose a large scale Basic Question Dataset\n(BQD) and Rscore (novel robustness measure), for analyzing the robustness of\nVQA models. We hope our BQD will be used as a benchmark for to evaluate the\nrobustness of VQA models, so as to help the community build more robust and\naccurate VQA models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 06:11:09 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 06:56:47 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 05:14:02 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Dao", "Cuong Duc", ""], ["Alfadly", "Modar", ""], ["Yang", "C. Huck", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1709.04647", "submitter": "Yair Meidan", "authors": "Yair Meidan, Michael Bohadana, Asaf Shabtai, Martin Ochoa, Nils Ole\n  Tippenhauer, Juan Davis Guarnizo, Yuval Elovici", "title": "Detection of Unauthorized IoT Devices Using Machine Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security experts have demonstrated numerous risks imposed by Internet of\nThings (IoT) devices on organizations. Due to the widespread adoption of such\ndevices, their diversity, standardization obstacles, and inherent mobility,\norganizations require an intelligent mechanism capable of automatically\ndetecting suspicious IoT devices connected to their networks. In particular,\ndevices not included in a white list of trustworthy IoT device types (allowed\nto be used within the organizational premises) should be detected. In this\nresearch, Random Forest, a supervised machine learning algorithm, was applied\nto features extracted from network traffic data with the aim of accurately\nidentifying IoT device types from the white list. To train and evaluate\nmulti-class classifiers, we collected and manually labeled network traffic data\nfrom 17 distinct IoT devices, representing nine types of IoT devices. Based on\nthe classification of 20 consecutive sessions and the use of majority rule, IoT\ndevice types that are not on the white list were correctly detected as unknown\nin 96% of test cases (on average), and white listed device types were correctly\nclassified by their actual types in 99% of cases. Some IoT device types were\nidentified quicker than others (e.g., sockets and thermostats were successfully\ndetected within five TCP sessions of connecting to the network). Perfect\ndetection of unauthorized IoT device types was achieved upon analyzing 110\nconsecutive sessions; perfect classification of white listed types required 346\nconsecutive sessions, 110 of which resulted in 99.49% accuracy. Further\nexperiments demonstrated the successful applicability of classifiers trained in\none location and tested on another. In addition, a discussion is provided\nregarding the resilience of our machine learning-based IoT white listing method\nto adversarial attacks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 07:50:46 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Meidan", "Yair", ""], ["Bohadana", "Michael", ""], ["Shabtai", "Asaf", ""], ["Ochoa", "Martin", ""], ["Tippenhauer", "Nils Ole", ""], ["Guarnizo", "Juan Davis", ""], ["Elovici", "Yuval", ""]]}, {"id": "1709.04666", "submitter": "Ryota Yoshihashi", "authors": "Ryota Yoshihashi, Tu Tuan Trinh, Rei Kawakami, Shaodi You, Makoto\n  Iida, Takeshi Naemura", "title": "Differentiating Objects by Motion: Joint Detection and Tracking of Small\n  Flying Objects", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While generic object detection has achieved large improvements with rich\nfeature hierarchies from deep nets, detecting small objects with poor visual\ncues remains challenging. Motion cues from multiple frames may be more\ninformative for detecting such hard-to-distinguish objects in each frame.\nHowever, how to encode discriminative motion patterns, such as deformations and\npose changes that characterize objects, has remained an open question. To learn\nthem and thereby realize small object detection, we present a neural model\ncalled the Recurrent Correlational Network, where detection and tracking are\njointly performed over a multi-frame representation learned through a single,\ntrainable, and end-to-end network. A convolutional long short-term memory\nnetwork is utilized for learning informative appearance change for detection,\nwhile learned representation is shared in tracking for enhancing its\nperformance. In experiments with datasets containing images of scenes with\nsmall flying objects, such as birds and unmanned aerial vehicles, the proposed\nmethod yielded consistent improvements in detection performance over deep\nsingle-frame detectors and existing motion-based detectors. Furthermore, our\nnetwork performs as well as state-of-the-art generic object trackers when it\nwas evaluated as a tracker on the bird dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 08:42:39 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 04:48:21 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 05:38:50 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Yoshihashi", "Ryota", ""], ["Trinh", "Tu Tuan", ""], ["Kawakami", "Rei", ""], ["You", "Shaodi", ""], ["Iida", "Makoto", ""], ["Naemura", "Takeshi", ""]]}, {"id": "1709.04695", "submitter": "Nikolay Jetchev", "authors": "Nikolay Jetchev, Urs Bergmann", "title": "The Conditional Analogy GAN: Swapping Fashion Articles on People Images", "comments": "To appear at the International Conference on Computer Vision, ICCV\n  2017, Workshop on Computer Vision for Fashion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to solve image analogy problems : it allows to\nlearn the relation between paired images present in training data, and then\ngeneralize and generate images that correspond to the relation, but were never\nseen in the training set. Therefore, we call the method Conditional Analogy\nGenerative Adversarial Network (CAGAN), as it is based on adversarial training\nand employs deep convolutional neural networks. An especially interesting\napplication of that technique is automatic swapping of clothing on fashion\nmodel photos. Our work has the following contributions. First, the definition\nof the end-to-end trainable CAGAN architecture, which implicitly learns\nsegmentation masks without expensive supervised labeling data. Second,\nexperimental results show plausible segmentation masks and often convincing\nswapped images, given the target article. Finally, we discuss the next steps\nfor that technique: neural network architecture improvements and more advanced\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 10:39:51 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Jetchev", "Nikolay", ""], ["Bergmann", "Urs", ""]]}, {"id": "1709.04725", "submitter": "Oriane Sim\\'eoni", "authors": "Oriane Sim\\'eoni and Ahmet Iscen and Giorgos Tolias and Yannis\n  Avrithis and Ondrej Chum", "title": "Unsupervised object discovery for instance recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Severe background clutter is challenging in many computer vision tasks,\nincluding large-scale image retrieval. Global descriptors, that are popular due\nto their memory and search efficiency, are especially prone to corruption by\nsuch a clutter. Eliminating the impact of the clutter on the image descriptor\nincreases the chance of retrieving relevant images and prevents topic drift due\nto actually retrieving the clutter in the case of query expansion. In this\nwork, we propose a novel salient region detection method. It captures, in an\nunsupervised manner, patterns that are both discriminative and common in the\ndataset. Saliency is based on a centrality measure of a nearest neighbor graph\nconstructed from regional CNN representations of dataset images. The\ndescriptors derived from the salient regions improve particular object\nretrieval, most noticeably in a large collections containing small objects.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 12:11:51 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 14:12:01 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Sim\u00e9oni", "Oriane", ""], ["Iscen", "Ahmet", ""], ["Tolias", "Giorgos", ""], ["Avrithis", "Yannis", ""], ["Chum", "Ondrej", ""]]}, {"id": "1709.04731", "submitter": "Takayoshi Yamashita", "authors": "Ryuji Kamiya, Takayoshi Yamashita, Mitsuru Ambai, Ikuro Sato, Yuji\n  Yamauchi, Hironobu Fujiyoshi", "title": "Binary-decomposed DCNN for accelerating computation and compressing\n  model without retraining", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends show recognition accuracy increasing even more profoundly.\nInference process of Deep Convolutional Neural Networks (DCNN) has a large\nnumber of parameters, requires a large amount of computation, and can be very\nslow. The large number of parameters also require large amounts of memory. This\nis resulting in increasingly long computation times and large model sizes. To\nimplement mobile and other low performance devices incorporating DCNN, model\nsizes must be compressed and computation must be accelerated. To that end, this\npaper proposes Binary-decomposed DCNN, which resolves these issues without the\nneed for retraining. Our method replaces real-valued inner-product computations\nwith binary inner-product computations in existing network models to accelerate\ncomputation of inference and decrease model size without the need for\nretraining. Binary computations can be done at high speed using logical\noperators such as XOR and AND, together with bit counting. In tests using\nAlexNet with the ImageNet classification task, speed increased by a factor of\n1.79, models were compressed by approximately 80%, and increase in error rate\nwas limited to 1.20%. With VGG-16, speed increased by a factor of 2.07, model\nsizes decreased by 81%, and error increased by only 2.16%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 12:30:41 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Kamiya", "Ryuji", ""], ["Yamashita", "Takayoshi", ""], ["Ambai", "Mitsuru", ""], ["Sato", "Ikuro", ""], ["Yamauchi", "Yuji", ""], ["Fujiyoshi", "Hironobu", ""]]}, {"id": "1709.04744", "submitter": "John Lipor", "authors": "John Lipor, David Hong, Yan Shuo Tan, and Laura Balzano", "title": "Subspace Clustering using Ensembles of $K$-Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is the unsupervised grouping of points lying near a union\nof low-dimensional linear subspaces. Algorithms based directly on geometric\nproperties of such data tend to either provide poor empirical performance, lack\ntheoretical guarantees, or depend heavily on their initialization. We present a\nnovel geometric approach to the subspace clustering problem that leverages\nensembles of the K-subspaces (KSS) algorithm via the evidence accumulation\nclustering framework. Our algorithm, referred to as ensemble K-subspaces\n(EKSS), forms a co-association matrix whose (i,j)th entry is the number of\ntimes points i and j are clustered together by several runs of KSS with random\ninitializations. We prove general recovery guarantees for any algorithm that\nforms an affinity matrix with entries close to a monotonic transformation of\npairwise absolute inner products. We then show that a specific instance of EKSS\nresults in an affinity matrix with entries of this form, and hence our proposed\nalgorithm can provably recover subspaces under similar conditions to\nstate-of-the-art algorithms. The finding is, to the best of our knowledge, the\nfirst recovery guarantee for evidence accumulation clustering and for KSS\nvariants. We show on synthetic data that our method performs well in the\ntraditionally challenging settings of subspaces with large intersection,\nsubspaces with small principal angles, and noisy data. Finally, we evaluate our\nalgorithm on six common benchmark datasets and show that unlike existing\nmethods, EKSS achieves excellent empirical performance when there are both a\nsmall and large number of points per subspace.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 12:55:56 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 22:18:48 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 23:39:59 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Lipor", "John", ""], ["Hong", "David", ""], ["Tan", "Yan Shuo", ""], ["Balzano", "Laura", ""]]}, {"id": "1709.04751", "submitter": "Florian Kraemer", "authors": "Florian Kraemer, Alexander Schaefer, Andreas Eitel, Johan Vertens,\n  Wolfram Burgard", "title": "From Plants to Landmarks: Time-invariant Plant Localization that uses\n  Deep Pose Regression in Agricultural Fields", "comments": "IROS 2017 AGROB Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agricultural robots are expected to increase yields in a sustainable way and\nautomate precision tasks, such as weeding and plant monitoring. At the same\ntime, they move in a continuously changing, semi-structured field environment,\nin which features can hardly be found and reproduced at a later time.\nChallenges for Lidar and visual detection systems stem from the fact that\nplants can be very small, overlapping and have a steadily changing appearance.\nTherefore, a popular way to localize vehicles with high accuracy is based on\nex- pensive global navigation satellite systems and not on natural landmarks.\nThe contribution of this work is a novel image- based plant localization\ntechnique that uses the time-invariant stem emerging point as a reference. Our\napproach is based on a fully convolutional neural network that learns landmark\nlocalization from RGB and NIR image input in an end-to-end manner. The network\nperforms pose regression to generate a plant location likelihood map. Our\napproach allows us to cope with visual variances of plants both for different\nspecies and different growth stages. We achieve high localization accuracies as\nshown in detailed evaluations of a sugar beet cultivation phase. In experiments\nwith our BoniRob we demonstrate that detections can be robustly reproduced with\ncentimeter accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 13:03:51 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Kraemer", "Florian", ""], ["Schaefer", "Alexander", ""], ["Eitel", "Andreas", ""], ["Vertens", "Johan", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1709.04762", "submitter": "Giacomo Spigler", "authors": "Giacomo Spigler", "title": "Denoising Autoencoders for Overgeneralization in Neural Networks", "comments": "9 pages, 5 figures, submitted", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2909876", "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent developments that allowed neural networks to achieve\nimpressive performance on a variety of applications, these models are\nintrinsically affected by the problem of overgeneralization, due to their\npartitioning of the full input space into the fixed set of target classes used\nduring training. Thus it is possible for novel inputs belonging to categories\nunknown during training or even completely unrecognizable to humans to fool the\nsystem into classifying them as one of the known classes, even with a high\ndegree of confidence. Solving this problem may help improve the security of\nsuch systems in critical applications, and may further lead to applications in\nthe context of open set recognition and 1-class recognition. This paper\npresents a novel way to compute a confidence score using denoising autoencoders\nand shows that such confidence score can correctly identify the regions of the\ninput space close to the training distribution by approximately identifying its\nlocal maxima.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 13:12:03 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 11:09:10 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 22:35:13 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Spigler", "Giacomo", ""]]}, {"id": "1709.04800", "submitter": "Eduardo Aguilar", "authors": "Eduardo Aguilar, Marc Bola\\~nos, Petia Radeva", "title": "Exploring Food Detection using CNNs", "comments": null, "journal-ref": "EUROCAST 2017 10672 (2018) 339-347", "doi": "10.1007/978-3-319-74727-9_40", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common critical factors directly related to the cause of a\nchronic disease is unhealthy diet consumption. In this sense, building an\nautomatic system for food analysis could allow a better understanding of the\nnutritional information with respect to the food eaten and thus it could help\nin taking corrective actions in order to consume a better diet. The Computer\nVision community has focused its efforts on several areas involved in the\nvisual food analysis such as: food detection, food recognition, food\nlocalization, portion estimation, among others. For food detection, the best\nresults evidenced in the state of the art were obtained using Convolutional\nNeural Network. However, the results of all these different approaches were\ngotten on different datasets and therefore are not directly comparable. This\narticle proposes an overview of the last advances on food detection and an\noptimal model based on GoogLeNet Convolutional Neural Network method, principal\ncomponent analysis, and a support vector machine that outperforms the state of\nthe art on two public food/non-food datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 14:03:01 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Aguilar", "Eduardo", ""], ["Bola\u00f1os", "Marc", ""], ["Radeva", "Petia", ""]]}, {"id": "1709.04821", "submitter": "Mennatullah Siam M.S.", "authors": "Mennatullah Siam, Heba Mahgoub, Mohamed Zahran, Senthil Yogamani,\n  Martin Jagersand, Ahmad El-Sallab", "title": "MODNet: Moving Object Detection Network with Motion and Appearance for\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel multi-task learning system that combines appearance and\nmotion cues for a better semantic reasoning of the environment. A unified\narchitecture for joint vehicle detection and motion segmentation is introduced.\nIn this architecture, a two-stream encoder is shared among both tasks. In order\nto evaluate our method in autonomous driving setting, KITTI annotated sequences\nwith detection and odometry ground truth are used to automatically generate\nstatic/dynamic annotations on the vehicles. This dataset is called KITTI Moving\nObject Detection dataset (KITTI MOD). The dataset will be made publicly\navailable to act as a benchmark for the motion detection task. Our experiments\nshow that the proposed method outperforms state of the art methods that utilize\nmotion cue only with 21.5% in mAP on KITTI MOD. Our method performs on par with\nthe state of the art unsupervised methods on DAVIS benchmark for generic object\nsegmentation. One of our interesting conclusions is that joint training of\nmotion segmentation and vehicle detection benefits motion segmentation. Motion\nsegmentation has relatively fewer data, unlike the detection task. However, the\nshared fusion encoder benefits from joint training to learn a generalized\nrepresentation. The proposed method runs in 120 ms per frame, which beats the\nstate of the art motion detection/segmentation in computational efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 14:48:43 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 18:20:58 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Siam", "Mennatullah", ""], ["Mahgoub", "Heba", ""], ["Zahran", "Mohamed", ""], ["Yogamani", "Senthil", ""], ["Jagersand", "Martin", ""], ["El-Sallab", "Ahmad", ""]]}, {"id": "1709.04836", "submitter": "Jiankang Deng", "authors": "Niannan Xue, Jiankang Deng, Yannis Panagakis, Stefanos Zafeiriou", "title": "Informed Non-convex Robust Principal Component Analysis with Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of robust principal component analysis with features\nacting as prior side information. To this aim, a novel, elegant, non-convex\noptimization approach is proposed to decompose a given observation matrix into\na low-rank core and the corresponding sparse residual. Rigorous theoretical\nanalysis of the proposed algorithm results in exact recovery guarantees with\nlow computational complexity. Aptly designed synthetic experiments demonstrate\nthat our method is the first to wholly harness the power of non-convexity over\nconvexity in terms of both recoverability and speed. That is, the proposed\nnon-convex approach is more accurate and faster compared to the best available\nalgorithms for the problem under study. Two real-world applications, namely\nimage classification and face denoising further exemplify the practical\nsuperiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 15:06:21 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Xue", "Niannan", ""], ["Deng", "Jiankang", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1709.04864", "submitter": "Eduardo Aguilar", "authors": "Eduardo Aguilar, Marc Bola\\~nos, Petia Radeva", "title": "Food Recognition using Fusion of Classifiers based on CNNs", "comments": null, "journal-ref": "ICIAP 10485 (2017) 213-224", "doi": "10.1007/978-3-319-68548-9_20", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the arrival of convolutional neural networks, the complex problem of\nfood recognition has experienced an important improvement in recent years. The\nbest results have been obtained using methods based on very deep convolutional\nneural networks, which show that the deeper the model,the better the\nclassification accuracy will be obtain. However, very deep neural networks may\nsuffer from the overfitting problem. In this paper, we propose a combination of\nmultiple classifiers based on different convolutional models that complement\neach other and thus, achieve an improvement in performance. The evaluation of\nour approach is done on two public datasets: Food-101 as a dataset with a wide\nvariety of fine-grained dishes, and Food-11 as a dataset of high-level food\ncategories, where our approach outperforms the independent CNN models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 16:35:40 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Aguilar", "Eduardo", ""], ["Bola\u00f1os", "Marc", ""], ["Radeva", "Petia", ""]]}, {"id": "1709.04881", "submitter": "Thomas K\\\"ohler", "authors": "Thomas K\\\"ohler, Michel B\\\"atz, Farzad Naderi, Andr\\'e Kaup, Andreas\n  K. Maier, Christian Riess", "title": "Benchmarking Super-Resolution Algorithms on Real Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decades, various super-resolution (SR) techniques have been\ndeveloped to enhance the spatial resolution of digital images. Despite the\ngreat number of methodical contributions, there is still a lack of comparative\nvalidations of SR under practical conditions, as capturing real ground truth\ndata is a challenging task. Therefore, current studies are either evaluated 1)\non simulated data or 2) on real data without a pixel-wise ground truth.\n  To facilitate comprehensive studies, this paper introduces the publicly\navailable Super-Resolution Erlangen (SupER) database that includes real\nlow-resolution images along with high-resolution ground truth data. Our\ndatabase comprises image sequences with more than 20k images captured from 14\nscenes under various types of motions and photometric conditions. The datasets\ncover four spatial resolution levels using camera hardware binning. With this\ndatabase, we benchmark 15 single-image and multi-frame SR algorithms. Our\nexperiments quantitatively analyze SR accuracy and robustness under realistic\nconditions including independent object and camera motion or photometric\nvariations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 21:44:29 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["K\u00f6hler", "Thomas", ""], ["B\u00e4tz", "Michel", ""], ["Naderi", "Farzad", ""], ["Kaup", "Andr\u00e9", ""], ["Maier", "Andreas K.", ""], ["Riess", "Christian", ""]]}, {"id": "1709.04905", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, Sergey Levine", "title": "One-Shot Visual Imitation Learning via Meta-Learning", "comments": "Conference on Robot Learning, 2017 (to appear). First two authors\n  contributed equally. Video available at\n  https://sites.google.com/view/one-shot-imitation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for a robot to be a generalist that can perform a wide range of\njobs, it must be able to acquire a wide variety of skills quickly and\nefficiently in complex unstructured environments. High-capacity models such as\ndeep neural networks can enable a robot to represent complex skills, but\nlearning each skill from scratch then becomes infeasible. In this work, we\npresent a meta-imitation learning method that enables a robot to learn how to\nlearn more efficiently, allowing it to acquire new skills from just a single\ndemonstration. Unlike prior methods for one-shot imitation, our method can\nscale to raw pixel inputs and requires data from significantly fewer prior\ntasks for effective learning of new skills. Our experiments on both simulated\nand real robot platforms demonstrate the ability to learn new tasks,\nend-to-end, from a single visual demonstration.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 17:50:18 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Finn", "Chelsea", ""], ["Yu", "Tianhe", ""], ["Zhang", "Tianhao", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1709.04989", "submitter": "Tomas Werner", "authors": "Tomas Werner", "title": "On Coordinate Minimization of Convex Piecewise-Affine Functions", "comments": "Research Report of Dept. of Cybernetics, Faculty of Electrical\n  Engineering, Czech Technical University in Prague", "journal-ref": null, "doi": null, "report-no": "CTU--CMP--2017--05", "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular class of algorithms to optimize the dual LP relaxation of the\ndiscrete energy minimization problem (a.k.a.\\ MAP inference in graphical models\nor valued constraint satisfaction) are convergent message-passing algorithms,\nsuch as max-sum diffusion, TRW-S, MPLP and SRMP. These algorithms are\nsuccessful in practice, despite the fact that they are a version of coordinate\nminimization applied to a convex piecewise-affine function, which is not\nguaranteed to converge to a global minimizer. These algorithms converge only to\na local minimizer, characterized by local consistency known from constraint\nprogramming. We generalize max-sum diffusion to a version of coordinate\nminimization applicable to an arbitrary convex piecewise-affine function, which\nconverges to a local consistency condition. This condition can be seen as the\nsign relaxation of the global optimality condition.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 21:44:53 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Werner", "Tomas", ""]]}, {"id": "1709.05011", "submitter": "Yang You", "authors": "Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, Kurt Keutzer", "title": "ImageNet Training in Minutes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finishing 90-epoch ImageNet-1k training with ResNet-50 on a NVIDIA M40 GPU\ntakes 14 days. This training requires 10^18 single precision operations in\ntotal. On the other hand, the world's current fastest supercomputer can finish\n2 * 10^17 single precision operations per second (Dongarra et al 2017,\nhttps://www.top500.org/lists/2017/06/). If we can make full use of the\nsupercomputer for DNN training, we should be able to finish the 90-epoch\nResNet-50 training in one minute. However, the current bottleneck for fast DNN\ntraining is in the algorithm level. Specifically, the current batch size (e.g.\n512) is too small to make efficient use of many processors. For large-scale DNN\ntraining, we focus on using large-batch data-parallelism synchronous SGD\nwithout losing accuracy in the fixed epochs. The LARS algorithm (You, Gitman,\nGinsburg, 2017, arXiv:1708.03888) enables us to scale the batch size to\nextremely large case (e.g. 32K). We finish the 100-epoch ImageNet training with\nAlexNet in 11 minutes on 1024 CPUs. About three times faster than Facebook's\nresult (Goyal et al 2017, arXiv:1706.02677), we finish the 90-epoch ImageNet\ntraining with ResNet-50 in 20 minutes on 2048 KNLs without losing accuracy.\nState-of-the-art ImageNet training speed with ResNet-50 is 74.9% top-1 test\naccuracy in 15 minutes. We got 74.9% top-1 test accuracy in 64 epochs, which\nonly needs 14 minutes. Furthermore, when we increase the batch size to above\n16K, our accuracy is much higher than Facebook's on corresponding batch sizes.\nOur source code is available upon request.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 23:59:52 GMT"}, {"version": "v10", "created": "Wed, 31 Jan 2018 05:15:27 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 16:16:05 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 09:35:12 GMT"}, {"version": "v4", "created": "Thu, 2 Nov 2017 14:39:48 GMT"}, {"version": "v5", "created": "Mon, 6 Nov 2017 04:41:02 GMT"}, {"version": "v6", "created": "Tue, 7 Nov 2017 18:40:04 GMT"}, {"version": "v7", "created": "Wed, 6 Dec 2017 05:03:46 GMT"}, {"version": "v8", "created": "Thu, 7 Dec 2017 18:55:56 GMT"}, {"version": "v9", "created": "Tue, 12 Dec 2017 00:02:50 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["You", "Yang", ""], ["Zhang", "Zhao", ""], ["Hsieh", "Cho-Jui", ""], ["Demmel", "James", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1709.05021", "submitter": "Ervin Teng", "authors": "Ervin Teng, Jo\\~ao Diogo Falc\\~ao, Bob Iannucci", "title": "ClickBAIT: Click-based Accelerated Incremental Training of Convolutional\n  Neural Networks", "comments": "11 pages, 14 figures. Datasets available at\n  http://clickbait.crossmobile.info", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's general-purpose deep convolutional neural networks (CNN) for image\nclassification and object detection are trained offline on large static\ndatasets. Some applications, however, will require training in real-time on\nlive video streams with a human-in-the-loop. We refer to this class of problem\nas Time-ordered Online Training (ToOT) - these problems will require a\nconsideration of not only the quantity of incoming training data, but the human\neffort required to tag and use it. In this paper, we define training benefit as\na metric to measure the effectiveness of a sequence in using each user\ninteraction. We demonstrate and evaluate a system tailored to performing ToOT\nin the field, capable of training an image classifier on a live video stream\nthrough minimal input from a human operator. We show that by exploiting the\ntime-ordered nature of the video stream through optical flow-based object\ntracking, we can increase the effectiveness of human actions by about 8 times.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 00:58:38 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Teng", "Ervin", ""], ["Falc\u00e3o", "Jo\u00e3o Diogo", ""], ["Iannucci", "Bob", ""]]}, {"id": "1709.05038", "submitter": "Yang Xian", "authors": "Yang Xian, Yingli Tian", "title": "Self-Guiding Multimodal LSTM - when we do not have a perfect training\n  dataset for image captioning", "comments": "The paper is under consideration at Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning\nmodel is proposed to handle uncontrolled imbalanced real-world image-sentence\ndataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165\nimages and the original text descriptions uploaded by the users are utilized as\nthe ground truth for training. Descriptions in FlickrNYC dataset vary\ndramatically ranging from short term-descriptions to long\nparagraph-descriptions and can describe any visual aspects, or even refer to\nobjects that are not depicted. To deal with the imbalanced and noisy situation\nand to fully explore the dataset itself, we propose a novel guiding textual\nfeature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of\nm-LSTM is based on the portion of data in which the image content and the\ncorresponding descriptions are strongly bonded. Afterwards, during the training\nof sg-LSTM on the rest training data, this guiding information serves as\nadditional input to the network along with the image representations and the\nground-truth descriptions. By integrating these input components into a\nmultimodal block, we aim to form a training scheme with the textual information\ntightly coupled with the image content. The experimental results demonstrate\nthat the proposed sg-LSTM model outperforms the traditional state-of-the-art\nmultimodal RNN captioning framework in successfully describing the key\ncomponents of the input images.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 02:53:16 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Xian", "Yang", ""], ["Tian", "Yingli", ""]]}, {"id": "1709.05054", "submitter": "Xuemei Xie", "authors": "Guimei Cao, Xuemei Xie, Wenzhe Yang, Quan Liao, Guangming Shi and\n  Jinjian Wu", "title": "Feature-Fused SSD: Fast Detection for Small Objects", "comments": "Artificial Intelligence;8 pages,8 figures", "journal-ref": "Ninth International Conference on Graphic and Image Processing\n  (ICGIP 2017)", "doi": "10.1117/12.2304811", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small objects detection is a challenging task in computer vision due to its\nlimited resolution and information. In order to solve this problem, the\nmajority of existing methods sacrifice speed for improvement in accuracy. In\nthis paper, we aim to detect small objects at a fast speed, using the best\nobject detector Single Shot Multibox Detector (SSD) with respect to\naccuracy-vs-speed trade-off as base architecture. We propose a multi-level\nfeature fusion method for introducing contextual information in SSD, in order\nto improve the accuracy for small objects. In detailed fusion operation, we\ndesign two feature fusion modules, concatenation module and element-sum module,\ndifferent in the way of adding contextual information. Experimental results\nshow that these two fusion modules obtain higher mAP on PASCALVOC2007 than\nbaseline SSD by 1.6 and 1.7 points respectively, especially with 2-3 points\nimprovement on some smallobjects categories. The testing speed of them is 43\nand 40 FPS respectively, superior to the state of the art Deconvolutional\nsingle shot detector (DSSD) by 29.4 and 26.4 FPS. Code is available at\nhttps://github.com/wnzhyee/Feature-Fused-SSD. Keywords: small object detection,\nfeature fusion, real-time, single shot multi-box detector\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 04:21:05 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 13:00:21 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 03:54:04 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Cao", "Guimei", ""], ["Xie", "Xuemei", ""], ["Yang", "Wenzhe", ""], ["Liao", "Quan", ""], ["Shi", "Guangming", ""], ["Wu", "Jinjian", ""]]}, {"id": "1709.05056", "submitter": "Marc Khoury", "authors": "Marc Khoury, Qian-Yi Zhou, Vladlen Koltun", "title": "Learning Compact Geometric Features", "comments": "International Conference on Computer Vision (ICCV), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to learning features that represent the local geometry\naround a point in an unstructured point cloud. Such features play a central\nrole in geometric registration, which supports diverse applications in robotics\nand 3D vision. Current state-of-the-art local features for unstructured point\nclouds have been manually crafted and none combines the desirable properties of\nprecision, compactness, and robustness. We show that features with these\nproperties can be learned from data, by optimizing deep networks that map\nhigh-dimensional histograms into low-dimensional Euclidean spaces. The\npresented approach yields a family of features, parameterized by dimension,\nthat are both more compact and more accurate than existing descriptors.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 04:44:28 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Khoury", "Marc", ""], ["Zhou", "Qian-Yi", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1709.05065", "submitter": "Behzad Mahaseni", "authors": "Behzad Mahaseni, Nabhan D. Salih", "title": "Asian Stamps Identification and Classification System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of stamp recognition. The goal is to\nclassify a given stamp to a certain country and also identify the year it is\npublished. We propose a new approach for stamp recognition based on describing\na given stamp image using color information and texture information. For color\ninformation we use color histogram for the entire image and for texture we use\ntwo features. SIFT which is based on local feature descriptors and HOG which is\na dens texture descriptor. As a result on total we have three different types\nof features. Our initial evaluation shows that give these information we are\nable to classify the images with a reasonable accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 05:38:13 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Mahaseni", "Behzad", ""], ["Salih", "Nabhan D.", ""]]}, {"id": "1709.05072", "submitter": "Yanyun Qu", "authors": "Yanyun Qu, Li Lin, Fumin Shen, Chang Lu, Yang Wu, Yuan Xie, Dacheng\n  Tao", "title": "Joint Hierarchical Category Structure Learning and Large-Scale Image\n  Classification", "comments": "16 pages, 14 figures", "journal-ref": "IEEE Trans. on Image Processing,2017", "doi": "10.1109/TIP.2016.2615423", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the scalable image classification problem with a large number\nof categories. Hierarchical visual data structures are helpful for improving\nthe efficiency and performance of large-scale multi-class classification. We\npropose a novel image classification method based on learning hierarchical\ninter-class structures. Specifically, we first design a fast algorithm to\ncompute the similarity metric between categories, based on which a visual tree\nis constructed by hierarchical spectral clustering. Using the learned visual\ntree, a test sample label is efficiently predicted by searching for the best\npath over the entire tree. The proposed method is extensively evaluated on the\nILSVRC2010 and Caltech 256 benchmark datasets. Experimental results show that\nour method obtains significantly better category hierarchies than other\nstate-of-the-art visual tree-based methods and, therefore, much more accurate\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 06:49:12 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Qu", "Yanyun", ""], ["Lin", "Li", ""], ["Shen", "Fumin", ""], ["Lu", "Chang", ""], ["Wu", "Yang", ""], ["Xie", "Yuan", ""], ["Tao", "Dacheng", ""]]}, {"id": "1709.05083", "submitter": "Yanyun Qu", "authors": "Yanyun Qu, Jinyan Liu, Yuan Xie, and Wensheng Zhang", "title": "Robust Kernelized Multi-View Self-Representations for Clustering by\n  Tensor Multi-Rank Minimization", "comments": "8 pages, 5 figures, AAAI2018 submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recently, tensor-SVD is implemented on multi-view self-representation\nclustering and has achieved the promising results in many real-world\napplications such as face clustering, scene clustering and generic object\nclustering. However, tensor-SVD based multi-view self-representation clustering\nis proposed originally to solve the clustering problem in the multiple linear\nsubspaces, leading to unsatisfactory results when dealing with the case of\nnon-linear subspaces. To handle data clustering from the non-linear subspaces,\na kernelization method is designed by mapping the data from the original input\nspace to a new feature space in which the transformed data can be clustered by\na multiple linear clustering method. In this paper, we make an optimization\nmodel for the kernelized multi-view self-representation clustering problem. We\nalso develop a new efficient algorithm based on the alternation direction\nmethod and infer a closed-form solution. Since all the subproblems can be\nsolved exactly, the proposed optimization algorithm is guaranteed to obtain the\noptimal solution. In particular, the original tensor-based multi-view\nself-representation clustering problem is a special case of our approach and\ncan be solved by our algorithm. Experimental results on several popular\nreal-world clustering datasets demonstrate that our approach achieves the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 07:32:20 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Qu", "Yanyun", ""], ["Liu", "Jinyan", ""], ["Xie", "Yuan", ""], ["Zhang", "Wensheng", ""]]}, {"id": "1709.05087", "submitter": "Naveed Akhtar Dr.", "authors": "Jian Liu, Naveed Akhtar, Ajmal Mian", "title": "Viewpoint Invariant Action Recognition using RGB-D Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video-based action recognition, viewpoint variations often pose major\nchallenges because the same actions can appear different from different views.\nWe use the complementary RGB and Depth information from the RGB-D cameras to\naddress this problem. The proposed technique capitalizes on the spatio-temporal\ninformation available in the two data streams to the extract action features\nthat are largely insensitive to the viewpoint variations. We use the RGB data\nto compute dense trajectories that are translated to viewpoint insensitive deep\nfeatures under a non-linear knowledge transfer model. Similarly, the Depth\nstream is used to extract CNN-based view invariant features on which Fourier\nTemporal Pyramid is computed to incorporate the temporal information. The\nheterogeneous features from the two streams are combined and used as a\ndictionary to predict the label of the test samples. To that end, we propose a\nsparse-dense collaborative representation classification scheme that strikes a\nbalance between the discriminative abilities of the dense and the sparse\nrepresentations of the samples over the extracted heterogeneous dictionary.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 07:39:34 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 03:32:59 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Liu", "Jian", ""], ["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1709.05107", "submitter": "Ke Chen", "authors": "Qian Wang and Ke Chen", "title": "Multi-Label Zero-Shot Human Action Recognition via Joint Latent Ranking\n  Embedding", "comments": "27 pages, 10 figures and 7 tables. Technical report submitted to a\n  journal. More experimental results/references were added and typos were\n  corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition refers to automatic recognizing human actions from a\nvideo clip. In reality, there often exist multiple human actions in a video\nstream. Such a video stream is often weakly-annotated with a set of relevant\nhuman action labels at a global level rather than assigning each label to a\nspecific video episode corresponding to a single action, which leads to a\nmulti-label learning problem. Furthermore, there are many meaningful human\nactions in reality but it would be extremely difficult to collect/annotate\nvideo clips regarding all of various human actions, which leads to a zero-shot\nlearning scenario. To the best of our knowledge, there is no work that has\naddressed all the above issues together in human action recognition. In this\npaper, we formulate a real-world human action recognition task as a multi-label\nzero-shot learning problem and propose a framework to tackle this problem in a\nholistic way. Our framework holistically tackles the issue of unknown temporal\nboundaries between different actions for multi-label learning and exploits the\nside information regarding the semantic relationship between different human\nactions for knowledge transfer. Consequently, our framework leads to a joint\nlatent ranking embedding for multi-label zero-shot human action recognition. A\nnovel neural architecture of two component models and an alternate learning\nalgorithm are proposed to carry out the joint latent ranking embedding\nlearning. Thus, multi-label zero-shot recognition is done by measuring\nrelatedness scores of action labels to a test video clip in the joint latent\nvisual and semantic embedding spaces. We evaluate our framework with different\nsettings, including a novel data split scheme designed especially for\nevaluating multi-label zero-shot learning, on two datasets: Breakfast and\nCharades. The experimental results demonstrate the effectiveness of our\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 08:44:02 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 08:51:15 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 12:42:58 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Wang", "Qian", ""], ["Chen", "Ke", ""]]}, {"id": "1709.05165", "submitter": "Xuelin Qian", "authors": "Xuelin Qian, Yanwei Fu, Yu-Gang Jiang, Tao Xiang, Xiangyang Xue", "title": "Multi-scale Deep Learning Architectures for Person Re-identification", "comments": "9 pages, 3 figures, accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-identification (re-id) aims to match people across non-overlapping\ncamera views in a public space. It is a challenging problem because many people\ncaptured in surveillance videos wear similar clothes. Consequently, the\ndifferences in their appearance are often subtle and only detectable at the\nright location and scales. Existing re-id models, particularly the recently\nproposed deep learning based ones match people at a single scale. In contrast,\nin this paper, a novel multi-scale deep learning model is proposed. Our model\nis able to learn deep discriminative feature representations at different\nscales and automatically determine the most suitable scales for matching. The\nimportance of different spatial locations for extracting discriminative\nfeatures is also learned explicitly. Experiments are carried out to demonstrate\nthat the proposed model outperforms the state-of-the art on a number of\nbenchmarks\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 11:53:59 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Qian", "Xuelin", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""], ["Xiang", "Tao", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1709.05185", "submitter": "Timoth\\'ee Lesort", "authors": "Timoth\\'ee Lesort, Mathieu Seurin, Xinrui Li, Natalia\n  D\\'iaz-Rodr\\'iguez and David Filliat", "title": "Unsupervised state representation learning with robotic priors: a\n  robustness benchmark", "comments": "ICRA 2018 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our understanding of the world depends highly on our capacity to produce\nintuitive and simplified representations which can be easily used to solve\nproblems. We reproduce this simplification process using a neural network to\nbuild a low dimensional state representation of the world from images acquired\nby a robot. As in Jonschkowski et al. 2015, we learn in an unsupervised way\nusing prior knowledge about the world as loss functions called robotic priors\nand extend this approach to high dimension richer images to learn a 3D\nrepresentation of the hand position of a robot from RGB images. We propose a\nquantitative evaluation of the learned representation using nearest neighbors\nin the state space that allows to assess its quality and show both the\npotential and limitations of robotic priors in realistic environments. We\naugment image size, add distractors and domain randomization, all crucial\ncomponents to achieve transfer learning to real robots. Finally, we also\ncontribute a new prior to improve the robustness of the representation. The\napplications of such low dimensional state representation range from easing\nreinforcement learning (RL) and knowledge transfer across tasks, to\nfacilitating learning from raw data with more efficient and compact high level\nrepresentations. The results show that the robotic prior approach is able to\nextract high level representation as the 3D position of an arm and organize it\ninto a compact and coherent space of states in a challenging dataset.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 13:15:58 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Lesort", "Timoth\u00e9e", ""], ["Seurin", "Mathieu", ""], ["Li", "Xinrui", ""], ["D\u00edaz-Rodr\u00edguez", "Natalia", ""], ["Filliat", "David", ""]]}, {"id": "1709.05188", "submitter": "Yujia Chen", "authors": "Yujia Chen, Lingxiao Song, Ran He", "title": "Adversarial Occlusion-aware Face Detection", "comments": "Accepted by ACPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occluded face detection is a challenging detection task due to the large\nappearance variations incurred by various real-world occlusions. This paper\nintroduces an Adversarial Occlusion-aware Face Detector (AOFD) by\nsimultaneously detecting occluded faces and segmenting occluded areas.\nSpecifically, we employ an adversarial training strategy to generate\nocclusion-like face features that are difficult for a face detector to\nrecognize. Occlusion mask is predicted simultaneously while detecting occluded\nfaces and the occluded area is utilized as an auxiliary instead of being\nregarded as a hindrance. Moreover, the supervisory signals from the\nsegmentation branch will reversely affect the features, aiding in detecting\nheavily-occluded faces accordingly. Consequently, AOFD is able to find the\nfaces with few exposed facial landmarks with very high confidences and keeps\nhigh detection accuracy even for masked faces. Extensive experiments\ndemonstrate that AOFD not only significantly outperforms state-of-the-art\nmethods on the MAFA occluded face detection dataset, but also achieves\ncompetitive detection accuracy on benchmark dataset for general face detection\nsuch as FDDB.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 13:22:22 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 14:42:00 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 13:40:55 GMT"}, {"version": "v4", "created": "Sat, 13 Jan 2018 12:46:03 GMT"}, {"version": "v5", "created": "Thu, 22 Feb 2018 03:32:23 GMT"}, {"version": "v6", "created": "Sat, 29 Sep 2018 22:25:40 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Chen", "Yujia", ""], ["Song", "Lingxiao", ""], ["He", "Ran", ""]]}, {"id": "1709.05256", "submitter": "Zhifeng Li", "authors": "Yitong Wang, Xing Ji, Zheng Zhou, Hao Wang, Zhifeng Li", "title": "Detecting Faces Using Region-based Fully Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection has achieved great success using the region-based methods. In\nthis report, we propose a region-based face detector applying deep networks in\na fully convolutional fashion, named Face R-FCN. Based on Region-based Fully\nConvolutional Networks (R-FCN), our face detector is more accurate and\ncomputational efficient compared with the previous R-CNN based face detectors.\nIn our approach, we adopt the fully convolutional Residual Network (ResNet) as\nthe backbone network. Particularly, We exploit several new techniques including\nposition-sensitive average pooling, multi-scale training and testing and\non-line hard example mining strategy to improve the detection accuracy. Over\ntwo most popular and challenging face detection benchmarks, FDDB and WIDER\nFACE, Face R-FCN achieves superior performance over state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 09:05:54 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 13:44:16 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Wang", "Yitong", ""], ["Ji", "Xing", ""], ["Zhou", "Zheng", ""], ["Wang", "Hao", ""], ["Li", "Zhifeng", ""]]}, {"id": "1709.05283", "submitter": "Soumyabrata Dev", "authors": "Shilpa Manandhar, Soumyabrata Dev, Yee Hui Lee and Yu Song Meng", "title": "Correlating Satellite Cloud Cover with Sky Cameras", "comments": "Published in Proc. Progress In Electromagnetics Research Symposium\n  (PIERS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of clouds is manifold in understanding the various events in the\natmosphere, and also in studying the radiative balance of the earth. The\nconventional manner of such cloud analysis is performed mainly via satellite\nimages. However, because of its low temporal- and spatial- resolutions,\nground-based sky cameras are now getting popular. In this paper, we study the\nrelation between the cloud cover obtained from MODIS images, with the coverage\nobtained from ground-based sky cameras. This will help us to better understand\ncloud formation in the atmosphere - both from satellite images and ground-based\nobservations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 17:54:57 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Manandhar", "Shilpa", ""], ["Dev", "Soumyabrata", ""], ["Lee", "Yee Hui", ""], ["Meng", "Yu Song", ""]]}, {"id": "1709.05293", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt", "title": "Commonsense Scene Semantics for Cognitive Robotics: Towards Grounding\n  Embodied Visuo-Locomotive Interactions", "comments": "to appear in: ICCV 2017 Workshop - Vision in Practice on Autonomous\n  Robots (ViPAR), International Conference on Computer Vision (ICCV), Venice,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a commonsense, qualitative model for the semantic grounding of\nembodied visuo-spatial and locomotive interactions. The key contribution is an\nintegrative methodology combining low-level visual processing with high-level,\nhuman-centred representations of space and motion rooted in artificial\nintelligence. We demonstrate practical applicability with examples involving\nobject interactions, and indoor movement.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 16:22:56 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""]]}, {"id": "1709.05307", "submitter": "Francesca Murabito", "authors": "Francesca Murabito, Concetto Spampinato, Simone Palazzo, Konstantin\n  Pogorelov and Michael Riegler", "title": "Top-Down Saliency Detection Driven by Visual Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for top-down saliency detection guided by\nvisual classification tasks. We first learn how to compute visual saliency when\na specific visual task has to be accomplished, as opposed to most\nstate-of-the-art methods which assess saliency merely through bottom-up\nprinciples. Afterwards, we investigate if and to what extent visual saliency\ncan support visual classification in nontrivial cases. To achieve this, we\npropose SalClassNet, a CNN framework consisting of two networks jointly\ntrained: a) the first one computing top-down saliency maps from input images,\nand b) the second one exploiting the computed saliency maps for visual\nclassification. To test our approach, we collected a dataset of eye-gaze maps,\nusing a Tobii T60 eye tracker, by asking several subjects to look at images\nfrom the Stanford Dogs dataset, with the objective of distinguishing dog\nbreeds. Performance analysis on our dataset and other saliency bench-marking\ndatasets, such as POET, showed that SalClassNet out-performs state-of-the-art\nsaliency detectors, such as SalNet and SALICON. Finally, we analyzed the\nperformance of SalClassNet in a fine-grained recognition task and found out\nthat it generalizes better than existing visual classifiers. The achieved\nresults, thus, demonstrate that 1) conditioning saliency detectors with object\nclasses reaches state-of-the-art performance, and 2) providing explicitly\ntop-down saliency maps to visual classifiers enhances classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 16:58:57 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 10:05:33 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 18:39:30 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Murabito", "Francesca", ""], ["Spampinato", "Concetto", ""], ["Palazzo", "Simone", ""], ["Pogorelov", "Konstantin", ""], ["Riegler", "Michael", ""]]}, {"id": "1709.05311", "submitter": "Arif Ahmed Sk", "authors": "A. Ahmed, D. P. Dogra, S. Kar, R. Patnaik, S. Lee, H. Choi, I. Kim", "title": "Video Synopsis Generation Using Spatio-Temporal Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of surveillance cameras operate at 24x7 generating huge amount of\nvisual data for processing. However, retrieval of important activities from\nsuch a large data can be time consuming. Thus, researchers are working on\nfinding solutions to present hours of visual data in a compressed, but\nmeaningful way. Video synopsis is one of the ways to represent activities using\nrelatively shorter duration clips. So far, two main approaches have been used\nby researchers to address this problem, namely synopsis by tracking moving\nobjects and synopsis by clustering moving objects. Synopses outputs, mainly\ndepend on tracking, segmenting, and shifting of moving objects temporally as\nwell as spatially. In many situations, tracking fails, thus produces multiple\ntrajectories of the same object. Due to this, the object may appear and\ndisappear multiple times within the same synopsis output, which is misleading.\nThis also leads to discontinuity and often can be confusing to the viewer of\nthe synopsis. In this paper, we present a new approach for generating\ncompressed video synopsis by grouping tracklets of moving objects. Grouping\nhelps to generate a synopsis where chronologically related objects appear\ntogether with meaningful spatio-temporal relation. Our proposed method produces\ncontinuous, but a less confusing synopses when tested on publicly available\ndataset videos as well as in-house dataset videos.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 17:05:58 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Ahmed", "A.", ""], ["Dogra", "D. P.", ""], ["Kar", "S.", ""], ["Patnaik", "R.", ""], ["Lee", "S.", ""], ["Choi", "H.", ""], ["Kim", "I.", ""]]}, {"id": "1709.05324", "submitter": "Stuart Gibson", "authors": "Fangliang Bai, Manuel J. Marques and Stuart J. Gibson", "title": "Cystoid macular edema segmentation of Optical Coherence Tomography\n  images using fully convolutional neural networks and fully connected CRFs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new method for cystoid macular edema (CME)\nsegmentation in retinal Optical Coherence Tomography (OCT) images, using a\nfully convolutional neural network (FCN) and a fully connected conditional\nrandom fields (dense CRFs). As a first step, the framework trains the FCN model\nto extract features from retinal layers in OCT images, which exhibit CME, and\nthen segments CME regions using the trained model. Thereafter, dense CRFs are\nused to refine the segmentation according to the edema appearance. We have\ntrained and tested the framework with OCT images from 10 patients with diabetic\nmacular edema (DME). Our experimental results show that fluid and concrete\nmacular edema areas were segmented with good adherence to boundaries. A\nsegmentation accuracy of $0.61\\pm 0.21$ (Dice coefficient) was achieved, with\nrespect to the ground truth, which compares favourably with the previous\nstate-of-the-art that used a kernel regression based method ($0.51\\pm 0.34$).\nOur approach is versatile and we believe it can be easily adapted to detect\nother macular defects.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 17:33:19 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Bai", "Fangliang", ""], ["Marques", "Manuel J.", ""], ["Gibson", "Stuart J.", ""]]}, {"id": "1709.05360", "submitter": "Zhongang Qi", "authors": "Zhongang Qi, Saeed Khorram, Fuxin Li", "title": "Embedding Deep Networks into Visual Explanations", "comments": null, "journal-ref": "Artificial Intelligence (2020)", "doi": "10.1016/j.artint.2020.103435", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Explanation Neural Network (XNN) to explain\nthe predictions made by a deep network. The XNN works by learning a nonlinear\nembedding of a high-dimensional activation vector of a deep network layer into\na low-dimensional explanation space while retaining faithfulness i.e., the\noriginal deep learning predictions can be constructed from the few concepts\nextracted by our explanation network. We then visualize such concepts for human\nto learn about the high-level concepts that the deep network is using to make\ndecisions. We propose an algorithm called Sparse Reconstruction Autoencoder\n(SRAE) for learning the embedding to the explanation space. SRAE aims to\nreconstruct part of the original feature space while retaining faithfulness. A\npull-away term is applied to SRAE to make the bases of the explanation space\nmore orthogonal to each other. A visualization system is then introduced for\nhuman understanding of the features in the explanation space. The proposed\nmethod is applied to explain CNN models in image classification tasks. We\nconducted a human study, which shows that the proposed approach outperforms\nsingle saliency map baselines, and improves human performance on a difficult\nclassification tasks. Also, several novel metrics are introduced to evaluate\nthe performance of explanations quantitatively without human involvement.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 18:16:34 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 01:13:52 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 09:26:19 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Qi", "Zhongang", ""], ["Khorram", "Saeed", ""], ["Li", "Fuxin", ""]]}, {"id": "1709.05374", "submitter": "Frank Ong", "authors": "Frank Ong, Joseph Cheng, and Michael Lustig", "title": "General Phase Regularized Reconstruction using Phase Cycling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop a general phase regularized image reconstruction method,\nwith applications to partial Fourier imaging, water-fat imaging and flow\nimaging.\n  Theory and Methods: The problem of enforcing phase constraints in\nreconstruction was studied under a regularized inverse problem framework. A\ngeneral phase regularized reconstruction algorithm was proposed to enable\nvarious joint reconstruction of partial Fourier imaging, water-fat imaging and\nflow imaging, along with parallel imaging (PI) and compressed sensing (CS).\nSince phase regularized reconstruction is inherently non-convex and sensitive\nto phase wraps in the initial solution, a reconstruction technique, named phase\ncycling, was proposed to render the overall algorithm invariant to phase wraps.\nThe proposed method was applied to retrospectively under-sampled in vivo\ndatasets and compared with state of the art reconstruction methods.\n  Results: Phase cycling reconstructions showed reduction of artifacts compared\nto reconstructions with- out phase cycling and achieved similar performances as\nstate of the art results in partial Fourier, water-fat and divergence-free\nregularized flow reconstruction. Joint reconstruction of partial Fourier +\nwater-fat imaging + PI + CS, and partial Fourier + divergence-free regularized\nflow imaging + PI + CS were demonstrated.\n  Conclusion: The proposed phase cycling reconstruction provides an alternative\nway to perform phase regularized reconstruction, without the need to perform\nphase unwrapping. It is robust to the choice of initial solutions and\nencourages the joint reconstruction of phase imaging applications.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 19:17:13 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Ong", "Frank", ""], ["Cheng", "Joseph", ""], ["Lustig", "Michael", ""]]}, {"id": "1709.05397", "submitter": "Kanji Tanaka", "authors": "Tanaka Kanji", "title": "Zero-Shot Learning to Manage a Large Number of Place-Specific\n  Compressive Change Classifiers", "comments": "8 pages, 11 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent progress in large-scale map maintenance and long-term map\nlearning, the task of change detection on a large-scale map from a visual image\ncaptured by a mobile robot has become a problem of increasing criticality.\nPrevious approaches for change detection are typically based on image\ndifferencing and require the memorization of a prohibitively large number of\nmapped images in the above context. In contrast, this study follows the recent,\nefficient paradigm of change-classifier-learning and specifically employs a\ncollection of place-specific change classifiers. Our change-classifier-learning\nalgorithm is based on zero-shot learning (ZSL) and represents a place-specific\nchange classifier by its training examples mined from an external knowledge\nbase (EKB). The proposed algorithm exhibits several advantages. First, we are\nrequired to memorize only training examples (rather than the classifier\nitself), which can be further compressed in the form of bag-of-words (BoW).\nSecondly, we can incorporate the most recent map into the classifiers by\nstraightforwardly adding or deleting a few training examples that correspond to\nthese classifiers. Thirdly, we can share the BoW vocabulary with other related\ntask scenarios (e.g., BoW-based self-localization), wherein the vocabulary is\ngenerally designed as a rich, continuously growing, and domain-adaptive\nknowledge base. In our contribution, the proposed algorithm is applied and\nevaluated on a practical long-term cross-season change detection system that\nconsists of a large number of place-specific object-level change classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 20:43:40 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Kanji", "Tanaka", ""]]}, {"id": "1709.05424", "submitter": "Hossein Talebi", "authors": "Hossein Talebi, Peyman Milanfar", "title": "NIMA: Neural Image Assessment", "comments": "IEEE Transactions on Image Processing 2018", "journal-ref": null, "doi": "10.1109/TIP.2018.2831899", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically learned quality assessment for images has recently become a hot\ntopic due to its usefulness in a wide variety of applications such as\nevaluating image capture pipelines, storage techniques and sharing media.\nDespite the subjective nature of this problem, most existing methods only\npredict the mean opinion score provided by datasets such as AVA [1] and TID2013\n[2]. Our approach differs from others in that we predict the distribution of\nhuman opinion scores using a convolutional neural network. Our architecture\nalso has the advantage of being significantly simpler than other methods with\ncomparable performance. Our proposed approach relies on the success (and\nretraining) of proven, state-of-the-art deep object recognition networks. Our\nresulting network can be used to not only score images reliably and with high\ncorrelation to human perception, but also to assist with adaptation and\noptimization of photo editing/enhancement algorithms in a photographic\npipeline. All this is done without need for a \"golden\" reference image,\nconsequently allowing for single-image, semantic- and perceptually-aware,\nno-reference quality assessment.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 22:04:49 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 23:35:58 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Talebi", "Hossein", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1709.05436", "submitter": "Yuanlu Xu", "authors": "Hang Qi, Yuanlu Xu, Tao Yuan, Tianfu Wu, Song-Chun Zhu", "title": "Scene-centric Joint Parsing of Cross-view Videos", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view video understanding is an important yet under-explored area in\ncomputer vision. In this paper, we introduce a joint parsing framework that\nintegrates view-centric proposals into scene-centric parse graphs that\nrepresent a coherent scene-centric understanding of cross-view scenes. Our key\nobservations are that overlapping fields of views embed rich appearance and\ngeometry correlations and that knowledge fragments corresponding to individual\nvision tasks are governed by consistency constraints available in commonsense\nknowledge. The proposed joint parsing framework represents such correlations\nand constraints explicitly and generates semantic scene-centric parse graphs.\nQuantitative experiments show that scene-centric predictions in the parse graph\noutperform view-centric predictions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 00:21:29 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 23:01:01 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 05:59:11 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Qi", "Hang", ""], ["Xu", "Yuanlu", ""], ["Yuan", "Tao", ""], ["Wu", "Tianfu", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1709.05437", "submitter": "Yuanlu Xu", "authors": "Yuanlu Xu, Lei Qin, Xiaobai Liu, Jianwen Xie, Song-Chun Zhu", "title": "A Causal And-Or Graph Model for Visibility Fluent Reasoning in Tracking\n  Interacting Objects", "comments": "accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking humans that are interacting with the other subjects or environment\nremains unsolved in visual tracking, because the visibility of the human of\ninterests in videos is unknown and might vary over time. In particular, it is\nstill difficult for state-of-the-art human trackers to recover complete human\ntrajectories in crowded scenes with frequent human interactions. In this work,\nwe consider the visibility status of a subject as a fluent variable, whose\nchange is mostly attributed to the subject's interaction with the surrounding,\ne.g., crossing behind another object, entering a building, or getting into a\nvehicle, etc. We introduce a Causal And-Or Graph (C-AOG) to represent the\ncausal-effect relations between an object's visibility fluent and its\nactivities, and develop a probabilistic graph model to jointly reason the\nvisibility fluent change (e.g., from visible to invisible) and track humans in\nvideos. We formulate this joint task as an iterative search of a feasible\ncausal graph structure that enables fast search algorithm, e.g., dynamic\nprogramming method. We apply the proposed method on challenging video sequences\nto evaluate its capabilities of estimating visibility fluent changes of\nsubjects and tracking subjects of interests over time. Results with comparisons\ndemonstrate that our method outperforms the alternative trackers and can\nrecover complete trajectories of humans in complicated scenarios with frequent\nhuman interactions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 00:21:44 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 02:05:45 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Xu", "Yuanlu", ""], ["Qin", "Lei", ""], ["Liu", "Xiaobai", ""], ["Xie", "Jianwen", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1709.05439", "submitter": "Amir Sadeghian", "authors": "Noriaki Hirose, Amir Sadeghian, Patrick Goebel and Silvio Savarese", "title": "To Go or Not To Go? A Near Unsupervised Learning Approach For Robot\n  Navigation", "comments": "Noriaki Hirose and Amir Sadeghian contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is important for robots to be able to decide whether they can go through a\nspace or not, as they navigate through a dynamic environment. This capability\ncan help them avoid injury or serious damage, e.g., as a result of running into\npeople and obstacles, getting stuck, or falling off an edge. To this end, we\npropose an unsupervised and a near-unsupervised method based on Generative\nAdversarial Networks (GAN) to classify scenarios as traversable or not based on\nvisual data. Our method is inspired by the recent success of data-driven\napproaches on computer vision problems and anomaly detection, and reduces the\nneed for vast amounts of negative examples at training time. Collecting\nnegative data indicating that a robot should not go through a space is\ntypically hard and dangerous because of collisions, whereas collecting positive\ndata can be automated and done safely based on the robot's own traveling\nexperience. We verify the generality and effectiveness of the proposed approach\non a test dataset collected in a previously unseen environment with a mobile\nrobot. Furthermore, we show that our method can be used to build costmaps (we\ncall as \"GoNoGo\" costmaps) for robot path planning using visual data only.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 00:39:19 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Hirose", "Noriaki", ""], ["Sadeghian", "Amir", ""], ["Goebel", "Patrick", ""], ["Savarese", "Silvio", ""]]}, {"id": "1709.05470", "submitter": "Kanji Tanaka", "authors": "Xiaoxiao Fei, Kanji Tanaka, Yichu Fang, Akitaka Takayama", "title": "Long-Term Ensemble Learning of Visual Place Classifiers", "comments": "8 pages, 9 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of cross-season visual place classification\n(VPC) from a novel perspective of long-term map learning. Our goal is to enable\ntransfer learning efficiently from one season to the next, at a small constant\ncost, and without wasting the robot's available long-term-memory by memorizing\nvery large amounts of training data. To realize a good tradeoff between\ngeneralization and specialization abilities, we employ an ensemble of\nconvolutional neural network (DCN) classifiers and consider the task of\nscheduling (when and which classifiers to retrain), given a previous season's\nDCN classifiers as the sole prior knowledge. We present a unified framework for\nretraining scheduling and discuss practical implementation strategies.\nFurthermore, we address the task of partitioning a robot's workspace into\nplaces to define place classes in an unsupervised manner, rather than using\nuniform partitioning, so as to maximize VPC performance. Experiments using the\npublicly available NCLT dataset revealed that retraining scheduling of a DCN\nclassifier ensemble is crucial and performance is significantly increased by\nusing planned scheduling.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 06:52:20 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Fei", "Xiaoxiao", ""], ["Tanaka", "Kanji", ""], ["Fang", "Yichu", ""], ["Takayama", "Akitaka", ""]]}, {"id": "1709.05495", "submitter": "Boguslaw Obara", "authors": "\\c{C}i\\u{g}dem Sazak, Carl J. Nelson, Boguslaw Obara", "title": "The Multiscale Bowler-Hat Transform for Blood Vessel Enhancement in\n  Retinal Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancement, followed by segmentation, quantification and modelling, of blood\nvessels in retinal images plays an essential role in computer-aid retinopathy\ndiagnosis. In this paper, we introduce a new vessel enhancement method which is\nthe bowler-hat transform based on mathematical morphology. The proposed method\ncombines different structuring elements to detect innate features of\nvessel-like structures. We evaluate the proposed method qualitatively and\nquantitatively, and compare it with the existing, state-of-the-art methods\nusing both synthetic and real datasets. Our results show that the proposed\nmethod achieves high-quality vessel-like structure enhancement in both\nsynthetic examples and in clinically relevant retinal images, and is shown to\nbe able to detect fine vessels while remaining robust at junctions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 11:00:33 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 15:15:12 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 12:51:10 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Sazak", "\u00c7i\u011fdem", ""], ["Nelson", "Carl J.", ""], ["Obara", "Boguslaw", ""]]}, {"id": "1709.05538", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Chaochun Liu, Wei Fan, Xiaohui Xie", "title": "DeepLung: 3D Deep Convolutional Nets for Automated Pulmonary Nodule\n  Detection and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we present a fully automated lung CT cancer diagnosis system,\nDeepLung. DeepLung contains two parts, nodule detection and classification.\nConsidering the 3D nature of lung CT data, two 3D networks are designed for the\nnodule detection and classification respectively. Specifically, a 3D Faster\nR-CNN is designed for nodule detection with a U-net-like encoder-decoder\nstructure to effectively learn nodule features. For nodule classification,\ngradient boosting machine (GBM) with 3D dual path network (DPN) features is\nproposed. The nodule classification subnetwork is validated on a public dataset\nfrom LIDC-IDRI, on which it achieves better performance than state-of-the-art\napproaches, and surpasses the average performance of four experienced doctors.\nFor the DeepLung system, candidate nodules are detected first by the nodule\ndetection subnetwork, and nodule diagnosis is conducted by the classification\nsubnetwork. Extensive experimental results demonstrate the DeepLung is\ncomparable to the experienced doctors both for the nodule-level and\npatient-level diagnosis on the LIDC-IDRI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 16:18:22 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Zhu", "Wentao", ""], ["Liu", "Chaochun", ""], ["Fan", "Wei", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1709.05665", "submitter": "Kevis-Kokitsi Maninis", "authors": "Thomas Probst, Kevis-Kokitsi Maninis, Ajad Chhatkuli, Mouloud Ourak,\n  Emmanuel Vander Poorten, Luc Van Gool", "title": "Automatic Tool Landmark Detection for Stereo Vision in Robot-Assisted\n  Retinal Surgery", "comments": "Accepted in Robotics and Automation Letters (RA-L). Project page:\n  http://www.vision.ee.ethz.ch/~kmaninis/keypoints2stereo/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision and robotics are being increasingly applied in medical\ninterventions. Especially in interventions where extreme precision is required\nthey could make a difference. One such application is robot-assisted retinal\nmicrosurgery. In recent works, such interventions are conducted under a\nstereo-microscope, and with a robot-controlled surgical tool. The\ncomplementarity of computer vision and robotics has however not yet been fully\nexploited. In order to improve the robot control we are interested in 3D\nreconstruction of the anatomy and in automatic tool localization using a stereo\nmicroscope. In this paper, we solve this problem for the first time using a\nsingle pipeline, starting from uncalibrated cameras to reach metric 3D\nreconstruction and registration, in retinal microsurgery. The key ingredients\nof our method are: (a) surgical tool landmark detection, and (b) 3D\nreconstruction with the stereo microscope, using the detected landmarks. To\naddress the former, we propose a novel deep learning method that detects and\nrecognizes keypoints in high definition images at higher than real-time speed.\nWe use the detected 2D keypoints along with their corresponding 3D coordinates\nobtained from the robot sensors to calibrate the stereo microscope using an\naffine projection model. We design an online 3D reconstruction pipeline that\nmakes use of smoothness constraints and performs robot-to-camera registration.\nThe entire pipeline is extensively validated on open-sky porcine eye sequences.\nQuantitative and qualitative results are presented for all steps.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 14:00:26 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 17:59:08 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Probst", "Thomas", ""], ["Maninis", "Kevis-Kokitsi", ""], ["Chhatkuli", "Ajad", ""], ["Ourak", "Mouloud", ""], ["Poorten", "Emmanuel Vander", ""], ["Van Gool", "Luc", ""]]}, {"id": "1709.05669", "submitter": "Rajat Gupta", "authors": "Rajat Gupta, Kanishk Aman, Nalin Shiva, Yadvendra Singh", "title": "An Improved Fatigue Detection System Based on Behavioral Characteristics\n  of Driver", "comments": "4 pages, 2 figures, edited version of published paper in IEEE ICITE\n  2017", "journal-ref": "2017 2nd IEEE International Conference on Intelligent\n  Transportation Engineering, pp 227-230", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, road accidents have increased significantly. One of the\nmajor reasons for these accidents, as reported is driver fatigue. Due to\ncontinuous and longtime driving, the driver gets exhausted and drowsy which may\nlead to an accident. Therefore, there is a need for a system to measure the\nfatigue level of driver and alert him when he/she feels drowsy to avoid\naccidents. Thus, we propose a system which comprises of a camera installed on\nthe car dashboard. The camera detect the driver's face and observe the\nalteration in its facial features and uses these features to observe the\nfatigue level. Facial features include eyes and mouth. Principle Component\nAnalysis is thus implemented to reduce the features while minimizing the amount\nof information lost. The parameters thus obtained are processed through Support\nVector Classifier for classifying the fatigue level. After that classifier\noutput is sent to the alert unit.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 14:28:21 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Gupta", "Rajat", ""], ["Aman", "Kanishk", ""], ["Shiva", "Nalin", ""], ["Singh", "Yadvendra", ""]]}, {"id": "1709.05672", "submitter": "Taesup Moon", "authors": "Sungmin Cha, Taesup Moon", "title": "Neural Affine Grayscale Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new grayscale image denoiser, dubbed as Neural Affine Image\nDenoiser (Neural AIDE), which utilizes neural network in a novel way. Unlike\nother neural network based image denoising methods, which typically apply\nsimple supervised learning to learn a mapping from a noisy patch to a clean\npatch, we formulate to train a neural network to learn an \\emph{affine} mapping\nthat gets applied to a noisy pixel, based on its context. Our formulation\nenables both supervised training of the network from the labeled training\ndataset and adaptive fine-tuning of the network parameters using the given\nnoisy image subject to denoising. The key tool for devising Neural AIDE is to\ndevise an estimated loss function of the MSE of the affine mapping, solely\nbased on the noisy data. As a result, our algorithm can outperform most of the\nrecent state-of-the-art methods in the standard benchmark datasets. Moreover,\nour fine-tuning method can nicely overcome one of the drawbacks of the\npatch-level supervised learning methods in image denoising; namely, a\nsupervised trained model with a mismatched noise variance can be mostly\ncorrected as long as we have the matched noise variance during the fine-tuning\nstep.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 14:44:07 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Cha", "Sungmin", ""], ["Moon", "Taesup", ""]]}, {"id": "1709.05675", "submitter": "Andrey Savchenko", "authors": "Anastasiia D. Sokolova, Angelina S. Kharchevnikova, Andrey V.\n  Savchenko", "title": "Organizing Multimedia Data in Video Surveillance Systems Based on Face\n  Verification with Convolutional Neural Networks", "comments": "8 pages; 1 figure, accepted for publication at AIST17", "journal-ref": "Proceedings of the International Conference on Analysis of Images,\n  Social Networks and Texts (AIST), 2018, pp. 223-230", "doi": "10.1007/978-3-319-73013-4_20", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the two-stage approach of organizing information in\nvideo surveillance systems. At first, the faces are detected in each frame and\na video stream is split into sequences of frames with face region of one\nperson. Secondly, these sequences (tracks) that contain identical faces are\ngrouped using face verification algorithms and hierarchical agglomerative\nclustering. Gender and age are estimated for each cluster (person) in order to\nfacilitate the usage of the organized video collection. The particular\nattention is focused on the aggregation of features extracted from each frame\nwith the deep convolutional neural networks. The experimental results of the\nproposed approach using YTF and IJB-A datasets demonstrated that the most\naccurate and fast solution is achieved for matching of normalized average of\nfeature vectors of all frames in a track.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 14:57:55 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Sokolova", "Anastasiia D.", ""], ["Kharchevnikova", "Angelina S.", ""], ["Savchenko", "Andrey V.", ""]]}, {"id": "1709.05731", "submitter": "Yue Wu", "authors": "Yue Wu and Zuoguan Wang and Qiang Ji", "title": "Facial Feature Tracking under Varying Facial Expressions and Face Poses\n  based on Restricted Boltzmann Machines", "comments": "IEEE Conference on Computer Vision and Pattern Recognition, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial feature tracking is an active area in computer vision due to its\nrelevance to many applications. It is a nontrivial task, since faces may have\nvarying facial expressions, poses or occlusions. In this paper, we address this\nproblem by proposing a face shape prior model that is constructed based on the\nRestricted Boltzmann Machines (RBM) and their variants. Specifically, we first\nconstruct a model based on Deep Belief Networks to capture the face shape\nvariations due to varying facial expressions for near-frontal view. To handle\npose variations, the frontal face shape prior model is incorporated into a\n3-way RBM model that could capture the relationship between frontal face shapes\nand non-frontal face shapes. Finally, we introduce methods to systematically\ncombine the face shape prior models with image measurements of facial feature\npoints. Experiments on benchmark databases show that with the proposed method,\nfacial feature points can be tracked robustly and accurately even if faces have\nsignificant facial expressions and poses.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 00:11:25 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Wu", "Yue", ""], ["Wang", "Zuoguan", ""], ["Ji", "Qiang", ""]]}, {"id": "1709.05732", "submitter": "Yue Wu", "authors": "Yue Wu and Ziheng Wang and Qiang Ji", "title": "A Hierarchical Probabilistic Model for Facial Feature Detection", "comments": "IEEE Conference on Computer Vision and Pattern Recognition, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial feature detection from facial images has attracted great attention in\nthe field of computer vision. It is a nontrivial task since the appearance and\nshape of the face tend to change under different conditions. In this paper, we\npropose a hierarchical probabilistic model that could infer the true locations\nof facial features given the image measurements even if the face is with\nsignificant facial expression and pose. The hierarchical model implicitly\ncaptures the lower level shape variations of facial components using the\nmixture model. Furthermore, in the higher level, it also learns the joint\nrelationship among facial components, the facial expression, and the pose\ninformation through automatic structure learning and parameter estimation of\nthe probabilistic model. Experimental results on benchmark databases\ndemonstrate the effectiveness of the proposed hierarchical probabilistic model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 00:20:20 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Wu", "Yue", ""], ["Wang", "Ziheng", ""], ["Ji", "Qiang", ""]]}, {"id": "1709.05745", "submitter": "Haesol Park", "authors": "Haesol Park and Kyoung Mu Lee", "title": "Joint Estimation of Camera Pose, Depth, Deblurring, and Super-Resolution\n  from a Blurred Image Sequence", "comments": "accepted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional methods for estimating camera poses and scene structures\nfrom severely blurry or low resolution images often result in failure. The\noff-the-shelf deblurring or super-resolution methods may show visually pleasing\nresults. However, applying each technique independently before matching is\ngenerally unprofitable because this naive series of procedures ignores the\nconsistency between images. In this paper, we propose a pioneering unified\nframework that solves four problems simultaneously, namely, dense depth\nreconstruction, camera pose estimation, super-resolution, and deblurring. By\nreflecting a physical imaging process, we formulate a cost minimization problem\nand solve it using an alternating optimization technique. The experimental\nresults on both synthetic and real videos show high-quality depth maps derived\nfrom severely degraded images that contrast the failures of naive multi-view\nstereo methods. Our proposed method also produces outstanding deblurred and\nsuper-resolved images unlike the independent application or combination of\nconventional video deblurring, super-resolution methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 02:24:31 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Park", "Haesol", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1709.05746", "submitter": "Fangyi Zhang", "authors": "Fangyi Zhang, J\\\"urgen Leitner, Zongyuan Ge, Michael Milford, Peter\n  Corke", "title": "Adversarial Discriminative Sim-to-real Transfer of Visuo-motor Policies", "comments": "Under review for the International Journal of Robotics Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various approaches have been proposed to learn visuo-motor policies for\nreal-world robotic applications. One solution is first learning in simulation\nthen transferring to the real world. In the transfer, most existing approaches\nneed real-world images with labels. However, the labelling process is often\nexpensive or even impractical in many robotic applications. In this paper, we\npropose an adversarial discriminative sim-to-real transfer approach to reduce\nthe cost of labelling real data. The effectiveness of the approach is\ndemonstrated with modular networks in a table-top object reaching task where a\n7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter\nthrough visual observations. The adversarial transfer approach reduced the\nlabelled real data requirement by 50%. Policies can be transferred to real\nenvironments with only 93 labelled and 186 unlabelled real images. The\ntransferred visuo-motor policies are robust to novel (not seen in training)\nobjects in clutter and even a moving target, achieving a 97.8% success rate and\n1.8 cm control accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 02:27:02 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 09:38:25 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Zhang", "Fangyi", ""], ["Leitner", "J\u00fcrgen", ""], ["Ge", "Zongyuan", ""], ["Milford", "Michael", ""], ["Corke", "Peter", ""]]}, {"id": "1709.05769", "submitter": "Yang Wang", "authors": "Lin Wu, Yang Wang", "title": "Where to Focus: Deep Attention-based Spatially Recurrent Bilinear\n  Networks for Fine-Grained Visual Recognition", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual recognition typically depends on modeling subtle\ndifference from object parts. However, these parts often exhibit dramatic\nvisual variations such as occlusions, viewpoints, and spatial transformations,\nmaking it hard to detect. In this paper, we present a novel attention-based\nmodel to automatically, selectively and accurately focus on critical object\nregions with higher importance against appearance variations. Given an image,\ntwo different Convolutional Neural Networks (CNNs) are constructed, where the\noutputs of two CNNs are correlated through bilinear pooling to simultaneously\nfocus on discriminative regions and extract relevant features. To capture\nspatial distributions among the local regions with visual attention, soft\nattention based spatial Long-Short Term Memory units (LSTMs) are incorporated\nto realize spatially recurrent yet visually selective over local input\npatterns. All the above intuitions equip our network with the following novel\nmodel: two-stream CNN layers, bilinear pooling layer, spatial recurrent layer\nwith location attention are jointly trained via an end-to-end fashion to serve\nas the part detector and feature extractor, whereby relevant features are\nlocalized and extracted attentively. We show the significance of our network\nagainst two well-known visual recognition tasks: fine-grained image\nclassification and person re-identification.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 03:56:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Wu", "Lin", ""], ["Wang", "Yang", ""]]}, {"id": "1709.05774", "submitter": "Julian Straub", "authors": "Julian Straub, Randi Cabezas, John Leonard, John W. Fisher III", "title": "Direction-Aware Semi-Dense SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To aide simultaneous localization and mapping (SLAM), future perception\nsystems will incorporate forms of scene understanding. In a step towards fully\nintegrated probabilistic geometric scene understanding, localization and\nmapping we propose the first direction-aware semi-dense SLAM system. It jointly\ninfers the directional Stata Center World (SCW) segmentation and a surfel-based\nsemi-dense map while performing real-time camera tracking. The joint SCW map\nmodel connects a scene-wide Bayesian nonparametric Dirichlet Process\nvon-Mises-Fisher mixture model (DP-vMF) prior on surfel orientations with the\nlocal surfel locations via a conditional random field (CRF). Camera tracking\nleverages the SCW segmentation to improve efficiency via guided observation\nselection. Results demonstrate improved SLAM accuracy and tracking efficiency\nat state of the art performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 04:49:40 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Straub", "Julian", ""], ["Cabezas", "Randi", ""], ["Leonard", "John", ""], ["Fisher", "John W.", "III"]]}, {"id": "1709.05775", "submitter": "Cristian Canton Ferrer", "authors": "Maedeh Aghaei, Mariella Dimiccoli, Cristian Canton Ferrer, Petia\n  Radeva", "title": "Social Style Characterization from Egocentric Photo-streams", "comments": "International Conference on Computer Vision (ICCV). Workshop on\n  Egocentric Percetion, Interaction and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a system for automatic social pattern characterization\nusing a wearable photo-camera. The proposed pipeline consists of three major\nsteps. First, detection of people with whom the camera wearer interacts and,\nsecond, categorization of the detected social interactions into formal and\ninformal. These two steps act at event-level where each potential social event\nis modeled as a multi-dimensional time-series, whose dimensions correspond to a\nset of relevant features for each task, and a LSTM network is employed for\ntime-series classification. In the last step, recurrences of the same person\nacross the whole set of social interactions are clustered to achieve a\ncomprehensive understanding of the diversity and frequency of the social\nrelations of the user. Experiments over a dataset acquired by a user wearing a\nphoto-camera during a month show promising results on the task of social\npattern characterization from egocentric photo-streams.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 04:50:30 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Aghaei", "Maedeh", ""], ["Dimiccoli", "Mariella", ""], ["Ferrer", "Cristian Canton", ""], ["Radeva", "Petia", ""]]}, {"id": "1709.05788", "submitter": "Sanghyun Woo", "authors": "Sanghyun Woo, Soonmin Hwang, In So Kweon", "title": "StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-stage object detectors such as SSD or YOLO already have shown promising\naccuracy with small memory footprint and fast speed. However, it is widely\nrecognized that one-stage detectors have difficulty in detecting small objects\nwhile they are competitive with two-stage methods on large objects. In this\npaper, we investigate how to alleviate this problem starting from the SSD\nframework. Due to their pyramidal design, the lower layer that is responsible\nfor small objects lacks strong semantics(e.g contextual information). We\naddress this problem by introducing a feature combining module that spreads out\nthe strong semantics in a top-down manner. Our final model StairNet detector\nunifies the multi-scale representations and semantic distribution effectively.\nExperiments on PASCAL VOC 2007 and PASCAL VOC 2012 datasets demonstrate that\nStairNet significantly improves the weakness of SSD and outperforms the other\nstate-of-the-art one-stage detectors.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 06:38:10 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Woo", "Sanghyun", ""], ["Hwang", "Soonmin", ""], ["Kweon", "In So", ""]]}, {"id": "1709.05815", "submitter": "Darius Burschka", "authors": "Darius Burschka and Elmar Mair", "title": "Direct Pose Estimation with a Monocular Camera", "comments": "Robot Vision 2008, Auckland, New Zealand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a direct method to calculate a 6DoF pose change of a monocular\ncamera for mobile navigation. The calculated pose is estimated up to a constant\nunknown scale parameter that is kept constant over the entire reconstruction\nprocess. This method allows a direct cal- culation of the metric position and\nrotation without any necessity to fuse the information in a probabilistic\napproach over longer frame sequence as it is the case in most currently used\nVSLAM approaches. The algorithm provides two novel aspects to the field of\nmonocular navigation. It allows a direct pose estimation without any a-priori\nknowledge about the world directly from any two images and it provides a\nquality measure for the estimated motion parameters that allows to fuse the\nresulting information in Kalman Filters. We present the mathematical\nformulation of the approach together with experimental validation on real scene\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 08:37:05 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Burschka", "Darius", ""], ["Mair", "Elmar", ""]]}, {"id": "1709.05833", "submitter": "Lei Han", "authors": "Lei Han, Guyue Zhou, Lan Xu, Lu Fang", "title": "Beyond SIFT using Binary features for Loop Closure Detection", "comments": "IROS 2017 paper for loop closure detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a binary feature based Loop Closure Detection (LCD) method is\nproposed, which for the first time achieves higher precision-recall (PR)\nperformance compared with state-of-the-art SIFT feature based approaches. The\nproposed system originates from our previous work Multi-Index hashing for Loop\nclosure Detection (MILD), which employs Multi-Index Hashing\n(MIH)~\\cite{greene1994multi} for Approximate Nearest Neighbor (ANN) search of\nbinary features. As the accuracy of MILD is limited by repeating textures and\ninaccurate image similarity measurement, burstiness handling is introduced to\nsolve this problem and achieves considerable accuracy improvement.\nAdditionally, a comprehensive theoretical analysis on MIH used in MILD is\nconducted to further explore the potentials of hashing methods for ANN search\nof binary features from probabilistic perspective. This analysis provides more\nfreedom on best parameter choosing in MIH for different application scenarios.\nExperiments on popular public datasets show that the proposed approach achieved\nthe highest accuracy compared with state-of-the-art while running at 30Hz for\ndatabases containing thousands of images.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 09:36:13 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Han", "Lei", ""], ["Zhou", "Guyue", ""], ["Xu", "Lan", ""], ["Fang", "Lu", ""]]}, {"id": "1709.05860", "submitter": "Assaf Arbelle", "authors": "Assaf Arbelle and Tammy Riklin Raviv", "title": "Microscopy Cell Segmentation via Adversarial Neural Networks", "comments": "Accepted to IEEE International Symposium on Biomedical Imaging (ISBI)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for cell segmentation in microscopy images which is\ninspired by the Generative Adversarial Neural Network (GAN) approach. Our\nframework is built on a pair of two competitive artificial neural networks,\nwith a unique architecture, termed Rib Cage, which are trained simultaneously\nand together define a min-max game resulting in an accurate segmentation of a\ngiven image. Our approach has two main strengths, similar to the GAN, the\nmethod does not require a formulation of a loss function for the optimization\nprocess. This allows training on a limited amount of annotated data in a weakly\nsupervised manner. Promising segmentation results on real fluorescent\nmicroscopy data are presented. The code is freely available at:\nhttps://github.com/arbellea/DeepCellSeg.git\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 10:55:31 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 20:16:56 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 11:20:53 GMT"}, {"version": "v4", "created": "Thu, 13 Sep 2018 05:59:52 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Arbelle", "Assaf", ""], ["Raviv", "Tammy Riklin", ""]]}, {"id": "1709.05861", "submitter": "Narotam Singh", "authors": "Narotam Singh (1), Nittin Singh (1), Abhinav Dhall (1) ((1) Indian\n  Institute of Technology Ropar)", "title": "Continuous Multimodal Emotion Recognition Approach for AVEC 2017", "comments": "4 pages, 3 figures, arXiv:1605.06778, arXiv:1512.03385", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports the analysis of audio and visual features in predicting\nthe continuous emotion dimensions under the seventh Audio/Visual Emotion\nChallenge (AVEC 2017), which was done as part of a B.Tech. 2nd year internship\nproject. For visual features we used the HOG (Histogram of Gradients) features,\nFisher encodings of SIFT (Scale-Invariant Feature Transform) features based on\nGaussian mixture model (GMM) and some pretrained Convolutional Neural Network\nlayers as features; all these extracted for each video clip. For audio features\nwe used the Bag-of-audio-words (BoAW) representation of the LLDs (low-level\ndescriptors) generated by openXBOW provided by the organisers of the event.\nThen we trained fully connected neural network regression model on the dataset\nfor all these different modalities. We applied multimodal fusion on the output\nmodels to get the Concordance correlation coefficient on Development set as\nwell as Test set.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:01:43 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 12:08:09 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Singh", "Narotam", ""], ["Singh", "Nittin", ""], ["Dhall", "Abhinav", ""]]}, {"id": "1709.05862", "submitter": "Mohammad Reza Loghmani", "authors": "Mohammad Reza Loghmani and Barbara Caputo and Markus Vincze", "title": "Recognizing Objects In-the-wild: Where Do We Stand?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to recognize objects is an essential skill for a robotic system\nacting in human-populated environments. Despite decades of effort from the\nrobotic and vision research communities, robots are still missing good visual\nperceptual systems, preventing the use of autonomous agents for real-world\napplications. The progress is slowed down by the lack of a testbed able to\naccurately represent the world perceived by the robot in-the-wild. In order to\nfill this gap, we introduce a large-scale, multi-view object dataset collected\nwith an RGB-D camera mounted on a mobile robot. The dataset embeds the\nchallenges faced by a robot in a real-life application and provides a useful\ntool for validating object recognition algorithms. Besides describing the\ncharacteristics of the dataset, the paper evaluates the performance of a\ncollection of well-established deep convolutional networks on the new dataset\nand analyzes the transferability of deep representations from Web images to\nrobotic data. Despite the promising results obtained with such representations,\nthe experiments demonstrate that object classification with real-life robotic\ndata is far from being solved. Finally, we provide a comparative study to\nanalyze and highlight the open challenges in robot vision, explaining the\ndiscrepancies in the performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:11:31 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 11:55:27 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Loghmani", "Mohammad Reza", ""], ["Caputo", "Barbara", ""], ["Vincze", "Markus", ""]]}, {"id": "1709.05865", "submitter": "Shubham Dham", "authors": "Shubham Dham, Anirudh Sharma, Abhinav Dhall", "title": "Depression Scale Recognition from Audio, Visual and Text Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depression is a major mental health disorder that is rapidly affecting lives\nworldwide. Depression not only impacts emotional but also physical and\npsychological state of the person. Its symptoms include lack of interest in\ndaily activities, feeling low, anxiety, frustration, loss of weight and even\nfeeling of self-hatred. This report describes work done by us for Audio Visual\nEmotion Challenge (AVEC) 2017 during our second year BTech summer internship.\nWith the increase in demand to detect depression automatically with the help of\nmachine learning algorithms, we present our multimodal feature extraction and\ndecision level fusion approach for the same. Features are extracted by\nprocessing on the provided Distress Analysis Interview Corpus-Wizard of Oz\n(DAIC-WOZ) database. Gaussian Mixture Model (GMM) clustering and Fisher vector\napproach were applied on the visual data; statistical descriptors on gaze,\npose; low level audio features and head pose and text features were also\nextracted. Classification is done on fused as well as independent features\nusing Support Vector Machine (SVM) and neural networks. The results obtained\nwere able to cross the provided baseline on validation data set by 17% on audio\nfeatures and 24.5% on video features.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:26:01 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Dham", "Shubham", ""], ["Sharma", "Anirudh", ""], ["Dhall", "Abhinav", ""]]}, {"id": "1709.05867", "submitter": "Ninad Joshi", "authors": "N. Joshi", "title": "Combinational neural network using Gabor filters for the classification\n  of handwritten digits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classification algorithm that combines the components of k-nearest\nneighbours and multilayer neural networks has been designed and tested. With\nthis method the computational time required for training the dataset has been\nreduced substancially. Gabor filters were used for the feature extraction to\nensure a better performance. This algorithm is tested with MNIST dataset and it\nwill be integrated as a module in the object recognition software which is\ncurrently under development.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:28:15 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Joshi", "N.", ""]]}, {"id": "1709.05903", "submitter": "Xiaobin Liu", "authors": "Xiaobin Liu, Shiliang Zhang, Tiejun Huang, Qi Tian", "title": "E$^2$BoWs: An End-to-End Bag-of-Words Model via Deep Convolutional\n  Neural Network", "comments": "8 pages, ChinaMM 2017, image retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Bag-of-visual Words (BoWs) model is commonly generated with many\nsteps including local feature extraction, codebook generation, and feature\nquantization, etc. Those steps are relatively independent with each other and\nare hard to be jointly optimized. Moreover, the dependency on hand-crafted\nlocal feature makes BoWs model not effective in conveying high-level semantics.\nThese issues largely hinder the performance of BoWs model in large-scale image\napplications. To conquer these issues, we propose an End-to-End BoWs\n(E$^2$BoWs) model based on Deep Convolutional Neural Network (DCNN). Our model\ntakes an image as input, then identifies and separates the semantic objects in\nit, and finally outputs the visual words with high semantic discriminative\npower. Specifically, our model firstly generates Semantic Feature Maps (SFMs)\ncorresponding to different object categories through convolutional layers, then\nintroduces Bag-of-Words Layers (BoWL) to generate visual words for each\nindividual feature map. We also introduce a novel learning algorithm to\nreinforce the sparsity of the generated E$^2$BoWs model, which further ensures\nthe time and memory efficiency. We evaluate the proposed E$^2$BoWs model on\nseveral image search datasets including CIFAR-10, CIFAR-100, MIRFLICKR-25K and\nNUS-WIDE. Experimental results show that our method achieves promising accuracy\nand efficiency compared with recent deep learning based retrieval works.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 13:00:30 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 02:57:01 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Liu", "Xiaobin", ""], ["Zhang", "Shiliang", ""], ["Huang", "Tiejun", ""], ["Tian", "Qi", ""]]}, {"id": "1709.05910", "submitter": "Christoph Reinders", "authors": "Christoph Reinders and Hanno Ackermann and Michael Ying Yang and Bodo\n  Rosenhahn", "title": "Object Recognition from very few Training Examples for Enhancing Bicycle\n  Maps", "comments": "Submitted to IV 2018. This research was supported by German Research\n  Foundation DFG within Priority Research Programme 1894 \"Volunteered\n  Geographic Information: Interpretation, Visualization and Social Computing\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, data-driven methods have shown great success for extracting\ninformation about the infrastructure in urban areas. These algorithms are\nusually trained on large datasets consisting of thousands or millions of\nlabeled training examples. While large datasets have been published regarding\ncars, for cyclists very few labeled data is available although appearance,\npoint of view, and positioning of even relevant objects differ. Unfortunately,\nlabeling data is costly and requires a huge amount of work. In this paper, we\nthus address the problem of learning with very few labels. The aim is to\nrecognize particular traffic signs in crowdsourced data to collect information\nwhich is of interest to cyclists. We propose a system for object recognition\nthat is trained with only 15 examples per class on average. To achieve this, we\ncombine the advantages of convolutional neural networks and random forests to\nlearn a patch-wise classifier. In the next step, we map the random forest to a\nneural network and transform the classifier to a fully convolutional network.\nThereby, the processing of full images is significantly accelerated and\nbounding boxes can be predicted. Finally, we integrate data of the Global\nPositioning System (GPS) to localize the predictions on the map. In comparison\nto Faster R-CNN and other networks for object recognition or algorithms for\ntransfer learning, we considerably reduce the required amount of labeled data.\nWe demonstrate good performance on the recognition of traffic signs for\ncyclists as well as their localization in maps.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 13:21:46 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 13:20:10 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 21:45:35 GMT"}, {"version": "v4", "created": "Mon, 28 May 2018 09:41:06 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Reinders", "Christoph", ""], ["Ackermann", "Hanno", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1709.05929", "submitter": "Ken Chang", "authors": "Ken Chang, Niranjan Balachandar, Carson K Lam, Darvin Yi, James M\n  Brown, Andrew Beers, Bruce R Rosen, Daniel L Rubin, Jayashree Kalpathy-Cramer", "title": "Institutionally Distributed Deep Learning Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become a promising approach for automated medical\ndiagnoses. When medical data samples are limited, collaboration among multiple\ninstitutions is necessary to achieve high algorithm performance. However,\nsharing patient data often has limitations due to technical, legal, or ethical\nconcerns. In such cases, sharing a deep learning model is a more attractive\nalternative. The best method of performing such a task is unclear, however. In\nthis study, we simulate the dissemination of learning deep learning network\nmodels across four institutions using various heuristics and compare the\nresults with a deep learning model trained on centrally hosted patient data.\nThe heuristics investigated include ensembling single institution models,\nsingle weight transfer, and cyclical weight transfer. We evaluated these\napproaches for image classification in three independent image collections\n(retinal fundus photos, mammography, and ImageNet). We find that cyclical\nweight transfer resulted in a performance (testing accuracy = 77.3%) that was\nclosest to that of centrally hosted patient data (testing accuracy = 78.7%). We\nalso found that there is an improvement in the performance of cyclical weight\ntransfer heuristic with high frequency of weight transfer.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 15:36:17 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Chang", "Ken", ""], ["Balachandar", "Niranjan", ""], ["Lam", "Carson K", ""], ["Yi", "Darvin", ""], ["Brown", "James M", ""], ["Beers", "Andrew", ""], ["Rosen", "Bruce R", ""], ["Rubin", "Daniel L", ""], ["Kalpathy-Cramer", "Jayashree", ""]]}, {"id": "1709.05932", "submitter": "Benjamin Bischke", "authors": "Benjamin Bischke, Patrick Helber, Joachim Folz, Damian Borth, Andreas\n  Dengel", "title": "Multi-Task Learning for Segmentation of Building Footprints with Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased availability of high resolution satellite imagery allows to\nsense very detailed structures on the surface of our planet. Access to such\ninformation opens up new directions in the analysis of remote sensing imagery.\nHowever, at the same time this raises a set of new challenges for existing\npixel-based prediction methods, such as semantic segmentation approaches. While\ndeep neural networks have achieved significant advances in the semantic\nsegmentation of high resolution images in the past, most of the existing\napproaches tend to produce predictions with poor boundaries. In this paper, we\naddress the problem of preserving semantic segmentation boundaries in high\nresolution satellite imagery by introducing a new cascaded multi-task loss. We\nevaluate our approach on Inria Aerial Image Labeling Dataset which contains\nlarge-scale and high resolution images. Our results show that we are able to\noutperform state-of-the-art methods by 8.3\\% without any additional\npost-processing step.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 13:47:45 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bischke", "Benjamin", ""], ["Helber", "Patrick", ""], ["Folz", "Joachim", ""], ["Borth", "Damian", ""], ["Dengel", "Andreas", ""]]}, {"id": "1709.05936", "submitter": "Chen Wang", "authors": "Chen Wang, Le Zhang, Lihua Xie, Junsong Yuan", "title": "Kernel Cross-Correlator", "comments": "The Thirty-Second AAAI Conference on Artificial Intelligence\n  (AAAI-18)", "journal-ref": "The Thirty-Second AAAI Conference on Artificial Intelligence\n  (AAAI-2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-correlator plays a significant role in many visual perception tasks,\nsuch as object detection and tracking. Beyond the linear cross-correlator, this\npaper proposes a kernel cross-correlator (KCC) that breaks traditional\nlimitations. First, by introducing the kernel trick, the KCC extends the linear\ncross-correlation to non-linear space, which is more robust to signal noises\nand distortions. Second, the connection to the existing works shows that KCC\nprovides a unified solution for correlation filters. Third, KCC is applicable\nto any kernel function and is not limited to circulant structure on training\ndata, thus it is able to predict affine transformations with customized\nproperties. Last, by leveraging the fast Fourier transform (FFT), KCC\neliminates direct calculation of kernel vectors, thus achieves better\nperformance yet still with a reasonable computational cost. Comprehensive\nexperiments on visual tracking and human activity recognition using wearable\ndevices demonstrate its robustness, flexibility, and efficiency. The source\ncodes of both experiments are released at https://github.com/wang-chen/KCC\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 15:09:29 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 05:05:19 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 07:28:08 GMT"}, {"version": "v4", "created": "Mon, 26 Feb 2018 08:45:40 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Wang", "Chen", ""], ["Zhang", "Le", ""], ["Xie", "Lihua", ""], ["Yuan", "Junsong", ""]]}, {"id": "1709.05937", "submitter": "Yuan Liu", "authors": "Yuan Liu, St\\'ephane Canu, Paul Honeine, Su Ruan", "title": "Une v\\'eritable approche $\\ell_0$ pour l'apprentissage de dictionnaire", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation learning has recently gained a great success in signal\nand image processing, thanks to recent advances in dictionary learning. To this\nend, the $\\ell_0$-norm is often used to control the sparsity level.\nNevertheless, optimization problems based on the $\\ell_0$-norm are non-convex\nand NP-hard. For these reasons, relaxation techniques have been attracting much\nattention of researchers, by priorly targeting approximation solutions (e.g.\n$\\ell_1$-norm, pursuit strategies). On the contrary, this paper considers the\nexact $\\ell_0$-norm optimization problem and proves that it can be solved\neffectively, despite of its complexity. The proposed method reformulates the\nproblem as a Mixed-Integer Quadratic Program (MIQP) and gets the global optimal\nsolution by applying existing optimization software. Because the main\ndifficulty of this approach is its computational time, two techniques are\nintroduced that improve the computational speed. Finally, our method is applied\nto image denoising which shows its feasibility and relevance compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 13:21:47 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Liu", "Yuan", ""], ["Canu", "St\u00e9phane", ""], ["Honeine", "Paul", ""], ["Ruan", "Su", ""]]}, {"id": "1709.05939", "submitter": "Nancy Xin Ru Wang", "authors": "Nancy Xin Ru Wang, Ali Farhadi, Rajesh Rao, Bingni Brunton", "title": "AJILE Movement Prediction: Multimodal Deep Learning for Natural Human\n  Neural Recordings and Video", "comments": null, "journal-ref": "Thirty-Second AAAI Conference On Artificial Intelligence (2018)", "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing useful interfaces between brains and machines is a grand challenge\nof neuroengineering. An effective interface has the capacity to not only\ninterpret neural signals, but predict the intentions of the human to perform an\naction in the near future; prediction is made even more challenging outside\nwell-controlled laboratory experiments. This paper describes our approach to\ndetect and to predict natural human arm movements in the future, a key\nchallenge in brain computer interfacing that has never before been attempted.\nWe introduce the novel Annotated Joints in Long-term ECoG (AJILE) dataset;\nAJILE includes automatically annotated poses of 7 upper body joints for four\nhuman subjects over 670 total hours (more than 72 million frames), along with\nthe corresponding simultaneously acquired intracranial neural recordings. The\nsize and scope of AJILE greatly exceeds all previous datasets with movements\nand electrocorticography (ECoG), making it possible to take a deep learning\napproach to movement prediction. We propose a multimodal model that combines\ndeep convolutional neural networks (CNN) with long short-term memory (LSTM)\nblocks, leveraging both ECoG and video modalities. We demonstrate that our\nmodels are able to detect movements and predict future movements up to 800 msec\nbefore movement initiation. Further, our multimodal movement prediction models\nexhibit resilience to simulated ablation of input neural signals. We believe a\nmultimodal approach to natural neural decoding that takes context into account\nis critical in advancing bioelectronic technologies and human neuroscience.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 01:28:44 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 22:40:58 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wang", "Nancy Xin Ru", ""], ["Farhadi", "Ali", ""], ["Rao", "Rajesh", ""], ["Brunton", "Bingni", ""]]}, {"id": "1709.05940", "submitter": "Yvain Qu\\'eau", "authors": "Yvain Qu\\'eau, Jean-Denis Durou and Jean-Fran\\c{c}ois Aujol", "title": "Normal Integration: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for efficient normal integration methods is driven by several\ncomputer vision tasks such as shape-from-shading, photometric stereo,\ndeflectometry, etc. In the first part of this survey, we select the most\nimportant properties that one may expect from a normal integration method,\nbased on a thorough study of two pioneering works by Horn and Brooks [28] and\nby Frankot and Chellappa [19]. Apart from accuracy, an integration method\nshould at least be fast and robust to a noisy normal field. In addition, it\nshould be able to handle several types of boundary condition, including the\ncase of a free boundary, and a reconstruction domain of any shape i.e., which\nis not necessarily rectangular. It is also much appreciated that a minimum\nnumber of parameters have to be tuned, or even no parameter at all. Finally, it\nshould preserve the depth discontinuities. In the second part of this survey,\nwe review most of the existing methods in view of this analysis, and conclude\nthat none of them satisfies all of the required properties. This work is\ncomplemented by a companion paper entitled Variational Methods for Normal\nIntegration, in which we focus on the problem of normal integration in the\npresence of depth discontinuities, a problem which occurs as soon as there are\nocclusions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 13:56:11 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Qu\u00e9au", "Yvain", ""], ["Durou", "Jean-Denis", ""], ["Aujol", "Jean-Fran\u00e7ois", ""]]}, {"id": "1709.05943", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Brendan Chywl, Francis Li, and Alexander Wong", "title": "Fast YOLO: A Fast You Only Look Once System for Real-time Embedded\n  Object Detection in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is considered one of the most challenging problems in this\nfield of computer vision, as it involves the combination of object\nclassification and object localization within a scene. Recently, deep neural\nnetworks (DNNs) have been demonstrated to achieve superior object detection\nperformance compared to other approaches, with YOLOv2 (an improved You Only\nLook Once model) being one of the state-of-the-art in DNN-based object\ndetection methods in terms of both speed and accuracy. Although YOLOv2 can\nachieve real-time performance on a powerful GPU, it still remains very\nchallenging for leveraging this approach for real-time object detection in\nvideo on embedded computing devices with limited computational power and\nlimited memory. In this paper, we propose a new framework called Fast YOLO, a\nfast You Only Look Once framework which accelerates YOLOv2 to be able to\nperform object detection in video on embedded devices in a real-time manner.\nFirst, we leverage the evolutionary deep intelligence framework to evolve the\nYOLOv2 network architecture and produce an optimized architecture (referred to\nas O-YOLOv2 here) that has 2.8X fewer parameters with just a ~2% IOU drop. To\nfurther reduce power consumption on embedded devices while maintaining\nperformance, a motion-adaptive inference method is introduced into the proposed\nFast YOLO framework to reduce the frequency of deep inference with O-YOLOv2\nbased on temporal motion characteristics. Experimental results show that the\nproposed Fast YOLO framework can reduce the number of deep inferences by an\naverage of 38.13%, and an average speedup of ~3.3X for objection detection in\nvideo compared to the original YOLOv2, leading Fast YOLO to run an average of\n~18FPS on a Nvidia Jetson TX1 embedded system.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 13:57:16 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Chywl", "Brendan", ""], ["Li", "Francis", ""], ["Wong", "Alexander", ""]]}, {"id": "1709.05945", "submitter": "Bartolomeo Della Corte", "authors": "Bartolomeo Della Corte, Igor Bogoslavskyi, Cyrill Stachniss, Giorgio\n  Grisetti", "title": "A General Framework for Flexible Multi-Cue Photometric Point Cloud\n  Registration", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to build maps is a key functionality for the majority of mobile\nrobots. A central ingredient to most mapping systems is the registration or\nalignment of the recorded sensor data. In this paper, we present a general\nmethodology for photometric registration that can deal with multiple different\ncues. We provide examples for registering RGBD as well as 3D LIDAR data. In\ncontrast to popular point cloud registration approaches such as ICP our method\ndoes not rely on explicit data association and exploits multiple modalities\nsuch as raw range and image data streams. Color, depth, and normal information\nare handled in an uniform manner and the registration is obtained by minimizing\nthe pixel-wise difference between two multi-channel images. We developed a\nflexible and general framework and implemented our approach inside that\nframework. We also released our implementation as open source C++ code. The\nexperiments show that our approach allows for an accurate registration of the\nsensor data without requiring an explicit data association or model-specific\nadaptations to datasets or sensors. Our approach exploits the different cues in\na natural and consistent way and the registration can be done at framerate for\na typical range or imaging sensor.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 12:36:33 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Della Corte", "Bartolomeo", ""], ["Bogoslavskyi", "Igor", ""], ["Stachniss", "Cyrill", ""], ["Grisetti", "Giorgio", ""]]}, {"id": "1709.05952", "submitter": "Sultan Khan Daud", "authors": "Sultan Daud Khan, Muhammad Tayyab, Muhammad Khurram Amin, Akram Nour,\n  Anas Basalamah, Saleh Basalamah, Sohaib Ahmad Khan", "title": "Towards a Crowd Analytic Framework For Crowd Management in\n  Majid-al-Haram", "comments": "17th Scientific Meeting on Hajj & Umrah Research, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scared cities of Makkah Al Mukarramah and Madina Al Munawarah host\nmillions of pilgrims every year. During Hajj, the movement of large number of\npeople has a unique spatial and temporal constraints, which makes Hajj one of\ntoughest challenges for crowd management. In this paper, we propose a computer\nvision based framework that automatically analyses video sequence and computes\nimportant measurements which include estimation of crowd density,\nidentification of dominant patterns, detection and localization of congestion.\nIn addition, we analyze helpful statistics of the crowd like speed, and\ndirection, that could provide support to crowd management personnel. The\nframework presented in this paper indicate that new advances in computer vision\nand machine learning can be leveraged effectively for challenging and high\ndensity crowd management applications. However, significant customization of\nexisting approaches is required to apply them to the challenging crowd\nmanagement situations in Masjid Al Haram. Our results paint a promising picture\nfor deployment of computer vision technologies to assist in quantitative\nmeasurement of crowd size, density and congestion.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 11:26:25 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Khan", "Sultan Daud", ""], ["Tayyab", "Muhammad", ""], ["Amin", "Muhammad Khurram", ""], ["Nour", "Akram", ""], ["Basalamah", "Anas", ""], ["Basalamah", "Saleh", ""], ["Khan", "Sohaib Ahmad", ""]]}, {"id": "1709.05956", "submitter": "Nastaran Mohammadian Rad", "authors": "Nastaran Mohammadian Rad, Seyed Mostafa Kia, Calogero Zarbo, Twan van\n  Laarhoven, Giuseppe Jurman, Paola Venuti, Elena Marchiori, Cesare Furlanello", "title": "Deep Learning for Automatic Stereotypical Motor Movement Detection using\n  Wearable Sensors in Autism Spectrum Disorders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism Spectrum Disorders are associated with atypical movements, of which\nstereotypical motor movements (SMMs) interfere with learning and social\ninteraction. The automatic SMM detection using inertial measurement units (IMU)\nremains complex due to the strong intra and inter-subject variability,\nespecially when handcrafted features are extracted from the signal. We propose\na new application of the deep learning to facilitate automatic SMM detection\nusing multi-axis IMUs. We use a convolutional neural network (CNN) to learn a\ndiscriminative feature space from raw data. We show how the CNN can be used for\nparameter transfer learning to enhance the detection rate on longitudinal data.\nWe also combine the long short-term memory (LSTM) with CNN to model the\ntemporal patterns in a sequence of multi-axis signals. Further, we employ\nensemble learning to combine multiple LSTM learners into a more robust SMM\ndetector. Our results show that: 1) feature learning outperforms handcrafted\nfeatures; 2) parameter transfer learning is beneficial in longitudinal\nsettings; 3) using LSTM to learn the temporal dynamic of signals enhances the\ndetection rate especially for skewed training data; 4) an ensemble of LSTMs\nprovides more accurate and stable detectors. These findings provide a\nsignificant step toward accurate SMM detection in real-time scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 20:41:45 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Rad", "Nastaran Mohammadian", ""], ["Kia", "Seyed Mostafa", ""], ["Zarbo", "Calogero", ""], ["van Laarhoven", "Twan", ""], ["Jurman", "Giuseppe", ""], ["Venuti", "Paola", ""], ["Marchiori", "Elena", ""], ["Furlanello", "Cesare", ""]]}, {"id": "1709.05961", "submitter": "Dai Huidong Dr.", "authors": "Huidong Dai, Weiji He, Guohua Gu, Ling Ye, Tianyi Mao, and Qian Chen", "title": "Adaptive compressed 3D imaging based on wavelet trees and Hadamard\n  multiplexing with a single photon counting detector", "comments": "11 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photon counting 3D imaging allows to obtain 3D images with single-photon\nsensitivity and sub-ns temporal resolution. However, it is challenging to scale\nto high spatial resolution. In this work, we demonstrate a photon counting 3D\nimaging technique with short-pulsed structured illumination and a single-pixel\nphoton counting detector. The proposed multi-resolution photon counting 3D\nimaging technique acquires a high-resolution 3D image from a coarse image and\nedges at successfully finer resolution sampled by Hadamard multiplexing along\nthe wavelet trees. The detected power is significantly increased thanks to the\nHadamard multiplexing. Both the required measurements and the reconstruction\ntime can be significantly reduced by performing wavelet-tree-based regions of\nedges predication and Hadamard demultiplexing, which makes the proposed\ntechnique suitable for scenes with high spatial resolution. The experimental\nresults indicate that a 3D image at resolution up to 512*512 pixels can be\nacquired and retrieved with practical time as low as 17 seconds.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 02:23:36 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Dai", "Huidong", ""], ["He", "Weiji", ""], ["Gu", "Guohua", ""], ["Ye", "Ling", ""], ["Mao", "Tianyi", ""], ["Chen", "Qian", ""]]}, {"id": "1709.05965", "submitter": "Yvain Qu\\'eau", "authors": "Yvain Qu\\'eau, Jean-Denis Durou and Jean-Fran\\c{c}ois Aujol", "title": "Variational Methods for Normal Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for an efficient method of integration of a dense normal field is\ninspired by several computer vision tasks, such as shape-from-shading,\nphotometric stereo, deflectometry, etc. Inspired by edge-preserving methods\nfrom image processing, we study in this paper several variational approaches\nfor normal integration, with a focus on non-rectangular domains, free boundary\nand depth discontinuities. We first introduce a new discretization for\nquadratic integration, which is designed to ensure both fast recovery and the\nability to handle non-rectangular domains with a free boundary. Yet, with this\nsolver, discontinuous surfaces can be handled only if the scene is first\nsegmented into pieces without discontinuity. Hence, we then discuss several\ndiscontinuity-preserving strategies. Those inspired, respectively, by the\nMumford-Shah segmentation method and by anisotropic diffusion, are shown to be\nthe most effective for recovering discontinuities.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 14:17:48 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Qu\u00e9au", "Yvain", ""], ["Durou", "Jean-Denis", ""], ["Aujol", "Jean-Fran\u00e7ois", ""]]}, {"id": "1709.05972", "submitter": "Luis Angel Contreras-Toledo", "authors": "Luis Contreras and Walterio Mayol-Cuevas", "title": "Towards CNN map representation and compression for camera relocalisation", "comments": "Submitted to the 1st International Workshop on Deep Learning for\n  Visual SLAM, at the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study on the use of Convolutional Neural Networks for\ncamera relocalisation and its application to map compression. We follow state\nof the art visual relocalisation results and evaluate the response to different\ndata inputs. We use a CNN map representation and introduce the notion of map\ncompression under this paradigm by using smaller CNN architectures without\nsacrificing relocalisation performance. We evaluate this approach in a series\nof publicly available datasets over a number of CNN architectures with\ndifferent sizes, both in complexity and number of layers. This formulation\nallows us to improve relocalisation accuracy by increasing the number of\ntraining trajectories while maintaining a constant-size CNN.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 11:02:22 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 08:10:09 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Contreras", "Luis", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "1709.05982", "submitter": "Shaofei Wang", "authors": "Shaofei Wang, Chong Zhang, Miguel A. Gonzalez-Ballester, Alexander\n  Ihler, Julian Yarkony", "title": "Multi-Person Pose Estimation via Column Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of multi-person pose estimation in natural images. A\npose estimate describes the spatial position and identity (head, foot, knee,\netc.) of every non-occluded body part of a person. Pose estimation is difficult\ndue to issues such as deformation and variation in body configurations and\nocclusion of parts, while multi-person settings add complications such as an\nunknown number of people, with unknown appearance and possible interactions in\ntheir poses and part locations. We give a novel integer program formulation of\nthe multi-person pose estimation problem, in which variables correspond to\nassignments of parts in the image to poses in a two-tier, hierarchical way.\nThis enables us to develop an efficient custom optimization procedure based on\ncolumn generation, where columns are produced by exact optimization of very\nsmall scale integer programs. We demonstrate improved accuracy and speed for\nour method on the MPII multi-person pose estimation benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 14:40:57 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Wang", "Shaofei", ""], ["Zhang", "Chong", ""], ["Gonzalez-Ballester", "Miguel A.", ""], ["Ihler", "Alexander", ""], ["Yarkony", "Julian", ""]]}, {"id": "1709.06019", "submitter": "Thomas Ciarfuglia", "authors": "Gabriele Costante and Thomas A. Ciarfuglia", "title": "LS-VO: Learning Dense Optical Subspace for Robust Visual Odometry\n  Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2018.2803211", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel deep network architecture to solve the camera\nEgo-Motion estimation problem. A motion estimation network generally learns\nfeatures similar to Optical Flow (OF) fields starting from sequences of images.\nThis OF can be described by a lower dimensional latent space. Previous research\nhas shown how to find linear approximations of this space. We propose to use an\nAuto-Encoder network to find a non-linear representation of the OF manifold. In\naddition, we propose to learn the latent space jointly with the estimation\ntask, so that the learned OF features become a more robust description of the\nOF input. We call this novel architecture LS-VO.\n  The experiments show that LS-VO achieves a considerable increase in\nperformances in respect to baselines, while the number of parameters of the\nestimation network only slightly increases.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 16:11:34 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 22:25:23 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Costante", "Gabriele", ""], ["Ciarfuglia", "Thomas A.", ""]]}, {"id": "1709.06031", "submitter": "Kevis-Kokitsi Maninis", "authors": "Kevis-Kokitsi Maninis, Sergi Caelles, Yuhua Chen, Jordi Pont-Tuset,\n  Laura Leal-Taix\\'e, Daniel Cremers, Luc Van Gool", "title": "Video Object Segmentation Without Temporal Information", "comments": "Accepted to T-PAMI. Extended version of \"One-Shot Video Object\n  Segmentation\", CVPR 2017 (arXiv:1611.05198). Project page:\n  http://www.vision.ee.ethz.ch/~cvlsegmentation/osvos/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Object Segmentation, and video processing in general, has been\nhistorically dominated by methods that rely on the temporal consistency and\nredundancy in consecutive video frames. When the temporal smoothness is\nsuddenly broken, such as when an object is occluded, or some frames are missing\nin a sequence, the result of these methods can deteriorate significantly or\nthey may not even produce any result at all. This paper explores the orthogonal\napproach of processing each frame independently, i.e disregarding the temporal\ninformation. In particular, it tackles the task of semi-supervised video object\nsegmentation: the separation of an object from the background in a video, given\nits mask in the first frame. We present Semantic One-Shot Video Object\nSegmentation (OSVOS-S), based on a fully-convolutional neural network\narchitecture that is able to successively transfer generic semantic\ninformation, learned on ImageNet, to the task of foreground segmentation, and\nfinally to learning the appearance of a single annotated object of the test\nsequence (hence one shot). We show that instance level semantic information,\nwhen combined effectively, can dramatically improve the results of our previous\nmethod, OSVOS. We perform experiments on two recent video segmentation\ndatabases, which show that OSVOS-S is both the fastest and most accurate method\nin the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 16:28:02 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 12:16:48 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Maninis", "Kevis-Kokitsi", ""], ["Caelles", "Sergi", ""], ["Chen", "Yuhua", ""], ["Pont-Tuset", "Jordi", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Cremers", "Daniel", ""], ["Van Gool", "Luc", ""]]}, {"id": "1709.06035", "submitter": "Ahmed Elliethy", "authors": "Ahmed Elliethy and Gaurav Sharma", "title": "Vehicle Tracking in Wide Area Motion Imagery via Stochastic Progressive\n  Association Across Multiple Frames (SPAAM)", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2818443", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle tracking in Wide Area Motion Imagery (WAMI) relies on associating\nvehicle detections across multiple WAMI frames to form tracks corresponding to\nindividual vehicles. The temporal window length, i.e., the number $M$ of\nsequential frames, over which associations are collectively estimated poses a\ntrade-off between accuracy and computational complexity. A larger $M$ improves\nperformance because the increased temporal context enables the use of motion\nmodels and allows occlusions and spurious detections to be handled better. The\nnumber of total hypotheses tracks, on the other hand, grows exponentially with\nincreasing $M$, making larger values of $M$ computationally challenging to\ntackle. In this paper, we introduce SPAAM an iterative approach that\nprogressively grows $M$ with each iteration to improve estimated tracks by\nexploiting the enlarged temporal context while keeping computation manageable\nthrough two novel approaches for pruning association hypotheses. First, guided\nby a road network, accurately co-registered to the WAMI frames, we disregard\nunlikely associations that do not agree with the road network. Second, as $M$\nis progressively enlarged at each iteration, the related increase in\nassociation hypotheses is limited by revisiting only the subset of association\npossibilities rendered open by stochastically determined dis-associations for\nthe previous iteration. The stochastic dis-association at each iteration\nmaintains each estimated association according to an estimated probability for\nconfidence, obtained via a probabilistic model. Associations at each iteration\nare then estimated globally over the $M$ frames by (approximately) solving a\nbinary integer programming problem for selecting a set of compatible tracks.\nVehicle tracking results obtained over test WAMI datasets indicate that our\nproposed approach provides significant performance improvements over 3\nalternatives.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 16:40:28 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Elliethy", "Ahmed", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1709.06053", "submitter": "Georges Qu\\'enot", "authors": "Anuvabh Dutt, Denis Pellerin and Georges Qu\\'enot", "title": "Coupled Ensembles of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate in this paper the architecture of deep convolutional networks.\nBuilding on existing state of the art models, we propose a reconfiguration of\nthe model parameters into several parallel branches at the global network\nlevel, with each branch being a standalone CNN. We show that this arrangement\nis an efficient way to significantly reduce the number of parameters without\nlosing performance or to significantly improve the performance with the same\nlevel of performance. The use of branches brings an additional form of\nregularization. In addition to the split into parallel branches, we propose a\ntighter coupling of these branches by placing the \"fuse (averaging) layer\"\nbefore the Log-Likelihood and SoftMax layers during training. This gives\nanother significant performance improvement, the tighter coupling favouring the\nlearning of better representations, even at the level of the individual\nbranches. We refer to this branched architecture as \"coupled ensembles\". The\napproach is very generic and can be applied with almost any DCNN architecture.\nWith coupled ensembles of DenseNet-BC and parameter budget of 25M, we obtain\nerror rates of 2.92%, 15.68% and 1.50% respectively on CIFAR-10, CIFAR-100 and\nSVHN tasks. For the same budget, DenseNet-BC has error rate of 3.46%, 17.18%,\nand 1.8% respectively. With ensembles of coupled ensembles, of DenseNet-BC\nnetworks, with 50M total parameters, we obtain error rates of 2.72%, 15.13% and\n1.42% respectively on these tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 17:16:26 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Dutt", "Anuvabh", ""], ["Pellerin", "Denis", ""], ["Qu\u00e9not", "Georges", ""]]}, {"id": "1709.06054", "submitter": "Giuseppe Scarpa", "authors": "Giuseppe Scarpa, Sergio Vitale, and Davide Cozzolino", "title": "Target-adaptive CNN-based pansharpening", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2018.2817393", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently proposed a convolutional neural network (CNN) for remote sensing\nimage pansharpening obtaining a significant performance gain over the state of\nthe art. In this paper, we explore a number of architectural and training\nvariations to this baseline, achieving further performance gains with a\nlightweight network which trains very fast. Leveraging on this latter property,\nwe propose a target-adaptive usage modality which ensures a very good\nperformance also in the presence of a mismatch w.r.t. the training set, and\neven across different sensors. The proposed method, published online as an\noff-the-shelf software tool, allows users to perform fast and high-quality\nCNN-based pansharpening of their own target images on general-purpose hardware.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 17:16:55 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 14:15:51 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 21:44:16 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Scarpa", "Giuseppe", ""], ["Vitale", "Sergio", ""], ["Cozzolino", "Davide", ""]]}, {"id": "1709.06057", "submitter": "Litu Rout", "authors": "Litu Rout, Sidhartha, Gorthi R.K.S.S. Manyam, Deepak Mishra", "title": "Rotation Adaptive Visual Object Tracking with Motion Consistency", "comments": "Accepted conference paper WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Object tracking research has undergone significant improvement in the\npast few years. The emergence of tracking by detection approach in tracking\nparadigm has been quite successful in many ways. Recently, deep convolutional\nneural networks have been extensively used in most successful trackers. Yet,\nthe standard approach has been based on correlation or feature selection with\nminimal consideration given to motion consistency. Thus, there is still a need\nto capture various physical constraints through motion consistency which will\nimprove accuracy, robustness and more importantly rotation adaptiveness.\nTherefore, one of the major aspects of this paper is to investigate the outcome\nof rotation adaptiveness in visual object tracking. Among other key\ncontributions, the paper also includes various consistencies that turn out to\nbe extremely effective in numerous challenging sequences than the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 17:24:18 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 17:18:10 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Rout", "Litu", ""], ["Sidhartha", "", ""], ["Manyam", "Gorthi R. K. S. S.", ""], ["Mishra", "Deepak", ""]]}, {"id": "1709.06122", "submitter": "Itay Benou", "authors": "Itay Benou, Ronel Veksler, Alon Friedman, Tammy Riklin Raviv", "title": "Fiber-Flux Diffusion Density for White Matter Tracts Analysis:\n  Application to Mild Anomalies Localization in Contact Sports Players", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the concept of fiber-flux density for locally quantifying white\nmatter (WM) fiber bundles. By combining scalar diffusivity measures (e.g.,\nfractional anisotropy) with fiber-flux measurements, we define new local\ndescriptors called Fiber-Flux Diffusion Density (FFDD) vectors. Applying each\ndescriptor throughout fiber bundles allows along-tract coupling of a specific\ndiffusion measure with geometrical properties, such as fiber orientation and\ncoherence. A key step in the proposed framework is the construction of an FFDD\ndissimilarity measure for sub-voxel alignment of fiber bundles, based on the\nfast marching method (FMM). The obtained aligned WM tract-profiles enable\nmeaningful inter-subject comparisons and group-wise statistical analysis. We\ndemonstrate our method using two different datasets of contact sports players.\nAlong-tract pairwise comparison as well as group-wise analysis, with respect to\nnon-player healthy controls, reveal significant and spatially-consistent FFDD\nanomalies. Comparing our method with along-tract FA analysis shows improved\nsensitivity to subtle structural anomalies in football players over standard FA\nmeasurements.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 18:54:18 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Benou", "Itay", ""], ["Veksler", "Ronel", ""], ["Friedman", "Alon", ""], ["Raviv", "Tammy Riklin", ""]]}, {"id": "1709.06126", "submitter": "Zhennan Yan", "authors": "Zhennan Yan and Xiang Sean Zhou", "title": "How intelligent are convolutional neural networks?", "comments": "add one more experiment: common fate task; add link to github", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the Gestalt pattern theory, and the Winograd Challenge for\nlanguage understanding, we design synthetic experiments to investigate a deep\nlearning algorithm's ability to infer simple (at least for human) visual\nconcepts, such as symmetry, from examples. A visual concept is represented by\nrandomly generated, positive as well as negative, example images. We then test\nthe ability and speed of algorithms (and humans) to learn the concept from\nthese images. The training and testing are performed progressively in multiple\nrounds, with each subsequent round deliberately designed to be more complex and\nconfusing than the previous round(s), especially if the concept was not grasped\nby the learner. However, if the concept was understood, all the deliberate\ntests would become trivially easy. Our experiments show that humans can often\ninfer a semantic concept quickly after looking at only a very small number of\nexamples (this is often referred to as an \"aha moment\": a moment of sudden\nrealization), and performs perfectly during all testing rounds (except for\ncareless mistakes). On the contrary, deep convolutional neural networks (DCNN)\ncould approximate some concepts statistically, but only after seeing many\n(x10^4) more examples. And it will still make obvious mistakes, especially\nduring deliberate testing rounds or on samples outside the training\ndistributions. This signals a lack of true \"understanding\", or a failure to\nreach the right \"formula\" for the semantics. We did find that some concepts are\neasier for DCNN than others. For example, simple \"counting\" is more learnable\nthan \"symmetry\", while \"uniformity\" or \"conformance\" are much more difficult\nfor DCNN to learn. To conclude, we propose an \"Aha Challenge\" for visual\nperception, calling for focused and quantitative research on Gestalt-style\nmachine intelligence using limited training examples.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 19:04:36 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 20:29:12 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Yan", "Zhennan", ""], ["Zhou", "Xiang Sean", ""]]}, {"id": "1709.06129", "submitter": "Simon Du", "authors": "Simon S. Du, Jason D. Lee, Yuandong Tian", "title": "When is a Convolutional Filter Easy To Learn?", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the convergence of (stochastic) gradient descent algorithm for\nlearning a convolutional filter with Rectified Linear Unit (ReLU) activation\nfunction. Our analysis does not rely on any specific form of the input\ndistribution and our proofs only use the definition of ReLU, in contrast with\nprevious works that are restricted to standard Gaussian input. We show that\n(stochastic) gradient descent with random initialization can learn the\nconvolutional filter in polynomial time and the convergence rate depends on the\nsmoothness of the input distribution and the closeness of patches. To the best\nof our knowledge, this is the first recovery guarantee of gradient-based\nalgorithms for convolutional filter on non-Gaussian input distributions. Our\ntheory also justifies the two-stage learning rate strategy in deep neural\nnetworks. While our focus is theoretical, we also present experiments that\nillustrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 19:09:24 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 17:08:26 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Du", "Simon S.", ""], ["Lee", "Jason D.", ""], ["Tian", "Yuandong", ""]]}, {"id": "1709.06144", "submitter": "Olivier Colliot", "authors": "Kuldeep Kumar, Pietro Gori, Benjamin Charlier, Stanley Durrleman,\n  Olivier Colliot, Christian Desrosiers", "title": "White Matter Fiber Segmentation Using Functional Varifolds", "comments": null, "journal-ref": "Graphs in Biomedical Image Analysis, Computational Anatomy and\n  Imaging Genetics, pp 92-100, Lecture Notes in Computer Science, volume 10551,\n  Springer, 2017", "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The extraction of fibers from dMRI data typically produces a large number of\nfibers, it is common to group fibers into bundles. To this end, many\nspecialized distance measures, such as MCP, have been used for fiber\nsimilarity. However, these distance based approaches require point-wise\ncorrespondence and focus only on the geometry of the fibers. Recent\npublications have highlighted that using microstructure measures along fibers\nimproves tractography analysis. Also, many neurodegenerative diseases impacting\nwhite matter require the study of microstructure measures as well as the white\nmatter geometry. Motivated by these, we propose to use a novel computational\nmodel for fibers, called functional varifolds, characterized by a metric that\nconsiders both the geometry and microstructure measure (e.g. GFA) along the\nfiber pathway. We use it to cluster fibers with a dictionary learning and\nsparse coding-based framework, and present a preliminary analysis using HCP\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 20:05:19 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Kumar", "Kuldeep", ""], ["Gori", "Pietro", ""], ["Charlier", "Benjamin", ""], ["Durrleman", "Stanley", ""], ["Colliot", "Olivier", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1709.06151", "submitter": "Olivier Colliot", "authors": "Kuldeep Kumar, Laurent Chauvin, Mathew Toews, Olivier Colliot,\n  Christian Desrosiers", "title": "Multi-modal analysis of genetically-related subjects using SIFT\n  descriptors in brain MRI", "comments": null, "journal-ref": "Proc. Computational Diffusion MRI, MICCAI Workshop, Qu\\'ebec City,\n  Canada, September 2017", "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  So far, fingerprinting studies have focused on identifying features from\nsingle-modality MRI data, which capture individual characteristics in terms of\nbrain structure, function, or white matter microstructure. However, due to the\nlack of a framework for comparing across multiple modalities, studies based on\nmulti-modal data remain elusive. This paper presents a multi-modal analysis of\ngenetically-related subjects to compare and contrast the information provided\nby various MRI modalities. The proposed framework represents MRI scans as bags\nof SIFT features, and uses these features in a nearest-neighbor graph to\nmeasure subject similarity. Experiments using the T1/T2-weighted MRI and\ndiffusion MRI data of 861 Human Connectome Project subjects demonstrate strong\nlinks between the proposed similarity measure and genetic proximity.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 20:12:32 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Kumar", "Kuldeep", ""], ["Chauvin", "Laurent", ""], ["Toews", "Mathew", ""], ["Colliot", "Olivier", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1709.06158", "submitter": "Matthias Nie{\\ss}ner", "authors": "Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias\n  Nie{\\ss}ner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang", "title": "Matterport3D: Learning from RGB-D Data in Indoor Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to large, diverse RGB-D datasets is critical for training RGB-D scene\nunderstanding algorithms. However, existing datasets still cover only a limited\nnumber of views or a restricted scale of spaces. In this paper, we introduce\nMatterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views\nfrom 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided\nwith surface reconstructions, camera poses, and 2D and 3D semantic\nsegmentations. The precise global alignment and comprehensive, diverse\npanoramic set of views over entire buildings enable a variety of supervised and\nself-supervised computer vision tasks, including keypoint matching, view\noverlap prediction, normal prediction from color, semantic segmentation, and\nregion classification.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 20:34:48 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Chang", "Angel", ""], ["Dai", "Angela", ""], ["Funkhouser", "Thomas", ""], ["Halber", "Maciej", ""], ["Nie\u00dfner", "Matthias", ""], ["Savva", "Manolis", ""], ["Song", "Shuran", ""], ["Zeng", "Andy", ""], ["Zhang", "Yinda", ""]]}, {"id": "1709.06165", "submitter": "Yi Shang", "authors": "Chao Fang, Yi Shang, and Dong Xu", "title": "MUFold-SS: Protein Secondary Structure Prediction Using Deep\n  Inception-Inside-Inception Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Protein secondary structure prediction can provide important\ninformation for protein 3D structure prediction and protein functions. Deep\nlearning, which has been successfully applied to various research fields such\nas image classification and voice recognition, provides a new opportunity to\nsignificantly improve the secondary structure prediction accuracy. Although\nseveral deep-learning methods have been developed for secondary structure\nprediction, there is room for improvement. MUFold-SS was developed to address\nthese issues. Results: Here, a very deep neural network, the deep\ninception-inside-inception networks (Deep3I), is proposed for protein secondary\nstructure prediction and a software tool was implemented using this network.\nThis network takes two inputs: a protein sequence and a profile generated by\nPSI-BLAST. The output is the predicted eight states (Q8) or three states (Q3)\nof secondary structures. The proposed Deep3I not only achieves the\nstate-of-the-art performance but also runs faster than other tools. Deep3I\nachieves Q3 82.8% and Q8 71.1% accuracies on the CB513 benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 20:59:27 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Fang", "Chao", ""], ["Shang", "Yi", ""], ["Xu", "Dong", ""]]}, {"id": "1709.06178", "submitter": "Qi Wei", "authors": "Qi Wei, Emilie Chouzenoux, Jean-Yves Tourneret, Jean-Christophe\n  Pesquet", "title": "A Fast Algorithm Based on a Sylvester-like Equation for LS Regression\n  with GMRF Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast approach for penalized least squares (LS)\nregression problems using a 2D Gaussian Markov random field (GMRF) prior. More\nprecisely, the computation of the proximity operator of the LS criterion\nregularized by different GMRF potentials is formulated as solving a\nSylvester-like matrix equation. By exploiting the structural properties of\nGMRFs, this matrix equation is solved columnwise in an analytical way. The\nproposed algorithm can be embedded into a wide range of proximal algorithms to\nsolve LS regression problems including a convex penalty. Experiments carried\nout in the case of a constrained LS regression problem arising in a\nmultichannel image processing application, provide evidence that an alternating\ndirection method of multipliers performs quite efficiently in this context.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 21:57:47 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 16:37:14 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Wei", "Qi", ""], ["Chouzenoux", "Emilie", ""], ["Tourneret", "Jean-Yves", ""], ["Pesquet", "Jean-Christophe", ""]]}, {"id": "1709.06204", "submitter": "Jungseock Joo", "authors": "Donghyeon Won, Zachary C. Steinert-Threlkeld, Jungseock Joo", "title": "Protest Activity Detection and Perceived Violence Estimation from Social\n  Media Images", "comments": "To appear in Proceedings of the 25th ACM International Conference on\n  Multimedia 2017 (full research paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel visual model which can recognize protesters, describe\ntheir activities by visual attributes and estimate the level of perceived\nviolence in an image. Studies of social media and protests use natural language\nprocessing to track how individuals use hashtags and links, often with a focus\non those items' diffusion. These approaches, however, may not be effective in\nfully characterizing actual real-world protests (e.g., violent or peaceful) or\nestimating the demographics of participants (e.g., age, gender, and race) and\ntheir emotions. Our system characterizes protests along these dimensions. We\nhave collected geotagged tweets and their images from 2013-2017 and analyzed\nmultiple major protest events in that period. A multi-task convolutional neural\nnetwork is employed in order to automatically classify the presence of\nprotesters in an image and predict its visual attributes, perceived violence\nand exhibited emotions. We also release the UCLA Protest Image Dataset, our\nnovel dataset of 40,764 images (11,659 protest images and hard negatives) with\nvarious annotations of visual attributes and sentiments. Using this dataset, we\ntrain our model and demonstrate its effectiveness. We also present experimental\nresults from various analysis on geotagged image data in several prevalent\nprotest events. Our dataset will be made accessible at\nhttps://www.sscnet.ucla.edu/comm/jjoo/mm-protest/.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 23:57:42 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Won", "Donghyeon", ""], ["Steinert-Threlkeld", "Zachary C.", ""], ["Joo", "Jungseock", ""]]}, {"id": "1709.06229", "submitter": "Honggang Chen", "authors": "Honggang Chen, Xiaohai He, Chao Ren, Linbo Qing, Qizhi Teng", "title": "CISRDCNN: Super-resolution of compressed images using deep convolutional\n  neural networks", "comments": "32 pages, 17 figures, 5 tables, preprint submitted to Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, much research has been conducted on image super-resolution\n(SR). To the best of our knowledge, however, few SR methods were concerned with\ncompressed images. The SR of compressed images is a challenging task due to the\ncomplicated compression artifacts, while many images suffer from them in\npractice. The intuitive solution for this difficult task is to decouple it into\ntwo sequential but independent subproblems, i.e., compression artifacts\nreduction (CAR) and SR. Nevertheless, some useful details may be removed in CAR\nstage, which is contrary to the goal of SR and makes the SR stage more\nchallenging. In this paper, an end-to-end trainable deep convolutional neural\nnetwork is designed to perform SR on compressed images (CISRDCNN), which\nreduces compression artifacts and improves image resolution jointly.\nExperiments on compressed images produced by JPEG (we take the JPEG as an\nexample in this paper) demonstrate that the proposed CISRDCNN yields\nstate-of-the-art SR performance on commonly used test images and imagesets. The\nresults of CISRDCNN on real low quality web images are also very impressive,\nwith obvious quality enhancement. Further, we explore the application of the\nproposed SR method in low bit-rate image coding, leading to better\nrate-distortion performance than JPEG.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 02:45:12 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Chen", "Honggang", ""], ["He", "Xiaohai", ""], ["Ren", "Chao", ""], ["Qing", "Linbo", ""], ["Teng", "Qizhi", ""]]}, {"id": "1709.06247", "submitter": "Gangming Zhao", "authors": "Gangming Zhao, Zhaoxiang Zhang, He Guan, Peng Tang, Jingdong Wang", "title": "Rethink ReLU to Training Better CNNs", "comments": "8 pages,10 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of convolutional neural networks share the same characteristic: each\nconvolutional layer is followed by a nonlinear activation layer where Rectified\nLinear Unit (ReLU) is the most widely used. In this paper, we argue that the\ndesigned structure with the equal ratio between these two layers may not be the\nbest choice since it could result in the poor generalization ability. Thus, we\ntry to investigate a more suitable method on using ReLU to explore the better\nnetwork architectures. Specifically, we propose a proportional module to keep\nthe ratio between convolution and ReLU amount to be N:M (N>M). The proportional\nmodule can be applied in almost all networks with no extra computational cost\nto improve the performance. Comprehensive experimental results indicate that\nthe proposed method achieves better performance on different benchmarks with\ndifferent network architectures, thus verify the superiority of our work.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 04:27:56 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 08:25:33 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Zhao", "Gangming", ""], ["Zhang", "Zhaoxiang", ""], ["Guan", "He", ""], ["Tang", "Peng", ""], ["Wang", "Jingdong", ""]]}, {"id": "1709.06248", "submitter": "Haesol Park", "authors": "Haesol Park and Kyoung Mu Lee", "title": "Look Wider to Match Image Patches with Convolutional Neural Networks", "comments": "published in SPL", "journal-ref": "H. Park and K. M. Lee, \"Look Wider to Match Image Patches with\n  Convolutional Neural Networks,\" in IEEE Signal Processing Letters, vol. PP,\n  no. 99, pp. 1-1, 2016", "doi": "10.1109/LSP.2016.2637355", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a human matches two images, the viewer has a natural tendency to view\nthe wide area around the target pixel to obtain clues of right correspondence.\nHowever, designing a matching cost function that works on a large window in the\nsame way is difficult. The cost function is typically not intelligent enough to\ndiscard the information irrelevant to the target pixel, resulting in\nundesirable artifacts. In this paper, we propose a novel learn a stereo\nmatching cost with a large-sized window. Unlike conventional pooling layers\nwith strides, the proposed per-pixel pyramid-pooling layer can cover a large\narea without a loss of resolution and detail. Therefore, the learned matching\ncost function can successfully utilize the information from a large area\nwithout introducing the fattening effect. The proposed method is robust despite\nthe presence of weak textures, depth discontinuity, illumination, and exposure\ndifference. The proposed method achieves near-peak performance on the\nMiddlebury benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 04:31:43 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Park", "Haesol", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1709.06257", "submitter": "Ashish Mahabal", "authors": "Ashish Mahabal (1), Kshiteej Sheth (2), Fabian Gieseke (3), Akshay Pai\n  (3), S. George Djorgovski (1), Andrew Drake (1), Matthew Graham (1), the\n  CSS/CRTS/PTF Collaboration ((1) California Institute of Technology, USA, (2)\n  Indian Institute of Technology Gandhinagar, India, (3) University of\n  Copenhagen, Denmark)", "title": "Deep-Learnt Classification of Light Curves", "comments": "8 pages, 9 figures, 6 tables, 2 listings. Accepted to 2017 IEEE\n  Symposium Series on Computational Intelligence (SSCI)", "journal-ref": "2017 IEEE Symposium Series on Computational Intelligence (SSCI),\n  Honolulu, HI, USA, 2017, p2757", "doi": "10.1109/SSCI.2017.8280984", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomy light curves are sparse, gappy, and heteroscedastic. As a result\nstandard time series methods regularly used for financial and similar datasets\nare of little help and astronomers are usually left to their own instruments\nand techniques to classify light curves. A common approach is to derive\nstatistical features from the time series and to use machine learning methods,\ngenerally supervised, to separate objects into a few of the standard classes.\nIn this work, we transform the time series to two-dimensional light curve\nrepresentations in order to classify them using modern deep learning\ntechniques. In particular, we show that convolutional neural networks based\nclassifiers work well for broad characterization and classification. We use\nlabeled datasets of periodic variables from CRTS survey and show how this opens\ndoors for a quick classification of diverse classes with several possible\nexciting extensions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 05:00:58 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Mahabal", "Ashish", ""], ["Sheth", "Kshiteej", ""], ["Gieseke", "Fabian", ""], ["Pai", "Akshay", ""], ["Djorgovski", "S. George", ""], ["Drake", "Andrew", ""], ["Graham", "Matthew", ""], ["Collaboration", "the CSS/CRTS/PTF", ""]]}, {"id": "1709.06262", "submitter": "Julian Faraone", "authors": "Julian Faraone, Nicholas Fraser, Giulio Gambardella, Michaela Blott\n  and Philip H.W. Leong", "title": "Compressing Low Precision Deep Neural Networks Using Sparsity-Induced\n  Regularization in Ternary Networks", "comments": "To appear as a conference paper at the 24th International Conference\n  On Neural Information Processing (ICONIP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low precision deep neural network training technique for producing sparse,\nternary neural networks is presented. The technique incorporates hard- ware\nimplementation costs during training to achieve significant model compression\nfor inference. Training involves three stages: network training using L2\nregularization and a quantization threshold regularizer, quantization pruning,\nand finally retraining. Resulting networks achieve improved accuracy, reduced\nmemory footprint and reduced computational complexity compared with\nconventional methods, on MNIST and CIFAR10 datasets. Our networks are up to 98%\nsparse and 5 & 11 times smaller than equivalent binary and ternary models,\ntranslating to significant resource and speed benefits for hardware\nimplementations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 05:50:19 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 03:53:33 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Faraone", "Julian", ""], ["Fraser", "Nicholas", ""], ["Gambardella", "Giulio", ""], ["Blott", "Michaela", ""], ["Leong", "Philip H. W.", ""]]}, {"id": "1709.06300", "submitter": "Arash Akbarinia", "authors": "Arash Akbarinia and C. Alejandro Parraga", "title": "Colour Terms: a Categorisation Model Inspired by Visual Cortex Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although it seems counter-intuitive, categorical colours do not exist as\nexternal physical entities but are very much the product of our brains. Our\ncortical machinery segments the world and associate objects to specific colour\nterms, which is not only convenient for communication but also increases the\nefficiency of visual processing by reducing the dimensionality of input scenes.\nAlthough the neural substrate for this phenomenon is unknown, a recent study of\ncortical colour processing has discovered a set of neurons that are\nisoresponsive to stimuli in the shape of 3D-ellipsoidal surfaces in\ncolour-opponent space. We hypothesise that these neurons might help explain the\nunderlying mechanisms of colour naming in the visual cortex.\n  Following this, we propose a biologically-inspired colour naming model where\neach colour term - e.g. red, green, blue, yellow, etc. - is represented through\nan ellipsoid in 3D colour-opponent space. This paradigm is also supported by\nprevious psychophysical colour categorisation experiments whose results\nresemble such shapes. \"Belongingness\" of each pixel to different colour\ncategories is computed by a non-linear sigmoidal logistic function. The final\ncolour term for a given pixel is calculated by a maximum pooling mechanism. The\nsimplicity of our model allows its parameters to be learnt from a handful of\nsegmented images. It also offers a straightforward extension to include further\ncolour terms. Additionally, ellipsoids of proposed model can adapt to image\ncontents offering a dynamical solution in order to address phenomenon of colour\nconstancy. Our results on the Munsell chart and two datasets of real-world\nimages show an overall improvement comparing to state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 08:53:28 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Akbarinia", "Arash", ""], ["Parraga", "C. Alejandro", ""]]}, {"id": "1709.06308", "submitter": "Tingting Qiao", "authors": "Tingting Qiao, Jianfeng Dong, Duanqing Xu", "title": "Exploring Human-like Attention Supervision in Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have been widely applied in the Visual Question\nAnswering (VQA) task, as they help to focus on the area-of-interest of both\nvisual and textual information. To answer the questions correctly, the model\nneeds to selectively target different areas of an image, which suggests that an\nattention-based model may benefit from an explicit attention supervision. In\nthis work, we aim to address the problem of adding attention supervision to VQA\nmodels. Since there is a lack of human attention data, we first propose a Human\nAttention Network (HAN) to generate human-like attention maps, training on a\nrecently released dataset called Human ATtention Dataset (VQA-HAT). Then, we\napply the pre-trained HAN on the VQA v2.0 dataset to automatically produce the\nhuman-like attention maps for all image-question pairs. The generated\nhuman-like attention map dataset for the VQA v2.0 dataset is named as\nHuman-Like ATtention (HLAT) dataset. Finally, we apply human-like attention\nsupervision to an attention-based VQA model. The experiments show that adding\nhuman-like supervision yields a more accurate attention together with a better\nperformance, showing a promising future for human-like attention supervision in\nVQA.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:19:08 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Qiao", "Tingting", ""], ["Dong", "Jianfeng", ""], ["Xu", "Duanqing", ""]]}, {"id": "1709.06310", "submitter": "Antoni Rosinol Vidal", "authors": "Antoni Rosinol Vidal, Henri Rebecq, Timo Horstschaefer, Davide\n  Scaramuzza", "title": "Ultimate SLAM? Combining Events, Images, and IMU for Robust Visual SLAM\n  in HDR and High Speed Scenarios", "comments": "8 pages, 9 figures, 2 tables", "journal-ref": "Robot.Autom.Lett. 3 (2018) 994-1001", "doi": "10.1109/LRA.2018.2793357", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired vision sensors that output pixel-level\nbrightness changes instead of standard intensity frames. These cameras do not\nsuffer from motion blur and have a very high dynamic range, which enables them\nto provide reliable visual information during high speed motions or in scenes\ncharacterized by high dynamic range. However, event cameras output only little\ninformation when the amount of motion is limited, such as in the case of almost\nstill motion. Conversely, standard cameras provide instant and rich information\nabout the environment most of the time (in low-speed and good lighting\nscenarios), but they fail severely in case of fast motions, or difficult\nlighting such as high dynamic range or low light scenes. In this paper, we\npresent the first state estimation pipeline that leverages the complementary\nadvantages of these two sensors by fusing in a tightly-coupled manner events,\nstandard frames, and inertial measurements. We show on the publicly available\nEvent Camera Dataset that our hybrid pipeline leads to an accuracy improvement\nof 130% over event-only pipelines, and 85% over standard-frames-only\nvisual-inertial systems, while still being computationally tractable.\nFurthermore, we use our pipeline to demonstrate - to the best of our knowledge\n- the first autonomous quadrotor flight using an event camera for state\nestimation, unlocking flight scenarios that were not reachable with traditional\nvisual-inertial odometry, such as low-light environments and high-dynamic range\nscenes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:26:05 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 07:57:32 GMT"}, {"version": "v3", "created": "Thu, 12 Oct 2017 17:45:06 GMT"}, {"version": "v4", "created": "Mon, 22 Jan 2018 15:31:17 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Vidal", "Antoni Rosinol", ""], ["Rebecq", "Henri", ""], ["Horstschaefer", "Timo", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1709.06316", "submitter": "Lai Jiang", "authors": "Lai Jiang, Mai Xu and Zulin Wang", "title": "Predicting Video Saliency with Object-to-Motion CNN and Two-layer\n  Convolutional LSTM", "comments": "Jiang, Lai and Xu, Mai and Liu, Tie and Qiao, Minglang and Wang,\n  Zulin; DeepVS: A Deep Learning Based Video Saliency Prediction Approach;The\n  European Conference on Computer Vision (ECCV); September 2018", "journal-ref": null, "doi": "10.1007/978-3-030-01264-9_37", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, deep neural networks (DNNs) have exhibited great\nsuccess in predicting the saliency of images. However, there are few works that\napply DNNs to predict the saliency of generic videos. In this paper, we propose\na novel DNN-based video saliency prediction method. Specifically, we establish\na large-scale eye-tracking database of videos (LEDOV), which provides\nsufficient data to train the DNN models for predicting video saliency. Through\nthe statistical analysis of our LEDOV database, we find that human attention is\nnormally attracted by objects, particularly moving objects or the moving parts\nof objects. Accordingly, we propose an object-to-motion convolutional neural\nnetwork (OM-CNN) to learn spatio-temporal features for predicting the\nintra-frame saliency via exploring the information of both objectness and\nobject motion. We further find from our database that there exists a temporal\ncorrelation of human attention with a smooth saliency transition across video\nframes. Therefore, we develop a two-layer convolutional long short-term memory\n(2C-LSTM) network in our DNN-based method, using the extracted features of\nOM-CNN as the input. Consequently, the inter-frame saliency maps of videos can\nbe generated, which consider the transition of attention across video frames.\nFinally, the experimental results show that our method advances the\nstate-of-the-art in video saliency prediction.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 09:45:03 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 04:16:54 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 18:11:58 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Jiang", "Lai", ""], ["Xu", "Mai", ""], ["Wang", "Zulin", ""]]}, {"id": "1709.06328", "submitter": "Pedro Miraldo", "authors": "Pedro Miraldo and Joao R. Cardoso", "title": "On the Generalized Essential Matrix Correction: An efficient solution to\n  the problem and its applications", "comments": "14 pages, 7 figures, journal", "journal-ref": "Journal of Mathematical Imaging and Vision, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of finding the closest generalized essential\nmatrix from a given $6\\times 6$ matrix, with respect to the Frobenius norm. To\nthe best of our knowledge, this nonlinear constrained optimization problem has\nnot been addressed in the literature yet. Although it can be solved directly,\nit involves a large number of constraints, and any optimization method to solve\nit would require much computational effort. We start by deriving a couple of\nunconstrained formulations of the problem. After that, we convert the original\nproblem into a new one, involving only orthogonal constraints, and propose an\nefficient algorithm of steepest descent-type to find its solution. To test the\nalgorithms, we evaluate the methods with synthetic data and conclude that the\nproposed steepest descent-type approach is much faster than the direct\napplication of general optimization techniques to the original formulation with\n33 constraints and to the unconstrained ones. To further motivate the relevance\nof our method, we apply it in two pose problems (relative and absolute) using\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 10:10:09 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 19:08:58 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 11:39:40 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Miraldo", "Pedro", ""], ["Cardoso", "Joao R.", ""]]}, {"id": "1709.06341", "submitter": "Benjamin Hou", "authors": "Benjamin Hou, Bishesh Khanal, Amir Alansary, Steven McDonagh, Alice\n  Davidson, Mary Rutherford, Jo V. Hajnal, Daniel Rueckert, Ben Glocker and\n  Bernhard Kainz", "title": "3D Reconstruction in Canonical Co-ordinate Space from Arbitrarily\n  Oriented 2D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited capture range, and the requirement to provide high quality\ninitialization for optimization-based 2D/3D image registration methods, can\nsignificantly degrade the performance of 3D image reconstruction and motion\ncompensation pipelines. Challenging clinical imaging scenarios, which contain\nsignificant subject motion such as fetal in-utero imaging, complicate the 3D\nimage and volume reconstruction process. In this paper we present a learning\nbased image registration method capable of predicting 3D rigid transformations\nof arbitrarily oriented 2D image slices, with respect to a learned canonical\natlas co-ordinate system. Only image slice intensity information is used to\nperform registration and canonical alignment, no spatial transform\ninitialization is required. To find image transformations we utilize a\nConvolutional Neural Network (CNN) architecture to learn the regression\nfunction capable of mapping 2D image slices to a 3D canonical atlas space. We\nextensively evaluate the effectiveness of our approach quantitatively on\nsimulated Magnetic Resonance Imaging (MRI), fetal brain imagery with synthetic\nmotion and further demonstrate qualitative results on real fetal MRI data where\nour method is integrated into a full reconstruction and motion compensation\npipeline. Our learning based registration achieves an average spatial\nprediction error of 7 mm on simulated data and produces qualitatively improved\nreconstructions for heavily moving fetuses with gestational ages of\napproximately 20 weeks. Our model provides a general and computationally\nefficient solution to the 2D/3D registration initialization problem and is\nsuitable for real-time scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 10:50:20 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 08:17:57 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 18:55:18 GMT"}, {"version": "v4", "created": "Tue, 23 Jan 2018 18:21:29 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Hou", "Benjamin", ""], ["Khanal", "Bishesh", ""], ["Alansary", "Amir", ""], ["McDonagh", "Steven", ""], ["Davidson", "Alice", ""], ["Rutherford", "Mary", ""], ["Hajnal", "Jo V.", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1709.06366", "submitter": "Cihan Topal", "authors": "Cihan Topal, Halil Ibrahim Cakir, Cuneyt Akinlar", "title": "APPD: Adaptive and Precise Pupil Boundary Detection using Entropy of\n  Contour Gradients", "comments": "Demo video: https://www.youtube.com/watch?v=cPLmmVLGrdQ&t=5s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye tracking spreads through a vast area of applications from ophthalmology,\nassistive technologies to gaming and virtual reality. Precisely detecting the\npupil's contour and center is the very first step in many of these tasks, hence\nneeds to be performed accurately. Although detection of pupil is a simple\nproblem when it is entirely visible; occlusions and oblique view angles\ncomplicate the solution. In this study, we propose APPD, an adaptive and\nprecise pupil boundary detection method that is able to infer whether entire\npupil is in clearly visible by a heuristic that estimates the shape of a\ncontour in a computationally efficient way. Thus, a faster detection is\nperformed with the assumption of no occlusions. If the heuristic fails, a more\ncomprehensive search among extracted image features is executed to maintain\naccuracy. Furthermore, the algorithm can find out if there is no pupil as an\nhelpful information for many applications. We provide a dataset containing 3904\nhigh resolution eye images collected from 12 subjects and perform an extensive\nset of experiments to obtain quantitative results in terms of accuracy,\nlocalization and timing. The proposed method outperforms three other state of\nthe art algorithms and has an average execution time $\\sim$5 ms in\nsingle-thread on a standard laptop computer for 720p images.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 12:09:34 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 15:26:35 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Topal", "Cihan", ""], ["Cakir", "Halil Ibrahim", ""], ["Akinlar", "Cuneyt", ""]]}, {"id": "1709.06389", "submitter": "Frank Dennis Julca Aguilar", "authors": "Frank Julca-Aguilar, Harold Mouch\\`ere, Christian Viard-Gaudin and\n  Nina S. T. Hirata", "title": "A General Framework for the Recognition of Online Handwritten Graphics", "comments": "Submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for the recognition of online handwritten\ngraphics. Three main features of the framework are its ability to treat symbol\nand structural level information in an integrated way, its flexibility with\nrespect to different families of graphics, and means to control the tradeoff\nbetween recognition effectiveness and computational cost. We model a graphic as\na labeled graph generated from a graph grammar. Non-terminal vertices represent\nsubcomponents, terminal vertices represent symbols, and edges represent\nrelations between subcomponents or symbols. We then model the recognition\nproblem as a graph parsing problem: given an input stroke set, we search for a\nparse tree that represents the best interpretation of the input. Our graph\nparsing algorithm generates multiple interpretations (consistent with the\ngrammar) and then we extract an optimal interpretation according to a cost\nfunction that takes into consideration the likelihood scores of symbols and\nstructures. The parsing algorithm consists in recursively partitioning the\nstroke set according to structures defined in the grammar and it does not\nimpose constraints present in some previous works (e.g. stroke ordering). By\navoiding such constraints and thanks to the powerful representativeness of\ngraphs, our approach can be adapted to the recognition of different graphic\nnotations. We show applications to the recognition of mathematical expressions\nand flowcharts. Experimentation shows that our method obtains state-of-the-art\naccuracy in both applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 13:06:32 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Julca-Aguilar", "Frank", ""], ["Mouch\u00e8re", "Harold", ""], ["Viard-Gaudin", "Christian", ""], ["Hirata", "Nina S. T.", ""]]}, {"id": "1709.06391", "submitter": "Tengda Han", "authors": "Tengda Han, Jue Wang, Anoop Cherian, Stephen Gould", "title": "Human Action Forecasting by Learning Task Grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For effective human-robot interaction, it is important that a robotic\nassistant can forecast the next action a human will consider in a given task.\nUnfortunately, real-world tasks are often very long, complex, and repetitive;\nas a result forecasting is not trivial. In this paper, we propose a novel deep\nrecurrent architecture that takes as input features from a two-stream Residual\naction recognition framework, and learns to estimate the progress of human\nactivities from video sequences -- this surrogate progress estimation task\nimplicitly learns a temporal task grammar with respect to which activities can\nbe localized and forecasted. To learn the task grammar, we propose a stacked\nLSTM based multi-granularity progress estimation framework that uses a novel\ncumulative Euclidean loss as objective. To demonstrate the effectiveness of our\nproposed architecture, we showcase experiments on two challenging robotic\nassistive tasks, namely (i) assembling an Ikea table from its constituents, and\n(ii) changing the tires of a car. Our results demonstrate that learning task\ngrammars offers highly discriminative cues improving the forecasting accuracy\nby more than 9% over the baseline two-stream forecasting model, while also\noutperforming other competitive schemes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 13:12:36 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Han", "Tengda", ""], ["Wang", "Jue", ""], ["Cherian", "Anoop", ""], ["Gould", "Stephen", ""]]}, {"id": "1709.06437", "submitter": "Nantheera Anantrasirichai", "authors": "N. Anantrasirichai, Sion Hannuna, Nishan Canagarajah", "title": "Automatic Leaf Extraction from Outdoor Images", "comments": "13 pages, India-UK Advanced Technology Centre of Excellence in Next\n  Generation Networks, Systems and Services (IU-ATC), 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic plant recognition and disease analysis may be streamlined by an\nimage of a complete, isolated leaf as an initial input. Segmenting leaves from\nnatural images is a hard problem. Cluttered and complex backgrounds: often\ncomposed of other leaves are commonplace. Furthermore, their appearance is\nhighly dependent upon illumination and viewing perspective. In order to address\nthese issues we propose a methodology which exploits the leaves venous systems\nin tandem with other low level features. Background and leaf markers are\ncreated using colour, intensity and texture. Two approaches are investigated:\nwatershed and graph-cut and results compared. Primary-secondary vein detection\nand a protrusion-notch removal are applied to refine the extracted leaf. The\nefficacy of our approach is demonstrated against existing work.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 14:08:56 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Anantrasirichai", "N.", ""], ["Hannuna", "Sion", ""], ["Canagarajah", "Nishan", ""]]}, {"id": "1709.06447", "submitter": "Michalis Vrigkas", "authors": "Michalis Vrigkas and Evangelos Kazakos and Christophoros Nikou and\n  Ioannis A. Kakadiaris", "title": "Human Activity Recognition Using Robust Adaptive Privileged\n  Probabilistic Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a novel method based on the learning using privileged\ninformation (LUPI) paradigm for recognizing complex human activities is\nproposed that handles missing information during testing. We present a\nsupervised probabilistic approach that integrates LUPI into a hidden\nconditional random field (HCRF) model. The proposed model is called HCRF+ and\nmay be trained using both maximum likelihood and maximum margin approaches. It\nemploys a self-training technique for automatic estimation of the\nregularization parameters of the objective functions. Moreover, the method\nprovides robustness to outliers (such as noise or missing data) by modeling the\nconditional distribution of the privileged information by a Student's\n\\textit{t}-density function, which is naturally integrated into the HCRF+\nframework. Different forms of privileged information were investigated. The\nproposed method was evaluated using four challenging publicly available\ndatasets and the experimental results demonstrate its effectiveness with\nrespect to the-state-of-the-art in the LUPI framework using both hand-crafted\nfeatures and features extracted from a convolutional neural network.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 14:21:28 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Vrigkas", "Michalis", ""], ["Kazakos", "Evangelos", ""], ["Nikou", "Christophoros", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1709.06451", "submitter": "Pedro Miraldo", "authors": "Tiago Dias, Helder Araujo, and Pedro Miraldo", "title": "3D Reconstruction with Low Resolution, Small Baseline and High Radial\n  Distortion Stereo Images", "comments": null, "journal-ref": "ACM Int'l Conf. Distributed Smart Cameras (ICDSC), 2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze and compare approaches for 3D reconstruction from\nlow-resolution (250x250), high radial distortion stereo images, which are\nacquired with small baseline (approximately 1mm). These images are acquired\nwith the system NanEye Stereo manufactured by CMOSIS/AWAIBA. These stereo\ncameras have also small apertures, which means that high levels of illumination\nare required. The goal was to develop an approach yielding accurate\nreconstructions, with a low computational cost, i.e., avoiding non-linear\nnumerical optimization algorithms. In particular we focused on the analysis and\ncomparison of radial distortion models. To perform the analysis and comparison,\nwe defined a baseline method based on available software and methods, such as\nthe Bouguet toolbox [2] or the Computer Vision Toolbox from Matlab. The\napproaches tested were based on the use of the polynomial model of radial\ndistortion, and on the application of the division model. The issue of the\ncenter of distortion was also addressed within the framework of the application\nof the division model. We concluded that the division model with a single\nradial distortion parameter has limitations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 14:22:22 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Dias", "Tiago", ""], ["Araujo", "Helder", ""], ["Miraldo", "Pedro", ""]]}, {"id": "1709.06476", "submitter": "Frank Dennis Julca Aguilar", "authors": "Frank D. Julca-Aguilar and Nina S. T. Hirata", "title": "Image operator learning coupled with CNN classification and its\n  application to staff line removal", "comments": "To appear in ICDAR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many image transformations can be modeled by image operators that are\ncharacterized by pixel-wise local functions defined on a finite support window.\nIn image operator learning, these functions are estimated from training data\nusing machine learning techniques. Input size is usually a critical issue when\nusing learning algorithms, and it limits the size of practicable windows. We\npropose the use of convolutional neural networks (CNNs) to overcome this\nlimitation. The problem of removing staff-lines in music score images is chosen\nto evaluate the effects of window and convolutional mask sizes on the learned\nimage operator performance. Results show that the CNN based solution\noutperforms previous ones obtained using conventional learning algorithms or\nheuristic algorithms, indicating the potential of CNNs as base classifiers in\nimage operator learning. The implementations will be made available on the\nTRIOSlib project site.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 15:00:09 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Julca-Aguilar", "Frank D.", ""], ["Hirata", "Nina S. T.", ""]]}, {"id": "1709.06495", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran and Oswald Lanz", "title": "Convolutional Long Short-Term Memory Networks for Recognizing First\n  Person Interactions", "comments": "Accepted on the second International Workshop on Egocentric\n  Perception, Interaction and Computing(EPIC) at International Conference on\n  Computer Vision(ICCV-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel deep learning based approach for addressing\nthe problem of interaction recognition from a first person perspective. The\nproposed approach uses a pair of convolutional neural networks, whose\nparameters are shared, for extracting frame level features from successive\nframes of the video. The frame level features are then aggregated using a\nconvolutional long short-term memory. The hidden state of the convolutional\nlong short-term memory, after all the input video frames are processed, is used\nfor classification in to the respective categories. The two branches of the\nconvolutional neural network perform feature encoding on a short time interval\nwhereas the convolutional long short term memory encodes the changes on a\nlonger temporal duration. In our network the spatio-temporal structure of the\ninput is preserved till the very final processing stage. Experimental results\nshow that our method outperforms the state of the art on most recent first\nperson interactions datasets that involve complex ego-motion. In particular, on\nUTKinect-FirstPerson it competes with methods that use depth image and skeletal\njoints information along with RGB images, while it surpasses all previous\nmethods that use only RGB images by more than 20% in recognition accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 15:58:28 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Lanz", "Oswald", ""]]}, {"id": "1709.06505", "submitter": "Sebastian Lutz", "authors": "Rafael Monroy, Sebastian Lutz, Tejo Chalasani, Aljosa Smolic", "title": "SalNet360: Saliency Maps for omni-directional images with CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of Visual Attention data from any kind of media is of valuable\nuse to content creators and used to efficiently drive encoding algorithms. With\nthe current trend in the Virtual Reality (VR) field, adapting known techniques\nto this new kind of media is starting to gain momentum. In this paper, we\npresent an architectural extension to any Convolutional Neural Network (CNN) to\nfine-tune traditional 2D saliency prediction to Omnidirectional Images (ODIs)\nin an end-to-end manner. We show that each step in the proposed pipeline works\ntowards making the generated saliency map more accurate with respect to ground\ntruth data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 16:21:09 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 09:47:27 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Monroy", "Rafael", ""], ["Lutz", "Sebastian", ""], ["Chalasani", "Tejo", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1709.06508", "submitter": "Shiv Ram Dubey", "authors": "Shiv Ram Dubey", "title": "Face Retrieval using Frequency Decoded Local Descriptor", "comments": "Accepted in Multimedia Tools and Applications, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local descriptors have been the backbone of most of the computer vision\nproblems. Most of the existing local descriptors are generated over the raw\ninput images. In order to increase the discriminative power of the local\ndescriptors, some researchers converted the raw image into multiple images with\nthe help of some high and low pass frequency filters, then the local\ndescriptors are computed over each filtered image and finally concatenated into\na single descriptor. By doing so, these approaches do not utilize the inter\nfrequency relationship which causes the less improvement in the discriminative\npower of the descriptor that could be achieved. In this paper, this problem is\nsolved by utilizing the decoder concept of multi-channel decoded local binary\npattern over the multi-frequency patterns. A frequency decoded local binary\npattern (FDLBP) is proposed with two decoders. Each decoder works with one low\nfrequency pattern and two high frequency patterns. Finally, the descriptors\nfrom both decoders are concatenated to form the single descriptor. The face\nretrieval experiments are conducted over four benchmarks and challenging\ndatabases such as PaSC, LFW, PubFig, and ESSEX. The experimental results\nconfirm the superiority of the FDLBP descriptor as compared to the\nstate-of-the-art descriptors such as LBP, SOBEL_LBP, BoF_LBP, SVD_S_LBP, mdLBP,\netc.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 06:43:59 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 02:47:28 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Dubey", "Shiv Ram", ""]]}, {"id": "1709.06509", "submitter": "Raghavendra Kandukuri", "authors": "Raghavendra Kandukuri", "title": "A LBP Based Correspondence Identification Scheme for Multi-view Sensing\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describes a correspondence identification method between\ntwo-views of regular RGB camera that can be run in real-time. The basic idea is\nfirst applying normalized cross correlation to retrieve a sparse set of\nmatching pairs from image pair. Then loopy belief propagation scheme is applied\nto the the set of possible candidates to densely identify correspondences from\ndifferent views. The experiment results demonstrate superb accuracy and\nprecision that outperform the state-of-the-art in the computer vision field.\nMeanwhile, the implementation is simple enough that can be optimized for\nreal-time performance. We have given the detailed comparison of existing\napproaches and show that this method can enable various practical applications\nfrom 3D reconstruction to image search.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 00:49:44 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Kandukuri", "Raghavendra", ""]]}, {"id": "1709.06531", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran and Oswald Lanz", "title": "Learning to Detect Violent Videos using Convolutional Long Short-Term\n  Memory", "comments": "Accepted in International Conference on Advanced Video and Signal\n  based Surveillance(AVSS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a technique for the automatic analysis of surveillance videos in\norder to identify the presence of violence is of broad interest. In this work,\nwe propose a deep neural network for the purpose of recognizing violent videos.\nA convolutional neural network is used to extract frame level features from a\nvideo. The frame level features are then aggregated using a variant of the long\nshort term memory that uses convolutional gates. The convolutional neural\nnetwork along with the convolutional long short term memory is capable of\ncapturing localized spatio-temporal features which enables the analysis of\nlocal motion taking place in the video. We also propose to use adjacent frame\ndifferences as the input to the model thereby forcing it to encode the changes\noccurring in the video. The performance of the proposed feature extraction\npipeline is evaluated on three standard benchmark datasets in terms of\nrecognition accuracy. Comparison of the results obtained with the state of the\nart techniques revealed the promising capability of the proposed method in\nrecognizing violent videos.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 16:58:32 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Lanz", "Oswald", ""]]}, {"id": "1709.06532", "submitter": "Xiang Xu", "authors": "Xiang Xu and Pengfei Dou and Ha A. Le and Ioannis A. Kakadiaris", "title": "When 3D-Aided 2D Face Recognition Meets Deep Learning: An extended UR2D\n  for Pose-Invariant Face Recognition", "comments": "Submitted to Special Issue on Biometrics in the Wild, Image and\n  Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the face recognition works focus on specific modules or demonstrate a\nresearch idea. This paper presents a pose-invariant 3D-aided 2D face\nrecognition system (UR2D) that is robust to pose variations as large as 90? by\nleveraging deep learning technology. The architecture and the interface of UR2D\nare described, and each module is introduced in detail. Extensive experiments\nare conducted on the UHDB31 and IJB-A, demonstrating that UR2D outperforms\nexisting 2D face recognition systems such as VGG-Face, FaceNet, and a\ncommercial off-the-shelf software (COTS) by at least 9% on the UHDB31 dataset\nand 3% on the IJB-A dataset on average in face identification tasks. UR2D also\nachieves state-of-the-art performance of 85% on the IJB-A dataset by comparing\nthe Rank-1 accuracy score from template matching. It fills a gap by providing a\n3D-aided 2D face recognition system that has compatible results with 2D face\nrecognition systems using deep learning techniques.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 17:02:15 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Xu", "Xiang", ""], ["Dou", "Pengfei", ""], ["Le", "Ha A.", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1709.06664", "submitter": "Nikolaos Sarafianos", "authors": "Nikolaos Sarafianos, Theodore Giannakopoulos, Christophoros Nikou,\n  Ioannis A. Kakadiaris", "title": "Curriculum Learning of Visual Attribute Clusters for Multi-Task\n  Classification", "comments": "Published in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attributes, from simple objects (e.g., backpacks, hats) to\nsoft-biometrics (e.g., gender, height, clothing) have proven to be a powerful\nrepresentational approach for many applications such as image description and\nhuman identification. In this paper, we introduce a novel method to combine the\nadvantages of both multi-task and curriculum learning in a visual attribute\nclassification framework. Individual tasks are grouped after performing\nhierarchical clustering based on their correlation. The clusters of tasks are\nlearned in a curriculum learning setup by transferring knowledge between\nclusters. The learning process within each cluster is performed in a multi-task\nclassification setup. By leveraging the acquired knowledge, we speed-up the\nprocess and improve performance. We demonstrate the effectiveness of our method\nvia ablation studies and a detailed analysis of the covariates, on a variety of\npublicly available datasets of humans standing with their full-body visible.\nExtensive experimentation has proven that the proposed approach boosts the\nperformance by 4% to 10%.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 22:37:42 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 17:24:08 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 01:17:35 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Sarafianos", "Nikolaos", ""], ["Giannakopoulos", "Theodore", ""], ["Nikou", "Christophoros", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1709.06750", "submitter": "Yi-Hsuan Tsai", "authors": "Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, Ming-Hsuan Yang", "title": "SegFlow: Joint Learning for Video Object Segmentation and Optical Flow", "comments": "Accepted in ICCV'17. Code is available at\n  https://sites.google.com/site/yihsuantsai/research/iccv17-segflow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an end-to-end trainable network, SegFlow, for\nsimultaneously predicting pixel-wise object segmentation and optical flow in\nvideos. The proposed SegFlow has two branches where useful information of\nobject segmentation and optical flow is propagated bidirectionally in a unified\nframework. The segmentation branch is based on a fully convolutional network,\nwhich has been proved effective in image segmentation task, and the optical\nflow branch takes advantage of the FlowNet model. The unified framework is\ntrained iteratively offline to learn a generic notion, and fine-tuned online\nfor specific objects. Extensive experiments on both the video object\nsegmentation and optical flow datasets demonstrate that introducing optical\nflow improves the performance of segmentation and vice versa, against the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 07:38:20 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Cheng", "Jingchun", ""], ["Tsai", "Yi-Hsuan", ""], ["Wang", "Shengjin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1709.06762", "submitter": "Giovanni De Magistris", "authors": "Tadanobu Inoue, Subhajit Chaudhury, Giovanni De Magistris and\n  Sakyasingha Dasgupta", "title": "Transfer learning from synthetic to real images using variational\n  autoencoders for robotic applications", "comments": "video: https://youtu.be/Wd-1WU8emkw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic learning in simulation environments provides a faster, more scalable,\nand safer training methodology than learning directly with physical robots.\nAlso, synthesizing images in a simulation environment for collecting\nlarge-scale image data is easy, whereas capturing camera images in the real\nworld is time consuming and expensive. However, learning from only synthetic\nimages may not achieve the desired performance in real environments due to the\ngap between synthetic and real images. We thus propose a method that transfers\nlearned capability of detecting object position from a simulation environment\nto the real world. Our method enables us to use only a very limited dataset of\nreal images while leveraging a large dataset of synthetic images using multiple\nvariational autoencoders. It detects object positions 6 to 7 times more\nprecisely than the baseline of directly learning from the dataset of the real\nimages. Object position estimation under varying environmental conditions forms\none of the underlying requirement for standard robotic manipulation tasks. We\nshow that the proposed method performs robustly in different lighting\nconditions or with other distractor objects present for this requirement. Using\nthis detected object position, we transfer pick-and-place or reaching tasks\nlearned in a simulation environment to an actual physical robot without\nre-training.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 08:18:07 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Inoue", "Tadanobu", ""], ["Chaudhury", "Subhajit", ""], ["De Magistris", "Giovanni", ""], ["Dasgupta", "Sakyasingha", ""]]}, {"id": "1709.06764", "submitter": "Andres Milioto", "authors": "Andres Milioto, Philipp Lottes, Cyrill Stachniss", "title": "Real-time Semantic Segmentation of Crop and Weed for Precision\n  Agriculture Robots Leveraging Background Knowledge in CNNs", "comments": "Accepted for publication at IEEE International Conference on Robotics\n  and Automation 2018 (ICRA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision farming robots, which target to reduce the amount of herbicides\nthat need to be brought out in the fields, must have the ability to identify\ncrops and weeds in real time to trigger weeding actions. In this paper, we\naddress the problem of CNN-based semantic segmentation of crop fields\nseparating sugar beet plants, weeds, and background solely based on RGB data.\nWe propose a CNN that exploits existing vegetation indexes and provides a\nclassification in real time. Furthermore, it can be effectively re-trained to\nso far unseen fields with a comparably small amount of training data. We\nimplemented and thoroughly evaluated our system on a real agricultural robot\noperating in different fields in Germany and Switzerland. The results show that\nour system generalizes well, can operate at around 20Hz, and is suitable for\nonline operation in the fields.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 08:24:18 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 15:46:48 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Milioto", "Andres", ""], ["Lottes", "Philipp", ""], ["Stachniss", "Cyrill", ""]]}, {"id": "1709.06770", "submitter": "Yongyi Tang", "authors": "Yongyi Tang, Peizhen Zhang, Jian-Fang Hu and Wei-Shi Zheng", "title": "Latent Embeddings for Collective Activity Recognition", "comments": "6pages, accepted by IEEE-AVSS2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rather than simply recognizing the action of a person individually,\ncollective activity recognition aims to find out what a group of people is\nacting in a collective scene. Previ- ous state-of-the-art methods using\nhand-crafted potentials in conventional graphical model which can only define a\nlimited range of relations. Thus, the complex structural de- pendencies among\nindividuals involved in a collective sce- nario cannot be fully modeled. In\nthis paper, we overcome these limitations by embedding latent variables into\nfeature space and learning the feature mapping functions in a deep learning\nframework. The embeddings of latent variables build a global relation\ncontaining person-group interac- tions and richer contextual information by\njointly modeling broader range of individuals. Besides, we assemble atten- tion\nmechanism during embedding for achieving more com- pact representations. We\nevaluate our method on three col- lective activity datasets, where we\ncontribute a much larger dataset in this work. The proposed model has achieved\nclearly better performance as compared to the state-of-the- art methods in our\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 08:46:53 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Tang", "Yongyi", ""], ["Zhang", "Peizhen", ""], ["Hu", "Jian-Fang", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1709.06818", "submitter": "Zhibin Niu", "authors": "Yan Ji, Licheng Liu, Hongcui Wang, Zhilei Liu, Zhibin Niu, Bruce Denby", "title": "Updating the silent speech challenge benchmark with deep learning", "comments": "25 pages, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2010 Silent Speech Challenge benchmark is updated with new results\nobtained in a Deep Learning strategy, using the same input features and\ndecoding strategy as in the original article. A Word Error Rate of 6.4% is\nobtained, compared to the published value of 17.4%. Additional results\ncomparing new auto-encoder-based features with the original features at reduced\ndimensionality, as well as decoding scenarios on two different language models,\nare also presented. The Silent Speech Challenge archive has been updated to\ncontain both the original and the new auto-encoder features, in addition to the\noriginal raw data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 11:28:40 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Ji", "Yan", ""], ["Liu", "Licheng", ""], ["Wang", "Hongcui", ""], ["Liu", "Zhilei", ""], ["Niu", "Zhibin", ""], ["Denby", "Bruce", ""]]}, {"id": "1709.06841", "submitter": "Ruihao Li", "authors": "Ruihao Li, Sen Wang, Zhiqiang Long and Dongbing Gu", "title": "UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning", "comments": "6 pages, 6 figures, Accepted by ICRA18. Video:\n  (https://www.youtube.com/watch?v=5RdjO93wJqo) Website:\n  (http://senwang.gitlab.io/UnDeepVO/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel monocular visual odometry (VO) system called UnDeepVO in\nthis paper. UnDeepVO is able to estimate the 6-DoF pose of a monocular camera\nand the depth of its view by using deep neural networks. There are two salient\nfeatures of the proposed UnDeepVO: one is the unsupervised deep learning\nscheme, and the other is the absolute scale recovery. Specifically, we train\nUnDeepVO by using stereo image pairs to recover the scale but test it by using\nconsecutive monocular images. Thus, UnDeepVO is a monocular system. The loss\nfunction defined for training the networks is based on spatial and temporal\ndense information. A system overview is shown in Fig. 1. The experiments on\nKITTI dataset show our UnDeepVO achieves good performance in terms of pose\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 12:54:26 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 14:44:30 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Li", "Ruihao", ""], ["Wang", "Sen", ""], ["Long", "Zhiqiang", ""], ["Gu", "Dongbing", ""]]}, {"id": "1709.06868", "submitter": "Kripasindhu Sarkar", "authors": "Kripasindhu Sarkar, Kiran Varanasi and Didier Stricker", "title": "Learning quadrangulated patches for 3D shape parameterization and\n  completion", "comments": "To be presented at International Conference on 3D Vision 2017, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3D shape parameterization by surface patches, that are\noriented by 3D mesh quadrangulation of the shape. By encoding 3D surface detail\non local patches, we learn a patch dictionary that identifies principal surface\nfeatures of the shape. Unlike previous methods, we are able to encode surface\npatches of variable size as determined by the user. We propose novel methods\nfor dictionary learning and patch reconstruction based on the query of a noisy\ninput patch with holes. We evaluate the patch dictionary towards various\napplications in 3D shape inpainting, denoising and compression. Our method is\nable to predict missing vertices and inpaint moderately sized holes. We\ndemonstrate a complete pipeline for reconstructing the 3D mesh from the patch\nencoding. We validate our shape parameterization and reconstruction methods on\nboth synthetic shapes and real world scans. We show that our patch dictionary\nperforms successful shape completion of complicated surface textures.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 13:56:34 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Sarkar", "Kripasindhu", ""], ["Varanasi", "Kiran", ""], ["Stricker", "Didier", ""]]}, {"id": "1709.06871", "submitter": "Chris Bleakley", "authors": "Philip J. Corr, Guenole C. Silvestre and Chris J. Bleakley", "title": "Open Source Dataset and Deep Learning Models for Online Digit Gesture\n  Recognition on Touchscreens", "comments": "Irish Machine Vision and Image Processing Conference (IMVIP) 2017,\n  Maynooth, Ireland, 30 August-1 September 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an evaluation of deep neural networks for recognition of\ndigits entered by users on a smartphone touchscreen. A new large dataset of\nArabic numerals was collected for training and evaluation of the network. The\ndataset consists of spatial and temporal touch data recorded for 80 digits\nentered by 260 users. Two neural network models were investigated. The first\nmodel was a 2D convolutional neural (ConvNet) network applied to bitmaps of the\nglpyhs created by interpolation of the sensed screen touches and its topology\nis similar to that of previously published models for offline handwriting\nrecognition from scanned images. The second model used a 1D ConvNet\narchitecture but was applied to the sequence of polar vectors connecting the\ntouch points. The models were found to provide accuracies of 98.50% and 95.86%,\nrespectively. The second model was much simpler, providing a reduction in the\nnumber of parameters from 1,663,370 to 287,690. The dataset has been made\navailable to the community as an open source resource.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 14:02:55 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Corr", "Philip J.", ""], ["Silvestre", "Guenole C.", ""], ["Bleakley", "Chris J.", ""]]}, {"id": "1709.07065", "submitter": "Wenqian Liu", "authors": "Wenqian Liu, Octavia Camps, Mario Sznaier", "title": "Multi-camera Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a pipeline for multi-target visual tracking under\nmulti-camera system. For multi-camera system tracking problem, efficient data\nassociation across cameras, and at the same time, across frames becomes more\nimportant than single-camera system tracking. However, most of the multi-camera\ntracking algorithms emphasis on single camera across frame data association.\nThus in our work, we model our tracking problem as a global graph, and adopt\nGeneralized Maximum Multi Clique optimization problem as our core algorithm to\ntake both across frame and across camera data correlation into account all\ntogether. Furthermore, in order to compute good similarity scores as the input\nof our graph model, we extract both appearance and dynamic motion similarities.\nFor appearance feature, Local Maximal Occurrence Representation(LOMO) feature\nextraction algorithm for ReID is conducted. When it comes to capturing the\ndynamic information, we build Hankel matrix for each tracklet of target and\napply rank estimation with Iterative Hankel Total Least Squares(IHTLS)\nalgorithm to it. We evaluate our tracker on the challenging Terrace Sequences\nfrom EPFL CVLAB as well as recently published Duke MTMC dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 20:08:16 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Liu", "Wenqian", ""], ["Camps", "Octavia", ""], ["Sznaier", "Mario", ""]]}, {"id": "1709.07077", "submitter": "Yihui He", "authors": "Yihui He", "title": "Estimated Depth Map Helps Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider image classification with estimated depth. This problem falls\ninto the domain of transfer learning, since we are using a model trained on a\nset of depth images to generate depth maps (additional features) for use in\nanother classification problem using another disjoint set of images. It's\nchallenging as no direct depth information is provided. Though depth estimation\nhas been well studied, none have attempted to aid image classification with\nestimated depth. Therefore, we present a way of transferring domain knowledge\non depth estimation to a separate image classification task over a disjoint set\nof train, and test data. We build a RGBD dataset based on RGB dataset and do\nimage classification on it. Then evaluation the performance of neural networks\non the RGBD dataset compared to the RGB dataset. From our experiments, the\nbenefit is significant with shallow and deep networks. It improves ResNet-20 by\n0.55% and ResNet-56 by 0.53%. Our code and dataset are available publicly.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 20:39:47 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["He", "Yihui", ""]]}, {"id": "1709.07158", "submitter": "Trung Pham", "authors": "Trung Pham, Thanh-Toan Do, Niko S\\\"underhauf and Ian Reid", "title": "SceneCut: Joint Geometric and Object Segmentation for Indoor Scenes", "comments": "Published in ICRA 2018", "journal-ref": "ICRA 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents SceneCut, a novel approach to jointly discover previously\nunseen objects and non-object surfaces using a single RGB-D image. SceneCut's\njoint reasoning over scene semantics and geometry allows a robot to detect and\nsegment object instances in complex scenes where modern deep learning-based\nmethods either fail to separate object instances, or fail to detect objects\nthat were not seen during training. SceneCut automatically decomposes a scene\ninto meaningful regions which either represent objects or scene surfaces. The\ndecomposition is qualified by an unified energy function over objectness and\ngeometric fitting. We show how this energy function can be optimized\nefficiently by utilizing hierarchical segmentation trees. Moreover, we leverage\na pre-trained convolutional oriented boundary network to predict accurate\nboundaries from images, which are used to construct high-quality region\nhierarchies. We evaluate SceneCut on several different indoor environments, and\nthe results show that SceneCut significantly outperforms all the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 05:08:35 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 06:44:56 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Pham", "Trung", ""], ["Do", "Thanh-Toan", ""], ["S\u00fcnderhauf", "Niko", ""], ["Reid", "Ian", ""]]}, {"id": "1709.07166", "submitter": "Benjamin Johnston", "authors": "Benjamin Johnston, Alistair McEwan and Philip de Chazal", "title": "Semi-Automated Nasal PAP Mask Sizing using Facial Photographs", "comments": "4 pages, 3 figures, 4 tables, IEEE Engineering Medicine and Biology\n  Conference 2017", "journal-ref": null, "doi": "10.1109/EMBC.2017.8037049", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a semi-automated system for sizing nasal Positive Airway Pressure\n(PAP) masks based upon a neural network model that was trained with facial\nphotographs of both PAP mask users and non-users. It demonstrated an accuracy\nof 72% in correctly sizing a mask and 96% accuracy sizing to within 1 mask size\ngroup. The semi-automated system performed comparably to sizing from manual\nmeasurements taken from the same images which produced 89% and 100% accuracy\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 06:08:58 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Johnston", "Benjamin", ""], ["McEwan", "Alistair", ""], ["de Chazal", "Philip", ""]]}, {"id": "1709.07192", "submitter": "Yikang Li", "authors": "Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang", "title": "Visual Question Generation as Dual Task of Visual Question Answering", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently visual question answering (VQA) and visual question generation (VQG)\nare two trending topics in the computer vision, which have been explored\nseparately. In this work, we propose an end-to-end unified framework, the\nInvertible Question Answering Network (iQAN), to leverage the complementary\nrelations between questions and answers in images by jointly training the model\non VQA and VQG tasks. Corresponding parameter sharing scheme and regular terms\nare proposed as constraints to explicitly leverage Q,A's dependencies to guide\nthe training process. After training, iQAN can take either question or answer\nas input, then output the counterpart. Evaluated on the large-scale visual\nquestion answering datasets CLEVR and VQA2, our iQAN improves the VQA accuracy\nover the baselines. We also show the dual learning framework of iQAN can be\ngeneralized to other VQA architectures and consistently improve the results\nover both the VQA and VQG tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 08:04:48 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Li", "Yikang", ""], ["Duan", "Nan", ""], ["Zhou", "Bolei", ""], ["Chu", "Xiao", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1709.07200", "submitter": "Valentin Vielzeuf", "authors": "Valentin Vielzeuf, St\\'ephane Pateux, Fr\\'ed\\'eric Jurie", "title": "Temporal Multimodal Fusion for Video Emotion Classification in the Wild", "comments": null, "journal-ref": "ACM - ICMI 2017, Nov 2017, Glasgow, United Kingdom", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the question of emotion classification. The task\nconsists in predicting emotion labels (taken among a set of possible labels)\nbest describing the emotions contained in short video clips. Building on a\nstandard framework -- lying in describing videos by audio and visual features\nused by a supervised classifier to infer the labels -- this paper investigates\nseveral novel directions. First of all, improved face descriptors based on 2D\nand 3D Convo-lutional Neural Networks are proposed. Second, the paper explores\nseveral fusion methods, temporal and multimodal, including a novel hierarchical\nmethod combining features and scores. In addition, we carefully reviewed the\ndifferent stages of the pipeline and designed a CNN architecture adapted to the\ntask; this is important as the size of the training set is small compared to\nthe difficulty of the problem, making generalization difficult. The so-obtained\nmodel ranked 4th at the 2017 Emotion in the Wild challenge with the accuracy of\n58.8 %.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 08:14:40 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Vielzeuf", "Valentin", ""], ["Pateux", "St\u00e9phane", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1709.07212", "submitter": "Ruobing Shen", "authors": "Ruobing Shen and Gerhard Reinelt and St\\'ephane Canu", "title": "A First Derivative Potts Model for Segmentation and Denoising Using ILP", "comments": "6 pages, 2 figures. To appear at Proceedings of International\n  Conference on Operations Research 2017, Berlin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image segmentation and denoising are two fundamental tasks in\nimage processing. Usually, graph based models such as multicut are used for\nsegmentation and variational models are employed for denoising. Our approach\naddresses both problems at the same time. We propose a novel ILP formulation of\nthe first derivative Potts model with the $\\ell_1$ data term, where binary\nvariables are introduced to deal with the $\\ell_0$ norm of the regularization\nterm. The ILP is then solved by a standard off-the-shelf MIP solver. Numerical\nexperiments are compared with the multicut problem.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 08:42:00 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 15:52:59 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Shen", "Ruobing", ""], ["Reinelt", "Gerhard", ""], ["Canu", "St\u00e9phane", ""]]}, {"id": "1709.07220", "submitter": "Cuiling Lan", "authors": "Ke Sun, Cuiling Lan, Junliang Xing, Wenjun Zeng, Dong Liu, Jingdong\n  Wang", "title": "Human Pose Estimation using Global and Local Normalization", "comments": "ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of estimating the positions of human\njoints, i.e., articulated pose estimation. Recent state-of-the-art solutions\nmodel two key issues, joint detection and spatial configuration refinement,\ntogether using convolutional neural networks. Our work mainly focuses on\nspatial configuration refinement by reducing variations of human poses\nstatistically, which is motivated by the observation that the scattered\ndistribution of the relative locations of joints e.g., the left wrist is\ndistributed nearly uniformly in a circular area around the left shoulder) makes\nthe learning of convolutional spatial models hard. We present a two-stage\nnormalization scheme, human body normalization and limb normalization, to make\nthe distribution of the relative joint locations compact, resulting in easier\nlearning of convolutional spatial models and more accurate pose estimation. In\naddition, our empirical results show that incorporating multi-scale supervision\nand multi-scale fusion into the joint detection network is beneficial.\nExperiment results demonstrate that our method consistently outperforms\nstate-of-the-art methods on the benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 09:13:31 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Sun", "Ke", ""], ["Lan", "Cuiling", ""], ["Xing", "Junliang", ""], ["Zeng", "Wenjun", ""], ["Liu", "Dong", ""], ["Wang", "Jingdong", ""]]}, {"id": "1709.07223", "submitter": "Roarke Horstmeyer", "authors": "Roarke Horstmeyer, Richard Y. Chen, Barbara Kappes and Benjamin\n  Judkewitz", "title": "Convolutional neural networks that teach microscopes how to image", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms offer a powerful means to automatically analyze the\ncontent of medical images. However, many biological samples of interest are\nprimarily transparent to visible light and contain features that are difficult\nto resolve with a standard optical microscope. Here, we use a convolutional\nneural network (CNN) not only to classify images, but also to optimize the\nphysical layout of the imaging device itself. We increase the classification\naccuracy of a microscope's recorded images by merging an optical model of image\nformation into the pipeline of a CNN. The resulting network simultaneously\ndetermines an ideal illumination arrangement to highlight important sample\nfeatures during image acquisition, along with a set of convolutional weights to\nclassify the detected images post-capture. We demonstrate our joint\noptimization technique with an experimental microscope configuration that\nautomatically identifies malaria-infected cells with 5-10% higher accuracy than\nstandard and alternative microscope lighting designs.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 09:17:47 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Horstmeyer", "Roarke", ""], ["Chen", "Richard Y.", ""], ["Kappes", "Barbara", ""], ["Judkewitz", "Benjamin", ""]]}, {"id": "1709.07244", "submitter": "Daniele Faccio", "authors": "Piergiorgio Caramazza, Alessandro Boccolini, Daniel Buschek, Matthias\n  Hullin, Catherine Higham, Robert Henderson, Roderick Murray-Smith, Daniele\n  Faccio", "title": "Neural network identification of people hidden from view with a\n  single-pixel, single-photon detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light scattered from multiple surfaces can be used to retrieve information of\nhidden environments. However, full three-dimensional retrieval of an object\nhidden from view by a wall has only been achieved with scanning systems and\nrequires intensive computational processing of the retrieved data. Here we use\na non-scanning, single-photon single-pixel detector in combination with an\nartificial neural network: this allows us to locate the position and to also\nsimultaneously provide the actual identity of a hidden person, chosen from a\ndatabase of people (N=3). Artificial neural networks applied to specific\ncomputational imaging problems can therefore enable novel imaging capabilities\nwith hugely simplified hardware and processing times\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 10:12:06 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Caramazza", "Piergiorgio", ""], ["Boccolini", "Alessandro", ""], ["Buschek", "Daniel", ""], ["Hullin", "Matthias", ""], ["Higham", "Catherine", ""], ["Henderson", "Robert", ""], ["Murray-Smith", "Roderick", ""], ["Faccio", "Daniele", ""]]}, {"id": "1709.07267", "submitter": "Olivier Colliot", "authors": "Jorge Samper-Gonz\\'alez, Ninon Burgos, Sabrina Fontanella, Hugo\n  Bertin, Marie-Odile Habert, Stanley Durrleman, Theodoros Evgeniou, Olivier\n  Colliot", "title": "Yet Another ADNI Machine Learning Paper? Paving The Way Towards\n  Fully-reproducible Research on Classification of Alzheimer's Disease", "comments": null, "journal-ref": "Proc. Machine Learning in Medical Imaging MLMI 2017, MICCAI\n  Worskhop, Lecture Notes in Computer Science, volume 10541, pp 53-60, Springer", "doi": "10.1007/978-3-319-67389-9_7", "report-no": null, "categories": "stat.ML cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the number of papers on Alzheimer's disease classification\nhas increased dramatically, generating interesting methodological ideas on the\nuse machine learning and feature extraction methods. However, practical impact\nis much more limited and, eventually, one could not tell which of these\napproaches are the most efficient. While over 90\\% of these works make use of\nADNI an objective comparison between approaches is impossible due to variations\nin the subjects included, image pre-processing, performance metrics and\ncross-validation procedures. In this paper, we propose a framework for\nreproducible classification experiments using multimodal MRI and PET data from\nADNI. The core components are: 1) code to automatically convert the full ADNI\ndatabase into BIDS format; 2) a modular architecture based on Nipype in order\nto easily plug-in different classification and feature extraction tools; 3)\nfeature extraction pipelines for MRI and PET data; 4) baseline classification\napproaches for unimodal and multimodal features. This provides a flexible\nframework for benchmarking different feature extraction and classification\ntools in a reproducible manner. We demonstrate its use on all (1519) baseline\nT1 MR images and all (1102) baseline FDG PET images from ADNI 1, GO and 2 with\nSPM-based feature extraction pipelines and three different classification\ntechniques (linear SVM, anatomically regularized SVM and multiple kernel\nlearning SVM). The highest accuracies achieved were: 91% for AD vs CN, 83% for\nMCIc vs CN, 75% for MCIc vs MCInc, 94% for AD-A$\\beta$+ vs CN-A$\\beta$- and 72%\nfor MCIc-A$\\beta$+ vs MCInc-A$\\beta$+. The code is publicly available at\nhttps://gitlab.icm-institute.org/aramislab/AD-ML (depends on the Clinica\nsoftware platform, publicly available at http://www.clinica.run).\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 11:37:01 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Samper-Gonz\u00e1lez", "Jorge", ""], ["Burgos", "Ninon", ""], ["Fontanella", "Sabrina", ""], ["Bertin", "Hugo", ""], ["Habert", "Marie-Odile", ""], ["Durrleman", "Stanley", ""], ["Evgeniou", "Theodoros", ""], ["Colliot", "Olivier", ""]]}, {"id": "1709.07322", "submitter": "Stephan R Richter", "authors": "Stephan R. Richter and Zeeshan Hayder and Vladlen Koltun", "title": "Playing for Benchmarks", "comments": "Published at the International Conference on Computer Vision (ICCV\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a benchmark suite for visual perception. The benchmark is based on\nmore than 250K high-resolution video frames, all annotated with ground-truth\ndata for both low-level and high-level vision tasks, including optical flow,\nsemantic instance segmentation, object detection and tracking, object-level 3D\nscene layout, and visual odometry. Ground-truth data for all tasks is available\nfor every frame. The data was collected while driving, riding, and walking a\ntotal of 184 kilometers in diverse ambient conditions in a realistic virtual\nworld. To create the benchmark, we have developed a new approach to collecting\nground-truth data from simulated worlds without access to their source code or\ncontent. We conduct statistical analyses that show that the composition of the\nscenes in the benchmark closely matches the composition of corresponding\nphysical environments. The realism of the collected data is further validated\nvia perceptual experiments. We analyze the performance of state-of-the-art\nmethods for multiple tasks, providing reference baselines and highlighting\nchallenges for future research. The supplementary video can be viewed at\nhttps://youtu.be/T9OybWv923Y\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 13:44:47 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Richter", "Stephan R.", ""], ["Hayder", "Zeeshan", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1709.07326", "submitter": "Anh Nguyen", "authors": "Thanh-Toan Do, Anh Nguyen, Ian Reid", "title": "AffordanceNet: An End-to-End Deep Learning Approach for Object\n  Affordance Detection", "comments": "In ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose AffordanceNet, a new deep learning approach to simultaneously\ndetect multiple objects and their affordances from RGB images. Our\nAffordanceNet has two branches: an object detection branch to localize and\nclassify the object, and an affordance detection branch to assign each pixel in\nthe object to its most probable affordance label. The proposed framework\nemploys three key components for effectively handling the multiclass problem in\nthe affordance mask: a sequence of deconvolutional layers, a robust resizing\nstrategy, and a multi-task loss function. The experimental results on the\npublic datasets show that our AffordanceNet outperforms recent state-of-the-art\nmethods by a fair margin, while its end-to-end architecture allows the\ninference at the speed of 150ms per image. This makes our AffordanceNet well\nsuitable for real-time robotic applications. Furthermore, we demonstrate the\neffectiveness of AffordanceNet in different testing environments and in real\nrobotic applications. The source code is available at\nhttps://github.com/nqanh/affordance-net\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 13:48:49 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 22:27:40 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2018 14:04:29 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Nguyen", "Anh", ""], ["Reid", "Ian", ""]]}, {"id": "1709.07330", "submitter": "Hao Chen", "authors": "Xiaomeng Li, Hao Chen, Xiaojuan Qi, Qi Dou, Chi-Wing Fu, Pheng Ann\n  Heng", "title": "H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor\n  Segmentation from CT Volumes", "comments": "Accept for publication at IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liver cancer is one of the leading causes of cancer death. To assist doctors\nin hepatocellular carcinoma diagnosis and treatment planning, an accurate and\nautomatic liver and tumor segmentation method is highly demanded in the\nclinical practice. Recently, fully convolutional neural networks (FCNs),\nincluding 2D and 3D FCNs, serve as the back-bone in many volumetric image\nsegmentation. However, 2D convolutions can not fully leverage the spatial\ninformation along the third dimension while 3D convolutions suffer from high\ncomputational cost and GPU memory consumption. To address these issues, we\npropose a novel hybrid densely connected UNet (H-DenseUNet), which consists of\na 2D DenseUNet for efficiently extracting intra-slice features and a 3D\ncounterpart for hierarchically aggregating volumetric contexts under the spirit\nof the auto-context algorithm for liver and tumor segmentation. We formulate\nthe learning process of H-DenseUNet in an end-to-end manner, where the\nintra-slice representations and inter-slice features can be jointly optimized\nthrough a hybrid feature fusion (HFF) layer. We extensively evaluated our\nmethod on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge\nand 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the\nsegmentation results of tumors and achieved very competitive performance for\nliver segmentation even with a single model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 13:58:21 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 14:47:10 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 13:00:48 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Li", "Xiaomeng", ""], ["Chen", "Hao", ""], ["Qi", "Xiaojuan", ""], ["Dou", "Qi", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "1709.07337", "submitter": "Julian Yarkony", "authors": "Chong Zhang, Shaofei Wang, Miguel A. Gonzalez-Ballester, Julian\n  Yarkony", "title": "Efficient Column Generation for Cell Detection and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of instance segmentation in biological images with\ncrowded and compact cells. We formulate this task as an integer program where\nvariables correspond to cells and constraints enforce that cells do not\noverlap. To solve this integer program, we propose a column generation\nformulation where the pricing program is solved via exact optimization of very\nsmall scale integer programs. Column generation is tightened using odd set\ninequalities which fit elegantly into pricing problem optimization. Our column\ngeneration approach achieves fast stable anytime inference for our instance\nsegmentation problems. We demonstrate on three distinct light microscopy\ndatasets, with several hundred cells each, that our proposed algorithm rapidly\nachieves or exceeds state of the art accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 14:15:23 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Zhang", "Chong", ""], ["Wang", "Shaofei", ""], ["Gonzalez-Ballester", "Miguel A.", ""], ["Yarkony", "Julian", ""]]}, {"id": "1709.07359", "submitter": "Lucas C. Uzal", "authors": "Guillermo L. Grinblat, Lucas C. Uzal and Pablo M. Granitto", "title": "Class-Splitting Generative Adversarial Networks", "comments": "Under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) produce systematically better quality\nsamples when class label information is provided., i.e. in the conditional GAN\nsetup. This is still observed for the recently proposed Wasserstein GAN\nformulation which stabilized adversarial training and allows considering high\ncapacity network architectures such as ResNet. In this work we show how to\nboost conditional GAN by augmenting available class labels. The new classes\ncome from clustering in the representation space learned by the same GAN model.\nThe proposed strategy is also feasible when no class information is available,\ni.e. in the unsupervised setup. Our generated samples reach state-of-the-art\nInception scores for CIFAR-10 and STL-10 datasets in both supervised and\nunsupervised setup.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 14:55:54 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 14:07:35 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Grinblat", "Guillermo L.", ""], ["Uzal", "Lucas C.", ""], ["Granitto", "Pablo M.", ""]]}, {"id": "1709.07368", "submitter": "Charalambos Poullis", "authors": "Yuanlie He, Sudhir Mudur, and Charalambos Poullis", "title": "Multi-label Pixelwise Classification for Reconstruction of Large-scale\n  Urban Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object classification is one of the many holy grails in computer vision and\nas such has resulted in a very large number of algorithms being proposed\nalready. Specifically in recent years there has been considerable progress in\nthis area primarily due to the increased efficiency and accessibility of deep\nlearning techniques. In fact, for single-label object classification [i.e. only\none object present in the image] the state-of-the-art techniques employ deep\nneural networks and are reporting very close to human-like performance. There\nare specialized applications in which single-label object-level classification\nwill not suffice; for example in cases where the image contains multiple\nintertwined objects of different labels.\n  In this paper, we address the complex problem of multi-label pixelwise\nclassification. We present our distinct solution based on a convolutional\nneural network (CNN) for performing multi-label pixelwise classification and\nits application to large-scale urban reconstruction. A supervised learning\napproach is followed for training a 13-layer CNN using both LiDAR and satellite\nimages. An empirical study has been conducted to determine the hyperparameters\nwhich result in the optimal performance of the CNN. Scale invariance is\nintroduced by training the network on five different scales of the input and\nlabeled data. This results in six pixelwise classifications for each different\nscale. An SVM is then trained to map the six pixelwise classifications into a\nsingle-label. Lastly, we refine boundary pixel labels using graph-cuts for\nmaximum a-posteriori (MAP) estimation with Markov Random Field (MRF) priors.\nThe resulting pixelwise classification is then used to accurately extract and\nreconstruct the buildings in large-scale urban areas. The proposed approach has\nbeen extensively tested and the results are reported.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 15:13:09 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 21:32:13 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["He", "Yuanlie", ""], ["Mudur", "Sudhir", ""], ["Poullis", "Charalambos", ""]]}, {"id": "1709.07383", "submitter": "Michael Kampffmeyer", "authors": "Michael Kampffmeyer, Arnt-B{\\o}rre Salberg, Robert Jenssen", "title": "Urban Land Cover Classification with Missing Data Modalities Using Deep\n  Convolutional Neural Networks", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Applied Earth Observations and\n  Remote Sensing 2018", "doi": "10.1109/JSTARS.2018.2834961", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic urban land cover classification is a fundamental problem in remote\nsensing, e.g. for environmental monitoring. The problem is highly challenging,\nas classes generally have high inter-class and low intra-class variance.\nTechniques to improve urban land cover classification performance in remote\nsensing include fusion of data from different sensors with different data\nmodalities. However, such techniques require all modalities to be available to\nthe classifier in the decision-making process, i.e. at test time, as well as in\ntraining. If a data modality is missing at test time, current state-of-the-art\napproaches have in general no procedure available for exploiting information\nfrom these modalities. This represents a waste of potentially useful\ninformation. We propose as a remedy a convolutional neural network (CNN)\narchitecture for urban land cover classification which is able to embed all\navailable training modalities in a so-called hallucination network. The network\nwill in effect replace missing data modalities in the test phase, enabling\nfusion capabilities even when data modalities are missing in testing. We\ndemonstrate the method using two datasets consisting of optical and digital\nsurface model (DSM) images. We simulate missing modalities by assuming that DSM\nimages are missing during testing. Our method outperforms both standard CNNs\ntrained only on optical images as well as an ensemble of two standard CNNs. We\nfurther evaluate the potential of our method to handle situations where only\nsome DSM images are missing during testing. Overall, we show that we can\nclearly exploit training time information of the missing modality during\ntesting.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 15:46:27 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 20:44:16 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Kampffmeyer", "Michael", ""], ["Salberg", "Arnt-B\u00f8rre", ""], ["Jenssen", "Robert", ""]]}, {"id": "1709.07429", "submitter": "Inzamam Anwar", "authors": "Inzamam Anwar and Naeem Ul Islam", "title": "Learned Features are better for Ethnicity Classification", "comments": "15 pages, 8 figures, 2 tables, code and framework available on\n  request", "journal-ref": "Cybernetics and Information Technologies, vol. 17, no. 3, pp.\n  152-164, Sep., 2017", "doi": "10.1515/cait-2017-0036", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethnicity is a key demographic attribute of human beings and it plays a vital\nrole in automatic facial recognition and have extensive real world applications\nsuch as Human Computer Interaction (HCI); demographic based classification;\nbiometric based recognition; security and defense to name a few. In this paper\nwe present a novel approach for extracting ethnicity from the facial images.\nThe proposed method makes use of a pre trained Convolutional Neural Network\n(CNN) to extract the features and then Support Vector Machine (SVM) with linear\nkernel is used as a classifier. This technique uses translational invariant\nhierarchical features learned by the network, in contrast to previous works,\nwhich use hand crafted features such as Local Binary Pattern (LBP); Gabor etc.\nThorough experiments are presented on ten different facial databases which\nstrongly suggest that our approach is robust to different expressions and\nilluminations conditions. Here we consider ethnicity classification as a three\nclass problem including Asian, African-American and Caucasian. Average\nclassification accuracy over all databases is 98.28%, 99.66% and 99.05% for\nAsian, African-American and Caucasian respectively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 17:45:41 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 11:39:22 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Anwar", "Inzamam", ""], ["Islam", "Naeem Ul", ""]]}, {"id": "1709.07492", "submitter": "Fangchang Ma", "authors": "Fangchang Ma, Sertac Karaman", "title": "Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single\n  Image", "comments": "accepted to ICRA 2018. 8 pages, 8 figures, 3 tables. Video at\n  https://www.youtube.com/watch?v=vNIIT_M7x7Y. Code at\n  https://github.com/fangchangma/sparse-to-dense", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of dense depth prediction from a sparse set of depth\nmeasurements and a single RGB image. Since depth estimation from monocular\nimages alone is inherently ambiguous and unreliable, to attain a higher level\nof robustness and accuracy, we introduce additional sparse depth samples, which\nare either acquired with a low-resolution depth sensor or computed via visual\nSimultaneous Localization and Mapping (SLAM) algorithms. We propose the use of\na single deep regression network to learn directly from the RGB-D raw data, and\nexplore the impact of number of depth samples on prediction accuracy. Our\nexperiments show that, compared to using only RGB images, the addition of 100\nspatially random depth samples reduces the prediction root-mean-square error by\n50% on the NYU-Depth-v2 indoor dataset. It also boosts the percentage of\nreliable prediction from 59% to 92% on the KITTI dataset. We demonstrate two\napplications of the proposed algorithm: a plug-in module in SLAM to convert\nsparse maps to dense maps, and super-resolution for LiDARs. Software and video\ndemonstration are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 18:50:04 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 04:16:14 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Ma", "Fangchang", ""], ["Karaman", "Sertac", ""]]}, {"id": "1709.07502", "submitter": "Akshay Rangesh", "authors": "Akshay Rangesh, Kevan Yuen, Ravi Kumar Satzoda, Rakesh Nattoji\n  Rajaram, Pujitha Gunaratne, Mohan M. Trivedi", "title": "A Multimodal, Full-Surround Vehicular Testbed for Naturalistic Studies\n  and Benchmarking: Design, Calibration and Deployment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in autonomous and semi-autonomous driving has been made\npossible in part through an assortment of sensors that provide the intelligent\nagent with an enhanced perception of its surroundings. It has been clear for\nquite some while now that for intelligent vehicles to function effectively in\nall situations and conditions, a fusion of different sensor technologies is\nessential. Consequently, the availability of synchronized multi-sensory data\nstreams are necessary to promote the development of fusion based algorithms for\nlow, mid and high level semantic tasks. In this paper, we provide a\ncomprehensive description of LISA-A: our heavily sensorized, full-surround\ntestbed capable of providing high quality data from a slew of synchronized and\ncalibrated sensors such as cameras, LIDARs, radars, and the IMU/GPS. The\nvehicle has recorded over 100 hours of real world data for a very diverse set\nof weather, traffic and daylight conditions. All captured data is accurately\ncalibrated and synchronized using timestamps, and stored safely in high\nperformance servers mounted inside the vehicle itself. Details on the testbed\ninstrumentation, sensor layout, sensor outputs, calibration and synchronization\nare described in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 19:40:06 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 21:57:13 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 00:05:16 GMT"}, {"version": "v4", "created": "Wed, 30 Jan 2019 23:15:14 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Rangesh", "Akshay", ""], ["Yuen", "Kevan", ""], ["Satzoda", "Ravi Kumar", ""], ["Rajaram", "Rakesh Nattoji", ""], ["Gunaratne", "Pujitha", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1709.07551", "submitter": "Qiuyu Chen", "authors": "Qiuyu Chen, Ryoma Bise, Lin Gu, Yinqiang Zheng, Imari Sato, Jenq-Neng\n  Hwang, Nobuaki Imanishi and Sadakazu Aiso", "title": "Virtual Blood Vessels in Complex Background using Stereo X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully automatic system to reconstruct and visualize 3D blood\nvessels in Augmented Reality (AR) system from stereo X-ray images with bones\nand body fat. Currently, typical 3D imaging technologies are expensive and\ncarrying the risk of irradiation exposure. To reduce the potential harm, we\nonly need to take two X-ray images before visualizing the vessels. Our system\ncan effectively reconstruct and visualize vessels in following steps. We first\nconduct initial segmentation using Markov Random Field and then refine\nsegmentation in an entropy based post-process. We parse the segmented vessels\nby extracting their centerlines and generating trees. We propose a\ncoarse-to-fine scheme for stereo matching, including initial matching using\naffine transform and dense matching using Hungarian algorithm guided by\nGaussian regression. Finally, we render and visualize the reconstructed model\nin a HoloLens based AR system, which can essentially change the way of\nvisualizing medical data. We have evaluated its performance by using synthetic\nand real stereo X-ray images, and achieved satisfactory quantitative and\nqualitative results.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 00:43:55 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Chen", "Qiuyu", ""], ["Bise", "Ryoma", ""], ["Gu", "Lin", ""], ["Zheng", "Yinqiang", ""], ["Sato", "Imari", ""], ["Hwang", "Jenq-Neng", ""], ["Imanishi", "Nobuaki", ""], ["Aiso", "Sadakazu", ""]]}, {"id": "1709.07565", "submitter": "Tam Nguyen", "authors": "Tam V. Nguyen, Guangyu Gao", "title": "Novel Evaluation Metrics for Seam Carving based Image Retargeting", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retargeting effectively resizes images by preserving the\nrecognizability of important image regions. Most of retargeting methods rely on\ngood importance maps as a cue to retain or remove certain regions in the input\nimage. In addition, the traditional evaluation exhaustively depends on user\nratings. There is a legitimate need for a methodological approach for\nevaluating retargeted results. Therefore, in this paper, we conduct a study and\nanalysis on the prominent method in image retargeting, Seam Carving. First, we\nintroduce two novel evaluation metrics which can be considered as the proxy of\nuser ratings. Second, we exploit salient object dataset as a benchmark for this\ntask. We then investigate different types of importance maps for this\nparticular problem. The experiments show that humans in general agree with the\nevaluation metrics on the retargeted results and some importance map methods\nare consistently more favorable than others.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 01:36:11 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Nguyen", "Tam V.", ""], ["Gao", "Guangyu", ""]]}, {"id": "1709.07566", "submitter": "Tam Nguyen", "authors": "Tam V. Nguyen, Luoqi Liu", "title": "Smart Mirror: Intelligent Makeup Recommendation and Synthesis", "comments": "accepted to ACM MM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The female facial image beautification usually requires professional editing\nsoftwares, which are relatively difficult for common users. In this demo, we\nintroduce a practical system for automatic and personalized facial makeup\nrecommendation and synthesis. First, a model describing the relations among\nfacial features, facial attributes and makeup attributes is learned as the\nmakeup recommendation model for suggesting the most suitable makeup attributes.\nThen the recommended makeup attributes are seamlessly synthesized onto the\ninput facial image.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 01:36:23 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Nguyen", "Tam V.", ""], ["Liu", "Luoqi", ""]]}, {"id": "1709.07581", "submitter": "Chiyu Jiang", "authors": "Chiyu \"Max\" Jiang, Philip Marcus", "title": "Hierarchical Detail Enhancing Mesh-Based Shape Generation with 3D\n  Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic mesh-based shape generation is of great interest across a wide\nrange of disciplines, from industrial design to gaming, computer graphics and\nvarious other forms of digital art. While most traditional methods focus on\nprimitive based model generation, advances in deep learning made it possible to\nlearn 3-dimensional geometric shape representations in an end-to-end manner.\nHowever, most current deep learning based frameworks focus on the\nrepresentation and generation of voxel and point-cloud based shapes, making it\nnot directly applicable to design and graphics communities. This study\naddresses the needs for automatic generation of mesh-based geometries, and\npropose a novel framework that utilizes signed distance function representation\nthat generates detail preserving three-dimensional surface mesh by a deep\nlearning based approach.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 03:17:34 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Jiang", "Chiyu \"Max\"", ""], ["Marcus", "Philip", ""]]}, {"id": "1709.07584", "submitter": "Xuefeng Liang", "authors": "Xuefeng Liang, Lixin Fan, Yuen Peng Loh, Yang Liu, Song Tong", "title": "Happy Travelers Take Big Pictures: A Psychological Study with Machine\n  Learning and Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In psychology, theory-driven researches are usually conducted with extensive\nlaboratory experiments, yet rarely tested or disproved with big data. In this\npaper, we make use of 418K travel photos with traveler ratings to test the\ninfluential \"broaden-and-build\" theory, that suggests positive emotions broaden\none's visual attention. The core hypothesis examined in this study is that\npositive emotion is associated with a wider attention, hence highly-rated sites\nwould trigger wide-angle photographs. By analyzing travel photos, we find a\nstrong correlation between a preference for wide-angle photos and the high\nrating of tourist sites on TripAdvisor. We are able to carry out this analysis\nthrough the use of deep learning algorithms to classify the photos into wide\nand narrow angles, and present this study as an exemplar of how big data and\ndeep learning can be used to test laboratory findings in the wild.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 03:25:34 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Liang", "Xuefeng", ""], ["Fan", "Lixin", ""], ["Loh", "Yuen Peng", ""], ["Liu", "Yang", ""], ["Tong", "Song", ""]]}, {"id": "1709.07592", "submitter": "Wei Xiong", "authors": "Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu and Jiebo Luo", "title": "Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic\n  Generative Adversarial Networks", "comments": "To appear in Proceedings of CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking a photo outside, can we predict the immediate future, e.g., how would\nthe cloud move in the sky? We address this problem by presenting a generative\nadversarial network (GAN) based two-stage approach to generating realistic\ntime-lapse videos of high resolution. Given the first frame, our model learns\nto generate long-term future frames. The first stage generates videos of\nrealistic contents for each frame. The second stage refines the generated video\nfrom the first stage by enforcing it to be closer to real videos with regard to\nmotion dynamics. To further encourage vivid motion in the final generated\nvideo, Gram matrix is employed to model the motion more precisely. We build a\nlarge scale time-lapse dataset, and test our approach on this new dataset.\nUsing our model, we are able to generate realistic videos of up to $128\\times\n128$ resolution for 32 frames. Quantitative and qualitative experiment results\nhave demonstrated the superiority of our model over the state-of-the-art\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 05:03:39 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 03:28:46 GMT"}, {"version": "v3", "created": "Fri, 30 Mar 2018 05:29:11 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Xiong", "Wei", ""], ["Luo", "Wenhan", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Luo", "Jiebo", ""]]}, {"id": "1709.07598", "submitter": "Aparna Bharati", "authors": "Aparna Bharati, Mayank Vatsa, Richa Singh, Kevin W. Bowyer and Xin\n  Tong", "title": "Demography-based Facial Retouching Detection using Subclass Supervised\n  Sparse Autoencoder", "comments": "Accepted in International Joint Conference on Biometrics, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital retouching of face images is becoming more widespread due to the\nintroduction of software packages that automate the task. Several researchers\nhave introduced algorithms to detect whether a face image is original or\nretouched. However, previous work on this topic has not considered whether or\nhow accuracy of retouching detection varies with the demography of face images.\nIn this paper, we introduce a new Multi-Demographic Retouched Faces (MDRF)\ndataset, which contains images belonging to two genders, male and female, and\nthree ethnicities, Indian, Chinese, and Caucasian. Further, retouched images\nare created using two different retouching software packages. The second major\ncontribution of this research is a novel semi-supervised autoencoder\nincorporating \"subclass\" information to improve classification. The proposed\napproach outperforms existing state-of-the-art detection algorithms for the\ntask of generalized retouching detection. Experiments conducted with multiple\ncombinations of ethnicities show that accuracy of retouching detection can vary\ngreatly based on the demographics of the training and testing images.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 05:13:48 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Bharati", "Aparna", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""], ["Bowyer", "Kevin W.", ""], ["Tong", "Xin", ""]]}, {"id": "1709.07599", "submitter": "Zhen Li", "authors": "Xiaoguang Han, Zhen Li, Haibin Huang, Evangelos Kalogerakis, and\n  Yizhou Yu", "title": "High-Resolution Shape Completion Using Deep Neural Networks for Global\n  Structure and Local Geometry Inference", "comments": "8 pages paper, 11 pages supplementary material, ICCV spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven method for recovering miss-ing parts of 3D shapes.\nOur method is based on a new deep learning architecture consisting of two\nsub-networks: a global structure inference network and a local geometry\nrefinement network. The global structure inference network incorporates a long\nshort-term memorized context fusion module (LSTM-CF) that infers the global\nstructure of the shape based on multi-view depth information provided as part\nof the input. It also includes a 3D fully convolutional (3DFCN) module that\nfurther enriches the global structure representation according to volumetric\ninformation in the input. Under the guidance of the global structure network,\nthe local geometry refinement network takes as input lo-cal 3D patches around\nmissing regions, and progressively produces a high-resolution, complete surface\nthrough a volumetric encoder-decoder architecture. Our method jointly trains\nthe global structure inference and local geometry refinement networks in an\nend-to-end manner. We perform qualitative and quantitative evaluations on six\nobject categories, demonstrating that our method outperforms existing\nstate-of-the-art work on shape completion.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 05:16:17 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Han", "Xiaoguang", ""], ["Li", "Zhen", ""], ["Huang", "Haibin", ""], ["Kalogerakis", "Evangelos", ""], ["Yu", "Yizhou", ""]]}, {"id": "1709.07634", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Guoliang Kang, Kun Zhan, Yi Yang", "title": "EraseReLU: A Simple Way to Ease the Training of Deep Convolution Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most state-of-the-art architectures, Rectified Linear Unit (ReLU) becomes\na standard component accompanied with each layer. Although ReLU can ease the\nnetwork training to an extent, the character of blocking negative values may\nsuppress the propagation of useful information and leads to the difficulty of\noptimizing very deep Convolutional Neural Networks (CNNs). Moreover, stacking\nlayers with nonlinear activations is hard to approximate the intrinsic linear\ntransformations between feature representations.\n  In this paper, we investigate the effect of erasing ReLUs of certain layers\nand apply it to various representative architectures following deterministic\nrules. It can ease the optimization and improve the generalization performance\nfor very deep CNN models. We find two key factors being essential to the\nperformance improvement: 1) the location where ReLU should be erased inside the\nbasic module; 2) the proportion of basic modules to erase ReLU; We show that\nerasing the last ReLU layer of all basic modules in a network usually yields\nimproved performance. In experiments, our approach successfully improves the\nperformance of various representative architectures, and we report the improved\nresults on SVHN, CIFAR-10/100, and ImageNet. Moreover, we achieve competitive\nsingle-model performance on CIFAR-100 with 16.53% error rate compared to\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 08:31:45 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 08:43:42 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Dong", "Xuanyi", ""], ["Kang", "Guoliang", ""], ["Zhan", "Kun", ""], ["Yang", "Yi", ""]]}, {"id": "1709.07646", "submitter": "Atsushi Takeda", "authors": "Atsushi Takeda", "title": "SwGridNet: A Deep Convolutional Neural Network based on Grid Topology\n  for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) achieve remarkable performance on\nimage classification tasks. Recent studies, however, have demonstrated that\ngeneralization abilities are more important than the depth of neural networks\nfor improving performance on image classification tasks. Herein, a new neural\nnetwork called SwGridNet is proposed. A SwGridNet includes many convolutional\nprocessing units which connect mutually as a grid network where many processing\npaths exist between input and output. A SwGridNet has high generalization\ncapability because the multipath architecture has the same effect of ensemble\nlearning. As described in this paper, details of the SwGridNet network\narchitecture are presented. Experimentally obtained results presented in this\npaper show that SwGridNets respectively achieve test error rates of 2.95% and\n15.67% in a CIFAR-10 and CIFAR-100 classification tasks. The results indicate\nthat the SwGridNet performance approximates that of state-of-the-art deep CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 09:24:08 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 17:06:08 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 04:43:34 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Takeda", "Atsushi", ""]]}, {"id": "1709.07665", "submitter": "Trung Pham", "authors": "A. Milan, T. Pham, K. Vijay, D. Morrison, A.W. Tow, L. Liu, J.\n  Erskine, R. Grinover, A. Gurman, T. Hunn, N. Kelly-Boxall, D. Lee, M.\n  McTaggart, G. Rallos, A. Razjigaev, T. Rowntree, T. Shen, R. Smith, S.\n  Wade-McCue, Z. Zhuang, C. Lehnert, G. Lin, I. Reid, P. Corke, and J. Leitner", "title": "Semantic Segmentation from Limited Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our approach for robotic perception in cluttered scenes that led\nto winning the recent Amazon Robotics Challenge (ARC) 2017. Next to small\nobjects with shiny and transparent surfaces, the biggest challenge of the 2017\ncompetition was the introduction of unseen categories. In contrast to\ntraditional approaches which require large collections of annotated data and\nmany hours of training, the task here was to obtain a robust perception\npipeline with only few minutes of data acquisition and training time. To that\nend, we present two strategies that we explored. One is a deep metric learning\napproach that works in three separate steps: semantic-agnostic boundary\ndetection, patch classification and pixel-wise voting. The other is a\nfully-supervised semantic segmentation approach with efficient dataset\ncollection. We conduct an extensive analysis of the two methods on our ARC 2017\ndataset. Interestingly, only few examples of each class are sufficient to\nfine-tune even very deep convolutional neural networks for this specific task.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 09:55:18 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Milan", "A.", ""], ["Pham", "T.", ""], ["Vijay", "K.", ""], ["Morrison", "D.", ""], ["Tow", "A. W.", ""], ["Liu", "L.", ""], ["Erskine", "J.", ""], ["Grinover", "R.", ""], ["Gurman", "A.", ""], ["Hunn", "T.", ""], ["Kelly-Boxall", "N.", ""], ["Lee", "D.", ""], ["McTaggart", "M.", ""], ["Rallos", "G.", ""], ["Razjigaev", "A.", ""], ["Rowntree", "T.", ""], ["Shen", "T.", ""], ["Smith", "R.", ""], ["Wade-McCue", "S.", ""], ["Zhuang", "Z.", ""], ["Lehnert", "C.", ""], ["Lin", "G.", ""], ["Reid", "I.", ""], ["Corke", "P.", ""], ["Leitner", "J.", ""]]}, {"id": "1709.07681", "submitter": "Ribana Roscher", "authors": "Ribana Roscher, Bernd Uebbing, J\\\"urgen Kusche", "title": "STAR: Spatio-Temporal Altimeter Waveform Retracking using Sparse\n  Representation and Conditional Random Fields", "comments": null, "journal-ref": "Remote Sensing of Environment, Vol. 201, pages 148-164, 2017", "doi": "10.1016/j.rse.2017.07.024", "report-no": null, "categories": "physics.ao-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite radar altimetry is one of the most powerful techniques for\nmeasuring sea surface height variations, with applications ranging from\noperational oceanography to climate research. Over open oceans, altimeter\nreturn waveforms generally correspond to the Brown model, and by inversion,\nestimated shape parameters provide mean surface height and wind speed. However,\nin coastal areas or over inland waters, the waveform shape is often distorted\nby land influence, resulting in peaks or fast decaying trailing edges. As a\nresult, derived sea surface heights are then less accurate and waveforms need\nto be reprocessed by sophisticated algorithms. To this end, this work suggests\na novel Spatio-Temporal Altimetry Retracking (STAR) technique. We show that\nSTAR enables the derivation of sea surface heights over the open ocean as well\nas over coastal regions of at least the same quality as compared to existing\nretracking methods, but for a larger number of cycles and thus retaining more\nuseful data. Novel elements of our method are (a) integrating information from\nspatially and temporally neighboring waveforms through a conditional random\nfield approach, (b) sub-waveform detection, where relevant sub-waveforms are\nseparated from corrupted or non-relevant parts through a sparse representation\napproach, and (c) identifying the final best set of sea surfaces heights from\nmultiple likely heights using Dijkstra's algorithm. We apply STAR to data from\nthe Jason-1, Jason-2 and Envisat missions for study sites in the Gulf of\nTrieste, Italy and in the coastal region of the Ganges-Brahmaputra-Meghna\nestuary, Bangladesh. We compare to several established and recent retracking\nmethods, as well as to tide gauge data. Our experiments suggest that the\nobtained sea surface heights are significantly less affected by outliers when\ncompared to results obtained by other approaches.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 10:32:35 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Roscher", "Ribana", ""], ["Uebbing", "Bernd", ""], ["Kusche", "J\u00fcrgen", ""]]}, {"id": "1709.07689", "submitter": "Xiao-Yun Zhou", "authors": "Xiao-Yun Zhou, Jianyu Lin, Celia Riga, Guang-Zhong Yang, Su-Lin Lee", "title": "Real-time 3D Shape Instantiation from Single Fluoroscopy Projection for\n  Fenestrated Stent Graft Deployment", "comments": "7 pages, 10 figures", "journal-ref": "IEEE Robotics and Automation Letters 2018", "doi": "10.1109/LRA.2018.2798286", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot-assisted deployment of fenestrated stent grafts in Fenestrated\nEndovascular Aortic Repair (FEVAR) requires accurate geometrical alignment.\nCurrently, this process is guided by 2D fluoroscopy, which is uninformative and\nerror prone. In this paper, a real-time framework is proposed to instantiate\nthe 3D shape of a fenestrated stent graft based on only a single low-dose 2D\nfluoroscopic image. Firstly, the fenestrated stent graft was placed with\nmarkers. Secondly, the 3D pose of each stent segment was instantiated by the\nRPnP (Robust Perspective-n-Point) method. Thirdly, the 3D shape of the whole\nstent graft was instantiated via graft gap interpolation. Focal-Unet was\nproposed to segment the markers from 2D fluoroscopic images to achieve\nsemi-automatic marker detection. The proposed framework was validated on five\npatient-specific 3D printed phantoms of aortic aneurysms and three stent grafts\nwith new marker placements, showing an average distance error of 1-3mm and an\naverage angle error of 4 degree.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 11:00:11 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 21:06:40 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Zhou", "Xiao-Yun", ""], ["Lin", "Jianyu", ""], ["Riga", "Celia", ""], ["Yang", "Guang-Zhong", ""], ["Lee", "Su-Lin", ""]]}, {"id": "1709.07720", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi, Marwa Nasser, Mostafa Korashy, Katherine Rohde, Aly\n  Abdelrahim", "title": "Can We Boost the Power of the Viola-Jones Face Detector Using\n  Pre-processing? An Empirical Study", "comments": "14 pages, 10 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Viola-Jones face detection algorithm was (and still is) a quite popular\nface detector. In spite of the numerous face detection techniques that have\nbeen recently presented, there are many research works that are still based on\nthe Viola-Jones algorithm because of its simplicity. In this paper, we study\nthe influence of a set of blind pre-processing methods on the face detection\nrate using the Viola-Jones algorithm. We focus on two aspects of improvement,\nspecifically badly illuminated faces and blurred faces. Many methods for\nlighting invariant and deblurring are used in order to improve the detection\naccuracy. We want to avoid using blind pre-processing methods that may obstruct\nthe face detector. To that end, we perform two sets of experiments. The first\nset is performed to avoid any blind pre-processing method that may hurt the\nface detector. The second set is performed to study the effect of the selected\npre-processing methods on images that suffer from hard conditions. We present\ntwo manners of applying the pre-processing method to the image prior to being\nused by the Viola-Jones face detector. Four different datasets are used to draw\na coherent conclusion about the potential improvement caused by using prior\nenhanced images. The results demonstrate that some of the pre-processing\nmethods may hurt the accuracy of Viola-Jones face detection algorithm. However,\nother pre-processing methods have an evident positive impact on the accuracy of\nthe face detector. Overall, we recommend three simple and fast blind\nphotometric normalization methods as a pre-processing step in order to improve\nthe accuracy of the pre-trained Viola-Jones face detector.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 12:44:01 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 18:46:18 GMT"}, {"version": "v3", "created": "Mon, 11 Dec 2017 04:50:52 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Nasser", "Marwa", ""], ["Korashy", "Mostafa", ""], ["Rohde", "Katherine", ""], ["Abdelrahim", "Aly", ""]]}, {"id": "1709.07739", "submitter": "Rafal Kotynski", "authors": "Krzysztof M. Czajkowski, Anna Pastuszczak and Rafa{\\l} Koty\\'nski", "title": "Single-pixel imaging with Morlet wavelet correlated random patterns", "comments": null, "journal-ref": "Sci. Rep. 8, 466 (2018)", "doi": "10.1038/s41598-017-18968-6", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-pixel imaging is an indirect imaging technique which utilizes\nsimplified optical hardware and advanced computational methods. It offers novel\nsolutions for hyper-spectral imaging, polarimetric imaging, three-dimensional\nimaging, holographic imaging, optical encryption and imaging through scattering\nmedia. The main limitations for its use come from relatively high measurement\nand reconstruction times. In this paper we propose to reduce the required\nsignal acquisition time by using a novel sampling scheme based on a random\nselection of Morlet wavelets convolved with white noise. While such functions\nexhibit random properties, they are locally determined by Morlet wavelet\nparameters. The proposed method is equivalent to random sampling of the\nproperly selected part of the feature space, which maps the measured images\naccurately both in the spatial and spatial frequency domains. We compare both\nnumerically and experimentally the image quality obtained with our sampling\nprotocol against widely-used sampling with Walsh-Hadamard or noiselet\nfunctions. The results show considerable improvement over the former methods,\nenabling single-pixel imaging at low compression rates on the order of a few\npercent.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 13:32:49 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 07:56:22 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Czajkowski", "Krzysztof M.", ""], ["Pastuszczak", "Anna", ""], ["Koty\u0144ski", "Rafa\u0142", ""]]}, {"id": "1709.07745", "submitter": "Vladimir Saveljev", "authors": "Vladimir Saveljev and Sung-Kyu Kim", "title": "Measurement of amplitude of the moir\\'e patterns in digital\n  autostereoscopic 3D display", "comments": "13 pages, 14 figures, 12 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents the experimental measurements of the amplitude of the\nmoir\\'e patterns in a digital autostereoscopic barrier-type 3D display across a\nwide angular range with a small increment. The period and orientation of the\nmoir\\'e patterns were also measured as functions of the angle. Simultaneous\nbranches are observed and analyzed. The theoretical interpretation is also\ngiven. The results can help preventing or minimizing the moir\\'e effect in\ndisplays.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 22:55:09 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Saveljev", "Vladimir", ""], ["Kim", "Sung-Kyu", ""]]}, {"id": "1709.07794", "submitter": "Ribana Roscher", "authors": "Ron Hagensieker, Ribana Roscher, Johannes Rosentreter, Benjamin\n  Jakimow, Bj\\\"orn Waske", "title": "Tropical Land Use Land Cover Mapping in Par\\'{a} (Brazil) using\n  Discriminative Markov Random Fields and Multi-temporal TerraSAR-X Data", "comments": null, "journal-ref": "International Journal of Applied Earth Observation and\n  Geoinformation, Volume 63, December 2017, Pages 244-256", "doi": "10.1016/j.jag.2017.07.019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing satellite data offer the unique possibility to map land use\nland cover transformations by providing spatially explicit information.\nHowever, detection of short-term processes and land use patterns of high\nspatial-temporal variability is a challenging task. We present a novel\nframework using multi-temporal TerraSAR-X data and machine learning techniques,\nnamely Discriminative Markov Random Fields with spatio-temporal priors, and\nImport Vector Machines, in order to advance the mapping of land cover\ncharacterized by short-term changes. Our study region covers a current\ndeforestation frontier in the Brazilian state Par\\'{a} with land cover\ndominated by primary forests, different types of pasture land and secondary\nvegetation, and land use dominated by short-term processes such as\nslash-and-burn activities. The data set comprises multi-temporal TerraSAR-X\nimagery acquired over the course of the 2014 dry season, as well as optical\ndata (RapidEye, Landsat) for reference. Results show that land use land cover\nis reliably mapped, resulting in spatially adjusted overall accuracies of up to\n$79\\%$ in a five class setting, yet limitations for the differentiation of\ndifferent pasture types remain. The proposed method is applicable on\nmulti-temporal data sets, and constitutes a feasible approach to map land use\nland cover in regions that are affected by high-frequent temporal changes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 14:53:56 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Hagensieker", "Ron", ""], ["Roscher", "Ribana", ""], ["Rosentreter", "Johannes", ""], ["Jakimow", "Benjamin", ""], ["Waske", "Bj\u00f6rn", ""]]}, {"id": "1709.07857", "submitter": "Alexander Irpan", "authors": "Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew\n  Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt\n  Konolige, Sergey Levine, Vincent Vanhoucke", "title": "Using Simulation and Domain Adaptation to Improve Efficiency of Deep\n  Robotic Grasping", "comments": "9 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumenting and collecting annotated visual grasping datasets to train\nmodern machine learning algorithms can be extremely time-consuming and\nexpensive. An appealing alternative is to use off-the-shelf simulators to\nrender synthetic data for which ground-truth annotations are generated\nautomatically. Unfortunately, models trained purely on simulated data often\nfail to generalize to the real world. We study how randomized simulated\nenvironments and domain adaptation methods can be extended to train a grasping\nsystem to grasp novel objects from raw monocular RGB images. We extensively\nevaluate our approaches with a total of more than 25,000 physical test grasps,\nstudying a range of simulation conditions and domain adaptation methods,\nincluding a novel extension of pixel-level domain adaptation that we term the\nGraspGAN. We show that, by using synthetic data and domain adaptation, we are\nable to reduce the number of real-world samples needed to achieve a given level\nof performance by up to 50 times, using only randomly generated simulated\nobjects. We also show that by using only unlabeled real-world data and our\nGraspGAN methodology, we obtain real-world grasping performance without any\nreal-world labels that is similar to that achieved with 939,777 labeled\nreal-world samples.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 17:23:12 GMT"}, {"version": "v2", "created": "Mon, 25 Sep 2017 21:35:45 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Bousmalis", "Konstantinos", ""], ["Irpan", "Alex", ""], ["Wohlhart", "Paul", ""], ["Bai", "Yunfei", ""], ["Kelcey", "Matthew", ""], ["Kalakrishnan", "Mrinal", ""], ["Downs", "Laura", ""], ["Ibarz", "Julian", ""], ["Pastor", "Peter", ""], ["Konolige", "Kurt", ""], ["Levine", "Sergey", ""], ["Vanhoucke", "Vincent", ""]]}, {"id": "1709.07871", "submitter": "Ethan Perez", "authors": "Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, Aaron\n  Courville", "title": "FiLM: Visual Reasoning with a General Conditioning Layer", "comments": "AAAI 2018. Code available at http://github.com/ethanjperez/film .\n  Extends arXiv:1707.03017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general-purpose conditioning method for neural networks called\nFiLM: Feature-wise Linear Modulation. FiLM layers influence neural network\ncomputation via a simple, feature-wise affine transformation based on\nconditioning information. We show that FiLM layers are highly effective for\nvisual reasoning - answering image-related questions which require a\nmulti-step, high-level process - a task which has proven difficult for standard\ndeep learning methods that do not explicitly model reasoning. Specifically, we\nshow on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error\nfor the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are\nrobust to ablations and architectural modifications, and 4) generalize well to\nchallenging, new data from few examples or even zero-shot.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 17:54:12 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 21:25:53 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Perez", "Ethan", ""], ["Strub", "Florian", ""], ["de Vries", "Harm", ""], ["Dumoulin", "Vincent", ""], ["Courville", "Aaron", ""]]}, {"id": "1709.07875", "submitter": "Chamberlain Fong", "authors": "Chamberlain Fong", "title": "Elliptification of Rectangular Imagery", "comments": "JMM2019 SIGMAA-ARTS; corrected typo on detJ surfaces; added\n  description on bijective spherize filter", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and discuss different algorithms for converting rectangular\nimagery into elliptical regions. We mainly focus on methods that use\nmathematical mappings with explicit and invertible equations. The key idea is\nto start with invertible mappings between the square and the circular disc then\nextend it to handle rectangles and ellipses. This extension can be done by\nsimply removing the eccentricity and reintroducing it back after using a chosen\nsquare-to-disc mapping.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 21:56:11 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 01:51:32 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 20:01:17 GMT"}, {"version": "v4", "created": "Tue, 12 Nov 2019 04:01:22 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Fong", "Chamberlain", ""]]}, {"id": "1709.07894", "submitter": "Fahimeh Rezazadegan", "authors": "Fahimeh Rezazadegan, Sareh Shirazi, Mahsa Baktashmotlagh, Larry S.\n  Davis", "title": "On Encoding Temporal Evolution for Real-time Action Prediction", "comments": "Submitted Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating future actions is a key component of intelligence, specifically\nwhen it applies to real-time systems, such as robots or autonomous cars. While\nrecent works have addressed prediction of raw RGB pixel values, we focus on\nanticipating the motion evolution in future video frames. To this end, we\nconstruct dynamic images (DIs) by summarising moving pixels through a sequence\nof future frames. We train a convolutional LSTMs to predict the next DIs based\non an unsupervised learning process, and then recognise the activity associated\nwith the predicted DI. We demonstrate the effectiveness of our approach on 3\nbenchmark action datasets showing that despite running on videos with complex\nactivities, our approach is able to anticipate the next human action with high\naccuracy and obtain better results than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 18:05:50 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 20:56:15 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 00:07:15 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Rezazadegan", "Fahimeh", ""], ["Shirazi", "Sareh", ""], ["Baktashmotlagh", "Mahsa", ""], ["Davis", "Larry S.", ""]]}, {"id": "1709.07914", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey and Sumeet Agarwal", "title": "Modeling Image Virality with Pairwise Spatial Transformer Networks", "comments": "9 pages, Accepted as a full paper at the ACM Multimedia Conference\n  (MM) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of virality and information diffusion online is a topic gaining\ntraction rapidly in the computational social sciences. Computer vision and\nsocial network analysis research have also focused on understanding the impact\nof content and information diffusion in making content viral, with prior\napproaches not performing significantly well as other traditional\nclassification tasks. In this paper, we present a novel pairwise reformulation\nof the virality prediction problem as an attribute prediction task and develop\na novel algorithm to model image virality on online media using a pairwise\nneural network. Our model provides significant insights into the features that\nare responsible for promoting virality and surpasses the existing\nstate-of-the-art by a 12% average improvement in prediction. We also\ninvestigate the effect of external category supervision on relative attribute\nprediction and observe an increase in prediction accuracy for the same across\nseveral attribute learning datasets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 19:17:15 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Agarwal", "Sumeet", ""]]}, {"id": "1709.07943", "submitter": "Yue Wu", "authors": "Yue Wu and Youzuo Lin and Zheng Zhou and David Chas Bolton and Ji Liu\n  and Paul Johnson", "title": "Cascaded Region-based Densely Connected Network for Event Detection: A\n  Seismic Application", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2018.2852302", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic event detection from time series signals has wide applications,\nsuch as abnormal event detection in video surveillance and event detection in\ngeophysical data. Traditional detection methods detect events primarily by the\nuse of similarity and correlation in data. Those methods can be inefficient and\nyield low accuracy. In recent years, because of the significantly increased\ncomputational power, machine learning techniques have revolutionized many\nscience and engineering domains. In this study, we apply a deep-learning-based\nmethod to the detection of events from time series seismic signals. However, a\ndirect adaptation of the similar ideas from 2D object detection to our problem\nfaces two challenges. The first challenge is that the duration of earthquake\nevent varies significantly; The other is that the proposals generated are\ntemporally correlated. To address these challenges, we propose a novel cascaded\nregion-based convolutional neural network to capture earthquake events in\ndifferent sizes, while incorporating contextual information to enrich features\nfor each individual proposal. To achieve a better generalization performance,\nwe use densely connected blocks as the backbone of our network. Because of the\nfact that some positive events are not correctly annotated, we further\nformulate the detection problem as a learning-from-noise problem. To verify the\nperformance of our detection methods, we employ our methods to seismic data\ngenerated from a bi-axial \"earthquake machine\" located at Rock Mechanics\nLaboratory, and we acquire labels with the help of experts. Through our\nnumerical tests, we show that our novel detection techniques yield high\naccuracy. Therefore, our novel deep-learning-based detection methods can\npotentially be powerful tools for locating events from time series data in\nvarious applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 08:00:46 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 04:15:26 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Wu", "Yue", ""], ["Lin", "Youzuo", ""], ["Zhou", "Zheng", ""], ["Bolton", "David Chas", ""], ["Liu", "Ji", ""], ["Johnson", "Paul", ""]]}, {"id": "1709.07944", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw, Marco Loog, Lambertus W. Bartels and Adri\\\"enne M.\n  Mendrik", "title": "MR Acquisition-Invariant Representation Learning", "comments": "36 pages, 2 appendices, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voxelwise classification approaches are popular and effective methods for\ntissue quantification in brain magnetic resonance imaging (MRI) scans. However,\ngeneralization of these approaches is hampered by large differences between\nsets of MRI scans such as differences in field strength, vendor or acquisition\nprotocols. Due to this acquisition related variation, classifiers trained on\ndata from a specific scanner fail or under-perform when applied to data that\nwas acquired differently. In order to address this lack of generalization, we\npropose a Siamese neural network (MRAI-net) to learn a representation that\nminimizes the between-scanner variation, while maintaining the contrast between\nbrain tissues necessary for brain tissue quantification. The proposed MRAI-net\nwas evaluated on both simulated and real MRI data. After learning the MR\nacquisition invariant representation, any supervised classification model that\nuses feature vectors can be applied. In this paper, we provide a proof of\nprinciple, which shows that a linear classifier applied on the MRAI\nrepresentation is able to outperform supervised convolutional neural network\nclassifiers for tissue classification when little target training data is\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 20:52:14 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 22:31:50 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Kouw", "Wouter M.", ""], ["Loog", "Marco", ""], ["Bartels", "Lambertus W.", ""], ["Mendrik", "Adri\u00ebnne M.", ""]]}, {"id": "1709.07992", "submitter": "Paul Hongsuck Seo", "authors": "Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, Leonid Sigal", "title": "Visual Reference Resolution using Attention Memory for Visual Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual dialog is a task of answering a series of inter-dependent questions\ngiven an input image, and often requires to resolve visual references among the\nquestions. This problem is different from visual question answering (VQA),\nwhich relies on spatial attention (a.k.a. visual grounding) estimated from an\nimage and question pair. We propose a novel attention mechanism that exploits\nvisual attentions in the past to resolve the current reference in the visual\ndialog scenario. The proposed model is equipped with an associative attention\nmemory storing a sequence of previous (attention, key) pairs. From this memory,\nthe model retrieves the previous attention, taking into account recency, which\nis most relevant for the current question, in order to resolve potentially\nambiguous references. The model then merges the retrieved attention with a\ntentative one to obtain the final attention for the current question;\nspecifically, we use dynamic parameter prediction to combine the two attentions\nconditioned on the question. Through extensive experiments on a new synthetic\nvisual dialog dataset, we show that our model significantly outperforms the\nstate-of-the-art (by ~16 % points) in situations, where visual reference\nresolution plays an important role. Moreover, the proposed model achieves\nsuperior performance (~ 2 % points improvement) in the Visual Dialog dataset,\ndespite having significantly fewer parameters than the baselines.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 02:53:48 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 18:09:07 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 21:03:18 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Seo", "Paul Hongsuck", ""], ["Lehrmann", "Andreas", ""], ["Han", "Bohyung", ""], ["Sigal", "Leonid", ""]]}, {"id": "1709.07993", "submitter": "Luis Soenksen Martinez", "authors": "Luis R Soenksen, Luis Jim\\'enez-Angeles, Gabriela Melendez and Aloha\n  Meave", "title": "A semi-automated segmentation method for detection of pulmonary embolism\n  in True-FISP MRI sequences", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary embolism (PE) is a highly mortal disease, currently assessed by\npulmonary CT angiography. True-FISP MRI has emerged as an innocuous alternative\nthat does not hold many of the limitations of x-ray imaging. However, True-FISP\nMRI is very sensitive to turbulent blood flow, generating artifacts that may\nresemble fake clots in the pulmonary vasculature. These misinterpretations\nreduce its overall diagnostic accuracy to 94%, limiting a wider use in clinical\nenvironments. A new segmentation algorithm is proposed to confirm the presence\nof real pulmonary clots in True-FISP MR images by quantitative means, measuring\nthe shape, intensity, and solidity of the formation. The algorithm was\nevaluated in 37 patients. The developed method increased the diagnostic\naccuracy of expert observers assessing Pulmonary True-FISP MRI sequences by 6%\nwithout the use of ionizing radiation, achieving a diagnostic accuracy\ncomparable to standard CT angiography.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 03:02:35 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Soenksen", "Luis R", ""], ["Jim\u00e9nez-Angeles", "Luis", ""], ["Melendez", "Gabriela", ""], ["Meave", "Aloha", ""]]}, {"id": "1709.08019", "submitter": "Jalal Mirakhorli", "authors": "Jalal Mirakhorli, Hamidreza Amindavar", "title": "Semi-Supervised Hierarchical Semantic Object Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models based on Convolutional Neural Networks (CNNs) have been proven very\nsuccessful for semantic segmentation and object parsing that yield hierarchies\nof features. Our key insight is to build convolutional networks that take input\nof arbitrary size and produce object parsing output with efficient inference\nand learning. In this work, we focus on the task of instance segmentation and\nparsing which recognizes and localizes objects down to a pixel level base on\ndeep CNN. Therefore, unlike some related work, a pixel cannot belong to\nmultiple instances and parsing. Our model is based on a deep neural network\ntrained for object masking that supervised with input image and follow\nincorporates a Conditional Random Field (CRF) with end-to-end trainable\npiecewise order potentials based on object parsing outputs. In each CRF unit we\ndesigned terms to capture the short range and long range dependencies from\nvarious neighbors. The accurate instance-level segmentation that our network\nproduce is reflected by the considerable improvements obtained over previous\nwork at high APr thresholds. We demonstrate the effectiveness of our model with\nextensive experiments on challenging dataset subset of PASCAL VOC2012.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 08:01:44 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 13:54:11 GMT"}, {"version": "v3", "created": "Sun, 29 Oct 2017 16:24:53 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Mirakhorli", "Jalal", ""], ["Amindavar", "Hamidreza", ""]]}, {"id": "1709.08068", "submitter": "Wenye He", "authors": "Wenye He", "title": "A Generic Regression Framework for Pose Recognition on Color and Depth\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascaded regression method is a fast and accurate method on finding 2D pose\nof objects in RGB images. It is able to find the accurate pose of objects in an\nimage by a great number of corrections on the good initial guess of the pose of\nobjects. This paper explains the algorithm and shows the result of two\nexperiments carried by the researchers. The presented new method to quickly and\naccurately predict 3D positions of body joints from a single depth image, using\nno temporal information. We take an object recognition approach, designing an\nintermediate body parts representation that maps the difficult pose estimation\nproblem into a simpler per-pixel classification problem. Our large and highly\nvaried training dataset allows the classifier to estimate body parts invariant\nto pose, body shape, clothing. Finally, we generate confidence-scored 3D\nproposals of several body parts by re-projecting the classification result and\nfinding local modes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 15:56:58 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["He", "Wenye", ""]]}, {"id": "1709.08103", "submitter": "Unnat Jain", "authors": "Unnat Jain, Vinay P. Namboodiri, Gaurav Pandey", "title": "Compact Environment-Invariant Codes for Robust Visual Place Recognition", "comments": "Conference on Computer and Robot Vision (CRV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust visual place recognition (VPR) requires scene representations that are\ninvariant to various environmental challenges such as seasonal changes and\nvariations due to ambient lighting conditions during day and night. Moreover, a\npractical VPR system necessitates compact representations of environmental\nfeatures. To satisfy these requirements, in this paper we suggest a\nmodification to the existing pipeline of VPR systems to incorporate supervised\nhashing. The modified system learns (in a supervised setting) compact binary\ncodes from image feature descriptors. These binary codes imbibe robustness to\nthe visual variations exposed to it during the training phase, thereby, making\nthe system adaptive to severe environmental changes. Also, incorporating\nsupervised hashing makes VPR computationally more efficient and easy to\nimplement on simple hardware. This is because binary embeddings can be learned\nover simple-to-compute features and the distance computation is also in the\nlow-dimensional hamming space of binary codes. We have performed experiments on\nseveral challenging data sets covering seasonal, illumination and viewpoint\nvariations. We also compare two widely used supervised hashing methods of\nCCAITQ and MLH and show that this new pipeline out-performs or closely matches\nthe state-of-the-art deep learning VPR methods that are based on\nhigh-dimensional features extracted from pre-trained deep convolutional neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 19:08:47 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Jain", "Unnat", ""], ["Namboodiri", "Vinay P.", ""], ["Pandey", "Gaurav", ""]]}, {"id": "1709.08127", "submitter": "Yue Wu", "authors": "Yue Wu and Qiang Ji", "title": "Robust Facial Landmark Detection under Significant Head Poses and\n  Occlusion", "comments": "International Conference on Computer Vision, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been tremendous improvements for facial landmark detection on\ngeneral \"in-the-wild\" images. However, it is still challenging to detect the\nfacial landmarks on images with severe occlusion and images with large head\nposes (e.g. profile face). In fact, the existing algorithms usually can only\nhandle one of them. In this work, we propose a unified robust cascade\nregression framework that can handle both images with severe occlusion and\nimages with large head poses. Specifically, the method iteratively predicts the\nlandmark occlusions and the landmark locations. For occlusion estimation,\ninstead of directly predicting the binary occlusion vectors, we introduce a\nsupervised regression method that gradually updates the landmark visibility\nprobabilities in each iteration to achieve robustness. In addition, we\nexplicitly add occlusion pattern as a constraint to improve the performance of\nocclusion prediction. For landmark detection, we combine the landmark\nvisibility probabilities, the local appearances, and the local shapes to\niteratively update their positions. The experimental results show that the\nproposed method is significantly better than state-of-the-art works on images\nwith severe occlusion and images with large head poses. It is also comparable\nto other methods on general \"in-the-wild\" images.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 23:34:53 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Wu", "Yue", ""], ["Ji", "Qiang", ""]]}, {"id": "1709.08128", "submitter": "Yue Wu", "authors": "Yue Wu and Qiang Ji", "title": "Constrained Deep Transfer Feature Learning and its Applications", "comments": "International Conference on Computer Vision and Pattern Recognition,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature learning with deep models has achieved impressive results for both\ndata representation and classification for various vision tasks. Deep feature\nlearning, however, typically requires a large amount of training data, which\nmay not be feasible for some application domains. Transfer learning can be one\nof the approaches to alleviate this problem by transferring data from data-rich\nsource domain to data-scarce target domain. Existing transfer learning methods\ntypically perform one-shot transfer learning and often ignore the specific\nproperties that the transferred data must satisfy. To address these issues, we\nintroduce a constrained deep transfer feature learning method to perform\nsimultaneous transfer learning and feature learning by performing transfer\nlearning in a progressively improving feature space iteratively in order to\nbetter narrow the gap between the target domain and the source domain for\neffective transfer of the data from the source domain to target domain.\nFurthermore, we propose to exploit the target domain knowledge and incorporate\nsuch prior knowledge as a constraint during transfer learning to ensure that\nthe transferred data satisfies certain properties of the target domain. To\ndemonstrate the effectiveness of the proposed constrained deep transfer feature\nlearning method, we apply it to thermal feature learning for eye detection by\ntransferring from the visible domain. We also applied the proposed method for\ncross-view facial expression recognition as a second application. The\nexperimental results demonstrate the effectiveness of the proposed method for\nboth applications.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 23:44:00 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Wu", "Yue", ""], ["Ji", "Qiang", ""]]}, {"id": "1709.08129", "submitter": "Yue Wu", "authors": "Yue Wu and Qiang Ji", "title": "Constrained Joint Cascade Regression Framework for Simultaneous Facial\n  Action Unit Recognition and Facial Landmark Detection", "comments": "International Conference on Computer Vision and Pattern Recognition,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascade regression framework has been shown to be effective for facial\nlandmark detection. It starts from an initial face shape and gradually predicts\nthe face shape update from the local appearance features to generate the facial\nlandmark locations in the next iteration until convergence. In this paper, we\nimprove upon the cascade regression framework and propose the Constrained Joint\nCascade Regression Framework (CJCRF) for simultaneous facial action unit\nrecognition and facial landmark detection, which are two related face analysis\ntasks, but are seldomly exploited together. In particular, we first learn the\nrelationships among facial action units and face shapes as a constraint. Then,\nin the proposed constrained joint cascade regression framework, with the help\nfrom the constraint, we iteratively update the facial landmark locations and\nthe action unit activation probabilities until convergence. Experimental\nresults demonstrate that the intertwined relationships of facial action units\nand face shapes boost the performances of both facial action unit recognition\nand facial landmark detection. The experimental results also demonstrate the\neffectiveness of the proposed method comparing to the state-of-the-art works.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 23:46:02 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Wu", "Yue", ""], ["Ji", "Qiang", ""]]}, {"id": "1709.08130", "submitter": "Yue Wu", "authors": "Yue Wu and Chao Gou and Qiang Ji", "title": "Simultaneous Facial Landmark Detection, Pose and Deformation Estimation\n  under Facial Occlusion", "comments": "International Conference on Computer Vision and Pattern Recognition,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark detection, head pose estimation, and facial deformation\nanalysis are typical facial behavior analysis tasks in computer vision. The\nexisting methods usually perform each task independently and sequentially,\nignoring their interactions. To tackle this problem, we propose a unified\nframework for simultaneous facial landmark detection, head pose estimation, and\nfacial deformation analysis, and the proposed model is robust to facial\nocclusion. Following a cascade procedure augmented with model-based head pose\nestimation, we iteratively update the facial landmark locations, facial\nocclusion, head pose and facial de- formation until convergence. The\nexperimental results on benchmark databases demonstrate the effectiveness of\nthe proposed method for simultaneous facial landmark detection, head pose and\nfacial deformation estimation, even if the images are under facial occlusion.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 23:53:44 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Wu", "Yue", ""], ["Gou", "Chao", ""], ["Ji", "Qiang", ""]]}, {"id": "1709.08142", "submitter": "Qixing Zhang", "authors": "Gao Xu, Yongming Zhang, Qixing Zhang, Gaohua Lin, Jinjun Wang", "title": "Domain Adaptation from Synthesis to Reality in Single-model Detector for\n  Video Smoke Detection", "comments": "The manuscript approved by all authors is our original work, and has\n  submitted to Pattern Recognition for peer review previously. There are 4532\n  words, 6 figures and 1 table in this manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for video smoke detection using synthetic smoke\nsamples. The virtual data can automatically offer precise and rich annotated\nsamples. However, the learning of smoke representations will be hurt by the\nappearance gap between real and synthetic smoke samples. The existed researches\nmainly work on the adaptation to samples extracted from original annotated\nsamples. These methods take the object detection and domain adaptation as two\nindependent parts. To train a strong detector with rich synthetic samples, we\nconstruct the adaptation to the detection layer of state-of-the-art\nsingle-model detectors (SSD and MS-CNN). The training procedure is an\nend-to-end stage. The classification, location and adaptation are combined in\nthe learning. The performance of the proposed model surpasses the original\nbaseline in our experiments. Meanwhile, our results show that the detectors\nbased on the adversarial adaptation are superior to the detectors based on the\ndiscrepancy adaptation. Code will be made publicly available on\nhttp://smoke.ustc.edu.cn. Moreover, the domain adaptation for two-stage\ndetector is described in Appendix A.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 03:33:41 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 03:46:16 GMT"}, {"version": "v3", "created": "Wed, 21 Nov 2018 01:12:34 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Xu", "Gao", ""], ["Zhang", "Yongming", ""], ["Zhang", "Qixing", ""], ["Lin", "Gaohua", ""], ["Wang", "Jinjun", ""]]}, {"id": "1709.08145", "submitter": "Igor Gitman", "authors": "Igor Gitman, Boris Ginsburg", "title": "Comparison of Batch Normalization and Weight Normalization Algorithms\n  for the Large-scale Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization (BN) has become a de facto standard for training deep\nconvolutional networks. However, BN accounts for a significant fraction of\ntraining run-time and is difficult to accelerate, since it is a\nmemory-bandwidth bounded operation. Such a drawback of BN motivates us to\nexplore recently proposed weight normalization algorithms (WN algorithms), i.e.\nweight normalization, normalization propagation and weight normalization with\ntranslated ReLU. These algorithms don't slow-down training iterations and were\nexperimentally shown to outperform BN on relatively small networks and\ndatasets. However, it is not clear if these algorithms could replace BN in\npractical, large-scale applications. We answer this question by providing a\ndetailed comparison of BN and WN algorithms using ResNet-50 network trained on\nImageNet. We found that although WN achieves better training accuracy, the\nfinal test accuracy is significantly lower ($\\approx 6\\%$) than that of BN.\nThis result demonstrates the surprising strength of the BN regularization\neffect which we were unable to compensate for using standard regularization\ntechniques like dropout and weight decay. We also found that training of deep\nnetworks with WN algorithms is significantly less stable compared to BN,\nlimiting their practical applications.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 04:33:53 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 23:28:57 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Gitman", "Igor", ""], ["Ginsburg", "Boris", ""]]}, {"id": "1709.08154", "submitter": "Xin Jie Tang Mr", "authors": "Xin Jie Tang, Yong Haur Tay, Nordahlia Abdullah Siam, Seng Choon Lim", "title": "Rapid and Robust Automated Macroscopic Wood Identification System using\n  Smartphone with Macro-lens", "comments": "Accepted by PRWAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wood Identification has never been more important to serve the purpose of\nglobal forest species protection and timber regulation. Macroscopic level wood\nidentification practiced by wood anatomists can identify wood up to genus\nlevel. This is sufficient to serve as a frontline identification to fight\nagainst illegal wood logging and timber trade for law enforcement authority.\nHowever, frontline enforcement official may lack of the accuracy and confidence\nof a well trained wood anatomist. Hence, computer assisted method such as\nmachine vision methods are developed to do rapid field identification for law\nenforcement official. In this paper, we proposed a rapid and robust macroscopic\nwood identification system using machine vision method with off-the-shelf\nsmartphone and retrofitted macro-lens. Our system is cost effective, easily\naccessible, fast and scalable at the same time provides human-level accuracy on\nidentification. Camera-enabled smartphone with Internet connectivity coupled\nwith a macro-lens provides a simple and effective digital acquisition of\nmacroscopic wood images which are essential for macroscopic wood\nidentification. The images are immediately streamed to a cloud server via\nInternet connection for identification which are done within seconds.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 06:10:52 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Tang", "Xin Jie", ""], ["Tay", "Yong Haur", ""], ["Siam", "Nordahlia Abdullah", ""], ["Lim", "Seng Choon", ""]]}, {"id": "1709.08164", "submitter": "Konstantinos Makantasis", "authors": "Konstantinos Makantasis, Anastasios Doulamis, Nikolaos Doulamis and\n  Antonis Nikitakis", "title": "Tensor-Based Classifiers for Hyperspectral Data Analysis", "comments": "To appear in IEEE Transactions on Geoscience and Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2018.2845450", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present tensor-based linear and nonlinear models for\nhyperspectral data classification and analysis. By exploiting principles of\ntensor algebra, we introduce new classification architectures, the weight\nparameters of which satisfies the {\\it rank}-1 canonical decomposition\nproperty. Then, we introduce learning algorithms to train both the linear and\nthe non-linear classifier in a way to i) to minimize the error over the\ntraining samples and ii) the weight coefficients satisfies the {\\it rank}-1\ncanonical decomposition property. The advantages of the proposed classification\nmodel is that i) it reduces the number of parameters required and thus reduces\nthe respective number of training samples required to properly train the model,\nii) it provides a physical interpretation regarding the model coefficients on\nthe classification output and iii) it retains the spatial and spectral\ncoherency of the input samples. To address issues related with linear\nclassification, characterizing by low capacity, since it can produce rules that\nare linear in the input space, we introduce non-linear classification models\nbased on a modification of a feedforward neural network. We call the proposed\narchitecture {\\it rank}-1 Feedfoward Neural Network (FNN), since their weights\nsatisfy the {\\it rank}-1 caconical decomposition property. Appropriate learning\nalgorithms are also proposed to train the network. Experimental results and\ncomparisons with state of the art classification methods, either linear (e.g.,\nSVM) and non-linear (e.g., deep learning) indicates the outperformance of the\nproposed scheme, especially in cases where a small number of training samples\nare available. Furthermore, the proposed tensor-based classfiers are evaluated\nagainst their capabilities in dimensionality reduction.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 09:05:36 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 13:25:08 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Makantasis", "Konstantinos", ""], ["Doulamis", "Anastasios", ""], ["Doulamis", "Nikolaos", ""], ["Nikitakis", "Antonis", ""]]}, {"id": "1709.08172", "submitter": "Shuang Li", "authors": "Shuang Li, Peter Mathews", "title": "Can Image Retrieval help Visual Saliency Detection?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel image retrieval framework for visual saliency detection\nusing information about salient objects contained within bounding box\nannotations for similar images. For each test image, we train a customized SVM\nfrom similar example images to predict the saliency values of its object\nproposals and generate an external saliency map (ES) by aggregating the\nregional scores. To overcome limitations caused by the size of the training\ndataset, we also propose an internal optimization module which computes an\ninternal saliency map (IS) by measuring the low-level contrast information of\nthe test image. The two maps, ES and IS, have complementary properties so we\ntake a weighted combination to further improve the detection performance.\nExperimental results on several challenging datasets demonstrate that the\nproposed algorithm performs favorably against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 09:50:48 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Li", "Shuang", ""], ["Mathews", "Peter", ""]]}, {"id": "1709.08202", "submitter": "Bruno Ferrarini", "authors": "Bruno Ferrarini, Shoaib Ehsan, Ales Leonardis, Naveed Ur Rehman, Klaus\n  D.McDonald-Maier", "title": "Performance Characterization of Image Feature Detectors in Relation to\n  the Scene Content Utilizing a Large Image Database", "comments": "Extended version of the conference paper available at\n  http://ieeexplore.ieee.org/abstract/document/7314191/?reload=true", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the most suitable local invariant feature detector for a particular\napplication has rendered the task of evaluating feature detectors a critical\nissue in vision research. Although the literature offers a variety of\ncomparison works focusing on performance evaluation of image feature detectors\nunder several types of image transformations, the influence of the scene\ncontent on the performance of local feature detectors has received little\nattention so far. This paper aims to bridge this gap with a new framework for\ndetermining the type of scenes which maximize and minimize the performance of\ndetectors in terms of repeatability rate. The results are presented for several\nstate-of-the-art feature detectors that have been obtained using a large image\ndatabase of 20482 images under JPEG compression, uniform light and blur changes\nwith 539 different scenes captured from real-world scenarios. These results\nprovide new insights into the behavior of feature detectors.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 14:22:52 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 15:42:32 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Ferrarini", "Bruno", ""], ["Ehsan", "Shoaib", ""], ["Leonardis", "Ales", ""], ["Rehman", "Naveed Ur", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1709.08203", "submitter": "Supriya Pandhre", "authors": "Supriya Pandhre, Shagun Sodhani", "title": "Survey of Recent Advances in Visual Question Answering", "comments": "7 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual Question Answering (VQA) presents a unique challenge as it requires\nthe ability to understand and encode the multi-modal inputs - in terms of image\nprocessing and natural language processing. The algorithm further needs to\nlearn how to perform reasoning over this multi-modal representation so it can\nanswer the questions correctly. This paper presents a survey of different\napproaches proposed to solve the problem of Visual Question Answering. We also\ndescribe the current state of the art model in later part of paper. In\nparticular, the paper describes the approaches taken by various algorithms to\nextract image features, text features and the way these are employed to predict\nanswers. We also briefly discuss the experiments performed to evaluate the VQA\nmodels and report their performances on diverse datasets including newly\nreleased VQA2.0[8].\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 14:42:17 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Pandhre", "Supriya", ""], ["Sodhani", "Shagun", ""]]}, {"id": "1709.08248", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee and Alexander Wong", "title": "Discovery Radiomics via Deep Multi-Column Radiomic Sequencers for Skin\n  Cancer Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While skin cancer is the most diagnosed form of cancer in men and women, with\nmore cases diagnosed each year than all other cancers combined, sufficiently\nearly diagnosis results in very good prognosis and as such makes early\ndetection crucial. While radiomics have shown considerable promise as a\npowerful diagnostic tool for significantly improving oncological diagnostic\naccuracy and efficiency, current radiomics-driven methods have largely rely on\npre-defined, hand-crafted quantitative features, which can greatly limit the\nability to fully characterize unique cancer phenotype that distinguish it from\nhealthy tissue. Recently, the notion of discovery radiomics was introduced,\nwhere a large amount of custom, quantitative radiomic features are directly\ndiscovered from the wealth of readily available medical imaging data. In this\nstudy, we present a novel discovery radiomics framework for skin cancer\ndetection, where we leverage novel deep multi-column radiomic sequencers for\nhigh-throughput discovery and extraction of a large amount of custom radiomic\nfeatures tailored for characterizing unique skin cancer tissue phenotype. The\ndiscovered radiomic sequencer was tested against 9,152 biopsy-proven clinical\nimages comprising of different skin cancers such as melanoma and basal cell\ncarcinoma, and demonstrated sensitivity and specificity of 91% and 75%,\nrespectively, thus achieving dermatologist-level performance and \\break hence\ncan be a powerful tool for assisting general practitioners and dermatologists\nalike in improving the efficiency, consistency, and accuracy of skin cancer\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 19:53:20 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Wong", "Alexander", ""]]}, {"id": "1709.08267", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Donald E. Brown, Mojtaba Heidarysafa, Kiana Jafari\n  Meimandi, Matthew S. Gerber, Laura E. Barnes", "title": "HDLTex: Hierarchical Deep Learning for Text Classification", "comments": "ICMLA 2017", "journal-ref": null, "doi": "10.1109/ICMLA.2017.0-134", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continually increasing number of documents produced each year\nnecessitates ever improving information processing methods for searching,\nretrieving, and organizing text. Central to these information processing\nmethods is document classification, which has become an important application\nfor supervised learning. Recently the performance of these traditional\nclassifiers has degraded as the number of documents has increased. This is\nbecause along with this growth in the number of documents has come an increase\nin the number of categories. This paper approaches this problem differently\nfrom current document classification methods that view the problem as\nmulti-class classification. Instead we perform hierarchical classification\nusing an approach we call Hierarchical Deep Learning for Text classification\n(HDLTex). HDLTex employs stacks of deep learning architectures to provide\nspecialized understanding at each level of the document hierarchy.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 21:58:12 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 18:16:31 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Kowsari", "Kamran", ""], ["Brown", "Donald E.", ""], ["Heidarysafa", "Mojtaba", ""], ["Meimandi", "Kiana Jafari", ""], ["Gerber", "Matthew S.", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1709.08271", "submitter": "Ahmed M. Siddek", "authors": "Ahmed M. Siddek, Mohsen A. Rashwan, Islam A. Eshrah", "title": "3D Camouflaging Object using RGB-D Sensors", "comments": "6 pages, 12 figures, 2017 IEEE International Conference on SMC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a new optical camouflage system that uses RGB-D cameras,\nfor acquiring point cloud of background scene, and tracking observers eyes.\nThis system enables a user to conceal an object located behind a display that\nsurrounded by 3D objects. If we considered here the tracked point of observer s\neyes is a light source, the system will work on estimating shadow shape of the\ndisplay device that falls on the objects in background. The system uses the 3d\nobserver s eyes and the locations of display corners to predict their shadow\npoints which have nearest neighbors in the constructed point cloud of\nbackground scene.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 22:19:24 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Siddek", "Ahmed M.", ""], ["Rashwan", "Mohsen A.", ""], ["Eshrah", "Islam A.", ""]]}, {"id": "1709.08295", "submitter": "Yuxin Peng", "authors": "Xiangteng He, Yuxin Peng and Junjie Zhao", "title": "Fine-grained Discriminative Localization via Saliency-guided Faster\n  R-CNN", "comments": "9 pages, to appear in ACM MM 2017", "journal-ref": null, "doi": "10.1145/3123266.3123319", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative localization is essential for fine-grained image\nclassification task, which devotes to recognizing hundreds of subcategories in\nthe same basic-level category. Reflecting on discriminative regions of objects,\nkey differences among different subcategories are subtle and local. Existing\nmethods generally adopt a two-stage learning framework: The first stage is to\nlocalize the discriminative regions of objects, and the second is to encode the\ndiscriminative features for training classifiers. However, these methods\ngenerally have two limitations: (1) Separation of the two-stage learning is\ntime-consuming. (2) Dependence on object and parts annotations for\ndiscriminative localization learning leads to heavily labor-consuming labeling.\nIt is highly challenging to address these two important limitations\nsimultaneously. Existing methods only focus on one of them. Therefore, this\npaper proposes the discriminative localization approach via saliency-guided\nFaster R-CNN to address the above two limitations at the same time, and our\nmain novelties and advantages are: (1) End-to-end network based on Faster R-CNN\nis designed to simultaneously localize discriminative regions and encode\ndiscriminative features, which accelerates classification speed. (2)\nSaliency-guided localization learning is proposed to localize the\ndiscriminative region automatically, avoiding labor-consuming labeling. Both\nare jointly employed to simultaneously accelerate classification speed and\neliminate dependence on object and parts annotations. Comparing with the\nstate-of-the-art methods on the widely-used CUB-200-2011 dataset, our approach\nachieves both the best classification accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 02:43:49 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["He", "Xiangteng", ""], ["Peng", "Yuxin", ""], ["Zhao", "Junjie", ""]]}, {"id": "1709.08325", "submitter": "Jianing Li", "authors": "Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, Qi Tian", "title": "Pose-driven Deep Convolutional Model for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction and matching are two crucial components in person\nRe-Identification (ReID). The large pose deformations and the complex view\nvariations exhibited by the captured person images significantly increase the\ndifficulty of learning and matching of the features from person images. To\novercome these difficulties, in this work we propose a Pose-driven Deep\nConvolutional (PDC) model to learn improved feature extraction and matching\nmodels from end to end. Our deep architecture explicitly leverages the human\npart cues to alleviate the pose variations and learn robust feature\nrepresentations from both the global image and different local parts. To match\nthe features from global human body and local body parts, a pose driven feature\nweighting sub-network is further designed to learn adaptive feature fusions.\nExtensive experimental analyses and results on three popular datasets\ndemonstrate significant performance improvements of our model over all\npublished state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 05:36:39 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Su", "Chi", ""], ["Li", "Jianing", ""], ["Zhang", "Shiliang", ""], ["Xing", "Junliang", ""], ["Gao", "Wen", ""], ["Tian", "Qi", ""]]}, {"id": "1709.08340", "submitter": "Hayato Okumoto", "authors": "Hayato Okumoto, Mitsuo Yoshida, Kyoji Umemura", "title": "Realizing Half-Diminished Reality from Video Stream of Manipulating\n  Objects", "comments": "The 2016 International Conference On Advanced Informatics: Concepts,\n  Theory And Application (ICAICTA2016)", "journal-ref": null, "doi": "10.1109/ICAICTA.2016.7803120", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we watch a video, in which human hands manipulate objects, these hands\nmay obscure some parts of those objects. We are willing to make clear how the\nobjects are manipulated by making the image of hands semi-transparent, and\nshowing the complete images of the hands and the object. By carefully choosing\na Half-Diminished Reality method, this paper proposes a method that can process\nthe video in real time and verifies that the proposed method works well.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 06:40:03 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Okumoto", "Hayato", ""], ["Yoshida", "Mitsuo", ""], ["Umemura", "Kyoji", ""]]}, {"id": "1709.08362", "submitter": "Romany Mansour", "authors": "Romany F. Mansour and Elsaid MD. Abdelrahim", "title": "An Evolutionary Computing Enriched RS Attack Resilient Medical Image\n  Steganography Model for Telemedicine Applications", "comments": "14 page / 3 figures / 6 tables, Multidimensional Systems and Signal\n  Processing 2018", "journal-ref": null, "doi": "10.1007/s11045-018-0575-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advancement in computing technologies and resulting vision based\napplications have gives rise to a novel practice called telemedicine that\nrequires patient diagnosis images or allied information to recommend or even\nperform diagnosis practices being located remotely. However, to ensure accurate\nand optimal telemedicine there is the requirement of seamless or flawless\nbiomedical information about patient. On the contrary, medical data transmitted\nover insecure channel often remains prone to get manipulated or corrupted by\nattackers. The existing cryptosystems alone are not sufficient to deal with\nthese issues and hence in this paper a highly robust reversible image\nsteganography model has been developed for secret information hiding. Unlike\ntraditional wavelet transform techniques, we incorporated Discrete Ripplet\nTransformation (DRT) technique for message embedding in the medical cover\nimages. In addition, to assure seamless communication over insecure channel, a\ndual cryptosystem model containing proposed steganography scheme and RSA\ncryptosystem has been developed. One of the key novelties of the proposed\nresearch work is the use of adaptive genetic algorithm (AGA) for optimal pixel\nadjustment process (OPAP) that enriches data hiding capacity as well as\nimperceptibility features. The performance assessment reveals that the proposed\nsteganography model outperforms other wavelet transformation based approaches\nin terms of high PSNR, embedding capacity, imperceptibility etc.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 08:05:39 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 09:17:18 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Mansour", "Romany F.", ""], ["Abdelrahim", "Elsaid MD.", ""]]}, {"id": "1709.08364", "submitter": "Xin Jin", "authors": "Xin Jin, Shuyun Zhu, Le Wu, Geng Zhao, Xiaodong Li, Quan Zhou, Huimin\n  Lu", "title": "Multi-level Chaotic Maps for 3D Textured Model Encryption", "comments": "ROSENET 2018 and SCIS paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid progress of Virtual Reality and Augmented Reality technologies, 3D\ncontents are the next widespread media in many applications. Thus, the\nprotection of 3D models is primarily important. Encryption of 3D models is\nessential to maintain confidentiality. Previous work on encryption of 3D\nsurface model often consider the point clouds, the meshes and the textures\nindividually. In this work, a multi-level chaotic maps models for 3D textured\nencryption was presented by observing the different contributions for\nrecognizing cipher 3D models between vertices (point cloud), polygons and\ntextures. For vertices which make main contribution for recognizing, we use\nhigh level 3D Lu chaotic map to encrypt them. For polygons and textures which\nmake relatively smaller contributions for recognizing, we use 2D Arnold's cat\nmap and 1D Logistic map to encrypt them, respectively. The experimental results\nshow that our method can get similar performance with the other method use the\nsame high level chaotic map for point cloud, polygons and textures, while we\nuse less time. Besides, our method can resist more method of attacks such as\nstatistic attack, brute-force attack, correlation attack.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 08:20:17 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 12:15:58 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Jin", "Xin", ""], ["Zhu", "Shuyun", ""], ["Wu", "Le", ""], ["Zhao", "Geng", ""], ["Li", "Xiaodong", ""], ["Zhou", "Quan", ""], ["Lu", "Huimin", ""]]}, {"id": "1709.08374", "submitter": "Xi Peng", "authors": "Xi Peng and Jiashi Feng and Shijie Xiao and Jiwen Lu and Zhang Yi and\n  Shuicheng Yan", "title": "Deep Sparse Subspace Clustering", "comments": "The initial version is completed at the beginning of 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep extension of Sparse Subspace Clustering,\ntermed Deep Sparse Subspace Clustering (DSSC). Regularized by the unit sphere\ndistribution assumption for the learned deep features, DSSC can infer a new\ndata affinity matrix by simultaneously satisfying the sparsity principle of SSC\nand the nonlinearity given by neural networks. One of the appealing advantages\nbrought by DSSC is: when original real-world data do not meet the\nclass-specific linear subspace distribution assumption, DSSC can employ neural\nnetworks to make the assumption valid with its hierarchical nonlinear\ntransformations. To the best of our knowledge, this is among the first deep\nlearning based subspace clustering methods. Extensive experiments are conducted\non four real-world datasets to show the proposed DSSC is significantly superior\nto 12 existing methods for subspace clustering.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 08:43:04 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Peng", "Xi", ""], ["Feng", "Jiashi", ""], ["Xiao", "Shijie", ""], ["Lu", "Jiwen", ""], ["Yi", "Zhang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1709.08378", "submitter": "Yvain Qu\\'eau", "authors": "Jean M\\'elou, Yvain Qu\\'eau, Jean-Denis Durou, Fabien Castan and\n  Daniel Cremers", "title": "Variational Reflectance Estimation from Multi-view Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of reflectance estimation from a set of multi-view\nimages, assuming known geometry. The approach we put forward turns the input\nimages into reflectance maps, through a robust variational method. The\nvariational model comprises an image-driven fidelity term and a term which\nenforces consistency of the reflectance estimates with respect to each view. If\nillumination is fixed across the views, then reflectance estimation remains\nunder-constrained: a regularization term, which ensures piecewise-smoothness of\nthe reflectance, is thus used. Reflectance is parameterized in the image\ndomain, rather than on the surface, which makes the numerical solution much\neasier, by resorting to an alternating majorization-minimization approach.\nExperiments on both synthetic and real datasets are carried out to validate the\nproposed strategy.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 08:53:03 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 08:36:13 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["M\u00e9lou", "Jean", ""], ["Qu\u00e9au", "Yvain", ""], ["Durou", "Jean-Denis", ""], ["Castan", "Fabien", ""], ["Cremers", "Daniel", ""]]}, {"id": "1709.08393", "submitter": "Congcong Jin", "authors": "Congcong Jin, Jihua Zhu, Yaochen Li, Shanmin Pang, Lei Chen, Jun Wang", "title": "Multi-view Registration Based on Weighted Low Rank and Sparse Matrix\n  Decomposition of Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the low rank and sparse (LRS) matrix decomposition has been\nintroduced as an effective mean to solve the multi-view registration. It views\neach available relative motion as a block element to reconstruct one matrix so\nas to approximate the low rank matrix, where global motions can be recovered\nfor multi-view registration. However, this approach is sensitive to the\nsparsity of the reconstructed matrix and it treats all block elements equally\nin spite of their varied reliability. Therefore, this paper proposes an\neffective approach for multi-view registration by the weighted LRS\ndecomposition. Based on the anti-symmetry property of relative motions, it\nfirstly proposes a completion strategy to reduce the sparsity of the\nreconstructed matrix. The reduced sparsity of reconstructed matrix can improve\nthe robustness of LRS decomposition. Then, it proposes the weighted LRS\ndecomposition, where each block element is assigned with one estimated weight\nto denote its reliability. By introducing the weight, more accurate\nregistration results can be recovered from the estimated low rank matrix with\ngood efficiency. Experimental results tested on public data sets illustrate the\nsuperiority of the proposed approach over the state-of-the-art approaches on\nrobustness, accuracy, and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 09:30:02 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 01:22:46 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Jin", "Congcong", ""], ["Zhu", "Jihua", ""], ["Li", "Yaochen", ""], ["Pang", "Shanmin", ""], ["Chen", "Lei", ""], ["Wang", "Jun", ""]]}, {"id": "1709.08398", "submitter": "Thomas Gerig", "authors": "Thomas Gerig, Andreas Morel-Forster, Clemens Blumer, Bernhard Egger,\n  Marcel L\\\"uthi, Sandro Sch\\\"onborn and Thomas Vetter", "title": "Morphable Face Models - An Open Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel open-source pipeline for face registration\nbased on Gaussian processes as well as an application to face image analysis.\nNon-rigid registration of faces is significant for many applications in\ncomputer vision, such as the construction of 3D Morphable face models (3DMMs).\nGaussian Process Morphable Models (GPMMs) unify a variety of non-rigid\ndeformation models with B-splines and PCA models as examples. GPMM separate\nproblem specific requirements from the registration algorithm by incorporating\ndomain-specific adaptions as a prior model. The novelties of this paper are the\nfollowing: (i) We present a strategy and modeling technique for face\nregistration that considers symmetry, multi-scale and spatially-varying\ndetails. The registration is applied to neutral faces and facial expressions.\n(ii) We release an open-source software framework for registration and\nmodel-building, demonstrated on the publicly available BU3D-FE database. The\nreleased pipeline also contains an implementation of an Analysis-by-Synthesis\nmodel adaption of 2D face images, tested on the Multi-PIE and LFW database.\nThis enables the community to reproduce, evaluate and compare the individual\nsteps of registration to model-building and 3D/2D model fitting. (iii) Along\nwith the framework release, we publish a new version of the Basel Face Model\n(BFM-2017) with an improved age distribution and an additional facial\nexpression model.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 09:36:19 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 07:18:15 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Gerig", "Thomas", ""], ["Morel-Forster", "Andreas", ""], ["Blumer", "Clemens", ""], ["Egger", "Bernhard", ""], ["L\u00fcthi", "Marcel", ""], ["Sch\u00f6nborn", "Sandro", ""], ["Vetter", "Thomas", ""]]}, {"id": "1709.08421", "submitter": "Antonio Tejero-de-Pablos", "authors": "Antonio Tejero-de-Pablos, Yuta Nakashima, Tomokazu Sato, Naokazu\n  Yokoya, Marko Linna, Esa Rahtu", "title": "Summarization of User-Generated Sports Video by Using Deep Action\n  Recognition Features", "comments": "12 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating a summary of sports video poses the challenge of\ndetecting interesting moments, or highlights, of a game. Traditional sports\nvideo summarization methods leverage editing conventions of broadcast sports\nvideo that facilitate the extraction of high-level semantics. However,\nuser-generated videos are not edited, and thus traditional methods are not\nsuitable to generate a summary. In order to solve this problem, this work\nproposes a novel video summarization method that uses players' actions as a cue\nto determine the highlights of the original video. A deep neural network-based\napproach is used to extract two types of action-related features and to\nclassify video segments into interesting or uninteresting parts. The proposed\nmethod can be applied to any sports in which games consist of a succession of\nactions. Especially, this work considers the case of Kendo (Japanese fencing)\nas an example of a sport to evaluate the proposed method. The method is trained\nusing Kendo videos with ground truth labels that indicate the video highlights.\nThe labels are provided by annotators possessing different experience with\nrespect to Kendo to demonstrate how the proposed method adapts to different\nneeds. The performance of the proposed method is compared with several\ncombinations of different features, and the results show that it outperforms\nprevious summarization methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 10:57:30 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 09:28:40 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Tejero-de-Pablos", "Antonio", ""], ["Nakashima", "Yuta", ""], ["Sato", "Tomokazu", ""], ["Yokoya", "Naokazu", ""], ["Linna", "Marko", ""], ["Rahtu", "Esa", ""]]}, {"id": "1709.08429", "submitter": "Sen Wang", "authors": "Sen Wang, Ronald Clark, Hongkai Wen, Niki Trigoni", "title": "DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent\n  Convolutional Neural Networks", "comments": "Published in 2017 IEEE International Conference on Robotics and\n  Automation (ICRA 2017). Official version at\n  http://ieeexplore.ieee.org/document/7989236/ . Project website and videos at\n  http://senwang.gitlab.io/DeepVO/", "journal-ref": null, "doi": "10.1109/ICRA.2017.7989236", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies monocular visual odometry (VO) problem. Most of existing\nVO algorithms are developed under a standard pipeline including feature\nextraction, feature matching, motion estimation, local optimisation, etc.\nAlthough some of them have demonstrated superior performance, they usually need\nto be carefully designed and specifically fine-tuned to work well in different\nenvironments. Some prior knowledge is also required to recover an absolute\nscale for monocular VO. This paper presents a novel end-to-end framework for\nmonocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs).\nSince it is trained and deployed in an end-to-end manner, it infers poses\ndirectly from a sequence of raw RGB images (videos) without adopting any module\nin the conventional VO pipeline. Based on the RCNNs, it not only automatically\nlearns effective feature representation for the VO problem through\nConvolutional Neural Networks, but also implicitly models sequential dynamics\nand relations using deep Recurrent Neural Networks. Extensive experiments on\nthe KITTI VO dataset show competitive performance to state-of-the-art methods,\nverifying that the end-to-end Deep Learning technique can be a viable\ncomplement to the traditional VO systems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 11:29:00 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Wang", "Sen", ""], ["Clark", "Ronald", ""], ["Wen", "Hongkai", ""], ["Trigoni", "Niki", ""]]}, {"id": "1709.08491", "submitter": "Olivier Colliot", "authors": "Igor Koval, Jean-Baptiste Schiratti, Alexandre Routier, Michael Bacci,\n  Olivier Colliot, St\\'ephanie Allassonni\\`ere, Stanley Durrleman", "title": "Statistical learning of spatiotemporal patterns from longitudinal\n  manifold-valued networks", "comments": null, "journal-ref": "Proc. Medical Image Computing and Computer-Assisted Intervention,\n  MICCAI 2017, Lecture Notes in Computer Science, volume 10433, pp 451-459,\n  Springer", "doi": "10.1007/978-3-319-66182-7_52", "report-no": null, "categories": "stat.ML cs.CV q-bio.NC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a mixed-effects model to learn spatiotempo-ral patterns on a\nnetwork by considering longitudinal measures distributed on a fixed graph. The\ndata come from repeated observations of subjects at different time points which\ntake the form of measurement maps distributed on a graph such as an image or a\nmesh. The model learns a typical group-average trajectory characterizing the\npropagation of measurement changes across the graph nodes. The subject-specific\ntrajectories are defined via spatial and temporal transformations of the\ngroup-average scenario, thus estimating the variability of spatiotemporal\npatterns within the group. To estimate population and individual model\nparameters, we adapted a stochastic version of the Expectation-Maximization\nalgorithm, the MCMC-SAEM. The model is used to describe the propagation of\ncortical atrophy during the course of Alzheimer's Disease. Model parameters\nshow the variability of this average pattern of atrophy in terms of\ntrajectories across brain regions, age at disease onset and pace of\npropagation. We show that the personalization of this model yields accurate\nprediction of maps of cortical thickness in patients.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 13:57:08 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Koval", "Igor", ""], ["Schiratti", "Jean-Baptiste", ""], ["Routier", "Alexandre", ""], ["Bacci", "Michael", ""], ["Colliot", "Olivier", ""], ["Allassonni\u00e8re", "St\u00e9phanie", ""], ["Durrleman", "Stanley", ""]]}, {"id": "1709.08515", "submitter": "Daniel Morris", "authors": "Daniel D. Morris, Brian Colonna, Paul Haley", "title": "LADAR-Based Mover Detection from Moving Vehicles", "comments": null, "journal-ref": "Proceedings of the 25th Army Science Conference, Nov 27-30, 2006", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting moving vehicles and people is crucial for safe operation of UGVs\nbut is challenging in cluttered, real world environments. We propose a\nregistration technique that enables objects to be robustly matched and tracked,\nand hence movers to be detected even in high clutter. Range data are acquired\nusing a 2D scanning Ladar from a moving platform. These are automatically\nclustered into objects and modeled using a surface density function. A\nBhattacharya similarity is optimized to register subsequent views of each\nobject enabling good discrimination and tracking, and hence mover detection.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 14:30:33 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Morris", "Daniel D.", ""], ["Colonna", "Brian", ""], ["Haley", "Paul", ""]]}, {"id": "1709.08524", "submitter": "Alexander Shekhovtsov", "authors": "Boris Flach, Alexander Shekhovtsov, Ondrej Fikar", "title": "Generative learning for deep networks", "comments": "submitted to AAAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning, taking into account full distribution of the data, referred to as\ngenerative, is not feasible with deep neural networks (DNNs) because they model\nonly the conditional distribution of the outputs given the inputs. Current\nsolutions are either based on joint probability models facing difficult\nestimation problems or learn two separate networks, mapping inputs to outputs\n(recognition) and vice-versa (generation). We propose an intermediate approach.\nFirst, we show that forward computation in DNNs with logistic sigmoid\nactivations corresponds to a simplified approximate Bayesian inference in a\ndirected probabilistic multi-layer model. This connection allows to interpret\nDNN as a probabilistic model of the output and all hidden units given the\ninput. Second, we propose that in order for the recognition and generation\nnetworks to be more consistent with the joint model of the data, weights of the\nrecognition and generator network should be related by transposition. We\ndemonstrate in a tentative experiment that such a coupled pair can be learned\ngeneratively, modelling the full distribution of the data, and has enough\ncapacity to perform well in both recognition and generation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 14:43:53 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Flach", "Boris", ""], ["Shekhovtsov", "Alexander", ""], ["Fikar", "Ondrej", ""]]}, {"id": "1709.08527", "submitter": "Emre Dogan", "authors": "Emre Dogan, Gonen Eren, Christian Wolf, Eric Lombardi, Atilla Baskurt", "title": "Multi-view pose estimation with mixtures-of-parts and adaptive viewpoint\n  selection", "comments": "8 pages, 7 figures, 4 tables. Second revision to the paper, as\n  submitted to IET Computer Vision on September 24th 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for human pose estimation which leverages information\nfrom multiple views to impose a strong prior on articulated pose. The novelty\nof the method concerns the types of coherence modelled. Consistency is\nmaximised over the different views through different terms modelling classical\ngeometric information (coherence of the resulting poses) as well as appearance\ninformation which is modelled as latent variables in the global energy\nfunction. Moreover, adequacy of each view is assessed and their contributions\nare adjusted accordingly. Experiments on the HumanEva and UMPM datasets show\nthat the proposed method significantly decreases the estimation error compared\nto single-view results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 14:45:23 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Dogan", "Emre", ""], ["Eren", "Gonen", ""], ["Wolf", "Christian", ""], ["Lombardi", "Eric", ""], ["Baskurt", "Atilla", ""]]}, {"id": "1709.08553", "submitter": "Jingya Wang", "authors": "Jingya Wang, Xiatian Zhu, Shaogang Gong, Wei Li", "title": "Attribute Recognition by Joint Recurrent Learning of Context and\n  Correlation", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising semantic pedestrian attributes in surveillance images is a\nchallenging task for computer vision, particularly when the imaging quality is\npoor with complex background clutter and uncontrolled viewing conditions, and\nthe number of labelled training data is small. In this work, we formulate a\nJoint Recurrent Learning (JRL) model for exploring attribute context and\ncorrelation in order to improve attribute recognition given small sized\ntraining data with poor quality images. The JRL model learns jointly pedestrian\nattribute correlations in a pedestrian image and in particular their sequential\nordering dependencies (latent high-order correlation) in an end-to-end\nencoder/decoder recurrent network. We demonstrate the performance advantage and\nrobustness of the JRL model over a wide range of state-of-the-art deep models\nfor pedestrian attribute recognition, multi-label image classification, and\nmulti-person image annotation on two largest pedestrian attribute benchmarks\nPETA and RAP.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 15:28:47 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Wang", "Jingya", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""], ["Li", "Wei", ""]]}, {"id": "1709.08603", "submitter": "Tony Lindeberg", "authors": "Tony Lindeberg", "title": "Dense scale selection over space, time and space-time", "comments": "43 pages, 14 figures, 3 tables", "journal-ref": "SIAM Journal on Imaging Sciences, 11(1): 407-441, 2018", "doi": "10.1137/17M114892X", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale selection methods based on local extrema over scale of scale-normalized\nderivatives have been primarily developed to be applied sparsely --- at image\npoints where the magnitude of a scale-normalized differential expression\nadditionally assumes local extrema over the domain where the data are defined.\nThis paper presents a methodology for performing dense scale selection, so that\nhypotheses about local characteristic scales in images, temporal signals and\nvideo can be computed at every image point and every time moment.\n  A critical problem when designing mechanisms for dense scale selection is\nthat the scale at which scale-normalized differential entities assume local\nextrema over scale can be strongly dependent on the local order of the locally\ndominant differential structure. To address this problem, we propose a\nmethodology where local extrema over scale are detected of a quasi quadrature\nmeasure involving scale-space derivatives up to order two and propose two\nindependent mechanisms to reduce the phase dependency of the local scale\nestimates by: (i) introducing a second layer of post-smoothing prior to the\ndetection of local extrema over scale and (ii) performing local phase\ncompensation based on a model of the phase dependency of the local scale\nestimates depending on the relative strengths between first- vs. second-order\ndifferential structure.\n  This general methodology is applied over three types of domains: (i) spatial\nimages, (ii) temporal signals and (iii) spatio-temporal video. Experiments show\nthat the proposed methodology leads to intuitively reasonable results with\nlocal scale estimates that reflect variations in the characteristic scales of\nlocally dominant structures over space and time.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 17:14:17 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 08:30:09 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 14:45:11 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 10:58:28 GMT"}, {"version": "v5", "created": "Fri, 3 Aug 2018 05:06:46 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Lindeberg", "Tony", ""]]}, {"id": "1709.08605", "submitter": "Maxim Borisyak", "authors": "Maxim Borisyak, Michail Usvyatsov, Michael Mulhearn, Chase Shimmin and\n  Andrey Ustyuzhanin", "title": "Muon Trigger for Mobile Phones", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/898/3/032048", "report-no": null, "categories": "cs.CV astro-ph.IM physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CRAYFIS experiment proposes to use privately owned mobile phones as a\nground detector array for Ultra High Energy Cosmic Rays. Upon interacting with\nEarth's atmosphere, these events produce extensive particle showers which can\nbe detected by cameras on mobile phones. A typical shower contains\nminimally-ionizing particles such as muons. As these particles interact with\nCMOS image sensors, they may leave tracks of faintly-activated pixels that are\nsometimes hard to distinguish from random detector noise. Triggers that rely on\nthe presence of very bright pixels within an image frame are not efficient in\nthis case.\n  We present a trigger algorithm based on Convolutional Neural Networks which\nselects images containing such tracks and are evaluated in a lazy manner: the\nresponse of each successive layer is computed only if activation of the current\nlayer satisfies a continuation criterion. Usage of neural networks increases\nthe sensitivity considerably comparable with image thresholding, while the lazy\nevaluation allows for execution of the trigger under the limited computational\npower of mobile phones.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 17:15:09 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Borisyak", "Maxim", ""], ["Usvyatsov", "Michail", ""], ["Mulhearn", "Michael", ""], ["Shimmin", "Chase", ""], ["Ustyuzhanin", "Andrey", ""]]}, {"id": "1709.08610", "submitter": "Maxim Borisyak", "authors": "Maxim Borisyak, Andrey Ustyuzhanin, Denis Derkach and Mikhail Belous", "title": "Numerical optimization for Artificial Retina Algorithm", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/898/3/032046", "report-no": null, "categories": "cs.CV hep-ex physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-energy physics experiments rely on reconstruction of the trajectories of\nparticles produced at the interaction point. This is a challenging task,\nespecially in the high track multiplicity environment generated by p-p\ncollisions at the LHC energies. A typical event includes hundreds of signal\nexamples (interesting decays) and a significant amount of noise (uninteresting\nexamples).\n  This work describes a modification of the Artificial Retina algorithm for\nfast track finding: numerical optimization methods were adopted for fast local\ntrack search. This approach allows for considerable reduction of the total\ncomputational time per event. Test results on simplified simulated model of\nLHCb VELO (VErtex LOcator) detector are presented. Also this approach is\nwell-suited for implementation of paralleled computations as GPGPU which look\nvery attractive in the context of upcoming detector upgrades.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 17:33:11 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 23:42:24 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Borisyak", "Maxim", ""], ["Ustyuzhanin", "Andrey", ""], ["Derkach", "Denis", ""], ["Belous", "Mikhail", ""]]}, {"id": "1709.08666", "submitter": "Jennifer Carlet", "authors": "Jennifer Carlet, Bernard Abayowa", "title": "Fast Vehicle Detection in Aerial Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several real-time or near real-time object detectors have\nbeen developed. However these object detectors are typically designed for\nfirst-person view images where the subject is large in the image and do not\ndirectly apply well to detecting vehicles in aerial imagery. Though some\ndetectors have been developed for aerial imagery, these are either slow or do\nnot handle multi-scale imagery very well. Here the popular YOLOv2 detector is\nmodified to vastly improve it's performance on aerial data. The modified\ndetector is compared to Faster RCNN on several aerial imagery datasets. The\nproposed detector gives near state of the art performance at more than 4x the\nspeed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 18:41:01 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Carlet", "Jennifer", ""], ["Abayowa", "Bernard", ""]]}, {"id": "1709.08680", "submitter": "Pingfan Song", "authors": "Pingfan Song (Student Member, IEEE), Xin Deng (Student Member, IEEE),\n  Jo\\~ao F. C. Mota (Member, IEEE), Nikos Deligiannis (Member, IEEE), Pier\n  Luigi Dragotti (Fellow, IEEE), and Miguel R. D. Rodrigues (Senior Member,\n  IEEE)", "title": "Multimodal Image Super-resolution via Joint Sparse Representations\n  induced by Coupled Dictionaries", "comments": "13 pages, 8 figures, 9 tables", "journal-ref": null, "doi": "10.1109/TCI.2019.2916502", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data processing problems often involve various image modalities\nassociated with a certain scene, including RGB images, infrared images or\nmulti-spectral images. The fact that different image modalities often share\ncertain attributes, such as certain edges, textures and other structure\nprimitives, represents an opportunity to enhance various image processing\ntasks. This paper proposes a new approach to construct a high-resolution (HR)\nversion of a low-resolution (LR) image given another HR image modality as\nreference, based on joint sparse representations induced by coupled\ndictionaries. Our approach, which captures the similarities and disparities\nbetween different image modalities in a learned sparse feature domain in\n\\emph{lieu} of the original image domain, consists of two phases. The coupled\ndictionary learning phase is used to learn a set of dictionaries that couple\ndifferent image modalities in the sparse feature domain given a set of training\ndata. In turn, the coupled super-resolution phase leverages such coupled\ndictionaries to construct a HR version of the LR target image given another\nrelated image modality. One of the merits of our sparsity-driven approach\nrelates to the fact that it overcomes drawbacks such as the texture copying\nartifacts commonly resulting from inconsistency between the guidance and target\nimages. Experiments on real multimodal images demonstrate that incorporating\nappropriate guidance information via joint sparse representation induced by\ncoupled dictionary learning brings notable benefits in the super-resolution\ntask with respect to the state-of-the-art. Of particular relevance, the\nproposed approach also demonstrates better robustness than competing\ndeep-learning-based methods in the presence of noise.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 19:04:08 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 19:41:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Song", "Pingfan", "", "Student Member, IEEE"], ["Deng", "Xin", "", "Student Member, IEEE"], ["Mota", "Jo\u00e3o F. C.", "", "Member, IEEE"], ["Deligiannis", "Nikos", "", "Member, IEEE"], ["Dragotti", "Pier Luigi", "", "Fellow, IEEE"], ["Rodrigues", "Miguel R. D.", "", "Senior Member,\n  IEEE"]]}, {"id": "1709.08739", "submitter": "Yeejin Lee", "authors": "Y. Lee, K. Hirakawa, and T. Nguyen", "title": "Camera-Aware Multi-Resolution Analysis (CAMRA) for Raw Sensor Data\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel lossless and lossy compression scheme for color filter\narray~(CFA) sampled images based on the wavelet transform of them. Our analysis\nsuggests that the wavelet coefficients of HL and LH subbands are highly\ncorrelated. Hence, we decorrelate Mallat wavelet packet decomposition to\nfurther sparsify the coefficients. In addition, we develop a camera processing\npipeline for compressing CFA sampled images aimed at maximizing the quality of\nthe color images constructed from the compressed CFA sampled images. We\nvalidated our theoretical analysis and the performance of the proposed\ncompression scheme using images of natural scenes captured in a raw format. The\nexperimental results verify that our proposed method improves coding efficiency\nrelative to the standard and the state-of-the-art compression schemes CFA\nsampled images.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 22:50:28 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Lee", "Y.", ""], ["Hirakawa", "K.", ""], ["Nguyen", "T.", ""]]}, {"id": "1709.08761", "submitter": "Srikar Appalaraju", "authors": "Srikar Appalaraju, Vineet Chaoji", "title": "Image similarity using Deep CNN and Curriculum Learning", "comments": "9 pages, 6 figures, GHCI 17 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image similarity involves fetching similar looking images given a reference\nimage. Our solution called SimNet, is a deep siamese network which is trained\non pairs of positive and negative images using a novel online pair mining\nstrategy inspired by Curriculum learning. We also created a multi-scale CNN,\nwhere the final image embedding is a joint representation of top as well as\nlower layer embedding's. We go on to show that this multi-scale siamese network\nis better at capturing fine grained image similarities than traditional CNN's.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 00:25:40 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 15:34:34 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Appalaraju", "Srikar", ""], ["Chaoji", "Vineet", ""]]}, {"id": "1709.08772", "submitter": "Md Jahidul Islam", "authors": "Md Jahidul Islam, Marc Ho and Junaed Sattar", "title": "Dynamic Reconfiguration of Mission Parameters in Underwater Human-Robot\n  Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a real-time programming and parameter reconfiguration\nmethod for autonomous underwater robots in human-robot collaborative tasks.\nUsing a set of intuitive and meaningful hand gestures, we develop a\nsyntactically simple framework that is computationally more efficient than a\ncomplex, grammar-based approach. In the proposed framework, a convolutional\nneural network is trained to provide accurate hand gesture recognition;\nsubsequently, a finite-state machine-based deterministic model performs\nefficient gesture-to-instruction mapping, and further improves robustness of\nthe interaction scheme. The key aspect of this framework is that it can be\neasily adopted by divers for communicating simple instructions to underwater\nrobots without using artificial tags such as fiducial markers, or requiring\nthem to memorize a potentially complex set of language rules. Extensive\nexperiments are performed both on field-trial data and through simulation,\nwhich demonstrate the robustness, efficiency, and portability of this framework\nin a number of different scenarios. Finally, a user interaction study is\npresented that illustrates the gain in usability of our proposed interaction\nframework compared to the existing methods for underwater domains.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 01:04:46 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 23:28:38 GMT"}, {"version": "v3", "created": "Mon, 2 Oct 2017 22:48:21 GMT"}, {"version": "v4", "created": "Sun, 19 Nov 2017 03:46:18 GMT"}, {"version": "v5", "created": "Sun, 21 Jan 2018 19:01:45 GMT"}, {"version": "v6", "created": "Tue, 30 Jan 2018 00:10:49 GMT"}, {"version": "v7", "created": "Fri, 9 Feb 2018 23:21:35 GMT"}, {"version": "v8", "created": "Tue, 20 Feb 2018 23:04:45 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Islam", "Md Jahidul", ""], ["Ho", "Marc", ""], ["Sattar", "Junaed", ""]]}, {"id": "1709.08820", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Quan Z. Sheng, Salil S. Kanhere, Tao Gu, Dalin\n  Zhang", "title": "Converting Your Thoughts to Texts: Enabling Brain Typing via Deep\n  Feature Learning of EEG Signals", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An electroencephalography (EEG) based Brain Computer Interface (BCI) enables\npeople to communicate with the outside world by interpreting the EEG signals of\ntheir brains to interact with devices such as wheelchairs and intelligent\nrobots. More specifically, motor imagery EEG (MI-EEG), which reflects a\nsubjects active intent, is attracting increasing attention for a variety of BCI\napplications. Accurate classification of MI-EEG signals while essential for\neffective operation of BCI systems, is challenging due to the significant noise\ninherent in the signals and the lack of informative correlation between the\nsignals and brain activities. In this paper, we propose a novel deep neural\nnetwork based learning framework that affords perceptive insights into the\nrelationship between the MI-EEG data and brain activities. We design a joint\nconvolutional recurrent neural network that simultaneously learns robust\nhigh-level feature presentations through low-dimensional dense embeddings from\nraw MI-EEG signals. We also employ an Autoencoder layer to eliminate various\nartifacts such as background activities. The proposed approach has been\nevaluated extensively on a large- scale public MI-EEG dataset and a limited but\neasy-to-deploy dataset collected in our lab. The results show that our approach\noutperforms a series of baselines and the competitive state-of-the- art\nmethods, yielding a classification accuracy of 95.53%. The applicability of our\nproposed approach is further demonstrated with a practical BCI system for\ntyping.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 04:20:34 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Sheng", "Quan Z.", ""], ["Kanhere", "Salil S.", ""], ["Gu", "Tao", ""], ["Zhang", "Dalin", ""]]}, {"id": "1709.08828", "submitter": "Chunhua Shen", "authors": "Hui Li, Peng Wang, Chunhua Shen", "title": "Towards End-to-End Car License Plates Detection and Recognition with\n  Deep Neural Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we tackle the problem of car license plate detection and\nrecognition in natural scene images. We propose a unified deep neural network\nwhich can localize license plates and recognize the letters simultaneously in a\nsingle forward pass. The whole network can be trained end-to-end. In contrast\nto existing approaches which take license plate detection and recognition as\ntwo separate tasks and settle them step by step, our method jointly solves\nthese two tasks by a single network. It not only avoids intermediate error\naccumulation, but also accelerates the processing speed. For performance\nevaluation, three datasets including images captured from various scenes under\ndifferent conditions are tested. Extensive experiments show the effectiveness\nand efficiency of our proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 04:50:23 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Li", "Hui", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""]]}, {"id": "1709.08831", "submitter": "Sandeep Konam", "authors": "Sandeep Konam, Stephanie Rosenthal, Manuela Veloso", "title": "UAV and Service Robot Coordination for Indoor Object Search Tasks", "comments": "IJCAI-2016 Workshop on Autonomous Mobile Service Robots", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our CoBot robots have successfully performed a variety of service tasks in\nour multi-building environment including accompanying people to meetings and\ndelivering objects to offices due to its navigation and localization\ncapabilities. However, they lack the capability to visually search over desks\nand other confined locations for an object of interest. Conversely, an\ninexpensive GPS-denied quadcopter platform such as the Parrot ARDrone 2.0 could\nperform this object search task if it had access to reasonable localization. In\nthis paper, we propose the concept of coordination between CoBot and the Parrot\nARDrone 2.0 to perform service-based object search tasks, in which CoBot\nlocalizes and navigates to the general search areas carrying the ARDrone and\nthe ARDrone searches locally for objects. We propose a vision-based moving\ntarget navigation algorithm that enables the ARDrone to localize with respect\nto CoBot, search for objects, and return to the CoBot for future searches. We\ndemonstrate our algorithm in indoor environments on several search\ntrajectories.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 05:04:37 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Konam", "Sandeep", ""], ["Rosenthal", "Stephanie", ""], ["Veloso", "Manuela", ""]]}, {"id": "1709.08855", "submitter": "Mohammad Haris Baig", "authors": "Mohammad Haris Baig, Vladlen Koltun, Lorenzo Torresani", "title": "Learning to Inpaint for Image Compression", "comments": "Published in Advances in Neural Information Processing Systems (NIPS\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of deep architectures for lossy image compression. We\npresent two architectural recipes in the context of multi-stage progressive\nencoders and empirically demonstrate their importance on compression\nperformance. Specifically, we show that: (a) predicting the original image data\nfrom residuals in a multi-stage progressive architecture facilitates learning\nand leads to improved performance at approximating the original content and (b)\nlearning to inpaint (from neighboring image pixels) before performing\ncompression reduces the amount of information that must be stored to achieve a\nhigh-quality approximation. Incorporating these design choices in a baseline\nprogressive encoder yields an average reduction of over $60\\%$ in file size\nwith similar quality compared to the original residual encoder.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 06:30:55 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 05:09:24 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 05:24:47 GMT"}, {"version": "v4", "created": "Fri, 10 Nov 2017 06:58:40 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Baig", "Mohammad Haris", ""], ["Koltun", "Vladlen", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1709.08868", "submitter": "Ruiqi Gao", "authors": "Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, Ying Nian Wu", "title": "Learning Energy-Based Models as Generative ConvNets via Multi-grid\n  Modeling and Sampling", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multi-grid method for learning energy-based generative\nConvNet models of images. For each grid, we learn an energy-based probabilistic\nmodel where the energy function is defined by a bottom-up convolutional neural\nnetwork (ConvNet or CNN). Learning such a model requires generating synthesized\nexamples from the model. Within each iteration of our learning algorithm, for\neach observed training image, we generate synthesized images at multiple grids\nby initializing the finite-step MCMC sampling from a minimal 1 x 1 version of\nthe training image. The synthesized image at each subsequent grid is obtained\nby a finite-step MCMC initialized from the synthesized image generated at the\nprevious coarser grid. After obtaining the synthesized examples, the parameters\nof the models at multiple grids are updated separately and simultaneously based\non the differences between synthesized and observed examples. We show that this\nmulti-grid method can learn realistic energy-based generative ConvNet models,\nand it outperforms the original contrastive divergence (CD) and persistent CD.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 07:48:52 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 08:30:01 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 22:43:45 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Gao", "Ruiqi", ""], ["Lu", "Yang", ""], ["Zhou", "Junpei", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1709.08872", "submitter": "Timo L\\\"uddecke", "authors": "Timo L\\\"uddecke, Florentin W\\\"org\\\"otter", "title": "Learning to Label Affordances from Simulated and Real Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An autonomous robot should be able to evaluate the affordances that are\noffered by a given situation. Here we address this problem by designing a\nsystem that can densely predict affordances given only a single 2D RGB image.\nThis is achieved with a convolutional neural network (ResNet), which we combine\nwith refinement modules recently proposed for addressing semantic image\nsegmentation. We define a novel cost function, which is able to handle\n(potentially multiple) affordances of objects and their parts in a pixel-wise\nmanner even in the case of incomplete data. We perform qualitative as well as\nquantitative evaluations with simulated and real data assessing 15 different\naffordances. In general, we find that affordances, which are well-enough\nrepresented in the training data, are correctly recognized with a substantial\nfraction of correctly assigned pixels. Furthermore, we show that our model\noutperforms several baselines. Hence, this method can give clear action\nguidelines for a robot.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 07:58:19 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["L\u00fcddecke", "Timo", ""], ["W\u00f6rg\u00f6tter", "Florentin", ""]]}, {"id": "1709.08924", "submitter": "Ranjeet Ranjan Jha", "authors": "Ranjeet Ranjan Jha, Daksh Thapar, Shreyas Malakarjun Patil, Aditya\n  Nigam", "title": "UBSegNet: Unified Biometric Region of Interest Segmentation Network", "comments": "4th Asian Conference on Pattern Recognition (ACPR 2017)", "journal-ref": null, "doi": "10.1109/ACPR.2017.148", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital human identity management, can now be seen as a social necessity, as\nit is essentially required in almost every public sector such as, financial\ninclusions, security, banking, social networking e.t.c. Hence, in today's\nrampantly emerging world with so many adversarial entities, relying on a single\nbiometric trait is being too optimistic. In this paper, we have proposed a\nnovel end-to-end, Unified Biometric ROI Segmentation Network (UBSegNet), for\nextracting region of interest from five different biometric traits viz. face,\niris, palm, knuckle and 4-slap fingerprint. The architecture of the proposed\nUBSegNet consists of two stages: (i) Trait classification and (ii) Trait\nlocalization. For these stages, we have used a state of the art region based\nconvolutional neural network (RCNN), comprising of three major parts namely\nconvolutional layers, region proposal network (RPN) along with classification\nand regression heads. The model has been evaluated over various huge publicly\navailable biometric databases. To the best of our knowledge this is the first\nunified architecture proposed, segmenting multiple biometric traits. It has\nbeen tested over around 5000 * 5 = 25,000 images (5000 images per trait) and\nproduces very good results. Our work on unified biometric segmentation, opens\nup the vast opportunities in the field of multiple biometric traits based\nauthentication systems.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 10:14:35 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Jha", "Ranjeet Ranjan", ""], ["Thapar", "Daksh", ""], ["Patil", "Shreyas Malakarjun", ""], ["Nigam", "Aditya", ""]]}, {"id": "1709.08962", "submitter": "S\\'everine Habert", "authors": "S\\'everine Habert and Ma Meng and Pascal Fallavollita and Nassir Navab", "title": "Multi-layer Visualization for Medical Mixed Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical Mixed Reality helps surgeons to contextualize intraoperative data\nwith video of the surgical scene. Nonetheless, the surgical scene and\nanatomical target are often occluded by surgical instruments and surgeon hands.\nIn this paper and to our knowledge, we propose a multi-layer visualization in\nMedical Mixed Reality solution which subtly improves a surgeon's visualization\nby making transparent the occluding objects. As an example scenario, we use an\naugmented reality C-arm fluoroscope device. A video image is created using a\nvolumetric-based image synthesization technique and stereo-RGBD cameras mounted\non the C-arm. From this synthesized view, the background which is occluded by\nthe surgical instruments and surgeon hands is recovered by modifying the\nvolumetric-based image synthesization technique. The occluding objects can,\ntherefore, become transparent over the surgical scene. Experimentation with\ndifferent augmented reality scenarios yield results demonstrating that the\nbackground of the surgical scenes can be recovered with accuracy between\n45%-99%. In conclusion, we presented a solution that a Mixed Reality solution\nfor medicine, providing transparency to objects occluding the surgical scene.\nThis work is also the first application of volumetric field for Diminished\nReality/ Mixed Reality.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 12:13:01 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Habert", "S\u00e9verine", ""], ["Meng", "Ma", ""], ["Fallavollita", "Pascal", ""], ["Navab", "Nassir", ""]]}, {"id": "1709.09075", "submitter": "Kaisar Kushibar", "authors": "Kaisar Kushibar, Sergi Valverde, Sandra Gonzalez-Villa, Jose Bernal,\n  Mariano Cabezas, Arnau Oliver, Xavier Llado", "title": "Automated sub-cortical brain structure segmentation combining spatial\n  and deep convolutional features", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2018.06.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sub-cortical brain structure segmentation in Magnetic Resonance Images (MRI)\nhas attracted the interest of the research community for a long time because\nmorphological changes in these structures are related to different\nneurodegenerative disorders. However, manual segmentation of these structures\ncan be tedious and prone to variability, highlighting the need for robust\nautomated segmentation methods. In this paper, we present a novel convolutional\nneural network based approach for accurate segmentation of the sub-cortical\nbrain structures that combines both convolutional and prior spatial features\nfor improving the segmentation accuracy. In order to increase the accuracy of\nthe automated segmentation, we propose to train the network using a restricted\nsample selection to force the network to learn the most difficult parts of the\nstructures. We evaluate the accuracy of the proposed method on the public\nMICCAI 2012 challenge and IBSR 18 datasets, comparing it with different\navailable state-of-the-art methods and other recently proposed deep learning\napproaches. On the MICCAI 2012 dataset, our method shows an excellent\nperformance comparable to the best challenge participant strategy, while\nperforming significantly better than state-of-the-art techniques such as\nFreeSurfer and FIRST. On the IBSR 18 dataset, our method also exhibits a\nsignificant increase in the performance with respect to not only FreeSurfer and\nFIRST, but also comparable or better results than other recent deep learning\napproaches. Moreover, our experiments show that both the addition of the\nspatial priors and the restricted sampling strategy have a significant effect\non the accuracy of the proposed method. In order to encourage the\nreproducibility and the use of the proposed method, a public version of our\napproach is available to download for the neuroimaging community.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 15:02:16 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Kushibar", "Kaisar", ""], ["Valverde", "Sergi", ""], ["Gonzalez-Villa", "Sandra", ""], ["Bernal", "Jose", ""], ["Cabezas", "Mariano", ""], ["Oliver", "Arnau", ""], ["Llado", "Xavier", ""]]}, {"id": "1709.09077", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Dalin Zhang, Xianzhi Wang, Quan Z. Sheng, Tao\n  Gu", "title": "Multi-Person Brain Activity Recognition via Comprehensive EEG Signal\n  Analysis", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An electroencephalography (EEG) based brain activity recognition is a\nfundamental field of study for a number of significant applications such as\nintention prediction, appliance control, and neurological disease diagnosis in\nsmart home and smart healthcare domains. Existing techniques mostly focus on\nbinary brain activity recognition for a single person, which limits their\ndeployment in wider and complex practical scenarios. Therefore, multi-person\nand multi-class brain activity recognition has obtained popularity recently.\nAnother challenge faced by brain activity recognition is the low recognition\naccuracy due to the massive noises and the low signal-to-noise ratio in EEG\nsignals. Moreover, the feature engineering in EEG processing is time-consuming\nand highly re- lies on the expert experience. In this paper, we attempt to\nsolve the above challenges by proposing an approach which has better EEG\ninterpretation ability via raw Electroencephalography (EEG) signal analysis for\nmulti-person and multi-class brain activity recognition. Specifically, we\nanalyze inter-class and inter-person EEG signal characteristics, based on which\nto capture the discrepancy of inter-class EEG data. Then, we adopt an\nAutoencoder layer to automatically refine the raw EEG signals by eliminating\nvarious artifacts. We evaluate our approach on both a public and a local EEG\ndatasets and conduct extensive experiments to explore the effect of several\nfactors (such as normalization methods, training data size, and Autoencoder\nhidden neuron size) on the recognition results. The experimental results show\nthat our approach achieves a high accuracy comparing to competitive\nstate-of-the-art methods, indicating its potential in promoting future research\non multi-person EEG recognition.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 15:03:13 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Zhang", "Dalin", ""], ["Wang", "Xianzhi", ""], ["Sheng", "Quan Z.", ""], ["Gu", "Tao", ""]]}, {"id": "1709.09106", "submitter": "Ryota Hinami", "authors": "Ryota Hinami, Yusuke Matsui, Shin'ichi Satoh", "title": "Region-Based Image Retrieval Revisited", "comments": "To appear in ACM Multimedia 2017 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region-based image retrieval (RBIR) technique is revisited. In early attempts\nat RBIR in the late 90s, researchers found many ways to specify region-based\nqueries and spatial relationships; however, the way to characterize the\nregions, such as by using color histograms, were very poor at that time. Here,\nwe revisit RBIR by incorporating semantic specification of objects and\nintuitive specification of spatial relationships. Our contributions are the\nfollowing. First, to support multiple aspects of semantic object specification\n(category, instance, and attribute), we propose a multitask CNN feature that\nallows us to use deep learning technique and to jointly handle multi-aspect\nobject specification. Second, to help users specify spatial relationships among\nobjects in an intuitive way, we propose recommendation techniques of spatial\nrelationships. In particular, by mining the search results, a system can\nrecommend feasible spatial relationships among the objects. The system also can\nrecommend likely spatial relationships by assigned object category names based\non language prior. Moreover, object-level inverted indexing supports very fast\nshortlist generation, and re-ranking based on spatial constraints provides\nusers with instant RBIR experiences.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 16:09:48 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Hinami", "Ryota", ""], ["Matsui", "Yusuke", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1709.09118", "submitter": "Qiuyuan Huang", "authors": "Qiuyuan Huang, Paul Smolensky, Xiaodong He, Li Deng, Dapeng Wu", "title": "Tensor Product Generation Networks for Deep NLP Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the design of deep networks for natural language\nprocessing (NLP), based on the general technique of Tensor Product\nRepresentations (TPRs) for encoding and processing symbol structures in\ndistributed neural networks. A network architecture --- the Tensor Product\nGeneration Network (TPGN) --- is proposed which is capable in principle of\ncarrying out TPR computation, but which uses unconstrained deep learning to\ndesign its internal representations. Instantiated in a model for image-caption\ngeneration, TPGN outperforms LSTM baselines when evaluated on the COCO dataset.\nThe TPR-capable structure enables interpretation of internal representations\nand operations, which prove to contain considerable grammatical content. Our\ncaption-generation model can be interpreted as generating sequences of\ngrammatical categories and retrieving words by their categories from a plan\nencoded as a distributed representation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 16:32:20 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 07:50:59 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 02:45:39 GMT"}, {"version": "v4", "created": "Tue, 10 Oct 2017 06:46:31 GMT"}, {"version": "v5", "created": "Sat, 16 Dec 2017 12:01:09 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Huang", "Qiuyuan", ""], ["Smolensky", "Paul", ""], ["He", "Xiaodong", ""], ["Deng", "Li", ""], ["Wu", "Dapeng", ""]]}, {"id": "1709.09121", "submitter": "Ryota Hinami", "authors": "Ryota Hinami, Tao Mei, Shin'ichi Satoh", "title": "Joint Detection and Recounting of Abnormal Events by Learning Deep\n  Generic Knowledge", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of joint detection and recounting of\nabnormal events in videos. Recounting of abnormal events, i.e., explaining why\nthey are judged to be abnormal, is an unexplored but critical task in video\nsurveillance, because it helps human observers quickly judge if they are false\nalarms or not. To describe the events in the human-understandable form for\nevent recounting, learning generic knowledge about visual concepts (e.g.,\nobject and action) is crucial. Although convolutional neural networks (CNNs)\nhave achieved promising results in learning such concepts, it remains an open\nquestion as to how to effectively use CNNs for abnormal event detection, mainly\ndue to the environment-dependent nature of the anomaly detection. In this\npaper, we tackle this problem by integrating a generic CNN model and\nenvironment-dependent anomaly detectors. Our approach first learns CNN with\nmultiple visual tasks to exploit semantic information that is useful for\ndetecting and recounting abnormal events. By appropriately plugging the model\ninto anomaly detectors, we can detect and recount abnormal events while taking\nadvantage of the discriminative power of CNNs. Our approach outperforms the\nstate-of-the-art on Avenue and UCSD Ped2 benchmarks for abnormal event\ndetection and also produces promising results of abnormal event recounting.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 16:38:03 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Hinami", "Ryota", ""], ["Mei", "Tao", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1709.09215", "submitter": "Zoya Bylinskii", "authors": "Zoya Bylinskii, Sami Alsheikh, Spandan Madan, Adria Recasens, Kimberli\n  Zhong, Hanspeter Pfister, Fredo Durand, Aude Oliva", "title": "Understanding Infographics through Textual and Visual Tag Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of visual hashtag discovery for infographics:\nextracting visual elements from an infographic that are diagnostic of its\ntopic. Given an infographic as input, our computational approach automatically\noutputs textual and visual elements predicted to be representative of the\ninfographic content. Concretely, from a curated dataset of 29K large\ninfographic images sampled across 26 categories and 391 tags, we present an\nautomated two step approach. First, we extract the text from an infographic and\nuse it to predict text tags indicative of the infographic content. And second,\nwe use these predicted text tags as a supervisory signal to localize the most\ndiagnostic visual elements from within the infographic i.e. visual hashtags. We\nreport performances on a categorization and multi-label tag prediction problem\nand compare our proposed visual hashtags to human annotations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 18:45:28 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Bylinskii", "Zoya", ""], ["Alsheikh", "Sami", ""], ["Madan", "Spandan", ""], ["Recasens", "Adria", ""], ["Zhong", "Kimberli", ""], ["Pfister", "Hanspeter", ""], ["Durand", "Fredo", ""], ["Oliva", "Aude", ""]]}, {"id": "1709.09233", "submitter": "Dan Nguyen", "authors": "Dan Nguyen, Troy Long, Xun Jia, Weiguo Lu, Xuejun Gu, Zohaib Iqbal,\n  Steve Jiang", "title": "A feasibility study for predicting optimal radiation therapy dose\n  distributions of prostate cancer patients from patient anatomy using deep\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of treatment modalities in radiation therapy for cancer\npatients, outcomes have improved, but at the cost of increased treatment plan\ncomplexity and planning time. The accurate prediction of dose distributions\nwould alleviate this issue by guiding clinical plan optimization to save time\nand maintain high quality plans. We have modified a convolutional deep network\nmodel, U-net (originally designed for segmentation purposes), for predicting\ndose from patient image contours of the planning target volume (PTV) and organs\nat risk (OAR). We show that, as an example, we are able to accurately predict\nthe dose of intensity-modulated radiation therapy (IMRT) for prostate cancer\npatients, where the average Dice similarity coefficient is 0.91 when comparing\nthe predicted vs. true isodose volumes between 0% and 100% of the prescription\ndose. The average value of the absolute differences in [max, mean] dose is\nfound to be under 5% of the prescription dose, specifically for each structure\nis [1.80%, 1.03%](PTV), [1.94%, 4.22%](Bladder), [1.80%, 0.48%](Body), [3.87%,\n1.79%](L Femoral Head), [5.07%, 2.55%](R Femoral Head), and [1.26%,\n1.62%](Rectum) of the prescription dose. We thus managed to map a desired\nradiation dose distribution from a patient's PTV and OAR contours. As an\nadditional advantage, relatively little data was used in the techniques and\nmodels described in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 19:43:29 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 06:26:06 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 23:45:25 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 21:34:27 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Nguyen", "Dan", ""], ["Long", "Troy", ""], ["Jia", "Xun", ""], ["Lu", "Weiguo", ""], ["Gu", "Xuejun", ""], ["Iqbal", "Zohaib", ""], ["Jiang", "Steve", ""]]}, {"id": "1709.09283", "submitter": "Sepideh Hosseinzadeh", "authors": "Sepideh Hosseinzadeh, Moein Shakeri, Hong Zhang", "title": "Fast Shadow Detection from a Single Image Using a Patched Convolutional\n  Neural Network", "comments": "6 pages, 5 figures, Submitted to IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, various shadow detection methods from a single image have\nbeen proposed and used in vision systems; however, most of them are not\nappropriate for the robotic applications due to the expensive time complexity.\nThis paper introduces a fast shadow detection method using a deep learning\nframework, with a time cost that is appropriate for robotic applications. In\nour solution, we first obtain a shadow prior map with the help of multi-class\nsupport vector machine using statistical features. Then, we use a semantic-\naware patch-level Convolutional Neural Network that efficiently trains on\nshadow examples by combining the original image and the shadow prior map.\nExperiments on benchmark datasets demonstrate the proposed method significantly\ndecreases the time complexity of shadow detection, by one or two orders of\nmagnitude compared with state-of-the-art methods, without losing accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 23:20:42 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 18:35:25 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Hosseinzadeh", "Sepideh", ""], ["Shakeri", "Moein", ""], ["Zhang", "Hong", ""]]}, {"id": "1709.09297", "submitter": "Mang Ye", "authors": "Mang Ye, Andy J Ma, Liang Zheng, Jiawei Li, P C Yuen", "title": "Dynamic Label Graph Matching for Unsupervised Video Re-Identification", "comments": "Accepted by ICCV 2017. Revised our IDE results on MARS dataset under\n  standard evaluation protocol", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label estimation is an important component in an unsupervised person\nre-identification (re-ID) system. This paper focuses on cross-camera label\nestimation, which can be subsequently used in feature learning to learn robust\nre-ID models. Specifically, we propose to construct a graph for samples in each\ncamera, and then graph matching scheme is introduced for cross-camera labeling\nassociation. While labels directly output from existing graph matching methods\nmay be noisy and inaccurate due to significant cross-camera variations, this\npaper proposes a dynamic graph matching (DGM) method. DGM iteratively updates\nthe image graph and the label estimation process by learning a better feature\nspace with intermediate estimated labels. DGM is advantageous in two aspects:\n1) the accuracy of estimated labels is improved significantly with the\niterations; 2) DGM is robust to noisy initial training data. Extensive\nexperiments conducted on three benchmarks including the large-scale MARS\ndataset show that DGM yields competitive performance to fully supervised\nbaselines, and outperforms competing unsupervised learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 01:07:10 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Ye", "Mang", ""], ["Ma", "Andy J", ""], ["Zheng", "Liang", ""], ["Li", "Jiawei", ""], ["Yuen", "P C", ""]]}, {"id": "1709.09304", "submitter": "Zhizhong Zhang", "authors": "Zhizhong Zhang, Yuan Xie, Wensheng Zhang, Qi Tian", "title": "Effective Image Retrieval via Multilinear Multi-index Fusion", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-index fusion has demonstrated impressive performances in retrieval task\nby integrating different visual representations in a unified framework.\nHowever, previous works mainly consider propagating similarities via neighbor\nstructure, ignoring the high order information among different visual\nrepresentations. In this paper, we propose a new multi-index fusion scheme for\nimage retrieval. By formulating this procedure as a multilinear based\noptimization problem, the complementary information hidden in different indexes\ncan be explored more thoroughly. Specially, we first build our multiple indexes\nfrom various visual representations. Then a so-called index-specific functional\nmatrix, which aims to propagate similarities, is introduced for updating the\noriginal index. The functional matrices are then optimized in a unified tensor\nspace to achieve a refinement, such that the relevant images can be pushed more\ncloser. The optimization problem can be efficiently solved by the augmented\nLagrangian method with theoretical convergence guarantee. Unlike the\ntraditional multi-index fusion scheme, our approach embeds the multi-index\nsubspace structure into the new indexes with sparse constraint, thus it has\nlittle additional memory consumption in online query stage. Experimental\nevaluation on three benchmark datasets reveals that the proposed approach\nachieves the state-of-the-art performance, i.e., N-score 3.94 on UKBench, mAP\n94.1\\% on Holiday and 62.39\\% on Market-1501.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 01:59:17 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Zhang", "Zhizhong", ""], ["Xie", "Yuan", ""], ["Zhang", "Wensheng", ""], ["Tian", "Qi", ""]]}, {"id": "1709.09323", "submitter": "Nicholas Fang Yew Chen Mr.", "authors": "Nicholas F. Y. Chen", "title": "Pseudo-labels for Supervised Learning on Dynamic Vision Sensor Data,\n  Applied to Object Detection under Ego-motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, dynamic vision sensors (DVS), also known as event-based\ncameras or neuromorphic sensors, have seen increased use due to various\nadvantages over conventional frame-based cameras. Using principles inspired by\nthe retina, its high temporal resolution overcomes motion blurring, its high\ndynamic range overcomes extreme illumination conditions and its low power\nconsumption makes it ideal for embedded systems on platforms such as drones and\nself-driving cars. However, event-based data sets are scarce and labels are\neven rarer for tasks such as object detection. We transferred discriminative\nknowledge from a state-of-the-art frame-based convolutional neural network\n(CNN) to the event-based modality via intermediate pseudo-labels, which are\nused as targets for supervised learning. We show, for the first time,\nevent-based car detection under ego-motion in a real environment at 100 frames\nper second with a test average precision of 40.3% relative to our annotated\nground truth. The event-based car detector handles motion blur and poor\nillumination conditions despite not explicitly trained to do so, and even\ncomplements frame-based CNN detectors, suggesting that it has learnt\ngeneralized visual representations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 03:45:27 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 05:32:46 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 12:13:18 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Chen", "Nicholas F. Y.", ""]]}, {"id": "1709.09328", "submitter": "Chen Gao", "authors": "Chen Gao, Brian E. Moore, and Raj Rao Nadakuditi", "title": "Augmented Robust PCA For Foreground-Background Separation on Noisy,\n  Moving Camera Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel approach for robust PCA with total variation\nregularization for foreground-background separation and denoising on noisy,\nmoving camera video. Our proposed algorithm registers the raw (possibly\ncorrupted) frames of a video and then jointly processes the registered frames\nto produce a decomposition of the scene into a low-rank background component\nthat captures the static components of the scene, a smooth foreground component\nthat captures the dynamic components of the scene, and a sparse component that\ncan isolate corruptions and other non-idealities. Unlike existing methods, our\nproposed algorithm produces a panoramic low-rank component that spans the\nentire field of view, automatically stitching together corrupted data from\npartially overlapping scenes. The low-rank portion of our robust PCA model is\nbased on a recently discovered optimal low-rank matrix estimator (OptShrink)\nthat requires no parameter tuning. We demonstrate the performance of our\nalgorithm on both static and moving camera videos corrupted by noise and\noutliers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 04:17:43 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Gao", "Chen", ""], ["Moore", "Brian E.", ""], ["Nadakuditi", "Raj Rao", ""]]}, {"id": "1709.09345", "submitter": "Seil Na", "authors": "Seil Na, Sangho Lee, Jisung Kim, Gunhee Kim", "title": "A Read-Write Memory Network for Movie Story Understanding", "comments": "accepted paper at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel memory network model named Read-Write Memory Network\n(RWMN) to perform question and answering tasks for large-scale, multimodal\nmovie story understanding. The key focus of our RWMN model is to design the\nread network and the write network that consist of multiple convolutional\nlayers, which enable memory read and write operations to have high capacity and\nflexibility. While existing memory-augmented network models treat each memory\nslot as an independent block, our use of multi-layered CNNs allows the model to\nread and write sequential memory cells as chunks, which is more reasonable to\nrepresent a sequential story because adjacent memory blocks often have strong\ncorrelations. For evaluation, we apply our model to all the six tasks of the\nMovieQA benchmark, and achieve the best accuracies on several tasks, especially\non the visual QA task. Our model shows a potential to better understand not\nonly the content in the story, but also more abstract information, such as\nrelationships between characters and the reasons for their actions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 06:02:57 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 11:36:34 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 08:40:50 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 13:43:15 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Na", "Seil", ""], ["Lee", "Sangho", ""], ["Kim", "Jisung", ""], ["Kim", "Gunhee", ""]]}, {"id": "1709.09348", "submitter": "Ankan Kumar Bhunia", "authors": "Ankan Kumar Bhunia, Alireza Alaei, Partha Pratim Roy", "title": "Signature Verification Approach using Fusion of Hybrid Texture Features", "comments": "Neural Computing and Application", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a writer-dependent signature verification method is proposed.\nTwo different types of texture features, namely Wavelet and Local Quantized\nPatterns (LQP) features, are employed to extract two kinds of transform and\nstatistical based information from signature images. For each writer two\nseparate one-class support vector machines (SVMs) corresponding to each set of\nLQP and Wavelet features are trained to obtain two different authenticity\nscores for a given signature. Finally, a score level classifier fusion method\nis used to integrate the scores obtained from the two one-class SVMs to achieve\nthe verification score. In the proposed method only genuine signatures are used\nto train the one-class SVMs. The proposed signature verification method has\nbeen tested using four different publicly available datasets and the results\ndemonstrate the generality of the proposed method. The proposed system\noutperforms other existing systems in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 06:13:04 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 19:40:07 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Bhunia", "Ankan Kumar", ""], ["Alaei", "Alireza", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1709.09354", "submitter": "Zhifeng Kong", "authors": "Zhifeng Kong, Shuo Ding", "title": "Generative Adversarial Networks with Inverse Transformation Unit", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new structure to Generative Adversarial Networks\nby adding an inverse transformation unit behind the generator. We present two\ntheorems to claim the convergence of the model, and two conjectures to nonideal\nsituations when the transformation is not bijection. A general survey on models\nwith different transformations was done on the MNIST dataset and the\nFashion-MNIST dataset, which shows the transformation does not necessarily need\nto be bijection. Also, with certain transformations that blurs an image, our\nmodel successfully learned to sharpen the images and recover blurred images,\nwhich was additionally verified by our measurement of sharpness.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 06:38:30 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Kong", "Zhifeng", ""], ["Ding", "Shuo", ""]]}, {"id": "1709.09384", "submitter": "Dylan Campbell", "authors": "Dylan Campbell, Lars Petersson, Laurent Kneip and Hongdong Li", "title": "Globally-Optimal Inlier Set Maximisation for Simultaneous Camera Pose\n  and Feature Correspondence", "comments": "Manuscript in press 2017 IEEE International Conference on Computer\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 6-DoF pose of a camera from a single image relative to a\npre-computed 3D point-set is an important task for many computer vision\napplications. Perspective-n-Point (PnP) solvers are routinely used for camera\npose estimation, provided that a good quality set of 2D-3D feature\ncorrespondences are known beforehand. However, finding optimal correspondences\nbetween 2D key-points and a 3D point-set is non-trivial, especially when only\ngeometric (position) information is known. Existing approaches to the\nsimultaneous pose and correspondence problem use local optimisation, and are\ntherefore unlikely to find the optimal solution without a good pose\ninitialisation, or introduce restrictive assumptions. Since a large proportion\nof outliers are common for this problem, we instead propose a globally-optimal\ninlier set cardinality maximisation approach which jointly estimates optimal\ncamera pose and optimal correspondences. Our approach employs branch-and-bound\nto search the 6D space of camera poses, guaranteeing global optimality without\nrequiring a pose prior. The geometry of SE(3) is used to find novel upper and\nlower bounds for the number of inliers and local optimisation is integrated to\naccelerate convergence. The evaluation empirically supports the optimality\nproof and shows that the method performs much more robustly than existing\napproaches, including on a large-scale outdoor data-set.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 08:36:07 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Campbell", "Dylan", ""], ["Petersson", "Lars", ""], ["Kneip", "Laurent", ""], ["Li", "Hongdong", ""]]}, {"id": "1709.09389", "submitter": "Vandit Gajjar", "authors": "Yash Khandhediya, Karishma Sav, Vandit Gajjar", "title": "Human Detection for Night Surveillance using Adaptive Background\n  Subtracted Image", "comments": "5 Pages, 7 Figures, 1 Table, National Conference on Advances in\n  Computer Science Engineering & Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance based on Computer Vision has become a major necessity in current\nera. Most of the surveillance systems operate on visible light imaging, but\nperformance based on visible light imaging is limited due to some factors like\nvariation in light intensity during the daytime. The matter of concern lies in\nthe need for processing images in low light, such as in the need of nighttime\nsurveillance. In this paper, we have proposed a novel approach for human\ndetection using FLIR(Forward Looking Infrared) camera. As the principle\ninvolves sensing based on thermal radiation in the Near IR Region, it is\npossible to detect Humans from an image captured using a FLIR camera even in\nlow light. The proposed method for human detection involves processing of\nThermal images by using HOG (Histogram of Oriented Gradients) feature\nextraction technique along with some enhancements. The principle of the\nproposed technique lies in an adaptive background subtraction algorithm, which\nworks in association with the HOG technique. By means of this method, we are\nable to reduce execution time, precision and some other parameters, which\nresult in improvement of overall accuracy of the human detection system.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 08:46:01 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Khandhediya", "Yash", ""], ["Sav", "Karishma", ""], ["Gajjar", "Vandit", ""]]}, {"id": "1709.09422", "submitter": "Bahadir Gunturk", "authors": "M. Umair Mukati and Bahadir K. Gunturk", "title": "Light field super resolution through controlled micro-shifts of light\n  field sensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras enable new capabilities, such as post-capture refocusing\nand aperture control, through capturing directional and spatial distribution of\nlight rays in space. Micro-lens array based light field camera design is often\npreferred due to its light transmission efficiency, cost-effectiveness and\ncompactness. One drawback of the micro-lens array based light field cameras is\nlow spatial resolution due to the fact that a single sensor is shared to\ncapture both spatial and angular information. To address the low spatial\nresolution issue, we present a light field imaging approach, where multiple\nlight fields are captured and fused to improve the spatial resolution. For each\ncapture, the light field sensor is shifted by a pre-determined fraction of a\nmicro-lens size using an XY translation stage for optimal performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 09:54:08 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 19:40:10 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Mukati", "M. Umair", ""], ["Gunturk", "Bahadir K.", ""]]}, {"id": "1709.09426", "submitter": "Hedi Ben-Younes", "authors": "Charles Corbi\\`ere, Hedi Ben-Younes, Alexandre Ram\\'e and Charles\n  Ollion", "title": "Leveraging Weakly Annotated Data for Fashion Image Retrieval and Label\n  Prediction", "comments": null, "journal-ref": "2017 IEEE International Conference on Computer Vision Workshop\n  (ICCVW)", "doi": "10.1109/ICCVW.2017.266", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method to learn a visual representation adapted\nfor e-commerce products. Based on weakly supervised learning, our model learns\nfrom noisy datasets crawled on e-commerce website catalogs and does not require\nany manual labeling. We show that our representation can be used for downward\nclassification tasks over clothing categories with different levels of\ngranularity. We also demonstrate that the learnt representation is suitable for\nimage retrieval. We achieve nearly state-of-art results on the DeepFashion\nIn-Shop Clothes Retrieval and Categories Attributes Prediction tasks, without\nusing the provided training set.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 10:02:10 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Corbi\u00e8re", "Charles", ""], ["Ben-Younes", "Hedi", ""], ["Ram\u00e9", "Alexandre", ""], ["Ollion", "Charles", ""]]}, {"id": "1709.09429", "submitter": "Bappaditya Mandal", "authors": "Paritosh Pandey, Akella Deepthi, Bappaditya Mandal and N. B. Puhan", "title": "FoodNet: Recognizing Foods Using Ensemble of Deep Networks", "comments": "5 pages, 3 figures, 3 tables, IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2017.2758862", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a methodology for an automatic food classification\nsystem which recognizes the contents of the meal from the images of the food.\nWe developed a multi-layered deep convolutional neural network (CNN)\narchitecture that takes advantages of the features from other deep networks and\nimproves the efficiency. Numerous classical handcrafted features and approaches\nare explored, among which CNNs are chosen as the best performing features.\nNetworks are trained and fine-tuned using preprocessed images and the filter\noutputs are fused to achieve higher accuracy. Experimental results on the\nlargest real-world food recognition database ETH Food-101 and newly contributed\nIndian food image database demonstrate the effectiveness of the proposed\nmethodology as compared to many other benchmark deep learned CNN frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 10:13:31 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Pandey", "Paritosh", ""], ["Deepthi", "Akella", ""], ["Mandal", "Bappaditya", ""], ["Puhan", "N. B.", ""]]}, {"id": "1709.09479", "submitter": "Lama Affara", "authors": "Lama Affara, Bernard Ghanem, Peter Wonka", "title": "Fast Convolutional Sparse Coding in the Dual Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional sparse coding (CSC) is an important building block of many\ncomputer vision applications ranging from image and video compression to deep\nlearning. We present two contributions to the state of the art in CSC. First,\nwe significantly speed up the computation by proposing a new optimization\nframework that tackles the problem in the dual domain. Second, we extend the\noriginal formulation to higher dimensions in order to process a wider range of\ninputs, such as RGB images and videos. Our results show up to 20 times speedup\ncompared to current state-of-the-art CSC solvers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 12:55:00 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 12:03:33 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Affara", "Lama", ""], ["Ghanem", "Bernard", ""], ["Wonka", "Peter", ""]]}, {"id": "1709.09490", "submitter": "Liang Lin", "authors": "Ruimao Zhang, Liang Lin, Guangrun Wang, Meng Wang and Wangmeng Zuo", "title": "Hierarchical Scene Parsing by Weakly Supervised Learning with Image\n  Descriptions", "comments": "Accepted by Transactions on Pattern Analysis and Machine Intelligence\n  (T-PAMI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a fundamental problem of scene understanding: how to\nparse a scene image into a structured configuration (i.e., a semantic object\nhierarchy with object interaction relations). We propose a deep architecture\nconsisting of two networks: i) a convolutional neural network (CNN) extracting\nthe image representation for pixel-wise object labeling and ii) a recursive\nneural network (RsNN) discovering the hierarchical object structure and the\ninter-object relations. Rather than relying on elaborative annotations (e.g.,\nmanually labeled semantic maps and relations), we train our deep model in a\nweakly-supervised learning manner by leveraging the descriptive sentences of\nthe training images. Specifically, we decompose each sentence into a semantic\ntree consisting of nouns and verb phrases, and apply these tree structures to\ndiscover the configurations of the training images. Once these scene\nconfigurations are determined, then the parameters of both the CNN and RsNN are\nupdated accordingly by back propagation. The entire model training is\naccomplished through an Expectation-Maximization method. Extensive experiments\nshow that our model is capable of producing meaningful scene configurations and\nachieving more favorable scene labeling results on two benchmarks (i.e., PASCAL\nVOC 2012 and SYSU-Scenes) compared with other state-of-the-art\nweakly-supervised deep learning methods. In particular, SYSU-Scenes contains\nmore than 5000 scene images with their semantic sentence descriptions, which is\ncreated by us for advancing research on scene parsing.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 13:17:41 GMT"}, {"version": "v2", "created": "Sat, 27 Jan 2018 03:25:40 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Zhang", "Ruimao", ""], ["Lin", "Liang", ""], ["Wang", "Guangrun", ""], ["Wang", "Meng", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1709.09496", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava, Swati Bhugra, Brejesh Lall, Santanu Chaudhury", "title": "Drought Stress Classification using 3D Plant Models", "comments": "Appears in Workshop on Computer Vision Problems in Plant Phenotyping\n  (CVPPP), International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of physiological changes in plants can capture different\ndrought mechanisms and assist in selection of tolerant varieties in a high\nthroughput manner. In this context, an accurate 3D model of plant canopy\nprovides a reliable representation for drought stress characterization in\ncontrast to using 2D images. In this paper, we propose a novel end-to-end\npipeline including 3D reconstruction, segmentation and feature extraction,\nleveraging deep neural networks at various stages, for drought stress study. To\novercome the high degree of self-similarities and self-occlusions in plant\ncanopy, prior knowledge of leaf shape based on features from deep siamese\nnetwork are used to construct an accurate 3D model using structure from motion\non wheat plants. The drought stress is characterized with a deep network based\nfeature aggregation. We compare the proposed methodology on several\ndescriptors, and show that the network outperforms conventional methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 05:20:13 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 06:09:47 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Srivastava", "Siddharth", ""], ["Bhugra", "Swati", ""], ["Lall", "Brejesh", ""], ["Chaudhury", "Santanu", ""]]}, {"id": "1709.09503", "submitter": "Zongqing Lu", "authors": "Zongqing Lu, Swati Rallapalli, Kevin Chan, Thomas La Porta", "title": "Modeling the Resource Requirements of Convolutional Neural Networks on\n  Mobile Devices", "comments": null, "journal-ref": "ACM Multimedia 2017", "doi": "10.1145/3123266.3123389", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have revolutionized the research in\ncomputer vision, due to their ability to capture complex patterns, resulting in\nhigh inference accuracies. However, the increasingly complex nature of these\nneural networks means that they are particularly suited for server computers\nwith powerful GPUs. We envision that deep learning applications will be\neventually and widely deployed on mobile devices, e.g., smartphones,\nself-driving cars, and drones. Therefore, in this paper, we aim to understand\nthe resource requirements (time, memory) of CNNs on mobile devices. First, by\ndeploying several popular CNNs on mobile CPUs and GPUs, we measure and analyze\nthe performance and resource usage for every layer of the CNNs. Our findings\npoint out the potential ways of optimizing the performance on mobile devices.\nSecond, we model the resource requirements of the different CNN computations.\nFinally, based on the measurement, pro ling, and modeling, we build and\nevaluate our modeling tool, Augur, which takes a CNN configuration (descriptor)\nas the input and estimates the compute time and resource usage of the CNN, to\ngive insights about whether and how e ciently a CNN can be run on a given\nmobile platform. In doing so Augur tackles several challenges: (i) how to\novercome pro ling and measurement overhead; (ii) how to capture the variance in\ndifferent mobile platforms with different processors, memory, and cache sizes;\nand (iii) how to account for the variance in the number, type and size of\nlayers of the different CNN configurations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 13:36:12 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Lu", "Zongqing", ""], ["Rallapalli", "Swati", ""], ["Chan", "Kevin", ""], ["La Porta", "Thomas", ""]]}, {"id": "1709.09518", "submitter": "Shiv Ram Dubey", "authors": "Shiv Ram Dubey", "title": "Local Directional Relation Pattern for Unconstrained and Robust Face\n  Retrieval", "comments": "Multimedia Tools and Applications", "journal-ref": null, "doi": "10.1007/s11042-019-07908-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is still a very demanding area of research. This problem\nbecomes more challenging in unconstrained environment and in the presence of\nseveral variations like pose, illumination, expression, etc. Local descriptors\nare widely used for this task. The most of the existing local descriptors\nconsider only few immediate local neighbors and not able to utilize the wider\nlocal information to make the descriptor more discriminative. The wider local\ninformation based descriptors mainly suffer due to the increased\ndimensionality. In this paper, this problem is solved by encoding the\nrelationship among directional neighbors in an efficient manner. The\nrelationship between the center pixel and the encoded directional neighbors is\nutilized further to form the proposed local directional relation pattern\n(LDRP). The descriptor is inherently uniform illumination invariant. The\nmulti-scale mechanism is also adapted to further boost the discriminative\nability of the descriptor. The proposed descriptor is evaluated under the image\nretrieval framework over face databases. Very challenging databases like PaSC,\nLFW, PubFig, ESSEX, FERET, AT&T, and FaceScrub are used to test the\ndiscriminative ability and robustness of LDRP descriptor. Results are also\ncompared with the recent state-of-the-art face descriptors such as LBP, LTP,\nLDP, LDN, LVP, DCP, LDGP and LGHP. Very promising performance is observed using\nthe proposed descriptor over very appealing face databases as compared to the\nexisting face descriptors. The proposed LDRP descriptor also outperforms the\npre-trained ImageNet CNN models over large-scale FaceScrub face dataset.\nMoreover, it also outperforms the deep learning based DLib face descriptor in\nmany scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 04:36:56 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 09:01:27 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Dubey", "Shiv Ram", ""]]}, {"id": "1709.09528", "submitter": "Hamid Shahdoosti", "authors": "Fatemeh Vakili Moghadam, Hamid Reza Shahdoosti", "title": "A New Multifocus Image Fusion Method Using Contourlet Transform", "comments": "8 pages, 7 figures, conference. in Persian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new multifocus image fusion approach is presented in this paper. First the\ncontourlet transform is used to decompose the source images into different\ncomponents. Then, some salient features are extracted from components. In order\nto extract salient features, spatial frequency is used. Subsequently, the best\ncoefficients from the components are selected by the maximum selection rule.\nFinally, the inverse contourlet transform is applied to the selected\ncoefficients. Experiments show the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 11:51:41 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Moghadam", "Fatemeh Vakili", ""], ["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1709.09550", "submitter": "Peter Meer", "authors": "Xiang Yang and Peter Meer", "title": "Scale Adaptive Clustering of Multiple Structures", "comments": "14 pages, 17 figures. arXiv admin note: substantial text overlap with\n  arXiv:1609.06371", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the segmentation of noisy datasets into Multiple Inlier Structures\nwith a new Robust Estimator (MISRE). The scale of each individual structure is\nestimated adaptively from the input data and refined by mean shift, without\ntuning any parameter in the process, or manually specifying thresholds for\ndifferent estimation problems. Once all the data points were classified into\nseparate structures, these structures are sorted by their densities with the\nstrongest inlier structures coming out first. Several 2D and 3D synthetic and\nreal examples are presented to illustrate the efficiency, robustness and the\nlimitations of the MISRE algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 15:15:52 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Yang", "Xiang", ""], ["Meer", "Peter", ""]]}, {"id": "1709.09559", "submitter": "Victor Fragoso", "authors": "Victor Fragoso, Chris Sweeney, Pradeep Sen and Matthew Turk", "title": "ANSAC: Adaptive Non-minimal Sample and Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While RANSAC-based methods are robust to incorrect image correspondences\n(outliers), their hypothesis generators are not robust to correct image\ncorrespondences (inliers) with positional error (noise). This slows down their\nconvergence because hypotheses drawn from a minimal set of noisy inliers can\ndeviate significantly from the optimal model. This work addresses this problem\nby introducing ANSAC, a RANSAC-based estimator that accounts for noise by\nadaptively using more than the minimal number of correspondences required to\ngenerate a hypothesis. ANSAC estimates the inlier ratio (the fraction of\ncorrect correspondences) of several ranked subsets of candidate correspondences\nand generates hypotheses from them. Its hypothesis-generation mechanism\nprioritizes the use of subsets with high inlier ratio to generate high-quality\nhypotheses. ANSAC uses an early termination criterion that keeps track of the\ninlier ratio history and terminates when it has not changed significantly for a\nperiod of time. The experiments show that ANSAC finds good homography and\nfundamental matrix estimates in a few iterations, consistently outperforming\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 14:47:03 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Fragoso", "Victor", ""], ["Sweeney", "Chris", ""], ["Sen", "Pradeep", ""], ["Turk", "Matthew", ""]]}, {"id": "1709.09582", "submitter": "Karim Ahmed", "authors": "Karim Ahmed and Lorenzo Torresani", "title": "Connectivity Learning in Multi-Branch Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While much of the work in the design of convolutional networks over the last\nfive years has revolved around the empirical investigation of the importance of\ndepth, filter sizes, and number of feature channels, recent studies have shown\nthat branching, i.e., splitting the computation along parallel but distinct\nthreads and then aggregating their outputs, represents a new promising\ndimension for significant improvements in performance. To combat the complexity\nof design choices in multi-branch architectures, prior work has adopted simple\nstrategies, such as a fixed branching factor, the same input being fed to all\nparallel branches, and an additive combination of the outputs produced by all\nbranches at aggregation points.\n  In this work we remove these predefined choices and propose an algorithm to\nlearn the connections between branches in the network. Instead of being chosen\na priori by the human designer, the multi-branch connectivity is learned\nsimultaneously with the weights of the network by optimizing a single loss\nfunction defined with respect to the end task. We demonstrate our approach on\nthe problem of multi-class image classification using three different datasets\nwhere it yields consistently higher accuracy compared to the state-of-the-art\n\"ResNeXt\" multi-branch network given the same learning capacity.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 15:34:21 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 16:57:45 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Ahmed", "Karim", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1709.09602", "submitter": "Yuanming Hu", "authors": "Yuanming Hu, Hao He, Chenxi Xu, Baoyuan Wang, Stephen Lin", "title": "Exposure: A White-Box Photo Post-Processing Framework", "comments": "ACM Transaction on Graphics (Accepted with minor revisions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retouching can significantly elevate the visual appeal of photos, but many\ncasual photographers lack the expertise to do this well. To address this\nproblem, previous works have proposed automatic retouching systems based on\nsupervised learning from paired training images acquired before and after\nmanual editing. As it is difficult for users to acquire paired images that\nreflect their retouching preferences, we present in this paper a deep learning\napproach that is instead trained on unpaired data, namely a set of photographs\nthat exhibits a retouching style the user likes, which is much easier to\ncollect. Our system is formulated using deep convolutional neural networks that\nlearn to apply different retouching operations on an input image. Network\ntraining with respect to various types of edits is enabled by modeling these\nretouching operations in a unified manner as resolution-independent\ndifferentiable filters. To apply the filters in a proper sequence and with\nsuitable parameters, we employ a deep reinforcement learning approach that\nlearns to make decisions on what action to take next, given the current state\nof the image. In contrast to many deep learning systems, ours provides users\nwith an understandable solution in the form of conventional retouching edits,\nrather than just a \"black-box\" result. Through quantitative comparisons and\nuser studies, we show that this technique generates retouching results\nconsistent with the provided photo set.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 16:15:58 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 16:47:58 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Hu", "Yuanming", ""], ["He", "Hao", ""], ["Xu", "Chenxi", ""], ["Wang", "Baoyuan", ""], ["Lin", "Stephen", ""]]}, {"id": "1709.09641", "submitter": "Heran Yang", "authors": "Heran Yang, Jian Sun, Huibin Li, Lisheng Wang, Zongben Xu", "title": "Neural Multi-Atlas Label Fusion: Application to Cardiac MR Images", "comments": "Medical Image Analysis", "journal-ref": "Heran Yang, et al. Neural Multi-Atlas Label Fusion: Application to\n  Cardiac MR Images, Medical Image Analysis, 2018, ISSN 1361-8415", "doi": "10.1016/j.media.2018.07.009", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-atlas segmentation approach is one of the most widely-used image\nsegmentation techniques in biomedical applications. There are two major\nchallenges in this category of methods, i.e., atlas selection and label fusion.\nIn this paper, we propose a novel multi-atlas segmentation method that\nformulates multi-atlas segmentation in a deep learning framework for better\nsolving these challenges. The proposed method, dubbed deep fusion net (DFN), is\na deep architecture that integrates a feature extraction subnet and a non-local\npatch-based label fusion (NL-PLF) subnet in a single network. The network\nparameters are learned by end-to-end training for automatically learning deep\nfeatures that enable optimal performance in a NL-PLF framework. The learned\ndeep features are further utilized in defining a similarity measure for atlas\nselection. By evaluating on two public cardiac MR datasets of SATA-13 and LV-09\nfor left ventricle segmentation, our approach achieved 0.833 in averaged Dice\nmetric (ADM) on SATA-13 dataset and 0.95 in ADM for epicardium segmentation on\nLV-09 dataset, comparing favorably with the other automatic left ventricle\nsegmentation methods. We also tested our approach on Cardiac Atlas Project\n(CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, and our method\nachieved 0.815 in ADM, ranking highest at the time of writing.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 17:25:17 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 18:34:11 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Yang", "Heran", ""], ["Sun", "Jian", ""], ["Li", "Huibin", ""], ["Wang", "Lisheng", ""], ["Xu", "Zongben", ""]]}, {"id": "1709.09683", "submitter": "Yunpeng Shi", "authors": "Gilad Lerman, Yunpeng Shi, Teng Zhang", "title": "Exact Camera Location Recovery by Least Unsquared Deviations", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences, 11 (2018), no. 4, 2692-2721", "doi": "10.1137/17M115061X", "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish exact recovery for the Least Unsquared Deviations (LUD)\nalgorithm of Ozyesil and Singer. More precisely, we show that for sufficiently\nmany cameras with given corrupted pairwise directions, where both camera\nlocations and pairwise directions are generated by a special probabilistic\nmodel, the LUD algorithm exactly recovers the camera locations with high\nprobability. A similar exact recovery guarantee was established for the\nShapeFit algorithm by Hand, Lee and Voroninski, but with typically less\ncorruption.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 18:11:40 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 16:34:03 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 16:21:51 GMT"}, {"version": "v4", "created": "Sun, 9 Sep 2018 17:38:04 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Lerman", "Gilad", ""], ["Shi", "Yunpeng", ""], ["Zhang", "Teng", ""]]}, {"id": "1709.09754", "submitter": "Hamid Tizhoosh", "authors": "Hamed Erfankhah, Mehran Yazdi, H.R.Tizhoosh", "title": "Combining Real-Valued and Binary Gabor-Radon Features for Classification\n  and Search in Medical Imaging Archives", "comments": "To appear in proceedings of The IEEE Symposium Series on\n  Computational Intelligence (IEEE SSCI 2017), Honolulu, Hawaii, USA, Nov. 27\n  -- Dec 1, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval (CBIR) of medical images in large datasets to\nidentify similar images when a query image is given can be very useful in\nimproving the diagnostic decision of the clinical experts and as well in\neducational scenarios. In this paper, we used two stage classification and\nretrieval approach to retrieve similar images. First, the Gabor filters are\napplied to Radon-transformed images to extract features and to train a\nmulti-class SVM. Then based on the classification results and using an\nextracted Gabor barcode, similar images are retrieved. The proposed method was\ntested on IRMA dataset which contains more than 14,000 images. Experimental\nresults show the efficiency of our approach in retrieving similar images\ncompared to other Gabor-Radon-oriented methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 22:53:13 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Erfankhah", "Hamed", ""], ["Yazdi", "Mehran", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1709.09780", "submitter": "Yading Yuan", "authors": "Yading Yuan and Yeh-Chi Lo", "title": "Improving Dermoscopic Image Segmentation with Enhanced\n  Convolutional-Deconvolutional Networks", "comments": "submitted to IEEE Journal of Biomedical and Health Informatics\n  special issue on \"Skin Lesion Analysis for Melanoma Detection\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic skin lesion segmentation on dermoscopic images is an essential step\nin computer-aided diagnosis of melanoma. However, this task is challenging due\nto significant variations of lesion appearances across different patients. This\nchallenge is further exacerbated when dealing with a large amount of image\ndata. In this paper, we extended our previous work by developing a deeper\nnetwork architecture with smaller kernels to enhance its discriminant capacity.\nIn addition, we explicitly included color information from multiple color\nspaces to facilitate network training and thus to further improve the\nsegmentation performance. We extensively evaluated our method on the ISBI 2017\nskin lesion segmentation challenge. By training with the 2000 challenge\ntraining images, our method achieved an average Jaccard Index (JA) of 0.765 on\nthe 600 challenge testing images, which ranked itself in the first place in the\nchallenge\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 02:01:11 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Yuan", "Yading", ""], ["Lo", "Yeh-Chi", ""]]}, {"id": "1709.09828", "submitter": "Roey Mechrez", "authors": "Roey Mechrez, Eli Shechtman and Lihi Zelnik-Manor", "title": "Photorealistic Style Transfer with Screened Poisson Equation", "comments": "presented in BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown impressive success in transferring painterly style to\nimages. These approaches, however, fall short of photorealistic style transfer.\nEven when both the input and reference images are photographs, the output still\nexhibits distortions reminiscent of a painting. In this paper we propose an\napproach that takes as input a stylized image and makes it more photorealistic.\nIt relies on the Screened Poisson Equation, maintaining the fidelity of the\nstylized image while constraining the gradients to those of the original input\nimage. Our method is fast, simple, fully automatic and shows positive progress\nin making a stylized image photorealistic. Our results exhibit finer details\nand are less prone to artifacts than the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 07:28:35 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Mechrez", "Roey", ""], ["Shechtman", "Eli", ""], ["Zelnik-Manor", "Lihi", ""]]}, {"id": "1709.09843", "submitter": "Sarah Taghavi Namin", "authors": "Sarah Taghavi Namin, Mohammad Najafi, Mathieu Salzmann, and Lars\n  Petersson", "title": "Soft Correspondences in Multimodal Scene Parsing", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting multiple modalities for semantic scene parsing has been shown to\nimprove accuracy over the singlemodality scenario. However multimodal datasets\noften suffer from problems such as data misalignment and label inconsistencies,\nwhere the existing methods assume that corresponding regions in two modalities\nmust have identical labels. We propose to address this issue, by formulating\nmultimodal semantic labeling as inference in a CRF and introducing latent nodes\nto explicitly model inconsistencies between two modalities. These latent nodes\nallow us not only to leverage information from both domains to improve their\nlabeling, but also to cut the edges between inconsistent regions. We propose to\nlearn intradomain and inter-domain potential functions from training data to\navoid hand-tuning of the model parameters. We evaluate our approach on two\npublicly available datasets containing 2D and 3D data. Thanks to our latent\nnodes and our learning strategy, our method outperforms the state-of-the-art in\nboth cases. Moreover, in order to highlight the benefits of the geometric\ninformation and the potential of our method in simultaneous 2D/3D semantic and\ngeometric inference, we performed simultaneous inference of semantic and\ngeometric classes both in 2D and 3D that led to satisfactory improvements of\nthe labeling results in both datasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 08:08:11 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Namin", "Sarah Taghavi", ""], ["Najafi", "Mohammad", ""], ["Salzmann", "Mathieu", ""], ["Petersson", "Lars", ""]]}, {"id": "1709.09844", "submitter": "Amit Mandelbaum", "authors": "Amit Mandelbaum and Daphna Weinshall", "title": "Distance-based Confidence Score for Neural Network Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliable measurement of confidence in classifiers' predictions is very\nimportant for many applications and is, therefore, an important part of\nclassifier design. Yet, although deep learning has received tremendous\nattention in recent years, not much progress has been made in quantifying the\nprediction confidence of neural network classifiers. Bayesian models offer a\nmathematically grounded framework to reason about model uncertainty, but\nusually come with prohibitive computational costs. In this paper we propose a\nsimple, scalable method to achieve a reliable confidence score, based on the\ndata embedding derived from the penultimate layer of the network. We\ninvestigate two ways to achieve desirable embeddings, by using either a\ndistance-based loss or Adversarial Training. We then test the benefits of our\nmethod when used for classification error prediction, weighting an ensemble of\nclassifiers, and novelty detection. In all tasks we show significant\nimprovement over traditional, commonly used confidence scores.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 08:09:47 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Mandelbaum", "Amit", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1709.09875", "submitter": "Jomy John", "authors": "Jomy John", "title": "Recognition of Documents in Braille", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually impaired people are integral part of the society and it has been a\nmust to provide them with means and system through which they may communicate\nwith the world. In this work, I would like to address how computers can be made\nuseful to read the scripts in Braille. The importance of this work is to reduce\ncommunication gap between visually impaired people and the society. Braille\nremains the most popular tactile reading code even in this century. There are\nnumerous amount of literature locked up in Braille. Braille recognition not\nonly reduces time in reading or extracting information from Braille document\nbut also helps people engaged in special education for correcting papers and\nother school related works. The availability of such a system will enhance\ncommunication and collaboration possibilities with visually impaired people.\nExisting works supports only documents in white either bright or dull in\ncolour. Hardly any work could be traced on hand printed ordinary documents in\nBraille.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 09:51:33 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["John", "Jomy", ""]]}, {"id": "1709.09882", "submitter": "Giulia Pasquale", "authors": "Giulia Pasquale, Carlo Ciliberto, Francesca Odone, Lorenzo Rosasco and\n  Lorenzo Natale", "title": "Are we done with object recognition? The iCub robot's perspective", "comments": "21 pages + supplementary material", "journal-ref": "Robotics and Autonomous Systems, Volume 112, February 2019, Pages\n  260-281", "doi": "10.1016/j.robot.2018.11.001", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on an extensive study of the benefits and limitations of current\ndeep learning approaches to object recognition in robot vision scenarios,\nintroducing a novel dataset used for our investigation. To avoid the biases in\ncurrently available datasets, we consider a natural human-robot interaction\nsetting to design a data-acquisition protocol for visual object recognition on\nthe iCub humanoid robot. Analyzing the performance of off-the-shelf models\ntrained off-line on large-scale image retrieval datasets, we show the necessity\nfor knowledge transfer. We evaluate different ways in which this last step can\nbe done, and identify the major bottlenecks affecting robotic scenarios. By\nstudying both object categorization and identification problems, we highlight\nkey differences between object recognition in robotics applications and in\nimage retrieval tasks, for which the considered deep learning approaches have\nbeen originally designed. In a nutshell, our results confirm the remarkable\nimprovements yield by deep learning in this setting, while pointing to specific\nopen challenges that need be addressed for seamless deployment in robotics.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 10:16:52 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 14:16:09 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Pasquale", "Giulia", ""], ["Ciliberto", "Carlo", ""], ["Odone", "Francesca", ""], ["Rosasco", "Lorenzo", ""], ["Natale", "Lorenzo", ""]]}, {"id": "1709.09888", "submitter": "Matthias Meyer", "authors": "Matthias Meyer, Lukas Cavigelli, Lothar Thiele", "title": "Efficient Convolutional Neural Network For Audio Event Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless distributed systems as used in sensor networks, Internet-of-Things\nand cyber-physical systems, impose high requirements on resource efficiency.\nAdvanced preprocessing and classification of data at the network edge can help\nto decrease the communication demand and to reduce the amount of data to be\nprocessed centrally. In the area of distributed acoustic sensing, the\ncombination of algorithms with a high classification rate and\nresource-constraint embedded systems is essential. Unfortunately, algorithms\nfor acoustic event detection have a high memory and computational demand and\nare not suited for execution at the network edge. This paper addresses these\naspects by applying structural optimizations to a convolutional neural network\nfor audio event detection to reduce the memory requirement by a factor of more\nthan 500 and the computational effort by a factor of 2.1 while performing 9.2%\nbetter.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 10:54:01 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Meyer", "Matthias", ""], ["Cavigelli", "Lukas", ""], ["Thiele", "Lothar", ""]]}, {"id": "1709.09890", "submitter": "Xinqi Zhu", "authors": "Xinqi Zhu, Michael Bain", "title": "B-CNN: Branch Convolutional Neural Network for Hierarchical\n  Classification", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) image classifiers are traditionally\ndesigned to have sequential convolutional layers with a single output layer.\nThis is based on the assumption that all target classes should be treated\nequally and exclusively. However, some classes can be more difficult to\ndistinguish than others, and classes may be organized in a hierarchy of\ncategories. At the same time, a CNN is designed to learn internal\nrepresentations that abstract from the input data based on its hierarchical\nlayered structure. So it is natural to ask if an inverse of this idea can be\napplied to learn a model that can predict over a classification hierarchy using\nmultiple output layers in decreasing order of class abstraction. In this paper,\nwe introduce a variant of the traditional CNN model named the Branch\nConvolutional Neural Network (B-CNN). A B-CNN model outputs multiple\npredictions ordered from coarse to fine along the concatenated convolutional\nlayers corresponding to the hierarchical structure of the target classes, which\ncan be regarded as a form of prior knowledge on the output. To learn with\nB-CNNs a novel training strategy, named the Branch Training strategy\n(BT-strategy), is introduced which balances the strictness of the prior with\nthe freedom to adjust parameters on the output layers to minimize the loss. In\nthis way we show that CNN based models can be forced to learn successively\ncoarse to fine concepts in the internal layers at the output stage, and that\nhierarchical prior knowledge can be adopted to boost CNN models' classification\nperformance. Our models are evaluated to show that the B-CNN extensions improve\nover the corresponding baseline CNN on the benchmark datasets MNIST, CIFAR-10\nand CIFAR-100.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 11:02:43 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 08:14:57 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Zhu", "Xinqi", ""], ["Bain", "Michael", ""]]}, {"id": "1709.09902", "submitter": "Dat Thanh Tran", "authors": "Dat Thanh Tran, Alexandros Iosifidis, Moncef Gabbouj", "title": "Improving Efficiency in Convolutional Neural Network with Multilinear\n  Filters", "comments": "10 pages, 3 figures", "journal-ref": "Neural Networks vol. 105, pp. 328-339, 2018", "doi": "10.1016/j.neunet.2018.05.017", "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The excellent performance of deep neural networks has enabled us to solve\nseveral automatization problems, opening an era of autonomous devices. However,\ncurrent deep net architectures are heavy with millions of parameters and\nrequire billions of floating point operations. Several works have been\ndeveloped to compress a pre-trained deep network to reduce memory footprint\nand, possibly, computation. Instead of compressing a pre-trained network, in\nthis work, we propose a generic neural network layer structure employing\nmultilinear projection as the primary feature extractor. The proposed\narchitecture requires several times less memory as compared to the traditional\nConvolutional Neural Networks (CNN), while inherits the similar design\nprinciples of a CNN. In addition, the proposed architecture is equipped with\ntwo computation schemes that enable computation reduction or scalability.\nExperimental results show the effectiveness of our compact projection that\noutperforms traditional CNN, while requiring far fewer parameters.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 11:55:13 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 12:45:30 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 15:42:11 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Tran", "Dat Thanh", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1709.09905", "submitter": "Abel Gawel", "authors": "Abel Gawel, Carlo Del Don, Roland Siegwart, Juan Nieto, Cesar Cadena", "title": "X-View: Graph-Based Semantic Multi-View Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global registration of multi-view robot data is a challenging task.\nAppearance-based global localization approaches often fail under drastic\nview-point changes, as representations have limited view-point invariance. This\nwork is based on the idea that human-made environments contain rich semantics\nwhich can be used to disambiguate global localization. Here, we present X-View,\na Multi-View Semantic Global Localization system. X-View leverages semantic\ngraph descriptor matching for global localization, enabling localization under\ndrastically different view-points. While the approach is general in terms of\nthe semantic input data, we present and evaluate an implementation on visual\ndata. We demonstrate the system in experiments on the publicly available\nSYNTHIA dataset, on a realistic urban dataset recorded with a simulator, and on\nreal-world StreetView data. Our findings show that X-View is able to globally\nlocalize aerial-to-ground, and ground-to-ground robot data of drastically\ndifferent view-points. Our approach achieves an accuracy of up to 85 % on\nglobal localizations in the multi-view case, while the benchmarked baseline\nappearance-based methods reach up to 75 %.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 11:58:58 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 09:19:19 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 11:26:34 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Gawel", "Abel", ""], ["Del Don", "Carlo", ""], ["Siegwart", "Roland", ""], ["Nieto", "Juan", ""], ["Cadena", "Cesar", ""]]}, {"id": "1709.09930", "submitter": "Xihui Liu", "authors": "Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng, Jing Shao, Shuai Yi,\n  Junjie Yan, Xiaogang Wang", "title": "HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian analysis plays a vital role in intelligent video surveillance and\nis a key component for security-centric computer vision systems. Despite that\nthe convolutional neural networks are remarkable in learning discriminative\nfeatures from images, the learning of comprehensive features of pedestrians for\nfine-grained tasks remains an open problem. In this study, we propose a new\nattention-based deep neural network, named as HydraPlus-Net (HP-net), that\nmulti-directionally feeds the multi-level attention maps to different feature\nlayers. The attentive deep features learned from the proposed HP-net bring\nunique advantages: (1) the model is capable of capturing multiple attentions\nfrom low-level to semantic-level, and (2) it explores the multi-scale\nselectiveness of attentive features to enrich the final feature representations\nfor a pedestrian image. We demonstrate the effectiveness and generality of the\nproposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute\nrecognition and person re-identification. Intensive experimental results have\nbeen provided to prove that the HP-net outperforms the state-of-the-art methods\non various datasets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 13:06:55 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Liu", "Xihui", ""], ["Zhao", "Haiyu", ""], ["Tian", "Maoqing", ""], ["Sheng", "Lu", ""], ["Shao", "Jing", ""], ["Yi", "Shuai", ""], ["Yan", "Junjie", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1709.10177", "submitter": "Maria-Laura Torrente", "authors": "Maria-Laura Torrente, Silvia Biasotti, Bianca Falcidieno", "title": "Recognition of feature curves on 3D shapes using an algebraic approach\n  to Hough transforms", "comments": null, "journal-ref": "Pattern Recognition, Volume 73, Pages 1-288 (January 2018)", "doi": "10.1016/j.patcog.2017.08.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature curves are largely adopted to highlight shape features, such as sharp\nlines, or to divide surfaces into meaningful segments, like convex or concave\nregions. Extracting these curves is not sufficient to convey prominent and\nmeaningful information about a shape. We have first to separate the curves\nbelonging to features from those caused by noise and then to select the lines,\nwhich describe non-trivial portions of a surface. The automatic detection of\nsuch features is crucial for the identification and/or annotation of relevant\nparts of a given shape. To do this, the Hough transform (HT) is a feature\nextraction technique widely used in image analysis, computer vision and digital\nimage processing, while, for 3D shapes, the extraction of salient feature\ncurves is still an open problem.\n  Thanks to algebraic geometry concepts, the HT technique has been recently\nextended to include a vast class of algebraic curves, thus proving to be a\ncompetitive tool for yielding an explicit representation of the diverse feature\nlines equations. In the paper, for the first time we apply this novel extension\nof the HT technique to the realm of 3D shapes in order to identify and localize\nsemantic features like patterns, decorations or anatomical details on 3D\nobjects (both complete and fragments), even in the case of features partially\ndamaged or incomplete. The method recognizes various features, possibly\ncompound, and it selects the most suitable feature profiles among families of\nalgebraic curves.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 21:36:05 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Torrente", "Maria-Laura", ""], ["Biasotti", "Silvia", ""], ["Falcidieno", "Bianca", ""]]}, {"id": "1709.10180", "submitter": "Daniel Suen", "authors": "Alina Zare, Nicholas Young, Daniel Suen, Thomas Nabelek, Aquila\n  Galusha, James Keller", "title": "Possibilistic Fuzzy Local Information C-Means for Sonar Image\n  Segmentation", "comments": "8 pages, 11 figures, to appear in the 2017 IEEE Symposium Series on\n  Computational Intelligence (SSCI) Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Side-look synthetic aperture sonar (SAS) can produce very high quality images\nof the sea-floor. When viewing this imagery, a human observer can often easily\nidentify various sea-floor textures such as sand ripple, hard-packed sand, sea\ngrass and rock. In this paper, we present the Possibilistic Fuzzy Local\nInformation C-Means (PFLICM) approach to segment SAS imagery into sea-floor\nregions that exhibit these various natural textures. The proposed PFLICM method\nincorporates fuzzy and possibilistic clustering methods and leverages (local)\nspatial information to perform soft segmentation. Results are shown on several\nSAS scenes and compared to alternative segmentation approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 21:47:50 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Zare", "Alina", ""], ["Young", "Nicholas", ""], ["Suen", "Daniel", ""], ["Nabelek", "Thomas", ""], ["Galusha", "Aquila", ""], ["Keller", "James", ""]]}, {"id": "1709.10190", "submitter": "Saeid Motiian", "authors": "Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gianfranco\n  Doretto", "title": "Unified Deep Supervised Domain Adaptation and Generalization", "comments": "International Conference on Computer Vision ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides a unified framework for addressing the problem of visual\nsupervised domain adaptation and generalization with deep models. The main idea\nis to exploit the Siamese architecture to learn an embedding subspace that is\ndiscriminative, and where mapped visual domains are semantically aligned and\nyet maximally separated. The supervised setting becomes attractive especially\nwhen only few target data samples need to be labeled. In this scenario,\nalignment and separation of semantic probability distributions is difficult\nbecause of the lack of data. We found that by reverting to point-wise\nsurrogates of distribution distances and similarities provides an effective\nsolution. In addition, the approach has a high speed of adaptation, which\nrequires an extremely low number of labeled target training samples, even one\nper category can be effective. The approach is extended to domain\ngeneralization. For both applications the experiments show very promising\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 22:35:36 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Motiian", "Saeid", ""], ["Piccirilli", "Marco", ""], ["Adjeroh", "Donald A.", ""], ["Doretto", "Gianfranco", ""]]}, {"id": "1709.10197", "submitter": "Hamid Tizhoosh", "authors": "H.R.Tizhoosh, G.J.Czarnota", "title": "Fast Barcode Retrieval for Consensus Contouring", "comments": "Images used in this paper are available to the public:\n  http://kimia.uwaterloo.ca/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marking tumors and organs is a challenging task suffering from both inter-\nand intra-observer variability. The literature quantifies observer variability\nby generating consensus among multiple experts when they mark the same image.\nAutomatically building consensus contours to establish quality assurance for\nimage segmentation is presently absent in the clinical practice. As the\n\\emph{big data} becomes more and more available, techniques to access a large\nnumber of existing segments of multiple experts becomes possible. Fast\nalgorithms are, hence, required to facilitate the search for similar cases. The\npresent work puts forward a potential framework that tested with small datasets\n(both synthetic and real images) displays the reliability of finding similar\nimages. In this paper, the idea of content-based barcodes is used to retrieve\nsimilar cases in order to build consensus contours in medical image\nsegmentation. This approach may be regarded as an extension of the conventional\natlas-based segmentation that generally works with rather small atlases due to\nrequired computational expenses. The fast segment-retrieval process via\nbarcodes makes it possible to create and use large atlases, something that\ndirectly contributes to the quality of the consensus building. Because the\naccuracy of experts' contours must be measured, we first used 500 synthetic\nprostate images with their gold markers and delineations by 20 simulated users.\nThe fast barcode-guided computed consensus delivered an average error of\n$8\\%\\!\\pm\\!5\\%$ compared against the gold standard segments. Furthermore, we\nused magnetic resonance images of prostates from 15 patients delineated by 5\noncologists and selected the best delineations to serve as the gold-standard\nsegments. The proposed barcode atlas achieved a Jaccard overlap of\n$87\\%\\!\\pm\\!9\\%$ with the contours of the gold-standard segments.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 23:28:49 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Tizhoosh", "H. R.", ""], ["Czarnota", "G. J.", ""]]}, {"id": "1709.10230", "submitter": "Keyu Lu", "authors": "Keyu Lu, Jianhui Chen, James J. Little, Hangen He", "title": "Light Cascaded Convolutional Neural Networks for Accurate Player\n  Detection", "comments": "Published in proceedings of BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision based player detection is important in sports applications. Accuracy,\nefficiency, and low memory consumption are desirable for real-time tasks such\nas intelligent broadcasting and automatic event classification. In this paper,\nwe present a cascaded convolutional neural network (CNN) that satisfies all\nthree of these requirements. Our method first trains a binary\n(player/non-player) classification network from labeled image patches. Then,\nour method efficiently applies the network to a whole image in testing. We\nconducted experiments on basketball and soccer games. Experimental results\ndemonstrate that our method can accurately detect players under challenging\nconditions such as varying illumination, highly dynamic camera movements and\nmotion blur. Comparing with conventional CNNs, our approach achieves\nstate-of-the-art accuracy on both games with 1000x fewer parameters (i.e., it\nis light}.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 03:52:00 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Lu", "Keyu", ""], ["Chen", "Jianhui", ""], ["Little", "James J.", ""], ["He", "Hangen", ""]]}, {"id": "1709.10282", "submitter": "Jia-Ren Chang", "authors": "Jia-Ren Chang and Yong-Sheng Chen", "title": "Deep Competitive Pathway Networks", "comments": "To appear in ACML17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the design of deep neural architectures, recent studies have demonstrated\nthe benefits of grouping subnetworks into a larger network. For examples, the\nInception architecture integrates multi-scale subnetworks and the residual\nnetwork can be regarded that a residual unit combines a residual subnetwork\nwith an identity shortcut. In this work, we embrace this observation and\npropose the Competitive Pathway Network (CoPaNet). The CoPaNet comprises a\nstack of competitive pathway units and each unit contains multiple parallel\nresidual-type subnetworks followed by a max operation for feature competition.\nThis mechanism enhances the model capability by learning a variety of features\nin subnetworks. The proposed strategy explicitly shows that the features\npropagate through pathways in various routing patterns, which is referred to as\npathway encoding of category information. Moreover, the cross-block shortcut\ncan be added to the CoPaNet to encourage feature reuse. We evaluated the\nproposed CoPaNet on four object recognition benchmarks: CIFAR-10, CIFAR-100,\nSVHN, and ImageNet. CoPaNet obtained the state-of-the-art or comparable results\nusing similar amounts of parameters. The code of CoPaNet is available at:\nhttps://github.com/JiaRenChang/CoPaNet.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 08:34:05 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Chang", "Jia-Ren", ""], ["Chen", "Yong-Sheng", ""]]}, {"id": "1709.10354", "submitter": "Yvain Qu\\'eau", "authors": "Yvain Qu\\'eau, Jean M\\'elou, Fabien Castan, Daniel Cremers and\n  Jean-Denis Durou", "title": "A Variational Approach to Shape-from-shading Under Natural Illumination", "comments": "Presented at EMMCVPR 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A numerical solution to shape-from-shading under natural illumination is\npresented. It builds upon an augmented Lagrangian approach for solving a\ngeneric PDE-based shape-from-shading model which handles directional or\nspherical harmonic lighting, orthographic or perspective projection, and\ngreylevel or multi-channel images. Real-world applications to shading-aware\ndepth map denoising, refinement and completion are presented.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 11:58:13 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 19:10:56 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Qu\u00e9au", "Yvain", ""], ["M\u00e9lou", "Jean", ""], ["Castan", "Fabien", ""], ["Cremers", "Daniel", ""], ["Durou", "Jean-Denis", ""]]}, {"id": "1709.10433", "submitter": "Vishnu Naresh Boddeti", "authors": "Sixue Gong, Vishnu Naresh Boddeti, Anil K. Jain", "title": "On the Capacity of Face Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the following question, given a face representation,\nhow many identities can it resolve? In other words, what is the capacity of the\nface representation? A scientific basis for estimating the capacity of a given\nface representation will not only benefit the evaluation and comparison of\ndifferent representation methods, but will also establish an upper bound on the\nscalability of an automatic face recognition system. We cast the face capacity\nproblem in terms of packing bounds on a low-dimensional manifold embedded\nwithin a deep representation space. By explicitly accounting for the manifold\nstructure of the representation as well two different sources of\nrepresentational noise: epistemic (model) uncertainty and aleatoric (data)\nvariability, our approach is able to estimate the capacity of a given face\nrepresentation. To demonstrate the efficacy of our approach, we estimate the\ncapacity of two deep neural network based face representations, namely\n128-dimensional FaceNet and 512-dimensional SphereFace. Numerical experiments\non unconstrained faces (IJB-C) provides a capacity upper bound of\n$2.7\\times10^4$ for FaceNet and $8.4\\times10^4$ for SphereFace representation\nat a false acceptance rate (FAR) of 1%. As expected, capacity reduces\ndrastically at lower FARs. The capacity at FAR of 0.1% and 0.001% is\n$2.2\\times10^3$ and $1.6\\times10^{1}$, respectively for FaceNet and\n$3.6\\times10^3$ and $6.0\\times10^0$, respectively for SphereFace.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 14:47:13 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 15:14:24 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 19:45:51 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Gong", "Sixue", ""], ["Boddeti", "Vishnu Naresh", ""], ["Jain", "Anil K.", ""]]}, {"id": "1709.10437", "submitter": "Laurent Hoeltgen", "authors": "Georg Radow, Laurent Hoeltgen, Yvain Qu\\'eau and Michael Breu{\\ss}", "title": "Optimisation of photometric stereo methods by non-convex variational\n  minimisation", "comments": "18 pages, 18 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating shape and appearance of a three dimensional object from a given\nset of images is a classic research topic that is still actively pursued. Among\nthe various techniques available, PS is distinguished by the assumption that\nthe underlying input images are taken from the same point of view but under\ndifferent lighting conditions. The most common techniques provide the shape\ninformation in terms of surface normals. In this work, we instead propose to\nminimise a much more natural objective function, namely the reprojection error\nin terms of depth. Minimising the resulting non-trivial variational model for\nPS allows to recover the depth of the photographed scene directly. As a solving\nstrategy, we follow an approach based on a recently published optimisation\nscheme for non-convex and non-smooth cost functions.\n  The main contributions of our paper are of theoretical nature. A technical\nnovelty in our framework is the usage of matrix differential calculus. We\nsupplement our approach by a detailed convergence analysis of the resulting\noptimisation algorithm and discuss possibilities to ease the computational\ncomplexity. At hand of an experimental evaluation we discuss important\nproperties of the method. Overall, our strategy achieves more accurate results\nthan competing approaches. The experiments also highlights some practical\naspects of the underlying optimisation algorithm that may be of interest in a\nmore general context.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 14:50:55 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Radow", "Georg", ""], ["Hoeltgen", "Laurent", ""], ["Qu\u00e9au", "Yvain", ""], ["Breu\u00df", "Michael", ""]]}, {"id": "1709.10459", "submitter": "Fred Bertsch", "authors": "Andrew Kyle Lampinen, David So, Douglas Eck, and Fred Bertsch", "title": "Improving image generative models with human interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GANs provide a framework for training generative models which mimic a data\ndistribution. However, in many cases we wish to train these generative models\nto optimize some auxiliary objective function within the data it generates,\nsuch as making more aesthetically pleasing images. In some cases, these\nobjective functions are difficult to evaluate, e.g. they may require human\ninteraction. Here, we develop a system for efficiently improving a GAN to\ntarget an objective involving human interaction, specifically generating images\nthat increase rates of positive user interactions. To improve the generative\nmodel, we build a model of human behavior in the targeted domain from a\nrelatively small set of interactions, and then use this behavioral model as an\nauxiliary loss function to improve the generative model. We show that this\nsystem is successful at improving positive interaction rates, at least on\nsimulated data, and characterize some of the factors that affect its\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 15:38:42 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Lampinen", "Andrew Kyle", ""], ["So", "David", ""], ["Eck", "Douglas", ""], ["Bertsch", "Fred", ""]]}, {"id": "1709.10494", "submitter": "Valsamis Ntouskos", "authors": "Marta Sanzari, Valsamis Ntouskos, Fiora Pirri", "title": "Discovery and recognition of motion primitives in human activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for the automatic discovery and recognition of\nmotion primitives in videos of human activities. Given the 3D pose of a human\nin a video, human motion primitives are discovered by optimizing the `motion\nflux', a quantity which captures the motion variation of a group of skeletal\njoints. A normalization of the primitives is proposed in order to make them\ninvariant with respect to a subject anatomical variations and data sampling\nrate. The discovered primitives are unknown and unlabeled and are\nunsupervisedly collected into classes via a hierarchical non-parametric Bayes\nmixture model. Once classes are determined and labeled they are further\nanalyzed for establishing models for recognizing discovered primitives. Each\nprimitive model is defined by a set of learned parameters.\n  Given new video data and given the estimated pose of the subject appearing on\nthe video, the motion is segmented into primitives, which are recognized with a\nprobability given according to the parameters of the learned models.\n  Using our framework we build a publicly available dataset of human motion\nprimitives, using sequences taken from well-known motion capture datasets. We\nexpect that our framework, by providing an objective way for discovering and\ncategorizing human motion, will be a useful tool in numerous research fields\nincluding video analysis, human inspired motion generation, learning by\ndemonstration, intuitive human-robot interaction, and human behavior analysis.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 16:59:06 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 12:29:13 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 14:02:03 GMT"}, {"version": "v4", "created": "Tue, 15 May 2018 10:24:48 GMT"}, {"version": "v5", "created": "Tue, 12 Jun 2018 12:32:24 GMT"}, {"version": "v6", "created": "Thu, 3 Jan 2019 20:18:03 GMT"}, {"version": "v7", "created": "Mon, 4 Feb 2019 13:17:58 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Sanzari", "Marta", ""], ["Ntouskos", "Valsamis", ""], ["Pirri", "Fiora", ""]]}, {"id": "1709.10507", "submitter": "Francesco Puja", "authors": "Francesco Puja, Simone Grazioso, Antonio Tammaro, Valsmis Ntouskos,\n  Marta Sanzari, Fiora Pirri", "title": "Vision-based deep execution monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Execution monitor of high-level robot actions can be effectively improved by\nvisual monitoring the state of the world in terms of preconditions and\npostconditions that hold before and after the execution of an action.\nFurthermore a policy for searching where to look at, either for verifying the\nrelations that specify the pre and postconditions or to refocus in case of a\nfailure, can tremendously improve the robot execution in an uncharted\nenvironment. It is now possible to strongly rely on visual perception in order\nto make the assumption that the environment is observable, by the amazing\nresults of deep learning. In this work we present visual execution monitoring\nfor a robot executing tasks in an uncharted Lab environment. The execution\nmonitor interacts with the environment via a visual stream that uses two DCNN\nfor recognizing the objects the robot has to deal with and manipulate, and a\nnon-parametric Bayes estimation to discover the relations out of the DCNN\nfeatures. To recover from lack of focus and failures due to missed objects we\nresort to visual search policies via deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 17:33:39 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Puja", "Francesco", ""], ["Grazioso", "Simone", ""], ["Tammaro", "Antonio", ""], ["Ntouskos", "Valsmis", ""], ["Sanzari", "Marta", ""], ["Pirri", "Fiora", ""]]}]