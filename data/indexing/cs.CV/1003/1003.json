[{"id": "1003.0487", "submitter": "Chunhua Shen", "authors": "Chunhua Shen, Junae Kim, and Lei Wang", "title": "Scalable Large-Margin Mahalanobis Distance Metric Learning", "comments": "To publish/Published in IEEE Transactions on Neural Networks, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  For many machine learning algorithms such as $k$-Nearest Neighbor ($k$-NN)\nclassifiers and $ k $-means clustering, often their success heavily depends on\nthe metric used to calculate distances between different data points.\n  An effective solution for defining such a metric is to learn it from a set of\nlabeled training samples. In this work, we propose a fast and scalable\nalgorithm to learn a Mahalanobis distance metric. By employing the principle of\nmargin maximization to achieve better generalization performances, this\nalgorithm formulates the metric learning as a convex optimization problem and a\npositive semidefinite (psd) matrix is the unknown variable. a specialized\ngradient descent method is proposed. our algorithm is much more efficient and\nhas a better performance in scalability compared with existing methods.\nExperiments on benchmark data sets suggest that, compared with state-of-the-art\nmetric learning algorithms, our algorithm can achieve a comparable\nclassification accuracy with reduced computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 01:12:34 GMT"}], "update_date": "2010-03-03", "authors_parsed": [["Shen", "Chunhua", ""], ["Kim", "Junae", ""], ["Wang", "Lei", ""]]}, {"id": "1003.0529", "submitter": "Suresh Venkatasubramanian", "authors": "Arvind Agarwal, Jeff M. Phillips, Suresh Venkatasubramanian", "title": "A Unified Algorithmic Framework for Multi-Dimensional Scaling", "comments": "18 pages, 7 figures. This version fixes a bug in the proof of Theorem\n  6.1 (dimensionality reduction for spherical data). The statement of the\n  result remains the same.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified algorithmic framework for solving many\nknown variants of \\mds. Our algorithm is a simple iterative scheme with\nguaranteed convergence, and is \\emph{modular}; by changing the internals of a\nsingle subroutine in the algorithm, we can switch cost functions and target\nspaces easily. In addition to the formal guarantees of convergence, our\nalgorithms are accurate; in most cases, they converge to better quality\nsolutions than existing methods, in comparable time. We expect that this\nframework will be useful for a number of \\mds variants that have not yet been\nstudied.\n  Our framework extends to embedding high-dimensional points lying on a sphere\nto points on a lower dimensional sphere, preserving geodesic distances. As a\ncompliment to this result, we also extend the Johnson-Lindenstrauss Lemma to\nthis spherical setting, where projecting to a random $O((1/\\eps^2) \\log\nn)$-dimensional sphere causes $\\eps$-distortion.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 09:11:44 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2010 17:21:53 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Agarwal", "Arvind", ""], ["Phillips", "Jeff M.", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1003.0642", "submitter": "Ayatullah Faruk Mollah", "authors": "Ayatullah Faruk Mollah, Subhadip Basu, Nibaran Das, Ram Sarkar, Mita\n  Nasipuri, Mahantapas Kundu", "title": "Text Region Extraction from Business Card Images for Mobile Devices", "comments": "Proc. of International Conference on Information Technology and\n  Business Intelligence (ITBI-09), pp.227-235, Nov 6-8, 2009, Nagpur, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a Business Card Reader (BCR) for mobile devices is a challenge to\nthe researchers because of huge deformation in acquired images, multiplicity in\nnature of the business cards and most importantly the computational constraints\nof the mobile devices. This paper presents a text extraction method designed in\nour work towards developing a BCR for mobile devices. At first, the background\nof a camera captured image is eliminated at a coarse level. Then, various rule\nbased techniques are applied on the Connected Components (CC) to filter out the\nnoises and picture regions. The CCs identified as text are then binarized using\nan adaptive but light-weight binarization technique. Experiments show that the\ntext extraction accuracy is around 98% for a wide range of resolutions with\nvarying computation time and memory requirements. The optimum performance is\nachieved for the images of resolution 1024x768 pixels with text extraction\naccuracy of 98.54% and, space and time requirements as 1.1 MB and 0.16 seconds\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 17:45:26 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2010 15:17:17 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["Mollah", "Ayatullah Faruk", ""], ["Basu", "Subhadip", ""], ["Das", "Nibaran", ""], ["Sarkar", "Ram", ""], ["Nasipuri", "Mita", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1003.0645", "submitter": "Ayatullah Faruk Mollah", "authors": "Ayatullah Faruk Mollah, Subhadip Basu, Nibaran Das, Ram Sarkar, Mita\n  Nasipuri, Mahantapas Kundu", "title": "Binarizing Business Card Images for Mobile Devices", "comments": "Proc. of International Conference on Computer Vision and Information\n  Technology (ACVIT-2009), pp. 968-975, Dec 16-19, 2009, Aurangabad, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business card images are of multiple natures as these often contain graphics,\npictures and texts of various fonts and sizes both in background and\nforeground. So, the conventional binarization techniques designed for document\nimages can not be directly applied on mobile devices. In this paper, we have\npresented a fast binarization technique for camera captured business card\nimages. A card image is split into small blocks. Some of these blocks are\nclassified as part of the background based on intensity variance. Then the\nnon-text regions are eliminated and the text ones are skew corrected and\nbinarized using a simple yet adaptive technique. Experiment shows that the\ntechnique is fast, efficient and applicable for the mobile devices.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2010 18:02:40 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2010 08:36:40 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Mollah", "Ayatullah Faruk", ""], ["Basu", "Subhadip", ""], ["Das", "Nibaran", ""], ["Sarkar", "Ram", ""], ["Nasipuri", "Mita", ""], ["Kundu", "Mahantapas", ""]]}, {"id": "1003.0723", "submitter": "Chengfang Fang", "authors": "Chengfang Fang and Ee-Chien Chang", "title": "Securing Interactive Sessions Using Mobile Device through Visual Channel\n  and Visual Inspection", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication channel established from a display to a device's camera is\nknown as visual channel, and it is helpful in securing key exchange protocol.\nIn this paper, we study how visual channel can be exploited by a network\nterminal and mobile device to jointly verify information in an interactive\nsession, and how such information can be jointly presented in a user-friendly\nmanner, taking into account that the mobile device can only capture and display\na small region, and the user may only want to authenticate selective\nregions-of-interests. Motivated by applications in Kiosk computing and\nmulti-factor authentication, we consider three security models: (1) the mobile\ndevice is trusted, (2) at most one of the terminal or the mobile device is\ndishonest, and (3) both the terminal and device are dishonest but they do not\ncollude or communicate. We give two protocols and investigate them under the\nabovementioned models. We point out a form of replay attack that renders some\nother straightforward implementations cumbersome to use. To enhance\nuser-friendliness, we propose a solution using visual cues embedded into the 2D\nbarcodes and incorporate the framework of \"augmented reality\" for easy\nverifications through visual inspection. We give a proof-of-concept\nimplementation to show that our scheme is feasible in practice.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 03:31:29 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2010 04:10:33 GMT"}], "update_date": "2010-07-09", "authors_parsed": [["Fang", "Chengfang", ""], ["Chang", "Ee-Chien", ""]]}, {"id": "1003.0776", "submitter": "Roumen Anguelov", "authors": "Roumen Anguelov and Inger Fabris-Rotelli", "title": "Properties of the Discrete Pulse Transform for Multi-Dimensional Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents properties of the Discrete Pulse Transform on\nmulti-dimensional arrays introduced by the authors two or so years ago. The\nmain result given here in Lemma 2.1 is also formulated in a paper to appear in\nIEEE Transactions on Image Processing. However, the proof, being too technical,\nwas omitted there and hence it appears in full in this publication.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 10:58:20 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Anguelov", "Roumen", ""], ["Fabris-Rotelli", "Inger", ""]]}, {"id": "1003.1072", "submitter": "Satadal Saha", "authors": "Satadal Saha, Subhadip Basu, Mita Nasipuri and Dipak Kumar Basu", "title": "An Offline Technique for Localization of License Plates for Indian\n  Commercial Vehicles", "comments": "National Conference on Computing and Communication Systems\n  (COCOSYS-09)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic License Plate Recognition (ALPR) is a challenging area of research\ndue to its importance to variety of commercial applications. The overall\nproblem may be subdivided into two key modules, firstly, localization of\nlicense plates from vehicle images, and secondly, optical character recognition\nof extracted license plates. In the current work, we have concentrated on the\nfirst part of the problem, i.e., localization of license plate regions from\nIndian commercial vehicles as a significant step towards development of a\ncomplete ALPR system for Indian vehicles. The technique is based on color based\nsegmentation of vehicle images and identification of potential license plate\nregions. True license plates are finally localized based on four spatial and\nhorizontal contrast features. The technique successfully localizes the actual\nlicense plates in 73.4% images.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 15:57:41 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 21:42:43 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Saha", "Satadal", ""], ["Basu", "Subhadip", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1003.1458", "submitter": "Rdv Ijcsis", "authors": "A. Jagadeesan, K.Duraiswamy", "title": "Secured Cryptographic Key Generation From Multimodal Biometrics: Feature\n  Level Fusion of Fingerprint and Iris", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS February 2010, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 7, No. 2, pp. 028-037, February 2010, USA", "doi": null, "report-no": "Computer Science ISSN 19475500", "categories": "cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Human users have a tough time remembering long cryptographic keys. Hence,\nresearchers, for so long, have been examining ways to utilize biometric\nfeatures of the user instead of a memorable password or passphrase, in an\neffort to generate strong and repeatable cryptographic keys. Our objective is\nto incorporate the volatility of the user's biometric features into the\ngenerated key, so as to make the key unguessable to an attacker lacking\nsignificant knowledge of the user's biometrics. We go one step further trying\nto incorporate multiple biometric modalities into cryptographic key generation\nso as to provide better security. In this article, we propose an efficient\napproach based on multimodal biometrics (Iris and fingerprint) for generation\nof secure cryptographic key. The proposed approach is composed of three modules\nnamely, 1) Feature extraction, 2) Multimodal biometric template generation and\n3) Cryptographic key generation. Initially, the features, minutiae points and\ntexture properties are extracted from the fingerprint and iris images\nrespectively. Subsequently, the extracted features are fused together at the\nfeature level to construct the multi-biometric template. Finally, a 256-bit\nsecure cryptographic key is generated from the multi-biometric template. For\nexperimentation, we have employed the fingerprint images obtained from publicly\navailable sources and the iris images from CASIA Iris Database. The\nexperimental results demonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 12:15:40 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Jagadeesan", "A.", ""], ["Duraiswamy", "K.", ""]]}, {"id": "1003.1511", "submitter": "Rdv Ijcsis", "authors": "Rohit Katiyar, Dr. Vinay Kumar Pathak", "title": "Clinical gait data analysis based on Spatio-Temporal features", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Analysing human gait has found considerable interest in recent computer\nvision research. So far, however, contributions to this topic exclusively dealt\nwith the tasks of person identification or activity recognition. In this paper,\nwe consider a different application for gait analysis and examine its use as a\nmeans of deducing the physical well-being of people. The proposed method is\nbased on transforming the joint motion trajectories using wavelets to extract\nspatio-temporal features which are then fed as input to a vector quantiser; a\nself-organising map for classification of walking patterns of individuals with\nand without pathology. We show that our proposed algorithm is successful in\nextracting features that successfully discriminate between individuals with and\nwithout locomotion impairment.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 18:46:12 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Katiyar", "Rohit", ""], ["Pathak", "Dr. Vinay Kumar", ""]]}, {"id": "1003.1803", "submitter": "Rdv Ijcsis", "authors": "T. K. Thivakaran, RM. Chandrasekaran", "title": "Nonlinear Filter Based Image Denoising Using AMF Approach", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper proposes a new technique based on nonlinear Adaptive Median filter\n(AMF) for image restoration. Image denoising is a common procedure in digital\nimage processing aiming at the removal of noise, which may corrupt an image\nduring its acquisition or transmission, while retaining its quality. This\nprocedure is traditionally performed in the spatial or frequency domain by\nfiltering. The aim of image enhancement is to reconstruct the true image from\nthe corrupted image. The process of image acquisition frequently leads to\ndegradation and the quality of the digitized image becomes inferior to the\noriginal image. Filtering is a technique for enhancing the image. Linear filter\nis the filtering in which the value of an output pixel is a linear combination\nof neighborhood values, which can produce blur in the image. Thus a variety of\nsmoothing techniques have been developed that are non linear. Median filter is\nthe one of the most popular non-linear filter. When considering a small\nneighborhood it is highly efficient but for large window and in case of high\nnoise it gives rise to more blurring to image. The Centre Weighted Median (CWM)\nfilter has got a better average performance over the median filter [8]. However\nthe original pixel corrupted and noise reduction is substantial under high\nnoise condition. Hence this technique has also blurring affect on the image. To\nillustrate the superiority of the proposed approach by overcoming the existing\nproblem, the proposed new scheme (AMF) Adaptive Median Filter has been\nsimulated along with the standard ones and various performance measures have\nbeen compared.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 07:05:47 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Thivakaran", "T. K.", ""], ["Chandrasekaran", "RM.", ""]]}, {"id": "1003.1819", "submitter": "Rdv Ijcsis", "authors": "Supriya Kapoor, Shruti Khanna, Rahul Bhatia", "title": "Facial Gesture Recognition Using Correlation And Mahalanobis Distance", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Augmenting human computer interaction with automated analysis and synthesis\nof facial expressions is a goal towards which much research effort has been\ndevoted recently. Facial gesture recognition is one of the important component\nof natural human-machine interfaces; it may also be used in behavioural\nscience, security systems and in clinical practice. Although humans recognise\nfacial expressions virtually without effort or delay, reliable expression\nrecognition by machine is still a challenge. The face expression recognition\nproblem is challenging because different individuals display the same\nexpression differently. This paper presents an overview of gesture recognition\nin real time using the concepts of correlation and Mahalanobis distance.We\nconsider the six universal emotional categories namely joy, anger, fear,\ndisgust, sadness and surprise.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 07:39:03 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["Kapoor", "Supriya", ""], ["Khanna", "Shruti", ""], ["Bhatia", "Rahul", ""]]}, {"id": "1003.1826", "submitter": "Rdv Ijcsis", "authors": "Syed Amjad Ali, Srinivasan Vathsal, K. Lal kishore", "title": "A GA based Window Selection Methodology to Enhance Window based Multi\n  wavelet transformation and thresholding aided CT image denoising technique", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Image denoising is getting more significance, especially in Computed\nTomography (CT), which is an important and most common modality in medical\nimaging. This is mainly due to that the effectiveness of clinical diagnosis\nusing CT image lies on the image quality. The denoising technique for CT images\nusing window-based Multi-wavelet transformation and thresholding shows the\neffectiveness in denoising, however, a drawback exists in selecting the closer\nwindows in the process of window-based multi-wavelet transformation and\nthresholding. Generally, the windows of the duplicate noisy image that are\ncloser to each window of original noisy image are obtained by the checking them\nsequentially. This leads to the possibility of missing out very closer windows\nand so enhancement is required in the aforesaid process of the denoising\ntechnique. In this paper, we propose a GA-based window selection methodology to\ninclude the denoising technique. With the aid of the GA-based window selection\nmethodology, the windows of the duplicate noisy image that are very closer to\nevery window of the original noisy image are extracted in an effective manner.\nBy incorporating the proposed GA-based window selection methodology, the\ndenoising the CT image is performed effectively. Eventually, a comparison is\nmade between the denoising technique with and without the proposed GA-based\nwindow selection methodology.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 08:09:02 GMT"}], "update_date": "2010-03-11", "authors_parsed": [["Ali", "Syed Amjad", ""], ["Vathsal", "Srinivasan", ""], ["kishore", "K. Lal", ""]]}, {"id": "1003.1827", "submitter": "Rdv Ijcsis", "authors": "Vidhi Rawat, Alok jain, Vibhakar shrimali", "title": "Investigation and Assessment of Disorder of Ultrasound B-mode Images", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Digital image plays a vital role in the early detection of cancers, such as\nprostate cancer, breast cancer, lungs cancer, cervical cancer. Ultrasound\nimaging method is also suitable for early detection of the abnormality of\nfetus. The accurate detection of region of interest in ultrasound image is\ncrucial. Since the result of reflection, refraction and deflection of\nultrasound waves from different types of tissues with different acoustic\nimpedance. Usually, the contrast in ultrasound image is very low and weak edges\nmake the image difficult to identify the fetus region in the ultrasound image.\nSo the analysis of ultrasound image is more challenging one. We try to develop\na new algorithmic approach to solve the problem of non clarity and find\ndisorder of it. Generally there is no common enhancement approach for noise\nreduction. This paper proposes different filtering techniques based on\nstatistical methods for the removal of various noise. The quality of the\nenhanced images is measured by the statistical quantity measures:\nSignal-to-Noise Ratio (SNR), Peak Signal-to-Noise Ratio (PSNR), and Root Mean\nSquare Error (RMSE).\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 08:13:37 GMT"}], "update_date": "2010-03-11", "authors_parsed": [["Rawat", "Vidhi", ""], ["jain", "Alok", ""], ["shrimali", "Vibhakar", ""]]}, {"id": "1003.1891", "submitter": "Ayatullah Faruk Mollah", "authors": "Nibaran Das, Ayatullah Faruk Mollah, Sudip Saha, Syed Sahidul Haque", "title": "Handwritten Arabic Numeral Recognition using a Multi Layer Perceptron", "comments": "Proc. National Conference on Recent Trends in Information Systems\n  (ReTIS-06), July 14-15, 2006, Kolkata, India, pp 200-203", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten numeral recognition is in general a benchmark problem of Pattern\nRecognition and Artificial Intelligence. Compared to the problem of printed\nnumeral recognition, the problem of handwritten numeral recognition is\ncompounded due to variations in shapes and sizes of handwritten characters.\nConsidering all these, the problem of handwritten numeral recognition is\naddressed under the present work in respect to handwritten Arabic numerals.\nArabic is spoken throughout the Arab World and the fifth most popular language\nin the world slightly before Portuguese and Bengali. For the present work, we\nhave developed a feature set of 88 features is designed to represent samples of\nhandwritten Arabic numerals for this work. It includes 72 shadow and 16 octant\nfeatures. A Multi Layer Perceptron (MLP) based classifier is used here for\nrecognition handwritten Arabic digits represented with the said feature set. On\nexperimentation with a database of 3000 samples, the technique yields an\naverage recognition rate of 94.93% evaluated after three-fold cross validation\nof results. It is useful for applications related to OCR of handwritten Arabic\nDigit and can also be extended to include OCR of handwritten characters of\nArabic alphabet.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 14:56:00 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["Das", "Nibaran", ""], ["Mollah", "Ayatullah Faruk", ""], ["Saha", "Sudip", ""], ["Haque", "Syed Sahidul", ""]]}, {"id": "1003.1894", "submitter": "Ayatullah Faruk Mollah", "authors": "Nibaran Das, Ayatullah Faruk Mollah, Ram Sarkar, Subhadip Basu", "title": "A comparative study of different feature sets for recognition of\n  handwritten Arabic numerals using a Multi Layer Perceptron", "comments": "Proc. National Conference on Recent Trends in Intelligent Computing\n  (ReTIC-06), Nov 17-19, 2006, Kalyani, India, pp. 86-92", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work presents a comparative assessment of seven different feature sets\nfor recognition of handwritten Arabic numerals using a Multi Layer Perceptron\n(MLP) based classifier. The seven feature sets employed here consist of shadow\nfeatures, octant centroids, longest runs, angular distances, effective spans,\ndynamic centers of gravity, and some of their combinations. On experimentation\nwith a database of 3000 samples, the maximum recognition rate of 95.80% is\nobserved with both of two separate combinations of features. One of these\ncombinations consists of shadow and centriod features, i. e. 88 features in\nall, and the other shadow, centroid and longest run features, i. e. 124\nfeatures in all. Out of these two, the former combination having a smaller\nnumber of features is finally considered effective for applications related to\nOptical Character Recognition (OCR) of handwritten Arabic numerals. The work\ncan also be extended to include OCR of handwritten characters of Arabic\nalphabet.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 15:05:37 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["Das", "Nibaran", ""], ["Mollah", "Ayatullah Faruk", ""], ["Sarkar", "Ram", ""], ["Basu", "Subhadip", ""]]}, {"id": "1003.2022", "submitter": "Kunal Narayan Chaudhury", "authors": "Kunal Narayan Chaudhury, Arrate Munoz-Barrutia, and Michael Unser", "title": "Fast space-variant elliptical filtering using box splines", "comments": "12 figures; IEEE Transactions on Image Processing, vol. 19, 2010", "journal-ref": "IEEE Transactions on Image Processing, vol. 19(9), pp. 2290 -\n  2306, 2010", "doi": "10.1109/TIP.2010.2046953", "report-no": null, "categories": "cs.CV cs.CE cs.IT cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient realization of linear space-variant (non-convolution) filters\nis a challenging computational problem in image processing. In this paper, we\ndemonstrate that it is possible to filter an image with a Gaussian-like\nelliptic window of varying size, elongation and orientation using a fixed\nnumber of computations per pixel. The associated algorithm, which is based on a\nfamily of smooth compactly supported piecewise polynomials, the\nradially-uniform box splines, is realized using pre-integration and local\nfinite-differences. The radially-uniform box splines are constructed through\nthe repeated convolution of a fixed number of box distributions, which have\nbeen suitably scaled and distributed radially in an uniform fashion. The\nattractive features of these box splines are their asymptotic behavior, their\nsimple covariance structure, and their quasi-separability. They converge to\nGaussians with the increase of their order, and are used to approximate\nanisotropic Gaussians of varying covariance simply by controlling the scales of\nthe constituent box distributions. Based on the second feature, we develop a\ntechnique for continuously controlling the size, elongation and orientation of\nthese Gaussian-like functions. Finally, the quasi-separable structure, along\nwith a certain scaling property of box distributions, is used to efficiently\nrealize the associated space-variant elliptical filtering, which requires O(1)\ncomputations per pixel irrespective of the shape and size of the filter.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2010 05:53:54 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2011 22:16:50 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chaudhury", "Kunal Narayan", ""], ["Munoz-Barrutia", "Arrate", ""], ["Unser", "Michael", ""]]}, {"id": "1003.3266", "submitter": "Olga Bunyak", "authors": "Olga Sofina, Yuriy Bunyak, Roman Kvetnyy", "title": "Pattern recognition using inverse resonance filtration", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach to textures pattern recognition based on inverse resonance\nfiltration (IRF) is considered. A set of principal resonance harmonics of\ntextured image signal fluctuations eigen harmonic decomposition (EHD) is used\nfor the IRF design. It was shown that EHD is invariant to textured image linear\nshift. The recognition of texture is made by transfer of its signal into\nunstructured signal which simple statistical parameters can be used for texture\npattern recognition. Anomalous variations of this signal point on foreign\nobjects. Two methods of 2D EHD parameters estimation are considered with the\naccount of texture signal breaks presence. The first method is based on the\nlinear symmetry model that is not sensitive to signal phase jumps. The\ncondition of characteristic polynomial symmetry provides the model stationarity\nand periodicity. Second method is based on the eigenvalues problem of matrices\npencil projection into principal vectors space of singular values decomposition\n(SVD) of 2D correlation matrix. Two methods of classification of retrieval from\ntextured image foreign objects are offered.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2010 22:30:12 GMT"}], "update_date": "2010-03-18", "authors_parsed": [["Sofina", "Olga", ""], ["Bunyak", "Yuriy", ""], ["Kvetnyy", "Roman", ""]]}, {"id": "1003.3654", "submitter": "Kadirvelu SivaKumar", "authors": "Chitrakala Gopalan, D.Manjula", "title": "Sliding window approach based Text Binarisation from Complex Textual\n  images", "comments": "5 Pages IEEE format, International Journal on Computer Science and\n  Engineering, IJCSE 2010, ISSN 0975-3397, Impact Factor 0.583", "journal-ref": "International Journal on Computer Science and Engineering, IJCSE,\n  Vol. 2, No. 2 March 2010", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text binarisation process classifies individual pixels as text or background\nin the textual images. Binarization is necessary to bridge the gap between\nlocalization and recognition by OCR. This paper presents Sliding window method\nto binarise text from textual images with textured background. Suitable\npreprocessing techniques are applied first to increase the contrast of the\nimage and blur the background noises due to textured background. Then Edges are\ndetected by iterative thresholding. Subsequently formed edge boxes are analyzed\nto remove unwanted edges due to complex background and binarised by sliding\nwindow approach based character size uniformity check algorithm. The proposed\nmethod has been applied on localized region from heterogeneous textual images\nand compared with Otsu, Niblack methods and shown encouraging performance of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2010 19:01:56 GMT"}], "update_date": "2010-03-19", "authors_parsed": [["Gopalan", "Chitrakala", ""], ["Manjula", "D.", ""]]}, {"id": "1003.3984", "submitter": "Javier Turek Mr.", "authors": "Javier Turek, Irad Yavneh, Matan Protter, Michael Elad", "title": "On MMSE and MAP Denoising Under Sparse Representation Modeling Over a\n  Unitary Dictionary", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TSP.2011.2151190", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the many ways to model signals, a recent approach that draws\nconsiderable attention is sparse representation modeling. In this model, the\nsignal is assumed to be generated as a random linear combination of a few atoms\nfrom a pre-specified dictionary. In this work we analyze two Bayesian denoising\nalgorithms -- the Maximum-Aposteriori Probability (MAP) and the\nMinimum-Mean-Squared-Error (MMSE) estimators, under the assumption that the\ndictionary is unitary. It is well known that both these estimators lead to a\nscalar shrinkage on the transformed coefficients, albeit with a different\nresponse curve. In this work we start by deriving closed-form expressions for\nthese shrinkage curves and then analyze their performance. Upper bounds on the\nMAP and the MMSE estimation errors are derived. We tie these to the error\nobtained by a so-called oracle estimator, where the support is given,\nestablishing a worst-case gain-factor between the MAP/MMSE estimation errors\nand the oracle's performance. These denoising algorithms are demonstrated on\nsynthetic signals and on true data (images).\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2010 08:39:51 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Turek", "Javier", ""], ["Yavneh", "Irad", ""], ["Protter", "Matan", ""], ["Elad", "Michael", ""]]}, {"id": "1003.3985", "submitter": "Raja Giryes", "authors": "Raja Giryes, Michael Elad, Yonina C Eldar", "title": "The Projected GSURE for Automatic Parameter Tuning in Iterative\n  Shrinkage Methods", "comments": "20 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear inverse problems are very common in signal and image processing. Many\nalgorithms that aim at solving such problems include unknown parameters that\nneed tuning. In this work we focus on optimally selecting such parameters in\niterative shrinkage methods for image deblurring and image zooming. Our work\nuses the projected Generalized Stein Unbiased Risk Estimator (GSURE) for\ndetermining the threshold value lambda and the iterations number K in these\nalgorithms. The proposed parameter selection is shown to handle any degradation\noperator, including ill-posed and even rectangular ones. This is achieved by\nusing GSURE on the projected expected error. We further propose an efficient\ngreedy parameter setting scheme, that tunes the parameter while iterating\nwithout impairing the resulting deblurring performance. Finally, we provide\nextensive comparisons to conventional methods for parameter selection, showing\nthe superiority of the use of the projected GSURE.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2010 08:43:52 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Giryes", "Raja", ""], ["Elad", "Michael", ""], ["Eldar", "Yonina C", ""]]}, {"id": "1003.4021", "submitter": "Vitaly Pimenov", "authors": "Vitaly Pimenov", "title": "System-theoretic approach to image interest point detection", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest point detection is a common task in various computer vision\napplications. Although a big variety of detector are developed so far\ncomputational efficiency of interest point based image analysis remains to be\nthe problem. Current paper proposes a system-theoretic approach to interest\npoint detection. Starting from the analysis of interdependency between detector\nand descriptor it is shown that given a descriptor it is possible to introduce\nto notion of detector redundancy. Furthermore for each detector it is possible\nto construct its irredundant and equivalent modification. Modified detector\npossesses lower computational complexity and is preferable. It is also shown\nthat several known approaches to reduce computational complexity of image\nregistration can be generalized in terms of proposed theory.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2010 20:21:09 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Pimenov", "Vitaly", ""]]}, {"id": "1003.4053", "submitter": "William Jackson", "authors": "Raman Maini, Himanshu Aggarwal", "title": "A Comprehensive Review of Image Enhancement Techniques", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 3, March 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principle objective of Image enhancement is to process an image so that\nresult is more suitable than original image for specific application. Digital\nimage enhancement techniques provide a multitude of choices for improving the\nvisual quality of images. Appropriate choice of such techniques is greatly\ninfluenced by the imaging modality, task at hand and viewing conditions. This\npaper will provide an overview of underlying concepts, along with algorithms\ncommonly used for image enhancement. The paper focuses on spatial domain\ntechniques for image enhancement, with particular reference to point processing\nmethods and histogram processing.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 03:39:46 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Maini", "Raman", ""], ["Aggarwal", "Himanshu", ""]]}, {"id": "1003.4087", "submitter": "William Jackson", "authors": "Ratika Pradhan, Mohan P. Pradhan, Ashish Bhusan, Ronak K. Pradhan, M.\n  K. Ghose", "title": "Land-cover Classification and Mapping for Eastern Himalayan State Sikkim", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 3, March 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Area of classifying satellite imagery has become a challenging task in\ncurrent era where there is tremendous growth in settlement i.e. construction of\nbuildings, roads, bridges, dam etc. This paper suggests an improvised k-means\nand Artificial Neural Network (ANN) classifier for land-cover mapping of\nEastern Himalayan state Sikkim. The improvised k-means algorithm shows\nsatisfactory results compared to existing methods that includes k-Nearest\nNeighbor and maximum likelihood classifier. The strength of the Artificial\nNeural Network (ANN) classifier lies in the fact that they are fast and have\ngood recognition rate and it's capability of self-learning compared to other\nclassification algorithms has made it widely accepted. Classifier based on ANN\nshows satisfactory and accurate result in comparison with the classical method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 06:49:30 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Pradhan", "Ratika", ""], ["Pradhan", "Mohan P.", ""], ["Bhusan", "Ashish", ""], ["Pradhan", "Ronak K.", ""], ["Ghose", "M. K.", ""]]}, {"id": "1003.4287", "submitter": "Mayank Kabra", "authors": "Mayank Kabra, Annie L. Conery, Eyleen J. O'Rourke, Xin Xie, Vebjorn\n  Ljosa, Thouis R. Jones, Frederick M. Ausubel, Gary Ruvkun, Anne E. Carpenter,\n  and Yoav Freund", "title": "Towards automated high-throughput screening of C. elegans on agar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput screening (HTS) using model organisms is a promising method\nto identify a small number of genes or drugs potentially relevant to human\nbiology or disease. In HTS experiments, robots and computers do a significant\nportion of the experimental work. However, one remaining major bottleneck is\nthe manual analysis of experimental results, which is commonly in the form of\nmicroscopy images. This manual inspection is labor intensive, slow and\nsubjective. Here we report our progress towards applying computer vision and\nmachine learning methods to analyze HTS experiments that use Caenorhabditis\nelegans (C. elegans) worms grown on agar. Our main contribution is a robust\nsegmentation algorithm for separating the worms from the background using\nbrightfield images. We also show that by combining the output of this\nsegmentation algorithm with an algorithm to detect the fluorescent dye, Nile\nRed, we can reliably distinguish different fluorescence-based phenotypes even\nthough the visual differences are subtle. The accuracy of our method is similar\nto that of expert human analysts. This new capability is a significant step\ntowards fully automated HTS experiments using C. elegans.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 21:08:43 GMT"}], "update_date": "2010-03-24", "authors_parsed": [["Kabra", "Mayank", ""], ["Conery", "Annie L.", ""], ["O'Rourke", "Eyleen J.", ""], ["Xie", "Xin", ""], ["Ljosa", "Vebjorn", ""], ["Jones", "Thouis R.", ""], ["Ausubel", "Frederick M.", ""], ["Ruvkun", "Gary", ""], ["Carpenter", "Anne E.", ""], ["Freund", "Yoav", ""]]}, {"id": "1003.4781", "submitter": "Xu Miao", "authors": "Xu Miao, Rajesh P.N. Rao", "title": "Large Margin Boltzmann Machines and Large Margin Sigmoid Belief Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "UW-CSE-09-04-01", "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current statistical models for structured prediction make simplifying\nassumptions about the underlying output graph structure, such as assuming a\nlow-order Markov chain, because exact inference becomes intractable as the\ntree-width of the underlying graph increases. Approximate inference algorithms,\non the other hand, force one to trade off representational power with\ncomputational efficiency. In this paper, we propose two new types of\nprobabilistic graphical models, large margin Boltzmann machines (LMBMs) and\nlarge margin sigmoid belief networks (LMSBNs), for structured prediction.\nLMSBNs in particular allow a very fast inference algorithm for arbitrary graph\nstructures that runs in polynomial time with a high probability. This\nprobability is data-distribution dependent and is maximized in learning. The\nnew approach overcomes the representation-efficiency trade-off in previous\nmodels and allows fast structured prediction with complicated graph structures.\nWe present results from applying a fully connected model to multi-label scene\nclassification and demonstrate that the proposed approach can yield significant\nperformance gains over current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2010 02:21:11 GMT"}], "update_date": "2010-03-26", "authors_parsed": [["Miao", "Xu", ""], ["Rao", "Rajesh P. N.", ""]]}, {"id": "1003.5249", "submitter": "Raphael Sznitman", "authors": "Raphael Sznitman, Bruno Jedynak", "title": "Active Testing for Face Detection and Localization", "comments": "16 pages, 5 figures, accepted in IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (TPAMI), 2010", "journal-ref": null, "doi": "10.1109/TPAMI.2010.106", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel search technique, which uses a hierarchical model and a\nmutual information gain heuristic to efficiently prune the search space when\nlocalizing faces in images. We show exponential gains in computation over\ntraditional sliding window approaches, while keeping similar performance\nlevels.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2010 00:17:19 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Sznitman", "Raphael", ""], ["Jedynak", "Bruno", ""]]}, {"id": "1003.5320", "submitter": "Michael Bronstein", "authors": "Alexander M. Bronstein, Michael M. Bronstein, Ron Kimmel", "title": "The Video Genome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast evolution of Internet technologies has led to an explosive growth of\nvideo data available in the public domain and created unprecedented challenges\nin the analysis, organization, management, and control of such content. The\nproblems encountered in video analysis such as identifying a video in a large\ndatabase (e.g. detecting pirated content in YouTube), putting together video\nfragments, finding similarities and common ancestry between different versions\nof a video, have analogous counterpart problems in genetic research and\nanalysis of DNA and protein sequences. In this paper, we exploit the analogy\nbetween genetic sequences and videos and propose an approach to video analysis\nmotivated by genomic research. Representing video information as video DNA\nsequences and applying bioinformatic algorithms allows to search, match, and\ncompare videos in large-scale databases. We show an application for\ncontent-based metadata mapping between versions of annotated video.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2010 20:57:47 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Bronstein", "Alexander M.", ""], ["Bronstein", "Michael M.", ""], ["Kimmel", "Ron", ""]]}, {"id": "1003.5435", "submitter": "Secretary Aircc Journal", "authors": "Kilari Veera Swamy (1), B.Chandra Mohan (2), Y.V.Bhaskar Reddy (3) and\n  S.Srinivas Kumar (4) ((1) QISCET, Ongole, India, (2) BEC, Bapatla, India, (3)\n  QISCET, Ongole, India and (4) JNTU, Kakinada, India)", "title": "Image Compression and Watermarking scheme using Scalar Quantization", "comments": "11 Pages, IJNGN Journal 2010", "journal-ref": "International Journal of Next-Generation Networks 2.1 (2010) 37-47", "doi": "10.5121/ijngn.2010.2104", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a new compression technique and image watermarking\nalgorithm based on Contourlet Transform (CT). For image compression, an energy\nbased quantization is used. Scalar quantization is explored for image\nwatermarking. Double filter bank structure is used in CT. The Laplacian Pyramid\n(LP) is used to capture the point discontinuities, and then followed by a\nDirectional Filter Bank (DFB) to link point discontinuities. The coefficients\nof down sampled low pass version of LP decomposed image are re-ordered in a\npre-determined manner and prediction algorithm is used to reduce entropy\n(bits/pixel). In addition, the coefficients of CT are quantized based on the\nenergy in the particular band. The superiority of proposed algorithm to JPEG is\nobserved in terms of reduced blocking artifacts. The results are also compared\nwith wavelet transform (WT). Superiority of CT to WT is observed when the image\ncontains more contours. The watermark image is embedded in the low pass image\nof contourlet decomposition. The watermark can be extracted with minimum error.\nIn terms of PSNR, the visual quality of the watermarked image is exceptional.\nThe proposed algorithm is robust to many image attacks and suitable for\ncopyright protection applications.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2010 06:51:17 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Swamy", "Kilari Veera", ""], ["Mohan", "B. Chandra", ""], ["Reddy", "Y. V. Bhaskar", ""], ["Kumar", "S. Srinivas", ""]]}, {"id": "1003.5821", "submitter": "Amelia Carolina Sparavigna", "authors": "Roberto Marazzato, Amelia Carolina Sparavigna", "title": "Tuning CLD Maps", "comments": "Keywords: coherence length, image analysis, 2D textures; texture\n  functions; defect localisation; directional defect pattern; optimization;\n  histogram", "journal-ref": "International Journal of Software Engineering and Computing, 2011,\n  Volume 3, Issue 1, Pages 17-23", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coherence Length Diagram and the related maps have been shown to\nrepresent a useful tool for image analysis. Setting threshold parameters is one\nof the most important issues when dealing with such applications, as they\naffect both the computability, which is outlined by the support map, and the\nappearance of the coherence length diagram itself and of defect maps. A coupled\noptimization analysis, returning a range for the basic (saturation) threshold,\nand a histogram based method, yielding suitable values for a desired map\nappearance, are proposed for an effective control of the analysis process.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 13:58:08 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Marazzato", "Roberto", ""], ["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1003.5861", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Hunny Mehrotra, Phalguni Gupta, Jamuna Kanta\n  Sing", "title": "Robust multi-camera view face recognition", "comments": "10 pages, 3 figures, IJCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents multi-appearance fusion of Principal Component Analysis\n(PCA) and generalization of Linear Discriminant Analysis (LDA) for multi-camera\nview offline face recognition (verification) system. The generalization of LDA\nhas been extended to establish correlations between the face classes in the\ntransformed representation and this is called canonical covariate. The proposed\nsystem uses Gabor filter banks for characterization of facial features by\nspatial frequency, spatial locality and orientation to make compensate to the\nvariations of face instances occurred due to illumination, pose and facial\nexpression changes. Convolution of Gabor filter bank to face images produces\nGabor face representations with high dimensional feature vectors. PCA and\ncanonical covariate are then applied on the Gabor face representations to\nreduce the high dimensional feature spaces into low dimensional Gabor\neigenfaces and Gabor canonical faces. Reduced eigenface vector and canonical\nface vector are fused together using weighted mean fusion rule. Finally,\nsupport vector machines (SVM) have trained with augmented fused set of features\nand perform the recognition task. The system has been evaluated with UMIST face\ndatabase consisting of multiview faces. The experimental results demonstrate\nthe efficiency and robustness of the proposed system for multi-view face images\nwith high recognition rates. Complexity analysis of the proposed system is also\npresented at the end of the experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 16:26:39 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Mehrotra", "Hunny", ""], ["Gupta", "Phalguni", ""], ["Sing", "Jamuna Kanta", ""]]}, {"id": "1003.5865", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Phalguni Gupta, Jamuna Kanta Sing", "title": "Offline Signature Identification by Fusion of Multiple Classifiers using\n  Statistical Learning Theory", "comments": "11 pages, 3 figures, IJSIA 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper uses Support Vector Machines (SVM) to fuse multiple classifiers\nfor an offline signature system. From the signature images, global and local\nfeatures are extracted and the signatures are verified with the help of\nGaussian empirical rule, Euclidean and Mahalanobis distance based classifiers.\nSVM is used to fuse matching scores of these matchers. Finally, recognition of\nquery signatures is done by comparing it with all signatures of the database.\nThe proposed system is tested on a signature database contains 5400 offline\nsignatures of 600 individuals and the results are found to be promising.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 16:36:36 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Gupta", "Phalguni", ""], ["Sing", "Jamuna Kanta", ""]]}, {"id": "1003.5886", "submitter": "Sandip Rakshit", "authors": "Sandip Rakshit, Subhadip Basu", "title": "Development of a multi-user handwriting recognition system using\n  Tesseract open source OCR engine", "comments": "Proc. International Conference on C3IT (2009) 240-247", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the paper is to recognize handwritten samples of lower case\nRoman script using Tesseract open source Optical Character Recognition (OCR)\nengine under Apache License 2.0. Handwritten data samples containing isolated\nand free-flow text were collected from different users. Tesseract is trained\nwith user-specific data samples of both the categories of document pages to\ngenerate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated and free-flow handwritten test samples\ncollected from the designated user. On a three user model, the system is\ntrained with 1844, 1535 and 1113 isolated handwritten character samples\ncollected from three different users and the performance is tested on 1133,\n1186 and 1204 character samples, collected form the test sets of the three\nusers respectively. The user specific character level accuracies were obtained\nas 87.92%, 81.53% and 65.71% respectively. The overall character-level accuracy\nof the system is observed as 78.39%. The system fails to segment 10.96%\ncharacters and erroneously classifies 10.65% characters on the overall dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 18:22:44 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Rakshit", "Sandip", ""], ["Basu", "Subhadip", ""]]}, {"id": "1003.5891", "submitter": "Sandip Rakshit", "authors": "Sandip Rakshit, Subhadip Basu", "title": "Recognition of Handwritten Roman Script Using Tesseract Open source OCR\n  Engine", "comments": "Proc. National Conference on NAQC (2008) 141-145", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work, we have used Tesseract 2.01 open source Optical\nCharacter Recognition (OCR) Engine under Apache License 2.0 for recognition of\nhandwriting samples of lower case Roman script. Handwritten isolated and\nfree-flow text samples were collected from multiple users. Tesseract is trained\nto recognize user-specific handwriting samples of both the categories of\ndocument pages. On a single user model, the system is trained with 1844\nisolated handwritten characters and the performance is tested on 1133\ncharacters, taken form the test set. The overall character-level accuracy of\nthe system is observed as 83.5%. The system fails to segment 5.56% characters\nand erroneously classifies 10.94% characters.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 18:35:37 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Rakshit", "Sandip", ""], ["Basu", "Subhadip", ""]]}, {"id": "1003.5893", "submitter": "Sandip Rakshit", "authors": "Sandip Rakshit, Subhadip Basu, Hisashi Ikeda", "title": "Recognition of Handwritten Textual Annotations using Tesseract Open\n  Source OCR Engine for information Just In Time (iJIT)", "comments": "Proc. Int. Conf. on Information Technology and Business Intelligence\n  (2009) 117-125", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective of the current work is to develop an Optical Character Recognition\n(OCR) engine for information Just In Time (iJIT) system that can be used for\nrecognition of handwritten textual annotations of lower case Roman script.\nTesseract open source OCR engine under Apache License 2.0 is used to develop\nuser-specific handwriting recognition models, viz., the language sets, for the\nsaid system, where each user is identified by a unique identification tag\nassociated with the digital pen. To generate the language set for any user,\nTesseract is trained with labeled handwritten data samples of isolated and\nfree-flow texts of Roman script, collected exclusively from that user. The\ndesigned system is tested on five different language sets with free- flow\nhandwritten annotations as test samples. The system could successfully segment\nand subsequently recognize 87.92%, 81.53%, 92.88%, 86.75% and 90.80%\nhandwritten characters in the test samples of five different users.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 18:48:47 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Rakshit", "Sandip", ""], ["Basu", "Subhadip", ""], ["Ikeda", "Hisashi", ""]]}, {"id": "1003.5897", "submitter": "Sandip Rakshit", "authors": "Sandip Rakshit, Debkumar Ghosal, Tanmoy Das, Subhrajit Dutta, Subhadip\n  Basu", "title": "Development of a Multi-User Recognition Engine for Handwritten Bangla\n  Basic Characters and Digits", "comments": "Proc. (CD) Int. Conf. on Information Technology and Business\n  Intelligence (2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the paper is to recognize handwritten samples of basic\nBangla characters using Tesseract open source Optical Character Recognition\n(OCR) engine under Apache License 2.0. Handwritten data samples containing\nisolated Bangla basic characters and digits were collected from different\nusers. Tesseract is trained with user-specific data samples of document pages\nto generate separate user-models representing a unique language-set. Each such\nlanguage-set recognizes isolated basic Bangla handwritten test samples\ncollected from the designated users. On a three user model, the system is\ntrained with 919, 928 and 648 isolated handwritten character and digit samples\nand the performance is tested on 1527, 14116 and 1279 character and digit\nsamples, collected form the test datasets of the three users respectively. The\nuser specific character/digit recognition accuracies were obtained as 90.66%,\n91.66% and 96.87% respectively. The overall basic character-level and digit\nlevel accuracy of the system is observed as 92.15% and 97.37%. The system fails\nto segment 12.33% characters and 15.96% digits and also erroneously classifies\n7.85% characters and 2.63% on the overall dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 18:54:57 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Rakshit", "Sandip", ""], ["Ghosal", "Debkumar", ""], ["Das", "Tanmoy", ""], ["Dutta", "Subhrajit", ""], ["Basu", "Subhadip", ""]]}, {"id": "1003.5898", "submitter": "Sandip Rakshit", "authors": "Sandip Rakshit, Amitava Kundu, Mrinmoy Maity, Subhajit Mandal, Satwika\n  Sarkar, Subhadip Basu", "title": "Recognition of handwritten Roman Numerals using Tesseract open source\n  OCR engine", "comments": "Proc. Int. Conf. on Advances in Computer Vision and Information\n  Technology (2009) 572-577", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the paper is to recognize handwritten samples of Roman\nnumerals using Tesseract open source Optical Character Recognition (OCR)\nengine. Tesseract is trained with data samples of different persons to generate\none user-independent language model, representing the handwritten Roman\ndigit-set. The system is trained with 1226 digit samples collected form the\ndifferent users. The performance is tested on two different datasets, one\nconsisting of samples collected from the known users (those who prepared the\ntraining data samples) and the other consisting of handwritten data samples of\nunknown users. The overall recognition accuracy is obtained as 92.1% and 86.59%\non these test datasets respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 18:59:49 GMT"}], "update_date": "2010-03-31", "authors_parsed": [["Rakshit", "Sandip", ""], ["Kundu", "Amitava", ""], ["Maity", "Mrinmoy", ""], ["Mandal", "Subhajit", ""], ["Sarkar", "Satwika", ""], ["Basu", "Subhadip", ""]]}, {"id": "1003.6052", "submitter": "Satadal Saha", "authors": "Satadal Saha, Subhadip Basu, Mita Nasipuri and Dipak Kumar Basu", "title": "Development of an automated Red Light Violation Detection System (RLVDS)\n  for Indian vehicles", "comments": "National Conference on Computing and Communication Systems\n  (COCOSYS-09)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated Traffic Management Systems (ITMS) are now implemented in different\ncities in India to primarily address the concerns of road-safety and security.\nAn automated Red Light Violation Detection System (RLVDS) is an integral part\nof the ITMS. In our present work we have designed and developed a complete\nsystem for generating the list of all stop-line violating vehicle images\nautomatically from video snapshots of road-side surveillance cameras. The\nsystem first generates adaptive background images for each camera view,\nsubtracts captured images from the corresponding background images and analyses\npotential occlusions over the stop-line in a traffic signal. Considering\nround-the-clock operations in a real-life test environment, the developed\nsystem could successfully track 92% images of vehicles with violations on the\nstop-line in a \"Red\" traffic signal.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2010 13:44:29 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 21:34:21 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Saha", "Satadal", ""], ["Basu", "Subhadip", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1003.6059", "submitter": "Satadal Saha", "authors": "Satadal Saha, Subhadip Basu, Mita Nasipuri and Dipak Kumar Basu", "title": "A novel scheme for binarization of vehicle images using hierarchical\n  histogram equalization technique", "comments": "International Conference on Computer, Communication, Control and\n  Information Technology (C3IT 2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic License Plate Recognition system is a challenging area of research\nnow-a-days and binarization is an integral and most important part of it. In\ncase of a real life scenario, most of existing methods fail to properly\nbinarize the image of a vehicle in a congested road, captured through a CCD\ncamera. In the current work we have applied histogram equalization technique\nover the complete image and also over different hierarchy of image\npartitioning. A novel scheme is formulated for giving the membership value to\neach pixel for each hierarchy of histogram equalization. Then the image is\nbinarized depending on the net membership value of each pixel. The technique is\nexhaustively evaluated on the vehicle image dataset as well as the license\nplate dataset, giving satisfactory performances.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2010 14:00:16 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 21:26:41 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Saha", "Satadal", ""], ["Basu", "Subhadip", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}]