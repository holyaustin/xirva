[{"id": "1907.00028", "submitter": "Luciano Oliveira", "authors": "Paulo Chagas, Luiz Souza, Ikaro Ara\\'ujo, Nayze Aldeman, Angelo\n  Duarte, Michele Angelo, Washington LC dos-Santos, Luciano Oliveira", "title": "Classification of glomerular hypercellularity using convolutional\n  features and support vector machine", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Glomeruli are histological structures of the kidney cortex formed by\ninterwoven blood capillaries, and are responsible for blood filtration.\nGlomerular lesions impair kidney filtration capability, leading to protein loss\nand metabolic waste retention. An example of lesion is the glomerular\nhypercellularity, which is characterized by an increase in the number of cell\nnuclei in different areas of the glomeruli. Glomerular hypercellularity is a\nfrequent lesion present in different kidney diseases. Automatic detection of\nglomerular hypercellularity would accelerate the screening of scanned\nhistological slides for the lesion, enhancing clinical diagnosis. Having this\nin mind, we propose a new approach for classification of hypercellularity in\nhuman kidney images. Our proposed method introduces a novel architecture of a\nconvolutional neural network (CNN) along with a support vector machine,\nachieving near perfect average results with the FIOCRUZ data set in a binary\nclassification (lesion or normal). Our deep-based classifier outperformed the\nstate-of-the-art results on the same data set. Additionally, classification of\nhypercellularity sub-lesions was also performed, considering mesangial,\nendocapilar and both lesions; in this multi-classification task, our proposed\nmethod just failed in 4\\% of the cases. To the best of our knowledge, this is\nthe first study on deep learning over a data set of glomerular hypercellularity\nimages of human kidney.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 18:29:45 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Chagas", "Paulo", ""], ["Souza", "Luiz", ""], ["Ara\u00fajo", "Ikaro", ""], ["Aldeman", "Nayze", ""], ["Duarte", "Angelo", ""], ["Angelo", "Michele", ""], ["dos-Santos", "Washington LC", ""], ["Oliveira", "Luciano", ""]]}, {"id": "1907.00058", "submitter": "Carlo Biffi", "authors": "Carlo Biffi, Juan J. Cerrolaza, Giacomo Tarroni, Wenjia Bai, Antonio\n  de Marvao, Ozan Oktay, Christian Ledig, Loic Le Folgoc, Konstantinos\n  Kamnitsas, Georgia Doumou, Jinming Duan, Sanjay K. Prasad, Stuart A. Cook,\n  Declan P. O'Regan, and Daniel Rueckert", "title": "Explainable Anatomical Shape Analysis through Deep Hierarchical\n  Generative Models", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging\n  (TMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of anatomical shape changes currently relies on scalar global\nindexes which are largely insensitive to regional or asymmetric modifications.\nAccurate assessment of pathology-driven anatomical remodeling is a crucial step\nfor the diagnosis and treatment of many conditions. Deep learning approaches\nhave recently achieved wide success in the analysis of medical images, but they\nlack interpretability in the feature extraction and decision processes. In this\nwork, we propose a new interpretable deep learning model for shape analysis. In\nparticular, we exploit deep generative networks to model a population of\nanatomical segmentations through a hierarchy of conditional latent variables.\nAt the highest level of this hierarchy, a two-dimensional latent space is\nsimultaneously optimised to discriminate distinct clinical conditions, enabling\nthe direct visualisation of the classification space. Moreover, the anatomical\nvariability encoded by this discriminative latent space can be visualised in\nthe segmentation space thanks to the generative properties of the model, making\nthe classification task transparent. This approach yielded high accuracy in the\ncategorisation of healthy and remodelled left ventricles when tested on unseen\nsegmentations from our own multi-centre dataset as well as in an external\nvalidation set, and on hippocampi from healthy controls and patients with\nAlzheimer's disease when tested on ADNI data. More importantly, it enabled the\nvisualisation in three-dimensions of both global and regional anatomical\nfeatures which better discriminate between the conditions under exam. The\nproposed approach scales effectively to large populations, facilitating\nhigh-throughput analysis of normal anatomy and pathology in large-scale studies\nof volumetric imaging.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 19:58:08 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 13:54:23 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Biffi", "Carlo", ""], ["Cerrolaza", "Juan J.", ""], ["Tarroni", "Giacomo", ""], ["Bai", "Wenjia", ""], ["de Marvao", "Antonio", ""], ["Oktay", "Ozan", ""], ["Ledig", "Christian", ""], ["Folgoc", "Loic Le", ""], ["Kamnitsas", "Konstantinos", ""], ["Doumou", "Georgia", ""], ["Duan", "Jinming", ""], ["Prasad", "Sanjay K.", ""], ["Cook", "Stuart A.", ""], ["O'Regan", "Declan P.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1907.00068", "submitter": "Dongyang Kuang", "authors": "Dongyang Kuang", "title": "On Reducing Negative Jacobian Determinant of the Deformation Predicted\n  by Deep Registration Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is a fundamental step in medical image analysis. Ideally,\nthe transformation that registers one image to another should be a\ndiffeomorphism that is both invertible and smooth. Traditional methods like\ngeodesic shooting approach the problem via differential geometry, with\ntheoretical guarantees that the resulting transformation will be smooth and\ninvertible. Most previous research using unsupervised deep neural networks for\nregistration have used a local smoothness constraint (typically, a spatial\nvariation loss) to address the smoothness issue. These networks usually produce\nnon-invertible transformations with ``folding'' in multiple voxel locations,\nindicated by a negative determinant of the Jacobian matrix of the\ntransformation. While using a loss function that specifically penalizes the\nfolding is a straightforward solution, this usually requires carefully tuning\nthe regularization strength, especially when there are also other losses. In\nthis paper we address this problem from a different angle, by investigating\npossible training mechanisms that will help the network avoid negative\nJacobians and produce smoother deformations. We contribute two independent\nideas in this direction. Both ideas greatly reduce the number of folding\nlocations in the predicted deformation, without making changes to the\nhyperparameters or the architecture used in the existing baseline registration\nnetwork.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 20:42:12 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Kuang", "Dongyang", ""]]}, {"id": "1907.00069", "submitter": "Dongyang Kuang", "authors": "Dongyang Kuang", "title": "A 1d convolutional network for leaf and time series classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a 1d convolutional neural network is designed for\nclassification tasks of plant leaves. This network based classifier is analyzed\nin two directions. In the forward direction, the proposed network can be used\nin two ways: a classifier and an automatic feature extractor. As a classifier,\nit takes the simple centroid contour distance curve as the single feature and\nachieves comparable performance with state-of-art methods that usually require\nmultiple extracted features. As a feature extractor, it produces nearly linear\nseparable features, hence can be used together with other classifiers such as\nsupport vector machines to provide better performance. The proposed network\nadopts simple 1d input and is generally applicable for other tasks such as\nclassifying one dimensional time series in an end-to-end fashion without\nchanges. Experiments on some benchmark datasets show this architecture can\nprovide classification accuracies that are comparable or higher than some\nexisting methods. In the backward direction, methods like gradient-weighted\nclass activation mapping and maximum activation map of neurons in the\nclassification layer with respect to inputs are performed to help investigate\nand further validate that hidden signatures helping trigger the trained\nclassifier's specific decisions can be human interpretable. Code for the paper\nis available at https://github.com/dykuang/Leaf_Project.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 20:44:16 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 03:06:49 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Kuang", "Dongyang", ""]]}, {"id": "1907.00098", "submitter": "Min Wu", "authors": "Min Wu and Marta Kwiatkowska", "title": "Robustness Guarantees for Deep Neural Networks on Videos", "comments": "To appear in 2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread adoption of deep learning models places demands on their\nrobustness. In this paper, we consider the robustness of deep neural networks\non videos, which comprise both the spatial features of individual frames\nextracted by a convolutional neural network and the temporal dynamics between\nadjacent frames captured by a recurrent neural network. To measure robustness,\nwe study the maximum safe radius problem, which computes the minimum distance\nfrom the optical flow sequence obtained from a given input to that of an\nadversarial example in the neighbourhood of the input. We demonstrate that,\nunder the assumption of Lipschitz continuity, the problem can be approximated\nusing finite optimisation via discretising the optical flow space, and the\napproximation has provable guarantees. We then show that the finite\noptimisation problem can be solved by utilising a two-player turn-based game in\na cooperative setting, where the first player selects the optical flows and the\nsecond player determines the dimensions to be manipulated in the chosen flow.\nWe employ an anytime approach to solve the game, in the sense of approximating\nthe value of the game by monotonically improving its upper and lower bounds. We\nexploit a gradient-based search algorithm to compute the upper bounds, and the\nadmissible A* algorithm to update the lower bounds. Finally, we evaluate our\nframework on the UCF101 video dataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 22:27:17 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 16:53:11 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 18:07:06 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wu", "Min", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "1907.00135", "submitter": "Liuyuan Deng", "authors": "Liuyuan Deng, Ming Yang, Tianyi Li, Yuesheng He, and Chunxiang Wang", "title": "RFBNet: Deep Multimodal Networks with Residual Fusion Blocks for RGB-D\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-D semantic segmentation methods conventionally use two independent\nencoders to extract features from the RGB and depth data. However, there lacks\nan effective fusion mechanism to bridge the encoders, for the purpose of fully\nexploiting the complementary information from multiple modalities. This paper\nproposes a novel bottom-up interactive fusion structure to model the\ninterdependencies between the encoders. The structure introduces an interaction\nstream to interconnect the encoders. The interaction stream not only\nprogressively aggregates modality-specific features from the encoders but also\ncomputes complementary features for them. To instantiate this structure, the\npaper proposes a residual fusion block (RFB) to formulate the interdependences\nof the encoders. The RFB consists of two residual units and one fusion unit\nwith gate mechanism. It learns complementary features for the modality-specific\nencoders and extracts modality-specific features as well as cross-modal\nfeatures. Based on the RFB, the paper presents the deep multimodal networks for\nRGB-D semantic segmentation called RFBNet. The experiments on two datasets\ndemonstrate the effectiveness of modeling the interdependencies and that the\nRFBNet achieved state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 02:51:29 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 13:03:50 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Deng", "Liuyuan", ""], ["Yang", "Ming", ""], ["Li", "Tianyi", ""], ["He", "Yuesheng", ""], ["Wang", "Chunxiang", ""]]}, {"id": "1907.00148", "submitter": "Amir Bar", "authors": "Amir Bar, Michal Mauda, Yoni Turner, Michal Safadi and Eldad Elnekave", "title": "Improved ICH classification using task-dependent learning", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head CT is one of the most commonly performed imaging studied in the\nEmergency Department setting and Intracranial hemorrhage (ICH) is among the\nmost critical and timesensitive findings to be detected on Head CT. We present\nBloodNet, a deep learning architecture designed for optimal triaging of Head\nCTs, with the goal of decreasing the time from CT acquisition to accurate ICH\ndetection. The BloodNet architecture incorporates dependency between the\notherwise independent tasks of segmentation and classification, achieving\nimproved classification results. AUCs of 0.9493 and 0.9566 are reported on held\nout positive-enriched and randomly sampled sets comprised of over 1400 studies\nacquired from over 10 different hospitals. These results are comparable to\npreviously reported results with smaller number of tagged studies.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 05:26:24 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Bar", "Amir", ""], ["Mauda", "Michal", ""], ["Turner", "Yoni", ""], ["Safadi", "Michal", ""], ["Elnekave", "Eldad", ""]]}, {"id": "1907.00157", "submitter": "Sandeep Singh Adhikari Mr", "authors": "Sandeep Singh Adhikari, Sukhneer Singh, Anoop Rajagopal, Aruna Rajan", "title": "Progressive Fashion Attribute Extraction", "comments": "6 pages, 6 figures, AI for fashion : KDD 2019 Workshop, August 2019,\n  Anchorage, Alaska - USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting fashion attributes from images of people wearing clothing/fashion\naccessories is a very hard multi-class classification problem. Most often, even\ncatalogues of fashion do not have all the fine-grained attributes tagged due to\nprohibitive cost of annotation. Using images of fashion articles, running\nmulti-class attribute extraction with a single model for all kinds of\nattributes (neck design detailing, sleeves detailing, etc) requires classifiers\nthat are robust to missing and ambiguously labelled data. In this work, we\npropose a progressive training approach for such multi-class classification,\nwhere weights learnt from an attribute are fine tuned for another attribute of\nthe same fashion article (say, dresses). We branch networks for each attributes\nfrom a base network progressively during training. While it may have many\nlabels, an image doesn't need to have all possible labels for fashion articles\npresent in it. We also compare our approach to multi-label classification, and\ndemonstrate improvements over overall classification accuracies using our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 06:51:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Adhikari", "Sandeep Singh", ""], ["Singh", "Sukhneer", ""], ["Rajagopal", "Anoop", ""], ["Rajan", "Aruna", ""]]}, {"id": "1907.00193", "submitter": "Debin Meng", "authors": "Debin Meng, Xiaojiang Peng, Kai Wang, Yu Qiao", "title": "Frame attention networks for facial expression recognition in videos", "comments": "Accepted by ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video-based facial expression recognition aims to classify a given video\ninto several basic emotions. How to integrate facial features of individual\nframes is crucial for this task. In this paper, we propose the Frame Attention\nNetworks (FAN), to automatically highlight some discriminative frames in an\nend-to-end framework. The network takes a video with a variable number of face\nimages as its input and produces a fixed-dimension representation. The whole\nnetwork is composed of two modules. The feature embedding module is a deep\nConvolutional Neural Network (CNN) which embeds face images into feature\nvectors. The frame attention module learns multiple attention weights which are\nused to adaptively aggregate the feature vectors to form a single\ndiscriminative video representation. We conduct extensive experiments on CK+\nand AFEW8.0 datasets. Our proposed FAN shows superior performance compared to\nother CNN based methods and achieves state-of-the-art performance on CK+.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 12:11:44 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 07:21:44 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Meng", "Debin", ""], ["Peng", "Xiaojiang", ""], ["Wang", "Kai", ""], ["Qiao", "Yu", ""]]}, {"id": "1907.00209", "submitter": "Xiaoyu Chen", "authors": "XiaoYu Chen, Xu Wang, Lianfa Bai, Jing Han and Zhuang Zhao", "title": "High Sensitivity Snapshot Spectrometer Based on Deep Network Unmixing", "comments": "16 pages, 13 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a convolution neural network based method to\nrecover the light intensity distribution from the overlapped dispersive spectra\ninstead of adding an extra light path to capture it directly for the first\ntime. Then, we construct a single-path sub-Hadamard snapshot spectrometer based\non our previous dual-path snapshot spectrometer. In the proposed single-path\nspectrometer, we use the reconstructed light intensity as the original light\nintensity and recover high signal-to-noise ratio spectra successfully. Compared\nwith dual-path snapshot spectrometer, the network based single-path\nspectrometer has a more compact structure and maintains snapshot and high\nsensitivity. Abundant simulated and experimental results have demonstrated that\nthe proposed method can obtain a better reconstructed signal-to-noise ratio\nspectrum than the dual-path sub-Hadamard spectrometer because of its higher\nlight throughput.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 14:20:05 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Chen", "XiaoYu", ""], ["Wang", "Xu", ""], ["Bai", "Lianfa", ""], ["Han", "Jing", ""], ["Zhao", "Zhuang", ""]]}, {"id": "1907.00211", "submitter": "Zheng Wang", "authors": "Feiping Nie, Hua Wang, Zheng Wang, Heng Huang", "title": "Robust Linear Discriminant Analysis Using Ratio Minimization of\n  L1,2-Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most popular linear subspace learning methods, the Linear\nDiscriminant Analysis (LDA) method has been widely studied in machine learning\ncommunity and applied to many scientific applications. Traditional LDA\nminimizes the ratio of squared L2-norms, which is sensitive to outliers. In\nrecent research, many L1-norm based robust Principle Component Analysis methods\nwere proposed to improve the robustness to outliers. However, due to the\ndifficulty of L1-norm ratio optimization, so far there is no existing work to\nutilize sparsity-inducing norms for LDA objective. In this paper, we propose a\nnovel robust linear discriminant analysis method based on the L1,2-norm ratio\nminimization. Minimizing the L1,2-norm ratio is a much more challenging problem\nthan the traditional methods, and there is no existing optimization algorithm\nto solve such non-smooth terms ratio problem. We derive a new efficient\nalgorithm to solve this challenging problem, and provide a theoretical analysis\non the convergence of our algorithm. The proposed algorithm is easy to\nimplement, and converges fast in practice. Extensive experiments on both\nsynthetic data and nine real benchmark data sets show the effectiveness of the\nproposed robust LDA method.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 14:39:27 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Nie", "Feiping", ""], ["Wang", "Hua", ""], ["Wang", "Zheng", ""], ["Huang", "Heng", ""]]}, {"id": "1907.00214", "submitter": "Mobarakol Islam", "authors": "Mobarakol Islam, Yueyuan Li, Hongliang Ren", "title": "Learning Where to Look While Tracking Instruments in Robot-assisted\n  Surgery", "comments": "This work is accepted in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directing of the task-specific attention while tracking instrument in surgery\nholds great potential in robot-assisted intervention. For this purpose, we\npropose an end-to-end trainable multitask learning (MTL) model for real-time\nsurgical instrument segmentation and attention prediction. Our model is\ndesigned with a weight-shared encoder and two task-oriented decoders and\noptimized for the joint tasks. We introduce batch-Wasserstein (bW) loss and\nconstruct a soft attention module to refine the distinctive visual region for\nefficient saliency learning. For multitask optimization, it is always\nchallenging to obtain convergence of both tasks in the same epoch. We deal with\nthis problem by adopting `poly' loss weight and two phases of training. We\nfurther propose a novel way to generate task-aware saliency map and scanpath of\nthe instruments on MICCAI robotic instrument segmentation dataset. Compared to\nthe state of the art segmentation and saliency models, our model outperforms\nmost of the evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 14:56:09 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Islam", "Mobarakol", ""], ["Li", "Yueyuan", ""], ["Ren", "Hongliang", ""]]}, {"id": "1907.00215", "submitter": "Xiaoyu Chen", "authors": "Xiaoyu Chen, Qixin Wang, Jinzhou Ge, Yi Zhang and Jing Han", "title": "Non-destructive three-dimensional measurement of hand vein based on\n  self-supervised network", "comments": "10 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, supervised stereo methods based on deep neural network have\nachieved impressive results. However, in some scenarios, accurate\nthree-dimensional labels are inaccessible for supervised training. In this\npaper, a self-supervised network is proposed for binocular disparity matching\n(SDMNet), which computes dense disparity maps from stereo image pairs without\ndisparity labels: In the self-supervised training, we match the stereo images\ndensely to approximate the disparity maps and use them to warp the left and\nright images to estimate the right and left images; we build the loss function\nbetween estimated images and original images for self-supervised training,\nwhich adopts perceptual loss to help improve the quality of disparity maps in\nboth detail and structure. Then, we use SDMNet to obtain disparities of hand\nvein. SDMNet has achieved excellent results on KITTI 2012, KITTI 2015,\nsimulated vein dataset and real vein dataset, outperforming many\nstate-of-the-art supervised matching methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 15:01:16 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Chen", "Xiaoyu", ""], ["Wang", "Qixin", ""], ["Ge", "Jinzhou", ""], ["Zhang", "Yi", ""], ["Han", "Jing", ""]]}, {"id": "1907.00217", "submitter": "Uwe Messer", "authors": "U. Messer and S. Fausser", "title": "Predicting Social Perception from Faces: A Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Warmth and competence represent the fundamental traits in social judgment\nthat determine emotional reactions and behavioral intentions towards social\ntargets. This research investigates whether an algorithm can learn visual\nrepresentations of social categorization and accurately predict human\nperceivers' impressions of warmth and competence in face images. In addition,\nthis research unravels which areas of a face are important for the\nclassification of warmth and competence. We use Deep Convolutional Neural\nNetworks to extract features from face images and the Gradient-weighted Class\nActivation Mapping (Grad CAM) method to understand the importance of face\nregions for the classification. Given a single face image the trained algorithm\ncould correctly predict warmth impressions with an accuracy of about 90% and\ncompetence impressions with an accuracy of about 80%. The findings have\nimplications for the automated processing of faces and the design of artificial\ncharacters.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 15:05:51 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Messer", "U.", ""], ["Fausser", "S.", ""]]}, {"id": "1907.00233", "submitter": "Jiaqi Yang", "authors": "Jiaqi Yang and Siwen Quan and Peng Wang and Yanning Zhang", "title": "Evaluating Local Geometric Feature Representations for 3D Rigid Data\n  Matching", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2959236", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local geometric descriptors remain an essential component for 3D rigid data\nmatching and fusion. The devise of a rotational invariant local geometric\ndescriptor usually consists of two steps: local reference frame (LRF)\nconstruction and feature representation. Existing evaluation efforts have\nmainly been paid on the LRF or the overall descriptor, yet the quantitative\ncomparison of feature representations remains unexplored. This paper fills this\ngap by comprehensively evaluating nine state-of-the-art local geometric feature\nrepresentations. Our evaluation is on the ground that ground-truth LRFs are\nleveraged such that the ranking of tested feature representations are more\nconvincing as opposed to existing studies. The experiments are deployed on six\nstandard datasets with various application scenarios (shape retrieval, point\ncloud registration, and object recognition) and data modalities (LiDAR, Kinect,\nand Space Time) as well as perturbations including Gaussian noise, shot noise,\ndata decimation, clutter, occlusion, and limited overlap. The evaluated terms\ncover the major concerns for a feature representation, e.g., distinctiveness,\nrobustness, compactness, and efficiency. The outcomes present interesting\nfindings that may shed new light on this community and provide complementary\nperspectives to existing evaluations on the topic of local geometric feature\ndescription. A summary of evaluated methods regarding their peculiarities is\nalso presented to guide real-world applications and new descriptor crafting.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 16:26:40 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Yang", "Jiaqi", ""], ["Quan", "Siwen", ""], ["Wang", "Peng", ""], ["Zhang", "Yanning", ""]]}, {"id": "1907.00262", "submitter": "Jonathan Frankle", "authors": "Jonathan Frankle and David Bau", "title": "Dissecting Pruned Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning is a standard technique for removing unnecessary structure from a\nneural network to reduce its storage footprint, computational demands, or\nenergy consumption. Pruning can reduce the parameter-counts of many\nstate-of-the-art neural networks by an order of magnitude without compromising\naccuracy, meaning these networks contain a vast amount of unnecessary\nstructure. In this paper, we study the relationship between pruning and\ninterpretability. Namely, we consider the effect of removing unnecessary\nstructure on the number of hidden units that learn disentangled representations\nof human-recognizable concepts as identified by network dissection. We aim to\nevaluate how the interpretability of pruned neural networks changes as they are\ncompressed. We find that pruning has no detrimental effect on this measure of\ninterpretability until so few parameters remain that accuracy beings to drop.\nResnet-50 models trained on ImageNet maintain the same number of interpretable\nconcepts and units until more than 90% of parameters have been pruned.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 19:27:57 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Frankle", "Jonathan", ""], ["Bau", "David", ""]]}, {"id": "1907.00267", "submitter": "Dawei Yang", "authors": "Dawei Yang, Jia Deng", "title": "Learning to Generate Synthetic 3D Training Data through Hybrid Gradient", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic images rendered by graphics engines are a promising source for\ntraining deep networks. However, it is challenging to ensure that they can help\ntrain a network to perform well on real images, because a graphics-based\ngeneration pipeline requires numerous design decisions such as the selection of\n3D shapes and the placement of the camera. In this work, we propose a new\nmethod that optimizes the generation of 3D training data based on what we call\n\"hybrid gradient\". We parametrize the design decisions as a real vector, and\ncombine the approximate gradient and the analytical gradient to obtain the\nhybrid gradient of the network performance with respect to this vector. We\nevaluate our approach on the task of estimating surface normal, depth or\nintrinsic decomposition from a single image. Experiments on standard benchmarks\nshow that our approach can outperform the prior state of the art on optimizing\nthe generation of 3D training data, particularly in terms of computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 19:34:19 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 18:42:03 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yang", "Dawei", ""], ["Deng", "Jia", ""]]}, {"id": "1907.00273", "submitter": "Wei-An Lin", "authors": "Wei-An Lin, Haofu Liao, Cheng Peng, Xiaohang Sun, Jingdan Zhang, Jiebo\n  Luo, Rama Chellappa, Shaohua Kevin Zhou", "title": "DuDoNet: Dual Domain Network for CT Metal Artifact Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computed tomography (CT) is an imaging modality widely used for medical\ndiagnosis and treatment. CT images are often corrupted by undesirable artifacts\nwhen metallic implants are carried by patients, which creates the problem of\nmetal artifact reduction (MAR). Existing methods for reducing the artifacts due\nto metallic implants are inadequate for two main reasons. First, metal\nartifacts are structured and non-local so that simple image domain enhancement\napproaches would not suffice. Second, the MAR approaches which attempt to\nreduce metal artifacts in the X-ray projection (sinogram) domain inevitably\nlead to severe secondary artifact due to sinogram inconsistency. To overcome\nthese difficulties, we propose an end-to-end trainable Dual Domain Network\n(DuDoNet) to simultaneously restore sinogram consistency and enhance CT images.\nThe linkage between the sigogram and image domains is a novel Radon inversion\nlayer that allows the gradients to back-propagate from the image domain to the\nsinogram domain during training. Extensive experiments show that our method\nachieves significant improvements over other single domain MAR approaches. To\nthe best of our knowledge, it is the first end-to-end dual-domain network for\nMAR.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 20:23:01 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lin", "Wei-An", ""], ["Liao", "Haofu", ""], ["Peng", "Cheng", ""], ["Sun", "Xiaohang", ""], ["Zhang", "Jingdan", ""], ["Luo", "Jiebo", ""], ["Chellappa", "Rama", ""], ["Zhou", "Shaohua Kevin", ""]]}, {"id": "1907.00274", "submitter": "Pedro Morgado", "authors": "Pedro Morgado and Nuno Vasconcelos", "title": "NetTailor: Tuning the Architecture, Not Just the Weights", "comments": null, "journal-ref": "CVF/IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world applications of object recognition often require the solution of\nmultiple tasks in a single platform. Under the standard paradigm of network\nfine-tuning, an entirely new CNN is learned per task, and the final network\nsize is independent of task complexity. This is wasteful, since simple tasks\nrequire smaller networks than more complex tasks, and limits the number of\ntasks that can be solved simultaneously. To address these problems, we propose\na transfer learning procedure, denoted NetTailor, in which layers of a\npre-trained CNN are used as universal blocks that can be combined with small\ntask-specific layers to generate new networks. Besides minimizing\nclassification error, the new network is trained to mimic the internal\nactivations of a strong unconstrained CNN, and minimize its complexity by the\ncombination of 1) a soft-attention mechanism over blocks and 2) complexity\nregularization constraints. In this way, NetTailor can adapt the network\narchitecture, not just its weights, to the target task. Experiments show that\nnetworks adapted to simple tasks, such as character or traffic sign\nrecognition, become significantly smaller than those adapted to hard tasks,\nsuch as fine-grained recognition. More importantly, due to the modular nature\nof the procedure, this reduction in network complexity is achieved without\ncompromise of either parameter sharing across tasks, or classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 20:32:58 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Morgado", "Pedro", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1907.00276", "submitter": "Alexander Vakhitov", "authors": "Alexander Vakhitov, Victor Lempitsky, and Yinqiang Zheng", "title": "Stereo relative pose from line and point feature triplets", "comments": "European Conference on Computer Vision 2018. Project page:\n  https://alexandervakhitov.github.io/sego/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo relative pose problem lies at the core of stereo visual odometry\nsystems that are used in many applications. In this work, we present two\nminimal solvers for the stereo relative pose. We specifically consider the case\nwhen a minimal set consists of three point or line features and each of them\nhas three known projections on two stereo cameras. We validate the importance\nof this formulation for practical purposes in our experiments with motion\nestimation. We then present a complete classification of minimal cases with\nthree point or line correspondences each having three projections, and present\ntwo new solvers that can handle all such cases. We demonstrate a considerable\neffect from the integration of the new solvers into a visual SLAM system.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 20:34:35 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Vakhitov", "Alexander", ""], ["Lempitsky", "Victor", ""], ["Zheng", "Yinqiang", ""]]}, {"id": "1907.00281", "submitter": "Po-Yu Kao", "authors": "Po-Yu Kao, Jefferson W. Chen, B.S. Manjunath", "title": "Improving 3D U-Net for Brain Tumor Segmentation by Utilizing Lesion\n  Prior", "comments": "5 pages, 4 figures, 1 table, LNCS format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel, simple and effective method to integrate lesion prior and\na 3D U-Net for improving brain tumor segmentation. First, we utilize the\nground-truth brain tumor lesions from a group of patients to generate the\nheatmaps of different types of lesions. These heatmaps are used to create the\nvolume-of-interest (VOI) map which contains prior information about brain tumor\nlesions. The VOI map is then integrated with the multimodal MR images and input\nto a 3D U-Net for segmentation. The proposed method is evaluated on a public\nbenchmark dataset, and the experimental results show that the proposed feature\nfusion method achieves an improvement over the baseline methods. In addition,\nour proposed method also achieves a competitive performance compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 21:29:21 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 23:24:01 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 02:28:06 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Kao", "Po-Yu", ""], ["Chen", "Jefferson W.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1907.00283", "submitter": "Richard Chen", "authors": "Richard J. Chen, Taylor L. Bobrow, Thomas Athey, Faisal Mahmood,\n  Nicholas J. Durr", "title": "SLAM Endoscopy enhanced by adversarial depth prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": "KDD'19 Workshop on Applied Data Science for Healthcare", "categories": "eess.IV cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical endoscopy remains a challenging application for simultaneous\nlocalization and mapping (SLAM) due to the sparsity of image features and size\nconstraints that prevent direct depth-sensing. We present a SLAM approach that\nincorporates depth predictions made by an adversarially-trained convolutional\nneural network (CNN) applied to monocular endoscopy images. The depth network\nis trained with synthetic images of a simple colon model, and then fine-tuned\nwith domain-randomized, photorealistic images rendered from computed tomography\nmeasurements of human colons. Each image is paired with an error-free depth map\nfor supervised adversarial learning. Monocular RGB images are then fused with\ncorresponding depth predictions, enabling dense reconstruction and mosaicing as\nan endoscope is advanced through the gastrointestinal tract. Our preliminary\nresults demonstrate that incorporating monocular depth estimation into a SLAM\narchitecture can enable dense reconstruction of endoscopic scenes.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 21:42:42 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Chen", "Richard J.", ""], ["Bobrow", "Taylor L.", ""], ["Athey", "Thomas", ""], ["Mahmood", "Faisal", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "1907.00294", "submitter": "Haofu Liao", "authors": "Haofu Liao, Wei-An Lin, Zhimin Huo, Levon Vogelsang, William J.\n  Sehnert, S. Kevin Zhou, Jiebo Luo", "title": "Generative Mask Pyramid Network for CT/CBCT Metal Artifact Reduction\n  with Joint Projection-Sinogram Correction", "comments": "This paper is accepted to MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A conventional approach to computed tomography (CT) or cone beam CT (CBCT)\nmetal artifact reduction is to replace the X-ray projection data within the\nmetal trace with synthesized data. However, existing projection or sinogram\ncompletion methods cannot always produce anatomically consistent information to\nfill the metal trace, and thus, when the metallic implant is large, significant\nsecondary artifacts are often introduced. In this work, we propose to replace\nmetal artifact affected regions with anatomically consistent content through\njoint projection-sinogram correction as well as adversarial learning. To handle\nthe metallic implants of diverse shapes and large sizes, we also propose a\nnovel mask pyramid network that enforces the mask information across the\nnetwork's encoding layers and a mask fusion loss that reduces early saturation\nof adversarial training. Our experimental results show that the proposed\nprojection-sinogram correction designs are effective and our method recovers\ninformation from the metal traces better than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 23:30:10 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 02:06:04 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 01:38:38 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Liao", "Haofu", ""], ["Lin", "Wei-An", ""], ["Huo", "Zhimin", ""], ["Vogelsang", "Levon", ""], ["Sehnert", "William J.", ""], ["Zhou", "S. Kevin", ""], ["Luo", "Jiebo", ""]]}, {"id": "1907.00318", "submitter": "Athanasios Vlontzos", "authors": "Athanasios Vlontzos, Amir Alansary, Konstantinos Kamnitsas, Daniel\n  Rueckert, Bernhard Kainz", "title": "Multiple Landmark Detection using Multi-Agent Reinforcement Learning", "comments": "Accepted in MICCAI 2019, Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of anatomical landmarks is a vital step for medical image\nanalysis and applications for diagnosis, interpretation and guidance. Manual\nannotation of landmarks is a tedious process that requires domain-specific\nexpertise and introduces inter-observer variability. This paper proposes a new\ndetection approach for multiple landmarks based on multi-agent reinforcement\nlearning. Our hypothesis is that the position of all anatomical landmarks is\ninterdependent and non-random within the human anatomy, thus finding one\nlandmark can help to deduce the location of others. Using a Deep Q-Network\n(DQN) architecture we construct an environment and agent with implicit\ninter-communication such that we can accommodate K agents acting and learning\nsimultaneously, while they attempt to detect K different landmarks. During\ntraining the agents collaborate by sharing their accumulated knowledge for a\ncollective gain. We compare our approach with state-of-the-art architectures\nand achieve significantly better accuracy by reducing the detection error by\n50%, while requiring fewer computational resources and time to train compared\nto the naive approach of training K agents separately.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 05:14:23 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 22:00:02 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Vlontzos", "Athanasios", ""], ["Alansary", "Amir", ""], ["Kamnitsas", "Konstantinos", ""], ["Rueckert", "Daniel", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1907.00327", "submitter": "Niranjan Balachandar", "authors": "Niranjan Balachandar, Justin Dieter, Govardana Sachithanandam\n  Ramachandran", "title": "Collaboration of AI Agents via Cooperative Multi-Agent Deep\n  Reinforcement Learning", "comments": "9 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many AI tasks involving multiple interacting agents where agents\nshould learn to cooperate and collaborate to effectively perform the task. Here\nwe develop and evaluate various multi-agent protocols to train agents to\ncollaborate with teammates in grid soccer. We train and evaluate our\nmulti-agent methods against a team operating with a smart hand-coded policy. As\na baseline, we train agents concurrently and independently, with no\ncommunication. Our collaborative protocols were parameter sharing, coordinated\nlearning with communication, and counterfactual policy gradients. Against the\nhand-coded team, the team trained with parameter sharing and the team trained\nwith coordinated learning performed the best, scoring on 89.5% and 94.5% of\nepisodes respectively when playing against the hand-coded team. Against the\nparameter sharing team, with adversarial training the coordinated learning team\nscored on 75% of the episodes, indicating it is the most adaptable of our\nmethods. The insights gained from our work can be applied to other domains\nwhere multi-agent collaboration could be beneficial.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 06:12:48 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Balachandar", "Niranjan", ""], ["Dieter", "Justin", ""], ["Ramachandran", "Govardana Sachithanandam", ""]]}, {"id": "1907.00330", "submitter": "Xinsheng Wang", "authors": "Xinsheng Wang, Shanmin Pang, Jihua Zhu, Zhongyu Li, Zhiqiang Tian, and\n  Yaochen Li", "title": "Visual Space Optimization for Zero-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning, which aims to recognize new categories that are not\nincluded in the training set, has gained popularity owing to its potential\nability in the real-word applications. Zero-shot learning models rely on\nlearning an embedding space, where both semantic descriptions of classes and\nvisual features of instances can be embedded for nearest neighbor search.\nRecently, most of the existing works consider the visual space formulated by\ndeep visual features as an ideal choice of the embedding space. However, the\ndiscrete distribution of instances in the visual space makes the data structure\nunremarkable. We argue that optimizing the visual space is crucial as it allows\nsemantic vectors to be embedded into the visual space more effectively. In this\nwork, we propose two strategies to accomplish this purpose. One is the visual\nprototype based method, which learns a visual prototype for each visual class,\nso that, in the visual space, a class can be represented by a prototype feature\ninstead of a series of discrete visual features. The other is to optimize the\nvisual feature structure in an intermediate embedding space, and in this method\nwe successfully devise a multilayer perceptron framework based algorithm that\nis able to learn the common intermediate embedding space and meanwhile to make\nthe visual data structure more distinctive. Through extensive experimental\nevaluation on four benchmark datasets, we demonstrate that optimizing visual\nspace is beneficial for zero-shot learning. Besides, the proposed prototype\nbased method achieves the new state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 06:44:24 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wang", "Xinsheng", ""], ["Pang", "Shanmin", ""], ["Zhu", "Jihua", ""], ["Li", "Zhongyu", ""], ["Tian", "Zhiqiang", ""], ["Li", "Yaochen", ""]]}, {"id": "1907.00338", "submitter": "Simon Lynen", "authors": "Simon Lynen, Bernhard Zeisl, Dror Aiger, Michael Bosse, Joel Hesch,\n  Marc Pollefeys, Roland Siegwart, Torsten Sattler", "title": "Large-scale, real-time visual-inertial localization revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overarching goals in image-based localization are scale, robustness and\nspeed. In recent years, approaches based on local features and sparse 3D\npoint-cloud models have both dominated the benchmarks and seen successful\nrealworld deployment. They enable applications ranging from robot navigation,\nautonomous driving, virtual and augmented reality to device geo-localization.\nRecently end-to-end learned localization approaches have been proposed which\nshow promising results on small scale datasets. However the positioning\naccuracy, scalability, latency and compute & storage requirements of these\napproaches remain open challenges. We aim to deploy localization at\nglobal-scale where one thus relies on methods using local features and sparse\n3D models. Our approach spans from offline model building to real-time\nclient-side pose fusion. The system compresses appearance and geometry of the\nscene for efficient model storage and lookup leading to scalability beyond what\nwhat has been previously demonstrated. It allows for low-latency localization\nqueries and efficient fusion run in real-time on mobile platforms by combining\nserver-side localization with real-time visual-inertial-based camera pose\ntracking. In order to further improve efficiency we leverage a combination of\npriors, nearest neighbor search, geometric match culling and a cascaded pose\ncandidate refinement step. This combination outperforms previous approaches\nwhen working with large scale models and allows deployment at unprecedented\nscale. We demonstrate the effectiveness of our approach on a proof-of-concept\nsystem localizing 2.5 million images against models from four cities in\ndifferent regions on the world achieving query latencies in the 200ms range.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 08:45:58 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lynen", "Simon", ""], ["Zeisl", "Bernhard", ""], ["Aiger", "Dror", ""], ["Bosse", "Michael", ""], ["Hesch", "Joel", ""], ["Pollefeys", "Marc", ""], ["Siegwart", "Roland", ""], ["Sattler", "Torsten", ""]]}, {"id": "1907.00348", "submitter": "Wei Shen", "authors": "Wei Shen, Fei Li, Rujie Liu", "title": "Learning to Find Correlated Features by Maximizing Information Flow in\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training convolutional neural networks for image classification tasks usually\ncauses information loss. Although most of the time the information lost is\nredundant with respect to the target task, there are still cases where\ndiscriminative information is also discarded. For example, if the samples that\nbelong to the same category have multiple correlated features, the model may\nonly learn a subset of the features and ignore the rest. This may not be a\nproblem unless the classification in the test set highly depends on the ignored\nfeatures. We argue that the discard of the correlated discriminative\ninformation is partially caused by the fact that the minimization of the\nclassification loss doesn't ensure to learn the overall discriminative\ninformation but only the most discriminative information. To address this\nproblem, we propose an information flow maximization (IFM) loss as a\nregularization term to find the discriminative correlated features. With less\ninformation loss the classifier can make predictions based on more informative\nfeatures. We validate our method on the shiftedMNIST dataset and show the\neffectiveness of IFM loss in learning representative and discriminative\nfeatures.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 09:58:18 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Shen", "Wei", ""], ["Li", "Fei", ""], ["Liu", "Rujie", ""]]}, {"id": "1907.00350", "submitter": "Rakesh Katuwal Mr.", "authors": "Rakesh Katuwal, P.N. Suganthan and M. Tanveer", "title": "Random Vector Functional Link Neural Network based Ensemble Deep\n  Learning", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep learning framework based on randomized\nneural network. In particular, inspired by the principles of Random Vector\nFunctional Link (RVFL) network, we present a deep RVFL network (dRVFL) with\nstacked layers. The parameters of the hidden layers of the dRVFL are randomly\ngenerated within a suitable range and kept fixed while the output weights are\ncomputed using the closed form solution as in a standard RVFL network. We also\npropose an ensemble deep network (edRVFL) that can be regarded as a marriage of\nensemble learning with deep learning. Unlike traditional ensembling approaches\nthat require training several models independently from scratch, edRVFL is\nobtained by training a single dRVFL network once. Both dRVFL and edRVFL\nframeworks are generic and can be used with any RVFL variant. To illustrate\nthis, we integrate the deep learning networks with a recently proposed\nsparse-pretrained RVFL (SP-RVFL). Extensive experiments on benchmark datasets\nfrom diverse domains show the superior performance of our proposed deep RVFL\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 10:05:22 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Katuwal", "Rakesh", ""], ["Suganthan", "P. N.", ""], ["Tanveer", "M.", ""]]}, {"id": "1907.00354", "submitter": "Xiaomeng Li", "authors": "Xiaomeng Li, Lequan Yu, Yueming Jin, Chi-Wing Fu, Lei Xing, Pheng-Ann\n  Heng", "title": "Difficulty-aware Meta-learning for Rare Disease Diagnosis", "comments": "MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rare diseases have extremely low-data regimes, unlike common diseases with\nlarge amount of available labeled data. Hence, to train a neural network to\nclassify rare diseases with a few per-class data samples is very challenging,\nand so far, catches very little attention. In this paper, we present a\ndifficulty-aware meta-learning method to address rare disease classifications\nand demonstrate its capability to classify dermoscopy images. Our key approach\nis to first train and construct a meta-learning model from data of common\ndiseases, then adapt the model to perform rare disease classification.To\nachieve this, we develop the difficulty-aware meta-learning method that\ndynamically monitors the importance of learning tasks during the\nmeta-optimization stage. To evaluate our method, we use the recent ISIC 2018\nskin lesion classification dataset, and show that with only five samples per\nclass, our model can quickly adapt to classify unseen classes by a high AUC of\n83.3%. Also, we evaluated several rare disease classification results in the\npublic Dermofit Image Library to demonstrate the potential of our method for\nreal clinical practice.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 10:18:02 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 05:07:17 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Li", "Xiaomeng", ""], ["Yu", "Lequan", ""], ["Jin", "Yueming", ""], ["Fu", "Chi-Wing", ""], ["Xing", "Lei", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1907.00374", "submitter": "Alexander Kreines", "authors": "Nir Morgulis, Alexander Kreines, Shachar Mendelowitz, Yuval Weisglass", "title": "Fooling a Real Car with Adversarial Traffic Signs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The attacks on the neural-network-based classifiers using adversarial images\nhave gained a lot of attention recently. An adversary can purposely generate an\nimage that is indistinguishable from a innocent image for a human being but is\nincorrectly classified by the neural networks. The adversarial images do not\nneed to be tuned to a particular architecture of the classifier - an image that\nfools one network can fool another one with a certain success rate.The\npublished works mostly concentrate on the use of modified image files for\nattacks against the classifiers trained on the model databases. Although there\nexists a general understanding that such attacks can be carried in the real\nworld as well, the works considering the real-world attacks are scarce.\nMoreover, to the best of our knowledge, there have been no reports on the\nattacks against real production-grade image classification systems.In our work\nwe present a robust pipeline for reproducible production of adversarial traffic\nsigns that can fool a wide range of classifiers, both open-source and\nproduction-grade in the real world. The efficiency of the attacks was checked\nboth with the neural-network-based classifiers and legacy computer vision\nsystems. Most of the attacks have been performed in the black-box mode, e.g.\nthe adversarial signs produced for a particular classifier were used to attack\na variety of other classifiers. The efficiency was confirmed in drive-by\nexperiments with a production-grade traffic sign recognition systems of a real\ncar.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 12:42:09 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Morgulis", "Nir", ""], ["Kreines", "Alexander", ""], ["Mendelowitz", "Shachar", ""], ["Weisglass", "Yuval", ""]]}, {"id": "1907.00382", "submitter": "Saket Singh", "authors": "Saket Singh, Debdoot Sheet and Mithun Dasgupta", "title": "Adversarially Trained Deep Neural Semantic Hashing Scheme for Subjective\n  Search in Fashion Inventory", "comments": "The paper comprises of 8 Pages that contain 9 figures and 3 tables to\n  support the work. The paper got accepted in the ACM's 25th KDD conference's\n  workshop titled AI for fashion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simple approach of retrieving a closest match of a query image from one\nin the gallery, compares an image pair using sum of absolute difference in\npixel or feature space. The process is computationally expensive, ill-posed to\nillumination, background composition, pose variation, as well as inefficient to\nbe deployed on gallery sets with more than 1000 elements. Hashing is a faster\nalternative which involves representing images in reduced dimensional simple\nfeature spaces. Encoding images into binary hash codes enables similarity\ncomparison in an image-pair using the Hamming distance measure. The challenge,\nhowever, lies in encoding the images using a semantic hashing scheme that lets\nsubjective neighbors lie within the tolerable Hamming radius. This work\npresents a solution employing adversarial learning of a deep neural semantic\nhashing network for fashion inventory retrieval. It consists of a feature\nextracting convolutional neural network (CNN) learned to (i) minimize error in\nclassifying type of clothing, (ii) minimize hamming distance between semantic\nneighbors and maximize distance between semantically dissimilar images, (iii)\nmaximally scramble a discriminator's ability to identify the corresponding hash\ncode-image pair when processing a semantically similar query-gallery image\npair. Experimental validation for fashion inventory search yields a mean\naverage precision (mAP) of 90.65% in finding the closest match as compared to\n53.26% obtained by the prior art of deep Cauchy hashing for hamming space\nretrieval.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 13:59:01 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Singh", "Saket", ""], ["Sheet", "Debdoot", ""], ["Dasgupta", "Mithun", ""]]}, {"id": "1907.00408", "submitter": "Shan Luo Dr", "authors": "Daniel Fernandes Gomes, Shan Luo and Luis F. Teixeira", "title": "GarmNet: Improving Global with Local Perception for Robotic Laundry\n  Folding", "comments": "13 pages, 5 figures, published in the 20th Towards Autonomous Robotic\n  Systems Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing autonomous assistants to help with domestic tasks is a vital topic\nin robotics research. Among these tasks, garment folding is one of them that is\nstill far from being achieved mainly due to the large number of possible\nconfigurations that a crumpled piece of clothing may exhibit. Research has been\ndone on either estimating the pose of the garment as a whole or detecting the\nlandmarks for grasping separately. However, such works constrain the capability\nof the robots to perceive the states of the garment by limiting the\nrepresentations for one single task. In this paper, we propose a novel\nend-to-end deep learning model named GarmNet that is able to simultaneously\nlocalize the garment and detect landmarks for grasping. The localization of the\ngarment represents the global information for recognising the category of the\ngarment, whereas the detection of landmarks can facilitate subsequent grasping\nactions. We train and evaluate our proposed GarmNet model using the CloPeMa\nGarment dataset that contains 3,330 images of different garment types in\ndifferent poses. The experiments show that the inclusion of landmark detection\n(GarmNet-B) can largely improve the garment localization, with an error rate of\n24.7% lower. Solutions as ours are important for robotics applications, as\nthese offer scalable to many classes, memory and processing efficient\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 16:29:14 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Gomes", "Daniel Fernandes", ""], ["Luo", "Shan", ""], ["Teixeira", "Luis F.", ""]]}, {"id": "1907.00420", "submitter": "Artit Wangperawong", "authors": "Pasawee Wirojwatanakul and Artit Wangperawong", "title": "Multi-Label Product Categorization Using Multi-Modal Fusion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigated multi-modal approaches using images,\ndescriptions, and titles to categorize e-commerce products on Amazon.\nSpecifically, we examined late fusion models, where the modalities are fused at\nthe decision level. Products were each assigned multiple labels, and the\nhierarchy in the labels were flattened and filtered. For our individual\nbaseline models, we modified a CNN architecture to classify the description and\ntitle, and then modified Keras' ResNet-50 to classify the images, achieving\n$F_1$ scores of 77.0%, 82.7%, and 61.0%, respectively. In comparison, our\ntri-modal late fusion model can classify products more effectively than single\nmodal models can, improving the $F_1$ score to 88.2%. Each modality\ncomplemented the shortcomings of the other modalities, demonstrating that\nincreasing the number of modalities can be an effective method for improving\nthe performance of multi-label classification problems.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 17:10:21 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 02:54:31 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Wirojwatanakul", "Pasawee", ""], ["Wangperawong", "Artit", ""]]}, {"id": "1907.00437", "submitter": "Rodney LaLonde Iii", "authors": "Rodney LaLonde, Irene Tanner, Katerina Nikiforaki, Georgios Z.\n  Papadakis, Pujan Kandel, Candice W. Bolan, Michael B. Wallace, Ulas Bagci", "title": "INN: Inflated Neural Networks for IPMN Diagnosis", "comments": "Accepted for publication at MICCAI 2019 (22nd International\n  Conference on Medical Image Computing and Computer Assisted Intervention).\n  Code is publicly available at\n  https://github.com/lalonderodney/INN-Inflated-Neural-Nets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intraductal papillary mucinous neoplasm (IPMN) is a precursor to pancreatic\nductal adenocarcinoma. While over half of patients are diagnosed with\npancreatic cancer at a distant stage, patients who are diagnosed early enjoy a\nmuch higher 5-year survival rate of $34\\%$ compared to $3\\%$ in the former;\nhence, early diagnosis is key. Unique challenges in the medical imaging domain\nsuch as extremely limited annotated data sets and typically large 3D volumetric\ndata have made it difficult for deep learning to secure a strong foothold. In\nthis work, we construct two novel \"inflated\" deep network architectures,\n$\\textit{InceptINN}$ and $\\textit{DenseINN}$, for the task of diagnosing IPMN\nfrom multisequence (T1 and T2) MRI. These networks inflate their 2D layers to\n3D and bootstrap weights from their 2D counterparts (Inceptionv3 and\nDenseNet121 respectively) trained on ImageNet to the new 3D kernels. We also\nextend the inflation process by further expanding the pre-trained kernels to\nhandle any number of input modalities and different fusion strategies. This is\none of the first studies to train an end-to-end deep network on multisequence\nMRI for IPMN diagnosis, and shows that our proposed novel inflated network\narchitectures are able to handle the extremely limited training data (139 MRI\nscans), while providing an absolute improvement of $8.76\\%$ in accuracy for\ndiagnosing IPMN over the current state-of-the-art. Code is publicly available\nat https://github.com/lalonderodney/INN-Inflated-Neural-Nets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 19:24:41 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["LaLonde", "Rodney", ""], ["Tanner", "Irene", ""], ["Nikiforaki", "Katerina", ""], ["Papadakis", "Georgios Z.", ""], ["Kandel", "Pujan", ""], ["Bolan", "Candice W.", ""], ["Wallace", "Michael B.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1907.00438", "submitter": "Yipeng Hu", "authors": "Yipeng Hu, Eli Gibson, Dean C. Barratt, Mark Emberton, J. Alison\n  Noble, Tom Vercauteren", "title": "Conditional Segmentation in Lieu of Image Registration", "comments": "Accepted to MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32245-8_45", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical pairwise image registration methods search for a spatial\ntransformation that optimises a numerical measure that indicates how well a\npair of moving and fixed images are aligned. Current learning-based\nregistration methods have adopted the same paradigm and typically predict, for\nany new input image pair, dense correspondences in the form of a dense\ndisplacement field or parameters of a spatial transformation model. However, in\nmany applications of registration, the spatial transformation itself is only\nrequired to propagate points or regions of interest (ROIs). In such cases,\ndetailed pixel- or voxel-level correspondence within or outside of these ROIs\noften have little clinical value. In this paper, we propose an alternative\nparadigm in which the location of corresponding image-specific ROIs, defined in\none image, within another image is learnt. This results in replacing image\nregistration by a conditional segmentation algorithm, which can build on\ntypical image segmentation networks and their widely-adopted training\nstrategies. Using the registration of 3D MRI and ultrasound images of the\nprostate as an example to demonstrate this new approach, we report a median\ntarget registration error (TRE) of 2.1 mm between the ground-truth ROIs defined\non intraoperative ultrasound images and those propagated from the preoperative\nMR images. Significantly lower (>34%) TREs were obtained using the proposed\nconditional segmentation compared with those obtained from a\npreviously-proposed spatial-transformation-predicting registration network\ntrained with the same multiple ROI labels for individual image pairs. We\nconclude this work by using a quantitative bias-variance analysis to provide\none explanation of the observed improvement in registration accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 19:33:08 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hu", "Yipeng", ""], ["Gibson", "Eli", ""], ["Barratt", "Dean C.", ""], ["Emberton", "Mark", ""], ["Noble", "J. Alison", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1907.00480", "submitter": "Vitaliy Lyudvichenko", "authors": "Vitaliy Lyudvichenko, Dmitriy Vatolin", "title": "Predicting video saliency using crowdsourced mouse-tracking data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new way of getting high-quality saliency maps for\nvideo, using a cheaper alternative to eye-tracking data. We designed a\nmouse-contingent video viewing system which simulates the viewers' peripheral\nvision based on the position of the mouse cursor. The system enables the use of\nmouse-tracking data recorded from an ordinary computer mouse as an alternative\nto real gaze fixations recorded by a more expensive eye-tracker. We developed a\ncrowdsourcing system that enables the collection of such mouse-tracking data at\nlarge scale. Using the collected mouse-tracking data we showed that it can\nserve as an approximation of eye-tracking data. Moreover, trying to increase\nthe efficiency of collected mouse-tracking data we proposed a novel deep neural\nnetwork algorithm that improves the quality of mouse-tracking saliency maps.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 22:03:39 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lyudvichenko", "Vitaliy", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "1907.00490", "submitter": "Lluis Gomez", "authors": "Ali Furkan Biten, Rub\\`en Tito, Andres Mafla, Lluis Gomez, Mar\\c{c}al\n  Rusi\\~nol, Minesh Mathew, C.V. Jawahar, Ernest Valveny, Dimosthenis Karatzas", "title": "ICDAR 2019 Competition on Scene Text Visual Question Answering", "comments": "15th International Conference on Document Analysis and Recognition\n  (ICDAR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents final results of ICDAR 2019 Scene Text Visual Question\nAnswering competition (ST-VQA). ST-VQA introduces an important aspect that is\nnot addressed by any Visual Question Answering system up to date, namely the\nincorporation of scene text to answer questions asked about an image. The\ncompetition introduces a new dataset comprising 23,038 images annotated with\n31,791 question/answer pairs where the answer is always grounded on text\ninstances present in the image. The images are taken from 7 different public\ncomputer vision datasets, covering a wide range of scenarios.\n  The competition was structured in three tasks of increasing difficulty, that\nrequire reading the text in a scene and understanding it in the context of the\nscene, to correctly answer a given question. A novel evaluation metric is\npresented, which elegantly assesses both key capabilities expected from an\noptimal model: text recognition and image understanding.\n  A detailed analysis of results from different participants is showcased,\nwhich provides insight into the current capabilities of VQA systems that can\nread. We firmly believe the dataset proposed in this challenge will be an\nimportant milestone to consider towards a path of more robust and general\nmodels that can exploit scene text to achieve holistic image understanding.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 22:46:11 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Biten", "Ali Furkan", ""], ["Tito", "Rub\u00e8n", ""], ["Mafla", "Andres", ""], ["Gomez", "Lluis", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Mathew", "Minesh", ""], ["Jawahar", "C. V.", ""], ["Valveny", "Ernest", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1907.00516", "submitter": "Weixia Zhang", "authors": "Weixia Zhang, Kede Ma, Guangtao Zhai, and Xiaokang Yang", "title": "Learning to Blindly Assess Image Quality in the Laboratory and Wild", "comments": "Accepted by ICIP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational models for blind image quality assessment (BIQA) are typically\ntrained in well-controlled laboratory environments with limited\ngeneralizability to realistically distorted images. Similarly, BIQA models\noptimized for images captured in the wild cannot adequately handle\nsynthetically distorted images. To face the cross-distortion-scenario\nchallenge, we develop a BIQA model and an approach of training it on multiple\nIQA databases (of different distortion scenarios) simultaneously. A key step in\nour approach is to create and combine image pairs within individual databases\nas the training set, which effectively bypasses the issue of perceptual scale\nrealignment. We compute a continuous quality annotation for each pair from the\ncorresponding human opinions, indicating the probability of one image having\nbetter perceptual quality. We train a deep neural network for BIQA over the\ntraining set of massive image pairs by minimizing the fidelity loss.\nExperiments on six IQA databases demonstrate that the optimized model by the\nproposed training strategy is effective in blindly assessing image quality in\nthe laboratory and wild, outperforming previous BIQA methods by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 02:31:07 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 02:25:30 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 05:51:24 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Zhang", "Weixia", ""], ["Ma", "Kede", ""], ["Zhai", "Guangtao", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1907.00528", "submitter": "Jiechao Ma", "authors": "Jiechao Ma, Sen Liang, Xiang Li, Hongwei Li, Bjoern H Menze, Rongguo\n  Zhang, Wei-Shi Zheng", "title": "Cross-view Relation Networks for Mammogram Mass Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammogram is the most effective imaging modality for the mass lesion\ndetection of breast cancer at the early stage. The information from the two\npaired views (i.e., medio-lateral oblique and cranio-caudal) are highly\nrelational and complementary, and this is crucial for doctors' decisions in\nclinical practice. However, existing mass detection methods do not consider\njointly learning effective features from the two relational views. To address\nthis issue, this paper proposes a novel mammogram mass detection framework,\ntermed Cross-View Relation Region-based Convolutional Neural Networks\n(CVR-RCNN). The proposed CVR-RCNN is expected to capture the latent relation\ninformation between the corresponding mass region of interests (ROIs) from the\ntwo paired views. Evaluations on a new large-scale private dataset and a public\nmammogram dataset show that the proposed CVR-RCNN outperforms existing\nstate-of-the-art mass detection methods. Meanwhile, our experimental results\nsuggest that incorporating the relation information across two views helps to\ntrain a superior detection model, which is a promising avenue for mammogram\nmass detection.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 03:27:09 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ma", "Jiechao", ""], ["Liang", "Sen", ""], ["Li", "Xiang", ""], ["Li", "Hongwei", ""], ["Menze", "Bjoern H", ""], ["Zhang", "Rongguo", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1907.00534", "submitter": "Christoph Heindl", "authors": "Christoph Heindl, Thomas P\\\"onitz, Andreas Pichler, and Josef\n  Scharinger", "title": "Large Area 3D Human Pose Detection Via Stereo Reconstruction in\n  Panoramic Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3D human pose detector using two panoramic cameras. We\nshow that transforming fisheye perspectives to rectilinear views allows a\ndirect application of two-dimensional deep-learning pose estimation methods,\nwithout the explicit need for a costly re-training step to compensate for\nfisheye image distortions. By utilizing panoramic cameras, our method is\ncapable of accurately estimating human poses over a large field of view. This\nrenders our method suitable for ergonomic analyses and other pose based\nassessments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 04:09:23 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Heindl", "Christoph", ""], ["P\u00f6nitz", "Thomas", ""], ["Pichler", "Andreas", ""], ["Scharinger", "Josef", ""]]}, {"id": "1907.00549", "submitter": "Christoph Heindl", "authors": "Christoph Heindl and Thomas P\\\"onitz and Gernot St\\\"ubl and Andreas\n  Pichler and Josef Scharinger", "title": "Spatio-thermal depth correction of RGB-D sensors based on Gaussian\n  Processes in real-time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commodity RGB-D sensors capture color images along with dense pixel-wise\ndepth information in real-time. Typical RGB-D sensors are provided with a\nfactory calibration and exhibit erratic depth readings due to coarse\ncalibration values, ageing and thermal influence effects. This limits their\napplicability in computer vision and robotics. We propose a novel method to\naccurately calibrate depth considering spatial and thermal influences jointly.\nOur work is based on Gaussian Process Regression in a four dimensional\nCartesian and thermal domain. We propose to leverage modern GPUs for dense\ndepth map correction in real-time. For reproducibility we make our dataset and\nsource code publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 05:07:33 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Heindl", "Christoph", ""], ["P\u00f6nitz", "Thomas", ""], ["St\u00fcbl", "Gernot", ""], ["Pichler", "Andreas", ""], ["Scharinger", "Josef", ""]]}, {"id": "1907.00559", "submitter": "Evgeny Burnaev", "authors": "Maria Taktasheva and Albert Matveev and Alexey Artemov and Evgeny\n  Burnaev", "title": "Learning to Approximate Directional Fields Defined over 2D Planes", "comments": "7 pages, 5 figures", "journal-ref": "Proc. of AIST, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of directional fields is a need in many geometry processing\ntasks, such as image tracing, extraction of 3D geometric features, and finding\nprincipal surface directions. A common approach to the construction of\ndirectional fields from data relies on complex optimization procedures, which\nare usually poorly formalizable, require a considerable computational effort,\nand do not transfer across applications. In this work, we propose a deep\nlearning-based approach and study the expressive power and generalization\nability.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 05:58:46 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Taktasheva", "Maria", ""], ["Matveev", "Albert", ""], ["Artemov", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1907.00593", "submitter": "Wen-Pu Cai", "authors": "Wen-Pu Cai and Wu-Jun Li", "title": "Weight Normalization based Quantization for Deep Neural Network\n  Compression", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep neural networks, the size of network models\nbecomes larger and larger. Model compression has become an urgent need for\ndeploying these network models to mobile or embedded devices. Model\nquantization is a representative model compression technique. Although a lot of\nquantization methods have been proposed, many of them suffer from a high\nquantization error caused by a long-tail distribution of network weights. In\nthis paper, we propose a novel quantization method, called weight normalization\nbased quantization (WNQ), for model compression. WNQ adopts weight\nnormalization to avoid the long-tail distribution of network weights and\nsubsequently reduces the quantization error. Experiments on CIFAR-100 and\nImageNet show that WNQ can outperform other baselines to achieve\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 07:59:39 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Cai", "Wen-Pu", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1907.00612", "submitter": "Tao He", "authors": "Tao He, Yuan-Fang Li, Lianli Gao, Dongxiang Zhang, Jingkuan Song", "title": "One Network for Multi-Domains: Domain Adaptive Hashing with Intersectant\n  Generative Adversarial Network", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent explosive increase of digital data, image recognition and\nretrieval become a critical practical application. Hashing is an effective\nsolution to this problem, due to its low storage requirement and high query\nspeed. However, most of past works focus on hashing in a single (source)\ndomain. Thus, the learned hash function may not adapt well in a new (target)\ndomain that has a large distributional difference with the source domain. In\nthis paper, we explore an end-to-end domain adaptive learning framework that\nsimultaneously and precisely generates discriminative hash codes and classifies\ntarget domain images. Our method encodes two domains images into a semantic\ncommon space, followed by two independent generative adversarial networks\narming at crosswise reconstructing two domains' images, reducing domain\ndisparity and improving alignment in the shared space. We evaluate our\nframework on {four} public benchmark datasets, all of which show that our\nmethod is superior to the other state-of-the-art methods on the tasks of object\nrecognition and image retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 08:48:39 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["He", "Tao", ""], ["Li", "Yuan-Fang", ""], ["Gao", "Lianli", ""], ["Zhang", "Dongxiang", ""], ["Song", "Jingkuan", ""]]}, {"id": "1907.00618", "submitter": "Alan Lukezic", "authors": "Alan Luke\\v{z}i\\v{c}, Ugur Kart, Jani K\\\"apyl\\\"a, Ahmed Durmush,\n  Joni-Kristian K\\\"am\\\"ar\\\"ainen, Ji\\v{r}\\'i Matas, Matej Kristan", "title": "CDTB: A Color and Depth Visual Object Tracking Dataset and Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-term visual object tracking performance evaluation methodology and a\nbenchmark are proposed. Performance measures are designed by following a\nlong-term tracking definition to maximize the analysis probing strength. The\nnew measures outperform existing ones in interpretation potential and in better\ndistinguishing between different tracking behaviors. We show that these\nmeasures generalize the short-term performance measures, thus linking the two\ntracking problems. Furthermore, the new measures are highly robust to temporal\nannotation sparsity and allow annotation of sequences hundreds of times longer\nthan in the current datasets without increasing manual annotation labor. A new\nchallenging dataset of carefully selected sequences with many target\ndisappearances is proposed. A new tracking taxonomy is proposed to position\ntrackers on the short-term/long-term spectrum. The benchmark contains an\nextensive evaluation of the largest number of long-term tackers and comparison\nto state-of-the-art short-term trackers. We analyze the influence of tracking\narchitecture implementations to long-term performance and explore various\nre-detection strategies as well as influence of visual model update strategies\nto long-term tracking drift. The methodology is integrated in the VOT toolkit\nto automate experimental analysis and benchmarking and to facilitate future\ndevelopment of long-term trackers.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 09:02:40 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Luke\u017ei\u010d", "Alan", ""], ["Kart", "Ugur", ""], ["K\u00e4pyl\u00e4", "Jani", ""], ["Durmush", "Ahmed", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""], ["Matas", "Ji\u0159\u00ed", ""], ["Kristan", "Matej", ""]]}, {"id": "1907.00641", "submitter": "Samuel Joutard", "authors": "Samuel Joutard, Reuben Dorent, Amanda Isaac, Sebastien Ourselin, Tom\n  Vercauteren, Marc Modat", "title": "Permutohedral Attention Module for Efficient Non-Local Neural Networks", "comments": "Accepted at MICCAI-2019", "journal-ref": null, "doi": "10.1007/978-3-030-32226-7_44", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image processing tasks such as segmentation often require capturing\nnon-local information. As organs, bones, and tissues share common\ncharacteristics such as intensity, shape, and texture, the contextual\ninformation plays a critical role in correctly labeling them. Segmentation and\nlabeling is now typically done with convolutional neural networks (CNNs) but\nthe context of the CNN is limited by the receptive field which itself is\nlimited by memory requirements and other properties. In this paper, we propose\na new attention module, that we call Permutohedral Attention Module (PAM), to\nefficiently capture non-local characteristics of the image. The proposed method\nis both memory and computationally efficient. We provide a GPU implementation\nof this module suitable for 3D medical imaging problems. We demonstrate the\nefficiency and scalability of our module with the challenging task of vertebrae\nsegmentation and labeling where context plays a crucial role because of the\nvery similar appearance of different vertebrae.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 10:18:37 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 14:30:25 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Joutard", "Samuel", ""], ["Dorent", "Reuben", ""], ["Isaac", "Amanda", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""], ["Modat", "Marc", ""]]}, {"id": "1907.00651", "submitter": "Masahiro Okuda", "authors": "Ryuji Imamura, Tatsuki Itasaka, Masahiro Okuda", "title": "Self-supervised Hyperspectral Image Restoration using Separable Image\n  Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning with a convolutional neural network is recognized as a\npowerful means of image restoration. However, most such methods have been\ndesigned for application to grayscale and/or color images; therefore, they have\nlimited success when applied to hyperspectral image restoration. This is\npartially owing to large datasets being difficult to collect, and also the\nheavy computational load associated with the restoration of an image with many\nspectral bands. To address this difficulty, we propose a novel self-supervised\nlearning strategy for application to hyperspectral image restoration. Our\nmethod automatically creates a training dataset from a single degraded image\nand trains a denoising network without any clear images. Another notable\nfeature of our method is the use of a separable convolutional layer. We\nundertake experiments to prove that the use of a separable network allows us to\nacquire the prior of a hyperspectral image and to realize efficient\nrestoration. We demonstrate the validity of our method through extensive\nexperiments and show that our method has better characteristics than those that\nare currently regarded as state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 10:51:57 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Imamura", "Ryuji", ""], ["Itasaka", "Tatsuki", ""], ["Okuda", "Masahiro", ""]]}, {"id": "1907.00652", "submitter": "Ashley Gritzman", "authors": "Ashley Daniel Gritzman", "title": "Avoiding Implementation Pitfalls of \"Matrix Capsules with EM Routing\" by\n  Hinton et al", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent progress on capsule networks by Hinton et al. has generated\nconsiderable excitement in the machine learning community. The idea behind a\ncapsule is inspired by a cortical minicolumn in the brain, whereby a vertically\norganised group of around 100 neurons receive common inputs, have common\noutputs, are interconnected, and may well constitute a fundamental computation\nunit of the cerebral cortex. However, Hinton's paper on \"Matrix Capsule with EM\nRouting'\" was unfortunately not accompanied by a release of source code, which\nleft interested researchers attempting to implement the architecture and\nreproduce the benchmarks on their own. This has certainly slowed the progress\nof research building on this work. While writing our own implementation, we\nnoticed several common mistakes in other open source implementations that we\ncame across. In this paper we share some of these learnings, specifically\nfocusing on three implementation pitfalls and how to avoid them: (1) parent\ncapsules with only one child; (2) normalising the amount of data assigned to\nparent capsules; (3) parent capsules at different positions compete for child\ncapsules. While our implementation is a considerable improvement over currently\navailable implementations, it still falls slightly short of the performance\nreported by Hinton et al. (2018). The source code for this implementation is\navailable on GitHub at the following URL:\nhttps://github.com/IBM/matrix-capsules-with-em-routing.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 10:51:58 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Gritzman", "Ashley Daniel", ""]]}, {"id": "1907.00661", "submitter": "Yusuke Yamaura", "authors": "Yusuke Yamaura, Nobuya Kanemaki, and Yukihiro Tsuboshita", "title": "The Resale Price Prediction of Secondhand Jewelry Items Using a\n  Multi-modal Deep Model with Iterative Co-Attention", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resale price assessment of secondhand jewelry items relies heavily on the\nindividual knowledge and skill of domain experts. In this paper, we propose a\nmethodology for reconstructing an AI system that autonomously assesses the\nresale prices of secondhand jewelry items without the need for professional\nknowledge. As shown in recent studies on fashion items, multimodal approaches\ncombining specifications and visual information of items have succeeded in\nobtaining fine-grained representations of fashion items, although they\ngenerally apply simple vector operations through a multimodal fusion. We\nsimilarly build a multimodal model using images and attributes of the product\nand further employ state-of-the-art multimodal deep neural networks applied in\ncomputer vision to achieve a practical performance level. In addition, we model\nthe pricing procedure of an expert using iterative co-attention networks in\nwhich the appearance and attributes of the product are carefully and\niteratively observed. Herein, we demonstrate the effectiveness of our model\nusing a large dataset of secondhand no brand jewelry items received from a\ncollaborating fashion retailer, and show that the iterative co-attention\nprocess operates effectively in the context of resale price prediction. Our\nmodel architecture is widely applicable to other fashion items where appearance\nand specifications are important aspects.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 11:17:33 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Yamaura", "Yusuke", ""], ["Kanemaki", "Nobuya", ""], ["Tsuboshita", "Yukihiro", ""]]}, {"id": "1907.00693", "submitter": "Seiichi Uchida", "authors": "Toshiki Nakamura, Anna Zhu, and Seiichi Uchida", "title": "Scene Text Magnifier", "comments": "to appear at the International Conference on Document Analysis and\n  Recognition (ICDAR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text magnifier aims to magnify text in natural scene images without\nrecognition. It could help the special groups, who have myopia or dyslexia to\nbetter understand the scene. In this paper, we design the scene text magnifier\nthrough interacted four CNN-based networks: character erasing, character\nextraction, character magnify, and image synthesis. The architecture of the\nnetworks are extended based on the hourglass encoder-decoders. It inputs the\noriginal scene text image and outputs the text magnified image while keeps the\nbackground unchange. Intermediately, we can get the side-output results of text\nerasing and text extraction. The four sub-networks are first trained\nindependently and fine-tuned in end-to-end mode. The training samples for each\nstage are processed through a flow with original image and text annotation in\nICDAR2013 and Flickr dataset as input, and corresponding text erased image,\nmagnified text annotation, and text magnified scene image as output. To\nevaluate the performance of text magnifier, the Structural Similarity is used\nto measure the regional changes in each character region. The experimental\nresults demonstrate our method can magnify scene text effectively without\neffecting the background.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 03:14:08 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 09:16:02 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Nakamura", "Toshiki", ""], ["Zhu", "Anna", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1907.00695", "submitter": "Florian Dubost", "authors": "Florian Dubost, Marleen de Bruijne, Marco Nardin, Adrian V. Dalca,\n  Kathleen L. Donahue, Anne-Katrin Giese, Mark R. Etherton, Ona Wu, Marius de\n  Groot, Wiro Niessen, Meike Vernooij, Natalia S. Rost, Markus D. Schirmer", "title": "Multi-atlas image registration of clinical data with automated quality\n  assessment using ventricle segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration is a core component of many imaging pipelines. In case of\nclinical scans, with lower resolution and sometimes substantial motion\nartifacts, registration can produce poor results. Visual assessment of\nregistration quality in large clinical datasets is inefficient. In this work,\nwe propose to automatically assess the quality of registration to an atlas in\nclinical FLAIR MRI scans of the brain. The method consists of automatically\nsegmenting the ventricles of a given scan using a neural network, and comparing\nthe segmentation to the atlas' ventricles propagated to image space. We used\nthe proposed method to improve clinical image registration to a general atlas\nby computing multiple registrations and then selecting the registration that\nyielded the highest ventricle overlap. Methods were evaluated in a single-site\ndataset of more than 1000 scans, as well as a multi-center dataset comprising\n142 clinical scans from 12 sites. The automated ventricle segmentation reached\na Dice coefficient with manual annotations of 0.89 in the single-site dataset,\nand 0.83 in the multi-center dataset. Registration via age-specific atlases\ncould improve ventricle overlap compared to a direct registration to the\ngeneral atlas (Dice similarity coefficient increase up to 0.15). Experiments\nalso showed that selecting scans with the registration quality assessment\nmethod could improve the quality of average maps of white matter hyperintensity\nburden, instead of using all scans for the computation of the white matter\nhyperintensity map. In this work, we demonstrated the utility of an automated\ntool for assessing image registration quality in clinical scans. This image\nquality assessment step could ultimately assist in the translation of automated\nneuroimaging pipelines to the clinic.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 12:22:39 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 11:40:41 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Dubost", "Florian", ""], ["de Bruijne", "Marleen", ""], ["Nardin", "Marco", ""], ["Dalca", "Adrian V.", ""], ["Donahue", "Kathleen L.", ""], ["Giese", "Anne-Katrin", ""], ["Etherton", "Mark R.", ""], ["Wu", "Ona", ""], ["de Groot", "Marius", ""], ["Niessen", "Wiro", ""], ["Vernooij", "Meike", ""], ["Rost", "Natalia S.", ""], ["Schirmer", "Markus D.", ""]]}, {"id": "1907.00734", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro", "title": "Learning Objectness from Sonar Images for Class-Independent Object\n  Detection", "comments": "European Conference on Mobile Robots 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting novel objects without class information is not trivial, as it is\ndifficult to generalize from a small training set. This is an interesting\nproblem for underwater robotics, as modeling marine objects is inherently more\ndifficult in sonar images, and training data might not be available apriori.\nDetection proposals algorithms can be used for this purpose but usually\nrequires a large amount of output bounding boxes. In this paper we propose the\nuse of a fully convolutional neural network that regresses an objectness value\ndirectly from a Forward-Looking sonar image. By ranking objectness, we can\nproduce high recall (96 %) with only 100 proposals per image. In comparison,\nEdgeBoxes requires 5000 proposals to achieve a slightly better recall of 97 %,\nwhile Selective Search requires 2000 proposals to achieve 95 % recall. We also\nshow that our method outperforms a template matching baseline by a considerable\nmargin, and is able to generalize to completely new objects. We expect that\nthis kind of technique can be used in the field to find lost objects under the\nsea.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 12:46:08 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Valdenegro-Toro", "Matias", ""]]}, {"id": "1907.00749", "submitter": "Vidyasagar Sadhu", "authors": "Vidyasagar Sadhu, Teruhisa Misu, Dario Pompili", "title": "Deep Multi-Task Learning for Anomalous Driving Detection Using CAN Bus\n  Scalar Sensor Data", "comments": "IROS 2019, 8 pages", "journal-ref": "2019 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), pp. 1-8", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corner cases are the main bottlenecks when applying Artificial Intelligence\n(AI) systems to safety-critical applications. An AI system should be\nintelligent enough to detect such situations so that system developers can\nprepare for subsequent planning. In this paper, we propose semi-supervised\nanomaly detection considering the imbalance of normal situations. In\nparticular, driving data consists of multiple positive/normal situations (e.g.,\nright turn, going straight), some of which (e.g., U-turn) could be as rare as\nanomalous situations. Existing machine learning based anomaly detection\napproaches do not fare sufficiently well when applied to such imbalanced data.\nIn this paper, we present a novel multi-task learning based approach that\nleverages domain-knowledge (maneuver labels) for anomaly detection in driving\ndata. We evaluate the proposed approach both quantitatively and qualitatively\non 150 hours of real-world driving data and show improved performance over\nbaseline approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 04:36:47 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Sadhu", "Vidyasagar", ""], ["Misu", "Teruhisa", ""], ["Pompili", "Dario", ""]]}, {"id": "1907.00758", "submitter": "Aciel Eshky", "authors": "Aciel Eshky, Manuel Sam Ribeiro, Korin Richmond, Steve Renals", "title": "Synchronising audio and ultrasound by learning cross-modal embeddings", "comments": "5 pages, 1 figure, 4 tables; Interspeech 2019 with the following\n  edits: 1) Loss and accuracy upon convergence were accidentally reported from\n  an older model. Now updated with model described throughout the paper. All\n  other results remain unchanged. 2) Max true offset in the training data\n  corrected from 179ms to 1789ms. 3) Detectability \"boundary/range\" renamed to\n  detectability \"thresholds\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audiovisual synchronisation is the task of determining the time offset\nbetween speech audio and a video recording of the articulators. In child speech\ntherapy, audio and ultrasound videos of the tongue are captured using\ninstruments which rely on hardware to synchronise the two modalities at\nrecording time. Hardware synchronisation can fail in practice, and no mechanism\nexists to synchronise the signals post hoc. To address this problem, we employ\na two-stream neural network which exploits the correlation between the two\nmodalities to find the offset. We train our model on recordings from 69\nspeakers, and show that it correctly synchronises 82.9% of test utterances from\nunseen therapy sessions and unseen speakers, thus considerably reducing the\nnumber of utterances to be manually synchronised. An analysis of model\nperformance on the test utterances shows that directed phone articulations are\nmore difficult to automatically synchronise compared to utterances containing\nnatural variation in speech such as words, sentences, or conversations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 13:22:48 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 11:24:26 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Eshky", "Aciel", ""], ["Ribeiro", "Manuel Sam", ""], ["Richmond", "Korin", ""], ["Renals", "Steve", ""]]}, {"id": "1907.00831", "submitter": "Young-chul Yoon", "authors": "Young-Chul Yoon, Du Yong Kim, Young-min Song, Kwangjin Yoon and Moongu\n  Jeon", "title": "Online Multiple Pedestrians Tracking using Deep Temporal Appearance\n  Matching Association", "comments": "Accepted in Information Sciences, Elsevier. 3rd Prize on 4th BMTT\n  MOTChallenge Workshop held in CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online multi-target tracking, modeling of appearance and geometric\nsimilarities between pedestrians visual scenes is of great importance. The\nhigher dimension of inherent information in the appearance model compared to\nthe geometric model is problematic in many ways. However, due to the recent\nsuccess of deep-learning-based methods, handling of high-dimensional appearance\ninformation becomes feasible. Among many deep neural networks, Siamese network\nwith triplet loss has been widely adopted as an effective appearance feature\nextractor. Since the Siamese network can extract the features of each input\nindependently, one can update and maintain target-specific features. However,\nit is not suitable for multi-target settings that require comparison with other\ninputs. To address this issue, we propose a novel track appearance model based\non the joint-inference network. The proposed method enables a comparison of two\ninputs to be used for adaptive appearance modeling and contributes to the\ndisambiguation of target-observation matching and to the consolidation of\nidentity consistency. Diverse experimental results support the effectiveness of\nour method. Our work was recognized as the 3rd-best tracker in BMTT\nMOTChallenge 2019, held at CVPR2019. The code is available at\nhttps://github.com/yyc9268/Deep-TAMA.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:44:41 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 19:12:15 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 17:29:37 GMT"}, {"version": "v4", "created": "Fri, 9 Oct 2020 14:32:42 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Yoon", "Young-Chul", ""], ["Kim", "Du Yong", ""], ["Song", "Young-min", ""], ["Yoon", "Kwangjin", ""], ["Jeon", "Moongu", ""]]}, {"id": "1907.00835", "submitter": "Aciel Eshky", "authors": "Aciel Eshky, Manuel Sam Ribeiro, Joanne Cleland, Korin Richmond, Zoe\n  Roxburgh, James Scobbie, Alan Wrench", "title": "UltraSuite: A Repository of Ultrasound and Acoustic Data from Child\n  Speech Therapy Sessions", "comments": "5 pages, 1 figure, 3 tables; accepted to Interspeech 2018: 19th\n  Annual Conference of the International Speech Communication Association\n  (ISCA)", "journal-ref": null, "doi": "10.21437/Interspeech.2018-1736", "report-no": null, "categories": "cs.CL cs.CV cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce UltraSuite, a curated repository of ultrasound and acoustic\ndata, collected from recordings of child speech therapy sessions. This release\nincludes three data collections, one from typically developing children and two\nfrom children with speech sound disorders. In addition, it includes a set of\nannotations, some manual and some automatically produced, and software tools to\nprocess, transform and visualise the data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:54:31 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Eshky", "Aciel", ""], ["Ribeiro", "Manuel Sam", ""], ["Cleland", "Joanne", ""], ["Richmond", "Korin", ""], ["Roxburgh", "Zoe", ""], ["Scobbie", "James", ""], ["Wrench", "Alan", ""]]}, {"id": "1907.00837", "submitter": "Dushyant Mehta", "authors": "Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu,\n  Mohamed Elgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard\n  Pons-Moll, Christian Theobalt", "title": "XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera", "comments": "To appear in ACM Transactions on Graphics (SIGGRAPH) 2020", "journal-ref": null, "doi": "10.1145/3386569.3392410", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time approach for multi-person 3D motion capture at over 30\nfps using a single RGB camera. It operates successfully in generic scenes which\nmay contain occlusions by objects and by other people. Our method operates in\nsubsequent stages. The first stage is a convolutional neural network (CNN) that\nestimates 2D and 3D pose features along with identity assignments for all\nvisible joints of all individuals.We contribute a new architecture for this\nCNN, called SelecSLS Net, that uses novel selective long and short range skip\nconnections to improve the information flow allowing for a drastically faster\nnetwork without compromising accuracy. In the second stage, a fully connected\nneural network turns the possibly partial (on account of occlusion) 2Dpose and\n3Dpose features for each subject into a complete 3Dpose estimate per\nindividual. The third stage applies space-time skeletal model fitting to the\npredicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose,\nand enforce temporal coherence. Our method returns the full skeletal pose in\njoint angles for each subject. This is a further key distinction from previous\nwork that do not produce joint angle results of a coherent skeleton in real\ntime for multi-person scenes. The proposed system runs on consumer hardware at\na previously unseen speed of more than 30 fps given 512x320 images as input\nwhile achieving state-of-the-art accuracy, which we will demonstrate on a range\nof challenging real-world scenes.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:59:02 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 09:55:59 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Mehta", "Dushyant", ""], ["Sotnychenko", "Oleksandr", ""], ["Mueller", "Franziska", ""], ["Xu", "Weipeng", ""], ["Elgharib", "Mohamed", ""], ["Fua", "Pascal", ""], ["Seidel", "Hans-Peter", ""], ["Rhodin", "Helge", ""], ["Pons-Moll", "Gerard", ""], ["Theobalt", "Christian", ""]]}, {"id": "1907.00856", "submitter": "Md. Mostafa Kamal Sarker", "authors": "Md. Mostafa Kamal Sarker, Hatem A. Rashwan, Farhan Akram, Vivek Kumar\n  Singh, Syeda Furruka Banu, Forhad U H Chowdhury, Kabir Ahmed Choudhury,\n  Sylvie Chambon, Petia Radeva, Domenec Puig, Mohamed Abdel-Nasser", "title": "SLSNet: Skin lesion segmentation using a lightweight generative\n  adversarial network", "comments": "Accepted in Expert Systems with Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of precise skin lesion boundaries in dermoscopic images\nusing automated methods faces many challenges, most importantly, the presence\nof hair, inconspicuous lesion edges and low contrast in dermoscopic images, and\nvariability in the color, texture and shapes of skin lesions. Existing deep\nlearning-based skin lesion segmentation algorithms are expensive in terms of\ncomputational time and memory. Consequently, running such segmentation\nalgorithms requires a powerful GPU and high bandwidth memory, which are not\navailable in dermoscopy devices. Thus, this article aims to achieve precise\nskin lesion segmentation with minimum resources: a lightweight, efficient\ngenerative adversarial network (GAN) model called SLSNet, which combines 1-D\nkernel factorized networks, position and channel attention, and multiscale\naggregation mechanisms with a GAN model. The 1-D kernel factorized network\nreduces the computational cost of 2D filtering. The position and channel\nattention modules enhance the discriminative ability between the lesion and\nnon-lesion feature representations in spatial and channel dimensions,\nrespectively. A multiscale block is also used to aggregate the coarse-to-fine\nfeatures of input skin images and reduce the effect of the artifacts. SLSNet is\nevaluated on two publicly available datasets: ISBI 2017 and the ISIC 2018.\nAlthough SLSNet has only 2.35 million parameters, the experimental results\ndemonstrate that it achieves segmentation results on a par with the\nstate-of-the-art skin lesion segmentation methods with an accuracy of 97.61%,\nand Dice and Jaccard similarity coefficients of 90.63% and 81.98%,\nrespectively. SLSNet can run at more than 110 frames per second (FPS) in a\nsingle GTX1080Ti GPU, which is faster than well-known deep learning-based image\nsegmentation models, such as FCN. Therefore, SLSNet can be used for practical\ndermoscopic applications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:21:18 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 13:00:36 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 19:35:47 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Sarker", "Md. Mostafa Kamal", ""], ["Rashwan", "Hatem A.", ""], ["Akram", "Farhan", ""], ["Singh", "Vivek Kumar", ""], ["Banu", "Syeda Furruka", ""], ["Chowdhury", "Forhad U H", ""], ["Choudhury", "Kabir Ahmed", ""], ["Chambon", "Sylvie", ""], ["Radeva", "Petia", ""], ["Puig", "Domenec", ""], ["Abdel-Nasser", "Mohamed", ""]]}, {"id": "1907.00887", "submitter": "Vivek Kumar Singh", "authors": "Vivek Kumar Singh, Hatem A. Rashwan, Mohamed Abdel-Nasser, Md. Mostafa\n  Kamal Sarker, Farhan Akram, Nidhi Pandey, Santiago Romani, Domenec Puig", "title": "An Efficient Solution for Breast Tumor Segmentation and Classification\n  in Ultrasound Images Using Deep Adversarial Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposes an efficient solution for tumor segmentation and\nclassification in breast ultrasound (BUS) images. We propose to add an atrous\nconvolution layer to the conditional generative adversarial network (cGAN)\nsegmentation model to learn tumor features at different resolutions of BUS\nimages. To automatically re-balance the relative impact of each of the highest\nlevel encoded features, we also propose to add a channel-wise weighting block\nin the network. In addition, the SSIM and L1-norm loss with the typical\nadversarial loss are used as a loss function to train the model. Our model\noutperforms the state-of-the-art segmentation models in terms of the Dice and\nIoU metrics, achieving top scores of 93.76% and 88.82%, respectively. In the\nclassification stage, we show that few statistics features extracted from the\nshape of the boundaries of the predicted masks can properly discriminate\nbetween benign and malignant tumors with an accuracy of 85%$\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 16:00:49 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Singh", "Vivek Kumar", ""], ["Rashwan", "Hatem A.", ""], ["Abdel-Nasser", "Mohamed", ""], ["Sarker", "Md. Mostafa Kamal", ""], ["Akram", "Farhan", ""], ["Pandey", "Nidhi", ""], ["Romani", "Santiago", ""], ["Puig", "Domenec", ""]]}, {"id": "1907.00932", "submitter": "Guido Muscioni", "authors": "Guido Muscioni, Riccardo Pressiani, Matteo Foglio, Margaret C.\n  Crofoot, Marco D. Santambrogio and Tanya Berger-Wolf", "title": "A Framework For Identifying Group Behavior Of Wild Animals", "comments": "KDD19 Workshop on Data Mining and AI for Conservation, Earth Day (5\n  August 2019), Anchorage, AL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition and, more generally, behavior inference tasks are\ngaining a lot of interest. Much of it is work in the context of human behavior.\nNew available tracking technologies for wild animals are generating datasets\nthat indirectly may provide information about animal behavior. In this work, we\npropose a method for classifying these data into behavioral annotation,\nparticularly collective behavior of a social group. Our method is based on\nsequence analysis with a direct encoding of the interactions of a group of wild\nanimals. We evaluate our approach on a real world dataset, showing significant\naccuracy improvements over baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:12:32 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Muscioni", "Guido", ""], ["Pressiani", "Riccardo", ""], ["Foglio", "Matteo", ""], ["Crofoot", "Margaret C.", ""], ["Santambrogio", "Marco D.", ""], ["Berger-Wolf", "Tanya", ""]]}, {"id": "1907.00939", "submitter": "Marc Eder", "authors": "Marc Eder, Pierre Moulon, Li Guan", "title": "Pano Popups: Indoor 3D Reconstruction with a Plane-Aware Network", "comments": "2019 International Conference on 3D Vision (3DV). IEEE, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we present a method to train a plane-aware convolutional neural\nnetwork for dense depth and surface normal estimation as well as plane\nboundaries from a single indoor $360^\\circ$ image. Using our proposed loss\nfunction, our network outperforms existing methods for single-view, indoor,\nomnidirectional depth estimation and provides an initial benchmark for surface\nnormal prediction from $360^\\circ$ images. Our improvements are due to the use\nof a novel plane-aware loss that leverages principal curvature as an indicator\nof planar boundaries. We also show that including geodesic coordinate maps as\nnetwork priors provides a significant boost in surface normal prediction\naccuracy. Finally, we demonstrate how we can combine our network's outputs to\ngenerate high quality 3D \"pop-up\" models of indoor scenes.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:24:18 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 18:59:47 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Eder", "Marc", ""], ["Moulon", "Pierre", ""], ["Guan", "Li", ""]]}, {"id": "1907.00943", "submitter": "Xinyang Feng", "authors": "Xinyang Feng, Zachary C. Lipton, Jie Yang, Scott A. Small, Frank A.\n  Provenzano", "title": "Estimating brain age based on a healthy population with deep learning\n  and structural MRI", "comments": "32 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous studies have established that estimated brain age, as derived from\nstatistical models trained on healthy populations, constitutes a valuable\nbiomarker that is predictive of cognitive decline and various neurological\ndiseases. In this work, we curate a large-scale heterogeneous dataset (N =\n10,158, age range 18 - 97) of structural brain MRIs in a healthy population\nfrom multiple publicly-available sources, upon which we train a deep learning\nmodel for brain age estimation. The availability of the large-scale dataset\nenables a more uniform age distribution across adult life-span for effective\nage estimation with no bias toward certain age groups. We demonstrate that the\nage estimation accuracy, evaluated with mean absolute error (MAE) and\ncorrelation coefficient (r), outperforms previously reported methods in both a\nhold-out test set reflective of the custom population (MAE = 4.06 years, r =\n0.970) and an independent life-span evaluation dataset (MAE = 4.21 years, r =\n0.960) on which a previous study has evaluated. We further demonstrate the\nutility of the estimated age in life-span aging analysis of cognitive\nfunctions. Furthermore, we conduct extensive ablation tests and employ\nfeature-attribution techniques to analyze which regions contribute the most\npredictive value, demonstrating the prominence of the frontal lobe as well as\npattern shift across life-span. In summary, we achieve superior age estimation\nperformance confirming the efficacy of deep learning and the added utility of\ntraining with data both in larger number and more uniformly distributed than in\nprevious studies. We demonstrate the regional contribution to our brain age\npredictions through multiple routes and confirm the association of divergence\nbetween estimated and chronological brain age with neuropsychological measures.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:30:18 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Feng", "Xinyang", ""], ["Lipton", "Zachary C.", ""], ["Yang", "Jie", ""], ["Small", "Scott A.", ""], ["Provenzano", "Frank A.", ""]]}, {"id": "1907.00945", "submitter": "Yash Patel", "authors": "Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowdhury,\n  Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Umapada Pal, Jean-Christophe\n  Burie, Cheng-lin Liu, Jean-Marc Ogier", "title": "ICDAR2019 Robust Reading Challenge on Multi-lingual Scene Text Detection\n  and Recognition -- RRC-MLT-2019", "comments": "ICDAR'19 camera-ready version. Competition available at\n  https://rrc.cvc.uab.es/?ch=15. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing cosmopolitan culture of modern cities, the need of robust\nMulti-Lingual scene Text (MLT) detection and recognition systems has never been\nmore immense. With the goal to systematically benchmark and push the\nstate-of-the-art forward, the proposed competition builds on top of the\nRRC-MLT-2017 with an additional end-to-end task, an additional language in the\nreal images dataset, a large scale multi-lingual synthetic dataset to assist\nthe training, and a baseline End-to-End recognition method. The real dataset\nconsists of 20,000 images containing text from 10 languages. The challenge has\n4 tasks covering various aspects of multi-lingual scene text: (a) text\ndetection, (b) cropped word script classification, (c) joint text detection and\nscript classification and (d) end-to-end detection and recognition. In total,\nthe competition received 60 submissions from the research and industrial\ncommunities. This paper presents the dataset, the tasks and the findings of the\npresented RRC-MLT-2019 challenge.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:30:31 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Nayef", "Nibal", ""], ["Patel", "Yash", ""], ["Busta", "Michal", ""], ["Chowdhury", "Pinaki Nath", ""], ["Karatzas", "Dimosthenis", ""], ["Khlif", "Wafa", ""], ["Matas", "Jiri", ""], ["Pal", "Umapada", ""], ["Burie", "Jean-Christophe", ""], ["Liu", "Cheng-lin", ""], ["Ogier", "Jean-Marc", ""]]}, {"id": "1907.00959", "submitter": "Dimitrios Stamoulis", "authors": "Dimitrios Stamoulis, Ruizhou Ding, Di Wang, Dimitrios Lymberopoulos,\n  Bodhi Priyantha, Jie Liu, Diana Marculescu", "title": "Single-Path Mobile AutoML: Efficient ConvNet Design and NAS\n  Hyperparameter Optimization", "comments": "Detailed extension (journal) of the Single-Path NAS ECMLPKDD'19 paper\n  (arXiv:1904.02877)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we reduce the search cost of Neural Architecture Search (NAS) from days\ndown to only few hours? NAS methods automate the design of Convolutional\nNetworks (ConvNets) under hardware constraints and they have emerged as key\ncomponents of AutoML frameworks. However, the NAS problem remains challenging\ndue to the combinatorially large design space and the significant search time\n(at least 200 GPU-hours). In this work, we alleviate the NAS search cost down\nto less than 3 hours, while achieving state-of-the-art image classification\nresults under mobile latency constraints. We propose a novel differentiable NAS\nformulation, namely Single-Path NAS, that uses one single-path\nover-parameterized ConvNet to encode all architectural decisions based on\nshared convolutional kernel parameters, hence drastically decreasing the search\noverhead. Single-Path NAS achieves state-of-the-art top-1 ImageNet accuracy\n(75.62%), hence outperforming existing mobile NAS methods in similar latency\nsettings (~80ms). In particular, we enhance the accuracy-runtime trade-off in\ndifferentiable NAS by treating the Squeeze-and-Excitation path as a fully\nsearchable operation with our novel single-path encoding. Our method has an\noverall cost of only 8 epochs (24 TPU-hours), which is up to 5,000x faster\ncompared to prior work. Moreover, we study how different NAS formulation\nchoices affect the performance of the designed ConvNets. Furthermore, we\nexploit the efficiency of our method to answer an interesting question: instead\nof empirically tuning the hyperparameters of the NAS solver (as in prior work),\ncan we automatically find the hyperparameter values that yield the desired\naccuracy-runtime trade-off? We open-source our entire codebase at:\nhttps://github.com/dstamoulis/single-path-nas.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:52:55 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Stamoulis", "Dimitrios", ""], ["Ding", "Ruizhou", ""], ["Wang", "Di", ""], ["Lymberopoulos", "Dimitrios", ""], ["Priyantha", "Bodhi", ""], ["Liu", "Jie", ""], ["Marculescu", "Diana", ""]]}, {"id": "1907.00960", "submitter": "Eric-Tuan Le", "authors": "Eric-Tuan Le, Iasonas Kokkinos, Niloy J. Mitra", "title": "Going Deeper with Lean Point Networks", "comments": "16 pages, 11 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce Lean Point Networks (LPNs) to train deeper and more\naccurate point processing networks by relying on three novel point processing\nblocks that improve memory consumption, inference time, and accuracy: a\nconvolution-type block for point sets that blends neighborhood information in a\nmemory-efficient manner; a crosslink block that efficiently shares information\nacross low- and high-resolution processing branches; and a multiresolution\npoint cloud processing block for faster diffusion of information. By combining\nthese blocks, we design wider and deeper point-based architectures. We report\nsystematic accuracy and memory consumption improvements on multiple publicly\navailable segmentation tasks by using our generic modules as drop-in\nreplacements for the blocks of multiple architectures (PointNet++, DGCNN,\nSpiderNet, PointCNN).\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:53:20 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 17:01:43 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Le", "Eric-Tuan", ""], ["Kokkinos", "Iasonas", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1907.01003", "submitter": "Wieland Brendel", "authors": "Wieland Brendel, Jonas Rauber, Matthias K\\\"ummerer, Ivan\n  Ustyuzhaninov, Matthias Bethge", "title": "Accurate, reliable and fast robustness evaluation", "comments": "Accepted at the 2019 Conference on Neural Information Processing\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the past five years, the susceptibility of neural networks to\nminimal adversarial perturbations has moved from a peculiar phenomenon to a\ncore issue in Deep Learning. Despite much attention, however, progress towards\nmore robust models is significantly impaired by the difficulty of evaluating\nthe robustness of neural network models. Today's methods are either fast but\nbrittle (gradient-based attacks), or they are fairly reliable but slow (score-\nand decision-based attacks). We here develop a new set of gradient-based\nadversarial attacks which (a) are more reliable in the face of gradient-masking\nthan other gradient-based attacks, (b) perform better and are more query\nefficient than current state-of-the-art gradient-based attacks, (c) can be\nflexibly adapted to a wide range of adversarial criteria and (d) require\nvirtually no hyperparameter tuning. These findings are carefully validated\nacross a diverse set of six different models and hold for L0, L1, L2 and Linf\nin both targeted as well as untargeted scenarios. Implementations will soon be\navailable in all major toolboxes (Foolbox, CleverHans and ART). We hope that\nthis class of attacks will make robustness evaluations easier and more\nreliable, thus contributing to more signal in the search for more robust\nmachine learning models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 18:18:10 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 18:32:51 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Brendel", "Wieland", ""], ["Rauber", "Jonas", ""], ["K\u00fcmmerer", "Matthias", ""], ["Ustyuzhaninov", "Ivan", ""], ["Bethge", "Matthias", ""]]}, {"id": "1907.01004", "submitter": "Md Iqbal Hossain", "authors": "Felice De Luca, Md Iqbal Hossain, Stephen Kobourov", "title": "Symmetry Detection and Classification in Drawings of Graphs", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is a key feature observed in nature (from flowers and leaves, to\nbutterflies and birds) and in human-made objects (from paintings and\nsculptures, to manufactured objects and architectural design). Rotational,\ntranslational, and especially reflectional symmetries, are also important in\ndrawings of graphs. Detecting and classifying symmetries can be very useful in\nalgorithms that aim to create symmetric graph drawings and in this paper we\npresent a machine learning approach for these tasks. Specifically, we show that\ndeep neural networks can be used to detect reflectional symmetries with 92%\naccuracy. We also build a multi-class classifier to distinguish between\nreflectional horizontal, reflectional vertical, rotational, and translational\nsymmetries. Finally, we make available a collection of images of graph drawings\nwith specific symmetric features that can be used in machine learning systems\nfor training, testing and validation purposes. Our datasets, best trained ML\nmodels, source code are available online.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 18:20:03 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 16:10:55 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 03:55:14 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["De Luca", "Felice", ""], ["Hossain", "Md Iqbal", ""], ["Kobourov", "Stephen", ""]]}, {"id": "1907.01023", "submitter": "Nader Asadi", "authors": "Nader Asadi, AmirMohammad Sarfi, Mehrdad Hosseinzadeh, Sahba Tahsini,\n  Mahdi Eftekhari", "title": "Diminishing the Effect of Adversarial Perturbations via Refining Feature\n  Representation", "comments": "Accepted at NeuralIPS 2019 workshop on Safety and Robustness in\n  Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are highly vulnerable to adversarial examples, which\nimposes severe security issues for these state-of-the-art models. Many defense\nmethods have been proposed to mitigate this problem. However, a lot of them\ndepend on modification or additional training of the target model. In this\nwork, we analytically investigate each layer's representation of non-perturbed\nand perturbed images and show the effect of perturbations on each of these\nrepresentations. Accordingly, a method based on whitening coloring transform is\nproposed in order to diminish the misrepresentation of any desirable layer\ncaused by adversaries. Our method can be applied to any layer of any arbitrary\nmodel without the need of any modification or additional training. Due to the\nfact that the full whitening of the layer's representation is not easily\ndifferentiable, our proposed method is superbly robust against white-box\nattacks. Furthermore, we demonstrate the strength of our method against some\nstate-of-the-art black-box attacks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 19:21:22 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 17:43:49 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Asadi", "Nader", ""], ["Sarfi", "AmirMohammad", ""], ["Hosseinzadeh", "Mehrdad", ""], ["Tahsini", "Sahba", ""], ["Eftekhari", "Mahdi", ""]]}, {"id": "1907.01034", "submitter": "Guy Gaziv", "authors": "Guy Gaziv", "title": "Learning to aggregate feature representations", "comments": "Report for Algonauts2019 challenge. 4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Algonauts challenge requires to construct a multi-subject encoder of\nimages to brain activity. Deep networks such as ResNet-50 and AlexNet trained\nfor image classification are known to produce feature representations along\ntheir intermediate stages which closely mimic the visual hierarchy. However the\nchallenges introduced in the Algonauts project, including combining data from\nmultiple subjects, relying on very few similarity data points, solving for\nvarious ROIs, and multi-modality, require devising a flexible framework which\ncan efficiently accommodate them. Here we build upon a recent state-of-the-art\nclassification network (SE-ResNeXt-50) and construct an adaptive combination of\nits intermediate representations. While the pretrained network serves as a\nbackbone of our model, we learn how to aggregate feature representations along\nfive stages of the network. During learning, our method enables to modulate and\nscreen outputs from each stage along the network as governed by the optimized\nobjective. We applied our method to the Algonauts2019 fMRI and MEG challenges.\nUsing the combined fMRI and MEG data, our approach was rated among the leading\nfive for both challenges. Surprisingly we find that for both the lower and\nhigher order areas (EVC and IT) the adaptive aggregation favors features\nstemming at later stages of the network.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 19:35:17 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 08:32:41 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2019 06:34:12 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Gaziv", "Guy", ""]]}, {"id": "1907.01058", "submitter": "Julien Moreau", "authors": "Maxime Istasse, Julien Moreau, Christophe De Vleeschouwer", "title": "Associative Embedding for Game-Agnostic Team Discrimination", "comments": "Published in CVPR 2019 workshop Computer Vision in Sports, under the\n  name \"Associative Embedding for Team Discrimination\"\n  (http://openaccess.thecvf.com/content_CVPRW_2019/html/CVSports/Istasse_Associative_Embedding_for_Team_Discrimination_CVPRW_2019_paper.html)", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) Workshops, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assigning team labels to players in a sport game is not a trivial task when\nno prior is known about the visual appearance of each team. Our work builds on\na Convolutional Neural Network (CNN) to learn a descriptor, namely a pixel-wise\nembedding vector, that is similar for pixels depicting players from the same\nteam, and dissimilar when pixels correspond to distinct teams. The advantage of\nthis idea is that no per-game learning is needed, allowing efficient team\ndiscrimination as soon as the game starts. In principle, the approach follows\nthe associative embedding framework introduced in arXiv:1611.05424 to\ndifferentiate instances of objects. Our work is however different in that it\nderives the embeddings from a lightweight segmentation network and, more\nfundamentally, because it considers the assignment of the same embedding to\nunconnected pixels, as required by pixels of distinct players from the same\nteam. Excellent results, both in terms of team labelling accuracy and\ngeneralization to new games/arenas, have been achieved on panoramic views of a\nlarge variety of basketball games involving players interactions and\nocclusions. This makes our method a good candidate to integrate team separation\nin many CNN-based sport analytics pipelines.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 20:20:12 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Istasse", "Maxime", ""], ["Moreau", "Julien", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "1907.01062", "submitter": "Gustavo Borges Moreno E Mello", "authors": "Gustavo Borges Moreno e Mello, Vibeke Devold Valderhaug, Sidney\n  Pontes-Filho, Evi Zouganeli, Ioanna Sandvig and Stefano Nichele", "title": "DeepTEGINN: Deep Learning Based Tools to Extract Graphs from Images of\n  Neural Networks", "comments": "5 pages, 2 figures, ICDL IEEE conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the brain, the structure of a network of neurons defines how these neurons\nimplement the computations that underlie the mind and the behavior of animals\nand humans. Provided that we can describe the network of neurons as a graph, we\ncan employ methods from graph theory to investigate its structure or use\ncellular automata to mathematically assess its function. Although, software for\nthe analysis of graphs and cellular automata are widely available. Graph\nextraction from the image of networks of brain cells remains difficult. Nervous\ntissue is heterogeneous, and differences in anatomy may reflect relevant\ndifferences in function. Here we introduce a deep learning based toolbox to\nextracts graphs from images of brain tissue. This toolbox provides an\neasy-to-use framework allowing system neuroscientists to generate graphs based\non images of brain tissue by combining methods from image processing, deep\nlearning, and graph theory. The goals are to simplify the training and usage of\ndeep learning methods for computer vision and facilitate its integration into\ngraph extraction pipelines. In this way, the toolbox provides an alternative to\nthe required laborious manual process of tracing, sorting and classifying. We\nexpect to democratize the machine learning methods to a wider community of\nusers beyond the computer vision experts and improve the time-efficiency of\ngraph extraction from large brain image datasets, which may lead to further\nunderstanding of the human mind.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 20:33:08 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Mello", "Gustavo Borges Moreno e", ""], ["Valderhaug", "Vibeke Devold", ""], ["Pontes-Filho", "Sidney", ""], ["Zouganeli", "Evi", ""], ["Sandvig", "Ioanna", ""], ["Nichele", "Stefano", ""]]}, {"id": "1907.01085", "submitter": "Srinath Sridhar", "authors": "Srinath Sridhar, Davis Rempe, Julien Valentin, Sofien Bouaziz,\n  Leonidas J. Guibas", "title": "Multiview Aggregation for Learning Category-Specific Shape\n  Reconstruction", "comments": "In Proceedings of Advances in Neural Information Processing Systems\n  (NeurIPS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of learning category-specific 3D shape\nreconstruction from a variable number of RGB views of previously unobserved\nobject instances. Most approaches for multiview shape reconstruction operate on\nsparse shape representations, or assume a fixed number of views. We present a\nmethod that can estimate dense 3D shape, and aggregate shape across multiple\nand varying number of input views. Given a single input view of an object\ninstance, we propose a representation that encodes the dense shape of the\nvisible object surface as well as the surface behind line of sight occluded by\nthe visible surface. When multiple input views are available, the shape\nrepresentation is designed to be aggregated into a single 3D shape using an\ninexpensive union operation. We train a 2D CNN to learn to predict this\nrepresentation from a variable number of views (1 or more). We further\naggregate multiview information by using permutation equivariant layers that\npromote order-agnostic view information exchange at the feature level.\nExperiments show that our approach is able to produce dense 3D reconstructions\nof objects that improve in quality as more views are added.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 22:01:37 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 22:01:21 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Sridhar", "Srinath", ""], ["Rempe", "Davis", ""], ["Valentin", "Julien", ""], ["Bouaziz", "Sofien", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1907.01102", "submitter": "Ravimal Bandara", "authors": "Ravimal Bandara, Lochandaka Ranathunga, Nor Aniza Abdullah", "title": "Nature Inspired Dimensional Reduction Technique for Fast and Invariant\n  Visual Feature Extraction", "comments": "11 pages, 11 figures, IJTCSE", "journal-ref": "International Journal of Advanced Trends in Computer Science and\n  Engineering (IJATCSE) Vol.8 No.3 (2019) 696-706", "doi": "10.30534/ijatcse/2019/57832019", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and invariant feature extraction is crucial in certain computer vision\napplications where the computation time is constrained in both training and\ntesting phases of the classifier. In this paper, we propose a nature-inspired\ndimensionality reduction technique for fast and invariant visual feature\nextraction. The human brain can exchange the spatial and spectral resolution to\nreconstruct missing colors in visual perception. The phenomenon is widely used\nin the printing industry to reduce the number of colors used to print, through\na technique, called color dithering. In this work, we adopt a fast\nerror-diffusion color dithering algorithm to reduce the spectral resolution and\nextract salient features by employing novel Hessian matrix analysis technique,\nwhich is then described by a spatial-chromatic histogram. The computation time,\ndescriptor dimensionality and classification performance of the proposed\nfeature are assessed under drastic variances in orientation, viewing angle and\nillumination of objects comparing with several different state-of-the-art\nhandcrafted and deep-learned features. Extensive experiments on two publicly\navailable object datasets, coil-100 and ALOI carried on both a desktop PC and a\nRaspberry Pi device show multiple advantages of using the proposed approach,\nsuch as the lower computation time, high robustness, and comparable\nclassification accuracy under weakly supervised environment. Further, it showed\nthe capability of operating solely inside a conventional SoC device utilizing a\nsmall fraction of the available hardware resources.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 16:58:45 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Bandara", "Ravimal", ""], ["Ranathunga", "Lochandaka", ""], ["Abdullah", "Nor Aniza", ""]]}, {"id": "1907.01108", "submitter": "Chaitanya Ahuja", "authors": "Chaitanya Ahuja and Louis-Philippe Morency", "title": "Language2Pose: Natural Language Grounded Pose Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating animations from natural language sentences finds its applications\nin a a number of domains such as movie script visualization, virtual human\nanimation and, robot motion planning. These sentences can describe different\nkinds of actions, speeds and direction of these actions, and possibly a target\ndestination. The core modeling challenge in this language-to-pose application\nis how to map linguistic concepts to motion animations.\n  In this paper, we address this multimodal problem by introducing a neural\narchitecture called Joint Language to Pose (or JL2P), which learns a joint\nembedding of language and pose. This joint embedding space is learned\nend-to-end using a curriculum learning approach which emphasizes shorter and\neasier sequences first before moving to longer and harder ones. We evaluate our\nproposed model on a publicly available corpus of 3D pose data and\nhuman-annotated sentences. Both objective metrics and human judgment evaluation\nconfirm that our proposed approach is able to generate more accurate animations\nand are deemed visually more representative by humans than other data driven\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 00:38:44 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 19:06:42 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ahuja", "Chaitanya", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1907.01113", "submitter": "Xiongjun Zhang", "authors": "Guangjing Song, Michael K. Ng, and Xiongjun Zhang", "title": "Robust Tensor Completion Using Transformed Tensor SVD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study robust tensor completion by using transformed tensor\nsingular value decomposition (SVD), which employs unitary transform matrices\ninstead of discrete Fourier transform matrix that is used in the traditional\ntensor SVD. The main motivation is that a lower tubal rank tensor can be\nobtained by using other unitary transform matrices than that by using discrete\nFourier transform matrix. This would be more effective for robust tensor\ncompletion. Experimental results for hyperspectral, video and face datasets\nhave shown that the recovery performance for the robust tensor completion\nproblem by using transformed tensor SVD is better in PSNR than that by using\nFourier transform and other robust tensor completion methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 00:50:31 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Song", "Guangjing", ""], ["Ng", "Michael K.", ""], ["Zhang", "Xiongjun", ""]]}, {"id": "1907.01131", "submitter": "Ya-Liang Chang", "authors": "Ya-Liang Chang, Zhe Yu Liu, Kuan-Ying Lee and Winston Hsu", "title": "Learnable Gated Temporal Shift Module for Deep Video Inpainting", "comments": "Accepted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to efficiently utilize temporal information to recover videos in a\nconsistent way is the main issue for video inpainting problems. Conventional 2D\nCNNs have achieved good performance on image inpainting but often lead to\ntemporally inconsistent results where frames will flicker when applied to\nvideos (see\nhttps://www.youtube.com/watch?v=87Vh1HDBjD0&list=PLPoVtv-xp_dL5uckIzz1PKwNjg1yI0I94&index=1);\n3D CNNs can capture temporal information but are computationally intensive and\nhard to train. In this paper, we present a novel component termed Learnable\nGated Temporal Shift Module (LGTSM) for video inpainting models that could\neffectively tackle arbitrary video masks without additional parameters from 3D\nconvolutions. LGTSM is designed to let 2D convolutions make use of neighboring\nframes more efficiently, which is crucial for video inpainting. Specifically,\nin each layer, LGTSM learns to shift some channels to its temporal neighbors so\nthat 2D convolutions could be enhanced to handle temporal information.\nMeanwhile, a gated convolution is applied to the layer to identify the masked\nareas that are poisoning for conventional convolutions. On the FaceForensics\nand Free-form Video Inpainting (FVI) dataset, our model achieves\nstate-of-the-art results with simply 33% of parameters and inference time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 02:38:24 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 07:16:48 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Chang", "Ya-Liang", ""], ["Liu", "Zhe Yu", ""], ["Lee", "Kuan-Ying", ""], ["Hsu", "Winston", ""]]}, {"id": "1907.01141", "submitter": "Yao Guo GGGyy", "authors": "Qing Song, Yao Guo, Jianan Jiang, Chun Liu, Mengjie Hu", "title": "High-speed Railway Fastener Detection and Localization Method based on\n  convolutional neural network", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Railway transportation is the artery of China's national economy and plays an\nimportant role in the development of today's society. Due to the late start of\nChina's railway security inspection technology, the current railway security\ninspection tasks mainly rely on manual inspection, but the manual inspection\nefficiency is low, and a lot of manpower and material resources are needed. In\nthis paper, we establish a steel rail fastener detection image dataset, which\ncontains 4,000 rail fastener pictures about 4 types. We use the regional\nsuggestion network to generate the region of interest, extracts the features\nusing the convolutional neural network, and fuses the classifier into the\ndetection network. With online hard sample mining to improve the accuracy of\nthe model, we optimize the Faster RCNN detection framework by reducing the\nnumber of regions of interest. Finally, the model accuracy reaches 99% and the\nspeed reaches 35FPS in the deployment environment of TITAN X GPU.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 03:11:57 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 06:51:58 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Song", "Qing", ""], ["Guo", "Yao", ""], ["Jiang", "Jianan", ""], ["Liu", "Chun", ""], ["Hu", "Mengjie", ""]]}, {"id": "1907.01144", "submitter": "Honglun Zhang", "authors": "Honglun Zhang, Wenqing Chen, Hao He, Yaohui Jin", "title": "Disentangled Makeup Transfer with Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial makeup transfer is a widely-used technology that aims to transfer the\nmakeup style from a reference face image to a non-makeup face. Existing\nliterature leverage the adversarial loss so that the generated faces are of\nhigh quality and realistic as real ones, but are only able to produce fixed\noutputs. Inspired by recent advances in disentangled representation, in this\npaper we propose DMT (Disentangled Makeup Transfer), a unified generative\nadversarial network to achieve different scenarios of makeup transfer. Our\nmodel contains an identity encoder as well as a makeup encoder to disentangle\nthe personal identity and the makeup style for arbitrary face images. Based on\nthe outputs of the two encoders, a decoder is employed to reconstruct the\noriginal faces. We also apply a discriminator to distinguish real faces from\nfake ones. As a result, our model can not only transfer the makeup styles from\none or more reference face images to a non-makeup face with controllable\nstrength, but also produce various outputs with styles sampled from a prior\ndistribution. Extensive experiments demonstrate that our model is superior to\nexisting literature by generating high-quality results for different scenarios\nof makeup transfer.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 03:19:07 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Zhang", "Honglun", ""], ["Chen", "Wenqing", ""], ["He", "Hao", ""], ["Jin", "Yaohui", ""]]}, {"id": "1907.01150", "submitter": "Chao Zhang", "authors": "Yi Zhang, Chao Zhang, Takuya Akashi", "title": "Multi-scale Template Matching with Scalable Diversity Similarity in an\n  Unconstrained Environment", "comments": "British Machine Vision Conference (BMVC2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel multi-scale template matching method which is robust\nagainst both scaling and rotation in unconstrained environments. The key\ncomponent behind is a similarity measure referred to as scalable diversity\nsimilarity (SDS). Specifically, SDS exploits bidirectional diversity of the\nnearest neighbor (NN) matches between two sets of points. To address the\nscale-robustness of the similarity measure, local appearance and rank\ninformation are jointly used for the NN search. Furthermore, by introducing\npenalty term on the scale change, and polar radius term into the similarity\nmeasure, SDS is shown to be a well-performing similarity measure against\noverall size and rotation changes, as well as non-rigid geometric deformations,\nbackground clutter, and occlusions. The properties of SDS are statistically\njustified, and experiments on both synthetic and real-world data show that SDS\ncan significantly outperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 03:49:05 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Zhang", "Yi", ""], ["Zhang", "Chao", ""], ["Akashi", "Takuya", ""]]}, {"id": "1907.01172", "submitter": "Chien-Yi Chang", "authors": "Chien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li Fei-Fei, Juan\n  Carlos Niebles", "title": "Procedure Planning in Instructional Videos", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of procedure planning in instructional\nvideos, which can be seen as a step towards enabling autonomous agents to plan\nfor complex tasks in everyday settings such as cooking. Given the current\nvisual observation of the world and a visual goal, we ask the question \"What\nactions need to be taken in order to achieve the goal?\". The key technical\nchallenge is to learn structured and plannable state and action spaces directly\nfrom unstructured videos. We address this challenge by proposing Dual Dynamics\nNetworks (DDN), a framework that explicitly leverages the structured priors\nimposed by the conjugate relationships between states and actions in a learned\nplannable latent space. We evaluate our method on real-world instructional\nvideos. Our experiments show that DDN learns plannable representations that\nlead to better planning performance compared to existing planning approaches\nand neural network policies.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 05:17:44 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 20:51:25 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 05:49:55 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Chang", "Chien-Yi", ""], ["Huang", "De-An", ""], ["Xu", "Danfei", ""], ["Adeli", "Ehsan", ""], ["Fei-Fei", "Li", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1907.01176", "submitter": "Noor Al-Shakarji", "authors": "Noor Al-Shakarji, Filiz Bunyak, Hadi Aliakbarpour, Guna Seetharaman,\n  Kannappan Palaniappan", "title": "Multi-Cue Vehicle Detection for Semantic Video Compression In\n  Georegistered Aerial Videos", "comments": "CVPR2019 Workshop on Detecting Objects in Aerial Images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of moving objects such as vehicles in videos acquired from an\nairborne camera is very useful for video analytics applications. Using fast low\npower algorithms for onboard moving object detection would also provide region\nof interest-based semantic information for scene content aware image\ncompression. This would enable more efficient and flexible communication link\nutilization in lowbandwidth airborne cloud computing networks. Despite recent\nadvances in both UAV or drone platforms and imaging sensor technologies,\nvehicle detection from aerial video remains challenging due to small object\nsizes, platform motion and camera jitter, obscurations, scene complexity and\ndegraded imaging conditions. This paper proposes an efficient moving vehicle\ndetection pipeline which synergistically fuses both appearance and motion-based\ndetections in a complementary manner using deep learning combined with flux\ntensor spatio-temporal filtering. Our proposed multi-cue pipeline is able to\ndetect moving vehicles with high precision and recall, while filtering out\nfalse positives such as parked vehicles, through intelligent fusion.\nExperimental results show that incorporating contextual information of moving\nvehicles enables high semantic compression ratios of over 100:1 with high image\nfidelity, for better utilization of limited bandwidth air-to-ground network\nlinks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 05:40:02 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Al-Shakarji", "Noor", ""], ["Bunyak", "Filiz", ""], ["Aliakbarpour", "Hadi", ""], ["Seetharaman", "Guna", ""], ["Palaniappan", "Kannappan", ""]]}, {"id": "1907.01187", "submitter": "Yong Man Ro", "authors": "Minho Park, Hak Gu Kim, Yong Man Ro", "title": "Generative Guiding Block: Synthesizing Realistic Looking Variants\n  Capable of Even Large Change Demands", "comments": "This work is accepted in ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic image synthesis is to generate an image that is perceptually\nindistinguishable from an actual image. Generating realistic looking images\nwith large variations (e.g., large spatial deformations and large pose change),\nhowever, is very challenging. Handing large variations as well as preserving\nappearance needs to be taken into account in the realistic looking image\ngeneration. In this paper, we propose a novel realistic looking image synthesis\nmethod, especially in large change demands. To do that, we devise generative\nguiding blocks. The proposed generative guiding block includes realistic\nappearance preserving discriminator and naturalistic variation transforming\ndiscriminator. By taking the proposed generative guiding blocks into generative\nmodel, the latent features at the layer of generative model are enhanced to\nsynthesize both realistic looking- and target variation- image. With\nqualitative and quantitative evaluation in experiments, we demonstrated the\neffectiveness of the proposed generative guiding blocks, compared to the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 06:24:21 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Park", "Minho", ""], ["Kim", "Hak Gu", ""], ["Ro", "Yong Man", ""]]}, {"id": "1907.01193", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A. Sindagi and Vishal M. Patel", "title": "Inverse Attention Guided Deep Crowd Counting Network", "comments": "Accepted at 16th IEEE International Conference on Advanced Video and\n  Signal-based Surveillance (AVSS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the challenging problem of crowd counting in\ncongested scenes. Specifically, we present Inverse Attention Guided Deep Crowd\nCounting Network (IA-DCCN) that efficiently infuses segmentation information\nthrough an inverse attention mechanism into the counting network, resulting in\nsignificant improvements. The proposed method, which is based on VGG-16, is a\nsingle-step training framework and is simple to implement. The use of\nsegmentation information results in minimal computational overhead and does not\nrequire any additional annotations. We demonstrate the significance of\nsegmentation guided inverse attention through a detailed analysis and ablation\nstudy. Furthermore, the proposed method is evaluated on three challenging crowd\ncounting datasets and is shown to achieve significant improvements over several\nrecent methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 06:48:18 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 03:25:42 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1907.01195", "submitter": "Dan Oneata", "authors": "Dan Oneata, Horia Cucu", "title": "Kite: Automatic speech recognition for unmanned aerial vehicles", "comments": "5 pages, accepted at Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper addresses the problem of building a speech recognition system\nattuned to the control of unmanned aerial vehicles (UAVs). Even though UAVs are\nbecoming widespread, the task of creating voice interfaces for them is largely\nunaddressed. To this end, we introduce a multi-modal evaluation dataset for UAV\ncontrol, consisting of spoken commands and associated images, which represent\nthe visual context of what the UAV \"sees\" when the pilot utters the command. We\nprovide baseline results and address two research directions: (i) how robust\nthe language models are, given an incomplete list of commands at train time;\n(ii) how to incorporate visual information in the language model. We find that\nrecurrent neural networks (RNNs) are a solution to both tasks: they can be\nsuccessfully adapted using a small number of commands and they can be extended\nto use visual cues. Our results show that the image-based RNN outperforms its\ntext-only counterpart even if the command-image training associations are\nautomatically generated and inherently imperfect. The dataset and our code are\navailable at http://kite.speed.pub.ro.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 06:50:24 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Oneata", "Dan", ""], ["Cucu", "Horia", ""]]}, {"id": "1907.01203", "submitter": "Qiang Zhou", "authors": "Qiang Zhou, Zilong Huang, Lichao Huang, Yongchao Gong, Han Shen, Chang\n  Huang, Wenyu Liu, Xinggang Wang", "title": "Proposal, Tracking and Segmentation (PTS): A Cascaded Network for Video\n  Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object segmentation (VOS) aims at pixel-level object tracking given\nonly the annotations in the first frame. Due to the large visual variations of\nobjects in video and the lack of training samples, it remains a difficult task\ndespite the upsurging development of deep learning. Toward solving the VOS\nproblem, we bring in several new insights by the proposed unified framework\nconsisting of object proposal, tracking and segmentation components. The object\nproposal network transfers objectness information as generic knowledge into\nVOS; the tracking network identifies the target object from the proposals; and\nthe segmentation network is performed based on the tracking results with a\nnovel dynamic-reference based model adaptation scheme. Extensive experiments\nhave been conducted on the DAVIS'17 dataset and the YouTube-VOS dataset, our\nmethod achieves the state-of-the-art performance on several video object\nsegmentation benchmarks. We make the code publicly available at\nhttps://github.com/sydney0zq/PTSNet.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 07:22:58 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 05:03:38 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Zhou", "Qiang", ""], ["Huang", "Zilong", ""], ["Huang", "Lichao", ""], ["Gong", "Yongchao", ""], ["Shen", "Han", ""], ["Huang", "Chang", ""], ["Liu", "Wenyu", ""], ["Wang", "Xinggang", ""]]}, {"id": "1907.01227", "submitter": "Chae Young Lee Ms.", "authors": "Chae Young Lee, Youngmin Baek, and Hwalsuk Lee", "title": "TedEval: A Fair Evaluation Metric for Scene Text Detectors", "comments": "7 pages, 10 figures, Accepted by Workshop on Industrial Applications\n  of Document Analysis and Recognition 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of scene text detection methods, common evaluation\nmetrics fail to provide a fair and reliable comparison among detectors. They\nhave obvious drawbacks in reflecting the inherent characteristic of text\ndetection tasks, unable to address issues such as granularity, multiline, and\ncharacter incompleteness. In this paper, we propose a novel evaluation protocol\ncalled TedEval (Text detector Evaluation), which evaluates text detections by\nan instance-level matching and a character-level scoring. Based on a firm\nstandard rewarding behaviors that result in successful recognition, TedEval can\nact as a reliable standard for comparing and quantizing the detection quality\nthroughout all difficulty levels. In this regard, we believe that TedEval can\nplay a key role in developing state-of-the-art scene text detectors. The code\nis publicly available at https://github.com/clovaai/TedEval.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 08:19:20 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Lee", "Chae Young", ""], ["Baek", "Youngmin", ""], ["Lee", "Hwalsuk", ""]]}, {"id": "1907.01262", "submitter": "Huidong Xie", "authors": "Huidong Xie, Hongming Shan, Wenxiang Cong, Xiaohua Zhang, Shaohua Liu,\n  Ruola Ning, Ge Wang", "title": "Dual Network Architecture for Few-view CT -- Trained on ImageNet Data\n  and Transferred for Medical Imaging", "comments": "11 pages, 5 figures, 2019 SPIE Optical Engineering + Applications", "journal-ref": null, "doi": "10.1117/12.2531198", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  X-ray computed tomography (CT) reconstructs cross-sectional images from\nprojection data. However, ionizing X-ray radiation associated with CT scanning\nmight induce cancer and genetic damage. Therefore, the reduction of radiation\ndose has attracted major attention. Few-view CT image reconstruction is an\nimportant topic to reduce the radiation dose. Recently, data-driven algorithms\nhave shown great potential to solve the few-view CT problem. In this paper, we\ndevelop a dual network architecture (DNA) for reconstructing images directly\nfrom sinograms. In the proposed DNA method, a point-based fully-connected layer\nlearns the backprojection process requesting significantly less memory than the\nprior arts do. Proposed method uses O(C*N*N_c) parameters where N and N_c\ndenote the dimension of reconstructed images and number of projections\nrespectively. C is an adjustable parameter that can be set as low as 1. Our\nexperimental results demonstrate that DNA produces a competitive performance\nover the other state-of-the-art methods. Interestingly, natural images can be\nused to pre-train DNA to avoid overfitting when the amount of real patient\nimages is limited.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 09:46:27 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 15:23:10 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 19:05:59 GMT"}, {"version": "v4", "created": "Wed, 17 Jul 2019 17:20:07 GMT"}, {"version": "v5", "created": "Sun, 25 Aug 2019 05:01:59 GMT"}, {"version": "v6", "created": "Thu, 12 Sep 2019 12:49:18 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Xie", "Huidong", ""], ["Shan", "Hongming", ""], ["Cong", "Wenxiang", ""], ["Zhang", "Xiaohua", ""], ["Liu", "Shaohua", ""], ["Ning", "Ruola", ""], ["Wang", "Ge", ""]]}, {"id": "1907.01268", "submitter": "Chen Chen", "authors": "Chen Chen, Wenjia Bai, Rhodri H. Davies, Anish N. Bhuva, Charlotte\n  Manisty, James C. Moon, Nay Aung, Aaron M. Lee, Mihir M. Sanghvi, Kenneth\n  Fung, Jose Miguel Paiva, Steffen E. Petersen, Elena Lukaschuk, Stefan K.\n  Piechnik, Stefan Neubauer, Daniel Rueckert", "title": "Improving the generalizability of convolutional neural network-based\n  segmentation on CMR images", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": "10.3389/fcvm.2020.00105", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) based segmentation methods provide an\nefficient and automated way for clinicians to assess the structure and function\nof the heart in cardiac MR images. While CNNs can generally perform the\nsegmentation tasks with high accuracy when training and test images come from\nthe same domain (e.g. same scanner or site), their performance often degrades\ndramatically on images from different scanners or clinical sites. We propose a\nsimple yet effective way for improving the network generalization ability by\ncarefully designing data normalization and augmentation strategies to\naccommodate common scenarios in multi-site, multi-scanner clinical imaging data\nsets. We demonstrate that a neural network trained on a single-site\nsingle-scanner dataset from the UK Biobank can be successfully applied to\nsegmenting cardiac MR images across different sites and different scanners\nwithout substantial loss of accuracy. Specifically, the method was trained on a\nlarge set of 3,975 subjects from the UK Biobank. It was then directly tested on\n600 different subjects from the UK Biobank for intra-domain testing and two\nother sets for cross-domain testing: the ACDC dataset (100 subjects, 1 site, 2\nscanners) and the BSCMR-AS dataset (599 subjects, 6 sites, 9 scanners). The\nproposed method produces promising segmentation results on the UK Biobank test\nset which are comparable to previously reported values in the literature, while\nalso performing well on cross-domain test sets, achieving a mean Dice metric of\n0.90 for the left ventricle, 0.81 for the myocardium and 0.82 for the right\nventricle on the ACDC dataset; and 0.89 for the left ventricle, 0.83 for the\nmyocardium on the BSCMR-AS dataset. The proposed method offers a potential\nsolution to improve CNN-based model generalizability for the cross-scanner and\ncross-site cardiac MR image segmentation task.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 09:57:15 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 10:15:20 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Chen", ""], ["Bai", "Wenjia", ""], ["Davies", "Rhodri H.", ""], ["Bhuva", "Anish N.", ""], ["Manisty", "Charlotte", ""], ["Moon", "James C.", ""], ["Aung", "Nay", ""], ["Lee", "Aaron M.", ""], ["Sanghvi", "Mihir M.", ""], ["Fung", "Kenneth", ""], ["Paiva", "Jose Miguel", ""], ["Petersen", "Steffen E.", ""], ["Lukaschuk", "Elena", ""], ["Piechnik", "Stefan K.", ""], ["Neubauer", "Stefan", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1907.01273", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran and Oswald Lanz", "title": "An Analysis of Deep Neural Networks with Attention for Action\n  Recognition from a Neurophysiological Perspective", "comments": "Presented as an extended abstract in the Mutual benefits of cognitive\n  and computer vision (MBCCV) workshop, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review three recent deep learning based methods for action recognition and\npresent a brief comparative analysis of the methods from a neurophyisiological\npoint of view. We posit that there are some analogy between the three presented\ndeep learning based methods and some of the existing hypotheses regarding the\nfunctioning of human brain.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 10:07:06 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Lanz", "Oswald", ""]]}, {"id": "1907.01284", "submitter": "Rajesh Shreedhar Bhat", "authors": "Pranay Dugar, Anirban Chatterjee, Rajesh Shreedhar Bhat, Saswata Sahoo", "title": "Semi-Bagging Based Deep Neural Architecture to Extract Text from High\n  Entropy Images", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting texts of various size and shape from images containing multiple\nobjects is an important problem in many contexts, especially, in connection to\ne-commerce, augmented reality assistance system in natural scene, etc. The\nexisting works (based on only CNN) often perform sub-optimally when the image\ncontains regions of high entropy having multiple objects. This paper presents\nan end-to-end text detection strategy combining a segmentation algorithm and an\nensemble of multiple text detectors of different types to detect text in every\nindividual image segments independently. The proposed strategy involves a\nsuper-pixel based image segmenter which splits an image into multiple regions.\nA convolutional deep neural architecture is developed which works on each of\nthe segments and detects texts of multiple shapes, sizes, and structures. It\noutperforms the competing methods in terms of coverage in detecting texts in\nimages especially the ones where the text of various types and sizes are\ncompacted in a small region along with various other objects. Furthermore, the\nproposed text detection method along with a text recognizer outperforms the\nexisting state-of-the-art approaches in extracting text from high entropy\nimages. We validate the results on a dataset consisting of product images on an\ne-commerce website.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 10:26:14 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Dugar", "Pranay", ""], ["Chatterjee", "Anirban", ""], ["Bhat", "Rajesh Shreedhar", ""], ["Sahoo", "Saswata", ""]]}, {"id": "1907.01294", "submitter": "Fabio Pizzati", "authors": "Fabio Pizzati, Marco Allodi, Alejandro Barrera, Fernando Garc\\'ia", "title": "Lane Detection and Classification using Cascaded CNNs", "comments": "Presented at Eurocast 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Lane detection is extremely important for autonomous vehicles. For this\nreason, many approaches use lane boundary information to locate the vehicle\ninside the street, or to integrate GPS-based localization. As many other\ncomputer vision based tasks, convolutional neural networks (CNNs) represent the\nstate-of-the-art technology to indentify lane boundaries. However, the position\nof the lane boundaries w.r.t. the vehicle may not suffice for a reliable\npositioning, as for path planning or localization information regarding lane\ntypes may also be needed. In this work, we present an end-to-end system for\nlane boundary identification, clustering and classification, based on two\ncascaded neural networks, that runs in real-time. To build the system, 14336\nlane boundaries instances of the TuSimple dataset for lane detection have been\nlabelled using 8 different classes. Our dataset and the code for inference are\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 10:54:06 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 18:52:21 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Pizzati", "Fabio", ""], ["Allodi", "Marco", ""], ["Barrera", "Alejandro", ""], ["Garc\u00eda", "Fernando", ""]]}, {"id": "1907.01296", "submitter": "Yujiang Wang", "authors": "Yujiang Wang, Mingzhi Dong, Jie Shen, Yang Wu, Shiyang Cheng, Maja\n  Pantic", "title": "Dynamic Face Video Segmentation via Reinforcement Learning", "comments": "CVPR 2020. 300VW with segmentation labels is available at:\n  https://github.com/mapleandfire/300VW-Mask", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00699", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For real-time semantic video segmentation, most recent works utilised a\ndynamic framework with a key scheduler to make online key/non-key decisions.\nSome works used a fixed key scheduling policy, while others proposed adaptive\nkey scheduling methods based on heuristic strategies, both of which may lead to\nsuboptimal global performance. To overcome this limitation, we model the online\nkey decision process in dynamic video segmentation as a deep reinforcement\nlearning problem and learn an efficient and effective scheduling policy from\nexpert information about decision history and from the process of maximising\nglobal return. Moreover, we study the application of dynamic video segmentation\non face videos, a field that has not been investigated before. By evaluating on\nthe 300VW dataset, we show that the performance of our reinforcement key\nscheduler outperforms that of various baselines in terms of both effective key\nselections and running speed. Further results on the Cityscapes dataset\ndemonstrate that our proposed method can also generalise to other scenarios. To\nthe best of our knowledge, this is the first work to use reinforcement learning\nfor online key-frame decision in dynamic video segmentation, and also the first\nwork on its application on face videos.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 11:07:26 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 16:19:11 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 11:30:26 GMT"}, {"version": "v4", "created": "Sat, 27 Feb 2021 17:27:53 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Yujiang", ""], ["Dong", "Mingzhi", ""], ["Shen", "Jie", ""], ["Wu", "Yang", ""], ["Cheng", "Shiyang", ""], ["Pantic", "Maja", ""]]}, {"id": "1907.01301", "submitter": "Siyuan Yan", "authors": "Dawei Li, Siyuan Yan, Xin Cai, Yan Cao, Sifan Wang", "title": "An Integrated Image Filter for Enhancing Change Detection Results", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2019.2927255", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection is a fundamental task in computer vision. Despite\nsignificant advances have been made, most of the change detection methods fail\nto work well in challenging scenes due to ubiquitous noise and interferences.\nNowadays, post-processing methods (e.g. MRF, and CRF) aiming to enhance the\nbinary change detection results still fall short of the requirements on\nuniversality for distinctive scenes, applicability for different types of\ndetection methods, accuracy, and real-time performance. Inspired by the nature\nof image filtering, which separates noise from pixel observations and recovers\nthe real structure of patches, we consider utilizing image filters to enhance\nthe detection masks. In this paper, we present an integrated filter which\ncomprises a weighted local guided image filter and a weighted spatiotemporal\ntree filter. The spatiotemporal tree filter leverages the global spatiotemporal\ninformation of adjacent video frames and meanwhile the guided filter carries\nout local window filtering of pixels, for enhancing the coarse change detection\nmasks. The main contributions are three: (i) the proposed filter can make full\nuse of the information of the same object in consecutive frames to improve its\ncurrent detection mask by computations on a spatiotemporal minimum spanning\ntree; (ii) the integrated filter possesses both advantages of local filtering\nand global filtering; it not only has good edge-preserving property but also\ncan handle heavily textured and colorful foreground regions; and (iii) Unlike\nsome popular enhancement methods (MRF, and CRF) that require either a priori\nbackground probabilities or a posteriori foreground probabilities for every\npixel to improve the coarse detection masks, our method is a versatile\nenhancement filter that can be applied after many different types of change\ndetection methods, and is particularly suitable for video sequences.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 11:32:35 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Li", "Dawei", ""], ["Yan", "Siyuan", ""], ["Cai", "Xin", ""], ["Cao", "Yan", ""], ["Wang", "Sifan", ""]]}, {"id": "1907.01307", "submitter": "Martin Ki\\v{s}\\v{s}", "authors": "Martin Ki\\v{s}\\v{s}, Michal Hradi\\v{s}, Old\\v{r}ich Kodym", "title": "Brno Mobile OCR Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Brno Mobile OCR Dataset (B-MOD) for document Optical\nCharacter Recognition from low-quality images captured by handheld mobile\ndevices. While OCR of high-quality scanned documents is a mature field where\nmany commercial tools are available, and large datasets of text in the wild\nexist, no existing datasets can be used to develop and test document OCR\nmethods robust to non-uniform lighting, image blur, strong noise, built-in\ndenoising, sharpening, compression and other artifacts present in many\nphotographs from mobile devices.\n  This dataset contains 2 113 unique pages from random scientific papers, which\nwere photographed by multiple people using 23 different mobile devices. The\nresulting 19 728 photographs of various visual quality are accompanied by\nprecise positions and text annotations of 500k text lines. We further provide\nan evaluation methodology, including an evaluation server and a testset with\nnon-public annotations.\n  We provide a state-of-the-art text recognition baseline build on\nconvolutional and recurrent neural networks trained with Connectionist Temporal\nClassification loss. This baseline achieves 2 %, 22 % and 73 % word error rates\non easy, medium and hard parts of the dataset, respectively, confirming that\nthe dataset is challenging.\n  The presented dataset will enable future development and evaluation of\ndocument analysis for low-quality images. It is primarily intended for\nline-level text recognition, and can be further used for line localization,\nlayout analysis, image restoration and text binarization.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 11:53:42 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Ki\u0161\u0161", "Martin", ""], ["Hradi\u0161", "Michal", ""], ["Kodym", "Old\u0159ich", ""]]}, {"id": "1907.01319", "submitter": "Jong Chul Ye", "authors": "Boah Kim, Jieun Kim, June-Goo Lee, Dong Hwan Kim, Seong Ho Park, Jong\n  Chul Ye", "title": "Unsupervised Deformable Image Registration Using Cycle-Consistent CNN", "comments": "accepted for MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image registration is one of the key processing steps for biomedical\nimage analysis such as cancer diagnosis. Recently, deep learning based\nsupervised and unsupervised image registration methods have been extensively\nstudied due to its excellent performance in spite of ultra-fast computational\ntime compared to the classical approaches. In this paper, we present a novel\nunsupervised medical image registration method that trains deep neural network\nfor deformable registration of 3D volumes using a cycle-consistency. Thanks to\nthe cycle consistency, the proposed deep neural networks can take diverse pair\nof image data with severe deformation for accurate registration. Experimental\nresults using multiphase liver CT images demonstrate that our method provides\nvery precise 3D image registration within a few seconds, resulting in more\naccurate cancer size estimation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 12:29:02 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Kim", "Boah", ""], ["Kim", "Jieun", ""], ["Lee", "June-Goo", ""], ["Kim", "Dong Hwan", ""], ["Park", "Seong Ho", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1907.01341", "submitter": "Ren\\'e Ranftl", "authors": "Ren\\'e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler,\n  Vladlen Koltun", "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot\n  Cross-dataset Transfer", "comments": "To appear in TPAMI (accepted August 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of monocular depth estimation relies on large and diverse\ntraining sets. Due to the challenges associated with acquiring dense\nground-truth depth across different environments at scale, a number of datasets\nwith distinct characteristics and biases have emerged. We develop tools that\nenable mixing multiple datasets during training, even if their annotations are\nincompatible. In particular, we propose a robust training objective that is\ninvariant to changes in depth range and scale, advocate the use of principled\nmulti-objective learning to combine data from different sources, and highlight\nthe importance of pretraining encoders on auxiliary tasks. Armed with these\ntools, we experiment with five diverse training datasets, including a new,\nmassive data source: 3D films. To demonstrate the generalization power of our\napproach we use zero-shot cross-dataset transfer}, i.e. we evaluate on datasets\nthat were not seen during training. The experiments confirm that mixing data\nfrom complementary sources greatly improves monocular depth estimation. Our\napproach clearly outperforms competing methods across diverse datasets, setting\na new state of the art for monocular depth estimation. Some results are shown\nin the supplementary video at https://youtu.be/D46FzVyL9I8\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 13:16:52 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 15:17:32 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 09:37:24 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Ranftl", "Ren\u00e9", ""], ["Lasinger", "Katrin", ""], ["Hafner", "David", ""], ["Schindler", "Konrad", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1907.01342", "submitter": "Robin Chan", "authors": "Robin Chan, Matthias Rottmann, Radin Dardashti, Fabian H\\\"uger, Peter\n  Schlicht and Hanno Gottschalk", "title": "The Ethical Dilemma when (not) Setting up Cost-based Decision Rules in\n  Semantic Segmentation", "comments": null, "journal-ref": "CVPR-W SAIAD 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks for semantic segmentation can be seen as statistical models\nthat provide for each pixel of one image a probability distribution on\npredefined classes. The predicted class is then usually obtained by the maximum\na-posteriori probability (MAP) which is known as Bayes rule in decision theory.\nFrom decision theory we also know that the Bayes rule is optimal regarding the\nsimple symmetric cost function. Therefore, it weights each type of confusion\nbetween two different classes equally, e.g., given images of urban street\nscenes there is no distinction in the cost function if the network confuses a\nperson with a street or a building with a tree. Intuitively, there might be\nconfusions of classes that are more important to avoid than others. In this\nwork, we want to raise awareness of the possibility of explicitly defining\nconfusion costs and the associated ethical difficulties if it comes down to\nproviding numbers. We define two cost functions from different extreme\nperspectives, an egoistic and an altruistic one, and show how safety relevant\nquantities like precision / recall and (segment-wise) false positive / negative\nrate change when interpolating between MAP, egoistic and altruistic decision\nrules.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 13:17:14 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Chan", "Robin", ""], ["Rottmann", "Matthias", ""], ["Dardashti", "Radin", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Gottschalk", "Hanno", ""]]}, {"id": "1907.01361", "submitter": "Matias Tassano", "authors": "Matias Tassano, Julie Delon, Thomas Veit", "title": "FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow\n  Estimation", "comments": "Code for this algorithm and results can be found in\n  https://github.com/m-tassano/fastdvdnet. arXiv admin note: text overlap with\n  arXiv:1906.11890", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a state-of-the-art video denoising algorithm based\non a convolutional neural network architecture. Until recently, video denoising\nwith neural networks had been a largely under explored domain, and existing\nmethods could not compete with the performance of the best patch-based methods.\nThe approach we introduce in this paper, called FastDVDnet, shows similar or\nbetter performance than other state-of-the-art competitors with significantly\nlower computing times. In contrast to other existing neural network denoisers,\nour algorithm exhibits several desirable properties such as fast runtimes, and\nthe ability to handle a wide range of noise levels with a single network model.\nThe characteristics of its architecture make it possible to avoid using a\ncostly motion compensation stage while achieving excellent performance. The\ncombination between its denoising performance and lower computational load\nmakes this algorithm attractive for practical denoising applications. We\ncompare our method with different state-of-art algorithms, both visually and\nwith respect to objective quality metrics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:10:34 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 18:29:49 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Tassano", "Matias", ""], ["Delon", "Julie", ""], ["Veit", "Thomas", ""]]}, {"id": "1907.01368", "submitter": "Kimmo Kartasalo", "authors": "Peter Str\\\"om (1), Kimmo Kartasalo (2), Henrik Olsson (1), Leslie\n  Solorzano (3), Brett Delahunt (4), Daniel M. Berney (5), David G. Bostwick\n  (6), Andrew J. Evans (7), David J. Grignon (8), Peter A. Humphrey (9),\n  Kenneth A. Iczkowski (10), James G. Kench (11), Glen Kristiansen (12),\n  Theodorus H. van der Kwast (7), Katia R.M. Leite (13), Jesse K. McKenney\n  (14), Jon Oxley (15), Chin-Chen Pan (16), Hemamali Samaratunga (17), John R.\n  Srigley (18), Hiroyuki Takahashi (19), Toyonori Tsuzuki (20), Murali Varma\n  (21), Ming Zhou (22), Johan Lindberg (1), Cecilia Bergstr\\\"om (23), Pekka\n  Ruusuvuori (2), Carolina W\\\"ahlby (3 and 24), Henrik Gr\\\"onberg (1 and 25),\n  Mattias Rantalainen (1), Lars Egevad (26), Martin Eklund (1) ((1) Department\n  of Medical Epidemiology and Biostatistics, Karolinska Institutet, Stockholm,\n  Sweden, (2) Faculty of Medicine and Health Technology, Tampere University,\n  Tampere, Finland, (3) Centre for Image Analysis, Department of Information\n  Technology, Uppsala University, Uppsala, Sweden, (4) Department of Pathology\n  and Molecular Medicine, Wellington School of Medicine and Health Sciences,\n  University of Otago, Wellington, New Zealand, (5) Barts Cancer Institute,\n  Queen Mary University of London, London, UK, (6) Bostwick Laboratories,\n  Orlando, FL, USA, (7) Laboratory Medicine Program, University Health Network,\n  Toronto General Hospital, Toronto, ON, Canada, (8) Department of Pathology\n  and Laboratory Medicine, Indiana University School of Medicine, Indianapolis,\n  IN, USA, (9) Department of Pathology, Yale University School of Medicine, New\n  Haven, CT, USA, (10) Department of Pathology, Medical College of Wisconsin,\n  Milwaukee, WI, USA, (11) Department of Tissue Pathology and Diagnostic\n  Oncology, Royal Prince Alfred Hospital and Central Clinical School,\n  University of Sydney, Sydney, NSW, Australia, (12) Institute of Pathology,\n  University Hospital Bonn, Bonn, Germany, (13) Department of Urology,\n  Laboratory of Medical Research, University of S\\~ao Paulo Medical School,\n  S\\~ao Paulo, Brazil, (14) Pathology and Laboratory Medicine Institute,\n  Cleveland Clinic, Cleveland, OH, USA, (15) Department of Cellular Pathology,\n  Southmead Hospital, Bristol, UK, (16) Department of Pathology, Taipei\n  Veterans General Hospital, Taipei, Taiwan, (17) Aquesta Uropathology and\n  University of Queensland, Brisbane, Qld, Australia, (18) Department of\n  Laboratory Medicine and Pathobiology, University of Toronto, Toronto, ON,\n  Canada, (19) Department of Pathology, Jikei University School of Medicine,\n  Tokyo, Japan, (20) Department of Surgical Pathology, School of Medicine,\n  Aichi Medical University, Nagoya, Japan, (21) Department of Cellular\n  Pathology, University Hospital of Wales, Cardiff, UK, (22) Department of\n  Pathology, UT Southwestern Medical Center, Dallas, TX, USA, (23) Department\n  of Immunology, Genetics and Pathology, Uppsala University, Uppsala, Sweden,\n  (24) BioImage Informatics Facility of SciLifeLab, Uppsala, Sweden, (25)\n  Department of Oncology, S:t G\\\"oran Hospital, Stockholm, Sweden, (26)\n  Department of Oncology and Pathology, Karolinska Institutet, Stockholm,\n  Sweden)", "title": "Pathologist-Level Grading of Prostate Biopsies with Artificial\n  Intelligence", "comments": "45 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: An increasing volume of prostate biopsies and a world-wide\nshortage of uro-pathologists puts a strain on pathology departments.\nAdditionally, the high intra- and inter-observer variability in grading can\nresult in over- and undertreatment of prostate cancer. Artificial intelligence\n(AI) methods may alleviate these problems by assisting pathologists to reduce\nworkload and harmonize grading.\n  Methods: We digitized 6,682 needle biopsies from 976 participants in the\npopulation based STHLM3 diagnostic study to train deep neural networks for\nassessing prostate biopsies. The networks were evaluated by predicting the\npresence, extent, and Gleason grade of malignant tissue for an independent test\nset comprising 1,631 biopsies from 245 men. We additionally evaluated grading\nperformance on 87 biopsies individually graded by 23 experienced urological\npathologists from the International Society of Urological Pathology. We\nassessed discriminatory performance by receiver operating characteristics (ROC)\nand tumor extent predictions by correlating predicted millimeter cancer length\nagainst measurements by the reporting pathologist. We quantified the\nconcordance between grades assigned by the AI and the expert urological\npathologists using Cohen's kappa.\n  Results: The performance of the AI to detect and grade cancer in prostate\nneedle biopsy samples was comparable to that of international experts in\nprostate pathology. The AI achieved an area under the ROC curve of 0.997 for\ndistinguishing between benign and malignant biopsy cores, and 0.999 for\ndistinguishing between men with or without prostate cancer. The correlation\nbetween millimeter cancer predicted by the AI and assigned by the reporting\npathologist was 0.96. For assigning Gleason grades, the AI achieved an average\npairwise kappa of 0.62. This was within the range of the corresponding values\nfor the expert pathologists (0.60 to 0.73).\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 13:52:02 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Str\u00f6m", "Peter", "", "3 and 24"], ["Kartasalo", "Kimmo", "", "3 and 24"], ["Olsson", "Henrik", "", "3 and 24"], ["Solorzano", "Leslie", "", "3 and 24"], ["Delahunt", "Brett", "", "3 and 24"], ["Berney", "Daniel M.", "", "3 and 24"], ["Bostwick", "David G.", "", "3 and 24"], ["Evans", "Andrew J.", "", "3 and 24"], ["Grignon", "David J.", "", "3 and 24"], ["Humphrey", "Peter A.", "", "3 and 24"], ["Iczkowski", "Kenneth A.", "", "3 and 24"], ["Kench", "James G.", "", "3 and 24"], ["Kristiansen", "Glen", "", "3 and 24"], ["van der Kwast", "Theodorus H.", "", "3 and 24"], ["Leite", "Katia R. M.", "", "3 and 24"], ["McKenney", "Jesse K.", "", "3 and 24"], ["Oxley", "Jon", "", "3 and 24"], ["Pan", "Chin-Chen", "", "3 and 24"], ["Samaratunga", "Hemamali", "", "3 and 24"], ["Srigley", "John R.", "", "3 and 24"], ["Takahashi", "Hiroyuki", "", "3 and 24"], ["Tsuzuki", "Toyonori", "", "3 and 24"], ["Varma", "Murali", "", "3 and 24"], ["Zhou", "Ming", "", "3 and 24"], ["Lindberg", "Johan", "", "3 and 24"], ["Bergstr\u00f6m", "Cecilia", "", "3 and 24"], ["Ruusuvuori", "Pekka", "", "3 and 24"], ["W\u00e4hlby", "Carolina", "", "3 and 24"], ["Gr\u00f6nberg", "Henrik", "", "1 and 25"], ["Rantalainen", "Mattias", ""], ["Egevad", "Lars", ""], ["Eklund", "Martin", ""]]}, {"id": "1907.01376", "submitter": "Hristina Uzunova", "authors": "Hristina Uzunova, Jan Ehrhardt, Fabian Jacob, Alex Frydrychowicz,\n  Heinz Handels", "title": "Multi-scale GANs for Memory-efficient Generation of High Resolution\n  Medical Images", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently generative adversarial networks (GANs) are rarely applied to\nmedical images of large sizes, especially 3D volumes, due to their large\ncomputational demand. We propose a novel multi-scale patch-based GAN approach\nto generate large high resolution 2D and 3D images. Our key idea is to first\nlearn a low-resolution version of the image and then generate patches of\nsuccessively growing resolutions conditioned on previous scales. In a domain\ntranslation use-case scenario, 3D thorax CTs of size 512x512x512 and thorax\nX-rays of size 2048x2048 are generated and we show that, due to the constant\nGPU memory demand of our method, arbitrarily large images of high resolution\ncan be generated. Moreover, compared to common patch-based approaches, our\nmulti-resolution scheme enables better image quality and prevents patch\nartifacts.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 13:59:24 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 09:09:17 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Uzunova", "Hristina", ""], ["Ehrhardt", "Jan", ""], ["Jacob", "Fabian", ""], ["Frydrychowicz", "Alex", ""], ["Handels", "Heinz", ""]]}, {"id": "1907.01377", "submitter": "Tak Ming Wong", "authors": "Tak Ming Wong, Matthias Kahl, Peter Haring Bol\\'ivar, Andreas Kolb,\n  Michael M\\\"oller", "title": "Training Auto-encoder-based Optimizers for Terahertz Image\n  Reconstruction", "comments": "This is a pre-print of a conference paper published in German\n  Conference on Pattern Recognition (GCPR) 2019", "journal-ref": "Pattern Recognition. DAGM GCPR 2019. Lecture Notes in Computer\n  Science, vol 11824. Springer, Cham", "doi": "10.1007/978-3-030-33676-9_7", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terahertz (THz) sensing is a promising imaging technology for a wide variety\nof different applications. Extracting the interpretable and physically\nmeaningful parameters for such applications, however, requires solving an\ninverse problem in which a model function determined by these parameters needs\nto be fitted to the measured data. Since the underlying optimization problem is\nnonconvex and very costly to solve, we propose learning the prediction of\nsuitable parameters from the measured data directly. More precisely, we develop\na model-based autoencoder in which the encoder network predicts suitable\nparameters and the decoder is fixed to a physically meaningful model function,\nsuch that we can train the encoding network in an unsupervised way. We\nillustrate numerically that the resulting network is more than 140 times faster\nthan classical optimization techniques while making predictions with only\nslightly higher objective values. Using such predictions as starting points of\nlocal optimization techniques allows us to converge to better local minima\nabout twice as fast as optimization without the network-based initialization.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 14:01:35 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 11:56:05 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Wong", "Tak Ming", ""], ["Kahl", "Matthias", ""], ["Bol\u00edvar", "Peter Haring", ""], ["Kolb", "Andreas", ""], ["M\u00f6ller", "Michael", ""]]}, {"id": "1907.01390", "submitter": "Fei Feng", "authors": "Fei Feng and Jiajia Luo", "title": "CSSegNet: Fine-Grained Cardiac Structures Segmentation Using Dilated\n  Pyramid Pooling in U-net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac structure segmentation plays an important role in medical analysis\nprocedures. Images' blurred boundaries issue always limits the segmentation\nperformance. To address this difficult problem, we presented a novel network\nstructure which embedded dilated pyramid pooling block in the skip connections\nbetween networks' encoding and decoding stage. A dilated pyramid pooling block\nis made up of convolutions and pooling operations with different vision scopes.\nEquipped the model with such module, it could be endowed with multi-scales\nvision ability. Together combining with other techniques, it included a\nmulti-scales initial features extraction and a multi-resolutions' prediction\naggregation module. As for backbone feature extraction network, we referred to\nthe basic idea of Xception network which benefited from separable convolutions.\nEvaluated on the Post 2017 MICCAI-ACDC challenge phase data, our proposed model\ncould achieve state-of-the-art performance in left ventricle (LVC) cavities and\nright ventricle cavities (RVC) segmentation tasks. Results revealed that our\nmethod has advantages on both geometrical (Dice coefficient, Hausdorff\ndistance) and clinical evaluation (Ejection Fraction, Volume), which represent\ncloser boundaries and more statistically significant separately.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 14:17:31 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Feng", "Fei", ""], ["Luo", "Jiajia", ""]]}, {"id": "1907.01399", "submitter": "Santiago L\\'opez-Tapia", "authors": "Santiago L\\'opez-Tapia and Alice Lucas and Rafael Molina and Aggelos\n  K. Katsaggelos", "title": "A Single Video Super-Resolution GAN for Multiple Downsampling Operators\n  based on Pseudo-Inverse Image Formation Models", "comments": null, "journal-ref": null, "doi": "10.1016/j.dsp.2020.102801", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of high and ultra-high definition displays has led to the need\nfor methods to improve the quality of videos already obtained at much lower\nresolutions. Current Video Super-Resolution methods are not robust to mismatch\nbetween training and testing degradation models since they are trained against\na single degradation model (usually bicubic downsampling). This causes their\nperformance to deteriorate in real-life applications. At the same time, the use\nof only the Mean Squared Error during learning causes the resulting images to\nbe too smooth. In this work we propose a new Convolutional Neural Network for\nvideo super resolution which is robust to multiple degradation models. During\ntraining, which is performed on a large dataset of scenes with slow and fast\nmotions, it uses the pseudo-inverse image formation model as part of the\nnetwork architecture in conjunction with perceptual losses, in addition to a\nsmoothness constraint that eliminates the artifacts originating from these\nperceptual losses. The experimental validation shows that our approach\noutperforms current state-of-the-art methods and is robust to multiple\ndegradations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 14:28:27 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["L\u00f3pez-Tapia", "Santiago", ""], ["Lucas", "Alice", ""], ["Molina", "Rafael", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1907.01406", "submitter": "Jwala Dhamala", "authors": "Jwala Dhamala, Sandesh Ghimire, John L. Sapp, B. Milan Horacek, Linwei\n  Wang", "title": "Bayesian Optimization on Large Graphs via a Graph Convolutional\n  Generative Model: Application in Cardiac Model Personalization", "comments": "9 pages, 5 figures, MICCAI", "journal-ref": null, "doi": "10.1007/978-3-030-32245-8_51", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization of cardiac models involves the optimization of organ tissue\nproperties that vary spatially over the non-Euclidean geometry model of the\nheart. To represent the high-dimensional (HD) unknown of tissue properties,\nmost existing works rely on a low-dimensional (LD) partitioning of the\ngeometrical model. While this exploits the geometry of the heart, it is of\nlimited expressiveness to allow partitioning that is small enough for effective\noptimization. Recently, a variational auto-encoder (VAE) was utilized as a more\nexpressive generative model to embed the HD optimization into the LD latent\nspace. Its Euclidean nature, however, neglects the rich geometrical information\nin the heart. In this paper, we present a novel graph convolutional VAE to\nallow generative modeling of non-Euclidean data, and utilize it to embed\nBayesian optimization of large graphs into a small latent space. This approach\nbridges the gap of previous works by introducing an expressive generative model\nthat is able to incorporate the knowledge of spatial proximity and hierarchical\ncompositionality of the underlying geometry. It further allows transferring of\nthe learned features across different geometries, which was not possible with a\nregular VAE. We demonstrate these benefits of the presented method in synthetic\nand real data experiments of estimating tissue excitability in a cardiac\nelectrophysiological model.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:47:21 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 16:01:35 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Dhamala", "Jwala", ""], ["Ghimire", "Sandesh", ""], ["Sapp", "John L.", ""], ["Horacek", "B. Milan", ""], ["Wang", "Linwei", ""]]}, {"id": "1907.01413", "submitter": "Sam Ribeiro", "authors": "Manuel Sam Ribeiro, Aciel Eshky, Korin Richmond and Steve Renals", "title": "Speaker-independent classification of phonetic segments from raw\n  ultrasound in child speech", "comments": "5 pages, 4 figures, published in ICASSP2019 (IEEE International\n  Conference on Acoustics, Speech and Signal Processing, 2019)", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8683564", "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.LG cs.SD eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound tongue imaging (UTI) provides a convenient way to visualize the\nvocal tract during speech production. UTI is increasingly being used for speech\ntherapy, making it important to develop automatic methods to assist various\ntime-consuming manual tasks currently performed by speech therapists. A key\nchallenge is to generalize the automatic processing of ultrasound tongue images\nto previously unseen speakers. In this work, we investigate the classification\nof phonetic segments (tongue shapes) from raw ultrasound recordings under\nseveral training scenarios: speaker-dependent, multi-speaker,\nspeaker-independent, and speaker-adapted. We observe that models underperform\nwhen applied to data from speakers not seen at training time. However, when\nprovided with minimal additional speaker information, such as the mean\nultrasound frame, the models generalize better to unseen speakers.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 12:04:13 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Ribeiro", "Manuel Sam", ""], ["Eshky", "Aciel", ""], ["Richmond", "Korin", ""], ["Renals", "Steve", ""]]}, {"id": "1907.01414", "submitter": "Dennis Madsen", "authors": "Dennis Madsen, Andreas Morel-Forster, Patrick Kahr, Dana Rahbani,\n  Thomas Vetter and Marcel L\\\"uthi", "title": "A Closest Point Proposal for MCMC-based Probabilistic Surface\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to view non-rigid surface registration as a probabilistic\ninference problem. Given a target surface, we estimate the posterior\ndistribution of surface registrations. We demonstrate how the posterior\ndistribution can be used to build shape models that generalize better and show\nhow to visualize the uncertainty in the established correspondence.\nFurthermore, in a reconstruction task, we show how to estimate the posterior\ndistribution of missing data without assuming a fixed point-to-point\ncorrespondence.\n  We introduce the closest-point proposal for the Metropolis-Hastings\nalgorithm. Our proposal overcomes the limitation of slow convergence compared\nto a random-walk strategy. As the algorithm decouples inference from modeling\nthe posterior using a propose-and-verify scheme, we show how to choose\ndifferent distance measures for the likelihood model.\n  All presented results are fully reproducible using publicly available data\nand our open-source implementation of the registration framework.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 14:43:22 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 13:31:04 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Madsen", "Dennis", ""], ["Morel-Forster", "Andreas", ""], ["Kahr", "Patrick", ""], ["Rahbani", "Dana", ""], ["Vetter", "Thomas", ""], ["L\u00fcthi", "Marcel", ""]]}, {"id": "1907.01424", "submitter": "Ruizheng Wu", "authors": "Ruizheng Wu, Xiaodong Gu, Xin Tao, Xiaoyong Shen, Yu-Wing Tai, J iaya\n  Jia", "title": "Landmark Assisted CycleGAN for Cartoon Face Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in generating an cartoon face of a person by\nusing unpaired training data between real faces and cartoon ones. A major\nchallenge of this task is that the structures of real and cartoon faces are in\ntwo different domains, whose appearance differs greatly from each other.\nWithout explicit correspondence, it is difficult to generate a high quality\ncartoon face that captures the essential facial features of a person. In order\nto solve this problem, we propose landmark assisted CycleGAN, which utilizes\nface landmarks to define landmark consistency loss and to guide the training of\nlocal discriminator in CycleGAN. To enforce structural consistency in\nlandmarks, we utilize the conditional generator and discriminator. Our approach\nis capable to generate high-quality cartoon faces even indistinguishable from\nthose drawn by artists and largely improves state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:00:50 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Wu", "Ruizheng", ""], ["Gu", "Xiaodong", ""], ["Tao", "Xin", ""], ["Shen", "Xiaoyong", ""], ["Tai", "Yu-Wing", ""], ["Jia", "J iaya", ""]]}, {"id": "1907.01427", "submitter": "Mark Scanlon", "authors": "Felix Anda, David Lillis, Aikaterini Kanta, Brett A. Becker, Elias\n  Bou-Harb, Nhien-An Le-Khac, Mark Scanlon", "title": "Improving Borderline Adulthood Facial Age Estimation through Ensemble\n  Learning", "comments": null, "journal-ref": "14th International Conference on Availability, Reliability and\n  Security (ARES 2019), Canterbury, UK, August 2019", "doi": "10.1145/3339252.3341491", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving high performance for facial age estimation with subjects in the\nborderline between adulthood and non-adulthood has always been a challenge.\nSeveral studies have used different approaches from the age of a baby to an\nelder adult and different datasets have been employed to measure the mean\nabsolute error (MAE) ranging between 1.47 to 8 years. The weakness of the\nalgorithms specifically in the borderline has been a motivation for this paper.\nIn our approach, we have developed an ensemble technique that improves the\naccuracy of underage estimation in conjunction with our deep learning model\n(DS13K) that has been fine-tuned on the Deep Expectation (DEX) model. We have\nachieved an accuracy of 68% for the age group 16 to 17 years old, which is 4\ntimes better than the DEX accuracy for such age range. We also present an\nevaluation of existing cloud-based and offline facial age prediction services,\nsuch as Amazon Rekognition, Microsoft Azure Cognitive Services, How-Old.net and\nDEX.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:05:24 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Anda", "Felix", ""], ["Lillis", "David", ""], ["Kanta", "Aikaterini", ""], ["Becker", "Brett A.", ""], ["Bou-Harb", "Elias", ""], ["Le-Khac", "Nhien-An", ""], ["Scanlon", "Mark", ""]]}, {"id": "1907.01430", "submitter": "David V\\'azquez", "authors": "Issam H. Laradji, David Vazquez, Mark Schmidt", "title": "Where are the Masks: Instance Segmentation with Image-level Supervision", "comments": "Accepted at BMVC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major obstacle in instance segmentation is that existing methods often need\nmany per-pixel labels in order to be effective. These labels require large\nhuman effort and for certain applications, such labels are not readily\navailable. To address this limitation, we propose a novel framework that can\neffectively train with image-level labels, which are significantly cheaper to\nacquire. For instance, one can do an internet search for the term \"car\" and\nobtain many images where a car is present with minimal effort. Our framework\nconsists of two stages: (1) train a classifier to generate pseudo masks for the\nobjects of interest; (2) train a fully supervised Mask R-CNN on these pseudo\nmasks. Our two main contribution are proposing a pipeline that is simple to\nimplement and is amenable to different segmentation methods; and achieves new\nstate-of-the-art results for this problem setup. Our results are based on\nevaluating our method on PASCAL VOC 2012, a standard dataset for weakly\nsupervised methods, where we demonstrate major performance gains compared to\nexisting methods with respect to mean average precision.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:12:20 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Laradji", "Issam H.", ""], ["Vazquez", "David", ""], ["Schmidt", "Mark", ""]]}, {"id": "1907.01432", "submitter": "Xujun Peng", "authors": "Peng Lu, Hao Zhang, Xujun Peng, Xiaofu Jin", "title": "An End-to-End Neural Network for Image Cropping by Learning Composition\n  from Aesthetic Photos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the fundamental techniques for image editing, image cropping\ndiscards unrelevant contents and remains the pleasing portions of the image to\nenhance the overall composition and achieve better visual/aesthetic perception.\nIn this paper, we primarily focus on improving the accuracy of automatic image\ncropping, and on further exploring its potential in public datasets with high\nefficiency. From this respect, we propose a deep learning based framework to\nlearn the objects composition from photos with high aesthetic qualities, where\nan anchor region is detected through a convolutional neural network (CNN) with\nthe Gaussian kernel to maintain the interested objects' integrity. This initial\ndetected anchor area is then fed into a light weighted regression network to\nobtain the final cropping result. Unlike the conventional methods that multiple\ncandidates are proposed and evaluated iteratively, only a single anchor region\nis produced in our model, which is mapped to the final output directly. Thus,\nlow computational resources are required for the proposed approach. The\nexperimental results on the public datasets show that both cropping accuracy\nand efficiency achieve the state-ofthe-art performances.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:18:33 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 15:44:00 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 16:25:26 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Lu", "Peng", ""], ["Zhang", "Hao", ""], ["Peng", "Xujun", ""], ["Jin", "Xiaofu", ""]]}, {"id": "1907.01452", "submitter": "Ruizheng Wu", "authors": "Ruizheng Wu, Xin Tao, Xiaodong Gu, Xiaoyong Shen, Jiaya Jia", "title": "Attribute-Driven Spontaneous Motion in Unpaired Image Translation", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current image translation methods, albeit effective to produce high-quality\nresults in various applications, still do not consider much geometric\ntransform. We in this paper propose the spontaneous motion estimation module,\nalong with a refinement part, to learn attribute-driven deformation between\nsource and target domains. Extensive experiments and visualization demonstrate\neffectiveness of these modules. We achieve promising results in unpaired-image\ntranslation tasks, and enable interesting applications based on spontaneous\nmotion.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:32:25 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 09:25:20 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Wu", "Ruizheng", ""], ["Tao", "Xin", ""], ["Gu", "Xiaodong", ""], ["Shen", "Xiaoyong", ""], ["Jia", "Jiaya", ""]]}, {"id": "1907.01478", "submitter": "Canwen Xu", "authors": "Canwen Xu, Zhenzhong Chen, Chenliang Li", "title": "Obj-GloVe: Scene-Based Contextual Object Embedding", "comments": "14 pages; not the final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the prevalence of large-scale image dataset, the co-occurrence\ninformation among classes becomes rich, calling for a new way to exploit it to\nfacilitate inference. In this paper, we propose Obj-GloVe, a generic\nscene-based contextual embedding for common visual objects, where we adopt the\nword embedding method GloVe to exploit the co-occurrence between entities. We\ntrain the embedding on pre-processed Open Images V4 dataset and provide\nextensive visualization and analysis by dimensionality reduction and projecting\nthe vectors along a specific semantic axis, and showcasing the nearest\nneighbors of the most common objects. Furthermore, we reveal the potential\napplications of Obj-GloVe on object detection and text-to-image synthesis, then\nverify its effectiveness on these two applications respectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 16:29:02 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Xu", "Canwen", ""], ["Chen", "Zhenzhong", ""], ["Li", "Chenliang", ""]]}, {"id": "1907.01481", "submitter": "Shreyas Hampali", "authors": "Shreyas Hampali, Mahdi Rad, Markus Oberweger and Vincent Lepetit", "title": "HOnnotate: A method for 3D Annotation of Hand and Object Poses", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for annotating images of a hand manipulating an object\nwith the 3D poses of both the hand and the object, together with a dataset\ncreated using this method. Our motivation is the current lack of annotated real\nimages for this problem, as estimating the 3D poses is challenging, mostly\nbecause of the mutual occlusions between the hand and the object. To tackle\nthis challenge, we capture sequences with one or several RGB-D cameras and\njointly optimize the 3D hand and object poses over all the frames\nsimultaneously. This method allows us to automatically annotate each frame with\naccurate estimates of the poses, despite large mutual occlusions. With this\nmethod, we created HO-3D, the first markerless dataset of color images with 3D\nannotations for both the hand and object. This dataset is currently made of\n77,558 frames, 68 sequences, 10 persons, and 10 objects. Using our dataset, we\ndevelop a single RGB image-based method to predict the hand pose when\ninteracting with objects under severe occlusions and show it generalizes to\nobjects not seen in the dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 16:39:05 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 07:57:48 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 16:08:19 GMT"}, {"version": "v4", "created": "Mon, 2 Mar 2020 16:54:59 GMT"}, {"version": "v5", "created": "Sun, 12 Apr 2020 11:28:30 GMT"}, {"version": "v6", "created": "Sat, 30 May 2020 20:37:29 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hampali", "Shreyas", ""], ["Rad", "Mahdi", ""], ["Oberweger", "Markus", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1907.01514", "submitter": "Jie Zhang", "authors": "Jie Zhang, Bohao Li, Kexin Xiang, Xuegang Shi", "title": "Method of diagnosing heart disease based on deep learning ECG signal", "comments": "9 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional method of diagnosing heart disease on ECG signal is\nartificial observation. Some have tried to combine expertise and signal\nprocessing to classify ECG signal by heart disease type. However, the currency\nis not so sufficient that it can be used in medical applications. We develop an\nalgorithm that combines signal processing and deep learning to classify ECG\nsignals into Normal AF other rhythm and noise, which help us solve this\nproblem. It is demonstrated that we can obtain the time-frequency diagram of\nECG signal by wavelet transform, and use DNN to classify the time-frequency\ndiagram to find out the heart disease that the signal collector may have.\nOverall, an accuracy of 94 percent is achieved on the validation set. According\nto the evaluation criteria of PhysioNet/Computing in Cardiology (CinC) in 2017,\nthe F1 score of this method is 0.957, which is higher than the first place in\nthe competition in 2017.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 05:30:29 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 04:07:14 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhang", "Jie", ""], ["Li", "Bohao", ""], ["Xiang", "Kexin", ""], ["Shi", "Xuegang", ""]]}, {"id": "1907.01589", "submitter": "Roy R. Lederman", "authors": "Roy R. Lederman, Joakim And\\'en, Amit Singer", "title": "Hyper-Molecules: on the Representation and Recovery of Dynamical\n  Structures, with Application to Flexible Macro-Molecular Structures in\n  Cryo-EM", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/ab5ede", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM), the subject of the 2017 Nobel Prize in\nChemistry, is a technology for determining the 3-D structure of macromolecules\nfrom many noisy 2-D projections of instances of these macromolecules, whose\norientations and positions are unknown. The molecular structures are not rigid\nobjects, but flexible objects involved in dynamical processes. The different\nconformations are exhibited by different instances of the macromolecule\nobserved in a cryo-EM experiment, each of which is recorded as a particle\nimage. The range of conformations and the conformation of each particle are not\nknown a priori; one of the great promises of cryo-EM is to map this\nconformation space. Remarkable progress has been made in determining rigid\nstructures from homogeneous samples of molecules in spite of the unknown\norientation of each particle image and significant progress has been made in\nrecovering a few distinct states from mixtures of rather distinct\nconformations, but more complex heterogeneous samples remain a major challenge.\nWe introduce the ``hyper-molecule'' framework for modeling structures across\ndifferent states of heterogeneous molecules, including continuums of states.\nThe key idea behind this framework is representing heterogeneous macromolecules\nas high-dimensional objects, with the additional dimensions representing the\nconformation space. This idea is then refined to model properties such as\nlocalized heterogeneity. In addition, we introduce an algorithmic framework for\nrecovering such maps of heterogeneous objects from experimental data using a\nBayesian formulation of the problem and Markov chain Monte Carlo (MCMC)\nalgorithms to address the computational challenges in recovering these high\ndimensional hyper-molecules. We demonstrate these ideas in a prototype applied\nto synthetic data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 19:16:12 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lederman", "Roy R.", ""], ["And\u00e9n", "Joakim", ""], ["Singer", "Amit", ""]]}, {"id": "1907.01649", "submitter": "Ryan Cunningham", "authors": "Ryan J. Cunningham and Ian D. Loram", "title": "Estimation of Absolute States of Human Skeletal Muscle via Standard\n  B-Mode Ultrasound Imaging and Deep Convolutional Neural Networks", "comments": "10 pages, 5 figures, 2 tables. This paper is currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Objective: To test automated in vivo estimation of active and passive\nskeletal muscle states using ultrasonic imaging. Background: Current technology\n(electromyography, dynamometry, shear wave imaging) provides no general,\nnon-invasive method for online estimation of skeletal intramuscular states.\nUltrasound (US) allows non-invasive imaging of muscle, yet current\ncomputational approaches have never achieved simultaneous extraction nor\ngeneralisation of independently varying, active and passive states. We use deep\nlearning to investigate the generalizable content of 2D US muscle images.\nMethod: US data synchronized with electromyography of the calf muscles, with\nmeasures of joint moment/angle were recorded from 32 healthy participants (7\nfemale, ages: 27.5, 19-65). We extracted a region of interest of medial\ngastrocnemius and soleus using our prior developed accurate segmentation\nalgorithm. From the segmented images, a deep convolutional neural network was\ntrained to predict three absolute, drift-free, components of the\nneurobiomechanical state (activity, joint angle, joint moment) during\nexperimentally designed, simultaneous, independent variation of passive (joint\nangle) and active (electromyography) inputs. Results: For all 32 held-out\nparticipants (16-fold cross-validation) the ankle joint angle,\nelectromyography, and joint moment were estimated to accuracy 55+-8%, 57+-11%,\nand 46+-9% respectively. Significance: With 2D US imaging, deep neural networks\ncan encode in generalizable form, the activity-length-tension state\nrelationship of muscle. Observation only, low power, 2D US imaging can provide\na new category of technology for non-invasive estimation of neural output,\nlength and tension in skeletal muscle. This proof of principle has value for\npersonalised muscle diagnosis in pain, injury, neurological conditions,\nneuropathies, myopathies and ageing.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 21:09:27 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Cunningham", "Ryan J.", ""], ["Loram", "Ian D.", ""]]}, {"id": "1907.01656", "submitter": "Vaishnavi Subramanian", "authors": "Vaishnavi Subramanian, Hongzhi Wang, Joy T. Wu, Ken C. L. Wong, Arjun\n  Sharma, and Tanveer Syeda-Mahmood", "title": "Automated Detection and Type Classification of Central Venous Catheters\n  in Chest X-Rays", "comments": "Accepted to Medical Image Computing and Computer Assisted\n  Intervention (MICCAI) 2019; Data available: ML-CDS Challenge, MICCAI2019\n  (http://www.mcbr-cds.org/challenge/challenge-description.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Central venous catheters (CVCs) are commonly used in critical care settings\nfor monitoring body functions and administering medications. They are often\ndescribed in radiology reports by referring to their presence, identity and\nplacement. In this paper, we address the problem of automatic detection of\ntheir presence and identity through automated segmentation using deep learning\nnetworks and classification based on their intersection with previously learned\nshape priors from clinician annotations of CVCs. The results not only\noutperform existing methods of catheter detection achieving 85.2% accuracy at\n91.6% precision, but also enable high precision (95.2%) classification of\ncatheter types on a large dataset of over 10,000 chest X-rays, presenting a\nrobust and practical solution to this problem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 21:31:20 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 22:45:10 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 17:33:48 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Subramanian", "Vaishnavi", ""], ["Wang", "Hongzhi", ""], ["Wu", "Joy T.", ""], ["Wong", "Ken C. L.", ""], ["Sharma", "Arjun", ""], ["Syeda-Mahmood", "Tanveer", ""]]}, {"id": "1907.01661", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li, Nicha C. Dvornek, Yuan Zhou, Juntang Zhuang, Pamela\n  Ventola and James S. Duncan", "title": "Graph Neural Network for Interpreting Task-fMRI Biomarkers", "comments": null, "journal-ref": "Medical Image Computing and Computer-Assisted Intervention 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the biomarkers associated with ASD is helpful for understanding the\nunderlying roots of the disorder and can lead to earlier diagnosis and more\ntargeted treatment. A promising approach to identify biomarkers is using Graph\nNeural Networks (GNNs), which can be used to analyze graph structured data,\ni.e. brain networks constructed by fMRI. One way to interpret important\nfeatures is through looking at how the classification probability changes if\nthe features are occluded or replaced. The major limitation of this approach is\nthat replacing values may change the distribution of the data and lead to\nserious errors. Therefore, we develop a 2-stage pipeline to eliminate the need\nto replace features for reliable biomarker interpretation. Specifically, we\npropose an inductive GNN to embed the graphs containing different properties of\ntask-fMRI for identifying ASD and then discover the brain regions/sub-graphs\nused as evidence for the GNN classifier. We first show GNN can achieve high\naccuracy in identifying ASD. Next, we calculate the feature importance scores\nusing GNN and compare the interpretation ability with Random Forest. Finally,\nwe run with different atlases and parameters, proving the robustness of the\nproposed method. The detected biomarkers reveal their association with social\nbehaviors. We also show the potential of discovering new informative\nbiomarkers. Our pipeline can be generalized to other graph feature importance\ninterpretation problems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 21:43:43 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 02:40:08 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Dvornek", "Nicha C.", ""], ["Zhou", "Yuan", ""], ["Zhuang", "Juntang", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S.", ""]]}, {"id": "1907.01683", "submitter": "Priya Kansal Dr.", "authors": "Sabari Nathan, Priya Kansal", "title": "SkeletonNet: Shape Pixel to Skeleton Pixel", "comments": "Published in CVPRw 2019", "journal-ref": "IEEE/CVPRw 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning for Geometric Shape Understating has organized a challenge for\nextracting different kinds of skeletons from the images of different objects.\nThis competition is organized in association with CVPR 2019. There are three\ndifferent tracks of this competition. The present manuscript describes the\nmethod used to train the model for the dataset provided in the first track. The\nfirst track aims to extract skeleton pixels from the shape pixels of 89\ndifferent objects. For the purpose of extracting the skeleton, a U-net model\nwhich is comprised of an encoder-decoder structure has been used. In our\nproposed architecture, unlike the plain decoder in the traditional Unet, we\nhave designed the decoder in the format of HED architecture, wherein we have\nintroduced 4 side layers and fused them to one dilation convolutional layer to\nconnect the broken links of the skeleton. Our proposed architecture achieved\nthe F1 score of 0.77 on test data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 23:44:05 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Nathan", "Sabari", ""], ["Kansal", "Priya", ""]]}, {"id": "1907.01696", "submitter": "Xiangyun Ding", "authors": "Yanyuet Man, Xiangyun Ding, Xingcheng Yao, Han Bao", "title": "A Semi-Supervised Framework for Automatic Pixel-Wise Breast Cancer\n  Grading of Histological Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the world, breast cancer is one of the leading causes of female\ndeath. Recently, deep learning methods are developed to automatically grade\nbreast cancer of histological slides. However, the performance of existing deep\nlearning models is limited due to the lack of large annotated biomedical\ndatasets. One promising way to relieve the annotating burden is to leverage the\nunannotated datasets to enhance the trained model. In this paper, we first\napply active learning method in breast cancer grading, and propose a\nsemi-supervised framework based on expectation maximization (EM) model. The\nproposed EM approach is based on the collaborative filtering among the\nannotated and unannotated datasets. The collaborative filtering method\neffectively extracts useful and credible datasets from the unannotated images.\nResults of pixel-wise prediction of whole-slide images (WSI) demonstrate that\nthe proposed method not only outperforms state-of-art methods, but also\nsignificantly reduces the annotation cost by over 70%.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 01:30:06 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Man", "Yanyuet", ""], ["Ding", "Xiangyun", ""], ["Yao", "Xingcheng", ""], ["Bao", "Han", ""]]}, {"id": "1907.01709", "submitter": "Kyoung-Woon On", "authors": "Kyoung-Woon On, Eun-Sol Kim, Yu-Jung Heo, Byoung-Tak Zhang", "title": "Compositional Structure Learning for Sequential Video Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional sequential learning methods such as Recurrent Neural Networks\n(RNNs) focus on interactions between consecutive inputs, i.e. first-order\nMarkovian dependency. However, most of sequential data, as seen with videos,\nhave complex temporal dependencies that imply variable-length semantic flows\nand their compositions, and those are hard to be captured by conventional\nmethods. Here, we propose Temporal Dependency Networks (TDNs) for learning\nvideo data by discovering these complex structures of the videos. The TDNs\nrepresent video as a graph whose nodes and edges correspond to frames of the\nvideo and their dependencies respectively. Via a parameterized kernel with\ngraph-cut and graph convolutions, the TDNs find compositional temporal\ndependencies of the data in multilevel graph forms. We evaluate the proposed\nmethod on the large-scale video dataset Youtube-8M. The experimental results\nshow that our model efficiently learns the complex semantic structure of video\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 02:28:48 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["On", "Kyoung-Woon", ""], ["Kim", "Eun-Sol", ""], ["Heo", "Yu-Jung", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1907.01710", "submitter": "Zhe Zhu", "authors": "Yinhao Ren, Zhe Zhu, Yingzhou Li, and Joseph Lo", "title": "Mask Embedding in conditional GAN for Guided Synthesis of High\n  Resolution Images", "comments": "11 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in conditional Generative Adversarial Networks (cGANs)\nhave shown promises in label guided image synthesis. Semantic masks, such as\nsketches and label maps, are another intuitive and effective form of guidance\nin image synthesis. Directly incorporating the semantic masks as constraints\ndramatically reduces the variability and quality of the synthesized results. We\nobserve this is caused by the incompatibility of features from different inputs\n(such as mask image and latent vector) of the generator. To use semantic masks\nas guidance whilst providing realistic synthesized results with fine details,\nwe propose to use mask embedding mechanism to allow for a more efficient\ninitial feature projection in the generator. We validate the effectiveness of\nour approach by training a mask guided face generator using CELEBA-HQ dataset.\nWe can generate realistic and high resolution facial images up to the\nresolution of 512*512 with a mask guidance. Our code is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 02:30:36 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Ren", "Yinhao", ""], ["Zhu", "Zhe", ""], ["Li", "Yingzhou", ""], ["Lo", "Joseph", ""]]}, {"id": "1907.01714", "submitter": "Nai Bian", "authors": "Nai Bian, Feng Liang, Haisheng Fu, Bo Lei", "title": "A Deep Image Compression Framework for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition technology has advanced rapidly and has been widely used in\nvarious applications. Due to the extremely huge amount of data of face images\nand the large computing resources required correspondingly in large-scale face\nrecognition tasks, there is a requirement for a face image compression approach\nthat is highly suitable for face recognition tasks. In this paper, we propose a\ndeep convolutional autoencoder compression network for face recognition tasks.\nIn the compression process, deep features are extracted from the original image\nby the convolutional neural networks to produce a compact representation of the\noriginal image, which is then encoded and saved by existing codec such as PNG.\nThis compact representation is utilized by the reconstruction network to\ngenerate a reconstructed image of the original one. In order to improve the\nface recognition accuracy when the compression framework is used in a face\nrecognition system, we combine this compression framework with a existing face\nrecognition network for joint optimization. We test the proposed scheme and\nfind that after joint training, the Labeled Faces in the Wild (LFW) dataset\ncompressed by our compression framework has higher face verification accuracy\nthan that compressed by JPEG2000, and is much higher than that compressed by\nJPEG.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 02:44:29 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Bian", "Nai", ""], ["Liang", "Feng", ""], ["Fu", "Haisheng", ""], ["Lei", "Bo", ""]]}, {"id": "1907.01717", "submitter": "Deepan Das", "authors": "Deepan Das, Deepak Mishra", "title": "Unsupervised Anomalous Trajectory Detection for Crowded Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": "CFP1858A-USB", "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present an improved clustering based, unsupervised anomalous trajectory\ndetection algorithm for crowded scenes. The proposed work is based on four\nmajor steps, namely, extraction of trajectories from crowded scene video,\nextraction of several features from these trajectories, independent mean-shift\nclustering and anomaly detection. First, the trajectories of all moving objects\nin a crowd are extracted using a multi feature video object tracker. These\ntrajectories are then transformed into a set of feature spaces. Mean shift\nclustering is applied on these feature matrices to obtain distinct clusters,\nwhile a Shannon Entropy based anomaly detector identifies corresponding\nanomalies. In the final step, a voting mechanism identifies the trajectories\nthat exhibit anomalous characteristics. The algorithm is tested on crowd scene\nvideos from datasets. The videos represent various possible crowd scenes with\ndifferent motion patterns and the method performs well to detect the expected\nanomalous trajectories from the scene.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 02:56:47 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Das", "Deepan", ""], ["Mishra", "Deepak", ""]]}, {"id": "1907.01743", "submitter": "Haoran Dou", "authors": "Yi Wang, Haoran Dou, Xiaowei Hu, Lei Zhu, Xin Yang, Ming Xu, Jing Qin,\n  Pheng-Ann Heng, Tianfu Wang, and Dong Ni", "title": "Deep Attentive Features for Prostate Segmentation in 3D Transrectal\n  Ultrasound", "comments": "11 pages, 10 figures, 2 tables. Accepted by IEEE transactions on\n  Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2019.2913184", "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic prostate segmentation in transrectal ultrasound (TRUS) images is of\nessential importance for image-guided prostate interventions and treatment\nplanning. However, developing such automatic solutions remains very challenging\ndue to the missing/ambiguous boundary and inhomogeneous intensity distribution\nof the prostate in TRUS, as well as the large variability in prostate shapes.\nThis paper develops a novel 3D deep neural network equipped with attention\nmodules for better prostate segmentation in TRUS by fully exploiting the\ncomplementary information encoded in different layers of the convolutional\nneural network (CNN). Our attention module utilizes the attention mechanism to\nselectively leverage the multilevel features integrated from different layers\nto refine the features at each individual layer, suppressing the non-prostate\nnoise at shallow layers of the CNN and increasing more prostate details into\nfeatures at deep layers. Experimental results on challenging 3D TRUS volumes\nshow that our method attains satisfactory segmentation performance. The\nproposed attention mechanism is a general strategy to aggregate multi-level\ndeep features and has the potential to be used for other medical image\nsegmentation tasks. The code is publicly available at\nhttps://github.com/wulalago/DAF3D.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 05:21:52 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Yi", ""], ["Dou", "Haoran", ""], ["Hu", "Xiaowei", ""], ["Zhu", "Lei", ""], ["Yang", "Xin", ""], ["Xu", "Ming", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""], ["Wang", "Tianfu", ""], ["Ni", "Dong", ""]]}, {"id": "1907.01744", "submitter": "Jian Wang", "authors": "Jian Wang, Xiaoyao Li, Xiangbo Shu, Weiqin Li", "title": "Region-Manipulated Fusion Networks for Pancreatitis Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work first attempts to automatically recognize pancreatitis on CT scan\nimages. However, different form the traditional object recognition, such\npancreatitis recognition is challenging due to the fine-grained and non-rigid\nappearance variability of the local diseased regions. To this end, we propose a\ncustomized Region-Manipulated Fusion Networks (RMFN) to capture the key\ncharacteristics of local lesion for pancreatitis recognition. Specifically, to\neffectively highlight the imperceptible lesion regions, a novel\nregion-manipulated scheme in RMFN is proposed to force the lesion regions while\nweaken the non-lesion regions by ceaselessly aggregating the multi-scale local\ninformation onto feature maps. The proposed scheme can be flexibly equipped\ninto the existing neural networks, such as AlexNet and VGG. To evaluate the\nperformance of the propose method, a real CT image database about pancreatitis\nis collected from hospitals \\footnote{The database is available later}. And\nexperimental results on such database well demonstrate the effectiveness of the\nproposed method for pancreatitis recognition.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 05:37:04 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Wang", "Jian", ""], ["Li", "Xiaoyao", ""], ["Shu", "Xiangbo", ""], ["Li", "Weiqin", ""]]}, {"id": "1907.01750", "submitter": "Hyun Seo", "authors": "Jaewoong Choi, Hyun Seo, Suii Im, Myungjoo Kang", "title": "Attention routing between capsules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new capsule network architecture called Attention\nRouting CapsuleNet (AR CapsNet). We replace the dynamic routing and squash\nactivation function of the capsule network with dynamic routing (CapsuleNet)\nwith the attention routing and capsule activation. The attention routing is a\nrouting between capsules through an attention module. The attention routing is\na fast forward-pass while keeping spatial information. On the other hand, the\nintuitive interpretation of the dynamic routing is finding a centroid of the\nprediction capsules. Thus, the squash activation function and its variant focus\non preserving a vector orientation. However, the capsule activation focuses on\nperforming a capsule-scale activation function.\n  We evaluate our proposed model on the MNIST, affNIST, and CIFAR-10\nclassification tasks. The proposed model achieves higher accuracy with fewer\nparameters (x0.65 in the MNIST, x0.82 in the CIFAR-10) and less training time\nthan CapsuleNet (x0.19 in the MNIST, x0.35 in the CIFAR-10). These results\nvalidate that designing a capsule-scale operation is a key factor to implement\nthe capsule concept.\n  Also, our experiment shows that our proposed model is transformation\nequivariant as CapsuleNet. As we perturb each element of the output capsule,\nthe decoder attached to the output capsules shows global variations. Further\nexperiments show that the difference in the capsule features caused by applying\naffine transformations on an input image is significantly aligned in one\ndirection.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 06:01:16 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 00:44:36 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 04:29:09 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 08:02:00 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Choi", "Jaewoong", ""], ["Seo", "Hyun", ""], ["Im", "Suii", ""], ["Kang", "Myungjoo", ""]]}, {"id": "1907.01759", "submitter": "Peter Fasogbon O.", "authors": "Peter Fasogbon, Emre Aksu", "title": "Calibration of fisheye camera using entrance pupil", "comments": "5 pages, 4 figures, Accepted for publication at ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most conventional camera calibration algorithms assume that the imaging\ndevice has a Single Viewpoint (SVP). This is not necessarily true for special\nimaging device such as fisheye lenses. As a consequence, the intrinsic camera\ncalibration result is not always reliable. In this paper, we propose a new\nformation model that tends to relax this assumption so that a Non-Single\nViewpoint (NSVP) system is corrected to always maintain a SVP, by taking into\naccount the variation of the Entrance Pupil (EP) using thin lens modeling. In\naddition, we present a calibration procedure for the image formation to\nestimate these EP parameters using non linear optimization procedure with\nbundle adjustment. From experiments, we are able to obtain slightly better\nre-projection error than traditional methods, and the camera parameters are\nbetter estimated. The proposed calibration procedure is simple and can easily\nbe integrated to any other thin lens image formation model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 06:45:02 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Fasogbon", "Peter", ""], ["Aksu", "Emre", ""]]}, {"id": "1907.01773", "submitter": "Kaijie Tu", "authors": "Dawen Xu, Ying Wang, Kaijie Tu, Cheng Liu, Bingsheng He, and Lei Zhang", "title": "Accelerating Generative Neural Networks on Unmodified Deep Learning\n  Processors -- A Software Approach", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural network is a new category of neural networks and it has\nbeen widely utilized in applications such as content generation, unsupervised\nlearning, segmentation and pose estimation. It typically involves massive\ncomputing-intensive deconvolution operations that cannot be fitted to\nconventional neural network processors directly. However, prior works mainly\ninvestigated specialized hardware architectures through intensive hardware\nmodifications to the existing deep learning processors to accelerate\ndeconvolution together with the convolution. In contrast, this work proposes a\nnovel deconvolution implementation with a software approach and enables fast\nand efficient deconvolution execution on the legacy deep learning processors.\nOur proposed method reorganizes the computation of deconvolution and allows the\ndeep learning processors to treat it as the standard convolution by splitting\nthe original deconvolution filters into multiple small filters. Compared to\nprior acceleration schemes, the implemented acceleration scheme achieves 2.41x\n- 4.34x performance speedup and reduces the energy consumption by 27.7% - 54.5%\non a set of realistic benchmarks. In addition, we also applied the\ndeconvolution computing approach to the off-the-shelf commodity deep learning\nprocessors. The performance of deconvolution also exhibits significant\nperformance speedup over prior deconvolution implementations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 07:18:57 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 08:19:41 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 02:50:01 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Xu", "Dawen", ""], ["Wang", "Ying", ""], ["Tu", "Kaijie", ""], ["Liu", "Cheng", ""], ["He", "Bingsheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1907.01806", "submitter": "Shaoze You", "authors": "Shaoze You, Hua Zhu, Menggang Li, Lei Wang, Chaoquan Tang", "title": "Tracking system of Mine Patrol Robot for Low Illumination Environment", "comments": "13 pages, 7 figures, 1 table, 27 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has received a significant attention in recent years, which\nis one of the important parts for robots to apperceive external environment.\nDiscriminative Correlation Filter (DCF) based trackers gained more popularity\ndue to their efficiency, however, tracking in low-illumination environments is\na challenging problem, not yet successfully addressed in the literature. In\nthis work, we tackle the problems by introducing Low-Illumination Long-term\nCorrelation Tracker (LLCT). First, fused features only including HOG and Color\nNames are employed to boost the tracking efficiency. Second, we used the\nstandard PCA to reduction scheme in the translation and scale estimation phase\nfor accelerating. Third, we learned a long-term correlation filter to keep the\nlong-term memory ability. Finally, update memory templates with interval\nupdates, then re-match existing and initial templates every few frames to\nmaintain template accuracy. The extensive experiments on popular Object\nTracking Benchmark OTB-50 datasets have demonstrated that the proposed tracker\noutperforms the state-of-the-art trackers significantly achieves a high\nreal-time (33FPS) performance. In addition, the proposed approach can be\nintegrated easily in robot system and the running speed performed well. The\nexperimental results show that the novel tracker performance in\nlow-illumination environment is better than that of general trackers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 09:14:37 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 07:25:18 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 03:07:51 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["You", "Shaoze", ""], ["Zhu", "Hua", ""], ["Li", "Menggang", ""], ["Wang", "Lei", ""], ["Tang", "Chaoquan", ""]]}, {"id": "1907.01821", "submitter": "Marcus M\\\"artens", "authors": "Marcus M\\\"artens, Dario Izzo, Andrej Krzic, Dani\\\"el Cox", "title": "Super-Resolution of PROBA-V Images Using Convolutional Neural Networks", "comments": "To appear in Special Issue on Applications of Artificial Intelligence\n  in Aerospace Engineering in the Journal \"Astrodynamics\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ESA's PROBA-V Earth observation satellite enables us to monitor our planet at\na large scale, studying the interaction between vegetation and climate and\nprovides guidance for important decisions on our common global future. However,\nthe interval at which high resolution images are recorded spans over several\ndays, in contrast to the availability of lower resolution images which is often\ndaily. We collect an extensive dataset of both, high and low resolution images\ntaken by PROBA-V instruments during monthly periods to investigate Multi Image\nSuper-resolution, a technique to merge several low resolution images to one\nimage of higher quality. We propose a convolutional neural network that is able\nto cope with changes in illumination, cloud coverage and landscape features\nwhich are challenges introduced by the fact that the different images are taken\nover successive satellite passages over the same region. Given a bicubic\nupscaling of low resolution images taken under optimal conditions, we find the\nPeak Signal to Noise Ratio of the reconstructed image of the network to be\nhigher for a large majority of different scenes. This shows that applied\nmachine learning has the potential to enhance large amounts of previously\ncollected earth observation data during multiple satellite passes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 09:53:05 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["M\u00e4rtens", "Marcus", ""], ["Izzo", "Dario", ""], ["Krzic", "Andrej", ""], ["Cox", "Dani\u00ebl", ""]]}, {"id": "1907.01826", "submitter": "Bin Duan", "authors": "Bin Duan, Wei Wang, Hao Tang, Hugo Latapie, Yan Yan", "title": "Cascade Attention Guided Residue Learning GAN for Cross-Modal\n  Translation", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since we were babies, we intuitively develop the ability to correlate the\ninput from different cognitive sensors such as vision, audio, and text.\nHowever, in machine learning, this cross-modal learning is a nontrivial task\nbecause different modalities have no homogeneous properties. Previous works\ndiscover that there should be bridges among different modalities. From\nneurology and psychology perspective, humans have the capacity to link one\nmodality with another one, e.g., associating a picture of a bird with the only\nhearing of its singing and vice versa. Is it possible for machine learning\nalgorithms to recover the scene given the audio signal? In this paper, we\npropose a novel Cascade Attention-Guided Residue GAN (CAR-GAN), aiming at\nreconstructing the scenes given the corresponding audio signals. Particularly,\nwe present a residue module to mitigate the gap between different modalities\nprogressively. Moreover, a cascade attention guided network with a novel\nclassification loss function is designed to tackle the cross-modal learning\ntask. Our model keeps the consistency in high-level semantic label domain and\nis able to balance two different modalities. The experimental results\ndemonstrate that our model achieves the state-of-the-art cross-modal\naudio-visual generation on the challenging Sub-URMP dataset. Code will be\navailable at https://github.com/tuffr5/CAR-GAN.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:04:54 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 09:48:24 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Duan", "Bin", ""], ["Wang", "Wei", ""], ["Tang", "Hao", ""], ["Latapie", "Hugo", ""], ["Yan", "Yan", ""]]}, {"id": "1907.01839", "submitter": "David Zu\\~niga-No\\\"el", "authors": "David Zu\\~niga-No\\\"el, Jose-Raul Ruiz-Sarmiento, Javier\n  Gonzalez-Jimenez", "title": "Intrinsic Calibration of Depth Cameras for Mobile Robots using a Radial\n  Laser Scanner", "comments": "Submitted to the 18th International Conference on Computer Analysis\n  of Images and Patterns. Project webpage (code):\n  http://github.com/dzunigan/depth_calibration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth cameras, typically in RGB-D configurations, are common devices in\nmobile robotic platforms given their appealing features: high frequency and\nresolution, low price and power requirements, among others. These sensors may\ncome with significant, non-linear errors in the depth measurements that\njeopardize robot tasks, like free-space detection, environment reconstruction\nor visual robot-human interaction. This paper presents a method to calibrate\nsuch systematic errors with the help of a second, more precise range sensor, in\nour case a radial laser scanner. In contrast to what it may seem at first, this\ndoes not mean a serious limitation in practice since these two sensors are\noften mounted jointly in many mobile robotic platforms, as they complement well\neach other. Moreover, the laser scanner can be used just for the calibration\nprocess and get rid of it after that. The main contributions of the paper are:\ni) the calibration is formulated from a probabilistic perspective through a\nMaximum Likelihood Estimation problem, and ii) the proposed method can be\neasily executed automatically by mobile robotic platforms. To validate the\nproposed approach we evaluated for both, local distortion of 3D planar\nreconstructions and global shifts in the measurements, obtaining considerably\nmore accurate results. A C++ open-source implementation of the presented method\nhas been released for the benefit of the community.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:35:19 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zu\u00f1iga-No\u00ebl", "David", ""], ["Ruiz-Sarmiento", "Jose-Raul", ""], ["Gonzalez-Jimenez", "Javier", ""]]}, {"id": "1907.01841", "submitter": "Hacer Yalim Keles", "authors": "Yahya Dogan and Hacer Yalim Keles", "title": "Semi-supervised Image Attribute Editing using Generative Adversarial\n  Networks", "comments": "This paper is the preprint of the accepted manuscript in\n  Neurocomputing Journal. To visualize the Figures in the manuscript in high\n  quality, please check the version at this URL:\n  https://github.com/yahyadogan72/CRG", "journal-ref": null, "doi": "10.1016/j.neucom.2020.03.071", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image attribute editing is a challenging problem that has been recently\nstudied by many researchers using generative networks. The challenge is in the\nmanipulation of selected attributes of images while preserving the other\ndetails. The method to achieve this goal is to find an accurate latent vector\nrepresentation of an image and a direction corresponding to the attribute.\nAlmost all the works in the literature use labeled datasets in a supervised\nsetting for this purpose. In this study, we introduce an architecture called\nCyclic Reverse Generator (CRG), which allows learning the inverse function of\nthe generator accurately via an encoder in an unsupervised setting by utilizing\ncyclic cost minimization. Attribute editing is then performed using the CRG\nmodels for finding desired attribute representations in the latent space. In\nthis work, we use two arbitrary reference images, with and without desired\nattributes, to compute an attribute direction for editing. We show that the\nproposed approach performs better in terms of image reconstruction compared to\nthe existing end-to-end generative models both quantitatively and\nqualitatively. We demonstrate state-of-the-art results on both real images and\ngenerated images in CelebA dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:38:56 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 17:18:48 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Dogan", "Yahya", ""], ["Keles", "Hacer Yalim", ""]]}, {"id": "1907.01845", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu and Bo Zhang and Ruijun Xu", "title": "FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural\n  Architecture Search", "comments": "Accepted to ICCV21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most critical problems in weight-sharing neural architecture\nsearch is the evaluation of candidate models within a predefined search space.\nIn practice, a one-shot supernet is trained to serve as an evaluator. A\nfaithful ranking certainly leads to more accurate searching results. However,\ncurrent methods are prone to making misjudgments. In this paper, we prove that\ntheir biased evaluation is due to inherent unfairness in the supernet training.\nIn view of this, we propose two levels of constraints: expectation fairness and\nstrict fairness. Particularly, strict fairness ensures equal optimization\nopportunities for all choice blocks throughout the training, which neither\noverestimates nor underestimates their capacity. We demonstrate that this is\ncrucial for improving the confidence of models' ranking. Incorporating the\none-shot supernet trained under the proposed fairness constraints with a\nmulti-objective evolutionary search algorithm, we obtain various\nstate-of-the-art models, e.g., FairNAS-A attains 77.5% top-1 validation\naccuracy on ImageNet. The models and their evaluation codes are made publicly\navailable online http://github.com/fairnas/FairNAS .\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:50:38 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 06:16:45 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 06:41:45 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2020 11:53:02 GMT"}, {"version": "v5", "created": "Wed, 28 Jul 2021 10:19:08 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Zhang", "Bo", ""], ["Xu", "Ruijun", ""]]}, {"id": "1907.01847", "submitter": "Wei Li", "authors": "Wei Li, Zehuan Yuan, Dashan Guo, Lei Huang, Xiangzhong Fang and\n  Changhu Wang", "title": "Deformable Tube Network for Action Detection in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of spatio-temporal action detection in videos.\nExisting methods commonly either ignore temporal context in action recognition\nand localization, or lack the modelling of flexible shapes of action tubes. In\nthis paper, we propose a two-stage action detector called Deformable Tube\nNetwork (DTN), which is composed of a Deformation Tube Proposal Network (DTPN)\nand a Deformable Tube Recognition Network (DTRN) similar to the Faster R-CNN\narchitecture. In DTPN, a fast proposal linking algorithm (FTL) is introduced to\nconnect region proposals across frames to generate multiple deformable action\ntube proposals. To perform action detection, we design a 3D convolution network\nwith skip connections for tube classification and regression. Modelling action\nproposals as deformable tubes explicitly considers the shape of action tubes\ncompared to 3D cuboids. Moreover, 3D convolution based recognition network can\nlearn temporal dynamics sufficiently for action detection. Our experimental\nresults show that we significantly outperform the methods with 3D cuboids and\nobtain the state-of-the-art results on both UCF-Sports and AVA datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:55:35 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Li", "Wei", ""], ["Yuan", "Zehuan", ""], ["Guo", "Dashan", ""], ["Huang", "Lei", ""], ["Fang", "Xiangzhong", ""], ["Wang", "Changhu", ""]]}, {"id": "1907.01869", "submitter": "Panagiotis Linardos", "authors": "Panagiotis Linardos, Eva Mohedano, Juan Jose Nieto, Noel E. O'Connor,\n  Xavier Giro-i-Nieto, Kevin McGuinness", "title": "Simple vs complex temporal recurrences for video saliency prediction", "comments": "Accepted at BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates modifying an existing neural network architecture for\nstatic saliency prediction using two types of recurrences that integrate\ninformation from the temporal domain. The first modification is the addition of\na ConvLSTM within the architecture, while the second is a conceptually simple\nexponential moving average of an internal convolutional state. We use weights\npre-trained on the SALICON dataset and fine-tune our model on DHF1K. Our\nresults show that both modifications achieve state-of-the-art results and\nproduce similar saliency maps. Source code is available at\nhttps://git.io/fjPiB.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 12:02:05 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 14:03:42 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 16:03:44 GMT"}, {"version": "v4", "created": "Tue, 16 Jul 2019 13:13:46 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Linardos", "Panagiotis", ""], ["Mohedano", "Eva", ""], ["Nieto", "Juan Jose", ""], ["O'Connor", "Noel E.", ""], ["Giro-i-Nieto", "Xavier", ""], ["McGuinness", "Kevin", ""]]}, {"id": "1907.01870", "submitter": "Wilhelm Wimmer", "authors": "Wilhelm Wimmer, Clair Vandersteen, Nicolas Guevara, Marco Caversaccio,\n  Herv\\'e Delingette", "title": "Robust Cochlear Modiolar Axis Detection in CT", "comments": "Accepted for MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cochlea, the auditory part of the inner ear, is a spiral-shaped organ\nwith large morphological variability. An individualized assessment of its shape\nis essential for clinical applications related to tonotopy and cochlear\nimplantation. To unambiguously reference morphological parameters, reliable\nrecognition of the cochlear modiolar axis in computed tomography (CT) images is\nrequired. The conventional method introduces measurement uncertainties, as it\nis based on manually selected and difficult to identify landmarks. Herein, we\npresent an algorithm for robust modiolar axis detection in clinical CT images.\nWe define the modiolar axis as the rotation component of the kinematic spiral\nmotion inherent in the cochlear shape. For surface fitting, we use a compact\nshape representation in a 7-dimensional kinematic parameter space based on\nextended Pl\\\"ucker coordinates. It is the first time such a kinematic\nrepresentation is used for shape analysis in medical images. Robust surface\nfitting is achieved with an adapted approximate maximum likelihood method\nassuming a Student-t distribution, enabling axis detection even in partially\navailable surface data. We verify the algorithm performance on a synthetic data\nset with cochlear surface subsets. In addition, we perform an experimental\nstudy with four experts in 23 human cochlea CT data sets to compare the\nautomated detection with the manually found axes. Axes found from co-registered\nhigh resolution micro-CT scans are used for reference. Our experiments show\nthat the algorithm reduces the alignment error providing more reliable modiolar\naxis detection for clinical and research applications.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 12:03:01 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Wimmer", "Wilhelm", ""], ["Vandersteen", "Clair", ""], ["Guevara", "Nicolas", ""], ["Caversaccio", "Marco", ""], ["Delingette", "Herv\u00e9", ""]]}, {"id": "1907.01879", "submitter": "Christoph Heindl", "authors": "Christoph Heindl and Sebastian Zambal and Josef Scharinger", "title": "Learning to Predict Robot Keypoints Using Artificially Generated Images", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers robot keypoint estimation on color images as a supervised\nmachine learning task. We propose the use of probabilistically created\nrenderings to overcome the lack of labeled real images. Rather than sampling\nfrom stationary distributions, our approach introduces a feedback mechanism\nthat constantly adapts probability distributions according to current training\nprogress. Initial results show, our approach achieves near-human-level accuracy\non real images. Additionally, we demonstrate that feedback leads to fewer\nrequired training steps, while maintaining the same model quality on synthetic\ndata sets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 12:20:16 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Heindl", "Christoph", ""], ["Zambal", "Sebastian", ""], ["Scharinger", "Josef", ""]]}, {"id": "1907.01922", "submitter": "Lihao Liu", "authors": "Lihao Liu, Xiaowei Hu, Lei Zhu, and Pheng-Ann Heng", "title": "Probabilistic Multilayer Regularization Network for Unsupervised 3D\n  Brain Image Registration", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain image registration transforms a pair of images into one system with the\nmatched imaging contents, which is of essential importance for brain image\nanalysis. This paper presents a novel framework for unsupervised 3D brain image\nregistration by capturing the feature-level transformation relationships\nbetween the unaligned image and reference image. To achieve this, we develop a\nfeature-level probabilistic model to provide the direct regularization to the\nhidden layers of two deep convolutional neural networks, which are constructed\nfrom two input images. This model design is developed into multiple layers of\nthese two networks to capture the transformation relationships at different\nlevels. We employ two common benchmark datasets for 3D brain image registration\nand perform various experiments to evaluate our method. Experimental results\nshow that our method clearly outperforms state-of-the-art methods on both\nbenchmark datasets by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:12:03 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Liu", "Lihao", ""], ["Hu", "Xiaowei", ""], ["Zhu", "Lei", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1907.01949", "submitter": "Shi Hu", "authors": "Shi Hu and Daniel Worrall and Stefan Knegt and Bas Veeling and Henkjan\n  Huisman and Max Welling", "title": "Supervised Uncertainty Quantification for Segmentation with Multiple\n  Annotations", "comments": "Accepted as a conference paper to MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate estimation of predictive uncertainty carries importance in\nmedical scenarios such as lung node segmentation. Unfortunately, most existing\nworks on predictive uncertainty do not return calibrated uncertainty estimates,\nwhich could be used in practice. In this work we exploit multi-grader\nannotation variability as a source of 'groundtruth' aleatoric uncertainty,\nwhich can be treated as a target in a supervised learning problem. We combine\nthis groundtruth uncertainty with a Probabilistic U-Net and test on the\nLIDC-IDRI lung nodule CT dataset and MICCAI2012 prostate MRI dataset. We find\nthat we are able to improve predictive uncertainty estimates. We also find that\nwe can improve sample accuracy and sample diversity.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:53:54 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Hu", "Shi", ""], ["Worrall", "Daniel", ""], ["Knegt", "Stefan", ""], ["Veeling", "Bas", ""], ["Huisman", "Henkjan", ""], ["Welling", "Max", ""]]}, {"id": "1907.01985", "submitter": "Keyan Ding", "authors": "Keyan Ding, Kede Ma, Shiqi Wang", "title": "Intrinsic Image Popularity Assessment", "comments": "Accepted by ACM Multimedia 2019", "journal-ref": "Proceedings of the 27th ACM International Conference on\n  Multimedia, 2019", "doi": "10.1145/3343031.3351007", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of research in automatic image popularity assessment (IPA) is to\ndevelop computational models that can accurately predict the potential of a\nsocial image to go viral on the Internet. Here, we aim to single out the\ncontribution of visual content to image popularity, i.e., intrinsic image\npopularity. Specifically, we first describe a probabilistic method to generate\nmassive popularity-discriminable image pairs, based on which the first\nlarge-scale image database for intrinsic IPA (I$^2$PA) is established. We then\ndevelop computational models for I$^2$PA based on deep neural networks,\noptimizing for ranking consistency with millions of popularity-discriminable\nimage pairs. Experiments on Instagram and other social platforms demonstrate\nthat the optimized model performs favorably against existing methods, exhibits\nreasonable generalizability on different databases, and even surpasses\nhuman-level performance on Instagram. In addition, we conduct a psychophysical\nexperiment to analyze various aspects of human behavior in I$^2$PA.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:15:21 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 15:38:50 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ding", "Keyan", ""], ["Ma", "Kede", ""], ["Wang", "Shiqi", ""]]}, {"id": "1907.01989", "submitter": "Juhyun Lee", "authors": "Juhyun Lee, Nikolay Chirkov, Ekaterina Ignasheva, Yury Pisarchyk,\n  Mogan Shieh, Fabio Riccardi, Raman Sarokin, Andrei Kulik, and Matthias\n  Grundmann", "title": "On-Device Neural Net Inference with Mobile GPUs", "comments": "Computer Vision and Pattern Recognition Workshop: Efficient Deep\n  Learning for Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-device inference of machine learning models for mobile phones is desirable\ndue to its lower latency and increased privacy. Running such a\ncompute-intensive task solely on the mobile CPU, however, can be difficult due\nto limited computing power, thermal constraints, and energy consumption. App\ndevelopers and researchers have begun exploiting hardware accelerators to\novercome these challenges. Recently, device manufacturers are adding neural\nprocessing units into high-end phones for on-device inference, but these\naccount for only a small fraction of hand-held devices. In this paper, we\npresent how we leverage the mobile GPU, a ubiquitous hardware accelerator on\nvirtually every phone, to run inference of deep neural networks in real-time\nfor both Android and iOS devices. By describing our architecture, we also\ndiscuss how to design networks that are mobile GPU-friendly. Our\nstate-of-the-art mobile GPU inference engine is integrated into the open-source\nproject TensorFlow Lite and publicly available at https://tensorflow.org/lite.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:23:20 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Lee", "Juhyun", ""], ["Chirkov", "Nikolay", ""], ["Ignasheva", "Ekaterina", ""], ["Pisarchyk", "Yury", ""], ["Shieh", "Mogan", ""], ["Riccardi", "Fabio", ""], ["Sarokin", "Raman", ""], ["Kulik", "Andrei", ""], ["Grundmann", "Matthias", ""]]}, {"id": "1907.01992", "submitter": "Andreas Maier", "authors": "Andreas K. Maier, Christopher Syben, Bernhard Stimpel, Tobias W\\\"urfl,\n  Mathis Hoffmann, Frank Schebesch, Weilin Fu, Leonid Mill, Lasse Kling, and\n  Silke Christiansen", "title": "Learning with Known Operators reduces Maximum Training Error Bounds", "comments": "Paper conditionally accepted in Nature Machine Intelligence", "journal-ref": "Nature Machine Intelligence 1, 373-380, 2019", "doi": "10.1038/s42256-019-0077-5", "report-no": null, "categories": "cs.LG cs.CV physics.med-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe an approach for incorporating prior knowledge into machine\nlearning algorithms. We aim at applications in physics and signal processing in\nwhich we know that certain operations must be embedded into the algorithm. Any\noperation that allows computation of a gradient or sub-gradient towards its\ninputs is suited for our framework. We derive a maximal error bound for deep\nnets that demonstrates that inclusion of prior knowledge results in its\nreduction. Furthermore, we also show experimentally that known operators reduce\nthe number of free parameters. We apply this approach to various tasks ranging\nfrom CT image reconstruction over vessel segmentation to the derivation of\npreviously unknown imaging algorithms. As such the concept is widely applicable\nfor many researchers in physics, imaging, and signal processing. We assume that\nour analysis will support further investigation of known operators in other\nfields of physics, imaging, and signal processing.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:35:16 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Maier", "Andreas K.", ""], ["Syben", "Christopher", ""], ["Stimpel", "Bernhard", ""], ["W\u00fcrfl", "Tobias", ""], ["Hoffmann", "Mathis", ""], ["Schebesch", "Frank", ""], ["Fu", "Weilin", ""], ["Mill", "Leonid", ""], ["Kling", "Lasse", ""], ["Christiansen", "Silke", ""]]}, {"id": "1907.01996", "submitter": "Thomas Gittings", "authors": "Thomas Gittings, Steve Schneider, John Collomosse", "title": "Robust Synthesis of Adversarial Visual Examples Using a Deep Image Prior", "comments": "Accepted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for generating robust adversarial image examples\nbuilding upon the recent `deep image prior' (DIP) that exploits convolutional\nnetwork architectures to enforce plausible texture in image synthesis.\nAdversarial images are commonly generated by perturbing images to introduce\nhigh frequency noise that induces image misclassification, but that is fragile\nto subsequent digital manipulation of the image. We show that using DIP to\nreconstruct an image under adversarial constraint induces perturbations that\nare more robust to affine deformation, whilst remaining visually imperceptible.\nFurthermore we show that our DIP approach can also be adapted to produce local\nadversarial patches (`adversarial stickers'). We demonstrate robust adversarial\nexamples over a broad gamut of images and object classes drawn from the\nImageNet dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:40:05 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Gittings", "Thomas", ""], ["Schneider", "Steve", ""], ["Collomosse", "John", ""]]}, {"id": "1907.02003", "submitter": "Pawel Mlynarski", "authors": "Pawel Mlynarski, Herv\\'e Delingette, Hamza Alghamdi, Pierre-Yves\n  Bondiau, Nicholas Ayache", "title": "Anatomically Consistent Segmentation of Organs at Risk in MRI with\n  Convolutional Neural Networks", "comments": "Submitted to Computerized Medical Imaging and Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning of radiotherapy involves accurate segmentation of a large number of\norgans at risk, i.e. organs for which irradiation doses should be minimized to\navoid important side effects of the therapy. We propose a deep learning method\nfor segmentation of organs at risk inside the brain region, from Magnetic\nResonance (MR) images. Our system performs segmentation of eight structures:\neye, lens, optic nerve, optic chiasm, pituitary gland, hippocampus, brainstem\nand brain. We propose an efficient algorithm to train neural networks for an\nend-to-end segmentation of multiple and non-exclusive classes, addressing\nproblems related to computational costs and missing ground truth segmentations\nfor a subset of classes. We enforce anatomical consistency of the result in a\npostprocessing step, in particular we introduce a graph-based algorithm for\nsegmentation of the optic nerves, enforcing the connectivity between the eyes\nand the optic chiasm. We report cross-validated quantitative results on a\ndatabase of 44 contrast-enhanced T1-weighted MRIs with provided segmentations\nof the considered organs at risk, which were originally used for radiotherapy\nplanning. In addition, the segmentations produced by our model on an\nindependent test set of 50 MRIs are evaluated by an experienced radiotherapist\nin order to qualitatively assess their accuracy. The mean distances between\nproduced segmentations and the ground truth ranged from 0.1 mm to 0.7 mm across\ndifferent organs. A vast majority (96 %) of the produced segmentations were\nfound acceptable for radiotherapy planning.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:55:43 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Mlynarski", "Pawel", ""], ["Delingette", "Herv\u00e9", ""], ["Alghamdi", "Hamza", ""], ["Bondiau", "Pierre-Yves", ""], ["Ayache", "Nicholas", ""]]}, {"id": "1907.02014", "submitter": "Sonam Damani", "authors": "Nitya Raviprakash, Sonam Damani, Ankush Chatterjee, Meghana Joshi,\n  Puneet Agrawal", "title": "Using AI for Economic Upliftment of Handicraft Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The handicraft industry is a strong pillar of Indian economy which provides\nlarge-scale employment opportunities to artisans in rural and underprivileged\ncommunities. However, in this era of globalization, diverse modern designs have\nrendered traditional designs old and monotonous, causing an alarming decline of\nhandicraft sales. For this age-old industry to survive the global competition,\nit is imperative to integrate contemporary designs with Indian handicrafts. In\nthis paper, we use novel AI techniques to generate contemporary designs for two\npopular Indian handicrafts - Ikat and Block Print. These techniques were\nsuccessfully employed by communities across India to manufacture and sell\nproducts with greater appeal and revenue. The designs are evaluated to be\nsignificantly more likeable and marketable than the current designs used by\nartisans.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 07:33:42 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Raviprakash", "Nitya", ""], ["Damani", "Sonam", ""], ["Chatterjee", "Ankush", ""], ["Joshi", "Meghana", ""], ["Agrawal", "Puneet", ""]]}, {"id": "1907.02022", "submitter": "Peter Anderson", "authors": "Peter Anderson, Ayush Shrivastava, Devi Parikh, Dhruv Batra and Stefan\n  Lee", "title": "Chasing Ghosts: Instruction Following as Bayesian State Tracking", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A visually-grounded navigation instruction can be interpreted as a sequence\nof expected observations and actions an agent following the correct trajectory\nwould encounter and perform. Based on this intuition, we formulate the problem\nof finding the goal location in Vision-and-Language Navigation (VLN) within the\nframework of Bayesian state tracking - learning observation and motion models\nconditioned on these expectable events. Together with a mapper that constructs\na semantic spatial map on-the-fly during navigation, we formulate an end-to-end\ndifferentiable Bayes filter and train it to identify the goal by predicting the\nmost likely trajectory through the map according to the instructions. The\nresulting navigation policy constitutes a new approach to instruction following\nthat explicitly models a probability distribution over states, encoding strong\ngeometric and algorithmic priors while enabling greater explainability. Our\nexperiments show that our approach outperforms a strong LingUNet baseline when\npredicting the goal location on the map. On the full VLN task, i.e. navigating\nto the goal location, our approach achieves promising results with less\nreliance on navigation constraints.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 16:39:05 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 18:52:33 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Anderson", "Peter", ""], ["Shrivastava", "Ayush", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""], ["Lee", "Stefan", ""]]}, {"id": "1907.02040", "submitter": "Michal Mackiewicz", "authors": "Ellen Bowler, Peter T. Fretwell, Geoffrey French, Michal Mackiewicz", "title": "Using Deep Learning to Count Albatrosses from Space", "comments": "4 pages, 5 figures, to be presented at IEEE 2019 International\n  Geoscience & Remote Sensing Symposium (IGARSS 2019), scheduled for July 28 -\n  August 2, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we test the use of a deep learning approach to automatically\ncount Wandering Albatrosses in Very High Resolution (VHR) satellite imagery. We\nuse a dataset of manually labelled imagery provided by the British Antarctic\nSurvey to train and develop our methods. We employ a U-Net architecture,\ndesigned for image segmentation, to simultaneously classify and localise\npotential albatrosses. We aid training with the use of the Focal Loss\ncriterion, to deal with extreme class imbalance in the dataset. Initial results\nachieve peak precision and recall values of approximately 80%. Finally we\nassess the model's performance in relation to inter-observer variation, by\ncomparing errors against an image labelled by multiple observers. We conclude\nmodel accuracy falls within the range of human counters. We hope that the\nmethods will streamline the analysis of VHR satellite images, enabling more\nfrequent monitoring of a species which is of high conservation concern.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 17:09:25 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Bowler", "Ellen", ""], ["Fretwell", "Peter T.", ""], ["French", "Geoffrey", ""], ["Mackiewicz", "Michal", ""]]}, {"id": "1907.02044", "submitter": "Francesco Croce", "authors": "Francesco Croce, Matthias Hein", "title": "Minimally distorted Adversarial Examples with a Fast Adaptive Boundary\n  Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of robustness against adversarial manipulation of neural\nnetworks-based classifiers is mainly tested with empirical attacks as methods\nfor the exact computation, even when available, do not scale to large networks.\nWe propose in this paper a new white-box adversarial attack wrt the $l_p$-norms\nfor $p \\in \\{1,2,\\infty\\}$ aiming at finding the minimal perturbation necessary\nto change the class of a given input. It has an intuitive geometric meaning,\nyields quickly high quality results, minimizes the size of the perturbation (so\nthat it returns the robust accuracy at every threshold with a single run). It\nperforms better or similar to state-of-the-art attacks which are partially\nspecialized to one $l_p$-norm, and is robust to the phenomenon of gradient\nmasking.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 17:22:05 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 15:18:47 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Croce", "Francesco", ""], ["Hein", "Matthias", ""]]}, {"id": "1907.02055", "submitter": "Tomas Jakab", "authors": "Tomas Jakab, Ankush Gupta, Hakan Bilen, Andrea Vedaldi", "title": "Self-supervised Learning of Interpretable Keypoints from Unlabelled\n  Videos", "comments": "CVPR 2020 (oral). Project page:\n  http://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2020, pp. 8787-8797", "doi": "10.1109/CVPR42600.2020.00881", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose KeypointGAN, a new method for recognizing the pose of objects from\na single image that for learning uses only unlabelled videos and a weak\nempirical prior on the object poses. Video frames differ primarily in the pose\nof the objects they contain, so our method distils the pose information by\nanalyzing the differences between frames. The distillation uses a new dual\nrepresentation of the geometry of objects as a set of 2D keypoints, and as a\npictorial representation, i.e. a skeleton image. This has three benefits: (1)\nit provides a tight `geometric bottleneck' which disentangles pose from\nappearance, (2) it can leverage powerful image-to-image translation networks to\nmap between photometry and geometry, and (3) it allows to incorporate empirical\npose priors in the learning process. The pose priors are obtained from unpaired\ndata, such as from a different dataset or modality such as mocap, such that no\nannotated image is ever used in learning the pose recognition network. In\nstandard benchmarks for pose recognition for humans and faces, our method\nachieves state-of-the-art performance among methods that do not require any\nlabelled images for training.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 17:47:08 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 18:59:02 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Jakab", "Tomas", ""], ["Gupta", "Ankush", ""], ["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1907.02060", "submitter": "Aneeq Zia", "authors": "Aneeq Zia, Liheng Guo, Linlin Zhou, Irfan Essa, Anthony Jarc", "title": "Novel evaluation of surgical activity recognition models using\n  task-based efficiency metrics", "comments": null, "journal-ref": "International Journal of Computer Assisted Radiology and Surgery\n  (IJCARS) 2019", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Surgical task-based metrics (rather than entire procedure metrics)\ncan be used to improve surgeon training and, ultimately, patient care through\nfocused training interventions. Machine learning models to automatically\nrecognize individual tasks or activities are needed to overcome the otherwise\nmanual effort of video review. Traditionally, these models have been evaluated\nusing frame-level accuracy. Here, we propose evaluating surgical activity\nrecognition models by their effect on task-based efficiency metrics. In this\nway, we can determine when models have achieved adequate performance for\nproviding surgeon feedback via metrics from individual tasks.\n  Methods: We propose a new CNN-LSTM model, RP-Net-V2, to recognize the 12\nsteps of robotic-assisted radical prostatectomies (RARP). We evaluated our\nmodel both in terms of conventional methods (e.g. Jaccard Index, task boundary\naccuracy) as well as novel ways, such as the accuracy of efficiency metrics\ncomputed from instrument movements and system events.\n  Results: Our proposed model achieves a Jaccard Index of 0.85 thereby\noutperforming previous models on robotic-assisted radical prostatectomies.\nAdditionally, we show that metrics computed from tasks automatically identified\nusing RP-Net-V2 correlate well with metrics from tasks labeled by clinical\nexperts.\n  Conclusions: We demonstrate that metrics-based evaluation of surgical\nactivity recognition models is a viable approach to determine when models can\nbe used to quantify surgical efficiencies. We believe this approach and our\nresults illustrate the potential for fully automated, post-operative efficiency\nreports.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 17:55:31 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zia", "Aneeq", ""], ["Guo", "Liheng", ""], ["Zhou", "Linlin", ""], ["Essa", "Irfan", ""], ["Jarc", "Anthony", ""]]}, {"id": "1907.02065", "submitter": "Lakshay Sharma", "authors": "Elaina Tan, Lakshay Sharma", "title": "Neural Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the biggest advances in major Computer Vision tasks, such as\nobject recognition, handwritten-digit identification, facial recognition, and\nmany others., have all come through the use of Convolutional Neural Networks\n(CNNs). Similarly, in the domain of Natural Language Processing, Recurrent\nNeural Networks (RNNs), and Long Short Term Memory networks (LSTMs) in\nparticular, have been crucial to some of the biggest breakthroughs in\nperformance for tasks such as machine translation, part-of-speech tagging,\nsentiment analysis, and many others. These individual advances have greatly\nbenefited tasks even at the intersection of NLP and Computer Vision, and\ninspired by this success, we studied some existing neural image captioning\nmodels that have proven to work well. In this work, we study some existing\ncaptioning models that provide near state-of-the-art performances, and try to\nenhance one such model. We also present a simple image captioning model that\nmakes use of a CNN, an LSTM, and the beam search1 algorithm, and study its\nperformance based on various qualitative and quantitative metrics.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 22:49:25 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Tan", "Elaina", ""], ["Sharma", "Lakshay", ""]]}, {"id": "1907.02096", "submitter": "Domonkos Varga", "authors": "Domonkos Varga", "title": "A comprehensive evaluation of full-reference image quality assessment\n  algorithms on KADID-10k", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Significant progress has been made in the past decade for full-reference\nimage quality assessment (FR-IQA). However, new large scale image quality\ndatabases have been released for evaluating image quality assessment\nalgorithms. In this study, our goal is to give a comprehensive evaluation of\nstate-of-the-art FR-IQA metrics using the recently published KADID-10k database\nwhich is largest available one at the moment. Our evaluation results and the\nassociated discussions is very helpful to obtain a clear understanding about\nthe status of state-of-the-art FR-IQA metrics.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:46:36 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Varga", "Domonkos", ""]]}, {"id": "1907.02110", "submitter": "Jimit Doshi", "authors": "Jimit Doshi, Guray Erus, Mohamad Habes, Christos Davatzikos", "title": "DeepMRSeg: A convolutional deep neural network for anatomy and\n  abnormality segmentation on MR images", "comments": "18 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation has been a major task in neuroimaging. A large number of\nautomated methods have been developed for segmenting healthy and diseased brain\ntissues. In recent years, deep learning techniques have attracted a lot of\nattention as a result of their high accuracy in different segmentation\nproblems. We present a new deep learning based segmentation method, DeepMRSeg,\nthat can be applied in a generic way to a variety of segmentation tasks. The\nproposed architecture combines recent advances in the field of biomedical image\nsegmentation and computer vision. We use a modified UNet architecture that\ntakes advantage of multiple convolution filter sizes to achieve multi-scale\nfeature extraction adaptive to the desired segmentation task. Importantly, our\nmethod operates on minimally processed raw MRI scan. We validated our method on\na wide range of segmentation tasks, including white matter lesion segmentation,\nsegmentation of deep brain structures and hippocampus segmentation. We provide\ncode and pre-trained models to allow researchers apply our method on their own\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 19:10:37 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Doshi", "Jimit", ""], ["Erus", "Guray", ""], ["Habes", "Mohamad", ""], ["Davatzikos", "Christos", ""]]}, {"id": "1907.02124", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Sheng Lin, Shaokai Ye, Zhezhi He, Linfeng Zhang, Geng\n  Yuan, Sia Huat Tan, Zhengang Li, Deliang Fan, Xuehai Qian, Xue Lin, Kaisheng\n  Ma, Yanzhi Wang", "title": "Non-Structured DNN Weight Pruning -- Is It Beneficial in Any Platform?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large deep neural network (DNN) models pose the key challenge to energy\nefficiency due to the significantly higher energy consumption of off-chip DRAM\naccesses than arithmetic or SRAM operations. It motivates the intensive\nresearch on model compression with two main approaches. Weight pruning\nleverages the redundancy in the number of weights and can be performed in a\nnon-structured, which has higher flexibility and pruning rate but incurs index\naccesses due to irregular weights, or structured manner, which preserves the\nfull matrix structure with lower pruning rate. Weight quantization leverages\nthe redundancy in the number of bits in weights. Compared to pruning,\nquantization is much more hardware-friendly, and has become a \"must-do\" step\nfor FPGA and ASIC implementations. This paper provides a definitive answer to\nthe question for the first time. First, we build ADMM-NN-S by extending and\nenhancing ADMM-NN, a recently proposed joint weight pruning and quantization\nframework. Second, we develop a methodology for fair and fundamental comparison\nof non-structured and structured pruning in terms of both storage and\ncomputation efficiency. Our results show that ADMM-NN-S consistently\noutperforms the prior art: (i) it achieves 348x, 36x, and 8x overall weight\npruning on LeNet-5, AlexNet, and ResNet-50, respectively, with (almost) zero\naccuracy loss; (ii) we demonstrate the first fully binarized (for all layers)\nDNNs can be lossless in accuracy in many cases. These results provide a strong\nbaseline and credibility of our study. Based on the proposed comparison\nframework, with the same accuracy and quantization, the results show that\nnon-structrued pruning is not competitive in terms of both storage and\ncomputation efficiency. Thus, we conclude that non-structured pruning is\nconsidered harmful. We urge the community not to continue the DNN inference\nacceleration for non-structured sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 20:27:51 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 19:43:16 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Ma", "Xiaolong", ""], ["Lin", "Sheng", ""], ["Ye", "Shaokai", ""], ["He", "Zhezhi", ""], ["Zhang", "Linfeng", ""], ["Yuan", "Geng", ""], ["Tan", "Sia Huat", ""], ["Li", "Zhengang", ""], ["Fan", "Deliang", ""], ["Qian", "Xuehai", ""], ["Lin", "Xue", ""], ["Ma", "Kaisheng", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1907.02129", "submitter": "Marat Dukhan", "authors": "Marat Dukhan", "title": "The Indirect Convolution Algorithm", "comments": "Presented on Efficient Deep Learning for Computer Vision workshop at\n  CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning frameworks commonly implement convolution operators with\nGEMM-based algorithms. In these algorithms, convolution is implemented on top\nof matrix-matrix multiplication (GEMM) functions, provided by highly optimized\nBLAS libraries. Convolutions with 1x1 kernels can be directly represented as a\nGEMM call, but convolutions with larger kernels require a special memory layout\ntransformation - im2col or im2row - to fit into GEMM interface.\n  The Indirect Convolution algorithm provides the efficiency of the GEMM\nprimitive without the overhead of im2col transformation. In contrast to\nGEMM-based algorithms, the Indirect Convolution does not reshuffle the data to\nfit into the GEMM primitive but introduces an indirection buffer - a buffer of\npointers to the start of each row of image pixels. This broadens the\napplication of our modified GEMM function to convolutions with arbitrary kernel\nsize, padding, stride, and dilation.\n  The Indirect Convolution algorithm reduces memory overhead proportionally to\nthe number of input channels and outperforms the GEMM-based algorithm by up to\n62% on convolution parameters which involve im2col transformations in\nGEMM-based algorithms. This, however, comes at cost of minor performance\nreduction on 1x1 stride-1 convolutions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 20:51:18 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Dukhan", "Marat", ""]]}, {"id": "1907.02149", "submitter": "Florian Piewak", "authors": "Florian Piewak, Peter Pinggera, and Marius Z\\\"ollner", "title": "Analyzing the Cross-Sensor Portability of Neural Network Architectures\n  for LiDAR-based Semantic Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art approaches for the semantic labeling of LiDAR point clouds\nheavily rely on the use of deep Convolutional Neural Networks (CNNs). However,\ntransferring network architectures across different LiDAR sensor types\nrepresents a significant challenge, especially due to sensor specific design\nchoices with regard to network architecture as well as data representation. In\nthis paper we propose a new CNN architecture for the point-wise semantic\nlabeling of LiDAR data which achieves state-of-the-art results while increasing\nportability across sensor types. This represents a significant advantage given\nthe fast-paced development of LiDAR hardware technology. We perform a thorough\nquantitative cross-sensor analysis of semantic labeling performance in\ncomparison to a state-of-the-art reference method. Our evaluation shows that\nthe proposed architecture is indeed highly portable, yielding an improvement of\n10 percentage points in the Intersection-over-Union (IoU) score when compared\nto the reference approach. Further, the results indicate that the proposed\nnetwork architecture can provide an efficient way for the automated generation\nof large-scale training data for novel LiDAR sensor types without the need for\nextensive manual annotation or multi-modal label transfer.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 22:19:37 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Piewak", "Florian", ""], ["Pinggera", "Peter", ""], ["Z\u00f6llner", "Marius", ""]]}, {"id": "1907.02157", "submitter": "Ankit Sharma", "authors": "Ankit Sharma, Hassan Foroosh", "title": "Slim-CNN: A Light-Weight CNN for Face Attribute Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a computationally-efficient CNN micro-architecture Slim Module\nto design a lightweight deep neural network Slim-Net for face attribute\nprediction. Slim Modules are constructed by assembling depthwise separable\nconvolutions with pointwise convolution to produce a computationally efficient\nmodule. The problem of facial attribute prediction is challenging because of\nthe large variations in pose, background, illumination, and dataset imbalance.\nWe stack these Slim Modules to devise a compact CNN which still maintains very\nhigh accuracy. Additionally, the neural network has a very low memory footprint\nwhich makes it suitable for mobile and embedded applications. Experiments on\nthe CelebA dataset show that Slim-Net achieves an accuracy of 91.24% with at\nleast 25 times fewer parameters than comparably performing methods, which\nreduces the memory storage requirement of Slim-net by at least 87%.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 23:02:44 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Sharma", "Ankit", ""], ["Foroosh", "Hassan", ""]]}, {"id": "1907.02161", "submitter": "Sarah Ostadabbas", "authors": "Shuangjun Liu and Sarah Ostadabbas", "title": "Seeing Under the Cover: A Physics Guided Learning Approach for In-Bed\n  Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human in-bed pose estimation has huge practical values in medical and\nhealthcare applications yet still mainly relies on expensive pressure mapping\n(PM) solutions. In this paper, we introduce our novel physics inspired\nvision-based approach that addresses the challenging issues associated with the\nin-bed pose estimation problem including monitoring a fully covered person in\ncomplete darkness. We reformulated this problem using our proposed Under the\nCover Imaging via Thermal Diffusion (UCITD) method to capture the high\nresolution pose information of the body even when it is fully covered by using\na long wavelength IR technique. We proposed a physical hyperparameter concept\nthrough which we achieved high quality groundtruth pose labels in different\nmodalities. A fully annotated in-bed pose dataset called\nSimultaneously-collected multimodal Lying Pose (SLP) is also formed/released\nwith the same order of magnitude as most existing large-scale human pose\ndatasets to support complex models' training and evaluation. A network trained\nfrom scratch on it and tested on two diverse settings, one in a living room and\nthe other in a hospital room showed pose estimation performance of 99.5% and\n95.7% in PCK0.2 standard, respectively. Moreover, in a multi-factor comparison\nwith a state-of-the art in-bed pose monitoring solution based on PM, our\nsolution showed significant superiority in all practical aspects by being 60\ntimes cheaper, 300 times smaller, while having higher pose recognition\ngranularity and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 23:41:23 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 15:23:37 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 15:00:51 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Liu", "Shuangjun", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1907.02198", "submitter": "XingJiao Wu", "authors": "Xingjiao Wu, Baohan Xu, Yingbin Zheng, Hao Ye, Jing Yang, Liang He", "title": "Fast Video Crowd Counting with a Temporal Aware Network", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting aims to count the number of instantaneous people in a crowded\nspace, and many promising solutions have been proposed for single image crowd\ncounting. With the ubiquitous video capture devices in public safety field, how\nto effectively apply the crowd counting technique to video content has become\nan urgent problem. In this paper, we introduce a novel framework based on\ntemporal aware modeling of the relationship between video frames. The proposed\nnetwork contains a few dilated residual blocks, and each of them consists of\nthe layers that compute the temporal convolutions of features from the adjacent\nframes to improve the prediction. To alleviate the expensive computation and\nsatisfy the demand of fast video crowd counting, we also introduce a\nlightweight network to balance the computational cost with representation\nability. We conduct experiments on the crowd counting benchmarks and\ndemonstrate its superiority in terms of effectiveness and efficiency over\nprevious video-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 03:07:22 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 07:25:48 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Wu", "Xingjiao", ""], ["Xu", "Baohan", ""], ["Zheng", "Yingbin", ""], ["Ye", "Hao", ""], ["Yang", "Jing", ""], ["He", "Liang", ""]]}, {"id": "1907.02228", "submitter": "Christen Miller", "authors": "Christen M, AB Saravanan", "title": "RFBTD: RFB Text Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text detection plays a critical role in the whole procedure of textual\ninformation extraction and understanding. On a high note, recent years have\nseen a surge in the high recall text detectors in scene text images, however\ntext boxes for individual words is still a challenging when dense text is\npresent in the scene. In this work, we propose an elegant solution that\npromotes prediction of words or text lines of arbitrary orientations and\ndirections, providing emphasis on individual words. We also investigate the\neffects of Receptive Field Blocks(RFB) and its impact in receptive fields for\ntext segments. Experiments were done on the ICDAR2015 and achieves an F-score\nof 47.09 at 720p\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 05:31:59 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["M", "Christen", ""], ["Saravanan", "AB", ""]]}, {"id": "1907.02244", "submitter": "Son Tran", "authors": "Son Tran, Ming Du, Sampath Chanda, R. Manmatha, Cj Taylor", "title": "Searching for Apparel Products from Images in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this age of social media, people often look at what others are wearing. In\nparticular, Instagram and Twitter influencers often provide images of\nthemselves wearing different outfits and their followers are often inspired to\nbuy similar clothes.We propose a system to automatically find the closest\nvisually similar clothes in the online Catalog (street-to-shop searching). The\nproblem is challenging since the original images are taken under different pose\nand lighting conditions. The system initially localizes high-level descriptive\nregions (top, bottom, wristwear. . . ) using multiple CNN detectors such as\nYOLO and SSD that are trained specifically for apparel domain. It then\nclassifies these regions into more specific regions such as t-shirts, tunic or\ndresses. Finally, a feature embedding learned using a multi-task function is\nrecovered for every item and then compared with corresponding items in the\nonline Catalog database and ranked according to distance. We validate our\napproach component-wise using benchmark datasets and end-to-end using human\nevaluation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 06:51:03 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Tran", "Son", ""], ["Du", "Ming", ""], ["Chanda", "Sampath", ""], ["Manmatha", "R.", ""], ["Taylor", "Cj", ""]]}, {"id": "1907.02248", "submitter": "Qi Chen", "authors": "Wenjun Liu, Yuchun Huang, Ying Li, and Qi Chen", "title": "FPCNet: Fast Pavement Crack Detection Network Based on Encoder-Decoder\n  Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely, accurate and automatic detection of pavement cracks is necessary for\nmaking cost-effective decisions concerning road maintenance. Conventional crack\ndetection algorithms focus on the design of single or multiple crack features\nand classifiers. However, complicated topological structures, varying degrees\nof damage and oil stains make the design of crack features difficult. In\naddition, the contextual information around a crack is not investigated\nextensively in the design process. Accordingly, these design features have\nlimited discriminative adaptability and cannot fuse effectively with the\nclassifiers. To solve these problems, this paper proposes a deep learning\nnetwork for pavement crack detection. Using the Encoder-Decoder structure,\ncrack characteristics with multiple contexts are automatically learned, and\nend-to-end crack detection is achieved. Specifically, we first propose the\nMulti-Dilation (MD) module, which can synthesize the crack features of multiple\ncontext sizes via dilated convolution with multiple rates. The crack MD\nfeatures obtained in this module can describe cracks of different widths and\ntopologies. Next, we propose the SE-Upsampling (SEU) module, which uses the\nSqueeze-and-Excitation learning operation to optimize the MD features. Finally,\nthe above two modules are integrated to develop the fast crack detection\nnetwork, namely, FPCNet. This network continuously optimizes the MD features\nstep-by-step to realize fast pixel-level crack detection. Experiments are\nconducted on challenging public CFD datasets and G45 crack datasets involving\nvarious crack types under different shooting conditions. The distinct\nperformance and speed improvements over all the datasets demonstrate that the\nproposed method outperforms other state-of-the-art crack detection methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 06:56:24 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Liu", "Wenjun", ""], ["Huang", "Yuchun", ""], ["Li", "Ying", ""], ["Chen", "Qi", ""]]}, {"id": "1907.02253", "submitter": "Byung-Hak Kim", "authors": "Byung-Hak Kim and Varun Ganapathi", "title": "Lumi\\`ereNet: Lecture Video Synthesis from Audio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Lumi\\`ereNet, a simple, modular, and completely deep-learning\nbased architecture that synthesizes, high quality, full-pose headshot lecture\nvideos from instructor's new audio narration of any length. Unlike prior works,\nLumi\\`ereNet is entirely composed of trainable neural network modules to learn\nmapping functions from the audio to video through (intermediate) estimated\npose-based compact and abstract latent codes. Our video demos are available at\n[22] and [23].\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 07:21:24 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Kim", "Byung-Hak", ""], ["Ganapathi", "Varun", ""]]}, {"id": "1907.02282", "submitter": "Zhichao Fu", "authors": "Zhichao Fu, Tianlong Ma, Yingbin Zheng, Hao Ye, Jing Yang, and Liang\n  He", "title": "Edge-Aware Deep Image Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deblurring is a fundamental and challenging low-level vision problem.\nPrevious vision research indicates that edge structure in natural scenes is one\nof the most important factors to estimate the abilities of human visual\nperception. In this paper, we resort to human visual demands of sharp edges and\npropose a two-phase edge-aware deep network to improve deep image deblurring.\nAn edge detection convolutional subnet is designed in the first phase and a\nresidual fully convolutional deblur subnet is then used for generating deblur\nresults. The introduction of the edge-aware network enables our model with the\nspecific capacity of enhancing images with sharp edges. We successfully apply\nour framework on standard benchmarks and promising results are achieved by our\nproposed deblur model.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 08:57:54 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 07:21:26 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Fu", "Zhichao", ""], ["Ma", "Tianlong", ""], ["Zheng", "Yingbin", ""], ["Ye", "Hao", ""], ["Yang", "Jing", ""], ["He", "Liang", ""]]}, {"id": "1907.02336", "submitter": "Alexandre Bruckert", "authors": "Alexandre Bruckert, Hamed R. Tavakoli, Zhi Liu, Marc Christie, Olivier\n  Le Meur", "title": "Deep Saliency Models : The Quest For The Loss Function", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have pushed the performances of visual\nsaliency models way further than it has ever been. Numerous models in the\nliterature present new ways to design neural networks, to arrange gaze pattern\ndata, or to extract as much high and low-level image features as possible in\norder to create the best saliency representation. However, one key part of a\ntypical deep learning model is often neglected: the choice of the loss\nfunction.\n  In this work, we explore some of the most popular loss functions that are\nused in deep saliency models. We demonstrate that on a fixed network\narchitecture, modifying the loss function can significantly improve (or\ndepreciate) the results, hence emphasizing the importance of the choice of the\nloss function when designing a model. We also introduce new loss functions that\nhave never been used for saliency prediction to our knowledge. And finally, we\nshow that a linear combination of several well-chosen loss functions leads to\nsignificant improvements in performances on different datasets as well as on a\ndifferent network architecture, hence demonstrating the robustness of a\ncombined metric.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 11:46:34 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Bruckert", "Alexandre", ""], ["Tavakoli", "Hamed R.", ""], ["Liu", "Zhi", ""], ["Christie", "Marc", ""], ["Meur", "Olivier Le", ""]]}, {"id": "1907.02364", "submitter": "Dongze Lian", "authors": "Dongze Lian and Zehao Yu and Shenghua Gao", "title": "Believe It or Not, We Know What You Are Looking at!", "comments": "ACCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By borrowing the wisdom of human in gaze following, we propose a two-stage\nsolution for gaze point prediction of the target persons in a scene.\nSpecifically, in the first stage, both head image and its position are fed into\na gaze direction pathway to predict the gaze direction, and then multi-scale\ngaze direction fields are generated to characterize the distribution of gaze\npoints without considering the scene contents. In the second stage, the\nmulti-scale gaze direction fields are concatenated with the image contents and\nfed into a heatmap pathway for heatmap regression. There are two merits for our\ntwo-stage solution based gaze following: i) our solution mimics the behavior of\nhuman in gaze following, therefore it is more psychological plausible; ii)\nbesides using heatmap to supervise the output of our network, we can also\nleverage gaze direction to facilitate the training of gaze direction pathway,\ntherefore our network can be more robustly trained. Considering that existing\ngaze following dataset is annotated by the third-view persons, we build a video\ngaze following dataset, where the ground truth is annotated by the observers in\nthe videos. Therefore it is more reliable. The evaluation with such a dataset\nreflects the capacity of different methods in real scenarios better. Extensive\nexperiments on both datasets show that our method significantly outperforms\nexisting methods, which validates the effectiveness of our solution for gaze\nfollowing. Our dataset and codes are released in\nhttps://github.com/svip-lab/GazeFollowing.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 12:29:06 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Lian", "Dongze", ""], ["Yu", "Zehao", ""], ["Gao", "Shenghua", ""]]}, {"id": "1907.02392", "submitter": "Lynton Ardizzone", "authors": "Lynton Ardizzone, Carsten L\\\"uth, Jakob Kruse, Carsten Rother, Ullrich\n  K\\\"othe", "title": "Guided Image Generation with Conditional Invertible Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the task of natural image generation guided by a\nconditioning input. We introduce a new architecture called conditional\ninvertible neural network (cINN). The cINN combines the purely generative INN\nmodel with an unconstrained feed-forward network, which efficiently\npreprocesses the conditioning input into useful features. All parameters of the\ncINN are jointly optimized with a stable, maximum likelihood-based training\nprocedure. By construction, the cINN does not experience mode collapse and\ngenerates diverse samples, in contrast to e.g. cGANs. At the same time our\nmodel produces sharp images since no reconstruction loss is required, in\ncontrast to e.g. VAEs. We demonstrate these properties for the tasks of MNIST\ndigit generation and image colorization. Furthermore, we take advantage of our\nbi-directional cINN architecture to explore and manipulate emergent properties\nof the latent space, such as changing the image style in an intuitive way.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 13:20:57 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 12:49:31 GMT"}, {"version": "v3", "created": "Wed, 10 Jul 2019 11:10:36 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Ardizzone", "Lynton", ""], ["L\u00fcth", "Carsten", ""], ["Kruse", "Jakob", ""], ["Rother", "Carsten", ""], ["K\u00f6the", "Ullrich", ""]]}, {"id": "1907.02413", "submitter": "Shaohua Li", "authors": "Shaohua Li, Yong Liu, Xiuchao Sui, Cheng Chen, Gabriel Tjio, Daniel\n  Shu Wei Ting, Rick Siow Mong Goh", "title": "Multi-Instance Multi-Scale CNN for Medical Image Classification", "comments": "Accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for medical image classification faces three major challenges:\n1) the number of annotated medical images for training are usually small; 2)\nregions of interest (ROIs) are relatively small with unclear boundaries in the\nwhole medical images, and may appear in arbitrary positions across the x,y (and\nalso z in 3D images) dimensions. However often only labels of the whole images\nare annotated, and localized ROIs are unavailable; and 3) ROIs in medical\nimages often appear in varying sizes (scales). We approach these three\nchallenges with a Multi-Instance Multi-Scale (MIMS) CNN: 1) We propose a\nmulti-scale convolutional layer, which extracts patterns of different receptive\nfields with a shared set of convolutional kernels, so that scale-invariant\npatterns are captured by this compact set of kernels. As this layer contains\nonly a small number of parameters, training on small datasets becomes feasible;\n2) We propose a \"top-k pooling\" to aggregate the feature maps in varying scales\nfrom multiple spatial dimensions, allowing the model to be trained using weak\nannotations within the multiple instance learning (MIL) framework. Our method\nis shown to perform well on three classification tasks involving two 3D and two\n2D medical image datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 14:11:22 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 06:39:42 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 05:56:06 GMT"}, {"version": "v4", "created": "Tue, 22 Oct 2019 06:34:12 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Li", "Shaohua", ""], ["Liu", "Yong", ""], ["Sui", "Xiuchao", ""], ["Chen", "Cheng", ""], ["Tjio", "Gabriel", ""], ["Ting", "Daniel Shu Wei", ""], ["Goh", "Rick Siow Mong", ""]]}, {"id": "1907.02499", "submitter": "Carl Doersch", "authors": "Carl Doersch, Andrew Zisserman", "title": "Sim2real transfer learning for 3D human pose estimation: motion to the\n  rescue", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic visual data can provide practically infinite diversity and rich\nlabels, while avoiding ethical issues with privacy and bias. However, for many\ntasks, current models trained on synthetic data generalize poorly to real data.\nThe task of 3D human pose estimation is a particularly interesting example of\nthis sim2real problem, because learning-based approaches perform reasonably\nwell given real training data, yet labeled 3D poses are extremely difficult to\nobtain in the wild, limiting scalability. In this paper, we show that standard\nneural-network approaches, which perform poorly when trained on synthetic RGB\nimages, can perform well when the data is pre-processed to extract cues about\nthe person's motion, notably as optical flow and the motion of 2D keypoints.\nTherefore, our results suggest that motion can be a simple way to bridge a\nsim2real gap when video is available. We evaluate on the 3D Poses in the Wild\ndataset, the most challenging modern benchmark for 3D pose estimation, where we\nshow full 3D mesh recovery that is on par with state-of-the-art methods trained\non real 3D sequences, despite training only on synthetic humans from the\nSURREAL dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 17:27:18 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 15:36:28 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Doersch", "Carl", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1907.02544", "submitter": "Jeff Donahue", "authors": "Jeff Donahue and Karen Simonyan", "title": "Large Scale Adversarial Representation Learning", "comments": "32 pages. In proceedings of NeurIPS 2019. This is the camera-ready\n  version of the paper, with supplementary material included as appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarially trained generative models (GANs) have recently achieved\ncompelling image synthesis results. But despite early successes in using GANs\nfor unsupervised representation learning, they have since been superseded by\napproaches based on self-supervision. In this work we show that progress in\nimage generation quality translates to substantially improved representation\nlearning performance. Our approach, BigBiGAN, builds upon the state-of-the-art\nBigGAN model, extending it to representation learning by adding an encoder and\nmodifying the discriminator. We extensively evaluate the representation\nlearning and generation capabilities of these BigBiGAN models, demonstrating\nthat these generation-based models achieve the state of the art in unsupervised\nrepresentation learning on ImageNet, as well as in unconditional image\ngeneration. Pretrained BigBiGAN models -- including image generators and\nencoders -- are available on TensorFlow Hub\n(https://tfhub.dev/s?publisher=deepmind&q=bigbigan).\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 18:00:17 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 18:05:57 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Donahue", "Jeff", ""], ["Simonyan", "Karen", ""]]}, {"id": "1907.02545", "submitter": "Weiwei Sun", "authors": "Weiwei Sun, Wei Jiang, Eduard Trulls, Andrea Tagliasacchi, Kwang Moo\n  Yi", "title": "ACNe: Attentive Context Normalization for Robust Permutation-Equivariant\n  Learning", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in computer vision require dealing with sparse, unordered data\nin the form of point clouds. Permutation-equivariant networks have become a\npopular solution-they operate on individual data points with simple perceptrons\nand extract contextual information with global pooling. This can be achieved\nwith a simple normalization of the feature maps, a global operation that is\nunaffected by the order. In this paper, we propose Attentive Context\nNormalization (ACN), a simple yet effective technique to build\npermutation-equivariant networks robust to outliers. Specifically, we show how\nto normalize the feature maps with weights that are estimated within the\nnetwork, excluding outliers from this normalization. We use this mechanism to\nleverage two types of attention: local and global-by combining them, our method\nis able to find the essential data points in high-dimensional space to solve a\ngiven task. We demonstrate through extensive experiments that our approach,\nwhich we call Attentive Context Networks (ACNe), provides a significant leap in\nperformance compared to the state-of-the-art on camera pose estimation, robust\nfitting, and point cloud classification under noise and outliers. Source code:\nhttps://github.com/vcg-uvic/acne.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 18:01:24 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 01:05:37 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 06:08:09 GMT"}, {"version": "v4", "created": "Thu, 23 Apr 2020 03:37:55 GMT"}, {"version": "v5", "created": "Mon, 1 Feb 2021 04:09:51 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Sun", "Weiwei", ""], ["Jiang", "Wei", ""], ["Trulls", "Eduard", ""], ["Tagliasacchi", "Andrea", ""], ["Yi", "Kwang Moo", ""]]}, {"id": "1907.02547", "submitter": "Le Thanh Nguyen-Meidine", "authors": "Hugo Masson, Amran Bhuiyan, Le Thanh Nguyen-Meidine, Mehrsan Javan,\n  Parthipan Siva, Ismail Ben Ayed, Eric Granger", "title": "Exploiting Prunability for Person Re-Identification", "comments": "Accepted for EURASIP Journal on Image and Video Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a substantial increase in the deep learning\n(DL)architectures proposed for visual recognition tasks like person\nre-identification,where individuals must be recognized over multiple\ndistributed cameras. Althoughthese architectures have greatly improved the\nstate-of-the-art accuracy, thecomputational complexity of the CNNs commonly\nused for feature extractionremains an issue, hindering their deployment on\nplatforms with limited resources,or in applications with real-time constraints.\nThere is an obvious advantage toaccelerating and compressing DL models without\nsignificantly decreasing theiraccuracy. However, the source (pruning) domain\ndiffers from operational (target)domains, and the domain shift between image\ndata captured with differentnon-overlapping camera viewpoints leads to lower\nrecognition accuracy. In thispaper, we investigate the prunability of these\narchitectures under different designscenarios. This paper first revisits\npruning techniques that are suitable forreducing the computational complexity\nof deep CNN networks applied to personre-identification. Then, these techniques\nare analysed according to their pruningcriteria and strategy, and according to\ndifferent scenarios for exploiting pruningmethods to fine-tuning networks to\ntarget domains. Experimental resultsobtained using DL models with ResNet\nfeature extractors, and multiplebenchmarks re-identification datasets, indicate\nthat pruning can considerablyreduce network complexity while maintaining a high\nlevel of accuracy. Inscenarios where pruning is performed with large\npre-training or fine-tuningdatasets, the number of FLOPS required by ResNet\narchitectures is reduced byhalf, while maintaining a comparable rank-1 accuracy\n(within 1% of the originalmodel). Pruning while training a larger CNNs can also\nprovide a significantlybetter performance than fine-tuning smaller ones.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 18:02:53 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 17:19:59 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Masson", "Hugo", ""], ["Bhuiyan", "Amran", ""], ["Nguyen-Meidine", "Le Thanh", ""], ["Javan", "Mehrsan", ""], ["Siva", "Parthipan", ""], ["Ayed", "Ismail Ben", ""], ["Granger", "Eric", ""]]}, {"id": "1907.02549", "submitter": "Hlynur Dav\\'i{\\dh} Hlynsson", "authors": "Hlynur Dav\\'i{\\dh} Hlynsson, Alberto N. Escalante-B., Laurenz Wiskott", "title": "Measuring the Data Efficiency of Deep Learning Methods", "comments": "8 pages", "journal-ref": "In Proceedings of the 8th International Conference on Pattern\n  Recognition Applications and Methods - Volume 1: ICPRAM (2019) pages 691-698", "doi": "10.5220/0007456306910698", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new experimental protocol and use it to benchmark\nthe data efficiency --- performance as a function of training set size --- of\ntwo deep learning algorithms, convolutional neural networks (CNNs) and\nhierarchical information-preserving graph-based slow feature analysis (HiGSFA),\nfor tasks in classification and transfer learning scenarios. The algorithms are\ntrained on different-sized subsets of the MNIST and Omniglot data sets. HiGSFA\noutperforms standard CNN networks when the models are trained on 50 and 200\nsamples per class for MNIST classification. In other cases, the CNNs perform\nbetter. The results suggest that there are cases where greedy, locally optimal\nbottom-up learning is equally or more powerful than global gradient-based\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 15:22:23 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Hlynsson", "Hlynur Dav\u00ed\u00f0", ""], ["Escalante-B.", "Alberto N.", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1907.02567", "submitter": "Rupert Brooks", "authors": "Jen-Tang Lu and Rupert Brooks and Stefan Hahn and Jin Chen and Varun\n  Buch and Gopal Kotecha and Katherine P. Andriole and Brian Ghoshhajra and\n  Joel Pinto and Paul Vozila and Mark Michalski and Neil A. Tenenholtz", "title": "DeepAAA: clinically applicable and generalizable detection of abdominal\n  aortic aneurysm using deep learning", "comments": "Accepted for publication at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning-based technique for detection and quantification\nof abdominal aortic aneurysms (AAAs). The condition, which leads to more than\n10,000 deaths per year in the United States, is asymptomatic, often detected\nincidentally, and often missed by radiologists. Our model architecture is a\nmodified 3D U-Net combined with ellipse fitting that performs aorta\nsegmentation and AAA detection. The study uses 321 abdominal-pelvic CT\nexaminations performed by Massachusetts General Hospital Department of\nRadiology for training and validation. The model is then further tested for\ngeneralizability on a separate set of 57 examinations with differing patient\ndemographics and acquisition characteristics than the original dataset. DeepAAA\nachieves high performance on both sets of data (sensitivity/specificity\n0.91/0.95 and 0.85 / 1.0 respectively), on contrast and non-contrast CT scans\nand works with image volumes with varying numbers of images. We find that\nDeepAAA exceeds literature-reported performance of radiologists on incidental\nAAA detection. It is expected that the model can serve as an effective\nbackground detector in routine CT examinations to prevent incidental AAAs from\nbeing missed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 19:41:37 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Lu", "Jen-Tang", ""], ["Brooks", "Rupert", ""], ["Hahn", "Stefan", ""], ["Chen", "Jin", ""], ["Buch", "Varun", ""], ["Kotecha", "Gopal", ""], ["Andriole", "Katherine P.", ""], ["Ghoshhajra", "Brian", ""], ["Pinto", "Joel", ""], ["Vozila", "Paul", ""], ["Michalski", "Mark", ""], ["Tenenholtz", "Neil A.", ""]]}, {"id": "1907.02586", "submitter": "Guangfeng Lin", "authors": "Guangfeng Lin and Jing Wang and Kaiyang Liao and Fan Zhao and Wanjun\n  Chen", "title": "Structure fusion based on graph convolutional networks for\n  semi-supervised classification", "comments": null, "journal-ref": "Electronics,2020", "doi": "10.3390/electronics9030432", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Suffering from the multi-view data diversity and complexity for\nsemi-supervised classification, most of existing graph convolutional networks\nfocus on the networks architecture construction or the salient graph structure\npreservation, and ignore the the complete graph structure for semi-supervised\nclassification contribution. To mine the more complete distribution structure\nfrom multi-view data with the consideration of the specificity and the\ncommonality, we propose structure fusion based on graph convolutional networks\n(SF-GCN) for improving the performance of semi-supervised classification.\nSF-GCN can not only retain the special characteristic of each view data by\nspectral embedding, but also capture the common style of multi-view data by\ndistance metric between multi-graph structures. Suppose the linear relationship\nbetween multi-graph structures, we can construct the optimization function of\nstructure fusion model by balancing the specificity loss and the commonality\nloss. By solving this function, we can simultaneously obtain the fusion\nspectral embedding from the multi-view data and the fusion structure as\nadjacent matrix to input graph convolutional networks for semi-supervised\nclassification. Experiments demonstrate that the performance of SF-GCN\noutperforms that of the state of the arts on three challenging datasets, which\nare Cora,Citeseer and Pubmed in citation networks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 23:43:05 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Lin", "Guangfeng", ""], ["Wang", "Jing", ""], ["Liao", "Kaiyang", ""], ["Zhao", "Fan", ""], ["Chen", "Wanjun", ""]]}, {"id": "1907.02634", "submitter": "Joshua Siegel", "authors": "Joshua E. Siegel and Maria F. Beemer and Steven M. Shepard", "title": "Automated Non-Destructive Inspection of Fused Filament Fabrication\n  Components Using Thermographic Signal Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1016/j.addma.2019.100923", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manufacturers struggle to produce low-cost, robust and complex components at\nmanufacturing lot-size one. Additive processes like Fused Filament Fabrication\n(FFF) inexpensively produce complex geometries, but defects limit viability in\ncritical applications. We present an approach to high-accuracy, high-throughput\nand low-cost automated non-destructive testing (NDT) for FFF interlayer\ndelamination using Flash Thermography (FT) data processed with Thermographic\nSignal Reconstruction (TSR) and Artificial Intelligence (AI). A Deep Neural\nNetwork (DNN) attains 95.4% per-pixel accuracy when differentiating four\ndelamination thicknesses 5mm subsurface in PolyLactic Acid (PLA) widgets, and\n98.6% accuracy in differentiating acceptable from unacceptable condition for\nthe same components. Automated inspection enables time- and cost-efficient 100%\ninspection for delamination defects, supporting FFF's use in critical and\nsmall-batch applications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 01:39:40 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Siegel", "Joshua E.", ""], ["Beemer", "Maria F.", ""], ["Shepard", "Steven M.", ""]]}, {"id": "1907.02642", "submitter": "Ankita Shukla", "authors": "Ankita Shukla, Gullal Singh Cheema, Saket Anand, Qamar Qureshi,\n  Yadvendradev Jhala", "title": "Primate Face Identification in the Wild", "comments": "arXiv admin note: text overlap with arXiv:1811.00743", "journal-ref": "PRICAI 2019, The 16th Pacific Rim International Conference on\n  Artificial Intelligence", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecological imbalance owing to rapid urbanization and deforestation has\nadversely affected the population of several wild animals. This loss of habitat\nhas skewed the population of several non-human primate species like chimpanzees\nand macaques and has constrained them to co-exist in close proximity of human\nsettlements, often leading to human-wildlife conflicts while competing for\nresources. For effective wildlife conservation and conflict management, regular\nmonitoring of population and of conflicted regions is necessary. However,\nexisting approaches like field visits for data collection and manual analysis\nby experts is resource intensive, tedious and time consuming, thus\nnecessitating an automated, non-invasive, more efficient alternative like image\nbased facial recognition. The challenge in individual identification arises due\nto unrelated factors like pose, lighting variations and occlusions due to the\nuncontrolled environments, that is further exacerbated by limited training\ndata. Inspired by human perception, we propose to learn representations that\nare robust to such nuisance factors and capture the notion of similarity over\nthe individual identity sub-manifolds. The proposed approach, Primate Face\nIdentification (PFID), achieves this by training the network to distinguish\nbetween positive and negative pairs of images. The PFID loss augments the\nstandard cross entropy loss with a pairwise loss to learn more discriminative\nand generalizable features, thus making it appropriate for other related\nidentification tasks like open-set, closed set and verification. We report\nstate-of-the-art accuracy on facial recognition of two primate species, rhesus\nmacaques and chimpanzees under the four protocols of classification,\nverification, closed-set identification and open-set recognition.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 19:26:09 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Shukla", "Ankita", ""], ["Cheema", "Gullal Singh", ""], ["Anand", "Saket", ""], ["Qureshi", "Qamar", ""], ["Jhala", "Yadvendradev", ""]]}, {"id": "1907.02662", "submitter": "Amit Rege", "authors": "Amit Rege, Claire Monteleoni", "title": "Evaluating the distribution learning capabilities of GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the distribution learning capabilities of generative adversarial\nnetworks by testing them on synthetic datasets. The datasets include common\ndistributions of points in $R^n$ space and images containing polygons of\nvarious shapes and sizes. We find that by and large GANs fail to faithfully\nrecreate point datasets which contain discontinous support or sharp bends with\nnoise. Additionally, on image datasets, we find that GANs do not seem to learn\nto count the number of objects of the same kind in an image. We also highlight\nthe apparent tension between generalization and learning in GANs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 02:59:40 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Rege", "Amit", ""], ["Monteleoni", "Claire", ""]]}, {"id": "1907.02665", "submitter": "Weixia Zhang", "authors": "Weixia Zhang and Kede Ma and Jia Yan and Dexiang Deng and Zhou Wang", "title": "Blind Image Quality Assessment Using A Deep Bilinear Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2018.2886771", "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep bilinear model for blind image quality assessment (BIQA)\nthat handles both synthetic and authentic distortions. Our model consists of\ntwo convolutional neural networks (CNN), each of which specializes in one\ndistortion scenario. For synthetic distortions, we pre-train a CNN to classify\nimage distortion type and level, where we enjoy large-scale training data. For\nauthentic distortions, we adopt a pre-trained CNN for image classification. The\nfeatures from the two CNNs are pooled bilinearly into a unified representation\nfor final quality prediction. We then fine-tune the entire model on target\nsubject-rated databases using a variant of stochastic gradient descent.\nExtensive experiments demonstrate that the proposed model achieves superior\nperformance on both synthetic and authentic databases. Furthermore, we verify\nthe generalizability of our method on the Waterloo Exploration Database using\nthe group maximum differentiation competition.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 03:35:35 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Zhang", "Weixia", ""], ["Ma", "Kede", ""], ["Yan", "Jia", ""], ["Deng", "Dexiang", ""], ["Wang", "Zhou", ""]]}, {"id": "1907.02704", "submitter": "Vincent Labatut", "authors": "Vincent Labatut (LIA), Xavier Bost (LIA)", "title": "Extraction and Analysis of Fictional Character Networks: A Survey", "comments": null, "journal-ref": "ACM Computing Surveys, Association for Computing Machinery, 2019,\n  52 (5), pp.89", "doi": "10.1145/3344548", "report-no": null, "categories": "cs.SI cs.CL cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A character network is a graph extracted from a narrative, in which vertices\nrepresent characters and edges correspond to interactions between them. A\nnumber of narrative-related problems can be addressed automatically through the\nanalysis of character networks, such as summarization, classification, or role\ndetection. Character networks are particularly relevant when considering works\nof fictions (e.g. novels, plays, movies, TV series), as their exploitation\nallows developing information retrieval and recommendation systems. However,\nworks of fiction possess specific properties making these tasks harder. This\nsurvey aims at presenting and organizing the scientific literature related to\nthe extraction of character networks from works of fiction, as well as their\nanalysis. We first describe the extraction process in a generic way, and\nexplain how its constituting steps are implemented in practice, depending on\nthe medium of the narrative, the goal of the network analysis, and other\nfactors. We then review the descriptive tools used to characterize character\nnetworks, with a focus on the way they are interpreted in this context. We\nillustrate the relevance of character networks by also providing a review of\napplications derived from their analysis. Finally, we identify the limitations\nof the existing approaches, and the most promising perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 07:27:31 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 09:00:13 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 12:51:45 GMT"}, {"version": "v4", "created": "Tue, 27 Jul 2021 13:01:30 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Labatut", "Vincent", "", "LIA"], ["Bost", "Xavier", "", "LIA"]]}, {"id": "1907.02711", "submitter": "Lakmal Meegahapola", "authors": "Lakmal Meegahapola, Vengateswaran Subramaniam, Lance Kaplan, Archan\n  Misra", "title": "Prior Activation Distribution (PAD): A Versatile Representation to\n  Utilize DNN Hidden Units", "comments": "Submitted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the concept of Prior Activation Distribution\n(PAD) as a versatile and general technique to capture the typical activation\npatterns of hidden layer units of a Deep Neural Network used for classification\ntasks. We show that the combined neural activations of such a hidden layer have\nclass-specific distributional properties, and then define multiple statistical\nmeasures to compute how far a test sample's activations deviate from such\ndistributions. Using a variety of benchmark datasets (including MNIST, CIFAR10,\nFashion-MNIST & notMNIST), we show how such PAD-based measures can be used,\nindependent of any training technique, to (a) derive fine-grained uncertainty\nestimates for inferences; (b) provide inferencing accuracy competitive with\nalternatives that require execution of the full pipeline, and (c) reliably\nisolate out-of-distribution test samples.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 07:55:09 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Meegahapola", "Lakmal", ""], ["Subramaniam", "Vengateswaran", ""], ["Kaplan", "Lance", ""], ["Misra", "Archan", ""]]}, {"id": "1907.02724", "submitter": "Junyu Gao", "authors": "Junyu Gao, Wei Lin, Bin Zhao, Dong Wang, Chenyu Gao, Jun Wen", "title": "C^3 Framework: An Open-source PyTorch Code for Crowd Counting", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report attempts to provide efficient and solid kits addressed\non the field of crowd counting, which is denoted as Crowd Counting Code\nFramework (C$^3$F). The contributions of C$^3$F are in three folds: 1) Some\nsolid baseline networks are presented, which have achieved the\nstate-of-the-arts. 2) Some flexible parameter setting strategies are provided\nto further promote the performance. 3) A powerful log system is developed to\nrecord the experiment process, which can enhance the reproducibility of each\nexperiment. Our code is made publicly available at\n\\url{https://github.com/gjy3035/C-3-Framework}. Furthermore, we also post a\nChinese blog\\footnote{\\url{https://zhuanlan.zhihu.com/p/65650998}} to describe\nthe details and insights of crowd counting.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 08:40:35 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Gao", "Junyu", ""], ["Lin", "Wei", ""], ["Zhao", "Bin", ""], ["Wang", "Dong", ""], ["Gao", "Chenyu", ""], ["Wen", "Jun", ""]]}, {"id": "1907.02731", "submitter": "Elena Burceanu", "authors": "Elena Burceanu, Marius Leordeanu", "title": "A 3D Convolutional Approach to Spectral Object Segmentation in Space and\n  Time", "comments": "accepted at International Joint Conference on Artificial Intelligence\n  2020 (IJCAI-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate object segmentation in video as a graph partitioning problem in\nspace and time, in which nodes are pixels and their relations form local\nneighborhoods. We claim that the strongest cluster in this pixel-level graph\nrepresents the salient object segmentation. We compute the main cluster using a\nnovel and fast 3D filtering technique that finds the spectral clustering\nsolution, namely the principal eigenvector of the graph's adjacency matrix,\nwithout building the matrix explicitly - which would be intractable. Our method\nis based on the power iteration for finding the principal eigenvector of a\nmatrix, which we prove is equivalent to performing a specific set of 3D\nconvolutions in the space-time feature volume. This allows us to avoid creating\nthe matrix and have a fast parallel implementation on GPU. We show that our\nmethod is much faster than classical power iteration applied directly on the\nadjacency matrix. Different from other works, ours is dedicated to preserving\nobject consistency in space and time at the level of pixels. For that, it\nrequires powerful pixel-wise features at the frame level. This makes it\nperfectly suitable for incorporating the output of a backbone network or other\nmethods and fast-improving over their solution without supervision. In\nexperiments, we obtain consistent improvement, with the same set of\nhyper-parameters, over the top state of the art methods on DAVIS-2016 dataset,\nboth in unsupervised and semi-supervised tasks. We also achieve top results on\nthe well-known SegTrackv2 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 09:07:19 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 11:12:55 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 15:33:39 GMT"}, {"version": "v4", "created": "Tue, 21 Apr 2020 06:17:45 GMT"}, {"version": "v5", "created": "Mon, 27 Apr 2020 19:35:48 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Burceanu", "Elena", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1907.02742", "submitter": "Farhan Akram", "authors": "Farhan Akram, Vivek Kumar Singh, Hatem A. Rashwan, Mohamed\n  Abdel-Nasser, Md. Mostafa Kamal Sarker, Nidhi Pandey, Domenec Puig", "title": "Adversarial Learning with Multiscale Features and Kernel Factorization\n  for Retinal Blood Vessel Segmentation", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose an efficient blood vessel segmentation method for\nthe eye fundus images using adversarial learning with multiscale features and\nkernel factorization. In the generator network of the adversarial framework,\nspatial pyramid pooling, kernel factorization and squeeze excitation block are\nemployed to enhance the feature representation in spatial domain on different\nscales with reduced computational complexity. In turn, the discriminator\nnetwork of the adversarial framework is formulated by combining convolutional\nlayers with an additional squeeze excitation block to differentiate the\ngenerated segmentation mask from its respective ground truth. Before feeding\nthe images to the network, we pre-processed them by using edge sharpening and\nGaussian regularization to reach an optimized solution for vessel segmentation.\nThe output of the trained model is post-processed using morphological\noperations to remove the small speckles of noise. The proposed method\nqualitatively and quantitatively outperforms state-of-the-art vessel\nsegmentation methods using DRIVE and STARE datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 09:35:38 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Akram", "Farhan", ""], ["Singh", "Vivek Kumar", ""], ["Rashwan", "Hatem A.", ""], ["Abdel-Nasser", "Mohamed", ""], ["Sarker", "Md. Mostafa Kamal", ""], ["Pandey", "Nidhi", ""], ["Puig", "Domenec", ""]]}, {"id": "1907.02757", "submitter": "Wenjia Bai", "authors": "Wenjia Bai, Chen Chen, Giacomo Tarroni, Jinming Duan, Florian Guitton,\n  Steffen E. Petersen, Yike Guo, Paul M. Matthews, Daniel Rueckert", "title": "Self-Supervised Learning for Cardiac MR Image Segmentation by Anatomical\n  Position Prediction", "comments": "Accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the recent years, convolutional neural networks have transformed the field\nof medical image analysis due to their capacity to learn discriminative image\nfeatures for a variety of classification and regression tasks. However,\nsuccessfully learning these features requires a large amount of manually\nannotated data, which is expensive to acquire and limited by the available\nresources of expert image analysts. Therefore, unsupervised, weakly-supervised\nand self-supervised feature learning techniques receive a lot of attention,\nwhich aim to utilise the vast amount of available data, while at the same time\navoid or substantially reduce the effort of manual annotation. In this paper,\nwe propose a novel way for training a cardiac MR image segmentation network, in\nwhich features are learnt in a self-supervised manner by predicting anatomical\npositions. The anatomical positions serve as a supervisory signal and do not\nrequire extra manual annotation. We demonstrate that this seemingly simple task\nprovides a strong signal for feature learning and with self-supervised\nlearning, we achieve a high segmentation accuracy that is better than or\ncomparable to a U-net trained from scratch, especially at a small data setting.\nWhen only five annotated subjects are available, the proposed method improves\nthe mean Dice metric from 0.811 to 0.852 for short-axis image segmentation,\ncompared to the baseline U-net.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 10:27:38 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Bai", "Wenjia", ""], ["Chen", "Chen", ""], ["Tarroni", "Giacomo", ""], ["Duan", "Jinming", ""], ["Guitton", "Florian", ""], ["Petersen", "Steffen E.", ""], ["Guo", "Yike", ""], ["Matthews", "Paul M.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1907.02766", "submitter": "Cheng Ouyang", "authors": "Cheng Ouyang, Konstantinos Kamnitsas, Carlo Biffi, Jinming Duan,\n  Daniel Rueckert", "title": "Data Efficient Unsupervised Domain Adaptation for Cross-Modality Image\n  Segmentation", "comments": "Accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models trained on medical images from a source domain (e.g.\nimaging modality) often fail when deployed on images from a different target\ndomain, despite imaging common anatomical structures. Deep unsupervised domain\nadaptation (UDA) aims to improve the performance of a deep neural network model\non a target domain, using solely unlabelled target domain data and labelled\nsource domain data. However, current state-of-the-art methods exhibit reduced\nperformance when target data is scarce. In this work, we introduce a new data\nefficient UDA method for multi-domain medical image segmentation. The proposed\nmethod combines a novel VAE-based feature prior matching, which is\ndata-efficient, and domain adversarial training to learn a shared\ndomain-invariant latent space which is exploited during segmentation. Our\nmethod is evaluated on a public multi-modality cardiac image segmentation\ndataset by adapting from the labelled source domain (3D MRI) to the unlabelled\ntarget domain (3D CT). We show that by using only one single unlabelled 3D CT\nscan, the proposed architecture outperforms the state-of-the-art in the same\nsetting. Finally, we perform ablation studies on prior matching and domain\nadversarial training to shed light on the theoretical grounding of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 10:47:39 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 19:58:10 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Ouyang", "Cheng", ""], ["Kamnitsas", "Konstantinos", ""], ["Biffi", "Carlo", ""], ["Duan", "Jinming", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1907.02787", "submitter": "Daniele Ravi", "authors": "Daniele Ravi, Daniel C. Alexander, Neil P. Oxtoby", "title": "Degenerative Adversarial NeuroImage Nets: Generating Images that Mimic\n  Disease Progression", "comments": "Paper accepted for MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulating images representative of neurodegenerative diseases is important\nfor predicting patient outcomes and for validation of computational models of\ndisease progression. This capability is valuable for secondary prevention\nclinical trials where outcomes and screening criteria involve neuroimaging.\nTraditional computational methods are limited by imposing a parametric model\nfor atrophy and are extremely resource-demanding. Recent advances in deep\nlearning have yielded data-driven models for longitudinal studies (e.g., face\nageing) that are capable of generating synthetic images in real-time. Similar\nsolutions can be used to model trajectories of atrophy in the brain, although\nnew challenges need to be addressed to ensure accurate disease progression\nmodelling. Here we propose Degenerative Adversarial NeuroImage Net (DaniNet)\n--- a new deep learning approach that learns to emulate the effect of\nneurodegeneration on MRI by simulating atrophy as a function of ages, and\ndisease progression. DaniNet uses an underlying set of Support Vector\nRegressors (SVRs) trained to capture the patterns of regional intensity changes\nthat accompany disease progression. DaniNet produces whole output images,\nconsisting of 2D-MRI slices that are constrained to match regional predictions\nfrom the SVRs. DaniNet is also able to maintain the unique brain morphology of\nindividuals. Adversarial training ensures realistic brain images and smooth\ntemporal progression. We train our model using 9652 T1-weighted (longitudinal)\nMRI extracted from the Alzheimer's Disease Neuroimaging Initiative (ADNI)\ndataset. We perform quantitative and qualitative evaluations on a separate test\nset of 1283 images (also from ADNI) demonstrating the ability of DaniNet to\nproduce accurate and convincing synthetic images that emulate disease\nprogression.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 12:11:01 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 18:56:35 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ravi", "Daniele", ""], ["Alexander", "Daniel C.", ""], ["Oxtoby", "Neil P.", ""]]}, {"id": "1907.02788", "submitter": "Huaiyu Li", "authors": "Huaiyu Li, Weiming Dong, Bao-Gang Hu", "title": "Incremental Concept Learning via Online Generative Memory Recall", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn more and more concepts over time from incrementally\narriving data is essential for the development of a life-long learning system.\nHowever, deep neural networks often suffer from forgetting previously learned\nconcepts when continually learning new concepts, which is known as catastrophic\nforgetting problem. The main reason for catastrophic forgetting is that the\npast concept data is not available and neural weights are changed during\nincrementally learning new concepts. In this paper, we propose a\npseudo-rehearsal based class incremental learning approach to make neural\nnetworks capable of continually learning new concepts. We use a conditional\ngenerative adversarial network to consolidate old concepts memory and recall\npseudo samples during learning new concepts and a balanced online memory recall\nstrategy is to maximally maintain old memories. And we design a comprehensible\nincremental concept learning network as well as a concept contrastive loss to\nalleviate the magnitude of neural weights change. We evaluate the proposed\napproach on MNIST, Fashion-MNIST and SVHN datasets and compare with other\nrehearsal based approaches. The extensive experiments demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 12:13:46 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Li", "Huaiyu", ""], ["Dong", "Weiming", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1907.02813", "submitter": "Natalia Efremova", "authors": "Natalia Efremova, Dennis West, Dmitry Zausaev", "title": "AI-based evaluation of the SDGs: The case of crop detection with earth\n  observation data", "comments": "ICLR workshop \"AI for Social Good\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of the seventeen sustainable development goals is a challenge\nfor developers and researchers applying artificial intelligence (AI). AI and\nearth observations (EO) can provide reliable and disaggregated data for better\nmonitoring of the sustainable development goals (SDGs). In this paper, we\npresent an overview of SDG targets, which can be effectively measured with AI\ntools. We identify indicators with the most significant contribution from the\nAI and EO and describe an application of state-of-the-art machine learning\nmodels to one of the indicators. We describe an application of U-net with SE\nblocks for efficient segmentation of satellite imagery for crop detection.\nFinally, we demonstrate how AI can be more effectively applied in solutions\ndirectly contributing towards specific SDGs and propose further research on an\nAI-based evaluative infrastructure for SDGs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 13:26:33 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Efremova", "Natalia", ""], ["West", "Dennis", ""], ["Zausaev", "Dmitry", ""]]}, {"id": "1907.02821", "submitter": "Lia Morra", "authors": "Lia Morra and Fabrizio Lamberti", "title": "Benchmarking unsupervised near-duplicate image detection", "comments": "Accepted for publication in Expert Systems with Applications", "journal-ref": "Expert Systems with Applications, online first, 2019", "doi": "10.1016/j.eswa.2019.05.002", "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised near-duplicate detection has many practical applications ranging\nfrom social media analysis and web-scale retrieval, to digital image forensics.\nIt entails running a threshold-limited query on a set of descriptors extracted\nfrom the images, with the goal of identifying all possible near-duplicates,\nwhile limiting the false positives due to visually similar images. Since the\nrate of false alarms grows with the dataset size, a very high specificity is\nthus required, up to $1 - 10^{-9}$ for realistic use cases; this important\nrequirement, however, is often overlooked in literature. In recent years,\ndescriptors based on deep convolutional neural networks have matched or\nsurpassed traditional feature extraction methods in content-based image\nretrieval tasks. To the best of our knowledge, ours is the first attempt to\nestablish the performance range of deep learning-based descriptors for\nunsupervised near-duplicate detection on a range of datasets, encompassing a\nbroad spectrum of near-duplicate definitions. We leverage both established and\nnew benchmarks, such as the Mir-Flick Near-Duplicate (MFND) dataset, in which a\nknown ground truth is provided for all possible pairs over a general, large\nscale image collection. To compare the specificity of different descriptors, we\nreduce the problem of unsupervised detection to that of binary classification\nof near-duplicate vs. not-near-duplicate images. The latter can be conveniently\ncharacterized using Receiver Operating Curve (ROC). Our findings in general\nfavor the choice of fine-tuning deep convolutional networks, as opposed to\nusing off-the-shelf features, but differences at high specificity settings\ndepend on the dataset and are often small. The best performance was observed on\nthe MFND benchmark, achieving 96\\% sensitivity at a false positive rate of\n$1.43 \\times 10^{-6}$.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 09:08:47 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Morra", "Lia", ""], ["Lamberti", "Fabrizio", ""]]}, {"id": "1907.02824", "submitter": "James Garforth", "authors": "James Garforth and Barbara Webb", "title": "Visual Appearance Analysis of Forest Scenes for Monocular SLAM", "comments": "Accepted to ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular simultaneous localisation and mapping (SLAM) is a cheap and energy\nefficient way to enable Unmanned Aerial Vehicles (UAVs) to safely navigate\nmanaged forests and gather data crucial for monitoring tree health. SLAM\nresearch, however, has mostly been conducted in structured human environments,\nand as such is poorly adapted to unstructured forests. In this paper, we\ncompare the performance of state of the art monocular SLAM systems on forest\ndata and use visual appearance statistics to characterise the differences\nbetween forests and other environments, including a photorealistic simulated\nforest. We find that SLAM systems struggle with all but the most\nstraightforward forest terrain and identify key attributes (lighting changes\nand in-scene motion) which distinguish forest scenes from \"classic\" urban\ndatasets. These differences offer an insight into what makes forests harder to\nmap and open the way for targeted improvements. We also demonstrate that even\nsimulations that look impressive to the human eye can fail to properly reflect\nthe difficult attributes of the environment they simulate, and provide\nsuggestions for more closely mimicking natural scenes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 13:38:08 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Garforth", "James", ""], ["Webb", "Barbara", ""]]}, {"id": "1907.02841", "submitter": "Li Qiang", "authors": "Wenxiang Zuo, Qiang Li, Xianming Liu", "title": "Depth Restoration: A fast low-rank matrix completion via dual-graph\n  regularization", "comments": "The paper will be added more experiments. The main idea of the paper\n  needs to be revamped. Please withdraw the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a real scenes sensing approach, depth information obtains the widespread\napplications. However, resulting from the restriction of depth sensing\ntechnology, the depth map captured in practice usually suffers terrible noise\nand missing values at plenty of pixels. In this paper, we propose a fast\nlow-rank matrix completion via dual-graph regularization for depth restoration.\nSpecifically, the depth restoration can be transformed into a low-rank matrix\ncompletion problem. In order to complete the low-rank matrix and restore it to\nthe depth map, the proposed dual-graph method containing the local and\nnon-local graph regularizations exploits the local similarity of depth maps and\nthe gradient consistency of depth-color counterparts respectively. In addition,\nthe proposed approach achieves the high speed depth restoration due to\nclosed-form solution. Experimental results demonstrate that the proposed method\noutperforms the state-of-the-art methods with respect to both objective and\nsubjective quality evaluations, especially for serious depth degeneration.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:09:31 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 11:06:38 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 13:14:36 GMT"}, {"version": "v4", "created": "Wed, 8 Jan 2020 09:29:44 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zuo", "Wenxiang", ""], ["Li", "Qiang", ""], ["Liu", "Xianming", ""]]}, {"id": "1907.02843", "submitter": "Xiaopeng Sun", "authors": "Xiaopeng Sun, Wen Lu, Rui Wang, Furui Bai", "title": "Distilling with Residual Network for Single Image Super Resolution", "comments": "6 pages; Accepted to ICME2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the deep convolutional neural network (CNN) has made remarkable\nprogress in single image super resolution(SISR). However, blindly using the\nresidual structure and dense structure to extract features from LR images, can\ncause the network to be bloated and difficult to train. To address these\nproblems, we propose a simple and efficient distilling with residual\nnetwork(DRN) for SISR. In detail, we propose residual distilling block(RDB)\ncontaining two branches, while one branch performs a residual operation and the\nother branch distills effective information. To further improve efficiency, we\ndesign residual distilling group(RDG) by stacking some RDBs and one long skip\nconnection, which can effectively extract local features and fuse them with\nglobal features. These efficient features beneficially contribute to image\nreconstruction. Experiments on benchmark datasets demonstrate that our DRN is\nsuperior to the state-of-the-art methods, specifically has a better trade-off\nbetween performance and model size.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:14:04 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Sun", "Xiaopeng", ""], ["Lu", "Wen", ""], ["Wang", "Rui", ""], ["Bai", "Furui", ""]]}, {"id": "1907.02865", "submitter": "Nathan Painchaud", "authors": "Nathan Painchaud and Youssef Skandarani and Thierry Judge and Olivier\n  Bernard and Alain Lalande and Pierre-Marc Jodoin", "title": "Cardiac MRI Segmentation with Strong Anatomical Guarantees", "comments": "9 pages, accepted for MICCAI 2019; camera ready corrections,\n  acknowledgments", "journal-ref": "in Medical Image Computing and Computer Assisted Intervention -\n  MICCAI 2019, 2019, pp. 632-640", "doi": "10.1007/978-3-030-32245-8_70", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent publications have shown that the segmentation accuracy of modern-day\nconvolutional neural networks (CNN) applied on cardiac MRI can reach the\ninter-expert variability, a great achievement in this area of research.\nHowever, despite these successes, CNNs still produce anatomically inaccurate\nsegmentations as they provide no guarantee on the anatomical plausibility of\ntheir outcome, even when using a shape prior. In this paper, we propose a\ncardiac MRI segmentation method which always produces anatomically plausible\nresults. At the core of the method is an adversarial variational autoencoder\n(aVAE) whose latent space encodes a smooth manifold on which lies a large\nspectrum of valid cardiac shapes. This aVAE is used to automatically warp\nanatomically inaccurate cardiac shapes towards a close but correct shape. Our\nmethod can accommodate any cardiac segmentation method and convert its\nanatomically implausible results to plausible ones without affecting its\noverall geometric and clinical metrics. With our method, CNNs can now produce\nresults that are both within the inter-expert variability and always\nanatomically plausible.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:45:21 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 16:19:41 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Painchaud", "Nathan", ""], ["Skandarani", "Youssef", ""], ["Judge", "Thierry", ""], ["Bernard", "Olivier", ""], ["Lalande", "Alain", ""], ["Jodoin", "Pierre-Marc", ""]]}, {"id": "1907.02873", "submitter": "Reyhane Alidousti", "authors": "Zahra Alidousti, Maryam Taghizadeh Dehkordi", "title": "A new method for determining the filled point of the tooth by Bit-Plane\n  Algorithm", "comments": "2019 IEEE 4th Conference on Technology In Electrical and Computer\n  Engineering (ETECH 2019) Information and Communication Technology (ICT)\n  Tehran, Iran", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Up to now, researchers have applied segmentation techniques in their studies\non teeth images, with construction on tooth root length and depth. In this\npaper, a new approach to the exact identification of the filled points of the\ntooth is proposed. In this method, the filled teeth are detection by applying\nthe Bit-Plane algorithm on the OPG images. The novelty of the proposed method\nis that we can use it in medicine for the detection of dental filling and we\ncalculate and present the area of the filled points which may help dentists to\nassess the filled point of the tooth. The experimental results, confirmed by\nthe dentists, clearly indicate that this method is able to separate the filled\npoints from the rest of healthy teeth completely.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:54:41 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Alidousti", "Zahra", ""], ["Dehkordi", "Maryam Taghizadeh", ""]]}, {"id": "1907.02882", "submitter": "Micha Pfeiffer", "authors": "Micha Pfeiffer, Isabel Funke, Maria R. Robu, Sebastian Bodenstedt,\n  Leon Strenger, Sandy Engelhardt, Tobias Ro{\\ss}, Matthew J. Clarkson,\n  Kurinchi Gurusamy, Brian R. Davidson, Lena Maier-Hein, Carina Riediger, Thilo\n  Welsch, J\\\"urgen Weitz and Stefanie Speidel", "title": "Generating large labeled data sets for laparoscopic image processing\n  tasks using unpaired image-to-image translation", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the medical domain, the lack of large training data sets and benchmarks is\noften a limiting factor for training deep neural networks. In contrast to\nexpensive manual labeling, computer simulations can generate large and fully\nlabeled data sets with a minimum of manual effort. However, models that are\ntrained on simulated data usually do not translate well to real scenarios. To\nbridge the domain gap between simulated and real laparoscopic images, we\nexploit recent advances in unpaired image-to-image translation. We extent an\nimage-to-image translation method to generate a diverse multitude of\nrealistically looking synthetic images based on images from a simple\nlaparoscopy simulation. By incorporating means to ensure that the image content\nis preserved during the translation process, we ensure that the labels given\nfor the simulated images remain valid for their realistically looking\ntranslations. This way, we are able to generate a large, fully labeled\nsynthetic data set of laparoscopic images with realistic appearance. We show\nthat this data set can be used to train models for the task of liver\nsegmentation of laparoscopic images. We achieve average dice scores of up to\n0.89 in some patients without manually labeling a single laparoscopic image and\nshow that using our synthetic data to pre-train models can greatly improve\ntheir performance. The synthetic data set will be made publicly available,\nfully labeled with segmentation maps, depth maps, normal maps, and positions of\ntools and camera (http://opencas.dkfz.de/image2image).\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:10:20 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Pfeiffer", "Micha", ""], ["Funke", "Isabel", ""], ["Robu", "Maria R.", ""], ["Bodenstedt", "Sebastian", ""], ["Strenger", "Leon", ""], ["Engelhardt", "Sandy", ""], ["Ro\u00df", "Tobias", ""], ["Clarkson", "Matthew J.", ""], ["Gurusamy", "Kurinchi", ""], ["Davidson", "Brian R.", ""], ["Maier-Hein", "Lena", ""], ["Riediger", "Carina", ""], ["Welsch", "Thilo", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""]]}, {"id": "1907.02890", "submitter": "Jiaqi Yang", "authors": "Jiaqi Yang and Ke Xian and Peng Wang and Yanning Zhang", "title": "A Performance Evaluation of Correspondence Grouping Methods for 3D Rigid\n  Data Matching", "comments": "Extension of 3DV 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seeking consistent point-to-point correspondences between 3D rigid data\n(point clouds, meshes, or depth maps) is a fundamental problem in 3D computer\nvision. While a number of correspondence selection methods have been proposed\nin recent years, their advantages and shortcomings remain unclear regarding\ndifferent applications and perturbations. To fill this gap, this paper gives a\ncomprehensive evaluation of nine state-of-the-art 3D correspondence grouping\nmethods. A good correspondence grouping algorithm is expected to retrieve as\nmany as inliers from initial feature matches, giving a rise in both precision\nand recall as well as facilitating accurate transformation estimation. Toward\nthis rule, we deploy experiments on three benchmarks with different application\ncontexts including shape retrieval, 3D object recognition, and point cloud\nregistration together with various perturbations such as noise, point density\nvariation, clutter, occlusion, partial overlap, different scales of initial\ncorrespondences, and different combinations of keypoint detectors and\ndescriptors. The rich variety of application scenarios and nuisances result in\ndifferent spatial distributions and inlier ratios of initial feature\ncorrespondences, thus enabling a thorough evaluation. Based on the outcomes, we\ngive a summary of the traits, merits, and demerits of evaluated approaches and\nindicate some potential future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:22:24 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Yang", "Jiaqi", ""], ["Xian", "Ke", ""], ["Wang", "Peng", ""], ["Zhang", "Yanning", ""]]}, {"id": "1907.02929", "submitter": "S\\'ebastien Bougleux", "authors": "Nicolas Boria, David B. Blumenthal, S\\'ebastien Bougleux and Luc Brun", "title": "Improved local search for graph edit distance", "comments": null, "journal-ref": "Pattern Recognition Letters 129, pages 19-25, 2020", "doi": "10.1016/j.patrec.2019.10.028", "report-no": null, "categories": "cs.DS cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The graph edit distance (GED) measures the dissimilarity between two graphs\nas the minimal cost of a sequence of elementary operations transforming one\ngraph into another. This measure is fundamental in many areas such as\nstructural pattern recognition or classification. However, exactly computing\nGED is NP-hard. Among different classes of heuristic algorithms that were\nproposed to compute approximate solutions, local search based algorithms\nprovide the tightest upper bounds for GED. In this paper, we present K-REFINE\nand RANDPOST. K-REFINE generalizes and improves an existing local search\nalgorithm and performs particularly well on small graphs. RANDPOST is a general\nwarm start framework that stochastically generates promising initial solutions\nto be used by any local search based GED algorithm. It is particularly\nefficient on large graphs. An extensive empirical evaluation demonstrates that\nboth K-REFINE and RANDPOST perform excellently in practice.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 16:52:40 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 11:55:25 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Boria", "Nicolas", ""], ["Blumenthal", "David B.", ""], ["Bougleux", "S\u00e9bastien", ""], ["Brun", "Luc", ""]]}, {"id": "1907.02940", "submitter": "Jae Seo", "authors": "Jae Duk Seo", "title": "Visualizing Uncertainty and Saliency Maps of Deep Convolutional Neural\n  Networks for Medical Imaging Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are now used in many different industries, while in\ncertain domains safety is not a critical issue in the medical field it is a\nhuge concern. Not only, we want the models to generalize well but we also want\nto know the models confidence respect to its decision and which features matter\nthe most. Our team aims to develop a full pipeline in which not only displays\nthe uncertainty of the models decision but also, the saliency map to show which\nsets of pixels of the input image contribute most to the predictions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 17:23:04 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Seo", "Jae Duk", ""]]}, {"id": "1907.02946", "submitter": "Li Ding", "authors": "Li Ding and Mohammad H. Bawany and Ajay E. Kuriyan and Rajeev S.\n  Ramchandran and Charles C. Wykoff and Gaurav Sharma", "title": "A Novel Deep Learning Pipeline for Retinal Vessel Detection in\n  Fluorescein Angiography", "comments": "A paper based on this pre-print has been published (after revisions)\n  in IEEE Trans. Image Processing. See the first footnote on front page of the\n  article for details", "journal-ref": "IEEE Trans. Image Proc., 29(1), 2020", "doi": "10.1109/TIP.2020.2991530", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent advances in deep learning have significantly advanced the state\nof the art for vessel detection in color fundus (CF) images, the success for\ndetecting vessels in fluorescein angiography (FA) has been stymied due to the\nlack of labeled ground truth datasets. We propose a novel pipeline to detect\nretinal vessels in FA images using deep neural networks that reduces the effort\nrequired for generating labeled ground truth data by combining two key\ncomponents: cross-modality transfer and human-in-the-loop learning. The\ncross-modality transfer exploits concurrently captured CF and fundus FA images.\nBinary vessels maps are first detected from CF images with a pre-trained neural\nnetwork and then are geometrically registered with and transferred to FA images\nvia robust parametric chamfer alignment to a preliminary FA vessel detection\nobtained with an unsupervised technique. Using the transferred vessels as\ninitial ground truth labels for deep learning, the human-in-the-loop approach\nprogressively improves the quality of the ground truth labeling by iterating\nbetween deep-learning and labeling. The approach significantly reduces manual\nlabeling effort while increasing engagement. We highlight several important\nconsiderations for the proposed methodology and validate the performance on\nthree datasets. Experimental results demonstrate that the proposed pipeline\nsignificantly reduces the annotation effort and the resulting deep learning\nmethods outperform prior existing FA vessel detection methods by a significant\nmargin. A new public dataset, RECOVERY-FA19, is introduced that includes\nhigh-resolution ultra-widefield images and accurately labeled ground truth\nbinary vessel maps.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 17:27:44 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 18:35:54 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ding", "Li", ""], ["Bawany", "Mohammad H.", ""], ["Kuriyan", "Ajay E.", ""], ["Ramchandran", "Rajeev S.", ""], ["Wykoff", "Charles C.", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1907.02957", "submitter": "Yao Qin", "authors": "Yao Qin, Nicholas Frosst, Sara Sabour, Colin Raffel, Garrison Cottrell\n  and Geoffrey Hinton", "title": "Detecting and Diagnosing Adversarial Images with Class-Conditional\n  Capsule Reconstructions", "comments": null, "journal-ref": "ICLR 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples raise questions about whether neural network models are\nsensitive to the same visual features as humans. In this paper, we first detect\nadversarial examples or otherwise corrupted images based on a class-conditional\nreconstruction of the input. To specifically attack our detection mechanism, we\npropose the Reconstructive Attack which seeks both to cause a misclassification\nand a low reconstruction error. This reconstructive attack produces undetected\nadversarial examples but with much smaller success rate. Among all these\nattacks, we find that CapsNets always perform better than convolutional\nnetworks. Then, we diagnose the adversarial examples for CapsNets and find that\nthe success of the reconstructive attack is highly related to the visual\nsimilarity between the source and target class. Additionally, the resulting\nperturbations can cause the input image to appear visually more like the target\nclass and hence become non-adversarial. This suggests that CapsNets use\nfeatures that are more aligned with human perception and have the potential to\naddress the central issue raised by adversarial examples.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 17:57:57 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 05:05:45 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Qin", "Yao", ""], ["Frosst", "Nicholas", ""], ["Sabour", "Sara", ""], ["Raffel", "Colin", ""], ["Cottrell", "Garrison", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "1907.02959", "submitter": "Diego Valsesia", "authors": "Diego Valsesia, Enrico Magli", "title": "High-throughput Onboard Hyperspectral Image Compression with\n  Ground-based CNN Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2927434", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression of hyperspectral images onboard of spacecrafts is a tradeoff\nbetween the limited computational resources and the ever-growing spatial and\nspectral resolution of the optical instruments. As such, it requires\nlow-complexity algorithms with good rate-distortion performance and high\nthroughput. In recent years, the Consultative Committee for Space Data Systems\n(CCSDS) has focused on lossless and near-lossless compression approaches based\non predictive coding, resulting in the recently published CCSDS 123.0-B-2\nrecommended standard. While the in-loop reconstruction of quantized prediction\nresiduals provides excellent rate-distortion performance for the near-lossless\noperating mode, it significantly constrains the achievable throughput due to\ndata dependencies. In this paper, we study the performance of a faster method\nbased on prequantization of the image followed by a lossless predictive\ncompressor. While this is well known to be suboptimal, one can exploit powerful\nsignal models to reconstruct the image at the ground segment, recovering part\nof the suboptimality. In particular, we show that convolutional neural networks\ncan be used for this task and that they can recover the whole SNR drop incurred\nat a bitrate of 2 bits per pixel.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 17:59:25 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Valsesia", "Diego", ""], ["Magli", "Enrico", ""]]}, {"id": "1907.02985", "submitter": "Federico Landi", "authors": "Federico Landi, Lorenzo Baraldi, Massimiliano Corsini, Rita Cucchiara", "title": "Embodied Vision-and-Language Navigation with Dynamic Convolutional\n  Filters", "comments": "BMVC 2019 (Oral). Code is available at\n  https://github.com/aimagelab/DynamicConv-agent", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Vision-and-Language Navigation (VLN), an embodied agent needs to reach a\ntarget destination with the only guidance of a natural language instruction. To\nexplore the environment and progress towards the target location, the agent\nmust perform a series of low-level actions, such as rotate, before stepping\nahead. In this paper, we propose to exploit dynamic convolutional filters to\nencode the visual information and the lingual description in an efficient way.\nDifferently from some previous works that abstract from the agent perspective\nand use high-level navigation spaces, we design a policy which decodes the\ninformation provided by dynamic convolution into a series of low-level, agent\nfriendly actions. Results show that our model exploiting dynamic filters\nperforms better than other architectures with traditional convolution, being\nthe new state of the art for embodied VLN in the low-level action space.\nAdditionally, we attempt to categorize recent work on VLN depending on their\narchitectural choices and distinguish two main groups: we call them low-level\nactions and high-level actions models. To the best of our knowledge, we are the\nfirst to propose this analysis and categorization for VLN.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 18:02:30 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 17:12:21 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Landi", "Federico", ""], ["Baraldi", "Lorenzo", ""], ["Corsini", "Massimiliano", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1907.03029", "submitter": "Majed El Helou", "authors": "Majed El Helou, Sabine S\\\"usstrunk", "title": "Blind Universal Bayesian Image Denoising with Gaussian Noise Level\n  Learning", "comments": "Final uncompressed TIP version available online in open access (DOI\n  attached)", "journal-ref": "IEEE Transactions on Image Processing, vol. 29, pp. 4885-4897,\n  2020", "doi": "10.1109/TIP.2020.2976814", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind and universal image denoising consists of using a unique model that\ndenoises images with any level of noise. It is especially practical as noise\nlevels do not need to be known when the model is developed or at test time. We\npropose a theoretically-grounded blind and universal deep learning image\ndenoiser for additive Gaussian noise removal. Our network is based on an\noptimal denoising solution, which we call fusion denoising. It is derived\ntheoretically with a Gaussian image prior assumption. Synthetic experiments\nshow our network's generalization strength to unseen additive noise levels. We\nalso adapt the fusion denoising network architecture for image denoising on\nreal images. Our approach improves real-world grayscale additive image\ndenoising PSNR results for training noise levels and further on noise levels\nnot seen during training. It also improves state-of-the-art color image\ndenoising performance on every single noise level, by an average of 0.1dB,\nwhether trained on or not.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 21:39:50 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 18:32:11 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Helou", "Majed El", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1907.03030", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, B.V.K Vijaya Kumar, Chao Yang, Qingming Tang, Jane You", "title": "Dependency-aware Attention Control for Unconstrained Face Recognition\n  with Image Sets", "comments": "Fixed the unreadable code in CVF version. arXiv admin note: text\n  overlap with arXiv:1707.00130 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper targets the problem of image set-based face verification and\nidentification. Unlike traditional single media (an image or video) setting, we\nencounter a set of heterogeneous contents containing orderless images and\nvideos. The importance of each image is usually considered either equal or\nbased on their independent quality assessment. How to model the relationship of\norderless images within a set remains a challenge. We address this problem by\nformulating it as a Markov Decision Process (MDP) in the latent space.\nSpecifically, we first present a dependency-aware attention control (DAC)\nnetwork, which resorts to actor-critic reinforcement learning for sequential\nattention decision of each image embedding to fully exploit the rich\ncorrelation cues among the unordered images. Moreover, we introduce its\nsample-efficient variant with off-policy experience replay to speed up the\nlearning process. The pose-guided representation scheme can further boost the\nperformance at the extremes of the pose variation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 21:40:56 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Kumar", "B. V. K Vijaya", ""], ["Yang", "Chao", ""], ["Tang", "Qingming", ""], ["You", "Jane", ""]]}, {"id": "1907.03049", "submitter": "Yu-Siang Wang", "authors": "Yu-Siang Wang, Hung-Ting Su, Chen-Hsi Chang, Zhe-Yu Liu, Winston H.\n  Hsu", "title": "Video Question Generation via Cross-Modal Self-Attention Networks\n  Learning", "comments": "Accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel task, Video Question Generation (Video QG). A Video QG\nmodel automatically generates questions given a video clip and its\ncorresponding dialogues. Video QG requires a range of skills -- sentence\ncomprehension, temporal relation, the interplay between vision and language,\nand the ability to ask meaningful questions. To address this, we propose a\nnovel semantic rich cross-modal self-attention (SRCMSA) network to aggregate\nthe multi-modal and diverse features. To be more precise, we enhance the video\nframes semantic by integrating the object-level information, and we jointly\nconsider the cross-modal attention for the video question generation task.\nExcitingly, our proposed model remarkably improves the baseline from 7.58 to\n14.48 in the BLEU-4 score on the TVQA dataset. Most of all, we arguably pave a\nnovel path toward understanding the challenging video input and we provide\ndetailed analysis in terms of diversity, which ushers the avenues for future\ninvestigations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 23:47:04 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 19:45:54 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 21:11:03 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Wang", "Yu-Siang", ""], ["Su", "Hung-Ting", ""], ["Chang", "Chen-Hsi", ""], ["Liu", "Zhe-Yu", ""], ["Hsu", "Winston H.", ""]]}, {"id": "1907.03069", "submitter": "Xiu-Shen Wei", "authors": "Xiu-Shen Wei, Jianxin Wu, Quan Cui", "title": "Deep Learning for Fine-Grained Image Analysis: A Survey", "comments": "Project page:\n  http://www.weixiushen.com/project/Awesome_FGIA/Awesome_FGIA.html. arXiv admin\n  note: text overlap with arXiv:1902.06068 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision (CV) is the process of using machines to understand and\nanalyze imagery, which is an integral branch of artificial intelligence. Among\nvarious research areas of CV, fine-grained image analysis (FGIA) is a\nlongstanding and fundamental problem, and has become ubiquitous in diverse\nreal-world applications. The task of FGIA targets analyzing visual objects from\nsubordinate categories, \\eg, species of birds or models of cars. The small\ninter-class variations and the large intra-class variations caused by the\nfine-grained nature makes it a challenging problem. During the booming of deep\nlearning, recent years have witnessed remarkable progress of FGIA using deep\nlearning techniques. In this paper, we aim to give a survey on recent advances\nof deep learning based FGIA techniques in a systematic way. Specifically, we\norganize the existing studies of FGIA techniques into three major categories:\nfine-grained image recognition, fine-grained image retrieval and fine-grained\nimage generation. In addition, we also cover some other important issues of\nFGIA, such as publicly available benchmark datasets and its related domain\nspecific applications. Finally, we conclude this survey by highlighting several\ndirections and open problems which need be further explored by the community in\nthe future.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 03:55:02 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Wei", "Xiu-Shen", ""], ["Wu", "Jianxin", ""], ["Cui", "Quan", ""]]}, {"id": "1907.03075", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra", "title": "AMD Severity Prediction And Explainability Using Image Registration And\n  Deep Embedded Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to predict severity of age related macular degeneration\n(AMD) from input optical coherence tomography (OCT) images. Although there is\nno standard clinical severity scale for AMD, we leverage deep learning (DL)\nbased image registration and clustering methods to identify diseased cases and\npredict their severity. Experiments demonstrate our approach's disease\nclassification performance matches state of the art methods. The predicted\ndisease severity performs well on previously unseen data. Registration output\nprovides better explainability than class activation maps regarding label and\nseverity decisions\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 04:23:50 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Mahapatra", "Dwarikanath", ""]]}, {"id": "1907.03077", "submitter": "Shusen Liu", "authors": "Shusen Liu, Bhavya Kailkhura, Donald Loveland, Yong Han", "title": "Generative Counterfactual Introspection for Explainable Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an introspection technique for deep neural networks\nthat relies on a generative model to instigate salient editing of the input\nimage for model interpretation. Such modification provides the fundamental\ninterventional operation that allows us to obtain answers to counterfactual\ninquiries, i.e., what meaningful change can be made to the input image in order\nto alter the prediction. We demonstrate how to reveal interesting properties of\nthe given classifiers by utilizing the proposed introspection approach on both\nthe MNIST and the CelebA dataset.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 04:30:13 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Liu", "Shusen", ""], ["Kailkhura", "Bhavya", ""], ["Loveland", "Donald", ""], ["Han", "Yong", ""]]}, {"id": "1907.03083", "submitter": "Zeng Shangzhi", "authors": "Risheng Liu, Long Ma, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang", "title": "Bilevel Integrative Optimization for Ill-posed Inverse Problems", "comments": "arXiv admin note: text overlap with arXiv:1706.04008 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical optimization techniques often formulate the feasibility of the\nproblems as set, equality or inequality constraints. However, explicitly\ndesigning these constraints is indeed challenging for complex real-world\napplications and too strict constraints may even lead to intractable\noptimization problems. On the other hand, it is still hard to incorporate\ndata-dependent information into conventional numerical iterations. To partially\naddress the above limits and inspired by the leader-follower gaming\nperspective, this work first introduces a bilevel-type formulation to jointly\ninvestigate the feasibility and optimality of nonconvex and nonsmooth\noptimization problems. Then we develop an algorithmic framework to couple\nforward-backward proximal computations to optimize our established bilevel\nleader-follower model. We prove its convergence and estimate the convergence\nrate. Furthermore, a learning-based extension is developed, in which we\nestablish an unrolling strategy to incorporate data-dependent network\narchitectures into our iterations. Fortunately, it can be proved that by\nintroducing some mild checking conditions, all our original convergence results\ncan still be preserved for this learnable extension. As a nontrivial byproduct,\nwe demonstrate how to apply this ensemble-like methodology to address different\nlow-level vision tasks. Extensive experiments verify the theoretical results\nand show the advantages of our method against existing state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 06:06:28 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Liu", "Risheng", ""], ["Ma", "Long", ""], ["Yuan", "Xiaoming", ""], ["Zeng", "Shangzhi", ""], ["Zhang", "Jin", ""]]}, {"id": "1907.03089", "submitter": "Jingbo Lin", "authors": "Jingbo Lin, Weipeng Jing, and Houbing Song", "title": "SAN: Scale-Aware Network for Semantic Segmentation of High-Resolution\n  Aerial Images", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution aerial images have a wide range of applications, such as\nmilitary exploration, and urban planning. Semantic segmentation is a\nfundamental method extensively used in the analysis of high-resolution aerial\nimages. However, the ground objects in high-resolution aerial images have the\ncharacteristics of inconsistent scales, and this feature usually leads to\nunexpected predictions. To tackle this issue, we propose a novel scale-aware\nmodule (SAM). In SAM, we employ the re-sampling method aimed to make pixels\nadjust their positions to fit the ground objects with different scales, and it\nimplicitly introduces spatial attention by employing a re-sampling map as the\nweighted map. As a result, the network with the proposed module named\nscale-aware network (SANet) has a stronger ability to distinguish the ground\nobjects with inconsistent scale. Other than this, our proposed modules can\neasily embed in most of the existing network to improve their performance. We\nevaluate our modules on the International Society for Photogrammetry and Remote\nSensing Vaihingen Dataset, and the experimental results and comprehensive\nanalysis demonstrate the effectiveness of our proposed module.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 07:05:13 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Lin", "Jingbo", ""], ["Jing", "Weipeng", ""], ["Song", "Houbing", ""]]}, {"id": "1907.03100", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel", "title": "Regularizing linear inverse problems with convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks trained on large datsets have emerged as\nan intriguing alternative for compressing images and solving inverse problems\nsuch as denoising and compressive sensing. However, it has only recently been\nrealized that even without training, convolutional networks can function as\nconcise image models, and thus regularize inverse problems. In this paper, we\nprovide further evidence for this finding by studying variations of\nconvolutional neural networks that map few weight parameters to an image. The\nnetworks we consider only consist of convolutional operations, with either\nfixed or parameterized filters followed by ReLU non-linearities. We demonstrate\nthat with both fixed and parameterized convolutional filters those networks\nenable representing images with few coefficients. What is more, the\nunderparameterization enables regularization of inverse problems, in particular\nrecovering an image from few observations. We show that, similar to standard\ncompressive sensing guarantees, on the order of the number of model parameters\nmany measurements suffice for recovering an image from compressive\nmeasurements. Finally, we demonstrate that signal recovery with a un-trained\nconvolutional network outperforms standard l1 and total variation minimization\nfor magnetic resonance imaging (MRI).\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 09:30:51 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Heckel", "Reinhard", ""]]}, {"id": "1907.03118", "submitter": "Jie An", "authors": "Jie An, Haoyi Xiong, Jiebo Luo, Jun Huan, Jinwen Ma", "title": "Fast Universal Style Transfer for Artistic and Photorealistic Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Universal style transfer is an image editing task that renders an input\ncontent image using the visual style of arbitrary reference images, including\nboth artistic and photorealistic stylization. Given a pair of images as the\nsource of content and the reference of style, existing solutions usually first\ntrain an auto-encoder (AE) to reconstruct the image using deep features and\nthen embeds pre-defined style transfer modules into the AE reconstruction\nprocedure to transfer the style of the reconstructed image through modifying\nthe deep features. While existing methods typically need multiple rounds of\ntime-consuming AE reconstruction for better stylization, our work intends to\ndesign novel neural network architectures on top of AE for fast style transfer\nwith fewer artifacts and distortions all in one pass of end-to-end inference.\nTo this end, we propose two network architectures named ArtNet and PhotoNet to\nimprove artistic and photo-realistic stylization, respectively. Extensive\nexperiments demonstrate that ArtNet generates images with fewer artifacts and\ndistortions against the state-of-the-art artistic transfer algorithms, while\nPhotoNet improves the photorealistic stylization results by creating sharp\nimages faithfully preserving rich details of the input content. Moreover,\nArtNet and PhotoNet can achieve 3X to 100X speed-up over the state-of-the-art\nalgorithms, which is a major advantage for large content images.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 11:57:40 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["An", "Jie", ""], ["Xiong", "Haoyi", ""], ["Luo", "Jiebo", ""], ["Huan", "Jun", ""], ["Ma", "Jinwen", ""]]}, {"id": "1907.03123", "submitter": "Xiaomeng Li", "authors": "Xiaomeng Li, Lequan Yu, Chi-Wing Fu, Meng Fang and Pheng-Ann Heng", "title": "Revisiting Metric Learning for Few-Shot Image Classification", "comments": "Accept at Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of few-shot learning is to recognize new visual concepts with just a\nfew amount of labeled samples in each class. Recent effective metric-based\nfew-shot approaches employ neural networks to learn a feature similarity\ncomparison between query and support examples. However, the importance of\nfeature embedding, i.e., exploring the relationship among training samples, is\nneglected. In this work, we present a simple yet powerful baseline for few-shot\nclassification by emphasizing the importance of feature embedding.\nSpecifically, we revisit the classical triplet network from deep metric\nlearning, and extend it into a deep K-tuplet network for few-shot learning,\nutilizing the relationship among the input samples to learn a general\nrepresentation learning via episode-training. Once trained, our network is able\nto extract discriminative features for unseen novel categories and can be\nseamlessly incorporated with a non-linear distance metric function to\nfacilitate the few-shot classification. Our result on the miniImageNet\nbenchmark outperforms other metric-based few-shot classification methods. More\nimportantly, when evaluated on completely different datasets (Caltech-101,\nCUB-200, Stanford Dogs and Cars) using the model trained with miniImageNet, our\nmethod significantly outperforms prior methods, demonstrating its superior\ncapability to generalize to unseen classes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 12:19:01 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 04:24:48 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Li", "Xiaomeng", ""], ["Yu", "Lequan", ""], ["Fu", "Chi-Wing", ""], ["Fang", "Meng", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1907.03128", "submitter": "Pengju Liu", "authors": "Pengju Liu, Hongzhi Zhang, Wei Lian, and Wangmeng Zuo", "title": "Multi-level Wavelet Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, convolutional networks (CNNs) often adopts pooling to\nenlarge receptive field which has the advantage of low computational\ncomplexity. However, pooling can cause information loss and thus is detrimental\nto further operations such as features extraction and analysis. Recently,\ndilated filter has been proposed to trade off between receptive field size and\nefficiency. But the accompanying gridding effect can cause a sparse sampling of\ninput images with checkerboard patterns. To address this problem, in this\npaper, we propose a novel multi-level wavelet CNN (MWCNN) model to achieve\nbetter trade-off between receptive field size and computational efficiency. The\ncore idea is to embed wavelet transform into CNN architecture to reduce the\nresolution of feature maps while at the same time, increasing receptive field.\nSpecifically, MWCNN for image restoration is based on U-Net architecture, and\ninverse wavelet transform (IWT) is deployed to reconstruct the high resolution\n(HR) feature maps. The proposed MWCNN can also be viewed as an improvement of\ndilated filter and a generalization of average pooling, and can be applied to\nnot only image restoration tasks, but also any CNNs requiring a pooling\noperation. The experimental results demonstrate effectiveness of the proposed\nMWCNN for tasks such as image denoising, single image super-resolution, JPEG\nimage artifacts removal and object classification.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 14:19:03 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Liu", "Pengju", ""], ["Zhang", "Hongzhi", ""], ["Lian", "Wei", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1907.03141", "submitter": "Ning Liu", "authors": "Ning Liu and Xiaolong Ma and Zhiyuan Xu and Yanzhi Wang and Jian Tang\n  and Jieping Ye", "title": "AutoCompress: An Automatic DNN Structured Pruning Framework for\n  Ultra-High Compression Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured weight pruning is a representative model compression technique of\nDNNs to reduce the storage and computation requirements and accelerate\ninference. An automatic hyperparameter determination process is necessary due\nto the large number of flexible hyperparameters. This work proposes\nAutoCompress, an automatic structured pruning framework with the following key\nperformance improvements: (i) effectively incorporate the combination of\nstructured pruning schemes in the automatic process; (ii) adopt the\nstate-of-art ADMM-based structured weight pruning as the core algorithm, and\npropose an innovative additional purification step for further weight reduction\nwithout accuracy loss; and (iii) develop effective heuristic search method\nenhanced by experience-based guided search, replacing the prior deep\nreinforcement learning technique which has underlying incompatibility with the\ntarget pruning problem. Extensive experiments on CIFAR-10 and ImageNet datasets\ndemonstrate that AutoCompress is the key to achieve ultra-high pruning rates on\nthe number of weights and FLOPs that cannot be achieved before. As an example,\nAutoCompress outperforms the prior work on automatic model compression by up to\n33x in pruning rate (120x reduction in the actual parameter count) under the\nsame accuracy. Significant inference speedup has been observed from the\nAutoCompress framework on actual measurements on smartphone. We release all\nmodels of this work at anonymous link: http://bit.ly/2VZ63dS.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 15:40:02 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 12:15:38 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Liu", "Ning", ""], ["Ma", "Xiaolong", ""], ["Xu", "Zhiyuan", ""], ["Wang", "Yanzhi", ""], ["Tang", "Jian", ""], ["Ye", "Jieping", ""]]}, {"id": "1907.03165", "submitter": "Thibault Groueix M.", "authors": "Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell,\n  Mathieu Aubry", "title": "Unsupervised cycle-consistent deformation for shape matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised approach to deep surface deformation. Given a\npair of shapes, our algorithm directly predicts a parametric transformation\nfrom one shape to the other respecting correspondences. Our insight is to use\ncycle-consistency to define a notion of good correspondences in groups of\nobjects and use it as a supervisory signal to train our network. Our method\ndoes not rely on a template, assume near isometric deformations or rely on\npoint-correspondence supervision. We demonstrate the efficacy of our approach\nby using it to transfer segmentation across shapes. We show, on Shapenet, that\nour approach is competitive with comparable state-of-the-art methods when\nannotated training data is readily available, but outperforms them by a large\nmargin in the few-shot segmentation scenario.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 18:43:47 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Groueix", "Thibault", ""], ["Fisher", "Matthew", ""], ["Kim", "Vladimir G.", ""], ["Russell", "Bryan C.", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1907.03196", "submitter": "Alessandro Lameiras Koerich", "authors": "Juan D. S. Ortega, Mohammed Senoussaoui, Eric Granger, Marco\n  Pedersoli, Patrick Cardinal and Alessandro L. Koerich", "title": "Multimodal Fusion with Deep Neural Networks for Audio-Video Emotion\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep neural network (DNN) for multimodal fusion\nof audio, video and text modalities for emotion recognition. The proposed DNN\narchitecture has independent and shared layers which aim to learn the\nrepresentation for each modality, as well as the best combined representation\nto achieve the best prediction. Experimental results on the AVEC Sentiment\nAnalysis in the Wild dataset indicate that the proposed DNN can achieve a\nhigher level of Concordance Correlation Coefficient (CCC) than other\nstate-of-the-art systems that perform early fusion of modalities at\nfeature-level (i.e., concatenation) and late fusion at score-level (i.e.,\nweighted average) fusion. The proposed DNN has achieved CCCs of 0.606, 0.534,\nand 0.170 on the development partition of the dataset for predicting arousal,\nvalence and liking, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 22:12:42 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Ortega", "Juan D. S.", ""], ["Senoussaoui", "Mohammed", ""], ["Granger", "Eric", ""], ["Pedersoli", "Marco", ""], ["Cardinal", "Patrick", ""], ["Koerich", "Alessandro L.", ""]]}, {"id": "1907.03217", "submitter": "Da He", "authors": "Da He, De Cai, Jiasheng Zhou, Jiajia Luo, Sung-Liang Chen", "title": "Adaptive Weighting Depth-variant Deconvolution of Fluorescence\n  Microscopy Images with Convolutional Neural Network", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescence microscopy plays an important role in biomedical research. The\ndepth-variant point spread function (PSF) of a fluorescence microscope produces\nlow-quality images especially in the out-of-focus regions of thick specimens.\nTraditional deconvolution to restore the out-of-focus images is usually\ninsufficient since a depth-invariant PSF is assumed. This article aims at\nhandling fluorescence microscopy images by learning-based depth-variant PSF and\nreducing artifacts. We propose adaptive weighting depth-variant deconvolution\n(AWDVD) with defocus level prediction convolutional neural network (DelpNet) to\nrestore the out-of-focus images. Depth-variant PSFs of image patches can be\nobtained by DelpNet and applied in the afterward deconvolution. AWDVD is\nadopted for a whole image which is patch-wise deconvolved and appropriately\ncropped before deconvolution. DelpNet achieves the accuracy of 98.2%, which\noutperforms the best-ever one using the same microscopy dataset. Image patches\nof 11 defocus levels after deconvolution are validated with maximum improvement\nin the peak signal-to-noise ratio and structural similarity index of 6.6 dB and\n11%, respectively. The adaptive weighting of the patch-wise deconvolved image\ncan eliminate patch boundary artifacts and improve deconvolved image quality.\nThe proposed method can accurately estimate depth-variant PSF and effectively\nrecover out-of-focus microscopy images. To our acknowledge, this is the first\nstudy of handling out-of-focus microscopy images using learning-based\ndepth-variant PSF. Facing one of the most common blurs in fluorescence\nmicroscopy, the novel method provides a practical technology to improve the\nimage quality.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 03:58:04 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["He", "Da", ""], ["Cai", "De", ""], ["Zhou", "Jiasheng", ""], ["Luo", "Jiajia", ""], ["Chen", "Sung-Liang", ""]]}, {"id": "1907.03220", "submitter": "Saket Chaturvedi", "authors": "Saket S. Chaturvedi, Kajol Gupta, and Prakash. S. Prasad", "title": "Skin Lesion Analyser: An Efficient Seven-Way Multi-Class Skin Cancer\n  Classification Using MobileNet", "comments": "This is a pre-copyedited version of a contribution published in\n  Advances in Intelligent Systems and Computing, Hassanien A., Bhatnagar R.,\n  Darwish A. (eds) published by Chaturvedi S.S., Gupta K., Prasad P.S. The\n  definitive authentication version is available online via\n  https://doi.org/10.1007/978-981-15-3383-9_15", "journal-ref": "In: Hassanien A., Bhatnagar R., Darwish A. (eds) Advanced Machine\n  Learning Technologies and Applications. AMLTA 2020. Advances in Intelligent\n  Systems and Computing, vol 1141. Springer, Singapore", "doi": "10.1007/978-981-15-3383-9_15", "report-no": "AISC, volume 1141", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer, a major form of cancer, is a critical public health problem with\n123,000 newly diagnosed melanoma cases and between 2 and 3 million non-melanoma\ncases worldwide each year. The leading cause of skin cancer is high exposure of\nskin cells to UV radiation, which can damage the DNA inside skin cells leading\nto uncontrolled growth of skin cells. Skin cancer is primarily diagnosed\nvisually employing clinical screening, a biopsy, dermoscopic analysis, and\nhistopathological examination. It has been demonstrated that the dermoscopic\nanalysis in the hands of inexperienced dermatologists may cause a reduction in\ndiagnostic accuracy. Early detection and screening of skin cancer have the\npotential to reduce mortality and morbidity. Previous studies have shown Deep\nLearning ability to perform better than human experts in several visual\nrecognition tasks. In this paper, we propose an efficient seven-way automated\nmulti-class skin cancer classification system having performance comparable\nwith expert dermatologists. We used a pretrained MobileNet model to train over\nHAM10000 dataset using transfer learning. The model classifies skin lesion\nimage with a categorical accuracy of 83.1 percent, top2 accuracy of 91.36\npercent and top3 accuracy of 95.34 percent. The weighted average of precision,\nrecall, and f1-score were found to be 0.89, 0.83, and 0.83 respectively. The\nmodel has been deployed as a web application for public use at\n(https://saketchaturvedi.github.io). This fast, expansible method holds the\npotential for substantial clinical impact, including broadening the scope of\nprimary care practice and augmenting clinical decision-making for dermatology\nspecialists.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 05:10:01 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 17:28:13 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 05:47:45 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Chaturvedi", "Saket S.", ""], ["Gupta", "Kajol", ""], ["Prasad", "Prakash. S.", ""]]}, {"id": "1907.03221", "submitter": "Xiaole Zhao", "authors": "Xiaole Zhao, Ying Liao, Tian He, Yulun Zhang, Yadong Wu, Tao Zhang", "title": "FC$^2$N: Fully Channel-Concatenated Network for Single Image\n  Super-Resolution", "comments": "17 pages, 8 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current image super-resolution (SR) methods based on convolutional\nneural networks (CNNs) use residual learning in network structural design,\nwhich favors to effective back propagation and hence improves SR performance by\nincreasing model scale. However, residual networks suffer from representational\nredundancy by introducing identity paths that impede the full exploitation of\nmodel capacity. Besides, blindly enlarging network scale can cause more\nproblems in model training, even with residual learning. In this paper, a novel\nfully channel-concatenated network (FC$^2$N) is presented to make further\nmining of representational capacity of deep models, in which all interlayer\nskips are implemented by a simple and straightforward operation, i.e., weighted\nchannel concatenation (WCC), followed by a 1$\\times$1 conv layer. Based on the\nWCC, the model can achieve the joint attention mechanism of linear and\nnonlinear features in the network, and presents better performance than other\nstate-of-the-art SR models with fewer model parameters. To our best knowledge,\nFC$^2$N is the first CNN model that does not use residual learning and reaches\nnetwork depth over 400 layers. Moreover, it shows excellent performance in both\nlargescale and lightweight implementations, which illustrates the full\nexploitation of the representational capacity of the model.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 05:18:12 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 12:01:36 GMT"}, {"version": "v3", "created": "Sun, 8 Mar 2020 15:19:02 GMT"}, {"version": "v4", "created": "Wed, 5 May 2021 07:17:17 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhao", "Xiaole", ""], ["Liao", "Ying", ""], ["He", "Tian", ""], ["Zhang", "Yulun", ""], ["Wu", "Yadong", ""], ["Zhang", "Tao", ""]]}, {"id": "1907.03241", "submitter": "Mo Zhang", "authors": "Mo Zhang, Jie Zhao, Xiang Li, Li Zhang, Quanzheng Li", "title": "ASCNet: Adaptive-Scale Convolutional Neural Networks for Multi-Scale\n  Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting multi-scale information is key to semantic segmentation. However,\nthe classic convolutional neural networks (CNNs) encounter difficulties in\nachieving multi-scale information extraction: expanding convolutional kernel\nincurs the high computational cost and using maximum pooling sacrifices image\ninformation. The recently developed dilated convolution solves these problems,\nbut with the limitation that the dilation rates are fixed and therefore the\nreceptive field cannot fit for all objects with different sizes in the image.\nWe propose an adaptivescale convolutional neural network (ASCNet), which\nintroduces a 3-layer convolution structure in the end-to-end training, to\nadaptively learn an appropriate dilation rate for each pixel in the image. Such\npixel-level dilation rates produce optimal receptive fields so that the\ninformation of objects with different sizes can be extracted at the\ncorresponding scale. We compare the segmentation results using the classic CNN,\nthe dilated CNN and the proposed ASCNet on two types of medical images (The\nHerlev dataset and SCD RBC dataset). The experimental results show that ASCNet\nachieves the highest accuracy. Moreover, the automatically generated dilation\nrates are positively correlated to the sizes of the objects, confirming the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 07:52:24 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhang", "Mo", ""], ["Zhao", "Jie", ""], ["Li", "Xiang", ""], ["Zhang", "Li", ""], ["Li", "Quanzheng", ""]]}, {"id": "1907.03246", "submitter": "Wei Song", "authors": "Yan Wang, Wei Song, Giancarlo Fortino, Lizhe Qi, Wenqiang Zhang,\n  Antonio Liotta", "title": "An Experimental-based Review of Image Enhancement and Image Restoration\n  Methods for Underwater Imaging", "comments": "19", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater images play a key role in ocean exploration, but often suffer from\nsevere quality degradation due to light absorption and scattering in water\nmedium. Although major breakthroughs have been made recently in the general\narea of image enhancement and restoration, the applicability of new methods for\nimproving the quality of underwater images has not specifically been captured.\nIn this paper, we review the image enhancement and restoration methods that\ntackle typical underwater image impairments, including some extreme\ndegradations and distortions. Firstly, we introduce the key causes of quality\nreduction in underwater images, in terms of the underwater image formation\nmodel (IFM). Then, we review underwater restoration methods, considering both\nthe IFM-free and the IFM-based approaches. Next, we present an\nexperimental-based comparative evaluation of state-of-the-art IFM-free and\nIFM-based methods, considering also the prior-based parameter estimation\nalgorithms of the IFM-based methods, using both subjective and objective\nanalysis (the used code is freely available at\nhttps://github.com/wangyanckxx/Single-Underwater-Image-Enhancement-and-Color-Restoration).\nStarting from this study, we pinpoint the key shortcomings of existing methods,\ndrawing recommendations for future research in this area. Our review of\nunderwater image enhancement and restoration provides researchers with the\nnecessary background to appreciate challenges and opportunities in this\nimportant field.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 08:22:49 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Wang", "Yan", ""], ["Song", "Wei", ""], ["Fortino", "Giancarlo", ""], ["Qi", "Lizhe", ""], ["Zhang", "Wenqiang", ""], ["Liotta", "Antonio", ""]]}, {"id": "1907.03248", "submitter": "Estephe Arnaud", "authors": "Estephe Arnaud, Arnaud Dapogny, Kevin Bailly", "title": "Tree-gated Deep Regressor Ensemble For Face Alignment In The Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment consists in aligning a shape model on a face in an image. It\nis an active domain in computer vision as it is a preprocessing for\napplications like facial expression recognition, face recognition and tracking,\nface animation, etc. Current state-of-the-art methods already perform well on\n\"easy\" datasets, i.e. those that present moderate variations in head pose,\nexpression, illumination or partial occlusions, but may not be robust to\n\"in-the-wild\" data. In this paper, we address this problem by using an ensemble\nof deep regressors instead of a single large regressor. Furthermore, instead of\naveraging the outputs of each regressor, we propose an adaptive weighting\nscheme that uses a tree-structured gate. Experiments on several challenging\nface datasets demonstrate that our approach outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 08:28:01 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 13:12:53 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Arnaud", "Estephe", ""], ["Dapogny", "Arnaud", ""], ["Bailly", "Kevin", ""]]}, {"id": "1907.03253", "submitter": "Peijia Chen", "authors": "Jiaxuan Zhuo, Jianhuang Lai and Peijia Chen", "title": "A Novel Teacher-Student Learning Framework For Occluded Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) has made great progress in recent years, but\nocclusion is still a challenging problem which significantly degenerates the\nidentification performance. In this paper, we design a teacher-student learning\nframework to learn an occlusion-robust model from the full-body person domain\nto the occluded person domain. Notably, the teacher network only uses\nlarge-scale full-body person data to simulate the learning process of occluded\nperson re-id. Based on the teacher network, the student network then trains a\nbetter model by using inadequate real-world occluded person data. In order to\ntransfer more knowledge from the teacher network to the student network, we\nequip the proposed framework with a co-saliency network and a cross-domain\nsimulator. The co-saliency network extracts the backbone features, and two\nseparated collaborative branches are followed by the backbone. One branch is a\nclassification branch for identity recognition and the other is a co-saliency\nbranch for guiding the network to highlight meaningful parts without any manual\nannotation. The cross-domain simulator generates artificial occlusions on\nfull-body person data under a growing probability so that the teacher network\ncould train a cross-domain model by observing more and more occluded cases.\nExperiments on four occluded person re-id benchmarks show that our method\noutperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 08:52:38 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhuo", "Jiaxuan", ""], ["Lai", "Jianhuang", ""], ["Chen", "Peijia", ""]]}, {"id": "1907.03261", "submitter": "Assia Benbihi", "authors": "Assia Benbihi, Matthieu Geist, C\\'edric Pradalier", "title": "ELF: Embedded Localisation of Features in pre-trained CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel feature detector based only on information\nembedded inside a CNN trained on standard tasks (e.g. classification). While\nprevious works already show that the features of a trained CNN are suitable\ndescriptors, we show here how to extract the feature locations from the network\nto build a detector. This information is computed from the gradient of the\nfeature map with respect to the input image. This provides a saliency map with\nlocal maxima on relevant keypoint locations. Contrary to recent CNN-based\ndetectors, this method requires neither supervised training nor finetuning. We\nevaluate how repeatable and how matchable the detected keypoints are with the\nrepeatability and matching scores. Matchability is measured with a simple\ndescriptor introduced for the sake of the evaluation. This novel detector\nreaches similar performances on the standard evaluation HPatches dataset, as\nwell as comparable robustness against illumination and viewpoint changes on\nWebcam and photo-tourism images. These results show that a CNN trained on a\nstandard task embeds feature location information that is as relevant as when\nthe CNN is specifically trained for feature detection.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 09:41:44 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Benbihi", "Assia", ""], ["Geist", "Matthieu", ""], ["Pradalier", "C\u00e9dric", ""]]}, {"id": "1907.03297", "submitter": "Dong Nie", "authors": "Dong Nie and Lei Xiang and Qian Wang and Dinggang Shen", "title": "Dual Adversarial Learning with Attention Mechanism for Fine-grained\n  Medical Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging plays a critical role in various clinical applications.\nHowever, due to multiple considerations such as cost and risk, the acquisition\nof certain image modalities could be limited. To address this issue, many\ncross-modality medical image synthesis methods have been proposed. However, the\ncurrent methods cannot well model the hard-to-synthesis regions (e.g., tumor or\nlesion regions). To address this issue, we propose a simple but effective\nstrategy, that is, we propose a dual-discriminator (dual-D) adversarial\nlearning system, in which, a global-D is used to make an overall evaluation for\nthe synthetic image, and a local-D is proposed to densely evaluate the local\nregions of the synthetic image. More importantly, we build an adversarial\nattention mechanism which targets at better modeling hard-to-synthesize regions\n(e.g., tumor or lesion regions) based on the local-D. Experimental results show\nthe robustness and accuracy of our method in synthesizing fine-grained target\nimages from the corresponding source images. In particular, we evaluate our\nmethod on two datasets, i.e., to address the tasks of generating T2 MRI from T1\nMRI for the brain tumor images and generating MRI from CT. Our method\noutperforms the state-of-the-art methods under comparison in all datasets and\ntasks. And the proposed difficult-region-aware attention mechanism is also\nproved to be able to help generate more realistic images, especially for the\nhard-to-synthesize regions.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 14:10:05 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Nie", "Dong", ""], ["Xiang", "Lei", ""], ["Wang", "Qian", ""], ["Shen", "Dinggang", ""]]}, {"id": "1907.03326", "submitter": "Emanuela Haller", "authors": "Emanuela Haller, Adina Magda Florea, Marius Leordeanu", "title": "Spacetime Graph Optimization for Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging task of foreground object discovery and\nsegmentation in video. We introduce an efficient solution, suitable for both\nunsupervised and supervised scenarios, based on a spacetime graph\nrepresentation of the video sequence. We ensure a fine grained representation\nwith one-to-one correspondences between graph nodes and video pixels. We\nformulate the task as a spectral clustering problem by exploiting the\nspatio-temporal consistency between the scene elements in terms of motion and\nappearance. Graph nodes that belong to the main object of interest should form\na strong cluster, as they are linked through long range optical flow chains and\nhave similar motion and appearance features along those chains. On one hand,\nthe optimization problem aims to maximize the segmentation clustering score\nbased on the motion structure through space and time. On the other hand, the\nsegmentation should be consistent with respect to node features. Our approach\nleads to a graph formulation in which the segmentation solution becomes the\nprincipal eigenvector of a novel Feature-Motion matrix. While the actual matrix\nis not computed explicitly, the proposed algorithm efficiently computes, in a\nfew iteration steps, the principal eigenvector that captures the segmentation\nof the main object in the video. The proposed algorithm, GO-VOS, produces a\nglobal optimum solution and, consequently, it does not depend on\ninitialization. In practice, GO-VOS achieves state of the art results on three\nchallenging datasets used in current literature: DAVIS, SegTrack and\nYouTube-Objects.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 18:05:04 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 08:59:14 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Haller", "Emanuela", ""], ["Florea", "Adina Magda", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1907.03327", "submitter": "Reuben Dorent", "authors": "Reuben Dorent, Wenqi Li, Jinendra Ekanayake, Sebastien Ourselin, Tom\n  Vercauteren", "title": "Learning joint lesion and tissue segmentation from task-specific\n  hetero-modal datasets", "comments": "Accepted as an oral presentation at MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "http://proceedings.mlr.press/v102/dorent19a.html", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain tissue segmentation from multimodal MRI is a key building block of many\nneuroscience analysis pipelines. It could also play an important role in many\nclinical imaging scenarios. Established tissue segmentation approaches have\nhowever not been developed to cope with large anatomical changes resulting from\npathology. The effect of the presence of brain lesions, for example, on their\nperformance is thus currently uncontrolled and practically unpredictable.\nContrastingly, with the advent of deep neural networks (DNNs), segmentation of\nbrain lesions has matured significantly and is achieving performance levels\nmaking it of interest for clinical use. However, few existing approaches allow\nfor jointly segmenting normal tissue and brain lesions. Developing a DNN for\nsuch joint task is currently hampered by the fact that annotated datasets\ntypically address only one specific task and rely on a task-specific\nhetero-modal imaging protocol. In this work, we propose a novel approach to\nbuild a joint tissue and lesion segmentation model from task-specific\nhetero-modal and partially annotated datasets. Starting from a variational\nformulation of the joint problem, we show how the expected risk can be\ndecomposed and optimised empirically. We exploit an upper-bound of the risk to\ndeal with missing imaging modalities. For each task, our approach reaches\ncomparable performance than task-specific and fully-supervised models.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 18:09:43 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Dorent", "Reuben", ""], ["Li", "Wenqi", ""], ["Ekanayake", "Jinendra", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1907.03338", "submitter": "Alain Jungo", "authors": "Alain Jungo, Mauricio Reyes", "title": "Assessing Reliability and Challenges of Uncertainty Estimations for\n  Medical Image Segmentation", "comments": "Appears in Medical Image Computing and Computer Assisted\n  Interventions (MICCAI), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent improvements in overall accuracy, deep learning systems\nstill exhibit low levels of robustness. Detecting possible failures is critical\nfor a successful clinical integration of these systems, where each data point\ncorresponds to an individual patient. Uncertainty measures are a promising\ndirection to improve failure detection since they provide a measure of a\nsystem's confidence. Although many uncertainty estimation methods have been\nproposed for deep learning, little is known on their benefits and current\nchallenges for medical image segmentation. Therefore, we report results of\nevaluating common voxel-wise uncertainty measures with respect to their\nreliability, and limitations on two medical image segmentation datasets.\nResults show that current uncertainty methods perform similarly and although\nthey are well-calibrated at the dataset level, they tend to be miscalibrated at\nsubject-level. Therefore, the reliability of uncertainty estimates is\ncompromised, highlighting the importance of developing subject-wise uncertainty\nestimations. Additionally, among the benchmarked methods, we found auxiliary\nnetworks to be a valid alternative to common uncertainty methods since they can\nbe applied to any previously trained segmentation model.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 19:36:28 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 09:17:20 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Jungo", "Alain", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1907.03381", "submitter": "Yanyan Xu", "authors": "Wuwei Lan, Yanyan Xu, Bin Zhao", "title": "Travel Time Estimation without Road Networks: An Urban Morphological\n  Layout Representation Approach", "comments": "Accepted at IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Travel time estimation is a crucial task for not only personal travel\nscheduling but also city planning. Previous methods focus on modeling toward\nroad segments or sub-paths, then summing up for a final prediction, which have\nbeen recently replaced by deep neural models with end-to-end training. Usually,\nthese methods are based on explicit feature representations, including\nspatio-temporal features, traffic states, etc. Here, we argue that the local\ntraffic condition is closely tied up with the land-use and built environment,\ni.e., metro stations, arterial roads, intersections, commercial area,\nresidential area, and etc, yet the relation is time-varying and too complicated\nto model explicitly and efficiently. Thus, this paper proposes an end-to-end\nmulti-task deep neural model, named Deep Image to Time (DeepI2T), to learn the\ntravel time mainly from the built environment images, a.k.a. the morphological\nlayout images, and showoff the new state-of-the-art performance on real-world\ndatasets in two cities. Moreover, our model is designed to tackle both\npath-aware and path-blind scenarios in the testing phase. This work opens up\nnew opportunities of using the publicly available morphological layout images\nas considerable information in multiple geography-related smart city\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 01:52:35 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Lan", "Wuwei", ""], ["Xu", "Yanyan", ""], ["Zhao", "Bin", ""]]}, {"id": "1907.03387", "submitter": "Fangqiao Hu", "authors": "Fangqiao Hu, Jin Zhao, Yong Huang, Hui Li", "title": "Learning Structural Graph Layouts and 3D Shapes for Long Span Bridges 3D\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A learning-based 3D reconstruction method for long-span bridges is proposed\nin this paper. 3D reconstruction generates a 3D computer model of a real object\nor scene from images, it involves many stages and open problems. Existing\npoint-based methods focus on generating 3D point clouds and their reconstructed\npolygonal mesh or fitting-based geometrical models in urban scenes civil\nstructures reconstruction within Manhattan world constrains and have made great\nachievements. Difficulties arise when an attempt is made to transfer these\nsystems to structures with complex topology and part relations like steel\ntrusses and long-span bridges, this could be attributed to point clouds are\noften unevenly distributed with noise and suffer from occlusions and\nincompletion, recovering a satisfactory 3D model from these highly unstructured\npoint clouds in a bottom-up pattern while preserving the geometrical and\ntopological properties makes enormous challenge to existing algorithms.\nConsidering the prior human knowledge that these structures are in conformity\nto regular spatial layouts in terms of components, a learning-based\ntopology-aware 3D reconstruction method which can obtain high-level structural\ngraph layouts and low-level 3D shapes from images is proposed in this paper. We\ndemonstrate the feasibility of this method by testing on two real long-span\nsteel truss cable-stayed bridges.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 02:39:04 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 12:42:03 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Hu", "Fangqiao", ""], ["Zhao", "Jin", ""], ["Huang", "Yong", ""], ["Li", "Hui", ""]]}, {"id": "1907.03395", "submitter": "Vineet Kosaraju", "authors": "Vineet Kosaraju, Amir Sadeghian, Roberto Mart\\'in-Mart\\'in, Ian Reid,\n  S. Hamid Rezatofighi, Silvio Savarese", "title": "Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and\n  Graph Attention Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Predicting the future trajectories of multiple interacting agents in a scene\nhas become an increasingly important problem for many different applications\nranging from control of autonomous vehicles and social robots to security and\nsurveillance. This problem is compounded by the presence of social interactions\nbetween humans and their physical interactions with the scene. While the\nexisting literature has explored some of these cues, they mainly ignored the\nmultimodal nature of each human's future trajectory. In this paper, we present\nSocial-BiGAT, a graph-based generative adversarial network that generates\nrealistic, multimodal trajectory predictions by better modelling the social\ninteractions of pedestrians in a scene. Our method is based on a graph\nattention network (GAT) that learns reliable feature representations that\nencode the social interactions between humans in the scene, and a recurrent\nencoder-decoder architecture that is trained adversarially to predict, based on\nthe features, the humans' paths. We explicitly account for the multimodal\nnature of the prediction problem by forming a reversible transformation between\neach scene and its latent noise vector, as in Bicycle-GAN. We show that our\nframework achieves state-of-the-art performance comparing it to several\nbaselines on existing trajectory forecasting benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 23:48:07 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 01:05:26 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kosaraju", "Vineet", ""], ["Sadeghian", "Amir", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Reid", "Ian", ""], ["Rezatofighi", "S. Hamid", ""], ["Savarese", "Silvio", ""]]}, {"id": "1907.03398", "submitter": "Xin Jin", "authors": "Xin Jin, Rui Han, Ning Ning, Xiaodong Li, Xiaokun Zhang", "title": "Facial Makeup Transfer Combining Illumination Transfer", "comments": "IEEE Access, conference short version: ISAIR2019", "journal-ref": "in IEEE Access, vol. 7, pp. 80928-80936, 2019", "doi": "10.1109/ACCESS.2019.2923116", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet the women appearance needs, we present a novel virtual experience\napproach of facial makeup transfer, developed into windows platform application\nsoftware. The makeup effects could present on the user's input image in real\ntime, with an only single reference image. The input image and reference image\nare divided into three layers by facial feature points landmarked: facial\nstructure layer, facial color layer, and facial detail layer. Except for the\nabove layers are processed by different algorithms to generate output image, we\nalso add illumination transfer, so that the illumination effect of the\nreference image is automatically transferred to the input image. Our approach\nhas the following three advantages: (1) Black or dark and white facial makeup\ncould be effectively transferred by introducing illumination transfer; (2)\nEfficiently transfer facial makeup within seconds compared to those methods\nbased on deep learning frameworks; (3) Reference images with the air-bangs\ncould transfer makeup perfectly.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 04:12:25 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Jin", "Xin", ""], ["Han", "Rui", ""], ["Ning", "Ning", ""], ["Li", "Xiaodong", ""], ["Zhang", "Xiaokun", ""]]}, {"id": "1907.03402", "submitter": "Sepidehsadat Hosseini", "authors": "Sepidehsadat Hosseini, Mohammad Amin Shabani, Nam Ik Cho", "title": "Distill-2MD-MTL: Data Distillation based on Multi-Dataset Multi-Domain\n  Multi-Task Frame Work to Solve Face Related Tasksks, Multi Task Learning,\n  Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new semi-supervised learning method on face-related tasks based\non Multi-Task Learning (MTL) and data distillation. The proposed method\nexploits multiple datasets with different labels for different-but-related\ntasks such as simultaneous age, gender, race, facial expression estimation.\nSpecifically, when there are only a few well-labeled data for a specific task\namong the multiple related ones, we exploit the labels of other related tasks\nin different domains. Our approach is composed of (1) a new MTL method which\ncan deal with weakly labeled datasets and perform several tasks simultaneously,\nand (2) an MTL-based data distillation framework which enables network\ngeneralization for the training and test data from different domains.\nExperiments show that the proposed multi-task system performs each task better\nthan the baseline single task. It is also demonstrated that using different\ndomain datasets along with the main dataset can enhance network generalization\nand overcome the domain differences between datasets. Also, comparing data\ndistillation both on the baseline and MTL framework, the latter shows more\naccurate predictions on unlabeled data from different domains. Furthermore, by\nproposing a new learning-rate optimization method, our proposed network is able\nto dynamically tune its learning rate.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 04:36:32 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 02:57:13 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Hosseini", "Sepidehsadat", ""], ["Shabani", "Mohammad Amin", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1907.03422", "submitter": "Kai Wang", "authors": "Kai Wang, Jianfei Yang, Da Guo, Kaipeng Zhang, Xiaojiang Peng, Yu Qiao", "title": "Bootstrap Model Ensemble and Rank Loss for Engagement Intensity\n  Regression", "comments": "This paper is about EmotiW 2019 engagement intensity regression\n  challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our approach for the engagement intensity regression task\nof EmotiW 2019. The task is to predict the engagement intensity value of a\nstudent when he or she is watching an online MOOCs video in various conditions.\nBased on our winner solution last year, we mainly explore head features and\nbody features with a bootstrap strategy and two novel loss functions in this\npaper. We maintain the framework of multi-instance learning with long\nshort-term memory (LSTM) network, and make three contributions. First, besides\nof the gaze and head pose features, we explore facial landmark features in our\nframework. Second, inspired by the fact that engagement intensity can be ranked\nin values, we design a rank loss as a regularization which enforces a distance\nmargin between the features of distant category pairs and adjacent category\npairs. Third, we use the classical bootstrap aggregation method to perform\nmodel ensemble which randomly samples a certain training data by several times\nand then averages the model predictions. We evaluate the performance of our\nmethod and discuss the influence of each part on the validation dataset. Our\nmethods finally win 3rd place with MSE of 0.0626 on the testing set.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 06:54:47 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Wang", "Kai", ""], ["Yang", "Jianfei", ""], ["Guo", "Da", ""], ["Zhang", "Kaipeng", ""], ["Peng", "Xiaojiang", ""], ["Qiao", "Yu", ""]]}, {"id": "1907.03424", "submitter": "Jianzhu Huai", "authors": "Jianzhu Huai, Yusen Qin, Fumin Pang, Zichong Chen", "title": "Segway DRIVE Benchmark: Place Recognition and SLAM Data Collected by A\n  Fleet of Delivery Robots", "comments": "8 pages, 4 figures, 4 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition and simultaneous localization and mapping (SLAM)\nhave recently begun to be used in real-world autonomous navigation tasks like\nfood delivery. Existing datasets for SLAM research are often not representative\nof in situ operations, leaving a gap between academic research and real-world\ndeployment. In response, this paper presents the Segway DRIVE benchmark, a\nnovel and challenging dataset suite collected by a fleet of Segway delivery\nrobots. Each robot is equipped with a global-shutter fisheye camera, a\nconsumer-grade IMU synced to the camera on chip, two low-cost wheel encoders,\nand a removable high-precision lidar for generating reference solutions. As\nthey routinely carry out tasks in office buildings and shopping malls while\ncollecting data, the dataset spanning a year is characterized by planar\nmotions, moving pedestrians in scenes, and changing environment and lighting.\nSuch factors typically pose severe challenges and may lead to failures for SLAM\nalgorithms. Moreover, several metrics are proposed to evaluate metric place\nrecognition algorithms. With these metrics, sample SLAM and metric place\nrecognition methods were evaluated on this benchmark.\n  The first release of our benchmark has hundreds of sequences, covering more\nthan 50 km of indoor floors. More data will be added as the robot fleet\ncontinues to operate in real life. The benchmark is available at\nhttp://drive.segwayrobotics.com/#/dataset/download.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 07:06:14 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Huai", "Jianzhu", ""], ["Qin", "Yusen", ""], ["Pang", "Fumin", ""], ["Chen", "Zichong", ""]]}, {"id": "1907.03448", "submitter": "Suiyi Ling", "authors": "Ling suiyi, Li Jing, Le Callet Patrick, Wang Junle", "title": "Perceptual representations of structural information in images:\n  application to quality assessment of synthesized view in FTV scenario", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the immersive multimedia techniques like Free-viewpoint TV (FTV) develop\nat an astonishing rate, user's demand for high-quality immersive contents\nincreases dramatically. Unlike traditional uniform artifacts, the distortions\nwithin immersive contents could be non-uniform structure-related and thus are\nchallenging for commonly used quality metrics. Recent studies have demonstrated\nthat the representation of visual features can be extracted from multiple\nlevels of the hierarchy. Inspired by the hierarchical representation mechanism\nin the human visual system (HVS), in this paper, we explore to adopt structural\nrepresentations to quantitatively measure the impact of such structure-related\ndistortion on perceived quality in FTV scenario. More specifically, a\nbio-inspired full reference image quality metric is proposed based on 1)\nlow-level contour descriptor; 2) mid-level contour category descriptor; and 3)\ntask-oriented non-natural structure descriptor. The experimental results show\nthat the proposed model outperforms significantly the state-of-the-art metrics.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 08:23:31 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["suiyi", "Ling", ""], ["Jing", "Li", ""], ["Patrick", "Le Callet", ""], ["Junle", "Wang", ""]]}, {"id": "1907.03465", "submitter": "Jiakui Wang", "authors": "Yuhao Xu and Jiakui Wang", "title": "A unified neural network for object detection, multiple object tracking\n  and vehicle re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep SORT\\cite{wojke2017simple} is a tracking-by-detetion approach to\nmultiple object tracking with a detector and a RE-ID model.\n  Both separately training and inference with the two model is time-comsuming.\n  In this paper, we unify the detector and RE-ID model into an end-to-end\nnetwork, by adding an additional track branch for tracking in Faster RCNN\narchitecture. With a unified network, we are able to train the whole model\nend-to-end with multi loss, which has shown much benefit in other recent works.\n  The RE-ID model in Deep SORT needs to use deep CNNs to extract feature map\nfrom detected object images, However, track branch in our proposed network\nstraight make use of\n  the RoI feature vector in Faster RCNN baseline, which reduced the amount of\ncalculation.\n  Since the single image lacks the same object which is necessary when we use\nthe triplet loss to optimizer the track branch, we concatenate the neighbouring\nframes in a video to construct our training dataset.\n  We have trained and evaluated our model on AIC19 vehicle tracking dataset,\nexperiment shows that our model with resnet101 backbone can achieve 57.79 \\%\nmAP and track vehicle well.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 09:07:22 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Xu", "Yuhao", ""], ["Wang", "Jiakui", ""]]}, {"id": "1907.03520", "submitter": "Huy-Hieu Pham", "authors": "Huy Hieu Pham, Houssam Salmane, Louahdi Khoudour, Alain Crouzil, Pablo\n  Zegers, Sergio A Velastin", "title": "A Deep Learning Approach for Real-Time 3D Human Action Recognition from\n  Skeletal Data", "comments": "Accepted by the 16th International Conference on Image Analysis and\n  Recognition (ICIAR2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deep learning approach for real-time 3D human action\nrecognition from skeletal data and apply it to develop a vision-based\nintelligent surveillance system. Given a skeleton sequence, we propose to\nencode skeleton poses and their motions into a single RGB image. An Adaptive\nHistogram Equalization (AHE) algorithm is then applied on the color images to\nenhance their local patterns and generate more discriminative features. For\nlearning and classification tasks, we design Deep Neural Networks based on the\nDensely Connected Convolutional Architecture (DenseNet) to extract features\nfrom enhanced-color images and classify them into classes. Experimental results\non two challenging datasets show that the proposed method reaches\nstate-of-the-art accuracy, whilst requiring low computational time for training\nand inference. This paper also introduces CEMEST, a new RGB-D dataset depicting\npassenger behaviors in public transport. It consists of 203 untrimmed\nreal-world surveillance videos of realistic normal and anomalous events. We\nachieve promising results on real conditions of this dataset with the support\nof data augmentation and transfer learning techniques. This enables the\nconstruction of real-world applications based on deep learning for enhancing\nmonitoring and security in public transport.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 11:22:41 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Pham", "Huy Hieu", ""], ["Salmane", "Houssam", ""], ["Khoudour", "Louahdi", ""], ["Crouzil", "Alain", ""], ["Zegers", "Pablo", ""], ["Velastin", "Sergio A", ""]]}, {"id": "1907.03537", "submitter": "Tomas Jenicek", "authors": "Tomas Jenicek, Ond\\v{r}ej Chum", "title": "Linking Art through Human Poses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the discovery of composition transfer in artworks based on their\nvisual content. Automated analysis of large art collections, which are growing\nas a result of art digitization among museums and galleries, is an important\ntool for art history and assists cultural heritage preservation. Modern image\nretrieval systems offer good performance on visually similar artworks, but fail\nin the cases of more abstract composition transfer. The proposed approach links\nartworks through a pose similarity of human figures depicted in images. Human\nfigures are the subject of a large fraction of visual art from middle ages to\nmodernity and their distinctive poses were often a source of inspiration among\nartists. The method consists of two steps -- fast pose matching and robust\nspatial verification. We experimentally show that explicit human pose matching\nis superior to standard content-based image retrieval methods on a manually\nannotated art composition transfer dataset.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 12:06:33 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Jenicek", "Tomas", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1907.03548", "submitter": "Wenguang Yuan", "authors": "Wenguang Yuan, Jia Wei, Jiabing Wang, Qianli Ma, Tolga Tasdizen", "title": "Unified Attentional Generative Adversarial Network for Brain Tumor\n  Segmentation From Multimodal Unpaired Images", "comments": "9 pages, 4 figures, Accepted by MICCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical applications, the same anatomical structures may be observed in\nmultiple modalities despite the different image characteristics. Currently,\nmost deep models for multimodal segmentation rely on paired registered images.\nHowever, multimodal paired registered images are difficult to obtain in many\ncases. Therefore, developing a model that can segment the target objects from\ndifferent modalities with unpaired images is significant for many clinical\napplications. In this work, we propose a novel two-stream translation and\nsegmentation unified attentional generative adversarial network (UAGAN), which\ncan perform any-to-any image modality translation and segment the target\nobjects simultaneously in the case where two or more modalities are available.\nThe translation stream is used to capture modality-invariant features of the\ntarget anatomical structures. In addition, to focus on segmentation-related\nfeatures, we add attentional blocks to extract valuable features from the\ntranslation stream. Experiments on three-modality brain tumor segmentation\nindicate that UAGAN outperforms the existing methods in most cases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 12:12:59 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Yuan", "Wenguang", ""], ["Wei", "Jia", ""], ["Wang", "Jiabing", ""], ["Ma", "Qianli", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1907.03576", "submitter": "Ashis Banerjee", "authors": "Ekta U. Samani, Wei Guo, and Ashis G. Banerjee", "title": "Deep Learning-Based Semantic Segmentation of Microscale Objects", "comments": "A condensed version of the paper is published in the Proceedings of\n  the 2019 International Conference on Manipulation, Automation and Robotics at\n  Small Scales", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of the positions and shapes of microscale objects is\ncrucial for automated imaging-guided manipulation using a non-contact technique\nsuch as optical tweezers. Perception methods that use traditional computer\nvision algorithms tend to fail when the manipulation environments are crowded.\nIn this paper, we present a deep learning model for semantic segmentation of\nthe images representing such environments. Our model successfully performs\nsegmentation with a high mean Intersection Over Union score of 0.91.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 23:07:01 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Samani", "Ekta U.", ""], ["Guo", "Wei", ""], ["Banerjee", "Ashis G.", ""]]}, {"id": "1907.03609", "submitter": "Yulei Niu", "authors": "Yulei Niu, Hanwang Zhang, Zhiwu Lu, Shih-Fu Chang", "title": "Variational Context: Exploiting Visual and Textual Context for Grounding\n  Referring Expressions", "comments": "Accepted as regular paper in IEEE Transactions on Pattern Analysis\n  and Machine Intelligence (TPAMI). Substantial text overlap with\n  arXiv:1712.01892", "journal-ref": null, "doi": "10.1109/TPAMI.2019.2926266", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on grounding (i.e., localizing or linking) referring expressions in\nimages, e.g., ``largest elephant standing behind baby elephant''. This is a\ngeneral yet challenging vision-language task since it does not only require the\nlocalization of objects, but also the multimodal comprehension of context --\nvisual attributes (e.g., ``largest'', ``baby'') and relationships (e.g.,\n``behind'') that help to distinguish the referent from other objects,\nespecially those of the same category. Due to the exponential complexity\ninvolved in modeling the context associated with multiple image regions,\nexisting work oversimplifies this task to pairwise region modeling by multiple\ninstance learning. In this paper, we propose a variational Bayesian method,\ncalled Variational Context, to solve the problem of complex context modeling in\nreferring expression grounding. Specifically, our framework exploits the\nreciprocal relation between the referent and context, i.e., either of them\ninfluences estimation of the posterior distribution of the other, and thereby\nthe search space of context can be greatly reduced. In addition to reciprocity,\nour framework considers the semantic information of context, i.e., the\nreferring expression can be reproduced based on the estimated context. We also\nextend the model to unsupervised setting where no annotation for the referent\nis available. Extensive experiments on various benchmarks show consistent\nimprovement over state-of-the-art methods in both supervised and unsupervised\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 13:37:48 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Niu", "Yulei", ""], ["Zhang", "Hanwang", ""], ["Lu", "Zhiwu", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1907.03644", "submitter": "Kirthi Shankar Sivamani", "authors": "Kirthi Shankar Sivamani", "title": "Unsupervised Domain Alignment to Mitigate Low Level Dataset Biases", "comments": "10 pages, 4 figures, 6 tables, submitted to ICAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataset bias is a well-known problem in the field of computer vision. The\npresence of implicit bias in any image collection hinders a model trained and\nvalidated on a particular dataset to yield similar accuracies when tested on\nother datasets. In this paper, we propose a novel debiasing technique to reduce\nthe effects of a biased training dataset. Our goal is to augment the training\ndata using a generative network by learning a non-linear mapping from the\nsource domain (training set) to the target domain (testing set) while retaining\ntraining set labels. The cycle consistency loss and adversarial loss for\ngenerative adversarial networks are used to learn the mapping. A structured\nsimilarity index (SSIM) loss is used to enforce label retention while\naugmenting the training set. Our methods and hypotheses are supported by\nquantitative comparisons with prior debiasing techniques. These comparisons\nshowcase the superiority of our method and its potential to mitigate the\neffects of dataset bias during the inference stage.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 14:22:54 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 00:35:13 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Sivamani", "Kirthi Shankar", ""]]}, {"id": "1907.03670", "submitter": "Shaoshuai Shi", "authors": "Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li", "title": "From Points to Parts: 3D Object Detection from Point Cloud with\n  Part-aware and Part-aggregation Network", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence 2020, code is available at\n  https://github.com/sshaoshuai/PointCloudDet3D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection from LiDAR point cloud is a challenging problem in 3D\nscene understanding and has many practical applications. In this paper, we\nextend our preliminary work PointRCNN to a novel and strong point-cloud-based\n3D object detection framework, the part-aware and aggregation neural network\n(Part-$A^2$ net). The whole framework consists of the part-aware stage and the\npart-aggregation stage. Firstly, the part-aware stage for the first time fully\nutilizes free-of-charge part supervisions derived from 3D ground-truth boxes to\nsimultaneously predict high quality 3D proposals and accurate intra-object part\nlocations. The predicted intra-object part locations within the same proposal\nare grouped by our new-designed RoI-aware point cloud pooling module, which\nresults in an effective representation to encode the geometry-specific features\nof each 3D proposal. Then the part-aggregation stage learns to re-score the box\nand refine the box location by exploring the spatial relationship of the pooled\nintra-object part locations. Extensive experiments are conducted to demonstrate\nthe performance improvements from each component of our proposed framework. Our\nPart-$A^2$ net outperforms all existing 3D detection methods and achieves new\nstate-of-the-art on KITTI 3D object detection dataset by utilizing only the\nLiDAR point cloud data. Code is available at\nhttps://github.com/sshaoshuai/PointCloudDet3D.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 15:19:48 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 13:56:17 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 04:33:20 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Shi", "Shaoshuai", ""], ["Wang", "Zhe", ""], ["Shi", "Jianping", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "1907.03697", "submitter": "Natalia Efremova", "authors": "Natalia Efremova and Dmitry Zausaev and Gleb Antipov", "title": "Prediction of Soil Moisture Content Based On Satellite Data and\n  Sequence-to-Sequence Networks", "comments": "Presented on NeurIPS 2018 WiML workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of this study is to combine remote sensing and machine\nlearning to detect soil moisture content. Growing population and food\nconsumption has led to the need to improve agricultural yield and to reduce\nwastage of natural resources. In this paper, we propose a neural network\narchitecture, based on recent work by the research community, that can make a\nstrong social impact and aid United Nations Sustainable Development Goal of\nZero Hunger. The main aims here are to: improve efficiency of water usage;\nreduce dependence on irrigation; increase overall crop yield; minimise risk of\ncrop loss due to drought and extreme weather conditions. We achieve this by\napplying satellite imagery, crop segmentation, soil classification and NDVI and\nsoil moisture prediction on satellite data, ground truth and climate data\nrecords. By applying machine learning to sensor data and ground data, farm\nmanagement systems can evolve into a real time AI enabled platform that can\nprovide actionable recommendations and decision support tools to the farmers.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 11:03:17 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Efremova", "Natalia", ""], ["Zausaev", "Dmitry", ""], ["Antipov", "Gleb", ""]]}, {"id": "1907.03698", "submitter": "Ts\\`i-U\\'i \\.Ik", "authors": "Yu-Chuan Huang, I-No Liao, Ching-Hsuan Chen, Ts\\`i-U\\'i \\.Ik, Wen-Chih\n  Peng", "title": "TrackNet: A Deep Learning Network for Tracking High-speed and Tiny\n  Objects in Sports Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ball trajectory data are one of the most fundamental and useful information\nin the evaluation of players' performance and analysis of game strategies.\nAlthough vision-based object tracking techniques have been developed to analyze\nsport competition videos, it is still challenging to recognize and position a\nhigh-speed and tiny ball accurately. In this paper, we develop a deep learning\nnetwork, called TrackNet, to track the tennis ball from broadcast videos in\nwhich the ball images are small, blurry, and sometimes with afterimage tracks\nor even invisible. The proposed heatmap-based deep learning network is trained\nto not only recognize the ball image from a single frame but also learn flying\npatterns from consecutive frames. TrackNet takes images with a size of\n$640\\times360$ to generate a detection heatmap from either a single frame or\nseveral consecutive frames to position the ball and can achieve high precision\neven on public domain videos. The network is evaluated on the video of the\nmen's singles final at the 2017 Summer Universiade, which is available on\nYouTube. The precision, recall, and F1-measure of TrackNet reach $99.7\\%$,\n$97.3\\%$, and $98.5\\%$, respectively. To prevent overfitting, 9 additional\nvideos are partially labeled together with a subset from the previous dataset\nto implement 10-fold cross-validation, and the precision, recall, and\nF1-measure are $95.3\\%$, $75.7\\%$, and $84.3\\%$, respectively. A conventional\nimage processing algorithm is also implemented to compare with TrackNet. Our\nexperiments indicate that TrackNet outperforms conventional method by a big\nmargin and achieves exceptional ball tracking performance. The dataset and demo\nvideo are available at https://nol.cs.nctu.edu.tw/ndo3je6av9/.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 16:08:43 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Huang", "Yu-Chuan", ""], ["Liao", "I-No", ""], ["Chen", "Ching-Hsuan", ""], ["\u0130k", "Ts\u00ec-U\u00ed", ""], ["Peng", "Wen-Chih", ""]]}, {"id": "1907.03728", "submitter": "Ziyue Xu", "authors": "Ziyue Xu, Xiaosong Wang, Hoo-Chang Shin, Dong Yang, Holger Roth,\n  Fausto Milletari, Ling Zhang, Daguang Xu", "title": "Correlation via synthesis: end-to-end nodule image generation and\n  radiogenomic map learning based on generative adversarial network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiogenomic map linking image features and gene expression profiles is\nuseful for noninvasively identifying molecular properties of a particular type\nof disease. Conventionally, such map is produced in three separate steps: 1)\ngene-clustering to \"metagenes\", 2) image feature extraction, and 3) statistical\ncorrelation between metagenes and image features. Each step is independently\nperformed and relies on arbitrary measurements. In this work, we investigate\nthe potential of an end-to-end method fusing gene data with image features to\ngenerate synthetic image and learn radiogenomic map simultaneously. To achieve\nthis goal, we develop a generative adversarial network (GAN) conditioned on\nboth background images and gene expression profiles, synthesizing the\ncorresponding image. Image and gene features are fused at different scales to\nensure the realism and quality of the synthesized image. We tested our method\non non-small cell lung cancer (NSCLC) dataset. Results demonstrate that the\nproposed method produces realistic synthetic images, and provides a promising\nway to find gene-image relationship in a holistic end-to-end manner.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 17:17:18 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Xu", "Ziyue", ""], ["Wang", "Xiaosong", ""], ["Shin", "Hoo-Chang", ""], ["Yang", "Dong", ""], ["Roth", "Holger", ""], ["Milletari", "Fausto", ""], ["Zhang", "Ling", ""], ["Xu", "Daguang", ""]]}, {"id": "1907.03739", "submitter": "Zhijian Liu", "authors": "Zhijian Liu, Haotian Tang, Yujun Lin, Song Han", "title": "Point-Voxel CNN for Efficient 3D Deep Learning", "comments": "NeurIPS 2019. The first two authors contributed equally to this work.\n  Project page: http://pvcnn.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Point-Voxel CNN (PVCNN) for efficient, fast 3D deep learning.\nPrevious work processes 3D data using either voxel-based or point-based NN\nmodels. However, both approaches are computationally inefficient. The\ncomputation cost and memory footprints of the voxel-based models grow cubically\nwith the input resolution, making it memory-prohibitive to scale up the\nresolution. As for point-based networks, up to 80% of the time is wasted on\nstructuring the sparse data which have rather poor memory locality, not on the\nactual feature extraction. In this paper, we propose PVCNN that represents the\n3D input data in points to reduce the memory consumption, while performing the\nconvolutions in voxels to reduce the irregular, sparse data access and improve\nthe locality. Our PVCNN model is both memory and computation efficient.\nEvaluated on semantic and part segmentation datasets, it achieves much higher\naccuracy than the voxel-based baseline with 10x GPU memory reduction; it also\noutperforms the state-of-the-art point-based models with 7x measured speedup on\naverage. Remarkably, the narrower version of PVCNN achieves 2x speedup over\nPointNet (an extremely efficient model) on part and scene segmentation\nbenchmarks with much higher accuracy. We validate the general effectiveness of\nPVCNN on 3D object detection: by replacing the primitives in Frustrum PointNet\nwith PVConv, it outperforms Frustrum PointNet++ by 2.4% mAP on average with\n1.5x measured speedup and GPU memory reduction.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 17:48:45 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 20:46:48 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Liu", "Zhijian", ""], ["Tang", "Haotian", ""], ["Lin", "Yujun", ""], ["Han", "Song", ""]]}, {"id": "1907.03798", "submitter": "Rafal Scherer", "authors": "Patryk Najgebauer, Rafal Scherer, Leszek Rutkowski", "title": "Fully Convolutional Network for Removing DCT Artefacts From Images", "comments": null, "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN),\n  2020, pp. 1-8", "doi": "10.1109/IJCNN48605.2020.9207249", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compression is one of the essential methods of image processing. Its\nmost prominent advantage is the significant reduction of image size allowing\nfor more efficient storage and transfer. However, lossy compression is\nassociated with the loss of some image details in favor of reducing its size.\nIn compressed images, the deficiencies are manifested by noticeable defects in\nthe form of artifacts; the most common are block artifacts, ringing effect, or\nblur. In this article, we propose three models of fully convolutional networks\nwith different configurations and examine their abilities in reducing\ncompression artifacts. In the experiments, we research the extent to which the\nresults are improved for models that will process the image in a similar way to\nthe compression algorithm, and whether the initialization with predefined\nfilters would allow for better image reconstruction than developed solely\nduring learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:31:11 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 10:51:13 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Najgebauer", "Patryk", ""], ["Scherer", "Rafal", ""], ["Rutkowski", "Leszek", ""]]}, {"id": "1907.03799", "submitter": "Vincenzo Lomonaco PhD", "authors": "Vincenzo Lomonaco, Davide Maltoni, Lorenzo Pellegrini", "title": "Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches", "comments": "Accepted in the CLVision Workshop at CVPR2020: 12 pages, 7 figures, 5\n  tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic vision is a field where continual learning can play a significant\nrole. An embodied agent operating in a complex environment subject to frequent\nand unpredictable changes is required to learn and adapt continuously. In the\ncontext of object recognition, for example, a robot should be able to learn\n(without forgetting) objects of never before seen classes as well as improving\nits recognition capabilities as new instances of already known classes are\ndiscovered. Ideally, continual learning should be triggered by the availability\nof short videos of single objects and performed on-line on on-board hardware\nwith fine-grained updates. In this paper, we introduce a novel continual\nlearning protocol based on the CORe50 benchmark and propose two rehearsal-free\ncontinual learning techniques, CWR* and AR1*, that can learn effectively even\nin the challenging case of nearly 400 small non-i.i.d. incremental batches. In\nparticular, our experiments show that AR1* can outperform other\nstate-of-the-art rehearsal-free techniques by more than 15% accuracy in some\ncases, with a very light and constant computational and memory overhead across\ntraining batches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:32:25 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 21:18:49 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 16:13:12 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lomonaco", "Vincenzo", ""], ["Maltoni", "Davide", ""], ["Pellegrini", "Lorenzo", ""]]}, {"id": "1907.03802", "submitter": "Carlos Rodr\\'iguez - Pardo", "authors": "Carlos Rodr\\'iguez - Pardo and Hakan Bilen", "title": "Personalised aesthetics with residual adapters", "comments": "12 pages, 4 figures. In Iberian Conference on Pattern Recognition and\n  Image Analysis proceedings", "journal-ref": null, "doi": "10.1007/978-3-030-31332-6_44", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of computational methods to evaluate aesthetics in photography has\ngained interest in recent years due to the popularization of convolutional\nneural networks and the availability of new annotated datasets. Most studies in\nthis area have focused on designing models that do not take into account\nindividual preferences for the prediction of the aesthetic value of pictures.\nWe propose a model based on residual learning that is capable of learning\nsubjective, user specific preferences over aesthetics in photography, while\nsurpassing the state-of-the-art methods and keeping a limited number of\nuser-specific parameters in the model. Our model can also be used for picture\nenhancement, and it is suitable for content-based or hybrid recommender systems\nin which the amount of computational resources is limited.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:40:16 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Pardo", "Carlos Rodr\u00edguez -", ""], ["Bilen", "Hakan", ""]]}, {"id": "1907.03842", "submitter": "Anastasia Zvezdakova", "authors": "Anastasia Zvezdakova, Dmitriy Kulikov, Denis Kondranin, Dmitriy\n  Vatolin", "title": "Barriers towards no-reference metrics application to compressed video\n  quality analysis: on the example of no-reference metric NIQE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyses the application of no-reference metric NIQE to the task\nof video-codec comparison. A number of issues in the metric behaviour on videos\nwas detected and described. The metric has outlying scores on black and\nsolid-coloured frames. The proposed averaging technique for metric quality\nscores helped to improve the results in some cases. Also, NIQE has low-quality\nscores for videos with detailed textures and higher scores for videos of lower\nbitrates due to the blurring of these textures after compression. Although NIQE\nshowed natural results for many tested videos, it is not universal and\ncurrently can not be used for video-codec comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 20:07:16 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 14:30:22 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zvezdakova", "Anastasia", ""], ["Kulikov", "Dmitriy", ""], ["Kondranin", "Denis", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "1907.03865", "submitter": "Svitlana Alkhimova", "authors": "S.M. Alkhimova, A. P. Krenevych", "title": "Brain Tissues Segmentation on MR Perfusion Images Using CUSUM Filter for\n  Boundary Pixels", "comments": null, "journal-ref": "International Journal of Computing (2019), V. 18, N 2., P. 127-134", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fully automated and relatively accurate method of brain tissues\nsegmentation on T2-weighted magnetic resonance perfusion images is proposed.\nSegmentation with this method provides a possibility to obtain perfusion region\nof interest on images with abnormal brain anatomy that is very important for\nperfusion analysis. In the proposed method the result is presented as a binary\nmask, which marks two regions: brain tissues pixels with unity values and\nskull, extracranial soft tissue and background pixels with zero values. The\nbinary mask is produced based on the location of boundary between two studied\nregions. Each boundary point is detected with CUSUM filter as a change point\nfor iteratively accumulated points at time of moving on a sinusoidal-like path\nalong the boundary from one region to another. The evaluation results for 20\nclinical cases showed that proposed segmentation method could significantly\nreduce the time and efforts required to obtain desirable results for perfusion\nregion of interest detection on T2-weighted magnetic resonance perfusion images\nwith abnormal brain anatomy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 20:46:36 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Alkhimova", "S. M.", ""], ["Krenevych", "A. P.", ""]]}, {"id": "1907.03892", "submitter": "Bao Xin Chen", "authors": "Bao Xin Chen and John K. Tsotsos", "title": "Fast Visual Object Tracking with Rotated Bounding Boxes", "comments": null, "journal-ref": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)\n  Workshop", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate a novel algorithm that uses ellipse fitting to\nestimate the bounding box rotation angle and size with the segmentation(mask)\non the target for online and real-time visual object tracking. Our method,\nSiamMask_E, improves the bounding box fitting procedure of the state-of-the-art\nobject tracking algorithm SiamMask and still retains a fast-tracking frame rate\n(80 fps) on a system equipped with GPU (GeForce GTX 1080 Ti or higher). We\ntested our approach on the visual object tracking datasets (VOT2016, VOT2018,\nand VOT2019) that were labeled with rotated bounding boxes. By comparing with\nthe original SiamMask, we achieved an improved Accuracy of 0.652 and 0.309 EAO\non VOT2019, which is 0.056 and 0.026 higher than the original SiamMask. The\nimplementation is available on GitHub:\nhttps://github.com/baoxinchen/siammask_e.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 22:06:39 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 19:02:35 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 08:45:45 GMT"}, {"version": "v4", "created": "Mon, 2 Sep 2019 01:32:36 GMT"}, {"version": "v5", "created": "Thu, 12 Sep 2019 06:26:54 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Chen", "Bao Xin", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1907.03950", "submitter": "Drew A. Hudson", "authors": "Drew A. Hudson and Christopher D. Manning", "title": "Learning by Abstraction: The Neural State Machine", "comments": "Published as a conference paper at NeurIPS 2019 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Neural State Machine, seeking to bridge the gap between the\nneural and symbolic views of AI and integrate their complementary strengths for\nthe task of visual reasoning. Given an image, we first predict a probabilistic\ngraph that represents its underlying semantics and serves as a structured world\nmodel. Then, we perform sequential reasoning over the graph, iteratively\ntraversing its nodes to answer a given question or draw a new inference. In\ncontrast to most neural architectures that are designed to closely interact\nwith the raw sensory data, our model operates instead in an abstract latent\nspace, by transforming both the visual and linguistic modalities into semantic\nconcept-based representations, thereby achieving enhanced transparency and\nmodularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets\nthat involve compositionality, multi-step inference and diverse reasoning\nskills, achieving state-of-the-art results in both cases. We provide further\nexperiments that illustrate the model's strong generalization capacity across\nmultiple dimensions, including novel compositions of concepts, changes in the\nanswer distribution, and unseen linguistic structures, demonstrating the\nqualities and efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:08:41 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 17:14:03 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 09:33:51 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2019 10:02:05 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Hudson", "Drew A.", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1907.03951", "submitter": "Jiahui Li", "authors": "Jiahui Li and Zhiqiang Hu and Shuang Yang", "title": "Accurate Nuclear Segmentation with Center Vector Encoding", "comments": "Published in The 26th international conference on Information\n  Processing in Medical Imaging (IPMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear segmentation is important and frequently demanded for pathology image\nanalysis, yet is also challenging due to nuclear crowdedness and possible\nocclusion. In this paper, we present a novel bottom-up method for nuclear\nsegmentation. The concepts of Center Mask and Center Vector are introduced to\nbetter depict the relationship between pixels and nuclear instances. The\ninstance differentiation process are thus largely simplified and easier to\nunderstand. Experiments demonstrate the effectiveness of Center Vector\nEncoding, where our method outperforms state-of-the-arts by a clear margin.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:10:23 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 02:05:43 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Li", "Jiahui", ""], ["Hu", "Zhiqiang", ""], ["Yang", "Shuang", ""]]}, {"id": "1907.03954", "submitter": "Jiahui Li", "authors": "Jiahui Li and Shuang Yang and Xiaodi Huang and Qian Da and Xiaoqun\n  Yang and Zhiqiang Hu and Qi Duan and Chaofu Wang and Hongsheng Li", "title": "Signet Ring Cell Detection With a Semi-supervised Learning Framework", "comments": "Published in The 26th international conference on Information\n  Processing in Medical Imaging (IPMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signet ring cell carcinoma is a type of rare adenocarcinoma with poor\nprognosis. Early detection leads to huge improvement of patients' survival\nrate. However, pathologists can only visually detect signet ring cells under\nthe microscope. This procedure is not only laborious but also prone to\nomission. An automatic and accurate signet ring cell detection solution is thus\nimportant but has not been investigated before. In this paper, we take the\nfirst step to present a semi-supervised learning framework for the signet ring\ncell detection problem. Self-training is proposed to deal with the challenge of\nincomplete annotations, and cooperative-training is adapted to explore the\nunlabeled regions. Combining the two techniques, our semi-supervised learning\nframework can make better use of both labeled and unlabeled data. Experiments\non large real clinical data demonstrate the effectiveness of our design. Our\nframework achieves accurate signet ring cell detection and can be readily\napplied in the clinical trails. The dataset will be released soon to facilitate\nthe development of the area.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:14:29 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Li", "Jiahui", ""], ["Yang", "Shuang", ""], ["Huang", "Xiaodi", ""], ["Da", "Qian", ""], ["Yang", "Xiaoqun", ""], ["Hu", "Zhiqiang", ""], ["Duan", "Qi", ""], ["Wang", "Chaofu", ""], ["Li", "Hongsheng", ""]]}, {"id": "1907.03958", "submitter": "Lijun Gong", "authors": "Qingbin Shao, Lijun Gong, Kai Ma, Hualuo Liu, Yefeng Zheng", "title": "Attentive CT Lesion Detection Using Deep Pyramid Inference with\n  Multi-Scale Booster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate lesion detection in computer tomography (CT) slices benefits\npathologic organ analysis in the medical diagnosis process. More recently, it\nhas been tackled as an object detection problem using the Convolutional Neural\nNetworks (CNNs). Despite the achievements from off-the-shelf CNN models, the\ncurrent detection accuracy is limited by the inability of CNNs on lesions at\nvastly different scales. In this paper, we propose a Multi-Scale Booster (MSB)\nwith channel and spatial attention integrated into the backbone Feature Pyramid\nNetwork (FPN). In each pyramid level, the proposed MSB captures fine-grained\nscale variations by using Hierarchically Dilated Convolutions (HDC). Meanwhile,\nthe proposed channel and spatial attention modules increase the network's\ncapability of selecting relevant features response for lesion detection.\nExtensive experiments on the DeepLesion benchmark dataset demonstrate that the\nproposed method performs superiorly against state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:23:32 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Shao", "Qingbin", ""], ["Gong", "Lijun", ""], ["Ma", "Kai", ""], ["Liu", "Hualuo", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1907.03960", "submitter": "Shahira Abousamra", "authors": "Shahira Abousamra, Le Hou, Rajarsi Gupta, Chao Chen, Dimitris Samaras,\n  Tahsin Kurc, Rebecca Batiste, Tianhao Zhao, Shroyer Kenneth, Joel Saltz", "title": "Learning from Thresholds: Fully Automated Classification of Tumor\n  Infiltrating Lymphocytes for Multiple Cancer Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning classifiers for characterization of whole slide tissue\nmorphology require large volumes of annotated data to learn variations across\ndifferent tissue and cancer types. As is well known, manual generation of\ndigital pathology training data is time consuming and expensive. In this paper,\nwe propose a semi-automated method for annotating a group of similar instances\nat once, instead of collecting only per-instance manual annotations. This\nallows for a much larger training set, that reflects visual variability across\nmultiple cancer types and thus training of a single network which can be\nautomatically applied to each cancer type without human adjustment. We apply\nour method to the important task of classifying Tumor Infiltrating Lymphocytes\n(TILs) in H&E images. Prior approaches were trained for individual cancer\ntypes, with smaller training sets and human-in-the-loop threshold adjustment.\nWe utilize these thresholded results as large scale \"semi-automatic\"\nannotations. Combined with existing manual annotations, our trained deep\nnetworks are able to automatically produce better TIL prediction results in 12\ncancer types, compared to the human-in-the-loop approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:25:50 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Abousamra", "Shahira", ""], ["Hou", "Le", ""], ["Gupta", "Rajarsi", ""], ["Chen", "Chao", ""], ["Samaras", "Dimitris", ""], ["Kurc", "Tahsin", ""], ["Batiste", "Rebecca", ""], ["Zhao", "Tianhao", ""], ["Kenneth", "Shroyer", ""], ["Saltz", "Joel", ""]]}, {"id": "1907.03961", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng and Jianren Wang and David Held and Kris Kitani", "title": "3D Multi-Object Tracking: A Baseline and New Evaluation Metrics", "comments": "Accepted at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D multi-object tracking (MOT) is an essential component for many\napplications such as autonomous driving and assistive robotics. Recent work on\n3D MOT focuses on developing accurate systems giving less attention to\npractical considerations such as computational cost and system complexity. In\ncontrast, this work proposes a simple real-time 3D MOT system. Our system first\nobtains 3D detections from a LiDAR point cloud. Then, a straightforward\ncombination of a 3D Kalman filter and the Hungarian algorithm is used for state\nestimation and data association. Additionally, 3D MOT datasets such as KITTI\nevaluate MOT methods in the 2D space and standardized 3D MOT evaluation tools\nare missing for a fair comparison of 3D MOT methods. Therefore, we propose a\nnew 3D MOT evaluation tool along with three new metrics to comprehensively\nevaluate 3D MOT methods. We show that, although our system employs a\ncombination of classical MOT modules, we achieve state-of-the-art 3D MOT\nperformance on two 3D MOT benchmarks (KITTI and nuScenes). Surprisingly,\nalthough our system does not use any 2D data as inputs, we achieve competitive\nperformance on the KITTI 2D MOT leaderboard. Our proposed system runs at a rate\nof $207.4$ FPS on the KITTI dataset, achieving the fastest speed among all\nmodern MOT systems. To encourage standardized 3D MOT evaluation, our system and\nevaluation code are made publicly available at\nhttps://github.com/xinshuoweng/AB3DMOT.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:26:21 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 03:19:28 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2019 01:20:59 GMT"}, {"version": "v4", "created": "Sat, 4 Jul 2020 19:50:27 GMT"}, {"version": "v5", "created": "Wed, 22 Jul 2020 03:22:51 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Weng", "Xinshuo", ""], ["Wang", "Jianren", ""], ["Held", "David", ""], ["Kitani", "Kris", ""]]}, {"id": "1907.03965", "submitter": "Hugo Germain", "authors": "Hugo Germain and Guillaume Bourmaud and Vincent Lepetit", "title": "Sparse-to-Dense Hypercolumn Matching for Long-Term Visual Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to feature point matching, suitable for robust\nand accurate outdoor visual localization in long-term scenarios. Given a query\nimage, we first match it against a database of registered reference images,\nusing recent retrieval techniques. This gives us a first estimate of the camera\npose. To refine this estimate, like previous approaches, we match 2D points\nacross the query image and the retrieved reference image. This step, however,\nis prone to fail as it is still very difficult to detect and match sparse\nfeature points across images captured in potentially very different conditions.\nOur key contribution is to show that we need to extract sparse feature points\nonly in the retrieved reference image: We then search for the corresponding 2D\nlocations in the query image exhaustively. This search can be performed\nefficiently using convolutional operations, and robustly by using hypercolumn\ndescriptors, i.e. image features computed for retrieval. We refer to this\nmethod as Sparse-to-Dense Hypercolumn Matching. Because we know the 3D\nlocations of the sparse feature points in the reference images thanks to an\noffline reconstruction stage, it is then possible to accurately estimate the\ncamera pose from these matches. Our experiments show that this method allows us\nto outperform the state-of-the-art on several challenging outdoor datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:32:23 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 03:44:33 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Germain", "Hugo", ""], ["Bourmaud", "Guillaume", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1907.03967", "submitter": "Abed Malti", "authors": "Abed Malti", "title": "On the Exact Recovery Conditions of 3D Human Motion from 2D Landmark\n  Motion with Sparse Articulated Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of exact recovery condition in\nretrieving 3D human motion from 2D landmark motion. We use a skeletal kinematic\nmodel to represent the 3D human motion as a vector of angular articulation\nmotion. We address this problem based on the observation that at high tracking\nrate, regardless of the global rigid motion, only few angular articulations\nhave non-zero motion. We propose a first ideal formulation with $\\ell_0$-norm\nto minimize the cardinal of non-zero angular articulation motion given an\nequality constraint on the time-differentiation of the reprojection error. The\nsecond relaxed formulation relies on an $\\ell_1$-norm to minimize the sum of\nabsolute values of the angular articulation motion. This formulation has the\nadvantage of being able to provide 3D motion even in the under-determined case\nwhen twice the number of 2D landmarks is smaller than the number of angular\narticulations. We define a specific property which is the Projective Kinematic\nSpace Property (PKSP) that takes into account the reprojection constraint and\nthe kinematic model. We prove that for the relaxed formulation we are able to\nrecover the exact 3D human motion from 2D landmarks if and only if the PKSP\nproperty is verified. We further demonstrate that solving the relaxed\nformulation provides the same ground-truth solution as the ideal formulation if\nand only if the PKSP condition is filled. Results with simulated sparse\nskeletal angular motion show the ability of the proposed method to recover\nexact location of angular motion. We provide results on publicly available real\ndata (HUMAN3.6M, PANOPTIC and MPI-I3DHP).\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:34:20 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Malti", "Abed", ""]]}, {"id": "1907.04011", "submitter": "Peter Hviid Christiansen", "authors": "Peter Hviid Christiansen, Mikkel Fly Kragh, Yury Brodskiy and Henrik\n  Karstoft", "title": "UnsuperPoint: End-to-end Unsupervised Interest Point Detector and\n  Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is hard to create consistent ground truth data for interest points in\nnatural images, since interest points are hard to define clearly and\nconsistently for a human annotator. This makes interest point detectors\nnon-trivial to build. In this work, we introduce an unsupervised deep\nlearning-based interest point detector and descriptor. Using a self-supervised\napproach, we utilize a siamese network and a novel loss function that enables\ninterest point scores and positions to be learned automatically. The resulting\ninterest point detector and descriptor is UnsuperPoint. We use regression of\npoint positions to 1) make UnsuperPoint end-to-end trainable and 2) to\nincorporate non-maximum suppression in the model. Unlike most trainable\ndetectors, it requires no generation of pseudo ground truth points, no\nstructure-from-motion-generated representations and the model is learned from\nonly one round of training. Furthermore, we introduce a novel loss function to\nregularize network predictions to be uniformly distributed. UnsuperPoint runs\nin real-time with 323 frames per second (fps) at a resolution of $224\\times320$\nand 90 fps at $480\\times640$. It is comparable or better than state-of-the-art\nperformance when measured for speed, repeatability, localization, matching\nscore and homography estimation on the HPatch dataset.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:50:13 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Christiansen", "Peter Hviid", ""], ["Kragh", "Mikkel Fly", ""], ["Brodskiy", "Yury", ""], ["Karstoft", "Henrik", ""]]}, {"id": "1907.04041", "submitter": "Benjamin Kiessling", "authors": "Benjamin Kiessling, Daniel St\\\"okl Ben Ezra, Matthew Thomas Miller", "title": "BADAM: A Public Dataset for Baseline Detection in Arabic-script\n  Manuscripts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The application of handwritten text recognition to historical works is highly\ndependant on accurate text line retrieval. A number of systems utilizing a\nrobust baseline detection paradigm have emerged recently but the advancement of\nlayout analysis methods for challenging scripts is held back by the lack of\nwell-established datasets including works in non-Latin scripts. We present a\ndataset of 400 annotated document images from different domains and time\nperiods. A short elaboration on the particular challenges posed by handwriting\nin Arabic script for layout analysis and subsequent processing steps is given.\nLastly, we propose a method based on a fully convolutional encoder-decoder\nnetwork to extract arbitrarily shaped text line images from manuscripts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 08:30:56 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kiessling", "Benjamin", ""], ["Ezra", "Daniel St\u00f6kl Ben", ""], ["Miller", "Matthew Thomas", ""]]}, {"id": "1907.04047", "submitter": "Anjith George", "authors": "Anjith George and Sebastien Marcel", "title": "Deep Pixel-wise Binary Supervision for Face Presentation Attack\n  Detection", "comments": "8 pages, 5 figures, To appear in : International Conference on\n  Biometrics, ICB 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has evolved as a prominent biometric authentication\nmodality. However, vulnerability to presentation attacks curtails its reliable\ndeployment. Automatic detection of presentation attacks is essential for secure\nuse of face recognition technology in unattended scenarios. In this work, we\nintroduce a Convolutional Neural Network (CNN) based framework for presentation\nattack detection, with deep pixel-wise supervision. The framework uses only\nframe level information making it suitable for deployment in smart devices with\nminimal computational and time overhead. We demonstrate the effectiveness of\nthe proposed approach in public datasets for both intra as well as\ncross-dataset experiments. The proposed approach achieves an HTER of 0% in\nReplay Mobile dataset and an ACER of 0.42% in Protocol-1 of OULU dataset\noutperforming state of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 08:45:33 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["George", "Anjith", ""], ["Marcel", "Sebastien", ""]]}, {"id": "1907.04048", "submitter": "Anjith George", "authors": "Olegs Nikisins, Anjith George, Sebastien Marcel", "title": "Domain Adaptation in Multi-Channel Autoencoder based Features for Robust\n  Face Anti-Spoofing", "comments": "8 pages, 7 figures, To appear in International Conference on\n  Biometrics, ICB 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the performance of face recognition systems has improved significantly\nin the last decade, they are proved to be highly vulnerable to presentation\nattacks (spoofing). Most of the research in the field of face presentation\nattack detection (PAD), was focused on boosting the performance of the systems\nwithin a single database. Face PAD datasets are usually captured with RGB\ncameras, and have very limited number of both bona-fide samples and\npresentation attack instruments. Training face PAD systems on such data leads\nto poor performance, even in the closed-set scenario, especially when\nsophisticated attacks are involved.\n  We explore two paths to boost the performance of the face PAD system against\nchallenging attacks. First, by using multi-channel (RGB, Depth and NIR) data,\nwhich is still easily accessible in a number of mass production devices.\nSecond, we develop a novel Autoencoders + MLP based face PAD algorithm.\nMoreover, instead of collecting more data for training of the proposed deep\narchitecture, the domain adaptation technique is proposed, transferring the\nknowledge of facial appearance from RGB to multi-channel domain. We also\ndemonstrate, that learning the features of individual facial regions, is more\ndiscriminative than the features learned from an entire face. The proposed\nsystem is tested on a very recent publicly available multi-channel PAD database\nwith a wide variety of presentation attacks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 08:53:32 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Nikisins", "Olegs", ""], ["George", "Anjith", ""], ["Marcel", "Sebastien", ""]]}, {"id": "1907.04052", "submitter": "Qingyi Tao", "authors": "Qingyi Tao, Zongyuan Ge, Jianfei Cai, Jianxiong Yin, Simon See", "title": "Improving Deep Lesion Detection Using 3D Contextual and Spatial\n  Attention", "comments": "Accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion detection from computed tomography (CT) scans is challenging compared\nto natural object detection because of two major reasons: small lesion size and\nsmall inter-class variation. Firstly, the lesions usually only occupy a small\nregion in the CT image. The feature of such small region may not be able to\nprovide sufficient information due to its limited spatial feature resolution.\nSecondly, in CT scans, the lesions are often indistinguishable from the\nbackground since the lesion and non-lesion areas may have very similar\nappearances. To tackle both problems, we need to enrich the feature\nrepresentation and improve the feature discriminativeness. Therefore, we\nintroduce a dual-attention mechanism to the 3D contextual lesion detection\nframework, including the cross-slice contextual attention to selectively\naggregate the information from different slices through a soft re-sampling\nprocess. Moreover, we propose intra-slice spatial attention to focus the\nfeature learning in the most prominent regions. Our method can be easily\ntrained end-to-end without adding heavy overhead on the base detection network.\nWe use DeepLesion dataset and train a universal lesion detector to detect all\nkinds of lesions such as liver tumors, lung nodules, and so on. The results\nshow that our model can significantly boost the results of the baseline lesion\ndetector (with 3D contextual information) but using much fewer slices.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 09:19:51 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Tao", "Qingyi", ""], ["Ge", "Zongyuan", ""], ["Cai", "Jianfei", ""], ["Yin", "Jianxiong", ""], ["See", "Simon", ""]]}, {"id": "1907.04057", "submitter": "Hao Fu", "authors": "Xiaoxiang Zhang, Hao Fu, Bin Dai", "title": "Lidar-based Object Classification with Explicit Occlusion Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LIDAR is one of the most important sensors for Unmanned Ground Vehicles\n(UGV). Object detection and classification based on lidar point cloud is a key\ntechnology for UGV. In object detection and classification, the mutual\nocclusion between neighboring objects is an important factor affecting the\naccuracy. In this paper, we consider occlusion as an intrinsic property of the\npoint cloud data. We propose a novel approach that explicitly model the\nocclusion. The occlusion property is then taken into account in the subsequent\nclassification step. We perform experiments on the KITTI dataset. Experimental\nresults indicate that by utilizing the occlusion property that we modeled, the\nclassifier obtains much better performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 09:46:27 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 00:56:47 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Zhang", "Xiaoxiang", ""], ["Fu", "Hao", ""], ["Dai", "Bin", ""]]}, {"id": "1907.04058", "submitter": "Peter Fasogbon O.", "authors": "Peter O. Fasogbon", "title": "Depth from Small Motion using Rank-1 Initialization", "comments": "8 pages, 6 figures", "journal-ref": "14th International Conference on Computer Vision Theory and\n  Applications, February 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depth from Small Motion (DfSM) (Ha et al., 2016) is particularly interesting\nfor commercial handheld devices because it allows the possibility to get depth\ninformation with minimal user effort and cooperation. Due to speed and memory\nissue on these devices, the self calibration optimization of the method using\nBundle Adjustment (BA) need as little as 10-15 images. Therefore, the\noptimization tends to take many iterations to converge or may not converge at\nall in some cases. This work propose a robust initialization for the bundle\nadjustment using the rank-1 factorization method (Tomasi and Kanade, 1992),\n(Aguiar and Moura, 1999a). We create a constraint matrix that is rank-1 in a\nnoiseless situation, then use SVD to compute the inverse depth values and the\ncamera motion. We only need about quarter fraction of the bundle adjustment\niteration to converge. We also propose grided feature extraction technique so\nthat only important and small features are tracked all over the image frames.\nThis also ensure speedup in the full execution time on the mobile device. For\nthe experiments, we have documented the execution time with the proposed Rank-1\ninitialization on two mobile device platforms using optimized accelerations\nwith CPU-GPU co-processing. The combination of Rank 1-BA generates more robust\ndepth-map and is significantly faster than using BA alone.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 09:50:04 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Fasogbon", "Peter O.", ""]]}, {"id": "1907.04061", "submitter": "Prerana Mukherjee", "authors": "Chandra Sekhar V., Anoushka Doctor, Prerana Mukherjee, Viswanath\n  Pulabaigiri", "title": "A Light weight and Hybrid Deep Learning Model based Online Signature\n  Verification", "comments": "accepted in ICDAR-WML: The 2nd International Workshop on Machine\n  Learning 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The augmented usage of deep learning-based models for various AI related\nproblems are as a result of modern architectures of deeper length and the\navailability of voluminous interpreted datasets. The models based on these\narchitectures require huge training and storage cost, which makes them\ninefficient to use in critical applications like online signature verification\n(OSV) and to deploy in resource constraint devices. As a solution, in this\nwork, our contribution is two-fold. 1) An efficient dimensionality reduction\ntechnique, to reduce the number of features to be considered and 2) a\nstate-of-the-art model CNN-LSTM based hybrid architecture for online signature\nverification. Thorough experiments on the publicly available datasets MCYT,\nSUSIG, SVC confirms that the proposed model achieves better accuracy even with\nas low as one training sample. The proposed models yield state-of-the-art\nperformance in various categories of all the three datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 09:55:38 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["V.", "Chandra Sekhar", ""], ["Doctor", "Anoushka", ""], ["Mukherjee", "Prerana", ""], ["Pulabaigiri", "Viswanath", ""]]}, {"id": "1907.04064", "submitter": "Jens Petersen", "authors": "Jens Petersen, Paul F. J\\\"ager, Fabian Isensee, Simon A. A. Kohl, Ulf\n  Neuberger, Wolfgang Wick, J\\\"urgen Debus, Sabine Heiland, Martin Bendszus,\n  Philipp Kickingereder, Klaus H. Maier-Hein", "title": "Deep Probabilistic Modeling of Glioma Growth", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to modeling the dynamics of brain tumor growth,\nspecifically glioma, employ biologically inspired models of cell diffusion,\nusing image data to estimate the associated parameters. In this work, we\npropose an alternative approach based on recent advances in probabilistic\nsegmentation and representation learning that implicitly learns growth dynamics\ndirectly from data without an underlying explicit model. We present evidence\nthat our approach is able to learn a distribution of plausible future tumor\nappearances conditioned on past observations of the same tumor.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 10:00:33 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Petersen", "Jens", ""], ["J\u00e4ger", "Paul F.", ""], ["Isensee", "Fabian", ""], ["Kohl", "Simon A. A.", ""], ["Neuberger", "Ulf", ""], ["Wick", "Wolfgang", ""], ["Debus", "J\u00fcrgen", ""], ["Heiland", "Sabine", ""], ["Bendszus", "Martin", ""], ["Kickingereder", "Philipp", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1907.04091", "submitter": "Alberto Antonio Del Barrio", "authors": "Ra\\'ul Murillo Montero, Alberto A. Del Barrio, Guillermo Botella", "title": "Template-Based Posit Multiplication for Training and Inferring in Neural\n  Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The posit number system is arguably the most promising and discussed topic in\nArithmetic nowadays. The recent breakthroughs claimed by the format proposed by\nJohn L. Gustafson have put posits in the spotlight. In this work, we first\ndescribe an algorithm for multiplying two posit numbers, even when the number\nof exponent bits is zero. This configuration, scarcely tackled in literature,\nis particularly interesting because it allows the deployment of a fast sigmoid\nfunction. The proposed multiplication algorithm is then integrated as a\ntemplate into the well-known FloPoCo framework. Synthesis results are shown to\ncompare with the floating point multiplication offered by FloPoCo as well.\nSecond, the performance of posits is studied in the scenario of Neural Networks\nin both training and inference stages. To the best of our knowledge, this is\nthe first time that training is done with posit format, achieving promising\nresults for a binary classification problem even with reduced posit\nconfigurations. In the inference stage, 8-bit posits are as good as floating\npoint when dealing with the MNIST dataset, but lose some accuracy with\nCIFAR-10.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:36:19 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Montero", "Ra\u00fal Murillo", ""], ["Del Barrio", "Alberto A.", ""], ["Botella", "Guillermo", ""]]}, {"id": "1907.04096", "submitter": "Pavel Rojtberg", "authors": "Pavel Rojtberg, Arjan Kuijper", "title": "Efficient Pose Selection for Interactive Camera Calibration", "comments": null, "journal-ref": null, "doi": "10.1109/ISMAR.2018.00026", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The choice of poses for camera calibration with planar patterns is only\nrarely considered - yet the calibration precision heavily depends on it. This\nwork presents a pose selection method that finds a compact and robust set of\ncalibration poses and is suitable for interactive calibration. Consequently,\nsingular poses that would lead to an unreliable solution are avoided\nexplicitly, while poses reducing the uncertainty of the calibration are\nfavoured. For this, we use uncertainty propagation.\n  Our method takes advantage of a self-identifying calibration pattern to track\nthe camera pose in real-time. This allows to iteratively guide the user to the\ntarget poses, until the desired quality level is reached. Therefore, only a\nsparse set of key-frames is needed for calibration.\n  The method is evaluated on separate training and testing sets, as well as on\nsynthetic data. Our approach performs better than comparable solutions while\nrequiring 30% less calibration frames.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:44:40 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Rojtberg", "Pavel", ""], ["Kuijper", "Arjan", ""]]}, {"id": "1907.04100", "submitter": "Pavel Rojtberg", "authors": "Pavel Rojtberg, Felix Gorschl\\\"uter", "title": "calibDB: enabling web based computer vision through on-the-fly camera\n  calibration", "comments": null, "journal-ref": null, "doi": "10.1145/3329714.3338132", "report-no": null, "categories": "cs.CV cs.NI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For many computer vision applications, the availability of camera calibration\ndata is crucial as overall quality heavily depends on it. While calibration\ndata is available on some devices through Augmented Reality (AR) frameworks\nlike ARCore and ARKit, for most cameras this information is not available.\nTherefore, we propose a web based calibration service that not only aggregates\ncalibration data, but also allows calibrating new cameras on-the-fly. We build\nupon a novel camera calibration framework that enables even novice users to\nperform a precise camera calibration in about 2 minutes. This allows general\ndeployment of computer vision algorithms on the web, which was previously not\npossible due to lack of calibration data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:51:44 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 12:30:15 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Rojtberg", "Pavel", ""], ["Gorschl\u00fcter", "Felix", ""]]}, {"id": "1907.04102", "submitter": "Christian Wachinger", "authors": "Christian Wachinger, Benjamin Gutierrez Becker, Anna Rieckmann,\n  Sebastian P\\\"olsterl", "title": "Quantifying Confounding Bias in Neuroimaging Datasets with Causal\n  Inference", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging datasets keep growing in size to address increasingly complex\nmedical questions. However, even the largest datasets today alone are too small\nfor training complex machine learning models. A potential solution is to\nincrease sample size by pooling scans from several datasets. In this work, we\ncombine 12,207 MRI scans from 15 studies and show that simple pooling is often\nill-advised due to introducing various types of biases in the training data.\nFirst, we systematically define these biases. Second, we detect bias by\nexperimentally showing that scans can be correctly assigned to their respective\ndataset with 73.3% accuracy. Finally, we propose to tell causal from\nconfounding factors by quantifying the extent of confounding and causality in a\nsingle dataset using causal inference. We achieve this by finding the simplest\ngraphical model in terms of Kolmogorov complexity. As Kolmogorov complexity is\nnot directly computable, we employ the minimum description length to\napproximate it. We empirically show that our approach is able to estimate\nplausible causal relationships from real neuroimaging data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:57:22 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Wachinger", "Christian", ""], ["Becker", "Benjamin Gutierrez", ""], ["Rieckmann", "Anna", ""], ["P\u00f6lsterl", "Sebastian", ""]]}, {"id": "1907.04124", "submitter": "Ahmadreza Mahmoudzadeh", "authors": "Ahmadreza Mahmoudzadeh, Sayna Firoozi Yeganeh, Amir Golroo", "title": "3D pavement surface reconstruction using an RGB-D sensor", "comments": "5 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core procedure of pavement management systems is data collection. The\nmodern technologies which are used for this purpose, such as point-based lasers\nand laser scanners, are too expensive to purchase, operate, and maintain. Thus,\nit is rarely feasible for city officials in developing countries to conduct\ndata collection using these devices. This paper aims to introduce a\ncost-effective technology which can be used for pavement distress data\ncollection and 3D pavement surface reconstruction. The applied technology in\nthis research is the Kinect sensor which is not only cost-effective but also\nsufficiently precise. The Kinect sensor can register both depth and color\nimages simultaneously. A cart is designed to mount an array of Kinect sensors.\nThe cameras are calibrated and the slopes of collected surfaces are corrected\nvia the Singular Value Decomposition (SVD) algorithm. Then, a procedure is\nproposed for stitching the RGB-D (Red Green Blue Depth) images using SURF\n(Speeded-up Robust Features) and MSAC (M-estimator SAmple Consensus) algorithms\nin order to create a 3D-structure of the pavement surface. Finally, transverse\nprofiles are extracted and some field experiments are conducted to evaluate the\nreliability of the proposed approach for detecting pavement surface defects.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:02:27 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 02:59:13 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Mahmoudzadeh", "Ahmadreza", ""], ["Yeganeh", "Sayna Firoozi", ""], ["Golroo", "Amir", ""]]}, {"id": "1907.04150", "submitter": "Chong Peng", "authors": "Chong Peng, Zhao Kang, Chenglizhao Chen, and Qiang Cheng", "title": "Nonnegative Matrix Factorization with Local Similarity Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing nonnegative matrix factorization methods focus on learning global\nstructure of the data to construct basis and coefficient matrices, which\nignores the local structure that commonly exists among data. In this paper, we\npropose a new type of nonnegative matrix factorization method, which learns\nlocal similarity and clustering in a mutually enhancing way. The learned new\nrepresentation is more representative in that it better reveals inherent\ngeometric property of the data. Nonlinear expansion is given and efficient\nmultiplicative updates are developed with theoretical convergence guarantees.\nExtensive experimental results have confirmed the effectiveness of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:25:50 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Peng", "Chong", ""], ["Kang", "Zhao", ""], ["Chen", "Chenglizhao", ""], ["Cheng", "Qiang", ""]]}, {"id": "1907.04160", "submitter": "Ninad Joshi", "authors": "N. Joshi", "title": "Learning in Competitive Network with Haeusslers Equation adapted using\n  FIREFLY algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the competitive neural network consists of spatially arranged\nneurons. The weigh matrix that connects cells represents local excitation and\nlong-range inhibition. They are known as soft-winner-take-all networks and\nshown to exhibit desirable information-processing. The local excitatory\nconnections are many times predefined hand-wired based depending on spatial\narrangement which is chosen using the previous knowledge of data. Here we\npresent learning in recurrent network through Haeusslers equation and modified\nwiring scheme based on biologically based Firefly algorithm. Following results\nshow learning in such network from input patterns without hand-wiring with\nfixed topology.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:40:14 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Joshi", "N.", ""]]}, {"id": "1907.04194", "submitter": "Yuhang Ding", "authors": "Yuhang Ding, Hehe Fan, Mingliang Xu and Yi Yang", "title": "Adaptive Exploration for Unsupervised Person Re-Identification", "comments": "ACM Transactions on Multimedia Computing, Communications and\n  Application (TOMCCAP)", "journal-ref": null, "doi": "10.1145/3369393", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to domain bias, directly deploying a deep person re-identification\n(re-ID) model trained on one dataset often achieves considerably poor accuracy\non another dataset. In this paper, we propose an Adaptive Exploration (AE)\nmethod to address the domain-shift problem for re-ID in an unsupervised manner.\nSpecifically, in the target domain, the re-ID model is inducted to 1) maximize\ndistances between all person images and 2) minimize distances between similar\nperson images. In the first case, by treating each person image as an\nindividual class, a non-parametric classifier with a feature memory is\nexploited to encourage person images to move far away from each other. In the\nsecond case, according to a similarity threshold, our method adaptively selects\nneighborhoods for each person image in the feature space. By treating these\nsimilar person images as the same class, the non-parametric classifier forces\nthem to stay closer. However, a problem of the adaptive selection is that, when\nan image has too many neighborhoods, it is more likely to attract other images\nas its neighborhoods. As a result, a minority of images may select a large\nnumber of neighborhoods while a majority of images have only a few\nneighborhoods. To address this issue, we additionally integrate a balance\nstrategy into the adaptive selection. We evaluate our methods with two\nprotocols. The first one is called \"target-only re-ID\", in which only the\nunlabeled target data is used for training. The second one is called \"domain\nadaptive re-ID\", in which both the source data and the target data are used\nduring training. Experimental results on large-scale re-ID datasets demonstrate\nthe effectiveness of our method. Our code has been released at\nhttps://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 14:36:08 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 05:25:45 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Ding", "Yuhang", ""], ["Fan", "Hehe", ""], ["Xu", "Mingliang", ""], ["Yang", "Yi", ""]]}, {"id": "1907.04253", "submitter": "Zhen Li", "authors": "Qilei Li, Zhen Li, Lu Lu, Gwanggil Jeon, Kai Liu, Xiaomin Yang", "title": "Gated Multiple Feedback Network for Image Super-Resolution", "comments": "Accepted to BMVC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of deep learning (DL) has driven single image\nsuper-resolution (SR) into a new era. However, in most existing DL based image\nSR networks, the information flows are solely feedforward, and the high-level\nfeatures cannot be fully explored. In this paper, we propose the gated multiple\nfeedback network (GMFN) for accurate image SR, in which the representation of\nlow-level features are efficiently enriched by rerouting multiple high-level\nfeatures. We cascade multiple residual dense blocks (RDBs) and recurrently\nunfolds them across time. The multiple feedback connections between two\nadjacent time steps in the proposed GMFN exploits multiple high-level features\ncaptured under large receptive fields to refine the low-level features lacking\nenough contextual information. The elaborately designed gated feedback module\n(GFM) efficiently selects and further enhances useful information from multiple\nrerouted high-level features, and then refine the low-level features with the\nenhanced high-level information. Extensive experiments demonstrate the\nsuperiority of our proposed GMFN against state-of-the-art SR methods in terms\nof both quantitative metrics and visual quality. Code is available at\nhttps://github.com/liqilei/GMFN.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 15:35:04 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 10:58:34 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Li", "Qilei", ""], ["Li", "Zhen", ""], ["Lu", "Lu", ""], ["Jeon", "Gwanggil", ""], ["Liu", "Kai", ""], ["Yang", "Xiaomin", ""]]}, {"id": "1907.04298", "submitter": "Pedro F. Proen\\c{c}a", "authors": "Pedro F. Proenca and Yang Gao", "title": "Deep Learning for Spacecraft Pose Estimation from Photorealistic\n  Rendering", "comments": "* Adding more related work and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-orbit proximity operations in space rendezvous, docking and debris removal\nrequire precise and robust 6D pose estimation under a wide range of lighting\nconditions and against highly textured background, i.e., the Earth. This paper\ninvestigates leveraging deep learning and photorealistic rendering for\nmonocular pose estimation of known uncooperative spacecrafts. We first present\na simulator built on Unreal Engine 4, named URSO, to generate labeled images of\nspacecrafts orbiting the Earth, which can be used to train and evaluate neural\nnetworks. Secondly, we propose a deep learning framework for pose estimation\nbased on orientation soft classification, which allows modelling orientation\nambiguity as a mixture of Gaussians. This framework was evaluated both on URSO\ndatasets and the ESA pose estimation challenge. In this competition, our best\nmodel achieved 3rd place on the synthetic test set and 2nd place on the real\ntest set. Moreover, our results show the impact of several architectural and\ntraining aspects, and we demonstrate qualitatively how models learned on URSO\ndatasets can perform on real images from space.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 17:32:38 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 16:03:11 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Proenca", "Pedro F.", ""], ["Gao", "Yang", ""]]}, {"id": "1907.04305", "submitter": "Md. Kamrul Hasan", "authors": "Md. Kamrul Hasan, Lavsen Dahal, Prasad N. Samarakoon, Fakrul Islam\n  Tushar, Robert Marti Marly", "title": "DSNet: Automatic Dermoscopic Skin Lesion Segmentation", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic segmentation of skin lesion is considered a crucial step in\nComputer Aided Diagnosis (CAD) for melanoma diagnosis. Despite its\nsignificance, skin lesion segmentation remains a challenging task due to their\ndiverse color, texture, and indistinguishable boundaries and forms an open\nproblem. Through this study, we present a new and automatic semantic\nsegmentation network for robust skin lesion segmentation named Dermoscopic Skin\nNetwork (DSNet). In order to reduce the number of parameters to make the\nnetwork lightweight, we used depth-wise separable convolution in lieu of\nstandard convolution to project the learned discriminating features onto the\npixel space at different stages of the encoder. Additionally, we implemented\nU-Net and Fully Convolutional Network (FCN8s) to compare against the proposed\nDSNet. We evaluate our proposed model on two publicly available datasets,\nnamely ISIC-2017 and PH2. The obtained mean Intersection over Union (mIoU) is\n77.5 % and 87.0 % respectively for ISIC-2017 and PH2 datasets which\noutperformed the ISIC-2017 challenge winner by 1.0 % with respect to mIoU. Our\nproposed network also outperformed U-Net and FCN8s respectively by 3.6 % and\n6.8 % with respect to mIoU on the ISIC-2017 dataset. Our network for skin\nlesion segmentation outperforms other methods and can provide better segmented\nmasks on two different test datasets which can lead to better performance in\nmelanoma detection. Our trained model along with the source code and predicted\nmasks are made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 17:42:51 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 22:06:47 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Hasan", "Md. Kamrul", ""], ["Dahal", "Lavsen", ""], ["Samarakoon", "Prasad N.", ""], ["Tushar", "Fakrul Islam", ""], ["Marly", "Robert Marti", ""]]}, {"id": "1907.04312", "submitter": "Boyi Li", "authors": "Boyi Li and Felix Wu and Kilian Q. Weinberger and Serge Belongie", "title": "Positional Normalization", "comments": "Accepted to NeurIPS 2019 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular method to reduce the training time of deep neural networks is to\nnormalize activations at each layer. Although various normalization schemes\nhave been proposed, they all follow a common theme: normalize across spatial\ndimensions and discard the extracted statistics. In this paper, we propose an\nalternative normalization method that noticeably departs from this convention\nand normalizes exclusively across channels. We argue that the channel dimension\nis naturally appealing as it allows us to extract the first and second moments\nof features extracted at a particular image position. These moments capture\nstructural information about the input image and extracted features, which\nopens a new avenue along which a network can benefit from feature\nnormalization: Instead of disregarding the normalization constants, we propose\nto re-inject them into later layers to preserve or transfer structural\ninformation in generative networks. Codes are available at\nhttps://github.com/Boyiliee/PONO.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 17:52:01 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 18:58:04 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Li", "Boyi", ""], ["Wu", "Felix", ""], ["Weinberger", "Kilian Q.", ""], ["Belongie", "Serge", ""]]}, {"id": "1907.04325", "submitter": "Anjith George", "authors": "Anjith George", "title": "Image based Eye Gaze Tracking and its Applications", "comments": "177 pages, PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements play a vital role in perceiving the world. Eye gaze can give a\ndirect indication of the users point of attention, which can be useful in\nimproving human-computer interaction. Gaze estimation in a non-intrusive manner\ncan make human-computer interaction more natural. Eye tracking can be used for\nseveral applications such as fatigue detection, biometric authentication,\ndisease diagnosis, activity recognition, alertness level estimation,\ngaze-contingent display, human-computer interaction, etc. Even though\neye-tracking technology has been around for many decades, it has not found much\nuse in consumer applications. The main reasons are the high cost of eye\ntracking hardware and lack of consumer level applications. In this work, we\nattempt to address these two issues. In the first part of this work,\nimage-based algorithms are developed for gaze tracking which includes a new\ntwo-stage iris center localization algorithm. We have developed a new algorithm\nwhich works in challenging conditions such as motion blur, glint, and varying\nillumination levels. A person independent gaze direction classification\nframework using a convolutional neural network is also developed which\neliminates the requirement of user-specific calibration.\n  In the second part of this work, we have developed two applications which can\nbenefit from eye tracking data. A new framework for biometric identification\nbased on eye movement parameters is developed. A framework for activity\nrecognition, using gaze data from a head-mounted eye tracker is also developed.\nThe information from gaze data, ego-motion, and visual features are integrated\nto classify the activities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 15:54:49 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["George", "Anjith", ""]]}, {"id": "1907.04360", "submitter": "Michael Burke Dr", "authors": "Michael Burke and Yordan Hristov and Subramanian Ramamoorthy", "title": "Hybrid system identification using switching density networks", "comments": null, "journal-ref": "Conference on Robot Learning (CoRL 2019)", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behaviour cloning is a commonly used strategy for imitation learning and can\nbe extremely effective in constrained domains. However, in cases where the\ndynamics of an environment may be state dependent and varying, behaviour\ncloning places a burden on model capacity and the number of demonstrations\nrequired. This paper introduces switching density networks, which rely on a\ncategorical reparametrisation for hybrid system identification. This results in\na network comprising a classification layer that is followed by a regression\nlayer. We use switching density networks to predict the parameters of hybrid\ncontrol laws, which are toggled by a switching layer to produce different\ncontroller outputs, when conditioned on an input state. This work shows how\nswitching density networks can be used for hybrid system identification in a\nvariety of tasks, successfully identifying the key joint angle goals that make\nup manipulation tasks, while simultaneously learning image-based goal\nclassifiers and regression networks that predict joint angles from images. We\nalso show that they can cluster the phase space of an inverted pendulum,\nidentifying the balance, spin and pump controllers required to solve this task.\nSwitching density networks can be difficult to train, but we introduce a cross\nentropy regularisation loss that stabilises training.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 18:31:51 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 13:20:59 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 15:36:46 GMT"}, {"version": "v4", "created": "Wed, 18 Sep 2019 10:04:09 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Burke", "Michael", ""], ["Hristov", "Yordan", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1907.04361", "submitter": "Zixing Wang", "authors": "Zixing Wang, Nikolaos Papanikolopoulos", "title": "Estimating Pedestrian Moving State Based on Single 2D Body Pose", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Crossing or Not-Crossing (C/NC) problem is important to autonomous\nvehicles (AVs) for safe vehicle/pedestrian interactions. However, this problem\nsetup often ignores pedestrians walking along the direction of the vehicles'\nmovement (LONG). To enhance the AVs' awareness of pedestrians behavior, we make\nthe first step towards extending the C/NC to the C/NC/LONG problem and\nrecognize them based on single body pose. In contrast, previous C/NC state\nclassifiers depend on multiple poses or contextual information. Our proposed\nshallow neural network classifier aims to recognize these three states swiftly.\nWe tested it on the JAAD dataset and reported an average 81.23% accuracy.\nFurthermore, this model can be integrated with different sensors and algorithms\nthat provide 2D pedestrian body pose so that it is able to function across\nmultiple light and weather conditions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 18:32:21 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 21:24:06 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 14:44:20 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Wang", "Zixing", ""], ["Papanikolopoulos", "Nikolaos", ""]]}, {"id": "1907.04362", "submitter": "Yang Yang", "authors": "Yang Yang", "title": "BASN -- Learning Steganography with Binary Attention Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secret information sharing through image carrier has aroused much research\nattention in recent years with images' growing domination on the Internet and\nmobile applications. However, with the booming trend of convolutional neural\nnetworks, image steganography is facing a more significant challenge from\nneural-network-automated tasks. To improve the security of image steganography\nand minimize task result distortion, models must maintain the feature maps\ngenerated by task-specific networks being irrelative to any hidden information\nembedded in the carrier. This paper introduces a binary attention mechanism\ninto image steganography to help alleviate the security issue, and in the\nmeanwhile, increase embedding payload capacity. The experimental results show\nthat our method has the advantage of high payload capacity with little feature\nmap distortion and still resist detection by state-of-the-art image\nsteganalysis algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 18:33:51 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Yang", "Yang", ""]]}, {"id": "1907.04378", "submitter": "Shuang Ma", "authors": "Shuang Ma, Daniel McDuff, Yale Song", "title": "M3D-GAN: Multi-Modal Multi-Domain Translation with Universal Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks have led to significant advances in\ncross-modal/domain translation. However, typically these networks are designed\nfor a specific task (e.g., dialogue generation or image synthesis, but not\nboth). We present a unified model, M3D-GAN, that can translate across a wide\nrange of modalities (e.g., text, image, and speech) and domains (e.g.,\nattributes in images or emotions in speech). Our model consists of modality\nsubnets that convert data from different modalities into unified\nrepresentations, and a unified computing body where data from different\nmodalities share the same network architecture. We introduce a universal\nattention module that is jointly trained with the whole network and learns to\nencode a large range of domain information into a highly structured latent\nspace. We use this to control synthesis in novel ways, such as producing\ndiverse realistic pictures from a sketch or varying the emotion of synthesized\nspeech. We evaluate our approach on extensive benchmark tasks, including\nimage-to-image, text-to-image, image captioning, text-to-speech, speech\nrecognition, and machine translation. Our results show state-of-the-art\nperformance on some of the tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 19:33:01 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Ma", "Shuang", ""], ["McDuff", "Daniel", ""], ["Song", "Yale", ""]]}, {"id": "1907.04404", "submitter": "Sonali Patil", "authors": "Sonali Patil, Bharath Comandur, Tanmay Prakash, Avinash C. Kak", "title": "A New Stereo Benchmarking Dataset for Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to facilitate further research in stereo reconstruction with\nmulti-date satellite images, the goal of this paper is to provide a set of\nstereo-rectified images and the associated groundtruthed disparities for 10\nAOIs (Area of Interest) drawn from two sources: 8 AOIs from IARPA's MVS\nChallenge dataset and 2 AOIs from the CORE3D-Public dataset. The disparities\nwere groundtruthed by first constructing a fused DSM from the stereo pairs and\nby aligning 30 cm LiDAR with the fused DSM. Unlike the existing benckmarking\ndatasets, we have also carried out a quantitative evaluation of our\ngroundtruthed disparities using human annotated points in two of the AOIs.\nAdditionally, the rectification accuracy in our dataset is comparable to the\nsame in the existing state-of-the-art stereo datasets. In general, we have used\nthe WorldView-3 (WV3) images for the dataset, the exception being the UCSD area\nfor which we have used both WV3 and WorldView-2 (WV2) images. All of the\ndataset images are now in the public domain. Since multi-date satellite images\nfrequently include images acquired in different seasons (which creates\nchallenges in finding corresponding pairs of pixels for stereo), our dataset\nalso includes for each image a building mask over which the disparities\nestimated by stereo should prove reliable. Additional metadata included in the\ndataset includes information about each image's acquisition date and time, the\nazimuth and elevation angles of the camera, and the intersection angles for the\ntwo views in a stereo pair. Also included in the dataset are both quantitative\nand qualitative analyses of the accuracy of the groundtruthed disparity maps.\nOur dataset is available for download at\n\\url{https://engineering.purdue.edu/RVL/Database/SatStereo/index.html}\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 20:41:08 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Patil", "Sonali", ""], ["Comandur", "Bharath", ""], ["Prakash", "Tanmay", ""], ["Kak", "Avinash C.", ""]]}, {"id": "1907.04409", "submitter": "Brendon G. Anderson", "authors": "Brendon G. Anderson, Somayeh Sojoudi", "title": "Global Optimality Guarantees for Nonconvex Unsupervised Video\n  Segmentation", "comments": "Proceedings of the 57th Annual Allerton Conference on Communication,\n  Control, and Computing, 2019; added funding source information and notation\n  definitions", "journal-ref": "Proceedings of the 57th Annual Allerton Conference on\n  Communication, Control, and Computing, pp. 965--972, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of unsupervised video object\nsegmentation via background subtraction. Specifically, we pose the nonsemantic\nextraction of a video's moving objects as a nonconvex optimization problem via\na sum of sparse and low-rank matrices. The resulting formulation, a nonnegative\nvariant of robust principal component analysis, is more computationally\ntractable than its commonly employed convex relaxation, although not generally\nsolvable to global optimality. In spite of this limitation, we derive intuitive\nand interpretable conditions on the video data under which the uniqueness and\nglobal optimality of the object segmentation are guaranteed using local search\nmethods. We illustrate these novel optimality criteria through example\nsegmentations using real video data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 20:53:13 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 21:45:47 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Anderson", "Brendon G.", ""], ["Sojoudi", "Somayeh", ""]]}, {"id": "1907.04424", "submitter": "Md. Kamrul Hasan", "authors": "Md. Kamrul Hasan, Tajwar Abrar Aleef", "title": "Automatic Mass Detection in Breast Using Deep Convolutional Neural\n  Network and SVM Classifier", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mammography is the most widely used gold standard for screening breast\ncancer, where, mass detection is considered as the prominent step. Detecting\nmass in the breast is, however, an arduous problem as they usually have large\nvariations between them in terms of shape, size, boundary, and texture. In this\nliterature, the process of mass detection is automated with the use of transfer\nlearning techniques of Deep Convolutional Neural Networks (DCNN). Pre-trained\nVGG19 network is used to extract features which are then followed by bagged\ndecision tree for features selection and then a Support Vector Machine (SVM)\nclassifier is trained and used for classifying between the mass and non-mass.\nArea Under ROC Curve (AUC) is chosen as the performance metric, which is then\nmaximized during classifier selection and hyper-parameter tuning. The\nrobustness of the two selected type of classifiers, C-SVM, and \\u{psion}-SVM,\nare investigated with extensive experiments before selecting the best\nperforming classifier. All experiments in this paper were conducted using the\nINbreast dataset. The best AUC obtained from the experimental results is 0.994\n+/- 0.003 i.e. [0.991, 0.997]. Our results conclude that by using pre-trained\nVGG19 network, high-level distinctive features can be extracted from Mammograms\nwhich when used with the proposed SVM classifier is able to robustly\ndistinguish between the mass and non-mass present in breast.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 21:39:23 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Hasan", "Md. Kamrul", ""], ["Aleef", "Tajwar Abrar", ""]]}, {"id": "1907.04433", "submitter": "Aston Zhang", "authors": "Jian Guo, He He, Tong He, Leonard Lausen, Mu Li, Haibin Lin, Xingjian\n  Shi, Chenguang Wang, Junyuan Xie, Sheng Zha, Aston Zhang, Hang Zhang, Zhi\n  Zhang, Zhongyue Zhang, Shuai Zheng, Yi Zhu", "title": "GluonCV and GluonNLP: Deep Learning in Computer Vision and Natural\n  Language Processing", "comments": null, "journal-ref": "Journal of Machine Learning Research 21 (2020) 1-7", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GluonCV and GluonNLP, the deep learning toolkits for computer\nvision and natural language processing based on Apache MXNet (incubating).\nThese toolkits provide state-of-the-art pre-trained models, training scripts,\nand training logs, to facilitate rapid prototyping and promote reproducible\nresearch. We also provide modular APIs with flexible building blocks to enable\nefficient customization. Leveraging the MXNet ecosystem, the deep learning\nmodels in GluonCV and GluonNLP can be deployed onto a variety of platforms with\ndifferent programming languages. The Apache 2.0 license has been adopted by\nGluonCV and GluonNLP to allow for software distribution, modification, and\nusage.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 21:59:44 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 00:54:42 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Guo", "Jian", ""], ["He", "He", ""], ["He", "Tong", ""], ["Lausen", "Leonard", ""], ["Li", "Mu", ""], ["Lin", "Haibin", ""], ["Shi", "Xingjian", ""], ["Wang", "Chenguang", ""], ["Xie", "Junyuan", ""], ["Zha", "Sheng", ""], ["Zhang", "Aston", ""], ["Zhang", "Hang", ""], ["Zhang", "Zhi", ""], ["Zhang", "Zhongyue", ""], ["Zheng", "Shuai", ""], ["Zhu", "Yi", ""]]}, {"id": "1907.04444", "submitter": "David Griffiths Mr", "authors": "David Griffiths, Jan Boehm", "title": "A review on deep learning techniques for 3D sensed data classification", "comments": "25 pages, 9 figures. Review paper", "journal-ref": null, "doi": "10.3390/rs11121499", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decade deep learning has driven progress in 2D image\nunderstanding. Despite these advancements, techniques for automatic 3D sensed\ndata understanding, such as point clouds, is comparatively immature. However,\nwith a range of important applications from indoor robotics navigation to\nnational scale remote sensing there is a high demand for algorithms that can\nlearn to automatically understand and classify 3D sensed data. In this paper we\nreview the current state-of-the-art deep learning architectures for processing\nunstructured Euclidean data. We begin by addressing the background concepts and\ntraditional methodologies. We review the current main approaches including;\nRGB-D, multi-view, volumetric and fully end-to-end architecture designs.\nDatasets for each category are documented and explained. Finally, we give a\ndetailed discussion about the future of deep learning for 3D sensed data, using\nliterature to justify the areas where future research would be most valuable.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:30:03 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Griffiths", "David", ""], ["Boehm", "Jan", ""]]}, {"id": "1907.04449", "submitter": "Zelun Kong", "authors": "Zelun Kong, Junfeng Guo, Ang Li and Cong Liu", "title": "PhysGAN: Generating Physical-World-Resilient Adversarial Examples for\n  Autonomous Driving", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Deep neural networks (DNNs) are being pervasively used in\nvision-based autonomous driving systems, they are found vulnerable to\nadversarial attacks where small-magnitude perturbations into the inputs during\ntest time cause dramatic changes to the outputs. While most of the recent\nattack methods target at digital-world adversarial scenarios, it is unclear how\nthey perform in the physical world, and more importantly, the generated\nperturbations under such methods would cover a whole driving scene including\nthose fixed background imagery such as the sky, making them inapplicable to\nphysical world implementation. We present PhysGAN, which generates\nphysical-world-resilient adversarial examples for mislead-ing autonomous\ndriving systems in a continuous manner. We show the effectiveness and\nrobustness of PhysGAN via extensive digital and real-world evaluations. Digital\nexperiments show that PhysGAN is effective for various steer-ing models and\nscenes, which misleads the average steer-ing angle by up to 23.06 degrees under\nvarious scenarios. The real-world studies further demonstrate that PhysGAN is\nsufficiently resilient in practice, which misleads the average steering angle\nby up to 19.17 degrees. We compare PhysGAN with a set of state-of-the-art\nbaseline methods including several of our self-designed ones, which further\ndemonstrate the robustness and efficacy of our approach. We also show that\nPhysGAN outperforms state-of-the-art baseline methods To the best of our\nknowledge, PhysGANis probably the first technique of generating realistic and\nphysical-world-resilient adversarial examples for attacking common autonomous\ndriving scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:46:10 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 20:51:41 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 20:56:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Kong", "Zelun", ""], ["Guo", "Junfeng", ""], ["Li", "Ang", ""], ["Liu", "Cong", ""]]}, {"id": "1907.04463", "submitter": "Nathan Brugnone", "authors": "Nathan Brugnone (1), Alex Gonopolskiy (2), Mark W. Moyle (3), Manik\n  Kuchroo (3), David van Dijk (3), Kevin R. Moon (4), Daniel Colon-Ramos (3),\n  Guy Wolf (5), Matthew J. Hirn (1) and Smita Krishnaswamy (3) ((1) Michigan\n  State University, (2) PicnicHealth, (3) Yale University, (4) Utah State\n  University, (5) Universit\\'e de Montr\\'eal)", "title": "Coarse Graining of Data via Inhomogeneous Diffusion Condensation", "comments": "14 pages, 7 figures", "journal-ref": "Proceedings of the 2019 IEEE International Conference on Big Data,\n  pages 2624-2633, 2019", "doi": "10.1109/BigData47090.2019.9006013", "report-no": null, "categories": "cs.HC cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data often has emergent structure that exists at multiple levels of\nabstraction, which are useful for characterizing complex interactions and\ndynamics of the observations. Here, we consider multiple levels of abstraction\nvia a multiresolution geometry of data points at different granularities. To\nconstruct this geometry we define a time-inhomogeneous diffusion process that\neffectively condenses data points together to uncover nested groupings at\nlarger and larger granularities. This inhomogeneous process creates a deep\ncascade of intrinsic low pass filters on the data affinity graph that are\napplied in sequence to gradually eliminate local variability while adjusting\nthe learned data geometry to increasingly coarser resolutions. We provide\nvisualizations to exhibit our method as a continuously-hierarchical clustering\nwith directions of eliminated variation highlighted at each step. The utility\nof our algorithm is demonstrated via neuronal data condensation, where the\nconstructed multiresolution data geometry uncovers the organization, grouping,\nand connectivity between neurons.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 00:08:07 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 23:43:22 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 20:12:26 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Brugnone", "Nathan", ""], ["Gonopolskiy", "Alex", ""], ["Moyle", "Mark W.", ""], ["Kuchroo", "Manik", ""], ["van Dijk", "David", ""], ["Moon", "Kevin R.", ""], ["Colon-Ramos", "Daniel", ""], ["Wolf", "Guy", ""], ["Hirn", "Matthew J.", ""], ["Krishnaswamy", "Smita", ""]]}, {"id": "1907.04476", "submitter": "Yuxin Peng", "authors": "Xiangteng He, Yuxin Peng and Liu Xie", "title": "A New Benchmark and Approach for Fine-grained Cross-media Retrieval", "comments": "9 pages, ACM MM 2019", "journal-ref": null, "doi": "10.1145/3343031.3350974", "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-media retrieval is to return the results of various media types\ncorresponding to the query of any media type. Existing researches generally\nfocus on coarse-grained cross-media retrieval. When users submit an image of\n\"Slaty-backed Gull\" as a query, coarse-grained cross-media retrieval treats it\nas \"Bird\", so that users can only get the results of \"Bird\", which may include\nother bird species with similar appearance (image and video), descriptions\n(text) or sounds (audio), such as \"Herring Gull\". Such coarse-grained\ncross-media retrieval is not consistent with human lifestyle, where we\ngenerally have the fine-grained requirement of returning the exactly relevant\nresults of \"Slaty-backed Gull\" instead of \"Herring Gull\". However, few\nresearches focus on fine-grained cross-media retrieval, which is a highly\nchallenging and practical task. Therefore, in this paper, we first construct a\nnew benchmark for fine-grained cross-media retrieval, which consists of 200\nfine-grained subcategories of the \"Bird\", and contains 4 media types, including\nimage, text, video and audio. To the best of our knowledge, it is the first\nbenchmark with 4 media types for fine-grained cross-media retrieval. Then, we\npropose a uniform deep model, namely FGCrossNet, which simultaneously learns 4\ntypes of media without discriminative treatments. We jointly consider three\nconstraints for better common representation learning: classification\nconstraint ensures the learning of discriminative features, center constraint\nensures the compactness characteristic of the features of the same subcategory,\nand ranking constraint ensures the sparsity characteristic of the features of\ndifferent subcategories. Extensive experiments verify the usefulness of the new\nbenchmark and the effectiveness of our FGCrossNet. They will be made available\nat https://github.com/PKU-ICST-MIPL/FGCrossNet_ACMMM2019.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 01:15:22 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 06:37:53 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["He", "Xiangteng", ""], ["Peng", "Yuxin", ""], ["Xie", "Liu", ""]]}, {"id": "1907.04500", "submitter": "Junshen Xu", "authors": "Junshen Xu, Molin Zhang, Esra Abaci Turk, Larry Zhang, Ellen Grant,\n  Kui Ying, Polina Golland, Elfar Adalsteinsson", "title": "Fetal Pose Estimation in Volumetric MRI using a 3D Convolution Neural\n  Network", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance and diagnostic utility of magnetic resonance imaging (MRI) in\npregnancy is fundamentally constrained by fetal motion. Motion of the fetus,\nwhich is unpredictable and rapid on the scale of conventional imaging times,\nlimits the set of viable acquisition techniques to single-shot imaging with\nsevere compromises in signal-to-noise ratio and diagnostic contrast, and\nfrequently results in unacceptable image quality. Surprisingly little is known\nabout the characteristics of fetal motion during MRI and here we propose and\ndemonstrate methods that exploit a growing repository of MRI observations of\nthe gravid abdomen that are acquired at low spatial resolution but relatively\nhigh temporal resolution and over long durations (10-30 minutes). We estimate\nfetal pose per frame in MRI volumes of the pregnant abdomen via deep learning\nalgorithms that detect key fetal landmarks. Evaluation of the proposed method\nshows that our framework achieves quantitatively an average error of 4.47 mm\nand 96.4\\% accuracy (with error less than 10 mm). Fetal pose estimation in MRI\ntime series yields novel means of quantifying fetal movements in health and\ndisease, and enables the learning of kinematic models that may enhance\nprospective mitigation of fetal motion artifacts during MRI acquisition.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 03:56:02 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Xu", "Junshen", ""], ["Zhang", "Molin", ""], ["Turk", "Esra Abaci", ""], ["Zhang", "Larry", ""], ["Grant", "Ellen", ""], ["Ying", "Kui", ""], ["Golland", "Polina", ""], ["Adalsteinsson", "Elfar", ""]]}, {"id": "1907.04508", "submitter": "Xing Liu", "authors": "Xing Liu, Masanori Suganuma, Xiyang Luo, Takayuki Okatani", "title": "Restoring Images with Unknown Degradation Factors by Recurrent Use of a\n  Multi-branch Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The employment of convolutional neural networks has achieved unprecedented\nperformance in the task of image restoration for a variety of degradation\nfactors. However, high-performance networks have been specifically designed for\na single degradation factor. In this paper, we tackle a harder problem,\nrestoring a clean image from its degraded version with an unknown degradation\nfactor, subject to the condition that it is one of the known factors. Toward\nthis end, we design a network having multiple pairs of input and output\nbranches and use it in a recurrent fashion such that a different branch pair is\nused at each of the recurrent paths. We reinforce the shared part of the\nnetwork with improved components so that it can handle different degradation\nfactors. We also propose a two-step training method for the network, which\nconsists of multi-task learning and finetuning. The experimental results show\nthat the proposed network yields at least comparable or sometimes even better\nperformance on four degradation factors as compared with the best dedicated\nnetwork for each of the four. We also test it on a further harder task where\nthe input image contains multiple degradation factors that are mixed with\nunknown mixture ratios, showing that it achieves better performance than the\nprevious state-of-the-art method designed for the task.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 04:58:10 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 07:00:11 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Liu", "Xing", ""], ["Suganuma", "Masanori", ""], ["Luo", "Xiyang", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1907.04519", "submitter": "Andrey Savchenko", "authors": "A.V. Savchenko, K.V. Demochkin, I.S. Grechikhin", "title": "Preferences Prediction using a Gallery of Mobile Device based on Scene\n  Recognition and Object Detection", "comments": "19 pages; 9 figures, preprint submitter to Pattern Recognition\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper user modeling task is examined by processing a gallery of\nphotos and videos on a mobile device. We propose novel engine for user\npreference prediction based on scene recognition, object detection and facial\nanalysis. At first, all faces in a gallery are clustered and all private photos\nand videos with faces from large clusters are processed on the embedded system\nin offline mode. Other photos may be sent to the remote server to be analyzed\nby very deep models. The visual features of each photo are obtained from scene\nrecognition and object detection models. These features are aggregated into a\nsingle user descriptor in the neural attention block. The proposed pipeline is\nimplemented for the Android mobile platform. Experimental results with a subset\nof Photo Event Collection, Web Image Dataset for Event Recognition and Amazon\nFashion datasets demonstrate the possibility to process images very efficiently\nwithout significant accuracy degradation. The source code of Android mobile\napplication is publicly available at\nhttps://github.com/HSE-asavchenko/mobile-visual-preferences.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 05:45:08 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 13:33:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Savchenko", "A. V.", ""], ["Demochkin", "K. V.", ""], ["Grechikhin", "I. S.", ""]]}, {"id": "1907.04525", "submitter": "Dooseop Choi Dr", "authors": "Dooseop Choi and Kyoungwook Min and Jeongdan Choi", "title": "Regularizing Neural Networks for Future Trajectory Prediction via\n  Inverse Reinforcement Learning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting distant future trajectories of agents in a dynamic scene is not an\neasy problem because the future trajectory of an agent is affected by not only\nhis/her past trajectory but also the scene contexts. To tackle this problem, we\npropose a model based on recurrent neural networks (RNNs) and a novel method\nfor training the model. The proposed model is based on an encoder-decoder\narchitecture where the encoder encodes inputs (past trajectories and scene\ncontext information) while the decoder produces a trajectory from the context\nvector given by the encoder. We train the networks of the proposed model to\nproduce a future trajectory, which is the closest to the true trajectory, while\nmaximizing a reward from a reward function. The reward function is also trained\nat the same time to maximize the margin between the rewards from the\nground-truth trajectory and its estimate. The reward function plays the role of\na regularizer for the proposed model so the trained networks are able to better\nutilize the scene context information for the prediction task. We evaluated the\nproposed model on several public datasets. Experimental results show that the\nprediction performance of the proposed model is much improved by the\nregularization, which outperforms the-state-of-the-arts in terms of accuracy.\nThe implementation codes are available at\nhttps://github.com/d1024choi/traj-pred-irl/.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 06:00:51 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 00:32:53 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Choi", "Dooseop", ""], ["Min", "Kyoungwook", ""], ["Choi", "Jeongdan", ""]]}, {"id": "1907.04553", "submitter": "Thao Minh Le", "authors": "Thao Minh Le, Vuong Le, Svetha Venkatesh, Truyen Tran", "title": "Neural Reasoning, Fast and Slow, for Video Question Answering", "comments": null, "journal-ref": "International Joint Conference on Neural Networks (IJCNN) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does it take to design a machine that learns to answer natural questions\nabout a video? A Video QA system must simultaneously understand language,\nrepresent visual content over space-time, and iteratively transform these\nrepresentations in response to lingual content in the query, and finally\narriving at a sensible answer. While recent advances in lingual and visual\nquestion answering have enabled sophisticated representations and neural\nreasoning mechanisms, major challenges in Video QA remain on dynamic grounding\nof concepts, relations and actions to support the reasoning process. Inspired\nby the dual-process account of human reasoning, we design a dual process neural\narchitecture, which is composed of a question-guided video processing module\n(System 1, fast and reactive) followed by a generic reasoning module (System 2,\nslow and deliberative). System 1 is a hierarchical model that encodes visual\npatterns about objects, actions and relations in space-time given the textual\ncues from the question. The encoded representation is a set of high-level\nvisual features, which are then passed to System 2. Here multi-step inference\nfollows to iteratively chain visual elements as instructed by the textual\nelements. The system is evaluated on the SVQA (synthetic) and TGIF-QA datasets\n(real), demonstrating competitive results, with a large margin in the case of\nmulti-step reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 07:53:17 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 00:30:34 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Le", "Thao Minh", ""], ["Le", "Vuong", ""], ["Venkatesh", "Svetha", ""], ["Tran", "Truyen", ""]]}, {"id": "1907.04563", "submitter": "Thomas Kurmann", "authors": "Thomas Kurmann and Pablo Marquez Neila and Sebastian Wolf and Raphael\n  Sznitman", "title": "Deep Multi Label Classification in Affine Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification (MLC) problems are becoming increasingly popular\nin the context of medical imaging. This has in part been driven by the fact\nthat acquiring annotations for MLC is far less burdensome than for semantic\nsegmentation and yet provides more expressiveness than multi-class\nclassification. However, to train MLCs, most methods have resorted to similar\nobjective functions as with traditional multi-class classification settings. We\nshow in this work that such approaches are not optimal and instead propose a\nnovel deep MLC classification method in affine subspace. At its core, the\nmethod attempts to pull features of class-labels towards different affine\nsubspaces while maximizing the distance between them. We evaluate the method\nusing two MLC medical imaging datasets and show a large performance increase\ncompared to previous multi-label frameworks. This method can be seen as a\nplug-in replacement loss function and is trainable in an end-to-end fashion.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 08:16:27 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Kurmann", "Thomas", ""], ["Neila", "Pablo Marquez", ""], ["Wolf", "Sebastian", ""], ["Sznitman", "Raphael", ""]]}, {"id": "1907.04565", "submitter": "Jules Vidal", "authors": "Jules Vidal, Joseph Budin, and Julien Tierny", "title": "Progressive Wasserstein Barycenters of Persistence Diagrams", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934256", "report-no": null, "categories": "cs.GR cs.CG cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient algorithm for the progressive approximation\nof Wasserstein barycenters of persistence diagrams, with applications to the\nvisual analysis of ensemble data. Given a set of scalar fields, our approach\nenables the computation of a persistence diagram which is representative of the\nset, and which visually conveys the number, data ranges and saliences of the\nmain features of interest found in the set. Such representative diagrams are\nobtained by computing explicitly the discrete Wasserstein barycenter of the set\nof persistence diagrams, a notoriously computationally intensive task. In\nparticular, we revisit efficient algorithms for Wasserstein distance\napproximation [12,51] to extend previous work on barycenter estimation [94]. We\npresent a new fast algorithm, which progressively approximates the barycenter\nby iteratively increasing the computation accuracy as well as the number of\npersistent features in the output diagram. Such a progressivity drastically\nimproves convergence in practice and allows to design an interruptible\nalgorithm, capable of respecting computation time constraints. This enables the\napproximation of Wasserstein barycenters within interactive times. We present\nan application to ensemble clustering where we revisit the k-means algorithm to\nexploit our barycenters and compute, within execution time constraints,\nmeaningful clusters of ensemble data along with their barycenter diagram.\nExtensive experiments on synthetic and real-life data sets report that our\nalgorithm converges to barycenters that are qualitatively meaningful with\nregard to the applications, and quantitatively comparable to previous\ntechniques, while offering an order of magnitude speedup when run until\nconvergence (without time constraint). Our algorithm can be trivially\nparallelized to provide additional speedups in practice on standard\nworkstations. [...]\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 08:24:11 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 16:36:24 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Vidal", "Jules", ""], ["Budin", "Joseph", ""], ["Tierny", "Julien", ""]]}, {"id": "1907.04569", "submitter": "Tom Bruls M.Sc.", "authors": "Tom Bruls, Horia Porav, Lars Kunze, and Paul Newman", "title": "Generating All the Roads to Rome: Road Layout Randomization for Improved\n  Road Marking Segmentation", "comments": "presented at ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road markings provide guidance to traffic participants and enforce safe\ndriving behaviour, understanding their semantic meaning is therefore paramount\nin (automated) driving. However, producing the vast quantities of road marking\nlabels required for training state-of-the-art deep networks is costly,\ntime-consuming, and simply infeasible for every domain and condition. In\naddition, training data retrieved from virtual worlds often lack the richness\nand complexity of the real world and consequently cannot be used directly. In\nthis paper, we provide an alternative approach in which new road marking\ntraining pairs are automatically generated. To this end, we apply principles of\ndomain randomization to the road layout and synthesize new images from altered\nsemantic labels. We demonstrate that training on these synthetic pairs improves\nmIoU of the segmentation of rare road marking classes during real-world\ndeployment in complex urban environments by more than 12 percentage points,\nwhile performance for other classes is retained. This framework can easily be\nscaled to all domains and conditions to generate large-scale road marking\ndatasets, while avoiding manual labelling effort.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 08:27:59 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Bruls", "Tom", ""], ["Porav", "Horia", ""], ["Kunze", "Lars", ""], ["Newman", "Paul", ""]]}, {"id": "1907.04572", "submitter": "Yujia Huang", "authors": "Yujia Huang, Sihui Dai, Tan Nguyen, Richard G. Baraniuk, Anima\n  Anandkumar", "title": "Out-of-Distribution Detection Using Neural Rendering Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out-of-distribution (OoD) detection is a natural downstream task for deep\ngenerative models, due to their ability to learn the input probability\ndistribution. There are mainly two classes of approaches for OoD detection\nusing deep generative models, viz., based on likelihood measure and the\nreconstruction loss. However, both approaches are unable to carry out OoD\ndetection effectively, especially when the OoD samples have smaller variance\nthan the training samples. For instance, both flow based and VAE models assign\nhigher likelihood to images from SVHN when trained on CIFAR-10 images. We use a\nrecently proposed generative model known as neural rendering model (NRM) and\nderive metrics for OoD. We show that NRM unifies both approaches since it\nprovides a likelihood estimate and also carries out reconstruction in each\nlayer of the neural network. Among various measures, we found the joint\nlikelihood of latent variables to be the most effective one for OoD detection.\nOur results show that when trained on CIFAR-10, lower likelihood (of latent\nvariables) is assigned to SVHN images. Additionally, we show that this metric\nis consistent across other OoD datasets. To the best of our knowledge, this is\nthe first work to show consistently lower likelihood for OoD data with smaller\nvariance with deep generative models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 08:32:53 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Huang", "Yujia", ""], ["Dai", "Sihui", ""], ["Nguyen", "Tan", ""], ["Baraniuk", "Richard G.", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1907.04589", "submitter": "Shaodi You", "authors": "Tianxiu Yu, Shijie Zhang, Cong Lin, Shaodi You, Jian Wu, Jiawan Zhang,\n  Xiaohong Ding and Huili An", "title": "Dunhuang Grottoes Painting Dataset and Benchmark", "comments": "8 pages, 1 column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document introduces the background and the usage of the Dunhuang\nGrottoes Dataset and the benchmark. The documentation first starts with the\nbackground of the Dunhuang Grotto, which is widely recognised as an priceless\nheritage. Given that digital method is the modern trend for heritage protection\nand restoration. Follow the trend, we release the first public dataset for\nDunhuang Grotto Painting restoration. The rest of the documentation details the\npainting data generation. To enable a data driven fashion, this dataset\nprovided a large number of training and testing example which is sufficient for\na deep learning approach. The detailed usage of the dataset as well as the\nbenchmark is described.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 09:35:08 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 06:46:18 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Yu", "Tianxiu", ""], ["Zhang", "Shijie", ""], ["Lin", "Cong", ""], ["You", "Shaodi", ""], ["Wu", "Jian", ""], ["Zhang", "Jiawan", ""], ["Ding", "Xiaohong", ""], ["An", "Huili", ""]]}, {"id": "1907.04632", "submitter": "Wei Peng", "authors": "Wei Peng, Xiaopeng Hong, Guoying Zhao", "title": "Video Action Recognition Via Neural Architecture Searching", "comments": "Accepted by IEEE ICIP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks have achieved great success for video analysis and\nunderstanding. However, designing a high-performance neural architecture\nrequires substantial efforts and expertise. In this paper, we make the first\nattempt to let algorithm automatically design neural networks for video action\nrecognition tasks. Specifically, a spatio-temporal network is developed in a\ndifferentiable space modeled by a directed acyclic graph, thus a gradient-based\nstrategy can be performed to search an optimal architecture. Nonetheless, it is\ncomputationally expensive, since the computational burden to evaluate each\narchitecture candidate is still heavy. To alleviate this issue, we, for the\nvideo input, introduce a temporal segment approach to reduce the computational\ncost without losing global video information. For the architecture, we explore\nin an efficient search space by introducing pseudo 3D operators. Experiments\nshow that, our architecture outperforms popular neural architectures, under the\ntraining from scratch protocol, on the challenging UCF101 dataset,\nsurprisingly, with only around one percentage of parameters of its\nmanual-design counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:44:28 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Peng", "Wei", ""], ["Hong", "Xiaopeng", ""], ["Zhao", "Guoying", ""]]}, {"id": "1907.04637", "submitter": "Adri\\`a Arbu\\'es-Sang\\\"uesa", "authors": "Adri\\`a Arbu\\'es-Sang\\\"uesa, Gloria Haro, Coloma Ballester", "title": "Multi-Person tracking by multi-scale detection in Basketball scenarios", "comments": "Accepted in IMVIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking data is a powerful tool for basketball teams in order to extract\nadvanced semantic information and statistics that might lead to a performance\nboost. However, multi-person tracking is a challenging task to solve in\nsingle-camera video sequences, given the frequent occlusions and cluttering\nthat occur in a restricted scenario. In this paper, a novel multi-scale\ndetection method is presented, which is later used to extract geometric and\ncontent features, resulting in a multi-person video tracking system. Having\nbuilt a dataset from scratch together with its ground truth (more than 10k\nbounding boxes), standard metrics are evaluated, obtaining notable results both\nin terms of detection (F1-score) and tracking (MOTA). The presented system\ncould be used as a source of data gathering in order to extract useful\nstatistics and semantic analyses a posteriori.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:56:35 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Arbu\u00e9s-Sang\u00fcesa", "Adri\u00e0", ""], ["Haro", "Gloria", ""], ["Ballester", "Coloma", ""]]}, {"id": "1907.04641", "submitter": "Tobias Fechter", "authors": "Tobias Fechter, Dimos Baltas", "title": "One Shot Learning for Deformable Medical Image Registration and Periodic\n  Motion Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable image registration is a very important field of research in\nmedical imaging. Recently multiple deep learning approaches were published in\nthis area showing promising results. However, drawbacks of deep learning\nmethods are the need for a large amount of training datasets and their\ninability to register unseen images different from the training datasets. One\nshot learning comes without the need of large training datasets and has already\nbeen proven to be applicable to 3D data. In this work we present a one shot\nregistration approach for periodic motion tracking in 3D and 4D datasets. When\napplied to 3D dataset the algorithm calculates the inverse of a registration\nvector field simultaneously. For registration we employed a U-Net combined with\na coarse to fine approach and a differential spatial transformer module. The\nalgorithm was thoroughly tested with multiple 4D and 3D datasets publicly\navailable. The results show that the presented approach is able to track\nperiodic motion and to yield a competitive registration accuracy. Possible\napplications are the use as a stand-alone algorithm for 3D and 4D motion\ntracking or in the beginning of studies until enough datasets for a separate\ntraining phase are available.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 12:05:19 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 09:07:56 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 09:36:03 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Fechter", "Tobias", ""], ["Baltas", "Dimos", ""]]}, {"id": "1907.04675", "submitter": "S\\\"oren Dittmer", "authors": "S\\\"oren Dittmer and Peter Maass", "title": "A Projectional Ansatz to Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the field of inverse problems has seen a growing usage of\nmathematically only partially understood learned and non-learned priors. Based\non first principles, we develop a projectional approach to inverse problems\nthat addresses the incorporation of these priors, while still guaranteeing data\nconsistency. We implement this projectional method (PM) on the one hand via\nvery general Plug-and-Play priors and on the other hand, via an end-to-end\ntraining approach. To this end, we introduce a novel alternating neural\narchitecture, allowing for the incorporation of highly customized priors from\ndata in a principled manner. We also show how the recent success of\nRegularization by Denoising (RED) can, at least to some extent, be explained as\nan approximation of the PM. Furthermore, we demonstrate how the idea can be\napplied to stop the degradation of Deep Image Prior (DIP) reconstructions over\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 12:49:07 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 11:36:52 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Dittmer", "S\u00f6ren", ""], ["Maass", "Peter", ""]]}, {"id": "1907.04681", "submitter": "Nicolas Brieu", "authors": "Nicolas Brieu, Armin Meier, Ansh Kapil, Ralf Schoenmeyer, Christos G.\n  Gavriel, Peter D. Caie, G\\\"unter Schmidt", "title": "Domain Adaptation-based Augmentation for Weakly Supervised Nuclei\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of nuclei is one of the most fundamental components of\ncomputational pathology. Current state-of-the-art methods are based on deep\nlearning, with the prerequisite that extensive labeled datasets are available.\nThe increasing number of patient cohorts to be analyzed, the diversity of\ntissue stains and indications, as well as the cost of dataset labeling\nmotivates the development of novel methods to reduce labeling effort across\ndomains. We introduce in this work a weakly supervised 'inter-domain' approach\nthat (i) performs stain normalization and unpaired image-to-image translation\nto transform labeled images on a source domain to synthetic labeled images on\nan unlabeled target domain and (ii) uses the resulting synthetic labeled images\nto train a detection network on the target domain. Extensive experiments show\nthe superiority of the proposed approach against the state-of-the-art\n'intra-domain' detection based on fully-supervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 12:52:46 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Brieu", "Nicolas", ""], ["Meier", "Armin", ""], ["Kapil", "Ansh", ""], ["Schoenmeyer", "Ralf", ""], ["Gavriel", "Christos G.", ""], ["Caie", "Peter D.", ""], ["Schmidt", "G\u00fcnter", ""]]}, {"id": "1907.04728", "submitter": "Congcong Liu", "authors": "Congcong Liu, Yuying Chen, Lei Tai, Ming Liu, Bertram Shi", "title": "Utilizing Eye Gaze to Enhance the Generalization of Imitation Networks\n  to Unseen Environments", "comments": "4 pages, 3 figures, accepted by ICML 2019 Workshop on Understanding\n  and Improving Generalization in Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based autonomous driving through imitation learning mimics the\nbehaviors of human drivers by training on pairs of data of raw driver-view\nimages and actions. However, there are other cues, e.g. gaze behavior,\navailable from human drivers that have yet to be exploited. Previous research\nhas shown that novice human learners can benefit from observing experts' gaze\npatterns. We show here that deep neural networks can also benefit from this. We\ndemonstrate different approaches to integrating gaze information into imitation\nnetworks. Our results show that the integration of gaze information improves\nthe generalization performance of networks to unseen environments.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 13:55:05 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 03:51:20 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Liu", "Congcong", ""], ["Chen", "Yuying", ""], ["Tai", "Lei", ""], ["Liu", "Ming", ""], ["Shi", "Bertram", ""]]}, {"id": "1907.04758", "submitter": "David Griffiths Mr", "authors": "David Griffiths, Jan Boehm", "title": "SynthCity: A large scale synthetic point cloud", "comments": "6 pages, 4 figures, dataset white paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With deep learning becoming a more prominent approach for automatic\nclassification of three-dimensional point cloud data, a key bottleneck is the\namount of high quality training data, especially when compared to that\navailable for two-dimensional images. One potential solution is the use of\nsynthetic data for pre-training networks, however the ability for models to\ngeneralise from synthetic data to real world data has been poorly studied for\npoint clouds. Despite this, a huge wealth of 3D virtual environments exist\nwhich, if proved effective can be exploited. We therefore argue that research\nin this domain would be of significant use. In this paper we present SynthCity\nan open dataset to help aid research. SynthCity is a 367.9M point synthetic\nfull colour Mobile Laser Scanning point cloud. Every point is assigned a label\nfrom one of nine categories. We generate our point cloud in a typical\nUrban/Suburban environment using the Blensor plugin for Blender.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:43:56 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Griffiths", "David", ""], ["Boehm", "Jan", ""]]}, {"id": "1907.04759", "submitter": "Thomas Duboudin", "authors": "Thomas Duboudin (imagine), Maxime Petit (imagine), Liming Chen\n  (imagine)", "title": "Toward a Procedural Fruit Tree Rendering Framework for Image Analysis", "comments": null, "journal-ref": "7th International Workshop on Image Analysis Methods in the Plant\n  Sciences, Jul 2019, Lyon, France. pp.4 - 5", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a procedural fruit tree rendering framework, based on Blender and\nPython scripts allowing to generate quickly labeled dataset (i.e. including\nground truth semantic segmentation). It is designed to train image analysis\ndeep learning methods (e.g. in a robotic fruit harvesting context), where real\nlabeled training datasets are usually scarce and existing synthetic ones are\ntoo specialized. Moreover, the framework includes the possibility to introduce\nparametrized variations in the model (e.g. lightning conditions, background),\nproducing a dataset with embedded Domain Randomization aspect.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:45:14 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Duboudin", "Thomas", "", "imagine"], ["Petit", "Maxime", "", "imagine"], ["Chen", "Liming", "", "imagine"]]}, {"id": "1907.04761", "submitter": "Luca Cavalli", "authors": "Luca Cavalli, Gianpaolo Di Pietro, Matteo Matteucci", "title": "Towards Affordance Prediction with Vision via Task Oriented Grasp\n  Quality Metrics", "comments": "8 pages, presented at the Second International Workshop on\n  Computational Models of Affordance in Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many quality metrics exist to evaluate the quality of a grasp by\nitself, no clear quantification of the quality of a grasp relatively to the\ntask the grasp is used for has been defined yet. In this paper we propose a\nframework to extend the concept of grasp quality metric to task-oriented\ngrasping by defining affordance functions via basic grasp metrics for an open\nset of task affordances. We evaluate both the effectivity of the proposed task\noriented metrics and their practical applicability by learning to infer them\nfrom vision. Indeed, we assess the validity of our novel framework both in the\ncontext of perfect information, i.e., known object model, and in the partial\ninformation context, i.e., inferring task oriented metrics from vision,\nunderlining advantages and limitations of both situations. In the former,\nphysical metrics of grasp hypotheses on an object are defined and computed in\nknown object model simulation, in the latter deep models are trained to infer\nsuch properties from partial information in the form of synthesized range\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:45:37 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Cavalli", "Luca", ""], ["Di Pietro", "Gianpaolo", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1907.04774", "submitter": "Rohan Reddy Mekala", "authors": "Rohan Reddy Mekala, Gudjon Einar Magnusson, Adam Porter, Mikael\n  Lindvall, Madeline Diep", "title": "Metamorphic Detection of Adversarial Examples in Deep Learning Models\n  With Affine Transformations", "comments": null, "journal-ref": null, "doi": "10.1109/MET.2019.00016", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks are small, carefully crafted perturbations, imperceptible\nto the naked eye; that when added to an image cause deep learning models to\nmisclassify the image with potentially detrimental outcomes. With the rise of\nartificial intelligence models in consumer safety and security intensive\nindustries such as self-driving cars, camera surveillance and face recognition,\nthere is a growing need for guarding against adversarial attacks. In this\npaper, we present an approach that uses metamorphic testing principles to\nautomatically detect such adversarial attacks. The approach can detect image\nmanipulations that are so small, that they are impossible to detect by a human\nthrough visual inspection. By applying metamorphic relations based on distance\nratio preserving affine image transformations which compare the behavior of the\noriginal and transformed image; we show that our proposed approach can\ndetermine whether or not the input image is adversarial with a high degree of\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 15:04:18 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Mekala", "Rohan Reddy", ""], ["Magnusson", "Gudjon Einar", ""], ["Porter", "Adam", ""], ["Lindvall", "Mikael", ""], ["Diep", "Madeline", ""]]}, {"id": "1907.04822", "submitter": "Farzad Khalvati", "authors": "Yucheng Zhang, Edrise M. Lobo-Mueller, Paul Karanicolas, Steven\n  Gallinger, Masoom A. Haider, Farzad Khalvati", "title": "Improving Prognostic Performance in Resectable Pancreatic Ductal\n  Adenocarcinoma using Radiomics and Deep Learning Features Fusion in CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an analytic pipeline for quantitative imaging feature extraction and\nanalysis, radiomics has grown rapidly in the past a few years. Recent studies\nin radiomics aim to investigate the relationship between tumors imaging\nfeatures and clinical outcomes. Open source radiomics feature banks enable the\nextraction and analysis of thousands of predefined features. On the other hand,\nrecent advances in deep learning have shown significant potential in the\nquantitative medical imaging field, raising the research question of whether\npredefined radiomics features have predictive information in addition to deep\nlearning features. In this study, we propose a feature fusion method and\ninvestigate whether a combined feature bank of deep learning and predefined\nradiomics features can improve the prognostics performance. CT images from\nresectable Pancreatic Adenocarcinoma (PDAC) patients were used to compare the\nprognosis performance of common feature reduction and fusion methods and the\nproposed risk-score based feature fusion method for overall survival. It was\nshown that the proposed feature fusion method significantly improves the\nprognosis performance for overall survival in resectable PDAC cohorts,\nelevating the area under ROC curve by 51% compared to predefined radiomics\nfeatures alone, by 16% compared to deep learning features alone, and by 32%\ncompared to existing feature fusion and reduction methods for a combination of\ndeep learning and predefined radiomics features.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 16:43:50 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Zhang", "Yucheng", ""], ["Lobo-Mueller", "Edrise M.", ""], ["Karanicolas", "Paul", ""], ["Gallinger", "Steven", ""], ["Haider", "Masoom A.", ""], ["Khalvati", "Farzad", ""]]}, {"id": "1907.04834", "submitter": "Jiancong Wang", "authors": "Jiancong Wang, Long Xie, Paul Yushkevich, James Gee", "title": "Barnes-Hut Approximation for Point SetGeodesic Shooting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geodesic shooting has been successfully applied to diffeo-morphic\nregistration of point sets. Exact computation of the geodesicshooting between\npoint sets, however, requiresO(N2) calculations each time step on the number of\npoints in the point set. We proposean approximation approach based on the\nBarnes-Hut algorithm to speedup point set geodesic shooting. This approximation\ncan reduce the al-gorithm complexity toO(N b+N logN). The evaluation of the\nproposedmethod in both simulated images and the medial temporal lobe thick-ness\nanalysis demonstrates a comparable accuracy to the exact point set geodesic\nshooting while offering up to 3-fold speed up. This improvementopens up a range\nof clinical research studies and practical problems towhich the method can be\neffectively applied.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:32:07 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Wang", "Jiancong", ""], ["Xie", "Long", ""], ["Yushkevich", "Paul", ""], ["Gee", "James", ""]]}, {"id": "1907.04835", "submitter": "Jiancong Wang", "authors": "Jiancong Wang, Yuhua Chen, Yifan Wu, Jianbo Shi, James Gee", "title": "Enhanced generative adversarial network for 3D brain MRI\n  super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) reconstruction for magnetic resonance\nimaging (MRI) has generated significant interest because of its potential to\nnot only speed up imaging but to improve quantitative processing and analysis\nof available image data. Generative Adversarial Networks (GAN) have proven to\nperform well in recovering image texture detail, and many variants have\ntherefore been proposed for SISR. In this work, we develop an enhancement to\ntackle GAN-based 3D SISR by introducing a new residual-in-residual dense block\n(RRDG) generator that is both memory efficient and achieves state-of-the-art\nperformance in terms of PSNR (Peak Signal to Noise Ratio), SSIM (Structural\nSimilarity) and NRMSE (Normalized Root Mean Squared Error) metrics. We also\nintroduce a patch GAN discriminator with improved convergence behavior to\nbetter model brain image texture. We proposed a novel the anatomical fidelity\nevaluation of the results using a pre-trained brain parcellation network.\nFinally, these developments are combined through a simple and efficient method\nto balance etween image and texture quality in the final output.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:32:28 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 20:05:57 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Wang", "Jiancong", ""], ["Chen", "Yuhua", ""], ["Wu", "Yifan", ""], ["Shi", "Jianbo", ""], ["Gee", "James", ""]]}, {"id": "1907.04839", "submitter": "Jiancong Wang", "authors": "Jiancong Wang", "title": "Fast geodesic shooting for landmark matching using CUDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark matching via geodesic shooting is a prerequisite task for numerous\nregistration based applications in biomedicine. Geodesic shooting has been\ndeveloped as one solution approach and formulates the diffeomorphic\nregistration as an optimal control problem under the Hamiltonian framework. In\nthis framework, with landmark positions q0 fixed, the problem solely depends on\nthe initial momentum p0 and evolves through time steps according to a set of\nconstraint equations. Given an initial p0, the algorithm flows q and p forward\nthrough time steps, calculates a loss based on point-set mismatch and kinetic\nenergy, back-propagate through time to calculate gradient on p0 and update it.\nIn the forward and backward pass, a pair-wise kernel on landmark points K and\nadditional intermediate terms have to be calculated and marginalized, leading\nto O(N2) computational complexity, N being the number of points to be\nregistered. For medical image applications, N maybe in the range of thousands,\nrendering this operation computationally expensive. In this work we ropose a\nCUDA implementation based on shared memory reduction. Our implementation\nachieves nearly 2 orders magnitude speed up compared to a naive CPU-based\nimplementation, in addition to improved numerical accuracy as well as better\nregistration results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 17:36:57 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Wang", "Jiancong", ""]]}, {"id": "1907.04888", "submitter": "Dheeraj Peri", "authors": "Felipe Petroski Such, Dheeraj Peri, Frank Brockler, Paul Hutkowski,\n  Raymond Ptucha", "title": "Fully Convolutional Networks for Handwriting Recognition", "comments": "Published at International Conference on Frontiers in Handwriting\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Handwritten text recognition is challenging because of the virtually infinite\nways a human can write the same message. Our fully convolutional handwriting\nmodel takes in a handwriting sample of unknown length and outputs an arbitrary\nstream of symbols. Our dual stream architecture uses both local and global\ncontext and mitigates the need for heavy preprocessing steps such as symbol\nalignment correction as well as complex post processing steps such as\nconnectionist temporal classification, dictionary matching or language models.\nUsing over 100 unique symbols, our model is agnostic to Latin-based languages,\nand is shown to be quite competitive with state of the art dictionary based\nmethods on the popular IAM and RIMES datasets. When a dictionary is known, we\nfurther allow a probabilistic character error rate to correct errant word\nblocks. Finally, we introduce an attention based mechanism which can\nautomatically target variants of handwriting, such as slant, stroke width, or\nnoise.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 18:54:27 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Such", "Felipe Petroski", ""], ["Peri", "Dheeraj", ""], ["Brockler", "Frank", ""], ["Hutkowski", "Paul", ""], ["Ptucha", "Raymond", ""]]}, {"id": "1907.04917", "submitter": "Lalitha Giridhar", "authors": "Lalitha Giridhar, Aishwarya Dharani and, Velmathi Guruviah", "title": "A Novel Approach to OCR using Image Recognition based Classification for\n  Ancient Tamil Inscriptions in Temples", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of ancient Tamil characters has always been a challenge for\nepigraphers. This is primarily because the language has evolved over the\nseveral centuries and the character set over this time has both expanded and\ndiversified. This proposed work focuses on improving optical character\nrecognition techniques for ancient Tamil script which was in use between the\n7th and 12th centuries. While comprehensively curating a functional data set\nfor ancient Tamil characters is an arduous task, in this work, a data set has\nbeen curated using cropped images of characters found on certain temple\ninscriptions, specific to this time as a case study. After using Otsu\nthresholding method for binarization of the image a two dimensional convolution\nneural network is defined and used to train, classify and, recognize the\nancient Tamil characters. To implement the optical character recognition\ntechniques, the neural network is linked to the Tesseract using the pytesseract\nlibrary of Python. As an added feature, the work also incorporates Google's\ntext to speech voice engine to produce an audio output of the digitized text.\nVarious samples for both modern and ancient Tamil were collected and passed\nthrough the system. It is found that for Tamil inscriptions studied over the\nconsidered time period, a combined efficiency of 77.7 percent can be achieved.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 05:24:19 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Giridhar", "Lalitha", ""], ["and", "Aishwarya Dharani", ""], ["Guruviah", "Velmathi", ""]]}, {"id": "1907.04957", "submitter": "Jesse Thomason", "authors": "Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer", "title": "Vision-and-Dialog Navigation", "comments": "Conference on Robot Learning (CoRL) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots navigating in human environments should use language to ask for\nassistance and be able to understand human responses. To study this challenge,\nwe introduce Cooperative Vision-and-Dialog Navigation, a dataset of over 2k\nembodied, human-human dialogs situated in simulated, photorealistic home\nenvironments. The Navigator asks questions to their partner, the Oracle, who\nhas privileged access to the best next steps the Navigator should take\naccording to a shortest path planner. To train agents that search an\nenvironment for a goal location, we define the Navigation from Dialog History\ntask. An agent, given a target object and a dialog history between humans\ncooperating to find that object, must infer navigation actions towards the goal\nin unexplored environments. We establish an initial, multi-modal\nsequence-to-sequence model and demonstrate that looking farther back in the\ndialog history improves performance. Sourcecode and a live interface demo can\nbe found at https://cvdn.dev/\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 23:41:46 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 04:07:17 GMT"}, {"version": "v3", "created": "Sun, 13 Oct 2019 02:09:00 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Thomason", "Jesse", ""], ["Murray", "Michael", ""], ["Cakmak", "Maya", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1907.04967", "submitter": "Ye Yuan", "authors": "Ye Yuan, Kris Kitani", "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to forecast a set of likely yet diverse possible future behaviors\nof an agent (e.g., future trajectories of a pedestrian) is essential for\nsafety-critical perception systems (e.g., autonomous vehicles). In particular,\na set of possible future behaviors generated by the system must be diverse to\naccount for all possible outcomes in order to take necessary safety\nprecautions. It is not sufficient to maintain a set of the most likely future\noutcomes because the set may only contain perturbations of a single outcome.\nWhile generative models such as variational autoencoders (VAEs) have been shown\nto be a powerful tool for learning a distribution over future trajectories,\nrandomly drawn samples from the learned implicit likelihood model may not be\ndiverse -- the likelihood model is derived from the training data distribution\nand the samples will concentrate around the major mode that has most data. In\nthis work, we propose to learn a diversity sampling function (DSF) that\ngenerates a diverse and likely set of future trajectories. The DSF maps\nforecasting context features to a set of latent codes which can be decoded by a\ngenerative model (e.g., VAE) into a set of diverse trajectory samples.\nConcretely, the process of identifying the diverse set of samples is posed as a\nparameter estimation of the DSF. To learn the parameters of the DSF, the\ndiversity of the trajectory samples is evaluated by a diversity loss based on a\ndeterminantal point process (DPP). Gradient descent is performed over the DSF\nparameters, which in turn move the latent codes of the sample set to find an\noptimal diverse and likely set of trajectories. Our method is a novel\napplication of DPPs to optimize a set of items (trajectories) in continuous\nspace. We demonstrate the diversity of the trajectories produced by our\napproach on both low-dimensional 2D trajectory data and high-dimensional human\nmotion data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 00:59:22 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 10:23:40 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Yuan", "Ye", ""], ["Kitani", "Kris", ""]]}, {"id": "1907.04975", "submitter": "Joon Son Chung", "authors": "Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman", "title": "My lips are concealed: Audio-visual speech enhancement through\n  obstructions", "comments": "Accepted to Interspeech 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is an audio-visual model for separating a single speaker from a\nmixture of sounds such as other speakers and background noise. Moreover, we\nwish to hear the speaker even when the visual cues are temporarily absent due\nto occlusion. To this end we introduce a deep audio-visual speech enhancement\nnetwork that is able to separate a speaker's voice by conditioning on both the\nspeaker's lip movements and/or a representation of their voice. The voice\nrepresentation can be obtained by either (i) enrollment, or (ii) by\nself-enrollment -- learning the representation on-the-fly given sufficient\nunobstructed visual input. The model is trained by blending audios, and by\nintroducing artificial occlusions around the mouth region that prevent the\nvisual modality from dominating. The method is speaker-independent, and we\ndemonstrate it on real examples of speakers unheard (and unseen) during\ntraining. The method also improves over previous models in particular for cases\nof occlusion in the visual modality.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 02:05:48 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Afouras", "Triantafyllos", ""], ["Chung", "Joon Son", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1907.04978", "submitter": "Jingjing Li", "authors": "Jingjing Li, Mengmeng Jing, Yue Xie, Ke Lu, and Zi Huang", "title": "Agile Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation investigates the problem of leveraging knowledge from a\nwell-labeled source domain to an unlabeled target domain, where the two domains\nare drawn from different data distributions. Because of the distribution\nshifts, different target samples have distinct degrees of difficulty in\nadaptation. However, existing domain adaptation approaches overwhelmingly\nneglect the degrees of difficulty and deploy exactly the same framework for all\nof the target samples. Generally, a simple or shadow framework is fast but\nrough. A sophisticated or deep framework, on the contrary, is accurate but\nslow. In this paper, we aim to challenge the fundamental contradiction between\nthe accuracy and speed in domain adaptation tasks. We propose a novel approach,\nnamed {\\it agile domain adaptation}, which agilely applies optimal frameworks\nto different target samples and classifies the target samples according to\ntheir adaptation difficulties. Specifically, we propose a paradigm which\nperforms several early detections before the final classification. If a sample\ncan be classified at one of the early stage with enough confidence, the sample\nwould exit without the subsequent processes. Notably, the proposed method can\nsignificantly reduce the running cost of domain adaptation approaches, which\ncan extend the application scenarios of domain adaptation to even mobile\ndevices and real-time systems. Extensive experiments on two open benchmarks\nverify the effectiveness and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 02:51:27 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Li", "Jingjing", ""], ["Jing", "Mengmeng", ""], ["Xie", "Yue", ""], ["Lu", "Ke", ""], ["Huang", "Zi", ""]]}, {"id": "1907.04983", "submitter": "Xin Jin", "authors": "Xin Jin, Le Wu, Geng Zhao, Xiaodong Li, Xiaokun Zhang, Shiming Ge,\n  Dongqing Zou, Bin Zhou and Xinghui Zhou", "title": "Aesthetic Attributes Assessment of Images", "comments": "to appear in ACM MM 2019, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image aesthetic quality assessment has been a relatively hot topic during the\nlast decade. Most recently, comments type assessment (aesthetic captions) has\nbeen proposed to describe the general aesthetic impression of an image using\ntext. In this paper, we propose Aesthetic Attributes Assessment of Images,\nwhich means the aesthetic attributes captioning. This is a new formula of image\naesthetic assessment, which predicts aesthetic attributes captions together\nwith the aesthetic score of each attribute. We introduce a new dataset named\n\\emph{DPC-Captions} which contains comments of up to 5 aesthetic attributes of\none image through knowledge transfer from a full-annotated small-scale dataset.\nThen, we propose Aesthetic Multi-Attribute Network (AMAN), which is trained on\na mixture of fully-annotated small-scale PCCD dataset and weakly-annotated\nlarge-scale DPC-Captions dataset. Our AMAN makes full use of transfer learning\nand attention model in a single framework. The experimental results on our\nDPC-Captions and PCCD dataset reveal that our method can predict captions of 5\naesthetic attributes together with numerical score assessment of each\nattribute. We use the evaluation criteria used in image captions to prove that\nour specially designed AMAN model outperforms traditional CNN-LSTM model and\nmodern SCA-CNN model of image captions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 03:25:47 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 16:30:49 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Jin", "Xin", ""], ["Wu", "Le", ""], ["Zhao", "Geng", ""], ["Li", "Xiaodong", ""], ["Zhang", "Xiaokun", ""], ["Ge", "Shiming", ""], ["Zou", "Dongqing", ""], ["Zhou", "Bin", ""], ["Zhou", "Xinghui", ""]]}, {"id": "1907.04988", "submitter": "Xinggang Wang", "authors": "Hao Luo and Lichao Huang and Han Shen and Yuan Li and Chang Huang and\n  Xinggang Wang", "title": "Object Detection in Video with Spatial-temporal Context Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent cutting-edge feature aggregation paradigms for video object detection\nrely on inferring feature correspondence. The feature correspondence estimation\nproblem is fundamentally difficult due to poor image quality, motion blur, etc,\nand the results of feature correspondence estimation are unstable. To avoid the\nproblem, we propose a simple but effective feature aggregation framework which\noperates on the object proposal-level. It learns to enhance each proposal's\nfeature via modeling semantic and spatio-temporal relationships among object\nproposals from both within a frame and across adjacent frames. Experiments are\ncarried out on the ImageNet VID dataset. Without any bells and whistles, our\nmethod obtains 80.3\\% mAP on the ImageNet VID dataset, which is superior over\nthe previous state-of-the-arts. The proposed feature aggregation mechanism\nimproves the single frame Faster RCNN baseline by 5.8% mAP. Besides, under the\nsetting of no temporal post-processing, our method outperforms the previous\nstate-of-the-art by 1.4% mAP.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 03:49:55 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Luo", "Hao", ""], ["Huang", "Lichao", ""], ["Shen", "Han", ""], ["Li", "Yuan", ""], ["Huang", "Chang", ""], ["Wang", "Xinggang", ""]]}, {"id": "1907.05006", "submitter": "Chiwan Song", "authors": "Chiwan Song, Woobin Im, and Sung-eui Yoon", "title": "Two-stream Spatiotemporal Feature for Video QA Task", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding the content of videos is one of the core techniques for\ndeveloping various helpful applications in the real world, such as recognizing\nvarious human actions for surveillance systems or customer behavior analysis in\nan autonomous shop. However, understanding the content or story of the video\nstill remains a challenging problem due to its sheer amount of data and\ntemporal structure. In this paper, we propose a multi-channel neural network\nstructure that adopts a two-stream network structure, which has been shown high\nperformance in human action recognition field, and use it as a spatiotemporal\nvideo feature extractor for solving video question and answering task. We also\nadopt a squeeze-and-excitation structure to two-stream network structure for\nachieving a channel-wise attended spatiotemporal feature. For jointly modeling\nthe spatiotemporal features from video and the textual features from the\nquestion, we design a context matching module with a level adjusting layer to\nremove the gap of information between visual and textual features by applying\nattention mechanism on joint modeling. Finally, we adopt a scoring mechanism\nand smoothed ranking loss objective function for selecting the correct answer\nfrom answer candidates. We evaluate our model with TVQA dataset, and our\napproach shows the improved result in textual only setting, but the result with\nvisual feature shows the limitation and possibility of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 05:51:39 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Song", "Chiwan", ""], ["Im", "Woobin", ""], ["Yoon", "Sung-eui", ""]]}, {"id": "1907.05007", "submitter": "Minchul Shin", "authors": "Minchul Shin, Sanghyuk Park, Taeksoo Kim", "title": "Semi-supervised Feature-Level Attribute Manipulation for Fashion Image\n  Retrieval", "comments": "Accepted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With a growing demand for the search by image, many works have studied the\ntask of fashion instance-level image retrieval (FIR). Furthermore, the recent\nworks introduce a concept of fashion attribute manipulation (FAM) which\nmanipulates a specific attribute (e.g color) of a fashion item while\nmaintaining the rest of the attributes (e.g shape, and pattern). In this way,\nusers can search not only \"the same\" items but also \"similar\" items with the\ndesired attributes. FAM is a challenging task in that the attributes are hard\nto define, and the unique characteristics of a query are hard to be preserved.\nAlthough both FIR and FAM are important in real-life applications, most of the\nprevious studies have focused on only one of these problem. In this study, we\naim to achieve competitive performance on both FIR and FAM. To do so, we\npropose a novel method that converts a query into a representation with the\ndesired attributes. We introduce a new idea of attribute manipulation at the\nfeature level, by matching the distribution of manipulated features with real\nfeatures. In this fashion, the attribute manipulation can be done independently\nfrom learning a representation from the image. By introducing the feature-level\nattribute manipulation, the previous methods for FIR can perform attribute\nmanipulation without sacrificing their retrieval performance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 05:51:49 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Shin", "Minchul", ""], ["Park", "Sanghyuk", ""], ["Kim", "Taeksoo", ""]]}, {"id": "1907.05021", "submitter": "Yujiao Shi", "authors": "Yujiao Shi, Xin Yu, Liu Liu, Tong Zhang, and Hongdong Li", "title": "Optimal Feature Transport for Cross-View Image Geo-Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of cross-view image geo-localization, where\nthe geographic location of a ground-level street-view query image is estimated\nby matching it against a large scale aerial map (e.g., a high-resolution\nsatellite image). State-of-the-art deep-learning based methods tackle this\nproblem as deep metric learning which aims to learn global feature\nrepresentations of the scene seen by the two different views. Despite promising\nresults are obtained by such deep metric learning methods, they, however, fail\nto exploit a crucial cue relevant for localization, namely, the spatial layout\nof local features. Moreover, little attention is paid to the obvious domain gap\n(between aerial view and ground view) in the context of cross-view\nlocalization. This paper proposes a novel Cross-View Feature Transport (CVFT)\ntechnique to explicitly establish cross-view domain transfer that facilitates\nfeature alignment between ground and aerial images. Specifically, we implement\nthe CVFT as network layers, which transports features from one domain to the\nother, leading to more meaningful feature similarity comparison. Our model is\ndifferentiable and can be learned end-to-end. Experiments on large-scale\ndatasets have demonstrated that our method has remarkably boosted the\nstate-of-the-art cross-view localization performance, e.g., on the CVUSA\ndataset, with significant improvements for top-1 recall from 40.79% to 61.43%,\nand for top-10 from 76.36% to 90.49%. We expect the key insight of the paper\n(i.e., explicitly handling domain difference via domain transport) will prove\nto be useful for other similar problems in computer vision as well.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 06:56:40 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 05:41:46 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 05:53:04 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Shi", "Yujiao", ""], ["Yu", "Xin", ""], ["Liu", "Liu", ""], ["Zhang", "Tong", ""], ["Li", "Hongdong", ""]]}, {"id": "1907.05023", "submitter": "Yante Li", "authors": "Yante Li, Xiaohua Huang, Guoying Zhao", "title": "Micro-expression Action Unit Detection with Spatio-temporal Adaptive\n  Pooling", "comments": "There is a bug in the method. The results are not correct", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Action Unit (AU) detection plays an important role for facial expression\nrecognition. To the best of our knowledge, there is little research about AU\nanalysis for micro-expressions. In this paper, we focus on AU detection in\nmicro-expressions. Microexpression AU detection is challenging due to the small\nquantity of micro-expression databases, low intensity, short duration of facial\nmuscle change, and class imbalance. In order to alleviate the problems, we\npropose a novel Spatio-Temporal Adaptive Pooling (STAP) network for AU\ndetection in micro-expressions. Firstly, STAP is aggregated by a series of\nconvolutional filters of different sizes. In this way, STAP can obtain\nmulti-scale information on spatial and temporal domains. On the other hand,\nSTAP contains less parameters, thus it has less computational cost and is\nsuitable for micro-expression AU detection on very small databases.\nFurthermore, STAP module is designed to pool discriminative information for\nmicro-expression AUs on spatial and temporal domains.Finally, Focal loss is\nemployed to prevent the vast number of negatives from overwhelming the\nmicroexpression AU detector. In experiments, we firstly polish the AU\nannotations on three commonly used databases. We conduct intensive experiments\non three micro-expression databases, and provide several baseline results on\nmicro-expression AU detection. The results show that our proposed approach\noutperforms the basic Inflated inception-v1 (I3D) in terms of an average of F1-\nscore. We also evaluate the performance of our proposed method on\ncross-database protocol. It demonstrates that our proposed approach is feasible\nfor cross-database micro-expression AU detection. Importantly, the results on\nthree micro-expression databases and cross-database protocol provide extensive\nbaseline results for future research on micro-expression AU detection.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 07:00:47 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 06:13:38 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Li", "Yante", ""], ["Huang", "Xiaohua", ""], ["Zhao", "Guoying", ""]]}, {"id": "1907.05036", "submitter": "Daigo Okada", "authors": "Daigo Okada, Naotoshi Nakamura, Takuya Wada, Ayako Iwasaki, and Ryo\n  Yamada", "title": "Extension of Sinkhorn Method: Optimal Movement Estimation of Agents\n  Moving at Constant Velocity", "comments": "12 pages, 7 figures, 2 tables", "journal-ref": "Transactions of the Japanese Society for Artificial Intelligence\n  34.5 (2019) D-J13_1-7", "doi": "10.1527/tjsai.D-J13", "report-no": null, "categories": "eess.IV cs.CV q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of bioimaging, an important part of analyzing the motion of\nobjects is tracking. We propose a method that applies the Sinkhorn distance for\nsolving the optimal transport problem to track objects. The advantage of this\nmethod is that it can flexibly incorporate various assumptions in tracking as a\ncost matrix. First, we extend the Sinkhorn distance from two dimensions to\nthree dimensions. Using this three-dimensional distance, we compare the\nperformance of two types of tracking technique, namely tracking that associates\nobjects that are close to each other, which conventionally uses the\nnearest-neighbor method, and tracking that assumes that the object is moving at\nconstant velocity, using three types of simulation data. The results suggest\nthat when tracking objects moving at constant velocity, our method is superior\nto conventional nearest-neighbor tracking as long as the added noise is not\nexcessively large. We show that the Sinkhorn method can be applied effectively\nto object tracking. Our simulation data analysis suggests that when objects are\nmoving at constant velocity, our method, which sets acceleration as a cost,\noutperforms the traditional nearest-neighbor method in terms of tracking\nobjects. To apply the proposed method to real bioimaging data, it is necessary\nto set an appropriate cost indicator based on the movement features.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 08:01:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Okada", "Daigo", ""], ["Nakamura", "Naotoshi", ""], ["Wada", "Takuya", ""], ["Iwasaki", "Ayako", ""], ["Yamada", "Ryo", ""]]}, {"id": "1907.05047", "submitter": "Yury Kartynnik", "authors": "Valentin Bazarevsky, Yury Kartynnik, Andrey Vakunov, Karthik\n  Raveendran, Matthias Grundmann", "title": "BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs", "comments": "4 pages, 3 figures; CVPR Workshop on Computer Vision for Augmented\n  and Virtual Reality, Long Beach, CA, USA, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BlazeFace, a lightweight and well-performing face detector\ntailored for mobile GPU inference. It runs at a speed of 200-1000+ FPS on\nflagship devices. This super-realtime performance enables it to be applied to\nany augmented reality pipeline that requires an accurate facial region of\ninterest as an input for task-specific models, such as 2D/3D facial keypoint or\ngeometry estimation, facial features or expression classification, and face\nregion segmentation. Our contributions include a lightweight feature extraction\nnetwork inspired by, but distinct from MobileNetV1/V2, a GPU-friendly anchor\nscheme modified from Single Shot MultiBox Detector (SSD), and an improved tie\nresolution strategy alternative to non-maximum suppression.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 08:40:08 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 11:12:28 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bazarevsky", "Valentin", ""], ["Kartynnik", "Yury", ""], ["Vakunov", "Andrey", ""], ["Raveendran", "Karthik", ""], ["Grundmann", "Matthias", ""]]}, {"id": "1907.05062", "submitter": "Chengjia Wang", "authors": "Chengjia Wang, Giorgos Papanastasiou, Agisilaos Chartsias, Grzegorz\n  Jacenkow, Sotirios A. Tsaftaris, and Heye Zhang", "title": "FIRE: Unsupervised bi-directional inter-modality registration using deep\n  networks", "comments": "We submitted this paper to a top medical imaging conference,\n  srebuttal responded by the meta-reviewer. We were told that this work is not\n  important and will not have big impact as the \"reviewers were not\n  enthusiastic\". Here I publish the paper online for an open discussion. I will\n  publish the code, the pre-trained model, the results, especially the reviews,\n  and the meta reviews on github", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-modality image registration is an critical preprocessing step for many\napplications within the routine clinical pathway. This paper presents an\nunsupervised deep inter-modality registration network that can learn the\noptimal affine and non-rigid transformations simultaneously.\nInverse-consistency is an important property commonly ignored in recent deep\nlearning based inter-modality registration algorithms. We address this issue\nthrough the proposed multi-task architecture and the new comprehensive\ntransformation network. Specifically, the proposed model learns a\nmodality-independent latent representation to perform cycle-consistent\ncross-modality synthesis, and use an inverse-consistent loss to learn a pair of\ntransformations to align the synthesized image with the target. We name this\nproposed framework as FIRE due to the shape of its structure. Our method shows\ncomparable and better performances with the popular baseline method in\nexperiments on multi-sequence brain MR data and intra-modality 4D cardiac\nCine-MR data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 09:00:54 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wang", "Chengjia", ""], ["Papanastasiou", "Giorgos", ""], ["Chartsias", "Agisilaos", ""], ["Jacenkow", "Grzegorz", ""], ["Tsaftaris", "Sotirios A.", ""], ["Zhang", "Heye", ""]]}, {"id": "1907.05084", "submitter": "David Schlangen", "authors": "Nikolai Ilinykh, Sina Zarrie{\\ss}, David Schlangen", "title": "MeetUp! A Corpus of Joint Activity Dialogues in a Visual Environment", "comments": "In Proceedings of the 23rd Workshop on the Semantics and Pragmatics\n  of Dialogue (semdial / LondonLogue), London, September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building computer systems that can converse about their visual environment is\none of the oldest concerns of research in Artificial Intelligence and\nComputational Linguistics (see, for example, Winograd's 1972 SHRDLU system).\nOnly recently, however, have methods from computer vision and natural language\nprocessing become powerful enough to make this vision seem more attainable.\nPushed especially by developments in computer vision, many data sets and\ncollection environments have recently been published that bring together verbal\ninteraction and visual processing. Here, we argue that these datasets tend to\noversimplify the dialogue part, and we propose a task---MeetUp!---that requires\nboth visual and conversational grounding, and that makes stronger demands on\nrepresentations of the discourse. MeetUp! is a two-player coordination game\nwhere players move in a visual environment, with the objective of finding each\nother. To do so, they must talk about what they see, and achieve mutual\nunderstanding. We describe a data collection and show that the resulting\ndialogues indeed exhibit the dialogue phenomena of interest, while also\nchallenging the language & vision aspect.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:06:20 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Ilinykh", "Nikolai", ""], ["Zarrie\u00df", "Sina", ""], ["Schlangen", "David", ""]]}, {"id": "1907.05089", "submitter": "Aleksei Tiulpin", "authors": "Aleksei Tiulpin, Mikko Finnil\\\"a, Petri Lehenkari, Heikki J. Nieminen,\n  Simo Saarakkala", "title": "Deep-Learning for Tidemark Segmentation in Human Osteochondral Tissues\n  Imaged with Micro-computed Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional (3D) semi-quantitative grading of pathological features in\narticular cartilage (AC) offers significant improvements in basic research of\nosteoarthritis (OA). We have earlier developed the 3D protocol for imaging of\nAC and its structures which includes staining of the sample with a contrast\nagent (phosphotungstic acid, PTA) and a consequent scanning with micro-computed\ntomography. Such a protocol was designed to provide X-ray attenuation contrast\nto visualize AC structure. However, at the same time, this protocol has one\nmajor disadvantage: the loss of contrast at the tidemark (calcified cartilage\ninterface, CCI). An accurate segmentation of CCI can be very important for\nunderstanding the etiology of OA and ex-vivo evaluation of tidemark condition\nat early OA stages. In this paper, we present the first application of Deep\nLearning to PTA-stained osteochondral samples that allows to perform tidemark\nsegmentation in a fully-automatic manner. Our method is based on U-Net trained\nusing a combination of binary cross-entropy and soft Jaccard loss. On\ncross-validation, this approach yielded intersection over the union of 0.59,\n0.70, 0.79, 0.83 and 0.86 within 15 {\\mu}m, 30 {\\mu}m, 45 {\\mu}m, 60 {\\mu}m and\n75 {\\mu}m padded zones around the tidemark, respectively. Our codes and the\ndataset that consisted of 35 PTA-stained human AC samples are made publicly\navailable together with the segmentation masks to facilitate the development of\nbiomedical image segmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:20:50 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Tiulpin", "Aleksei", ""], ["Finnil\u00e4", "Mikko", ""], ["Lehenkari", "Petri", ""], ["Nieminen", "Heikki J.", ""], ["Saarakkala", "Simo", ""]]}, {"id": "1907.05091", "submitter": "Anbang Yao", "authors": "Jiahui Zhang, Hao Zhao, Anbang Yao, Yurong Chen, Li Zhang, Hongen Liao", "title": "Efficient Semantic Scene Completion Network with Spatial Group\n  Convolution", "comments": "An oral paper in ECCV 2018, and the code is available at\n  https://github.com/zjhthu/SGC-Release.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Spatial Group Convolution (SGC) for accelerating the computation\nof 3D dense prediction tasks. SGC is orthogonal to group convolution, which\nworks on spatial dimensions rather than feature channel dimension. It divides\ninput voxels into different groups, then conducts 3D sparse convolution on\nthese separated groups. As only valid voxels are considered when performing\nconvolution, computation can be significantly reduced with a slight loss of\naccuracy. The proposed operations are validated on semantic scene completion\ntask, which aims to predict a complete 3D volume with semantic labels from a\nsingle depth image. With SGC, we further present an efficient 3D sparse\nconvolutional network, which harnesses a multiscale architecture and a\ncoarse-to-fine prediction strategy. Evaluations are conducted on the SUNCG\ndataset, achieving state-of-the-art performance and fast speed. Code is\navailable at https://github.com/zjhthu/SGC-Release.git\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:29:03 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Zhang", "Jiahui", ""], ["Zhao", "Hao", ""], ["Yao", "Anbang", ""], ["Chen", "Yurong", ""], ["Zhang", "Li", ""], ["Liao", "Hongen", ""]]}, {"id": "1907.05092", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Yuqing Song, Yida Zhao, Qin Jin, Zhaoyang Zeng, Bei Liu,\n  Jianlong Fu, and Alexander Hauptmann", "title": "Activitynet 2019 Task 3: Exploring Contexts for Dense Captioning Events\n  in Videos", "comments": "Winner solution in CVPR 2019 Activitynet Dense Video Captioning\n  challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual reasoning is essential to understand events in long untrimmed\nvideos. In this work, we systematically explore different captioning models\nwith various contexts for the dense-captioning events in video task, which aims\nto generate captions for different events in the untrimmed video. We propose\nfive types of contexts as well as two categories of event captioning models,\nand evaluate their contributions for event captioning from both accuracy and\ndiversity aspects. The proposed captioning models are plugged into our pipeline\nsystem for the dense video captioning challenge. The overall system achieves\nthe state-of-the-art performance on the dense-captioning events in video task\nwith 9.91 METEOR score on the challenge testing set.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:29:04 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Chen", "Shizhe", ""], ["Song", "Yuqing", ""], ["Zhao", "Yida", ""], ["Jin", "Qin", ""], ["Zeng", "Zhaoyang", ""], ["Liu", "Bei", ""], ["Fu", "Jianlong", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1907.05099", "submitter": "Xiangyang Li", "authors": "Xiangyang Li, Luis Herranz, and Shuqiang Jiang", "title": "Multifaceted Analysis of Fine-Tuning in Deep Model for Visual\n  Recognition", "comments": "Accepted by ACM Transactions on Data Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, convolutional neural networks (CNNs) have achieved\nimpressive performance for various visual recognition scenarios. CNNs trained\non large labeled datasets can not only obtain significant performance on most\nchallenging benchmarks but also provide powerful representations, which can be\nused to a wide range of other tasks. However, the requirement of massive\namounts of data to train deep neural networks is a major drawback of these\nmodels, as the data available is usually limited or imbalanced. Fine-tuning\n(FT) is an effective way to transfer knowledge learned in a source dataset to a\ntarget task. In this paper, we introduce and systematically investigate several\nfactors that influence the performance of fine-tuning for visual recognition.\nThese factors include parameters for the retraining procedure (e.g., the\ninitial learning rate of fine-tuning), the distribution of the source and\ntarget data (e.g., the number of categories in the source dataset, the distance\nbetween the source and target datasets) and so on. We quantitatively and\nqualitatively analyze these factors, evaluate their influence, and present many\nempirical observations. The results reveal insights into what fine-tuning\nchanges CNN parameters and provide useful and evidence-backed intuitions about\nhow to implement fine-tuning for computer vision tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 10:47:49 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Li", "Xiangyang", ""], ["Herranz", "Luis", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "1907.05112", "submitter": "Max Frei", "authors": "Max Frei and Frank Einar Kruis", "title": "Image-Based Size Analysis of Agglomerated and Partially Sintered\n  Particles via Convolutional Neural Networks", "comments": "17 pages, 20 figures, 4 tables; supplementary materials", "journal-ref": null, "doi": "10.1016/j.powtec.2019.10.020", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a high demand for fully automated methods for the analysis of\nprimary particle size distributions of agglomerated, sintered or occluded\nprimary particles, due to their impact on material properties. Therefore, a\nnovel, deep learning-based, method for the detection of such primary particles\nwas proposed and tested, which renders a manual tuning of analysis parameters\nunnecessary. As a specialty, the training of the utilized convolutional neural\nnetworks was carried out using only synthetic images, thereby avoiding the\nlaborious task of manual annotation and increasing the ground truth quality.\nNevertheless, the proposed method performs excellent on real world samples of\nsintered silica nanoparticles with various sintering degrees and varying image\nconditions. In a direct comparison, the proposed method clearly outperforms two\nstate-of-the-art methods for automated image-based particle size analysis\n(Hough transformation and the ImageJ ParticleSizer plug-in), thereby attaining\nhuman-like performance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 11:12:17 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 20:43:35 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2019 22:42:13 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Frei", "Max", ""], ["Kruis", "Frank Einar", ""]]}, {"id": "1907.05143", "submitter": "Melanie Lubrano", "authors": "Melanie Lubrano di Scandalea, Christian S. Perone, Mathieu Boudreau,\n  Julien Cohen-Adad", "title": "Deep Active Learning for Axon-Myelin Segmentation on Histology Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a crucial task in biomedical image processing, which\nrecent breakthroughs in deep learning have allowed to improve. However, deep\nlearning methods in general are not yet widely used in practice since they\nrequire large amount of data for training complex models. This is particularly\nchallenging for biomedical images, because data and ground truths are a scarce\nresource. Annotation efforts for biomedical images come with a real cost, since\nexperts have to manually label images at pixel-level on samples usually\ncontaining many instances of the target anatomy (e.g. in histology samples:\nneurons, astrocytes, mitochondria, etc.). In this paper we provide a framework\nfor Deep Active Learning applied to a real-world scenario. Our framework relies\non the U-Net architecture and overall uncertainty measure to suggest which\nsample to annotate. It takes advantage of the uncertainty measure obtained by\ntaking Monte Carlo samples while using Dropout regularization scheme.\nExperiments were done on spinal cord and brain microscopic histology samples to\nperform a myelin segmentation task. Two realistic small datasets of 14 and 24\nimages were used, from different acquisition settings (Serial Block-Face\nElectron Microscopy and Transmitting Electron Microscopy) and showed that our\nmethod reached a maximum Dice value after adding 3 uncertainty-selected samples\nto the initial training set, versus 15 randomly-selected samples, thereby\nsignificantly reducing the annotation effort. We focused on a plausible\nscenario and showed evidence that this straightforward implementation achieves\na high segmentation performance with very few labelled samples. We believe our\nframework may benefit any biomedical researcher willing to obtain fast and\naccurate image segmentation on their own dataset. The code is freely available\nat https://github.com/neuropoly/deep-active-learning.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 12:31:30 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["di Scandalea", "Melanie Lubrano", ""], ["Perone", "Christian S.", ""], ["Boudreau", "Mathieu", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "1907.05164", "submitter": "Mark Graham", "authors": "Kanwal K. Bhatia, Mark S. Graham, Louise Terry, Ashley Wood, Paris\n  Tranos, Sameer Trikha, Nicolas Jaccard", "title": "Disease classification of macular Optical Coherence Tomography scans\n  using deep learning software: validation on independent, multi-centre data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To evaluate Pegasus-OCT, a clinical decision support software for\nthe identification of features of retinal disease from macula OCT scans, across\nheterogenous populations involving varying patient demographics, device\nmanufacturers, acquisition sites and operators.\n  Methods: 5,588 normal and anomalous macular OCT volumes (162,721 B-scans),\nacquired at independent centres in five countries, were processed using the\nsoftware. Results were evaluated against ground truth provided by the dataset\nowners.\n  Results: Pegasus-OCT performed with AUROCs of at least 98% for all datasets\nin the detection of general macular anomalies. For scans of sufficient quality,\nthe AUROCs for general AMD and DME detection were found to be at least 99% and\n98%, respectively.\n  Conclusions: The ability of a clinical decision support system to cater for\ndifferent populations is key to its adoption. Pegasus-OCT was shown to be able\nto detect AMD, DME and general anomalies in OCT volumes acquired across\nmultiple independent sites with high performance. Its use thus offers\nsubstantial promise, with the potential to alleviate the burden of growing\ndemand in eye care services caused by retinal disease.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 12:53:17 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Bhatia", "Kanwal K.", ""], ["Graham", "Mark S.", ""], ["Terry", "Louise", ""], ["Wood", "Ashley", ""], ["Tranos", "Paris", ""], ["Trikha", "Sameer", ""], ["Jaccard", "Nicolas", ""]]}, {"id": "1907.05185", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Zhenfeng Zhu, Jian Cheng, Yandong Guo, and Yao Zhao", "title": "Edge Heuristic GAN for Non-uniform Blind Deblurring", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": "10.1109/LSP.2019.2939752", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-uniform blur, mainly caused by camera shake and motions of multiple\nobjects, is one of the most common causes of image quality degradation.\nHowever, the traditional blind deblurring methods based on blur kernel\nestimation do not perform well on complicated non-uniform motion blurs. Recent\nstudies show that GAN-based approaches achieve impressive performance on\ndeblurring tasks. In this letter, to further improve the performance of\nGAN-based methods on deblurring tasks, we propose an edge heuristic multi-scale\ngenerative adversarial network(GAN), which uses the \"coarse-to-fine\" scheme to\nrestore clear images in an end-to-end manner. In particular, an edge-enhanced\nnetwork is designed to generate sharp edges as auxiliary information to guide\nthe deblurring process. Furthermore, We propose a hierarchical content loss\nfunction for deblurring tasks. Extensive experiments on different datasets show\nthat our method achieves state-of-the-art performance in dynamic scene\ndeblurring.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 13:33:34 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Zheng", "Shuai", ""], ["Zhu", "Zhenfeng", ""], ["Cheng", "Jian", ""], ["Guo", "Yandong", ""], ["Zhao", "Yao", ""]]}, {"id": "1907.05193", "submitter": "Kevin Lin", "authors": "Kevin Lin, Lijuan Wang, Kun Luo, Yinpeng Chen, Zicheng Liu, Ming-Ting\n  Sun", "title": "Cross-Domain Complementary Learning Using Pose for Multi-Person Part\n  Segmentation", "comments": "To appear in IEEE Transactions on Circuits and Systems for Video\n  Technology; Presented at ICCV 2019 Demonstration", "journal-ref": null, "doi": "10.1109/TCSVT.2020.2995122", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning with pixel-wise training labels has great successes\non multi-person part segmentation. However, data labeling at pixel-level is\nvery expensive. To solve the problem, people have been exploring to use\nsynthetic data to avoid the data labeling. Although it is easy to generate\nlabels for synthetic data, the results are much worse compared to those using\nreal data and manual labeling. The degradation of the performance is mainly due\nto the domain gap, i.e., the discrepancy of the pixel value statistics between\nreal and synthetic data. In this paper, we observe that real and synthetic\nhumans both have a skeleton (pose) representation. We found that the skeletons\ncan effectively bridge the synthetic and real domains during the training. Our\nproposed approach takes advantage of the rich and realistic variations of the\nreal data and the easily obtainable labels of the synthetic data to learn\nmulti-person part segmentation on real images without any human-annotated\nlabels. Through experiments, we show that without any human labeling, our\nmethod performs comparably to several state-of-the-art approaches which require\nhuman labeling on Pascal-Person-Parts and COCO-DensePose datasets. On the other\nhand, if part labels are also available in the real-images during training, our\nmethod outperforms the supervised state-of-the-art methods by a large margin.\nWe further demonstrate the generalizability of our method on predicting novel\nkeypoints in real images where no real data labels are available for the novel\nkeypoints detection. Code and pre-trained models are available at\nhttps://github.com/kevinlin311tw/CDCL-human-part-segmentation\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 13:47:18 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 00:22:29 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Lin", "Kevin", ""], ["Wang", "Lijuan", ""], ["Luo", "Kun", ""], ["Chen", "Yinpeng", ""], ["Liu", "Zicheng", ""], ["Sun", "Ming-Ting", ""]]}, {"id": "1907.05269", "submitter": "Leszek Pecyna", "authors": "Leszek Pecyna, Angelo Cangelosi", "title": "Influence of Pointing on Learning to Count: A Neuro-Robotics Model", "comments": "8 pages, 5 figures. In Proceedings of the 2018 IEEE Symposium Series\n  on Computational Intelligence (SSCI) (pp. 358-365). IEEE", "journal-ref": null, "doi": "10.1109/SSCI.2018.8628811", "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a neuro-robotics model capable of counting using gestures is\nintroduced. The contribution of gestures to learning to count is tested with\nvarious model and training conditions. Two studies were presented in this\narticle. In the first, we combine different modalities of the robot's neural\nnetwork, in the second, a novel training procedure for it is proposed. The\nmodel is trained with pointing data from an iCub robot simulator. The behaviour\nof the model is in line with that of human children in terms of performance\nchange depending on gesture production.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:59:36 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Pecyna", "Leszek", ""], ["Cangelosi", "Angelo", ""]]}, {"id": "1907.05270", "submitter": "Leszek Pecyna", "authors": "Leszek Pecyna, Angelo Cangelosi, Alessandro Di Nuovo", "title": "A Deep Neural Network for Finger Counting and Numerosity Estimation", "comments": "8 pages, accepted and presented on a conference. In Proceedings of\n  the 2019 IEEE Symposium Series on Computational Intelligence (SSCI)", "journal-ref": "2019 IEEE Symposium Series on Computational Intelligence", "doi": "10.1109/SSCI44817.2019.9002694", "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present neuro-robotics models with a deep artificial neural\nnetwork capable of generating finger counting positions and number estimation.\nWe first train the model in an unsupervised manner where each layer is treated\nas a Restricted Boltzmann Machine or an autoencoder. Such a model is further\ntrained in a supervised way. This type of pre-training is tested on our\nbaseline model and two methods of pre-training are compared. The network is\nextended to produce finger counting positions. The performance in number\nestimation of such an extended model is evaluated. We test the hypothesis if\nthe subitizing process can be obtained by one single model used also for\nestimation of higher numerosities. The results confirm the importance of\nunsupervised training in our enumeration task and show some similarities to\nhuman behaviour in the case of subitizing.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 13:10:28 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 17:33:53 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Pecyna", "Leszek", ""], ["Cangelosi", "Angelo", ""], ["Di Nuovo", "Alessandro", ""]]}, {"id": "1907.05271", "submitter": "Yang Wang", "authors": "Biao Qian, Yang Wang", "title": "A Targeted Acceleration and Compression Framework for Low bit Neural\n  Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  1 bit deep neural networks (DNNs), of which both the activations and weights\nare binarized , are attracting more and more attention due to their high\ncomputational efficiency and low memory requirement . However, the drawback of\nlarge accuracy dropping also restrict s its application. In this paper, we\npropose a novel Targeted Acceleration and Compression (TAC) framework to\nimprove the performance of 1 bit deep neural networks W e consider that the\nacceleration and compression effects of binarizing fully connected layer s are\nnot sufficient to compensate for the accuracy loss caused by it In the proposed\nframework, t he convolutional and fully connected layer are separated and\noptimized i ndividually . F or the convolutional layer s , both the activations\nand weights are binarized. For the fully connected layer s, the binarization\noperation is re placed by network pruning and low bit quantization. The\nproposed framework is implemented on the CIFAR 10, CIFAR 100 and ImageNet (\nILSVRC 12 ) datasets , and experimental results show that the proposed TAC can\nsignificantly improve the accuracy of 1 bit deep neural networks and\noutperforms the state of the art by more than 6 percentage points .\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 04:09:55 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Qian", "Biao", ""], ["Wang", "Yang", ""]]}, {"id": "1907.05272", "submitter": "Yoli Shavit", "authors": "Yoli Shavit and Ron Ferens", "title": "Introduction to Camera Pose Estimation with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, deep learning has transformed the field of\ncomputer vision. Deep convolutional networks were successfully applied to learn\ndifferent vision tasks such as image classification, image segmentation, object\ndetection and many more. By transferring the knowledge learned by deep models\non large generic datasets, researchers were further able to create fine-tuned\nmodels for other more specific tasks. Recently this idea was applied for\nregressing the absolute camera pose from an RGB image. Although the resulting\naccuracy was sub-optimal, compared to classic feature-based solutions, this\neffort led to a surge of learning-based pose estimation methods. Here, we\nreview deep learning approaches for camera pose estimation. We describe key\nmethods in the field and identify trends aiming at improving the original deep\npose regression solution. We further provide an extensive cross-comparison of\nexisting learning-based pose estimators, together with practical notes on their\nexecution for reproducibility purposes. Finally, we discuss emerging solutions\nand potential future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 14:43:06 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 07:48:14 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 04:18:26 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Shavit", "Yoli", ""], ["Ferens", "Ron", ""]]}, {"id": "1907.05273", "submitter": "Xiaowei Xu", "authors": "Xiaowei Xu, Tianchen Wang, Dewen Zeng, Yiyu Shi, Qianjun Jia, Haiyun\n  Yuan, Meiping Huang, Jian Zhuang", "title": "Accurate Congenital Heart Disease Model Generation for 3D Printing", "comments": "6 figures, 2 tables, accepted by the IEEE International Workshop on\n  Signal Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D printing has been widely adopted for clinical decision making and\ninterventional planning of Congenital heart disease (CHD), while whole heart\nand great vessel segmentation is the most significant but time-consuming step\nin the model generation for 3D printing. While various automatic whole heart\nand great vessel segmentation frameworks have been developed in the literature,\nthey are ineffective when applied to medical images in CHD, which have\nsignificant variations in heart structure and great vessel connections. To\naddress the challenge, we leverage the power of deep learning in processing\nregular structures and that of graph algorithms in dealing with large\nvariations and propose a framework that combines both for whole heart and great\nvessel segmentation in CHD. Particularly, we first use deep learning to segment\nthe four chambers and myocardium followed by the blood pool, where variations\nare usually small. We then extract the connection information and apply graph\nmatching to determine the categories of all the vessels. Experimental results\nusing 683D CT images covering 14 types of CHD show that our method can increase\nDice score by 11.9% on average compared with the state-of-the-art whole heart\nand great vessel segmentation method in normal anatomy. The segmentation\nresults are also printed out using 3D printers for validation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 15:15:06 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 03:11:44 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Xu", "Xiaowei", ""], ["Wang", "Tianchen", ""], ["Zeng", "Dewen", ""], ["Shi", "Yiyu", ""], ["Jia", "Qianjun", ""], ["Yuan", "Haiyun", ""], ["Huang", "Meiping", ""], ["Zhuang", "Jian", ""]]}, {"id": "1907.05274", "submitter": "Letao Liu", "authors": "Letao Liu, Martin Saerbeck, Justin Dauwels", "title": "Affine Disentangled GAN for Interpretable and Robust AV Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles (AV) have progressed rapidly with the advancements in\ncomputer vision algorithms. The deep convolutional neural network as the main\ncontributor to this advancement has boosted the classification accuracy\ndramatically. However, the discovery of adversarial examples reveals the\ngeneralization gap between dataset and the real world. Furthermore, affine\ntransformations may also confuse computer vision based object detectors. The\ndegradation of the perception system is undesirable for safety critical systems\nsuch as autonomous vehicles. In this paper, a deep learning system is proposed:\nAffine Disentangled GAN (ADIS-GAN), which is robust against affine\ntransformations and adversarial attacks. It is demonstrated that conventional\ndata augmentation for affine transformation and adversarial attacks are\northogonal, while ADIS-GAN can handle both attacks at the same time. Useful\ninformation such as image rotation angle and scaling factor are also generated\nin ADIS-GAN. On MNIST dataset, ADIS-GAN can achieve over 98 percent\nclassification accuracy within 30 degrees rotation, and over 90 percent\nclassification accuracy against FGSM and PGD adversarial attack.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 04:53:49 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Liu", "Letao", ""], ["Saerbeck", "Martin", ""], ["Dauwels", "Justin", ""]]}, {"id": "1907.05275", "submitter": "Yaohua Xie", "authors": "Yaohua Xie", "title": "Improving the resolution of microscope by deconvolution after dense scan", "comments": "This work has been patented, thereby is not open-source", "journal-ref": null, "doi": "10.5281/zenodo.3353772", "report-no": null, "categories": "eess.IV cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution microscopes (such as STED) illuminate samples with a tiny\nspot, and achieve very high resolution. But structures smaller than the spot\ncannot be resolved in this way. Therefore, we propose a technique to solve this\nproblem. It is termed \"Deconvolution after Dense Scan (DDS)\". First, a\npreprocessing stage is introduced to eliminate the optical uncertainty of the\nperipheral areas around the sample's ROI (Region of Interest). Then, the ROI is\nscanned densely together with its peripheral areas. Finally, the high\nresolution image is recovered by deconvolution. The proposed technique does not\nneed to modify the apparatus much, and is mainly performed by algorithm.\nSimulation experiments show that the technique can further improve the\nresolution of super-resolution microscopes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 03:35:50 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 01:57:19 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 03:05:19 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Xie", "Yaohua", ""]]}, {"id": "1907.05276", "submitter": "Matt Groh", "authors": "Matthew Groh, Ziv Epstein, Nick Obradovich, Manuel Cebrian, Iyad\n  Rahwan", "title": "Human detection of machine manipulated media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural networks for content generation enable artificial\nintelligence (AI) models to generate high-quality media manipulations. Here we\nreport on a randomized experiment designed to study the effect of exposure to\nmedia manipulations on over 15,000 individuals' ability to discern\nmachine-manipulated media. We engineer a neural network to plausibly and\nautomatically remove objects from images, and we deploy this neural network\nonline with a randomized experiment where participants can guess which image\nout of a pair of images has been manipulated. The system provides participants\nfeedback on the accuracy of each guess. In the experiment, we randomize the\norder in which images are presented, allowing causal identification of the\nlearning curve surrounding participants' ability to detect fake content. We\nfind sizable and robust evidence that individuals learn to detect fake content\nthrough exposure to manipulated media when provided iterative feedback on their\ndetection attempts. Over a succession of only ten images, participants increase\ntheir rating accuracy by over ten percentage points. Our study provides initial\nevidence that human ability to detect fake, machine-generated content may\nincrease alongside the prevalence of such media online.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 02:52:42 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 15:22:26 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Groh", "Matthew", ""], ["Epstein", "Ziv", ""], ["Obradovich", "Nick", ""], ["Cebrian", "Manuel", ""], ["Rahwan", "Iyad", ""]]}, {"id": "1907.05277", "submitter": "Elisabeth Hoppe", "authors": "Elisabeth Hoppe (1), Florian Thamm (1), Gregor K\\\"orzd\\\"orfer (2),\n  Christopher Syben (1), Franziska Schirrmacher (1), Mathias Nittka (2), Josef\n  Pfeuffer (2), Heiko Meyer (2) and Andreas Maier (1) ((1) Pattern Recognition\n  Lab, Department of Computer Science, Friedrich-Alexander-Universit\\\"at\n  Erlangen-N\\\"urnberg, Erlangen, Germany, (2) MR Application Development,\n  Siemens Healthcare, Erlangen, Germany)", "title": "RinQ Fingerprinting: Recurrence-informed Quantile Networks for Magnetic\n  Resonance Fingerprinting", "comments": "Accepted for MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Magnetic Resonance Fingerprinting (MRF) was proposed as a\nquantitative imaging technique for the simultaneous acquisition of tissue\nparameters such as relaxation times $T_1$ and $T_2$. Although the acquisition\nis highly accelerated, the state-of-the-art reconstruction suffers from long\ncomputation times: Template matching methods are used to find the most similar\nsignal to the measured one by comparing it to pre-simulated signals of possible\nparameter combinations in a discretized dictionary. Deep learning approaches\ncan overcome this limitation, by providing the direct mapping from the measured\nsignal to the underlying parameters by one forward pass through a network. In\nthis work, we propose a Recurrent Neural Network (RNN) architecture in\ncombination with a novel quantile layer. RNNs are well suited for the\nprocessing of time-dependent signals and the quantile layer helps to overcome\nthe noisy outliers by considering the spatial neighbors of the signal. We\nevaluate our approach using in-vivo data from multiple brain slices and several\nvolunteers, running various experiments. We show that the RNN approach with\nsmall patches of complex-valued input signals in combination with a quantile\nlayer outperforms other architectures, e.g. previously proposed CNNs for the\nMRF reconstruction reducing the error in $T_1$ and $T_2$ by more than 80%.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 10:29:55 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 13:06:51 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Hoppe", "Elisabeth", ""], ["Thamm", "Florian", ""], ["K\u00f6rzd\u00f6rfer", "Gregor", ""], ["Syben", "Christopher", ""], ["Schirrmacher", "Franziska", ""], ["Nittka", "Mathias", ""], ["Pfeuffer", "Josef", ""], ["Meyer", "Heiko", ""], ["Maier", "Andreas", ""]]}, {"id": "1907.05278", "submitter": "Youssef Mourchid", "authors": "Youssef Mourchid, Mohammed El Hassouni, Hocine Cherifi", "title": "A General Framework for Complex Network-Based Image Segmentation", "comments": null, "journal-ref": "Multimedia Tools and Applications (2019)", "doi": "10.1007/s11042-019-7304-2", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advances in complex networks theory, graph-based techniques\nfor image segmentation has attracted great attention recently. In order to\nsegment the image into meaningful connected components, this paper proposes an\nimage segmentation general framework using complex networks based community\ndetection algorithms. If we consider regions as communities, using community\ndetection algorithms directly can lead to an over-segmented image. To address\nthis problem, we start by splitting the image into small regions using an\ninitial segmentation. The obtained regions are used for building the complex\nnetwork. To produce meaningful connected components and detect homogeneous\ncommunities, some combinations of color and texture based features are employed\nin order to quantify the regions similarities. To sum up, the network of\nregions is constructed adaptively to avoid many small regions in the image, and\nthen, community detection algorithms are applied on the resulting adaptive\nsimilarity matrix to obtain the final segmented image. Experiments are\nconducted on Berkeley Segmentation Dataset and four of the most influential\ncommunity detection algorithms are tested. Experimental results have shown that\nthe proposed general framework increases the segmentation performances compared\nto some existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 11:59:42 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Mourchid", "Youssef", ""], ["Hassouni", "Mohammed El", ""], ["Cherifi", "Hocine", ""]]}, {"id": "1907.05279", "submitter": "Lukas Prantl", "authors": "Lukas Prantl, Nuttapong Chentanez, Stefan Jeschke, and Nils Thuerey", "title": "Tranquil Clouds: Neural Networks for Learning Temporally Coherent\n  Features in Point Clouds", "comments": "Further information and videos at\n  https://ge.in.tum.de/publications/2020-iclr-prantl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds, as a form of Lagrangian representation, allow for powerful and\nflexible applications in a large number of computational disciplines. We\npropose a novel deep-learning method to learn stable and temporally coherent\nfeature spaces for points clouds that change over time. We identify a set of\ninherent problems with these approaches: without knowledge of the time\ndimension, the inferred solutions can exhibit strong flickering, and easy\nsolutions to suppress this flickering can result in undesirable local minima\nthat manifest themselves as halo structures. We propose a novel temporal loss\nfunction that takes into account higher time derivatives of the point\npositions, and encourages mingling, i.e., to prevent the aforementioned halos.\nWe combine these techniques in a super-resolution method with a truncation\napproach to flexibly adapt the size of the generated positions. We show that\nour method works for large, deforming point sets from different sources to\ndemonstrate the flexibility of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:54:02 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 10:55:16 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Prantl", "Lukas", ""], ["Chentanez", "Nuttapong", ""], ["Jeschke", "Stefan", ""], ["Thuerey", "Nils", ""]]}, {"id": "1907.05280", "submitter": "Maximilian Bachl", "authors": "Maximilian Bachl and Daniel C. Ferreira", "title": "City-GAN: Learning architectural styles using a custom Conditional GAN\n  architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) are a well-known technique that is\ntrained on samples (e.g. pictures of fruits) and which after training is able\nto generate realistic new samples. Conditional GANs (CGANs) additionally\nprovide label information for subclasses (e.g. apple, orange, pear) which\nenables the GAN to learn more easily and increase the quality of its output\nsamples. We use GANs to learn architectural features of major cities and to\ngenerate images of buildings which do not exist. We show that currently\navailable GAN and CGAN architectures are unsuited for this task and propose a\ncustom architecture and demonstrate that our architecture has superior\nperformance for this task and verify its capabilities with extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 11:43:36 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 20:19:30 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Bachl", "Maximilian", ""], ["Ferreira", "Daniel C.", ""]]}, {"id": "1907.05281", "submitter": "Aras Dargazany", "authors": "Aras R. Dargazany", "title": "Human Body Parts Tracking: Applications to Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cameras and computers became popular, the applications of computer vision\ntechniques attracted attention enormously. One of the most important\napplications in the computer vision community is human activity recognition. In\norder to recognize human activities, we propose a human body parts tracking\nsystem that tracks human body parts such as head, torso, arms and legs in order\nto perform activity recognition tasks in real time. This thesis presents a\nreal-time human body parts tracking system (i.e. HBPT) from video sequences.\nOur body parts model is mostly represented by body components such as legs,\nhead, torso and arms. The body components are modeled using torso location and\nsize which are obtained by a torso tracking method in each frame. In order to\ntrack the torso, we are using a blob tracking module to find the approximate\nlocation and size of the torso in each frame. By tracking the torso, we will be\nable to track other body parts based on their location with respect to the\ntorso on the detected silhouette. In the proposed method for human body part\ntracking, we are also using a refining module to improve the detected\nsilhouette by refining the foreground mask (i.e. obtained by background\nsubtraction) in order to detect the body parts with respect to torso location\nand size. Having found the torso size and location, the region of each human\nbody part on the silhouette will be modeled by a 2D-Gaussian blob in each frame\nin order to show its location, size and pose. The proposed approach described\nin this thesis tracks accurately the body parts in different illumination\nconditions and in the presence of partial occlusions. The proposed approach is\napplied to activity recognition tasks such as approaching an object, carrying\nan object and opening a box or suitcase.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 12:40:13 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Dargazany", "Aras R.", ""]]}, {"id": "1907.05282", "submitter": "Zhuangzi Li", "authors": "Zhuangzi Li", "title": "Image Super-Resolution Using Attention Based DenseNet with Residual\n  Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution is a challenging task and has attracted increasing\nattention in research and industrial communities. In this paper, we propose a\nnovel end-to-end Attention-based DenseNet with Residual Deconvolution named as\nADRD. In our ADRD, a weighted dense block, in which the current layer receives\nweighted features from all previous levels, is proposed to capture valuable\nfeatures rely in dense layers adaptively. And a novel spatial attention module\nis presented to generate a group of attentive maps for emphasizing informative\nregions. In addition, we design an innovative strategy to upsample residual\ninformation via the deconvolution layer, so that the high-frequency details can\nbe accurately upsampled. Extensive experiments conducted on publicly available\ndatasets demonstrate the promising performance of the proposed ADRD against the\nstate-of-the-arts, both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 08:28:13 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Li", "Zhuangzi", ""]]}, {"id": "1907.05283", "submitter": "Cem Sahin", "authors": "Evan Koester, Cem Safak Sahin", "title": "A Comparison of Super-Resolution and Nearest Neighbors Interpolation\n  Applied to Object Detection on Satellite Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Super-Resolution (SR) has matured as a research topic, it has been applied\nto additional topics beyond image reconstruction. In particular, combining\nclassification or object detection tasks with a super-resolution preprocessing\nstage has yielded improvements in accuracy especially with objects that are\nsmall relative to the scene. While SR has shown promise, a study comparing SR\nand naive upscaling methods such as Nearest Neighbors (NN) interpolation when\napplied as a preprocessing step for object detection has not been performed. We\napply the topic to satellite data and compare the Multi-scale Deep\nSuper-Resolution (MDSR) system to NN on the xView challenge dataset. To do so,\nwe propose a pipeline for processing satellite data that combines multi-stage\nimage tiling and upscaling, the YOLOv2 object detection architecture, and label\nstitching. We compare the effects of training models using an upscaling factor\nof 4, upscaling images from 30cm Ground Sample Distance (GSD) to an effective\nGSD of 7.5cm. Upscaling by this factor significantly improves detection\nresults, increasing Average Precision (AP) of a generalized vehicle class by 23\npercent. We demonstrate that while SR produces upscaled images that are more\nvisually pleasing than their NN counterparts, object detection networks see\nlittle difference in accuracy with images upsampled using NN obtaining nearly\nidentical results to the MDSRx4 enhanced images with a difference of 0.0002 AP\nbetween the two methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 17:03:12 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Koester", "Evan", ""], ["Sahin", "Cem Safak", ""]]}, {"id": "1907.05284", "submitter": "Mhafuzul Islam", "authors": "Mhafuzul Islam, Mizanur Rahman, Mashrur Chowdhury, Gurcan Comert,\n  Eshaa Deepak Sood, Amy Apon", "title": "Vision-based Pedestrian Alert Safety System (PASS) for Signalized\n  Intersections", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Vehicle-to-Pedestrian (V2P) communication can significantly improve\npedestrian safety at a signalized intersection, this safety is hindered as\npedestrians often do not carry hand-held devices (e.g., Dedicated short-range\ncommunication (DSRC) and 5G enabled cell phone) to communicate with connected\nvehicles nearby. To overcome this limitation, in this study, traffic cameras at\na signalized intersection were used to accurately detect and locate pedestrians\nvia a vision-based deep learning technique to generate safety alerts in\nreal-time about possible conflicts between vehicles and pedestrians. The\ncontribution of this paper lies in the development of a system using a\nvision-based deep learning model that is able to generate personal safety\nmessages (PSMs) in real-time (every 100 milliseconds). We develop a pedestrian\nalert safety system (PASS) to generate a safety alert of an imminent\npedestrian-vehicle crash using generated PSMs to improve pedestrian safety at a\nsignalized intersection. Our approach estimates the location and velocity of a\npedestrian more accurately than existing DSRC-enabled pedestrian hand-held\ndevices. A connected vehicle application, the Pedestrian in Signalized\nCrosswalk Warning (PSCW), was developed to evaluate the vision-based PASS.\nNumerical analyses show that our vision-based PASS is able to satisfy the\naccuracy and latency requirements of pedestrian safety applications in a\nconnected vehicle environment.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 02:17:55 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Islam", "Mhafuzul", ""], ["Rahman", "Mizanur", ""], ["Chowdhury", "Mashrur", ""], ["Comert", "Gurcan", ""], ["Sood", "Eshaa Deepak", ""], ["Apon", "Amy", ""]]}, {"id": "1907.05285", "submitter": "Ali Lenjani", "authors": "Ali Lenjani, Shirley J. Dyke, Ilias Bilionis, Chul Min Yeum, Kenzo\n  Kamiya, Jongseong Choi, Xiaoyu Liu and Arindam G. Chowdhury", "title": "Towards fully automated post-event data collection and analysis:\n  pre-event and post-event information fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In post-event reconnaissance missions, engineers and researchers collect\nperishable information about damaged buildings in the affected geographical\nregion to learn from the consequences of the event. A typical post-event\nreconnaissance mission is conducted by first doing a preliminary survey,\nfollowed by a detailed survey. The preliminary survey is typically conducted by\ndriving slowly along a pre-determined route, observing the damage, and noting\nwhere further detailed data should be collected. This involves several manual,\ntime-consuming steps that can be accelerated by exploiting recent advances in\ncomputer vision and artificial intelligence. The objective of this work is to\ndevelop and validate an automated technique to support post-event\nreconnaissance teams in the rapid collection of reliable and sufficiently\ncomprehensive data, for planning the detailed survey. The technique\nincorporates several methods designed to automate the process of categorizing\nbuildings based on their key physical attributes, and rapidly assessing their\npost-event structural condition. It is divided into pre-event and post-event\nstreams, each intending to first extract all possible information about the\ntarget buildings using both pre-event and post-event images. Algorithms based\non convolutional neural network (CNNs) are implemented for scene (image)\nclassification. A probabilistic approach is developed to fuse the results\nobtained from analyzing several images to yield a robust decision regarding the\nattributes and condition of a target building. We validate the technique using\npost-event images captured during reconnaissance missions that took place after\nhurricanes Harvey and Irma. The validation data were collected by a structural\nwind and coastal engineering reconnaissance team, the National Science\nFoundation (NSF) funded Structural Extreme Events Reconnaissance (StEER)\nNetwork.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 01:39:26 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Lenjani", "Ali", ""], ["Dyke", "Shirley J.", ""], ["Bilionis", "Ilias", ""], ["Yeum", "Chul Min", ""], ["Kamiya", "Kenzo", ""], ["Choi", "Jongseong", ""], ["Liu", "Xiaoyu", ""], ["Chowdhury", "Arindam G.", ""]]}, {"id": "1907.05286", "submitter": "Bei Wang", "authors": "Bei Wang, Jianping An and Jiayan Cao", "title": "Voxel-FPN: multi-scale voxel feature aggregation in 3D object detection\n  from point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection in point cloud data is one of the key components in computer\nvision systems, especially for autonomous driving applications. In this work,\nwe present Voxel-FPN, a novel one-stage 3D object detector that utilizes raw\ndata from LIDAR sensors only. The core framework consists of an encoder network\nand a corresponding decoder followed by a region proposal network. Encoder\nextracts multi-scale voxel information in a bottom-up manner while decoder\nfuses multiple feature maps from various scales in a top-down way. Extensive\nexperiments show that the proposed method has better performance on extracting\nfeatures from point data and demonstrates its superiority over some baselines\non the challenging KITTI-3D benchmark, obtaining good performance on both speed\nand accuracy in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 09:49:10 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 08:22:44 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Wang", "Bei", ""], ["An", "Jianping", ""], ["Cao", "Jiayan", ""]]}, {"id": "1907.05287", "submitter": "Fan Jia", "authors": "Fan Jia, Jun Liu, Xue-cheng Tai", "title": "A Regularized Convolutional Neural Network for Semantic Image\n  Segmentation", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) show outstanding performance in many\nimage processing problems, such as image recognition, object detection and\nimage segmentation. Semantic segmentation is a very challenging task that\nrequires recognizing, understanding what's in the image in pixel level. Though\nthe state of the art has been greatly improved by CNNs, there is no explicit\nconnections between prediction of neighbouring pixels. That is, spatial\nregularity of the segmented objects is still a problem for CNNs. In this paper,\nwe propose a method to add spatial regularization to the segmented objects. In\nour method, the spatial regularization such as total variation (TV) can be\neasily integrated into CNN network. It can help CNN find a better local optimum\nand make the segmentation results more robust to noise. We apply our proposed\nmethod to Unet and Segnet, which are well established CNNs for image\nsegmentation, and test them on WBC, CamVid and SUN-RGBD datasets, respectively.\nThe results show that the regularized networks not only could provide better\nsegmentation results with regularization effect than the original ones but also\nhave certain robustness to noise.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 14:45:17 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Jia", "Fan", ""], ["Liu", "Jun", ""], ["Tai", "Xue-cheng", ""]]}, {"id": "1907.05288", "submitter": "Mikayla Timm", "authors": "Tsung-Yu Lin, Mikayla Timm, Chenyun Wu, Subhransu Maji", "title": "Visualizing and Describing Fine-grained Categories as Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze how categories from recent FGVC challenges can be described by\ntheir textural content. The motivation is that subtle differences between\nspecies of birds or butterflies can often be described in terms of the texture\nassociated with them and that several top-performing networks are inspired by\ntexture-based representations. These representations are characterized by\norderless pooling of second-order filter activations such as in bilinear CNNs\nand the winner of the iNaturalist 2018 challenge. Concretely, for each category\nwe (i) visualize the \"maximal images\" by obtaining inputs x that maximize the\nprobability of the particular class according to a texture-based deep network,\nand (ii) automatically describe the maximal images using a set of texture\nattributes. The models for texture captioning were trained on our ongoing\nefforts on collecting a dataset of describable textures building on the DTD\ndataset. These visualizations indicate what aspects of the texture is most\ndiscriminative for each category while the descriptions provide a\nlanguage-based explanation of the same.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 00:19:37 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Lin", "Tsung-Yu", ""], ["Timm", "Mikayla", ""], ["Wu", "Chenyun", ""], ["Maji", "Subhransu", ""]]}, {"id": "1907.05289", "submitter": "Heinrich L\\\"owen", "authors": "Heinrich L\\\"owen and Angela Schwering", "title": "An algorithm for the selection of route dependent orientation\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmarks are important features of spatial cognition. Landmarks are\nnaturally included in human route descriptions and in the past algorithms were\ndeveloped to select the most salient landmarks at decision points and\nautomatically incorporate them in route instructions. Moreover, it was shown\nthat human route descriptions contain a significant amount of orientation\ninformation and that these orientation information support the acquisition of\nsurvey knowledge. Thus, there is a need to extend the landmarks selection in\norder to automatically select orientation information. In this work we present\nan algorithm for the computational selection of route dependent orientation\ninformation, which extends previous algorithms and includes a salience\nevaluation of orientation information for any location along the route. We\nimplemented the algorithm and demonstrate the functionality on the basis of\nOpenStreetMap data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:43:45 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["L\u00f6wen", "Heinrich", ""], ["Schwering", "Angela", ""]]}, {"id": "1907.05310", "submitter": "William Andrew", "authors": "William Andrew, Colin Greatwood, Tilo Burghardt", "title": "Aerial Animal Biometrics: Individual Friesian Cattle Recovery and Visual\n  Identification via an Autonomous UAV with Onboard Deep Inference", "comments": "Accepted 7 page manuscript to be presented at IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a computationally-enhanced M100 UAV platform with an\nonboard deep learning inference system for integrated computer vision and\nnavigation able to autonomously find and visually identify by coat pattern\nindividual Holstein Friesian cattle in freely moving herds. We propose an\napproach that utilises three deep convolutional neural network architectures\nrunning live onboard the aircraft; that is, a YoloV2-based species detector, a\ndual-stream CNN delivering exploratory agency and an InceptionV3-based\nbiometric LRCN for individual animal identification. We evaluate the\nperformance of each of the components offline, and also online via real-world\nfield tests comprising 146.7 minutes of autonomous low altitude flight in a\nfarm environment over a dispersed herd of 17 heifer dairy cows. We report\nerror-free identification performance on this online experiment. The presented\nproof-of-concept system is the first of its kind and a successful step towards\nautonomous biometric identification of individual animals from the air in open\npasture environments for tag-less AI support in farming and ecology.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:37:58 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Andrew", "William", ""], ["Greatwood", "Colin", ""], ["Burghardt", "Tilo", ""]]}, {"id": "1907.05315", "submitter": "Peizhao Li", "authors": "Xiaolong Jiang, Peizhao Li, Yanjing Li, Xiantong Zhen", "title": "Graph Neural Based End-to-end Data Association Framework for Online\n  Multiple-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an end-to-end framework to settle data association\nin online Multiple-Object Tracking (MOT). Given detection responses, we\nformulate the frame-by-frame data association as Maximum Weighted Bipartite\nMatching problem, whose solution is learned using a neural network. The network\nincorporates an affinity learning module, wherein both appearance and motion\ncues are investigated to encode object feature representation and compute\npairwise affinities. Employing the computed affinities as edge weights, the\nfollowing matching problem on a bipartite graph is resolved by the optimization\nmodule, which leverages a graph neural network to adapt with the varying\ncardinalities of the association problem and solve the combinatorial hardness\nwith favorable scalability and compatibility. To facilitate effective training\nof the proposed tracking network, we design a multi-level matrix loss in\nconjunction with the assembled supervision methodology. Being trained\nend-to-end, all modules in the tracker can co-adapt and co-operate\ncollaboratively, resulting in improved model adaptiveness and less\nparameter-tuning efforts. Experiment results on the MOT benchmarks demonstrate\nthe efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:43:38 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Jiang", "Xiaolong", ""], ["Li", "Peizhao", ""], ["Li", "Yanjing", ""], ["Zhen", "Xiantong", ""]]}, {"id": "1907.05345", "submitter": "Huazhu Fu", "authors": "Huazhu Fu, Boyang Wang, Jianbing Shen, Shanshan Cui, Yanwu Xu, Jiang\n  Liu, Ling Shao", "title": "Evaluation of Retinal Image Quality Assessment Networks in Different\n  Color-spaces", "comments": "Accepted by MICCAI 2019. Corrected two typos in Table 1 as: (1) in\n  training set, the number of \"Usable + All\" should be '1,876'; (2) In testing\n  set, the number of \"Total + DR-0\" should be '11,362'. Project page:\n  https://github.com/hzfu/EyeQ", "journal-ref": null, "doi": "10.1007/978-3-030-32239-7_6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal image quality assessment (RIQA) is essential for controlling the\nquality of retinal imaging and guaranteeing the reliability of diagnoses by\nophthalmologists or automated analysis systems. Existing RIQA methods focus on\nthe RGB color-space and are developed based on small datasets with binary\nquality labels (i.e., `Accept' and `Reject'). In this paper, we first\nre-annotate an Eye-Quality (EyeQ) dataset with 28,792 retinal images from the\nEyePACS dataset, based on a three-level quality grading system (i.e., `Good',\n`Usable' and `Reject') for evaluating RIQA methods. Our RIQA dataset is\ncharacterized by its large-scale size, multi-level grading, and multi-modality.\nThen, we analyze the influences on RIQA of different color-spaces, and propose\na simple yet efficient deep network, named Multiple Color-space Fusion Network\n(MCF-Net), which integrates the different color-space representations at both a\nfeature-level and prediction-level to predict image quality grades. Experiments\non our EyeQ dataset show that our MCF-Net obtains a state-of-the-art\nperformance, outperforming the other deep learning methods. Furthermore, we\nalso evaluate diabetic retinopathy (DR) detection methods on images of\ndifferent quality, and demonstrate that the performances of automated\ndiagnostic systems are highly dependent on image quality.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 10:47:36 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 06:26:39 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 13:49:52 GMT"}, {"version": "v4", "created": "Thu, 9 Jan 2020 10:49:13 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Fu", "Huazhu", ""], ["Wang", "Boyang", ""], ["Shen", "Jianbing", ""], ["Cui", "Shanshan", ""], ["Xu", "Yanwu", ""], ["Liu", "Jiang", ""], ["Shao", "Ling", ""]]}, {"id": "1907.05358", "submitter": "Ankit Gupta", "authors": "Ankit Gupta", "title": "StrokeSave: A Novel, High-Performance Mobile Application for Stroke\n  Diagnosis using Deep Learning and Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the WHO, Cerebrovascular Stroke, or CS, is the second largest\ncause of death worldwide. Current diagnosis of CS relies on labor and cost\nintensive neuroimaging techniques, unsuitable for areas with inadequate access\nto quality medical facilities. Thus, there is a great need for an efficient\ndiagnosis alternative. StrokeSave is a platform for users to self-diagnose for\nprevalence to stroke. The mobile app is continuously updated with heart rate,\nblood pressure, and blood oxygen data from sensors on the patient wrist. Once\nthese measurements reach a threshold for possible stroke, the patient takes\nfacial images and vocal recordings to screen for paralysis attributed to\nstroke. A custom designed lens attached to a phone's camera then takes retinal\nimages for the deep learning model to classify based on presence of retinopathy\nand sends a comprehensive diagnosis. The deep learning model, which consists of\na RNN trained on 100 voice slurred audio files, a SVM trained on 410 vascular\ndata points, and a CNN trained on 520 retinopathy images, achieved a holistic\naccuracy of 95.0 percent when validated on 327 samples. This value exceeds that\nof clinical examination accuracy, which is around 40 to 89 percent, further\ndemonstrating the vital utility of such a medical device. Through this\nautomated platform, users receive efficient, highly accurate diagnosis without\nprofessional medical assistance, revolutionizing medical diagnosis of CS and\npotentially saving millions of lives.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 21:01:58 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Gupta", "Ankit", ""]]}, {"id": "1907.05375", "submitter": "Tarlan Suleymanov", "authors": "Tarlan Suleymanov, Lars Kunze and Paul Newman", "title": "Online Inference and Detection of Curbs in Partially Occluded Scenes\n  with Sparse LIDAR", "comments": "Accepted at the 22nd IEEE Intelligent Transportation Systems\n  Conference (ITSC19), October, 2019, Auckland, New Zealand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road boundaries, or curbs, provide autonomous vehicles with essential\ninformation when interpreting road scenes and generating behaviour plans.\nAlthough curbs convey important information, they are difficult to detect in\ncomplex urban environments (in particular in comparison to other elements of\nthe road such as traffic signs and road markings). These difficulties arise\nfrom occlusions by other traffic participants as well as changing lighting\nand/or weather conditions. Moreover, road boundaries have various shapes,\ncolours and structures while motion planning algorithms require accurate and\nprecise metric information in real-time to generate their plans.\n  In this paper, we present a real-time LIDAR-based approach for accurate curb\ndetection around the vehicle (360 degree). Our approach deals with both\nocclusions from traffic and changing environmental conditions. To this end, we\nproject 3D LIDAR pointcloud data into 2D bird's-eye view images (akin to\nInverse Perspective Mapping). These images are then processed by trained deep\nnetworks to infer both visible and occluded road boundaries. Finally, a\npost-processing step filters detected curb segments and tracks them over time.\nExperimental results demonstrate the effectiveness of the proposed approach on\nreal-world driving data. Hence, we believe that our LIDAR-based approach\nprovides an efficient and effective way to detect visible and occluded curbs\naround the vehicles in challenging driving scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 16:50:38 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Suleymanov", "Tarlan", ""], ["Kunze", "Lars", ""], ["Newman", "Paul", ""]]}, {"id": "1907.05376", "submitter": "Robert Amelard", "authors": "Robert Amelard, Kevin R Murray, Eric T Hedge, Taylor W Cleworth,\n  Mamiko Noguchi, Andrew Laing, Richard L Hughson", "title": "Monocular 3D Sway Tracking for Assessing Postural Instability in\n  Cerebral Hypoperfusion During Quiet Standing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Postural instability is prevalent in aging and neurodegenerative disease,\ndecreasing quality of life and independence. Quantitatively monitoring balance\ncontrol is important for assessing treatment efficacy and rehabilitation\nprogress. However, existing technologies for assessing postural sway are\ncomplex and expensive, limiting their widespread utility. Here, we propose a\nmonocular imaging system capable of assessing sub-millimeter 3D sway dynamics\nduring quiet standing. Two anatomical targets with known feature geometries\nwere placed on the lumbar and shoulder. Upper and lower trunk 3D kinematic\nmotion was automatically assessed from a set of 2D frames through geometric\nfeature tracking and an inverse motion model. Sway was tracked in 3D and\ncompared between control and hypoperfusion conditions in 14 healthy young\nadults. The proposed system demonstrated high agreement with a commercial\nmotion capture system (error $1.5 \\times 10^{-4}~\\text{mm}$, [$-0.52$,\n$0.52$]). Between-condition differences in sway dynamics were observed in\nanterior-posterior sway during early and mid stance, and medial-lateral sway\nduring mid stance commensurate with decreased cerebral perfusion, followed by\nrecovered sway dynamics during late stance with cerebral perfusion recovery.\nThis inexpensive single-camera system enables quantitative 3D sway monitoring\nfor assessing neuromuscular balance control in weakly constrained environments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 16:51:39 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 21:50:11 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Amelard", "Robert", ""], ["Murray", "Kevin R", ""], ["Hedge", "Eric T", ""], ["Cleworth", "Taylor W", ""], ["Noguchi", "Mamiko", ""], ["Laing", "Andrew", ""], ["Hughson", "Richard L", ""]]}, {"id": "1907.05380", "submitter": "Marija Vella", "authors": "Marija Vella, Jo\\~ao F. C. Mota", "title": "Single Image Super-Resolution via CNN Architectures and TV-TV\n  Minimization", "comments": "Accepted to BMVC 2019; v2 contains updated results and minor bug\n  fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) is a technique that allows increasing the resolution of\na given image. Having applications in many areas, from medical imaging to\nconsumer electronics, several SR methods have been proposed. Currently, the\nbest performing methods are based on convolutional neural networks (CNNs) and\nrequire extensive datasets for training. However, at test time, they fail to\nimpose consistency between the super-resolved image and the given\nlow-resolution image, a property that classic reconstruction-based algorithms\nnaturally enforce in spite of having poorer performance. Motivated by this\nobservation, we propose a new framework that joins both approaches and produces\nimages with superior quality than any of the prior methods. Although our\nframework requires additional computation, our experiments on Set5, Set14, and\nBSD100 show that it systematically produces images with better peak signal to\nnoise ratio (PSNR) and structural similarity (SSIM) than the current\nstate-of-the-art CNN architectures for SR.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 16:57:59 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 21:32:39 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Vella", "Marija", ""], ["Mota", "Jo\u00e3o F. C.", ""]]}, {"id": "1907.05418", "submitter": "Dawei Yang", "authors": "Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang Yang, Mingyan\n  Liu, Bo Li", "title": "Adversarial Objects Against LiDAR-Based Autonomous Driving Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are found to be vulnerable against adversarial\nexamples, which are carefully crafted inputs with a small magnitude of\nperturbation aiming to induce arbitrarily incorrect predictions. Recent studies\nshow that adversarial examples can pose a threat to real-world\nsecurity-critical applications: a \"physical adversarial Stop Sign\" can be\nsynthesized such that the autonomous driving cars will misrecognize it as\nothers (e.g., a speed limit sign). However, these image-space adversarial\nexamples cannot easily alter 3D scans of widely equipped LiDAR or radar on\nautonomous vehicles. In this paper, we reveal the potential vulnerabilities of\nLiDAR-based autonomous driving detection systems, by proposing an optimization\nbased approach LiDAR-Adv to generate adversarial objects that can evade the\nLiDAR-based detection system under various conditions. We first show the\nvulnerabilities using a blackbox evolution-based algorithm, and then explore\nhow much a strong adversary can do, using our gradient-based approach\nLiDAR-Adv. We test the generated adversarial objects on the Baidu Apollo\nautonomous driving platform and show that such physical systems are indeed\nvulnerable to the proposed attacks. We also 3D-print our adversarial objects\nand perform physical experiments to illustrate that such vulnerability exists\nin the real world. Please find more visualizations and results on the anonymous\nwebsite: https://sites.google.com/view/lidar-adv.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 17:59:13 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Cao", "Yulong", ""], ["Xiao", "Chaowei", ""], ["Yang", "Dawei", ""], ["Fang", "Jing", ""], ["Yang", "Ruigang", ""], ["Liu", "Mingyan", ""], ["Li", "Bo", ""]]}, {"id": "1907.05514", "submitter": "Sing-Ho Bae", "authors": "Abdul Muqeet, Md Tauhid Bin Iqbal, and Sung-Ho Bae", "title": "Hybrid Residual Attention Network for Single Image Super Resolution", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2942346", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction and proper utilization of convolution neural network (CNN)\nfeatures have a significant impact on the performance of image super-resolution\n(SR). Although CNN features contain both the spatial and channel information,\ncurrent deep techniques on SR often suffer to maximize performance due to using\neither the spatial or channel information. Moreover, they integrate such\ninformation within a deep or wide network rather than exploiting all the\navailable features, eventually resulting in high computational complexity. To\naddress these issues, we present a binarized feature fusion (BFF) structure\nthat utilizes the extracted features from residual groups (RG) in an effective\nway. Each residual group (RG) consists of multiple hybrid residual attention\nblocks (HRAB) that effectively integrates the multiscale feature extraction\nmodule and channel attention mechanism in a single block. Furthermore, we use\ndilated convolutions with different dilation factors to extract multiscale\nfeatures. We also propose to adopt global, short and long skip connections and\nresidual groups (RG) structure to ease the flow of information without losing\nimportant features details. In the paper, we call this overall network\narchitecture as hybrid residual attention network (HRAN). In the experiment, we\nhave observed the efficacy of our method against the state-of-the-art methods\nfor both the quantitative and qualitative comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 22:48:23 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Muqeet", "Abdul", ""], ["Iqbal", "Md Tauhid Bin", ""], ["Bae", "Sung-Ho", ""]]}, {"id": "1907.05518", "submitter": "Maximilian Sieb", "authors": "Maximilian Sieb, Zhou Xian, Audrey Huang, Oliver Kroemer, Katerina\n  Fragkiadaki", "title": "Graph-Structured Visual Imitation", "comments": "8 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We cast visual imitation as a visual correspondence problem. Our robotic\nagent is rewarded when its actions result in better matching of relative\nspatial configurations for corresponding visual entities detected in its\nworkspace and teacher's demonstration. We build upon recent advances in\nComputer Vision,such as human finger keypoint detectors, object detectors\ntrained on-the-fly with synthetic augmentations, and point detectors supervised\nby viewpoint changes and learn multiple visual entity detectors for each\ndemonstration without human annotations or robot interactions. We empirically\nshow the proposed factorized visual representations of entities and their\nspatial arrangements drive successful imitation of a variety of manipulation\nskills within minutes, using a single demonstration and without any environment\ninstrumentation. It is robust to background clutter and can effectively\ngeneralize across environment variations between demonstrator and imitator,\ngreatly outperforming unstructured non-factorized full-frame CNN encodings of\nprevious works.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 23:06:16 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 01:33:45 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Sieb", "Maximilian", ""], ["Xian", "Zhou", ""], ["Huang", "Audrey", ""], ["Kroemer", "Oliver", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1907.05552", "submitter": "Usman Nazir", "authors": "Usman Nazir, Numan Khurshid, Muhammad Ahmed Bhimra, Murtaza Taj", "title": "Tiny-Inception-ResNet-v2: Using Deep Learning for Eliminating Bonded\n  Labors of Brick Kilns in South Asia", "comments": null, "journal-ref": "CVPR 2019 workshop", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposes to employ a Inception-ResNet inspired deep learning\narchitecture called Tiny-Inception-ResNet-v2 to eliminate bonded labor by\nidentifying brick kilns within \"Brick-Kiln-Belt\" of South Asia. The framework\nis developed by training a network on the satellite imagery consisting of 11\ndifferent classes of South Asian region. The dataset developed during the\nprocess includes the geo-referenced images of brick kilns, houses, roads,\ntennis courts, farms, sparse trees, dense trees, orchards, parking lots, parks\nand barren lands. The dataset is made publicly available for further research.\nOur proposed network architecture with very fewer learning parameters\noutperforms all state-of-the-art architectures employed for recognition of\nbrick kilns. Our proposed solution would enable regional monitoring and\nevaluation mechanisms for the Sustainable Development Goals.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 07:43:42 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Nazir", "Usman", ""], ["Khurshid", "Numan", ""], ["Bhimra", "Muhammad Ahmed", ""], ["Taj", "Murtaza", ""]]}, {"id": "1907.05553", "submitter": "Aras Dargazany", "authors": "Aras R. Dargazany", "title": "MLR (Memory, Learning and Recognition): A General Cognitive Model --\n  applied to Intelligent Robots and Systems Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new perspective of intelligent robots and systems\ncontrol. The presented and proposed cognitive model: Memory, Learning and\nRecognition (MLR), is an effort to bridge the gap between Robotics, AI,\nCognitive Science, and Neuroscience. The currently existing gap prevents us\nfrom integrating the current advancement and achievements of these four\nresearch fields which are actively trying to define intelligence in either\napplication-based way or in generic way. This cognitive model defines\nintelligence more specifically, parametrically and detailed. The proposed MLR\nmodel helps us create a general control model for robots and systems\nindependent of their application domains and platforms since it is mainly based\non the dataset provided for robots and systems controls. This paper is mainly\nproposing and introducing this concept and trying to prove this concept in a\nsmall scale, firstly through experimentation. The proposed concept is also\napplicable to other different platforms in real-time as well as in simulation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 02:40:37 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Dargazany", "Aras R.", ""]]}, {"id": "1907.05570", "submitter": "Jian Ni", "authors": "Jian Ni, Shanghang Zhang, Haiyong Xie", "title": "Dual Adversarial Semantics-Consistent Network for Generalized Zero-Shot\n  Learning", "comments": "10 pages, 5 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot learning (GZSL) is a challenging class of vision and\nknowledge transfer problems in which both seen and unseen classes appear during\ntesting. Existing GZSL approaches either suffer from semantic loss and discard\ndiscriminative information at the embedding stage, or cannot guarantee the\nvisual-semantic interactions. To address these limitations, we propose the Dual\nAdversarial Semantics-Consistent Network (DASCN), which learns primal and dual\nGenerative Adversarial Networks (GANs) in a unified framework for GZSL. In\nparticular, the primal GAN learns to synthesize inter-class discriminative and\nsemantics-preserving visual features from both the semantic representations of\nseen/unseen classes and the ones reconstructed by the dual GAN. The dual GAN\nenforces the synthetic visual features to represent prior semantic knowledge\nwell via semantics-consistent adversarial learning. To the best of our\nknowledge, this is the first work that employs a novel dual-GAN mechanism for\nGZSL. Extensive experiments show that our approach achieves significant\nimprovements over the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 03:53:08 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Ni", "Jian", ""], ["Zhang", "Shanghang", ""], ["Xie", "Haiyong", ""]]}, {"id": "1907.05572", "submitter": "Zhiwei Wang", "authors": "Zhiwei Wang, Yao Ma, Zitao Liu, Jiliang Tang", "title": "R-Transformer: Recurrent Neural Network Enhanced Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks have long been the dominating choice for sequence\nmodeling. However, it severely suffers from two issues: impotent in capturing\nvery long-term dependencies and unable to parallelize the sequential\ncomputation procedure. Therefore, many non-recurrent sequence models that are\nbuilt on convolution and attention operations have been proposed recently.\nNotably, models with multi-head attention such as Transformer have demonstrated\nextreme effectiveness in capturing long-term dependencies in a variety of\nsequence modeling tasks. Despite their success, however, these models lack\nnecessary components to model local structures in sequences and heavily rely on\nposition embeddings that have limited effects and require a considerable amount\nof design efforts. In this paper, we propose the R-Transformer which enjoys the\nadvantages of both RNNs and the multi-head attention mechanism while avoids\ntheir respective drawbacks. The proposed model can effectively capture both\nlocal structures and global long-term dependencies in sequences without any use\nof position embeddings. We evaluate R-Transformer through extensive experiments\nwith data from a wide range of domains and the empirical results show that\nR-Transformer outperforms the state-of-the-art methods by a large margin in\nmost of the tasks. We have made the code publicly available at\n\\url{https://github.com/DSE-MSU/R-transformer}.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 04:01:57 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Wang", "Zhiwei", ""], ["Ma", "Yao", ""], ["Liu", "Zitao", ""], ["Tang", "Jiliang", ""]]}, {"id": "1907.05577", "submitter": "Yizhi Wang", "authors": "Yizhi Wang, Zhouhui Lian, Yingmin Tang and Jianguo Xiao", "title": "Boosting Scene Character Recognition by Learning Canonical Forms of\n  Glyphs", "comments": "Accepted by International Journal on Document Analysis and\n  Recognition (IJDAR), will appear in ICDAR 2019. Code:\n  https://github.com/Actasidiot/CGRN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the fundamental problems in document analysis, scene character\nrecognition has attracted considerable interests in recent years. But the\nproblem is still considered to be extremely challenging due to many\nuncontrollable factors including glyph transformation, blur, noisy background,\nuneven illumination, etc. In this paper, we propose a novel methodology for\nboosting scene character recognition by learning canonical forms of glyphs,\nbased on the fact that characters appearing in scene images are all derived\nfrom their corresponding canonical forms. Our key observation is that more\ndiscriminative features can be learned by solving specially-designed generative\ntasks compared to traditional classification-based feature learning frameworks.\nSpecifically, we design a GAN-based model to make the learned deep feature of a\ngiven scene character be capable of reconstructing corresponding glyphs in a\nnumber of standard font styles. In this manner, we obtain deep features for\nscene characters that are more discriminative in recognition and less sensitive\nagainst the above-mentioned factors. Our experiments conducted on several\npublicly-available databases demonstrate the superiority of our method compared\nto the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 04:55:03 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 09:42:53 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Wang", "Yizhi", ""], ["Lian", "Zhouhui", ""], ["Tang", "Yingmin", ""], ["Xiao", "Jianguo", ""]]}, {"id": "1907.05595", "submitter": "Xueyan Ding", "authors": "Xueyan Ding, Yafei Wang, Yang Yan, Zheng Liang, Zetian Mi, and\n  Xianping Fu", "title": "Jointly Adversarial Network to Wavelength Compensation and Dehazing of\n  Underwater Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Severe color casts, low contrast and blurriness of underwater images caused\nby light absorption and scattering result in a difficult task for exploring\nunderwater environments. Different from most of previous underwater image\nenhancement methods that compute light attenuation along object-camera path\nthrough hazy image formation model, we propose a novel jointly wavelength\ncompensation and dehazing network (JWCDN) that takes into account the\nwavelength attenuation along surface-object path and the scattering along\nobject-camera path simultaneously. By embedding a simplified underwater\nformation model into generative adversarial network, we can jointly estimates\nthe transmission map, wavelength attenuation and background light via different\nnetwork modules, and uses the simplified underwater image formation model to\nrecover degraded underwater images. Especially, a multi-scale densely connected\nencoder-decoder network is proposed to leverage features from multiple layers\nfor estimating the transmission map. To further improve the recovered image, we\nuse an edge preserving network module to enhance the detail of the recovered\nimage. Moreover, to train the proposed network, we propose a novel underwater\nimage synthesis method that generates underwater images with inherent optical\nproperties of different water types. The synthesis method can simulate the\ncolor, contrast and blurriness appearance of real-world underwater environments\nsimultaneously. Extensive experiments on synthetic and real-world underwater\nimages demonstrate that the proposed method yields comparable or better results\non both subjective and objective assessments, compared with several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 07:08:11 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Ding", "Xueyan", ""], ["Wang", "Yafei", ""], ["Yan", "Yang", ""], ["Liang", "Zheng", ""], ["Mi", "Zetian", ""], ["Fu", "Xianping", ""]]}, {"id": "1907.05598", "submitter": "Kai Wang", "authors": "Chun-Mei Feng, Kai Wang, Shijian Lu, Yong Xu, Heng Kong, Ling Shao", "title": "Coupled-Projection Residual Network for MRI Super-Resolution", "comments": "Our source code will be publicly available at\n  http://www.yongxu.org/lunwen.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging(MRI) has been widely used in clinical application\nand pathology research by helping doctors make more accurate diagnoses. On the\nother hand, accurate diagnosis by MRI remains a great challenge as images\nobtained via present MRI techniques usually have low resolutions. Improving MRI\nimage quality and resolution thus becomes a critically important task. This\npaper presents an innovative Coupled-Projection Residual Network (CPRN) for MRI\nsuper-resolution. The CPRN consists of two complementary sub-networks: a\nshallow network and a deep network that keep the content consistency while\nlearning high frequency differences between low-resolution and high-resolution\nimages. The shallow sub-network employs coupled-projection for better retaining\nthe MRI image details, where a novel feedback mechanism is introduced to guide\nthe reconstruction of high-resolution images. The deep sub-network learns from\nthe residuals of the high-frequency image information, where multiple residual\nblocks are cascaded to magnify the MRI images at the last network layer.\nFinally, the features from the shallow and deep sub-networks are fused for the\nreconstruction of high-resolution MRI images. For effective fusion of features\nfrom the deep and shallow sub-networks, a step-wise connection (CPRN S) is\ndesigned as inspired by the human cognitive processes (from simple to complex).\nExperiments over three public MRI datasets show that our proposed CPRN achieves\nsuperior MRI super-resolution performance as compared with the\nstate-of-the-art. Our source code will be publicly available at\nhttp://www.yongxu.org/lunwen.html.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 07:30:46 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Feng", "Chun-Mei", ""], ["Wang", "Kai", ""], ["Lu", "Shijian", ""], ["Xu", "Yong", ""], ["Kong", "Heng", ""], ["Shao", "Ling", ""]]}, {"id": "1907.05640", "submitter": "Mohammad Tavakolian", "authors": "Mohammad Tavakolian, Mohammad Sabokrou, Abdenour Hadid", "title": "AVD: Adversarial Video Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple yet efficient approach for video\nrepresentation, called Adversarial Video Distillation (AVD). The key idea is to\nrepresent videos by compressing them in the form of realistic images, which can\nbe used in a variety of video-based scene analysis applications. Representing a\nvideo as a single image enables us to address the problem of video analysis by\nimage analysis techniques. To this end, we exploit a 3D convolutional\nencoder-decoder network to encode the input video as an image by minimizing the\nreconstruction error. Furthermore, weak supervision by an adversarial training\nprocedure is imposed on the output of the encoder to generate semantically\nrealistic images. The encoder learns to extract semantically meaningful\nrepresentations from a given input video by mapping the 3D input into a 2D\nlatent representation. The obtained representation can be simply used as the\ninput of deep models pre-trained on images for video classification. We\nevaluated the effectiveness of our proposed method for video-based activity\nrecognition on three standard and challenging benchmark datasets, i.e. UCF101,\nHMDB51, and Kinetics. The experimental results demonstrate that AVD achieves\ninteresting performance, outperforming the state-of-the-art methods for video\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 09:23:32 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Tavakolian", "Mohammad", ""], ["Sabokrou", "Mohammad", ""], ["Hadid", "Abdenour", ""]]}, {"id": "1907.05642", "submitter": "Zhou Daquan", "authors": "Daquan Zhou, Xiaojie Jin, Qibin Hou, Kaixin Wang, Jianchao Yang,\n  Jiashi Feng", "title": "Neural Epitome Search for Architecture-Agnostic Network Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent WSNet [1] is a new model compression method through sampling\nfilterweights from a compact set and has demonstrated to be effective for 1D\nconvolutionneural networks (CNNs). However, the weights sampling strategy of\nWSNet ishandcrafted and fixed which may severely limit the expression ability\nof the resultedCNNs and weaken its compression ability. In this work, we\npresent a novel auto-sampling method that is applicable to both 1D and 2D CNNs\nwith significantperformance improvement over WSNet. Specifically, our proposed\nauto-samplingmethod learns the sampling rules end-to-end instead of being\nindependent of thenetwork architecture design. With such differentiable weight\nsampling rule learning,the sampling stride and channel selection from the\ncompact set are optimized toachieve better trade-off between model compression\nrate and performance. Wedemonstrate that at the same compression ratio, our\nmethod outperforms WSNetby6.5% on 1D convolution. Moreover, on ImageNet, our\nmethod outperformsMobileNetV2 full model by1.47%in classification accuracy\nwith25%FLOPsreduction. With the same backbone architecture as baseline models,\nour methodeven outperforms some neural architecture search (NAS) based methods\nsuch asAMC [2] and MNasNet [3].\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 09:38:52 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 05:54:33 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2019 13:26:40 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zhou", "Daquan", ""], ["Jin", "Xiaojie", ""], ["Hou", "Qibin", ""], ["Wang", "Kaixin", ""], ["Yang", "Jianchao", ""], ["Feng", "Jiashi", ""]]}, {"id": "1907.05653", "submitter": "Liangchen Song", "authors": "Qian Zhang, Jianjun Li, Meng Yao, Liangchen Song, Helong Zhou, Zhichao\n  Li, Wenming Meng, Xuezhi Zhang, Guoli Wang", "title": "VarGNet: Variable Group Convolutional Neural Network for Efficient\n  Embedded Computing", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel network design mechanism for efficient\nembedded computing. Inspired by the limited computing patterns, we propose to\nfix the number of channels in a group convolution, instead of the existing\npractice that fixing the total group numbers. Our solution based network, named\nVariable Group Convolutional Network (VarGNet), can be optimized easier on\nhardware side, due to the more unified computing schemes among the layers.\nExtensive experiments on various vision tasks, including classification,\ndetection, pixel-wise parsing and face recognition, have demonstrated the\npractical value of our VarGNet.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 10:08:44 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 00:49:23 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Zhang", "Qian", ""], ["Li", "Jianjun", ""], ["Yao", "Meng", ""], ["Song", "Liangchen", ""], ["Zhou", "Helong", ""], ["Li", "Zhichao", ""], ["Meng", "Wenming", ""], ["Zhang", "Xuezhi", ""], ["Wang", "Guoli", ""]]}, {"id": "1907.05681", "submitter": "D\\'avid Terj\\'ek", "authors": "D\\'avid Terj\\'ek", "title": "Adversarial Lipschitz Regularization", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are one of the most popular approaches\nwhen it comes to training generative models, among which variants of\nWasserstein GANs are considered superior to the standard GAN formulation in\nterms of learning stability and sample quality. However, Wasserstein GANs\nrequire the critic to be 1-Lipschitz, which is often enforced implicitly by\npenalizing the norm of its gradient, or by globally restricting its Lipschitz\nconstant via weight normalization techniques. Training with a regularization\nterm penalizing the violation of the Lipschitz constraint explicitly, instead\nof through the norm of the gradient, was found to be practically infeasible in\nmost situations. Inspired by Virtual Adversarial Training, we propose a method\ncalled Adversarial Lipschitz Regularization, and show that using an explicit\nLipschitz penalty is indeed viable and leads to competitive performance when\napplied to Wasserstein GANs, highlighting an important connection between\nLipschitz regularization and adversarial training.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 11:41:18 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 16:02:17 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2020 09:11:31 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Terj\u00e9k", "D\u00e1vid", ""]]}, {"id": "1907.05686", "submitter": "Pierre Stock", "authors": "Pierre Stock, Armand Joulin, R\\'emi Gribonval, Benjamin Graham,\n  Herv\\'e J\\'egou", "title": "And the Bit Goes Down: Revisiting the Quantization of Neural Networks", "comments": "ICLR 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of reducing the memory footprint of\nconvolutional network architectures. We introduce a vector quantization method\nthat aims at preserving the quality of the reconstruction of the network\noutputs rather than its weights. The principle of our approach is that it\nminimizes the loss reconstruction error for in-domain inputs. Our method only\nrequires a set of unlabelled data at quantization time and allows for efficient\ninference on CPU by using byte-aligned codebooks to store the compressed\nweights. We validate our approach by quantizing a high performing ResNet-50\nmodel to a memory size of 5MB (20x compression factor) while preserving a top-1\naccuracy of 76.1% on ImageNet object classification and by compressing a Mask\nR-CNN with a 26x factor.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 11:52:54 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 13:53:23 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2019 10:39:22 GMT"}, {"version": "v4", "created": "Fri, 20 Dec 2019 17:18:58 GMT"}, {"version": "v5", "created": "Mon, 9 Nov 2020 10:11:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Stock", "Pierre", ""], ["Joulin", "Armand", ""], ["Gribonval", "R\u00e9mi", ""], ["Graham", "Benjamin", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1907.05709", "submitter": "Andre Mastmeyer", "authors": "Niclas Kath, Heinz Handels, Andre Mastmeyer", "title": "Robust GPU-based Virtual Reality Simulation of Radio Frequency Ablations\n  for Various Needle Geometries and Locations", "comments": "18 pages, 14 figures, 1 table, 2 algorithms, 2 movies", "journal-ref": "International Journal of Computer Assisted Radiology and Surgery,\n  2019", "doi": "10.1007/s11548-019-02033-w", "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Radio-frequency ablations play an important role in the therapy of\nmalignant liver lesions. The navigation of a needle to the lesion poses a\nchallenge for both the trainees and intervening physicians. Methods: This\npublication presents a new GPU-based, accurate method for the simulation of\nradio-frequency ablations for lesions at the needle tip in general and for an\nexisting visuo-haptic 4D VR simulator. The method is implemented real-time\ncapable with Nvidia CUDA. Results: It performs better than a literature method\nconcerning the theoretical characteristic of monotonic convergence of the\nbioheat PDE and a in vitro gold standard with significant improvements (p <\n0.05) in terms of Pearson correlations. It shows no failure modes or\ntheoretically inconsistent individual simulation results after the initial\nphase of 10 seconds. On the Nvidia 1080 Ti GPU it achieves a very high frame\nrendering performance of >480 Hz. Conclusion: Our method provides a more robust\nand safer real-time ablation planning and intraoperative guidance technique,\nespecially avoiding the over-estimation of the ablated tissue death zone, which\nis risky for the patient in terms of tumor recurrence. Future in vitro\nmeasurements and optimization shall further improve the conservative estimate.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:53:36 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kath", "Niclas", ""], ["Handels", "Heinz", ""], ["Mastmeyer", "Andre", ""]]}, {"id": "1907.05737", "submitter": "Lingxi Xie", "authors": "Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian,\n  Hongkai Xiong", "title": "PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture\n  Search", "comments": "Accepted by ICLR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable architecture search (DARTS) provided a fast solution in\nfinding effective network architectures, but suffered from large memory and\ncomputing overheads in jointly training a super-network and searching for an\noptimal architecture. In this paper, we present a novel approach, namely,\nPartially-Connected DARTS, by sampling a small part of super-network to reduce\nthe redundancy in exploring the network space, thereby performing a more\nefficient search without comprising the performance. In particular, we perform\noperation search in a subset of channels while bypassing the held out part in a\nshortcut. This strategy may suffer from an undesired inconsistency on selecting\nthe edges of super-net caused by sampling different channels. We alleviate it\nusing edge normalization, which adds a new set of edge-level parameters to\nreduce uncertainty in search. Thanks to the reduced memory cost, PC-DARTS can\nbe trained with a larger batch size and, consequently, enjoys both faster speed\nand higher training stability. Experimental results demonstrate the\neffectiveness of the proposed method. Specifically, we achieve an error rate of\n2.57% on CIFAR10 with merely 0.1 GPU-days for architecture search, and a\nstate-of-the-art top-1 error rate of 24.2% on ImageNet (under the mobile\nsetting) using 3.8 GPU-days for search. Our code has been made available at:\nhttps://github.com/yuhuixu1993/PC-DARTS.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 13:26:09 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 19:26:34 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 03:53:42 GMT"}, {"version": "v4", "created": "Tue, 7 Apr 2020 06:20:35 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Xu", "Yuhui", ""], ["Xie", "Lingxi", ""], ["Zhang", "Xiaopeng", ""], ["Chen", "Xin", ""], ["Qi", "Guo-Jun", ""], ["Tian", "Qi", ""], ["Xiong", "Hongkai", ""]]}, {"id": "1907.05738", "submitter": "Alexander Liniger", "authors": "Simon Hecker, Alexander Liniger, Henrik Maurenbrecher, Dengxin Dai,\n  Luc Van Gool", "title": "Learning a Curve Guardian for Motorcycles", "comments": "8 pages, to be presented at IEEE-ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Up to 17% of all motorcycle accidents occur when the rider is maneuvering\nthrough a curve and the main cause of curve accidents can be attributed to\ninappropriate speed and wrong intra-lane position of the motorcycle. Existing\ncurve warning systems lack crucial state estimation components and do not scale\nwell. We propose a new type of road curvature warning system for motorcycles,\ncombining the latest advances in computer vision, optimal control and mapping\ntechnologies to alleviate these shortcomings. Our contributes are fourfold: 1)\nwe predict the motorcycle's intra-lane position using a convolutional neural\nnetwork (CNN), 2) we predict the motorcycle roll angle using a CNN, 3) we use\nan upgraded controller model that incorporates road incline for a more\nrealistic model and prediction, 4) we design a scale-able system by utilizing\nHERE Technologies map database to obtain the accurate road geometry of the\nfuture path. In addition, we present two datasets that are used for training\nand evaluating of our system respectively, both datasets will be made publicly\navailable. We test our system on a diverse set of real world scenarios and\npresent a detailed case-study. We show that our system is able to predict more\naccurate and safer curve trajectories, and consequently warn and improve the\nsafety for motorcyclists.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 13:26:42 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Hecker", "Simon", ""], ["Liniger", "Alexander", ""], ["Maurenbrecher", "Henrik", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1907.05740", "submitter": "Towaki Takikawa", "authors": "Towaki Takikawa, David Acuna, Varun Jampani, Sanja Fidler", "title": "Gated-SCNN: Gated Shape CNNs for Semantic Segmentation", "comments": "Project Website: https://nv-tlabs.github.io/GSCNN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art methods for image segmentation form a dense image\nrepresentation where the color, shape and texture information are all processed\ntogether inside a deep CNN. This however may not be ideal as they contain very\ndifferent type of information relevant for recognition. Here, we propose a new\ntwo-stream CNN architecture for semantic segmentation that explicitly wires\nshape information as a separate processing branch, i.e. shape stream, that\nprocesses information in parallel to the classical stream. Key to this\narchitecture is a new type of gates that connect the intermediate layers of the\ntwo streams. Specifically, we use the higher-level activations in the classical\nstream to gate the lower-level activations in the shape stream, effectively\nremoving noise and helping the shape stream to only focus on processing the\nrelevant boundary-related information. This enables us to use a very shallow\narchitecture for the shape stream that operates on the image-level resolution.\nOur experiments show that this leads to a highly effective architecture that\nproduces sharper predictions around object boundaries and significantly boosts\nperformance on thinner and smaller objects. Our method achieves\nstate-of-the-art performance on the Cityscapes benchmark, in terms of both mask\n(mIoU) and boundary (F-score) quality, improving by 2% and 4% over strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 13:37:46 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Takikawa", "Towaki", ""], ["Acuna", "David", ""], ["Jampani", "Varun", ""], ["Fidler", "Sanja", ""]]}, {"id": "1907.05793", "submitter": "Mingyu Zhang", "authors": "Guoping Zhao, Mingyu Zhang, Jiajun Liu, Ji-Rong Wen", "title": "Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with\n  GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies show that Deep Neural Network (DNN)-based image classification models\nare vulnerable to maliciously constructed adversarial examples. However, little\neffort has been made to investigate how DNN-based image retrieval models are\naffected by such attacks. In this paper, we introduce Unsupervised Adversarial\nAttacks with Generative Adversarial Networks (UAA-GAN) to attack deep\nfeature-based image retrieval systems. UAA-GAN is an unsupervised learning\nmodel that requires only a small amount of unlabeled data for training. Once\ntrained, it produces query-specific perturbations for query images to form\nadversarial queries. The core idea is to ensure that the attached perturbation\nis barely perceptible to human yet effective in pushing the query away from its\noriginal position in the deep feature space. UAA-GAN works with various\napplication scenarios that are based on deep features, including image\nretrieval, person Re-ID and face search. Empirical results show that UAA-GAN\ncripples retrieval performance without significant visual changes in the query\nimages. UAA-GAN generated adversarial examples are less distinguishable because\nthey tend to incorporate subtle perturbations in textured or salient areas of\nthe images, such as key body parts of human, dominant structural\npatterns/textures or edges, rather than in visually insignificant areas (e.g.,\nbackground and sky). Such tendency indicates that the model indeed learned how\nto toy with both image retrieval systems and human eyes.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 15:23:36 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Zhao", "Guoping", ""], ["Zhang", "Mingyu", ""], ["Liu", "Jiajun", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1907.05794", "submitter": "Syed Sameed Husain Dr", "authors": "Syed Sameed Husain, Eng-Jon Ong, Miroslaw Bober", "title": "ACTNET: end-to-end learning of feature activations and multi-stream\n  aggregation for effective instance image retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel CNN architecture called ACTNET for robust instance image\nretrieval from large-scale datasets. Our key innovation is a learnable\nactivation layer designed to improve the signal-to-noise ratio (SNR) of deep\nconvolutional feature maps. Further, we introduce a controlled multi-stream\naggregation, where complementary deep features from different convolutional\nlayers are optimally transformed and balanced using our novel activation\nlayers, before aggregation into a global descriptor. Importantly, the learnable\nparameters of our activation blocks are explicitly trained, together with the\nCNN parameters, in an end-to-end manner minimising triplet loss. This means\nthat our network jointly learns the CNN filters and their optimal activation\nand aggregation for retrieval tasks. To our knowledge, this is the first time\nparametric functions have been used to control and learn optimal aggregation.\nWe conduct an in-depth experimental study on three non-linear activation\nfunctions: Sine-Hyperbolic, Exponential and modified Weibull, showing that\nwhile all bring significant gains the Weibull function performs best thanks to\nits ability to equalise strong activations. The results clearly demonstrate\nthat our ACTNET architecture significantly enhances the discriminative power of\ndeep features, improving significantly over the state-of-the-art retrieval\nresults on all datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 15:24:40 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 13:00:25 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 16:04:46 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Husain", "Syed Sameed", ""], ["Ong", "Eng-Jon", ""], ["Bober", "Miroslaw", ""]]}, {"id": "1907.05813", "submitter": "Antonis Danelakis", "authors": "Giorgos Bouritsas, Stelios Daveas, Antonios Danelakis, Constantinos\n  Rizogiannis and Stelios C. A. Thomopoulos", "title": "Automated Real-time Anomaly Detection in Human Trajectories using\n  Sequence to Sequence Networks", "comments": "AVSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of anomalous trajectories is an important problem with potential\napplications to various domains, such as video surveillance, risk assessment,\nvessel monitoring and high-energy physics. Modeling the distribution of\ntrajectories with statistical approaches has been a challenging task due to the\nfact that such time series are usually non stationary and highly dimensional.\nHowever, modern machine learning techniques provide robust approaches for\ndata-driven modeling and critical information extraction. In this paper, we\npropose a Sequence to Sequence architecture for real-time detection of\nanomalies in human trajectories, in the context of risk-based security. Our\ndetection scheme is tested on a synthetic dataset of diverse and realistic\ntrajectories generated by the ISL iCrowd simulator. The experimental results\nindicate that our scheme accurately detects motion patterns that deviate from\nnormal behaviors and is promising for future real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 16:04:32 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 14:14:51 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bouritsas", "Giorgos", ""], ["Daveas", "Stelios", ""], ["Danelakis", "Antonios", ""], ["Rizogiannis", "Constantinos", ""], ["Thomopoulos", "Stelios C. A.", ""]]}, {"id": "1907.05820", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Cordelia Schmid, Cristian Sminchisescu", "title": "Self-supervised Learning with Geometric Constraints in Monocular Video:\n  Connecting Flow, Depth, and Camera", "comments": "ICCV'19 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GLNet, a self-supervised framework for learning depth, optical\nflow, camera pose and intrinsic parameters from monocular video - addressing\nthe difficulty of acquiring realistic ground-truth for such tasks. We propose\nthree contributions: 1) we design new loss functions that capture multiple\ngeometric constraints (eg. epipolar geometry) as well as an adaptive\nphotometric loss that supports multiple moving objects, rigid and non-rigid, 2)\nwe extend the model such that it predicts camera intrinsics, making it\napplicable to uncalibrated video, and 3) we propose several online refinement\nstrategies that rely on the symmetry of our self-supervised loss in training\nand testing, in particular optimizing model parameters and/or the output of\ndifferent tasks, thus leveraging their mutual interactions. The idea of jointly\noptimizing the system output, under all geometric and photometric constraints\ncan be viewed as a dense generalization of classical bundle adjustment. We\ndemonstrate the effectiveness of our method on KITTI and Cityscapes, where we\noutperform previous self-supervised approaches on multiple tasks. We also show\ngood generalization for transfer learning in YouTube videos.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 16:18:16 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 17:36:43 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Chen", "Yuhua", ""], ["Schmid", "Cordelia", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1907.05852", "submitter": "Qingnan Fan", "authors": "Qingnan Fan, Dongdong Chen, Lu Yuan, Gang Hua, Nenghai Yu, Baoquan\n  Chen", "title": "A General Decoupled Learning Framework for Parameterized Image Operators", "comments": "Published in TPAMI 2019. arXiv admin note: substantial text overlap\n  with arXiv:1807.08186", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different deep networks have been used to approximate, accelerate or\nimprove traditional image operators. Among these traditional operators, many\ncontain parameters which need to be tweaked to obtain the satisfactory results,\nwhich we refer to as parameterized image operators. However, most existing deep\nnetworks trained for these operators are only designed for one specific\nparameter configuration, which does not meet the needs of real scenarios that\nusually require flexible parameters settings. To overcome this limitation, we\npropose a new decoupled learning algorithm to learn from the operator\nparameters to dynamically adjust the weights of a deep network for image\noperators, denoted as the base network. The learned algorithm is formed as\nanother network, namely the weight learning network, which can be end-to-end\njointly trained with the base network. Experiments demonstrate that the\nproposed framework can be successfully applied to many traditional\nparameterized image operators. To accelerate the parameter tuning for practical\nscenarios, the proposed framework can be further extended to dynamically change\nthe weights of only one single layer of the base network while sharing most\ncomputation cost. We demonstrate that this cheap parameter-tuning extension of\nthe proposed decoupled learning framework even outperforms the state-of-the-art\nalternative approaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 00:22:39 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Fan", "Qingnan", ""], ["Chen", "Dongdong", ""], ["Yuan", "Lu", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""], ["Chen", "Baoquan", ""]]}, {"id": "1907.05916", "submitter": "Yahui Liu", "authors": "Yahui Liu, Marco De Nadai, Gloria Zen, Nicu Sebe, Bruno Lepri", "title": "Gesture-to-Gesture Translation in the Wild via Category-Independent\n  Conditional Maps", "comments": "15 pages, 12 figures", "journal-ref": "27th ACM International Conference on Multimedia, 2019", "doi": "10.1145/3343031.3351020", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown Generative Adversarial Networks (GANs) to be\nparticularly effective in image-to-image translations. However, in tasks such\nas body pose and hand gesture translation, existing methods usually require\nprecise annotations, e.g. key-points or skeletons, which are time-consuming to\ndraw. In this work, we propose a novel GAN architecture that decouples the\nrequired annotations into a category label - that specifies the gesture type -\nand a simple-to-draw category-independent conditional map - that expresses the\nlocation, rotation and size of the hand gesture. Our architecture synthesizes\nthe target gesture while preserving the background context, thus effectively\ndealing with gesture translation in the wild. To this aim, we use an attention\nmodule and a rolling guidance approach, which loops the generated images back\ninto the network and produces higher quality images compared to competing\nworks. Thus, our GAN learns to generate new images from simple annotations\nwithout requiring key-points or skeleton labels. Results on two public datasets\nshow that our method outperforms state of the art approaches both\nquantitatively and qualitatively. To the best of our knowledge, no work so far\nhas addressed the gesture-to-gesture translation in the wild by requiring\nuser-friendly annotations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 18:39:27 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 10:24:21 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 08:55:11 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Liu", "Yahui", ""], ["De Nadai", "Marco", ""], ["Zen", "Gloria", ""], ["Sebe", "Nicu", ""], ["Lepri", "Bruno", ""]]}, {"id": "1907.05982", "submitter": "Stefan Lattner", "authors": "Stefan Lattner, Monika D\\\"orfler, Andreas Arzt", "title": "Learning Complex Basis Functions for Invariant Representations of Audio", "comments": "Paper accepted at the 20th International Society for Music\n  Information Retrieval Conference, ISMIR 2019, Delft, The Netherlands,\n  November 4-8; 8 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning features from data has shown to be more successful than using\nhand-crafted features for many machine learning tasks. In music information\nretrieval (MIR), features learned from windowed spectrograms are highly variant\nto transformations like transposition or time-shift. Such variances are\nundesirable when they are irrelevant for the respective MIR task. We propose an\narchitecture called Complex Autoencoder (CAE) which learns features invariant\nto orthogonal transformations. Mapping signals onto complex basis functions\nlearned by the CAE results in a transformation-invariant \"magnitude space\" and\na transformation-variant \"phase space\". The phase space is useful to infer\ntransformations between data pairs. When exploiting the invariance-property of\nthe magnitude space, we achieve state-of-the-art results in audio-to-score\nalignment and repeated section discovery for audio. A PyTorch implementation of\nthe CAE, including the repeated section discovery method, is available online.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 00:23:26 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Lattner", "Stefan", ""], ["D\u00f6rfler", "Monika", ""], ["Arzt", "Andreas", ""]]}, {"id": "1907.06007", "submitter": "Minghui Liao", "authors": "Minghui Liao, Boyu Song, Shangbang Long, Minghang He, Cong Yao, Xiang\n  Bai", "title": "SynthText3D: Synthesizing Scene Text Images from 3D Virtual Worlds", "comments": "Accepted by SCIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep neural networks, the demand for a significant\namount of annotated training data becomes the performance bottlenecks in many\nfields of research and applications. Image synthesis can generate annotated\nimages automatically and freely, which gains increasing attention recently. In\nthis paper, we propose to synthesize scene text images from the 3D virtual\nworlds, where the precise descriptions of scenes, editable\nillumination/visibility, and realistic physics are provided. Different from the\nprevious methods which paste the rendered text on static 2D images, our method\ncan render the 3D virtual scene and text instances as an entirety. In this way,\nreal-world variations, including complex perspective transformations, various\nilluminations, and occlusions, can be realized in our synthesized scene text\nimages. Moreover, the same text instances with various viewpoints can be\nproduced by randomly moving and rotating the virtual camera, which acts as\nhuman eyes. The experiments on the standard scene text detection benchmarks\nusing the generated synthetic data demonstrate the effectiveness and\nsuperiority of the proposed method. The code and synthetic data is available\nat: https://github.com/MhLiao/SynthText3D\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 04:18:04 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 12:17:41 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liao", "Minghui", ""], ["Song", "Boyu", ""], ["Long", "Shangbang", ""], ["He", "Minghang", ""], ["Yao", "Cong", ""], ["Bai", "Xiang", ""]]}, {"id": "1907.06014", "submitter": "Qipei Mei", "authors": "Qipei Mei and Mustafa G\\\"ul", "title": "A Cost Effective Solution for Road Crack Inspection using Cameras and\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic crack detection on pavement surfaces is an important research field\nin the scope of developing an intelligent transportation infrastructure system.\nIn this paper, a cost effective solution for road crack inspection by mounting\ncommercial grade sport camera, GoPro, on the rear of the moving vehicle is\nintroduced. Also, a novel method called ConnCrack combining conditional\nWasserstein generative adversarial network and connectivity maps is proposed\nfor road crack detection. In this method, a 121-layer densely connected neural\nnetwork with deconvolution layers for multi-level feature fusion is used as\ngenerator, and a 5-layer fully convolutional network is used as discriminator.\nTo overcome the scattered output issue related to deconvolution layers,\nconnectivity maps are introduced to represent the crack information within the\nproposed ConnCrack. The proposed method is tested on a publicly available\ndataset as well our collected data. The results show that the proposed method\nachieves state-of-the-art performance compared with other existing methods in\nterms of precision, recall and F1 score.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 05:43:06 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 04:29:26 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Mei", "Qipei", ""], ["G\u00fcl", "Mustafa", ""]]}, {"id": "1907.06023", "submitter": "Xiaotian Chen", "authors": "Xiaotian Chen, Xuejin Chen, Zheng-Jun Zha", "title": "Structure-Aware Residual Pyramid Network for Monocular Depth Estimation", "comments": "7pages, 7figures, Accepted by the 28th International Joint Conference\n  on Artificial Intelligence (IJCAI-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation is an essential task for scene understanding. The\nunderlying structure of objects and stuff in a complex scene is critical to\nrecovering accurate and visually-pleasing depth maps. Global structure conveys\nscene layouts, while local structure reflects shape details. Recently developed\napproaches based on convolutional neural networks (CNNs) significantly improve\nthe performance of depth estimation. However, few of them take into account\nmulti-scale structures in complex scenes. In this paper, we propose a\nStructure-Aware Residual Pyramid Network (SARPN) to exploit multi-scale\nstructures for accurate depth prediction. We propose a Residual Pyramid Decoder\n(RPD) which expresses global scene structure in upper levels to represent\nlayouts, and local structure in lower levels to present shape details. At each\nlevel, we propose Residual Refinement Modules (RRM) that predict residual maps\nto progressively add finer structures on the coarser structure predicted at the\nupper level. In order to fully exploit multi-scale image features, an Adaptive\nDense Feature Fusion (ADFF) module, which adaptively fuses effective features\nfrom all scales for inferring structures of each scale, is introduced.\nExperiment results on the challenging NYU-Depth v2 dataset demonstrate that our\nproposed approach achieves state-of-the-art performance in both qualitative and\nquantitative evaluation. The code is available at\nhttps://github.com/Xt-Chen/SARPN.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 07:31:24 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Chen", "Xiaotian", ""], ["Chen", "Xuejin", ""], ["Zha", "Zheng-Jun", ""]]}, {"id": "1907.06038", "submitter": "Garrick Brazil", "authors": "Garrick Brazil, Xiaoming Liu", "title": "M3D-RPN: Monocular 3D Region Proposal Network for Object Detection", "comments": "To appear in ICCV 2019 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the world in 3D is a critical component of urban autonomous\ndriving. Generally, the combination of expensive LiDAR sensors and stereo RGB\nimaging has been paramount for successful 3D object detection algorithms,\nwhereas monocular image-only methods experience drastically reduced\nperformance. We propose to reduce the gap by reformulating the monocular 3D\ndetection problem as a standalone 3D region proposal network. We leverage the\ngeometric relationship of 2D and 3D perspectives, allowing 3D boxes to utilize\nwell-known and powerful convolutional features generated in the image-space. To\nhelp address the strenuous 3D parameter estimations, we further design\ndepth-aware convolutional layers which enable location specific feature\ndevelopment and in consequence improved 3D scene understanding. Compared to\nprior work in monocular 3D detection, our method consists of only the proposed\n3D region proposal network rather than relying on external networks, data, or\nmultiple stages. M3D-RPN is able to significantly improve the performance of\nboth monocular 3D Object Detection and Bird's Eye View tasks within the KITTI\nurban autonomous driving dataset, while efficiently using a shared multi-class\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 09:40:22 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 15:08:13 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Brazil", "Garrick", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1907.06053", "submitter": "Marek Kopicki", "authors": "Marek Kopicki, Dominik Belter and Jeremy L. Wyatt", "title": "Learning better generative models for dexterous, single-view grasping of\n  novel objects", "comments": "19 pages, 15 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the problem of how to learn to grasp dexterously, so as\nto be able to then grasp novel objects seen only from a single view-point.\nRecently, progress has been made in data-efficient learning of generative grasp\nmodels which transfer well to novel objects. These generative grasp models are\nlearned from demonstration (LfD). One weakness is that, as this paper shall\nshow, grasp transfer under challenging single view conditions is unreliable.\nSecond, the number of generative model elements rises linearly in the number of\ntraining examples. This, in turn, limits the potential of these generative\nmodels for generalisation and continual improvement. In this paper, it is shown\nhow to address these problems. Several technical contributions are made: (i) a\nview-based model of a grasp; (ii) a method for combining and compressing\nmultiple grasp models; (iii) a new way of evaluating contacts that is used both\nto generate and to score grasps. These, together, improve both grasp\nperformance and reduce the number of models learned for grasp transfer. These\nadvances, in turn, also allow the introduction of autonomous training, in which\nthe robot learns from self-generated grasps. Evaluation on a challenging test\nset shows that, with innovations (i)-(iii) deployed, grasp transfer success\nrises from 55.1% to 81.6%. By adding autonomous training this rises to 87.8%.\nThese differences are statistically significant. In total, across all\nexperiments, 539 test grasps were executed on real objects.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 11:37:32 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Kopicki", "Marek", ""], ["Belter", "Dominik", ""], ["Wyatt", "Jeremy L.", ""]]}, {"id": "1907.06062", "submitter": "Nibaran Das", "authors": "Bodhisatwa Mandal, Swarnendu Ghosh, Ritesh Sarkhel, Nibaran Das, Mita\n  Nasipuri", "title": "Using dynamic routing to extract intermediate features for developing\n  scalable capsule networks", "comments": "Second International Conference on Advanced Computational and\n  Communication Paradigms held at Sikkim Manipal Institute of Technology,\n  Sikkim, India during February 25 - 28 , 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capsule networks have gained a lot of popularity in short time due to its\nunique approach to model equivariant class specific properties as capsules from\nimages. However the dynamic routing algorithm comes with a steep computational\ncomplexity. In the proposed approach we aim to create scalable versions of the\ncapsule networks that are much faster and provide better accuracy in problems\nwith higher number of classes. By using dynamic routing to extract intermediate\nfeatures instead of generating output class specific capsules, a large increase\nin the computational speed has been observed. Moreover, by extracting\nequivariant feature capsules instead of class specific capsules, the\ngeneralization capability of the network has also increased as a result of\nwhich there is a boost in accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 12:12:36 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Mandal", "Bodhisatwa", ""], ["Ghosh", "Swarnendu", ""], ["Sarkhel", "Ritesh", ""], ["Das", "Nibaran", ""], ["Nasipuri", "Mita", ""]]}, {"id": "1907.06064", "submitter": "Islem Rekik", "authors": "Can Gafuroglu and Islem Rekik", "title": "Image Evolution Trajectory Prediction and Classification from Baseline\n  using Learning-based Patch Atlas Selection for Early Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients initially diagnosed with early mild cognitive impairment (eMCI) are\nknown to be a clinically heterogeneous group with very subtle patterns of brain\natrophy. To examine the boarders between normal controls (NC) and eMCI,\nMagnetic Resonance Imaging (MRI) was extensively used as a non-invasive imaging\nmodality to pin-down subtle changes in brain images of MCI patients. However,\neMCI research remains limited by the number of available MRI acquisition\ntimepoints. Ideally, one would learn how to diagnose MCI patients in an early\nstage from MRI data acquired at a single timepoint, while leveraging\n'non-existing' follow-up observations. To this aim, we propose novel supervised\nand unsupervised frameworks that learn how to jointly predict and label the\nevolution trajectory of intensity patches, each seeded at a specific brain\nlandmark, from a baseline intensity patch. Specifically, both strategies aim to\nidentify the best training atlas patches at baseline timepoint to predict and\nclassify the evolution trajectory of a given testing baseline patch. The\nsupervised technique learns how to select the best atlas patches by training\nbidirectional mappings from the space of pairwise patch similarities to their\ncorresponding prediction errors -when one patch was used to predict the other.\nOn the other hand, the unsupervised technique learns a manifold of baseline\natlas and testing patches using multiple kernels to well capture patch\ndistributions at multiple scales. Once the best baseline atlas patches are\nselected, we retrieve their evolution trajectories and average them to predict\nthe evolution trajectory of the testing baseline patch. Next, we input the\npredicted trajectories to an ensemble of linear classifiers, each trained at a\nspecific landmark. Our classification accuracy increased by up to 10% points in\ncomparison to single timepoint-based classification methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 12:17:00 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Gafuroglu", "Can", ""], ["Rekik", "Islem", ""]]}, {"id": "1907.06067", "submitter": "Iuliia Saveleva", "authors": "Evgenii Razinkov, Iuliia Saveleva, Ji\\v{r}i Matas", "title": "ALFA: Agglomerative Late Fusion Algorithm for Object Detection", "comments": "E. Razinkov, I. Saveleva and J. Matas, \"ALFA: Agglomerative Late\n  Fusion Algorithm for Object Detection,\" 2018 24th International Conference on\n  Pattern Recognition (ICPR), Beijing, 2018, pp. 2594-2599", "journal-ref": null, "doi": "10.1109/ICPR.2018.8545182", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ALFA - a novel late fusion algorithm for object detection. ALFA is\nbased on agglomerative clustering of object detector predictions taking into\nconsideration both the bounding box locations and the class scores. Each\ncluster represents a single object hypothesis whose location is a weighted\ncombination of the clustered bounding boxes.\n  ALFA was evaluated using combinations of a pair (SSD and DeNet) and a triplet\n(SSD, DeNet and Faster R-CNN) of recent object detectors that are close to the\nstate-of-the-art. ALFA achieves state of the art results on PASCAL VOC 2007 and\nPASCAL VOC 2012, outperforming the individual detectors as well as baseline\ncombination strategies, achieving up to 32% lower error than the best\nindividual detectors and up to 6% lower error than the reference fusion\nalgorithm DBF - Dynamic Belief Fusion.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 12:30:37 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Razinkov", "Evgenii", ""], ["Saveleva", "Iuliia", ""], ["Matas", "Ji\u0159i", ""]]}, {"id": "1907.06071", "submitter": "Lei Zhang", "authors": "Lei Zhang, Weihai Chen, Chao Hu, Xingming Wu, Zhengguo Li", "title": "S&CNet: Monocular Depth Completion for Autonomous Systems and 3D\n  Reconstruction", "comments": "10 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense depth completion is essential for autonomous systems and 3D\nreconstruction. In this paper, a lightweight yet efficient network (S\\&CNet) is\nproposed to obtain a good trade-off between efficiency and accuracy for the\ndense depth completion. A dual-stream attention module (S\\&C enhancer) is\nintroduced to measure both spatial-wise and the channel-wise global-range\nrelationship of extracted features so as to improve the performance. A\ncoarse-to-fine network is designed and the proposed S\\&C enhancer is plugged\ninto the coarse estimation network between its encoder and decoder network.\nExperimental results demonstrate that our approach achieves competitive\nperformance with existing works on KITTI dataset but almost four times faster.\nThe proposed S\\&C enhancer can be plugged into other existing works and boost\ntheir performance significantly with a negligible additional computational\ncost.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 13:07:31 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 09:32:59 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zhang", "Lei", ""], ["Chen", "Weihai", ""], ["Hu", "Chao", ""], ["Wu", "Xingming", ""], ["Li", "Zhengguo", ""]]}, {"id": "1907.06082", "submitter": "Congcong Wang", "authors": "Congcong Wang, Faouzi Alaya Cheikh, Azeddine Beghdadi, Ole Jakob Elle", "title": "Adaptive Context Encoding Module for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The object sizes in images are diverse, therefore, capturing multiple scale\ncontext information is essential for semantic segmentation. Existing context\naggregation methods such as pyramid pooling module (PPM) and atrous spatial\npyramid pooling (ASPP) design different pooling size or atrous rate, such that\nmultiple scale information is captured. However, the pooling sizes and atrous\nrates are chosen manually and empirically. In order to capture object context\ninformation adaptively, in this paper, we propose an adaptive context encoding\n(ACE) module based on deformable convolution operation to argument multiple\nscale information. Our ACE module can be embedded into other Convolutional\nNeural Networks (CNN) easily for context aggregation. The effectiveness of the\nproposed module is demonstrated on Pascal-Context and ADE20K datasets. Although\nour proposed ACE only consists of three deformable convolution blocks, it\noutperforms PPM and ASPP in terms of mean Intersection of Union (mIoU) on both\ndatasets. All the experiment study confirms that our proposed module is\neffective as compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 14:02:21 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Wang", "Congcong", ""], ["Cheikh", "Faouzi Alaya", ""], ["Beghdadi", "Azeddine", ""], ["Elle", "Ole Jakob", ""]]}, {"id": "1907.06091", "submitter": "Erez Posner", "authors": "Erez Posner, Rami Hagege", "title": "Motion Segmentation Using Locally Affine Atom Voting", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel method for motion segmentation called LAAV (Locally Affine\nAtom Voting). Our model's main novelty is using sets of features to segment\nmotion for all features in the scene. LAAV acts as a pre-processing pipeline\nstage for features in the image, followed by a fine-tuned version of the\nstate-of-the-art Random Voting (RV) method. Unlike standard approaches, LAAV\nsegments motion using feature-set affinities instead of pair-wise affinities\nbetween all features; therefore, it significantly simplifies complex scenarios\nand reduces the computational cost without a loss of accuracy. We describe how\nthe challenges encountered by using previously suggested approaches are\naddressed using our model. We then compare our algorithm with several\nstate-of-the-art methods. Experiments shows that our approach achieves the most\naccurate motion segmentation results and, in the presence of measurement noise,\nachieves comparable results to the other algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 14:55:32 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Posner", "Erez", ""], ["Hagege", "Rami", ""]]}, {"id": "1907.06099", "submitter": "Yueming Jin", "authors": "Yueming Jin, Huaxia Li, Qi Dou, Hao Chen, Jing Qin, Chi-Wing Fu,\n  Pheng-Ann Heng", "title": "Multi-Task Recurrent Convolutional Network with Correlation Loss for\n  Surgical Video Analysis", "comments": "Minor Revision at Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical tool presence detection and surgical phase recognition are two\nfundamental yet challenging tasks in surgical video analysis and also very\nessential components in various applications in modern operating rooms. While\nthese two analysis tasks are highly correlated in clinical practice as the\nsurgical process is well-defined, most previous methods tackled them\nseparately, without making full use of their relatedness. In this paper, we\npresent a novel method by developing a multi-task recurrent convolutional\nnetwork with correlation loss (MTRCNet-CL) to exploit their relatedness to\nsimultaneously boost the performance of both tasks. Specifically, our proposed\nMTRCNet-CL model has an end-to-end architecture with two branches, which share\nearlier feature encoders to extract general visual features while holding\nrespective higher layers targeting for specific tasks. Given that temporal\ninformation is crucial for phase recognition, long-short term memory (LSTM) is\nexplored to model the sequential dependencies in the phase recognition branch.\nMore importantly, a novel and effective correlation loss is designed to model\nthe relatedness between tool presence and phase identification of each video\nframe, by minimizing the divergence of predictions from the two branches.\nMutually leveraging both low-level feature sharing and high-level prediction\ncorrelating, our MTRCNet-CL method can encourage the interactions between the\ntwo tasks to a large extent, and hence can bring about benefits to each other.\nExtensive experiments on a large surgical video dataset (Cholec80) demonstrate\noutstanding performance of our proposed method, consistently exceeding the\nstate-of-the-art methods by a large margin (e.g., 89.1% v.s. 81.0% for the mAP\nin tool presence detection and 87.4% v.s. 84.5% for F1 score in phase\nrecognition). The code can be found on our project website.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 15:49:00 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Jin", "Yueming", ""], ["Li", "Huaxia", ""], ["Dou", "Qi", ""], ["Chen", "Hao", ""], ["Qin", "Jing", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1907.06119", "submitter": "Nibaran Das", "authors": "Swarnendu Ghosh, Nibaran Das, Ishita Das, Ujjwal Maulik", "title": "Understanding Deep Learning Techniques for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning community has been overwhelmed by a plethora of deep\nlearning based approaches. Many challenging computer vision tasks such as\ndetection, localization, recognition and segmentation of objects in\nunconstrained environment are being efficiently addressed by various types of\ndeep neural networks like convolutional neural networks, recurrent networks,\nadversarial networks, autoencoders and so on. While there have been plenty of\nanalytical studies regarding the object detection or recognition domain, many\nnew deep learning techniques have surfaced with respect to image segmentation\ntechniques. This paper approaches these various deep learning techniques of\nimage segmentation from an analytical perspective. The main goal of this work\nis to provide an intuitive understanding of the major techniques that has made\nsignificant contribution to the image segmentation domain. Starting from some\nof the traditional image segmentation approaches, the paper progresses\ndescribing the effect deep learning had on the image segmentation domain.\nThereafter, most of the major segmentation algorithms have been logically\ncategorized with paragraphs dedicated to their unique contribution. With an\nample amount of intuitive explanations, the reader is expected to have an\nimproved ability to visualize the internal dynamics of these processes.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 19:23:42 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Ghosh", "Swarnendu", ""], ["Das", "Nibaran", ""], ["Das", "Ishita", ""], ["Maulik", "Ujjwal", ""]]}, {"id": "1907.06134", "submitter": "Peiye Zhuang", "authors": "Peiye Zhuang, Alexander G. Schwing, Sanmi Koyejo", "title": "FMRI data augmentation via synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an empirical evaluation of fMRI data augmentation via synthesis.\nFor synthesis we use generative mod-els trained on real neuroimaging data to\nproduce novel task-dependent functional brain images. Analyzed generative\nmod-els include classic approaches such as the Gaussian mixture model (GMM),\nand modern implicit generative models such as the generative adversarial\nnetwork (GAN) and the variational auto-encoder (VAE). In particular, the\nproposed GAN and VAE models utilize 3-dimensional convolutions, which enables\nmodeling of high-dimensional brain image tensors with structured spatial\ncorrelations. The synthesized datasets are then used to augment classifiers\ndesigned to predict cognitive and behavioural outcomes. Our results suggest\nthat the proposed models are able to generate high-quality synthetic brain\nimages which are diverse and task-dependent. Perhaps most importantly, the\nperformance improvements of data aug-mentation via synthesis are shown to be\ncomplementary to the choice of the predictive model. Thus, our results suggest\nthat data augmentation via synthesis is a promising approach to address the\nlimited availability of fMRI data, and to improve the quality of predictive\nfMRI models.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 21:30:41 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Zhuang", "Peiye", ""], ["Schwing", "Alexander G.", ""], ["Koyejo", "Sanmi", ""]]}, {"id": "1907.06143", "submitter": "Andong Cao", "authors": "Lingzhi Zhang, Andong Cao, Rui Li, Jianbo Shi", "title": "Neural Embedding for Physical Manipulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In common real-world robotic operations, action and state spaces can be vast\nand sometimes unknown, and observations are often relatively sparse. How do we\nlearn the full topology of action and state spaces when given only few and\nsparse observations? Inspired by the properties of grid cells in mammalian\nbrains, we build a generative model that enforces a normalized pairwise\ndistance constraint between the latent space and output space to achieve\ndata-efficient discovery of output spaces. This method achieves substantially\nbetter results than prior generative models, such as Generative Adversarial\nNetworks (GANs) and Variational Auto-Encoders (VAEs). Prior models have the\ncommon issue of mode collapse and thus fail to explore the full topology of\noutput space. We demonstrate the effectiveness of our model on various datasets\nboth qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 22:57:23 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Zhang", "Lingzhi", ""], ["Cao", "Andong", ""], ["Li", "Rui", ""], ["Shi", "Jianbo", ""]]}, {"id": "1907.06147", "submitter": "Sohaib Ahmad", "authors": "Sohaib Ahmad, Benjamin Fuller", "title": "ThirdEye: Triplet Based Iris Recognition without Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most iris recognition pipelines involve three stages: segmenting into\niris/non-iris pixels, normalization the iris region to a fixed area, and\nextracting relevant features for comparison. Given recent advances in deep\nlearning it is prudent to ask which stages are required for accurate iris\nrecognition. Lojez et al. (IWBF 2019) recently concluded that the segmentation\nstage is still crucial for good accuracy.We ask if normalization is beneficial?\nTowards answering this question, we develop a new iris recognition system\ncalled ThirdEye based on triplet convolutional neural networks (Schroff et al.,\nICCV 2015). ThirdEye directly uses segmented images without normalization. We\nobserve equal error rates of 1.32%, 9.20%, and 0.59% on the ND-0405, UbirisV2,\nand IITD datasets respectively. For IITD, the most constrained dataset, this\nimproves on the best prior work. However, for ND-0405 and UbirisV2,our equal\nerror rate is slightly worse than prior systems. Our concluding hypothesis is\nthat normalization is more important for less constrained environments.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 23:27:24 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Ahmad", "Sohaib", ""], ["Fuller", "Benjamin", ""]]}, {"id": "1907.06160", "submitter": "Ziad Al-Halah", "authors": "Ziad Al-Halah, Andrew Aitken, Wenzhe Shi, Jose Caballero", "title": "Smile, Be Happy :) Emoji Embedding for Visual Sentiment Analysis", "comments": "International Conference on Computer Vision (ICCV 2019) Workshops.\n  Project page and the Visual Smiley Dataset:\n  https://www.cs.utexas.edu/~ziad/emoji_visual_sentiment.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the lack of large-scale datasets, the prevailing approach in visual\nsentiment analysis is to leverage models trained for object classification in\nlarge datasets like ImageNet. However, objects are sentiment neutral which\nhinders the expected gain of transfer learning for such tasks. In this work, we\npropose to overcome this problem by learning a novel sentiment-aligned image\nembedding that is better suited for subsequent visual sentiment analysis. Our\nembedding leverages the intricate relation between emojis and images in\nlarge-scale and readily available data from social media. Emojis are\nlanguage-agnostic, consistent, and carry a clear sentiment signal which make\nthem an excellent proxy to learn a sentiment aligned embedding. Hence, we\nconstruct a novel dataset of 4 million images collected from Twitter with their\nassociated emojis. We train a deep neural model for image embedding using emoji\nprediction task as a proxy. Our evaluation demonstrates that the proposed\nembedding outperforms the popular object-based counterpart consistently across\nseveral sentiment analysis benchmarks. Furthermore, without bell and whistles,\nour compact, effective and simple embedding outperforms the more elaborate and\ncustomized state-of-the-art deep models on these public benchmarks.\nAdditionally, we introduce a novel emoji representation based on their visual\nemotional response which supports a deeper understanding of the emoji modality\nand their usage on social media.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 03:29:02 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 01:07:16 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2020 02:48:03 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Al-Halah", "Ziad", ""], ["Aitken", "Andrew", ""], ["Shi", "Wenzhe", ""], ["Caballero", "Jose", ""]]}, {"id": "1907.06167", "submitter": "Karan Sikka", "authors": "Parneet Kaur, Karan Sikka, Weijun Wang, Serge Belongie, Ajay Divakaran", "title": "FoodX-251: A Dataset for Fine-grained Food Classification", "comments": "Published at Fine-Grained Visual Categorization Workshop, CVPR19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food classification is a challenging problem due to the large number of\ncategories, high visual similarity between different foods, as well as the lack\nof datasets for training state-of-the-art deep models. Solving this problem\nwill require advances in both computer vision models as well as datasets for\nevaluating these models. In this paper we focus on the second aspect and\nintroduce FoodX-251, a dataset of 251 fine-grained food categories with 158k\nimages collected from the web. We use 118k images as a training set and provide\nhuman verified labels for 40k images that can be used for validation and\ntesting. In this work, we outline the procedure of creating this dataset and\nprovide relevant baselines with deep learning models. The FoodX-251 dataset has\nbeen used for organizing iFood-2019 challenge in the Fine-Grained Visual\nCategorization workshop (FGVC6 at CVPR 2019) and is available for download.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 05:01:31 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Kaur", "Parneet", ""], ["Sikka", "Karan", ""], ["Wang", "Weijun", ""], ["Belongie", "Serge", ""], ["Divakaran", "Ajay", ""]]}, {"id": "1907.06194", "submitter": "Weilin Fu", "authors": "Weilin Fu, Katharina Breininger, Roman Schaffert, Nishant Ravikumar,\n  Andreas Maier", "title": "A Divide-and-Conquer Approach towards Understanding Deep Networks", "comments": "This paper is accepted in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved tremendous success in various fields\nincluding medical image segmentation. However, they have long been criticized\nfor being a black-box, in that interpretation, understanding and correcting\narchitectures is difficult as there is no general theory for deep neural\nnetwork design. Previously, precision learning was proposed to fuse deep\narchitectures and traditional approaches. Deep networks constructed in this way\nbenefit from the original known operator, have fewer parameters, and improved\ninterpretability. However, they do not yield state-of-the-art performance in\nall applications. In this paper, we propose to analyze deep networks using\nknown operators, by adopting a divide-and-conquer strategy to replace network\ncomponents, whilst retaining its performance. The task of retinal vessel\nsegmentation is investigated for this purpose. We start with a high-performance\nU-Net and show by step-by-step conversion that we are able to divide the\nnetwork into modules of known operators. The results indicate that a\ncombination of a trainable guided filter and a trainable version of the Frangi\nfilter yields a performance at the level of U-Net (AUC 0.974 vs. 0.972) with a\ntremendous reduction in parameters (111,536 vs. 9,575). In addition, the\ntrained layers can be mapped back into their original algorithmic\ninterpretation and analyzed using standard tools of signal processing.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 09:50:45 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Fu", "Weilin", ""], ["Breininger", "Katharina", ""], ["Schaffert", "Roman", ""], ["Ravikumar", "Nishant", ""], ["Maier", "Andreas", ""]]}, {"id": "1907.06206", "submitter": "Thanh Huy Nguyen", "authors": "Thanh Huy Nguyen, Sylvie Daniel, Didier Gueriot, Christophe Sintes,\n  Jean-Marc Le Caillec", "title": "Unsupervised Automatic Building Extraction Using Active Contour Model on\n  Unregistered Optical Imagery and Airborne LiDAR Data", "comments": "PIA19 - Photogrammetric Image Analysis 2019 which will be held in\n  conjunction with MRSS19 - Munich Remote Sensing Symposium 2019 on September\n  18th-20th, 2019 in Munich, Germany. Proceeding: The International Archives of\n  the Photogrammetry, Remote Sensing and Spatial Information Sciences", "journal-ref": "Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci. XLII-2/W16\n  (2019) 181-188", "doi": "10.5194/isprs-archives-XLII-2-W16-181-2019", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of buildings in urban scenes has become a subject of\ngrowing interest in the domain of photogrammetry and remote sensing,\nparticularly with the emergence of LiDAR systems since mid-1990s. However, in\nreality, this task is still very challenging due to the complexity of building\nsize and shapes, as well as its surrounding environment. Active contour model,\ncolloquially called snake model, which has been extensively used in many\napplications in computer vision and image processing, is also applied to\nextract buildings from aerial/satellite imagery. Motivated by the limitations\nof existing snake models addressing to the building extraction, this paper\npresents an unsupervised and fully automatic snake model to extract buildings\nusing optical imagery and an unregistered airborne LiDAR dataset, without\nmanual initial points or training data. The proposed method is shown to be\ncapable of extracting buildings with varying color from complex environments,\nand yielding high overall accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 11:18:56 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Nguyen", "Thanh Huy", ""], ["Daniel", "Sylvie", ""], ["Gueriot", "Didier", ""], ["Sintes", "Christophe", ""], ["Caillec", "Jean-Marc Le", ""]]}, {"id": "1907.06247", "submitter": "Thinh Hoang", "authors": "Thinh Hoang Dinh, Hieu Le Thi Hong, Tri Ngo Dinh", "title": "State Estimation in Visual Inertial Autonomous Helicopter Landing Using\n  Optimisation on Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous helicopter landing is a challenging task that requires precise\ninformation about the aircraft states regarding the helicopters position,\nattitude, as well as position of the helipad. To this end, we propose a\nsolution that fuses data from an Inertial Measurement Unit (IMU) and a\nmonocular camera which is capable of detecting helipads position in the image\nplane. The algorithm utilises manifold based nonlinear optimisation over\npreintegrated IMU measurements and reprojection error in temporally uniformly\ndistributed keyframes, exhibiting good performance in terms of accuracy and\nbeing computationally feasible. Our contributions of this paper are the formal\naddress of the landmarks Jacobian expressions and the adaptation of equality\nconstrained Gauss-Newton method to this specific problem. Numerical simulations\non MATLAB/Simulink confirm the validity of given claims.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 17:01:50 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Dinh", "Thinh Hoang", ""], ["Hong", "Hieu Le Thi", ""], ["Dinh", "Tri Ngo", ""]]}, {"id": "1907.06286", "submitter": "Viktor Toth", "authors": "Viktor T\\'oth, Lauri Parkkonen", "title": "Autoencoding sensory substitution", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.10576.87048", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tens of millions of people live blind, and their number is ever increasing.\nVisual-to-auditory sensory substitution (SS) encompasses a family of cheap,\ngeneric solutions to assist the visually impaired by conveying visual\ninformation through sound. The required SS training is lengthy: months of\neffort is necessary to reach a practical level of adaptation. There are two\nreasons for the tedious training process: the elongated substituting audio\nsignal, and the disregard for the compressive characteristics of the human\nhearing system. To overcome these obstacles, we developed a novel class of SS\nmethods, by training deep recurrent autoencoders for image-to-sound conversion.\nWe successfully trained deep learning models on different datasets to execute\nvisual-to-auditory stimulus conversion. By constraining the visual space, we\ndemonstrated the viability of shortened substituting audio signals, while\nproposing mechanisms, such as the integration of computational hearing models,\nto optimally convey visual features in the substituting stimulus as\nperceptually discernible auditory components. We tested our approach in two\nseparate cases. In the first experiment, the author went blindfolded for 5\ndays, while performing SS training on hand posture discrimination. The second\nexperiment assessed the accuracy of reaching movements towards objects on a\ntable. In both test cases, above-chance-level accuracy was attained after a few\nhours of training. Our novel SS architecture broadens the horizon of\nrehabilitation methods engineered for the visually impaired. Further\nimprovements on the proposed model shall yield hastened rehabilitation of the\nblind and a wider adaptation of SS devices as a consequence.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 21:58:10 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["T\u00f3th", "Viktor", ""], ["Parkkonen", "Lauri", ""]]}, {"id": "1907.06291", "submitter": "Deyan Petrov", "authors": "Deyan Petrov, Timothy M. Hospedales", "title": "Measuring the Transferability of Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial examples are of wide concern due to their impact on the\nreliability of contemporary machine learning systems. Effective adversarial\nexamples are mostly found via white-box attacks. However, in some cases they\ncan be transferred across models, thus enabling them to attack black-box\nmodels. In this work we evaluate the transferability of three adversarial\nattacks - the Fast Gradient Sign Method, the Basic Iterative Method, and the\nCarlini & Wagner method, across two classes of models - the VGG class(using\nVGG16, VGG19 and an ensemble of VGG16 and VGG19), and the Inception\nclass(Inception V3, Xception, Inception Resnet V2, and an ensemble of the\nthree). We also outline the problems with the assessment of transferability in\nthe current body of research and attempt to amend them by picking specific\n\"strong\" parameters for the attacks, and by using a L-Infinity clipping\ntechnique and the SSIM metric for the final evaluation of the attack\ntransferability.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 22:20:58 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Petrov", "Deyan", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1907.06296", "submitter": "Ivan Molodetskikh", "authors": "Ivan Molodetskikh, Mikhail Erofeev, Dmitry Vatolin", "title": "Perceptually Motivated Method for Image Inpainting Comparison", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of automatic image inpainting has progressed rapidly in recent\nyears, but no one has yet proposed a standard method of evaluating algorithms.\nThis absence is due to the problem's challenging nature: image-inpainting\nalgorithms strive for realism in the resulting images, but realism is a\nsubjective concept intrinsic to human perception. Existing objective\nimage-quality metrics provide a poor approximation of what humans consider more\nor less realistic.\n  To improve the situation and to better organize both prior and future\nresearch in this field, we conducted a subjective comparison of nine\nstate-of-the-art inpainting algorithms and propose objective quality metrics\nthat exhibit high correlation with the results of our comparison.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 23:50:42 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Molodetskikh", "Ivan", ""], ["Erofeev", "Mikhail", ""], ["Vatolin", "Dmitry", ""]]}, {"id": "1907.06312", "submitter": "Xiaoyan Li", "authors": "Xiaoyan Li, Iluju Kiringa, Tet Yeap, Xiaodan Zhu, Yifeng Li", "title": "Exploring Deep Anomaly Detection Methods Based on Capsule Net", "comments": "Presented in the \"ICML 2019 Workshop on Uncertainty & Robustness in\n  Deep Learning\", June 14, Long Beach, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop and explore deep anomaly detection techniques based\non the capsule network (CapsNet) for image data. Being able to encoding\nintrinsic spatial relationship between parts and a whole, CapsNet has been\napplied as both a classifier and deep autoencoder. This inspires us to design a\nprediction-probability-based and a reconstruction-error-based normality score\nfunctions for evaluating the \"outlierness\" of unseen images. Our results on\nthree datasets demonstrate that the prediction-probability-based method\nperforms consistently well, while the reconstruction-error-based approach is\nrelatively sensitive to the similarity between labeled and unlabeled images.\nFurthermore, both of the CapsNet-based methods outperform the principled\nbenchmark methods in many cases.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 02:15:58 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Li", "Xiaoyan", ""], ["Kiringa", "Iluju", ""], ["Yeap", "Tet", ""], ["Zhu", "Xiaodan", ""], ["Li", "Yifeng", ""]]}, {"id": "1907.06319", "submitter": "Vishwesh Nath", "authors": "Vishwesh Nath, Ilwoo Lyu, Kurt G. Schilling, Prasanna Parvathaneni,\n  Colin B. Hansen, Yucheng Tang, Yuankai Huo, Vaibhav A. Janve, Yurui Gao,\n  Iwona Stepniewska, Adam W. Anderson, Bennett A. Landman", "title": "Enabling Multi-Shell b-Value Generalizability of Data-Driven Diffusion\n  Models with Deep SHORE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-voxel models of the diffusion signal are essential for interpreting\norganization of the tissue environment at micrometer level with data at\nmillimeter resolution. Recent advances in data driven methods have enabled\ndirect compari-son and optimization of methods for in-vivo data with externally\nvalidated histological sections with both 2-D and 3-D histology. Yet, all\nexisting methods make limiting assumptions of either (1) model-based linkages\nbetween b-values or (2) limited associations with single shell data. We\ngeneralize prior deep learning models that used single shell spherical harmonic\ntransforms to integrate the re-cently developed simple harmonic oscillator\nreconstruction (SHORE) basis. To enable learning on the SHORE manifold, we\npresent an alternative formulation of the fiber orientation distribution (FOD)\nobject using the SHORE basis while rep-resenting the observed diffusion\nweighted data in the SHORE basis. To ensure consistency of hyper-parameter\noptimization for SHORE, we present our Deep SHORE approach to learn on a\ndata-optimized manifold. Deep SHORE is evalu-ated with eight-fold\ncross-validation of a preclinical MRI-histology data with four b-values.\nGeneralizability of in-vivo human data is evaluated on two separate 3T MRI\nscanners. Specificity in terms of angular correlation (ACC) with the\npreclinical data improved on single shell: 0.78 relative to 0.73 and 0.73,\nmulti-shell: 0.80 relative to 0.74 (p < 0.001). In the in-vivo human data, Deep\nSHORE was more consistent across scanners with 0.63 relative to other\nmulti-shell methods 0.39, 0.52 and 0.57 in terms of ACC. In conclusion, Deep\nSHORE is a promising method to enable data driven learning with DW-MRI under\nconditions with varying b-values, number of diffusion shells, and gradient\ndirections per shell.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 03:05:00 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 16:51:44 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 15:42:29 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Nath", "Vishwesh", ""], ["Lyu", "Ilwoo", ""], ["Schilling", "Kurt G.", ""], ["Parvathaneni", "Prasanna", ""], ["Hansen", "Colin B.", ""], ["Tang", "Yucheng", ""], ["Huo", "Yuankai", ""], ["Janve", "Vaibhav A.", ""], ["Gao", "Yurui", ""], ["Stepniewska", "Iwona", ""], ["Anderson", "Adam W.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1907.06327", "submitter": "Rohan Lekhwani", "authors": "Rohan Lekhwani, Bhupendra Singh", "title": "FastV2C-HandNet: Fast Voxel to Coordinate Hand Pose Estimation with 3D\n  Convolutional Neural Networks", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pose estimation from monocular depth images has been an important and\nchallenging problem in the Computer Vision community. In this paper, we present\na novel approach to estimate 3D hand joint locations from 2D depth images.\nUnlike most of the previous methods, our model captures the 3D spatial\ninformation from a depth image thereby giving it a greater understanding of the\ninput. We voxelize the input depth map to capture the 3D features of the input\nand perform 3D data augmentations to make our network robust to real-world\nimages. Our network is trained in an end-to-end manner which reduces time and\nspace complexity significantly when compared to other methods. Through\nextensive experiments, we show that our model outperforms state-of-the-art\nmethods with respect to the time it takes to train and predict 3D hand joint\nlocations. This makes our method more suitable for real-world hand pose\nestimation scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 04:04:01 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 07:06:18 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 14:31:45 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Lekhwani", "Rohan", ""], ["Singh", "Bhupendra", ""]]}, {"id": "1907.06358", "submitter": "Ziqiang Li", "authors": "Ziqiang Li, Rentuo Tao, Qianrun Wu, Bin Li", "title": "DA-RefineNet:A Dual Input Whole Slide Image Segmentation Algorithm Based\n  on Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic medical image segmentation has wide applications for disease\ndiagnosing. However, it is much more challenging than natural optical image\nsegmentation due to the high-resolution of medical images and the corresponding\nhuge computation cost. The sliding window is a commonly used technique for\nwhole slide image (WSI) segmentation, however, for these methods based on the\nsliding window, the main drawback is lacking global contextual information for\nsupervision. In this paper, we propose a dual-inputs attention network (denoted\nas DA-RefineNet) for WSI segmentation, where both local fine-grained\ninformation and global coarse information can be efficiently utilized.\nSufficient comparative experiments are conducted to evaluate the effectiveness\nof the proposed method, the results prove that the proposed method can achieve\nbetter performance on WSI segmentation compared to methods relying on\nsingle-input.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 08:15:48 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 03:01:14 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 03:38:14 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Li", "Ziqiang", ""], ["Tao", "Rentuo", ""], ["Wu", "Qianrun", ""], ["Li", "Bin", ""]]}, {"id": "1907.06370", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert, Catherine Herold, Kuider Slimani and C\\'edric Vidal", "title": "Multimodal deep networks for text and image-based document\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of document images is a critical step for archival of old\nmanuscripts, online subscription and administrative procedures. Computer vision\nand deep learning have been suggested as a first solution to classify documents\nbased on their visual appearance. However, achieving the fine-grained\nclassification that is required in real-world setting cannot be achieved by\nvisual analysis alone. Often, the relevant information is in the actual text\ncontent of the document. We design a multimodal neural network that is able to\nlearn from word embeddings, computed on text extracted by OCR, and from the\nimage. We show that this approach boosts pure image accuracy by 3% on\nTobacco3482 and RVL-CDIP augmented by our new QS-OCR text dataset\n(https://github.com/Quicksign/ocrized-text-dataset), even without clean text\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 08:43:49 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Audebert", "Nicolas", ""], ["Herold", "Catherine", ""], ["Slimani", "Kuider", ""], ["Vidal", "C\u00e9dric", ""]]}, {"id": "1907.06371", "submitter": "Ali Cheraghian", "authors": "Ali Cheraghian, Shafin Rahman, Dylan Campbell, Lars Petersson", "title": "Mitigating the Hubness Problem for Zero-Shot Learning of 3D Objects", "comments": "BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of advanced 3D sensors has enabled many objects to be\ncaptured in the wild at a large scale, and a 3D object recognition system may\ntherefore encounter many objects for which the system has received no training.\nZero-Shot Learning (ZSL) approaches can assist such systems in recognizing\npreviously unseen objects. Applying ZSL to 3D point cloud objects is an\nemerging topic in the area of 3D vision, however, a significant problem that\nZSL often suffers from is the so-called hubness problem, which is when a model\nis biased to predict only a few particular labels for most of the test\ninstances. We observe that this hubness problem is even more severe for 3D\nrecognition than for 2D recognition. One reason for this is that in 2D one can\nuse pre-trained networks trained on large datasets like ImageNet, which\nproduces high-quality features. However, in the 3D case there are no such\nlarge-scale, labelled datasets available for pre-training which means that the\nextracted 3D features are of poorer quality which, in turn, exacerbates the\nhubness problem. In this paper, we therefore propose a loss to specifically\naddress the hubness problem. Our proposed method is effective for both\nZero-Shot and Generalized Zero-Shot Learning, and we perform extensive\nevaluations on the challenging datasets ModelNet40, ModelNet10, McGill and\nSHREC2015. A new state-of-the-art result for both zero-shot tasks in the 3D\ncase is established.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 08:47:14 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Cheraghian", "Ali", ""], ["Rahman", "Shafin", ""], ["Campbell", "Dylan", ""], ["Petersson", "Lars", ""]]}, {"id": "1907.06390", "submitter": "Yuntao Chen", "authors": "Haiping Wu, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang", "title": "Sequence Level Semantics Aggregation for Video Object Detection", "comments": "ICCV 2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video objection detection (VID) has been a rising research direction in\nrecent years. A central issue of VID is the appearance degradation of video\nframes caused by fast motion. This problem is essentially ill-posed for a\nsingle frame. Therefore, aggregating features from other frames becomes a\nnatural choice. Existing methods rely heavily on optical flow or recurrent\nneural networks for feature aggregation. However, these methods emphasize more\non the temporally nearby frames. In this work, we argue that aggregating\nfeatures in the full-sequence level will lead to more discriminative and robust\nfeatures for video object detection. To achieve this goal, we devise a novel\nSequence Level Semantics Aggregation (SELSA) module. We further demonstrate the\nclose relationship between the proposed method and the classic spectral\nclustering method, providing a novel view for understanding the VID problem. We\ntest the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and\nachieve new state-of-the-art results. Our method does not need complicated\npostprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the\npipeline simple and clean.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 09:37:40 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 02:38:11 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Wu", "Haiping", ""], ["Chen", "Yuntao", ""], ["Wang", "Naiyan", ""], ["Zhang", "Zhaoxiang", ""]]}, {"id": "1907.06406", "submitter": "Xiaodong Cun", "authors": "Xiaodong Cun and Chi-Man Pun", "title": "Improving the Harmony of the Composite Image by Spatial-Separated\n  Attention Module", "comments": "Accepted by IEEE Transactions on Image Processing (TIP) 2020", "journal-ref": null, "doi": "10.1109/TIP.2020.2975979", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image composition is one of the most important applications in image\nprocessing. However, the inharmonious appearance between the spliced region and\nbackground degrade the quality of the image. Thus, we address the problem of\nImage Harmonization: Given a spliced image and the mask of the spliced region,\nwe try to harmonize the \"style\" of the pasted region with the background\n(non-spliced region). Previous approaches have been focusing on learning\ndirectly by the neural network. In this work, we start from an empirical\nobservation: the differences can only be found in the spliced region between\nthe spliced image and the harmonized result while they share the same semantic\ninformation and the appearance in the non-spliced region. Thus, in order to\nlearn the feature map in the masked region and the others individually, we\npropose a novel attention module named Spatial-Separated Attention Module\n(S2AM). Furthermore, we design a novel image harmonization framework by\ninserting the S2AM in the coarser low-level features of the Unet structure in\ntwo different ways. Besides image harmonization, we make a big step for\nharmonizing the composite image without the specific mask under previous\nobservation. The experiments show that the proposed S2AM performs better than\nother state-of-the-art attention modules in our task. Moreover, we demonstrate\nthe advantages of our model against other state-of-the-art image harmonization\nmethods via criteria from multiple points of view. Code is available at\nhttps://github.com/vinthony/s2am\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 10:06:42 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 05:35:21 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 09:09:38 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Cun", "Xiaodong", ""], ["Pun", "Chi-Man", ""]]}, {"id": "1907.06417", "submitter": "Raul Fernandez", "authors": "Raul Fernandez-Fernandez, Juan G. Victores, David Estevez and Carlos\n  Balaguer", "title": "Quick, Stat!: A Statistical Analysis of the Quick, Draw! Dataset", "comments": "12 pages, Eurosim 2019", "journal-ref": null, "doi": "10.11128/arep.58", "report-no": null, "categories": "cs.CV cs.DB eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Quick, Draw! Dataset is a Google dataset with a collection of 50 million\ndrawings, divided in 345 categories, collected from the users of the game\nQuick, Draw!. In contrast with most of the existing image datasets, in the\nQuick, Draw! Dataset, drawings are stored as time series of pencil positions\ninstead of a bitmap matrix composed by pixels. This aspect makes this dataset\nthe largest doodle dataset available at the time. The Quick, Draw! Dataset is\npresented as a great opportunity to researchers for developing and studying\nmachine learning techniques. Due to the size of this dataset and the nature of\nits source, there is a scarce of information about the quality of the drawings\ncontained. In this paper, a statistical analysis of three of the classes\ncontained in the Quick, Draw! Dataset is depicted: mountain, book and whale.\nThe goal is to give to the reader a first impression of the data collected in\nthis dataset. For the analysis of the quality of the drawings, a Classification\nNeural Network was trained to obtain a classification score. Using this\nclassification score and the parameters provided by the dataset, a statistical\nanalysis of the quality and nature of the drawings contained in this dataset is\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 10:28:34 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 09:07:23 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Fernandez-Fernandez", "Raul", ""], ["Victores", "Juan G.", ""], ["Estevez", "David", ""], ["Balaguer", "Carlos", ""]]}, {"id": "1907.06483", "submitter": "Egor Ershov I", "authors": "A.~Savchik, E.~Ershov, S.~Karpenko", "title": "Color Cerberus", "comments": null, "journal-ref": null, "doi": "10.1109/ISPA.2019.8868425", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple convolutional neural network was able to win ISISPA color constancy\ncompetition. Partial reimplementation of (Bianco, 2017) neural architecture\nwould have shown even better results in this setup.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 13:04:31 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["~Savchik", "A.", ""], ["~Ershov", "E.", ""], ["~Karpenko", "S.", ""]]}, {"id": "1907.06498", "submitter": "Emrah Basaran", "authors": "Emrah Basaran, Muhittin Gokmen, Mustafa E. Kamasak", "title": "An Efficient Framework for Visible-Infrared Cross Modality Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": "10.1016/j.image.2020.115933", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible-infrared cross-modality person re-identification (VI-ReId) is an\nessential task for video surveillance in poorly illuminated or dark\nenvironments. Despite many recent studies on person re-identification in the\nvisible domain (ReId), there are few studies dealing specifically with VI-ReId.\nBesides challenges that are common for both ReId and VI-ReId such as\npose/illumination variations, background clutter and occlusion, VI-ReId has\nadditional challenges as color information is not available in infrared images.\nAs a result, the performance of VI-ReId systems is typically lower than that of\nReId systems. In this work, we propose a four-stream framework to improve\nVI-ReId performance. We train a separate deep convolutional neural network in\neach stream using different representations of input images. We expect that\ndifferent and complementary features can be learned from each stream. In our\nframework, grayscale and infrared input images are used to train the ResNet in\nthe first stream. In the second stream, RGB and three-channel infrared images\n(created by repeating the infrared channel) are used. In the remaining two\nstreams, we use local pattern maps as input images. These maps are generated\nutilizing local Zernike moments transformation. Local pattern maps are obtained\nfrom grayscale and infrared images in the third stream and from RGB and\nthree-channel infrared images in the last stream. We improve the performance of\nthe proposed framework by employing a re-ranking algorithm for post-processing.\nOur results indicate that the proposed framework outperforms current\nstate-of-the-art with a large margin by improving Rank-1/mAP by 29.79%/30.91%\non SYSU-MM01 dataset, and by 9.73%/16.36% on RegDB dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 13:32:15 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 03:41:05 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Basaran", "Emrah", ""], ["Gokmen", "Muhittin", ""], ["Kamasak", "Mustafa E.", ""]]}, {"id": "1907.06515", "submitter": "Xu Zhang", "authors": "Xu Zhang, Svebor Karaman, Shih-Fu Chang", "title": "Detecting and Simulating Artifacts in GAN Fake Images", "comments": "This is an extended version of our original AutoGAN paper which will\n  be appeared in WIFS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To detect GAN generated images, conventional supervised machine learning\nalgorithms require collection of a number of real and fake images from the\ntargeted GAN model. However, the specific model used by the attacker is often\nunavailable. To address this, we propose a GAN simulator, AutoGAN, which can\nsimulate the artifacts produced by the common pipeline shared by several\npopular GAN models. Additionally, we identify a unique artifact caused by the\nup-sampling component included in the common GAN pipeline. We show\ntheoretically such artifacts are manifested as replications of spectra in the\nfrequency domain and thus propose a classifier model based on the spectrum\ninput, rather than the pixel input. By using the simulated images to train a\nspectrum based classifier, even without seeing the fake images produced by the\ntargeted GAN model during training, our approach achieves state-of-the-art\nperformances on detecting fake images generated by popular GAN models such as\nCycleGAN.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 14:22:34 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 22:53:46 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Zhang", "Xu", ""], ["Karaman", "Svebor", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1907.06543", "submitter": "Sophia Bano Dr", "authors": "Sophia Bano, Francisco Vasconcelos, Marcel Tella Amo, George Dwyer,\n  Caspar Gruijthuijsen, Jan Deprest, Sebastien Ourselin, Emmanuel Vander\n  Poorten, Tom Vercauteren, Danail Stoyanov", "title": "Deep Sequential Mosaicking of Fetoscopic Videos", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32239-7_35", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twin-to-twin transfusion syndrome treatment requires fetoscopic laser\nphotocoagulation of placental vascular anastomoses to regulate blood flow to\nboth fetuses. Limited field-of-view (FoV) and low visual quality during\nfetoscopy make it challenging to identify all vascular connections. Mosaicking\ncan align multiple overlapping images to generate an image with increased FoV,\nhowever, existing techniques apply poorly to fetoscopy due to the low visual\nquality, texture paucity, and hence fail in longer sequences due to the drift\naccumulated over time. Deep learning techniques can facilitate in overcoming\nthese challenges. Therefore, we present a new generalized Deep Sequential\nMosaicking (DSM) framework for fetoscopic videos captured from different\nsettings such as simulation, phantom, and real environments. DSM extends an\nexisting deep image-based homography model to sequential data by proposing\ncontrolled data augmentation and outlier rejection methods. Unlike existing\nmethods, DSM can handle visual variations due to specular highlights and\nreflection across adjacent frames, hence reducing the accumulated drift. We\nperform experimental validation and comparison using 5 diverse fetoscopic\nvideos to demonstrate the robustness of our framework.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 15:11:09 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Bano", "Sophia", ""], ["Vasconcelos", "Francisco", ""], ["Amo", "Marcel Tella", ""], ["Dwyer", "George", ""], ["Gruijthuijsen", "Caspar", ""], ["Deprest", "Jan", ""], ["Ourselin", "Sebastien", ""], ["Poorten", "Emmanuel Vander", ""], ["Vercauteren", "Tom", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1907.06565", "submitter": "Jasjeet Dhaliwal", "authors": "Jasjeet Dhaliwal, Kyle Hambrook", "title": "Recovery Guarantees for Compressible Signals with Adversarial Noise", "comments": "Theorem 1 updated, \\ell_\\infty defense added, Lemma 9 added, comp.\n  section updated, abstract updated, and other minor writing edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.DS cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide recovery guarantees for compressible signals that have been\ncorrupted with noise and extend the framework introduced in\n\\cite{bafna2018thwarting} to defend neural networks against $\\ell_0$-norm,\n$\\ell_2$-norm, and $\\ell_{\\infty}$-norm attacks. Our results are general as\nthey can be applied to most unitary transforms used in practice and hold for\n$\\ell_0$-norm, $\\ell_2$-norm, and $\\ell_\\infty$-norm bounded noise. In the case\nof $\\ell_0$-norm noise, we prove recovery guarantees for Iterative Hard\nThresholding (IHT) and Basis Pursuit (BP). For $\\ell_2$-norm bounded noise, we\nprovide recovery guarantees for BP and for the case of $\\ell_\\infty$-norm\nbounded noise, we provide recovery guarantees for Dantzig Selector (DS). These\nguarantees theoretically bolster the defense framework introduced in\n\\cite{bafna2018thwarting} for defending neural networks against adversarial\ninputs. Finally, we experimentally demonstrate the effectiveness of this\ndefense framework against an array of $\\ell_0$, $\\ell_2$ and $\\ell_\\infty$ norm\nattacks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:15:12 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 12:56:03 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 16:53:34 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Dhaliwal", "Jasjeet", ""], ["Hambrook", "Kyle", ""]]}, {"id": "1907.06571", "submitter": "Aidan Clark", "authors": "Aidan Clark, Jeff Donahue, Karen Simonyan", "title": "Adversarial Video Generation on Complex Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models of natural images have progressed towards high fidelity\nsamples by the strong leveraging of scale. We attempt to carry this success to\nthe field of video modeling by showing that large Generative Adversarial\nNetworks trained on the complex Kinetics-600 dataset are able to produce video\nsamples of substantially higher complexity and fidelity than previous work. Our\nproposed model, Dual Video Discriminator GAN (DVD-GAN), scales to longer and\nhigher resolution videos by leveraging a computationally efficient\ndecomposition of its discriminator. We evaluate on the related tasks of video\nsynthesis and video prediction, and achieve new state-of-the-art Fr\\'echet\nInception Distance for prediction for Kinetics-600, as well as state-of-the-art\nInception Score for synthesis on the UCF-101 dataset, alongside establishing a\nstrong baseline for synthesis on Kinetics-600.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:27:04 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 16:37:55 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Clark", "Aidan", ""], ["Donahue", "Jeff", ""], ["Simonyan", "Karen", ""]]}, {"id": "1907.06592", "submitter": "Paschalis Bizopoulos", "authors": "Paschalis Bizopoulos, and Dimitrios Koutsouris", "title": "Sparsely Activated Networks", "comments": "10 pages, 5 figures, 4 algorithms, 4 tables, submission to IEEE\n  Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2020.2984514", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous literature on unsupervised learning focused on designing structural\npriors with the aim of learning meaningful features. However, this was done\nwithout considering the description length of the learned representations which\nis a direct and unbiased measure of the model complexity. In this paper, first\nwe introduce the $\\varphi$ metric that evaluates unsupervised models based on\ntheir reconstruction accuracy and the degree of compression of their internal\nrepresentations. We then present and define two activation functions (Identity,\nReLU) as base of reference and three sparse activation functions (top-k\nabsolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize\nthe previously defined $\\varphi$. We lastly present Sparsely Activated Networks\n(SANs) that consist of kernels with shared weights that, during encoding, are\nconvolved with the input and then passed through a sparse activation function.\nDuring decoding, the same weights are convolved with the sparse activation map\nand subsequently the partial reconstructions from each weight are summed to\nreconstruct the input. We compare SANs using the five previously defined\nactivation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,\nFMNIST) and show that models that are selected using $\\varphi$ have small\ndescription representation length and consist of interpretable kernels.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 08:01:47 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 14:24:02 GMT"}, {"version": "v3", "created": "Sun, 2 Feb 2020 13:05:55 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 16:25:28 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Bizopoulos", "Paschalis", ""], ["Koutsouris", "Dimitrios", ""]]}, {"id": "1907.06625", "submitter": "Kilian Hett", "authors": "Kilian Hett, Vinh-Thong Ta, Jos\\'e V. Manj\\'on, Pierrick Coup\\'e", "title": "Multi-scale Graph-based Grading for Alzheimer's Disease Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of subjects with mild cognitive impairment (MCI) who will\nprogress to Alzheimer's disease (AD) is clinically relevant, and may above all\nhave a significant impact on accelerate the development of new treatments. In\nthis paper, we present a new MRI-based biomarker that enables us to predict\nconversion of MCI subjects to AD accurately. In order to better capture the AD\nsignature, we introduce two main contributions. First, we present a new\ngraph-based grading framework to combine inter-subject similarity features and\nintra-subject variability features. This framework involves patch-based grading\nof anatomical structures and graph-based modeling of structure alteration\nrelationships. Second, we propose an innovative multiscale brain analysis to\ncapture alterations caused by AD at different anatomical levels. Based on a\ncascade of classifiers, this multiscale approach enables the analysis of\nalterations of whole brain structures and hippocampus subfields at the same\ntime. During our experiments using the ADNI-1 dataset, the proposed multiscale\ngraph-based grading method obtained an area under the curve (AUC) of 81% to\npredict conversion of MCI subjects to AD within three years. Moreover, when\ncombined with cognitive scores, the proposed method obtained 85% of AUC. These\nresults are competitive in comparison to state-of-the-art methods evaluated on\nthe same dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 17:57:17 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Hett", "Kilian", ""], ["Ta", "Vinh-Thong", ""], ["Manj\u00f3n", "Jos\u00e9 V.", ""], ["Coup\u00e9", "Pierrick", ""]]}, {"id": "1907.06627", "submitter": "Babak Ehteshami Bejnordi", "authors": "Babak Ehteshami Bejnordi, Tijmen Blankevoort and Max Welling", "title": "Batch-Shaping for Learning Conditional Channel Gated Networks", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that trains large capacity neural networks with\nsignificantly improved accuracy and lower dynamic computational cost. We\nachieve this by gating the deep-learning architecture on a fine-grained-level.\nIndividual convolutional maps are turned on/off conditionally on features in\nthe network. To achieve this, we introduce a new residual block architecture\nthat gates convolutional channels in a fine-grained manner. We also introduce a\ngenerally applicable tool $batch$-$shaping$ that matches the marginal aggregate\nposteriors of features in a neural network to a pre-specified prior\ndistribution. We use this novel technique to force gates to be more conditional\non the data. We present results on CIFAR-10 and ImageNet datasets for image\nclassification, and Cityscapes for semantic segmentation. Our results show that\nour method can slim down large architectures conditionally, such that the\naverage computational cost on the data is on par with a smaller architecture,\nbut with higher accuracy. In particular, on ImageNet, our ResNet50 and ResNet34\ngated networks obtain 74.60% and 72.55% top-1 accuracy compared to the 69.76%\naccuracy of the baseline ResNet18 model, for similar complexity. We also show\nthat the resulting networks automatically learn to use more features for\ndifficult examples and fewer features for simple examples.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 17:58:04 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 12:13:32 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 09:10:52 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 08:42:24 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Bejnordi", "Babak Ehteshami", ""], ["Blankevoort", "Tijmen", ""], ["Welling", "Max", ""]]}, {"id": "1907.06670", "submitter": "Zhang Zhang", "authors": "Zhang Zhang and Dacheng Tao", "title": "Slow Feature Analysis for Human Action Recognition", "comments": null, "journal-ref": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,\n  VOL. 34, NO. 3, MARCH 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow Feature Analysis (SFA) extracts slowly varying features from a quickly\nvarying input signal. It has been successfully applied to modeling the visual\nreceptive fields of the cortical neurons. Sufficient experimental results in\nneuroscience suggest that the temporal slowness principle is a general learning\nprinciple in visual perception. In this paper, we introduce the SFA framework\nto the problem of human action recognition by incorporating the discriminative\ninformation with SFA learning and considering the spatial relationship of body\nparts. In particular, we consider four kinds of SFA learning strategies,\nincluding the original unsupervised SFA (U-SFA), the supervised SFA (S-SFA),\nthe discriminative SFA (D-SFA), and the spatial discriminative SFA (SD-SFA), to\nextract slow feature functions from a large amount of training cuboids which\nare obtained by random sampling in motion boundaries. Afterward, to represent\naction sequences, the squared first order temporal derivatives are accumulated\nover all transformed cuboids into one feature vector, which is termed the\nAccumulated Squared Derivative (ASD) feature. The ASD feature encodes the\nstatistical distribution of slow features in an action sequence. Finally, a\nlinear support vector machine (SVM) is trained to classify actions represented\nby ASD features. We conduct extensive experiments, including two sets of\ncontrol experiments, two sets of large scale experiments on the KTH and\nWeizmann databases, and two sets of experiments on the CASIA and UT-interaction\ndatabases, to demonstrate the effectiveness of SFA for human action\nrecognition.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 18:05:37 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Zhang", "Zhang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1907.06713", "submitter": "Shichao Xu", "authors": "Shichao Xu, Shuyue Lan, Qi Zhu", "title": "MaskPlus: Improving Mask Generation for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is a promising yet challenging topic in computer\nvision. Recent approaches such as Mask R-CNN typically divide this problem into\ntwo parts -- a detection component and a mask generation branch, and mostly\nfocus on the improvement of the detection part. In this paper, we present an\napproach that extends Mask R-CNN with five novel optimization techniques for\nimproving the mask generation branch and reducing the conflicts between the\nmask branch and the detection component in training. These five techniques are\nindependent to each other and can be flexibly utilized in building various\ninstance segmentation architectures for increasing the overall accuracy. We\ndemonstrate the effectiveness of our approach with tests on the COCO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 19:36:54 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 20:37:08 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 08:34:29 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Xu", "Shichao", ""], ["Lan", "Shuyue", ""], ["Zhu", "Qi", ""]]}, {"id": "1907.06724", "submitter": "Yury Kartynnik", "authors": "Yury Kartynnik, Artsiom Ablavatski, Ivan Grishchenko, Matthias\n  Grundmann", "title": "Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs", "comments": "4 pages, 4 figures; CVPR Workshop on Computer Vision for Augmented\n  and Virtual Reality, Long Beach, CA, USA, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end neural network-based model for inferring an\napproximate 3D mesh representation of a human face from single camera input for\nAR applications. The relatively dense mesh model of 468 vertices is well-suited\nfor face-based AR effects. The proposed model demonstrates super-realtime\ninference speed on mobile GPUs (100-1000+ FPS, depending on the device and\nmodel variant) and a high prediction quality that is comparable to the variance\nin manual annotations of the same image.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:08:17 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Kartynnik", "Yury", ""], ["Ablavatski", "Artsiom", ""], ["Grishchenko", "Ivan", ""], ["Grundmann", "Matthias", ""]]}, {"id": "1907.06727", "submitter": "Aydogan Ozcan", "authors": "Tairan Liu, Zhensong Wei, Yair Rivenson, Kevin de Haan, Yibo Zhang,\n  Yichen Wu, Aydogan Ozcan", "title": "Deep learning-based color holographic microscopy", "comments": "25 pages, 8 Figures, 2 Tables", "journal-ref": "Journal of Biophotonics (2019)", "doi": "10.1002/jbio.201900107", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report a framework based on a generative adversarial network (GAN) that\nperforms high-fidelity color image reconstruction using a single hologram of a\nsample that is illuminated simultaneously by light at three different\nwavelengths. The trained network learns to eliminate missing-phase-related\nartifacts, and generates an accurate color transformation for the reconstructed\nimage. Our framework is experimentally demonstrated using lung and prostate\ntissue sections that are labeled with different histological stains. This\nframework is envisaged to be applicable to point-of-care histopathology, and\npresents a significant improvement in the throughput of coherent microscopy\nsystems given that only a single hologram of the specimen is required for\naccurate color imaging.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:15:21 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Liu", "Tairan", ""], ["Wei", "Zhensong", ""], ["Rivenson", "Yair", ""], ["de Haan", "Kevin", ""], ["Zhang", "Yibo", ""], ["Wu", "Yichen", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1907.06740", "submitter": "Yury Kartynnik", "authors": "Andrei Tkachenka, Gregory Karpiak, Andrey Vakunov, Yury Kartynnik,\n  Artsiom Ablavatski, Valentin Bazarevsky, Siargey Pisarchyk", "title": "Real-time Hair Segmentation and Recoloring on Mobile GPUs", "comments": "4 pages, 5 figures; CVPR Workshop on Computer Vision for Augmented\n  and Virtual Reality, Long Beach, CA, USA, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for neural network-based hair segmentation from a\nsingle camera input specifically designed for real-time, mobile application.\nOur relatively small neural network produces a high-quality hair segmentation\nmask that is well suited for AR effects, e.g. virtual hair recoloring. The\nproposed model achieves real-time inference speed on mobile GPUs (30-100+ FPS,\ndepending on the device) with high accuracy. We also propose a very realistic\nhair recoloring scheme. Our method has been deployed in major AR application\nand is used by millions of users.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:39:15 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Tkachenka", "Andrei", ""], ["Karpiak", "Gregory", ""], ["Vakunov", "Andrey", ""], ["Kartynnik", "Yury", ""], ["Ablavatski", "Artsiom", ""], ["Bazarevsky", "Valentin", ""], ["Pisarchyk", "Siargey", ""]]}, {"id": "1907.06757", "submitter": "Binod Bhattarai", "authors": "Binod Bhattarai, Rumeysa Bodur, Tae-Kyun Kim", "title": "AugLabel: Exploiting Word Representations to Augment Labels for Face\n  Attribute Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmenting data in image space (eg. flipping, cropping etc) and activation\nspace (eg. dropout) are being widely used to regularise deep neural networks\nand have been successfully applied on several computer vision tasks. Unlike\nprevious works, which are mostly focused on doing augmentation in the\naforementioned domains, we propose to do augmentation in label space. In this\npaper, we present a novel method to generate fixed dimensional labels with\ncontinuous values for images by exploiting the word2vec representations of the\nexisting categorical labels. We then append these representations with existing\ncategorical labels and train the model. We validated our idea on two\nchallenging face attribute classification data sets viz. CelebA and LFWA. Our\nextensive experiments show that the augmented labels improve the performance of\nthe competitive deep learning baseline and reduce the need of annotated real\ndata up to 50%, while attaining a performance similar to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 21:15:09 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Bhattarai", "Binod", ""], ["Bodur", "Rumeysa", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1907.06772", "submitter": "Sara Beery", "authors": "Sara Beery, Dan Morris, Siyu Yang", "title": "Efficient Pipeline for Camera Trap Image Review", "comments": "From the Data Mining and AI for Conservation Workshop at KDD19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biologists all over the world use camera traps to monitor biodiversity and\nwildlife population density. The computer vision community has been making\nstrides towards automating the species classification challenge in camera\ntraps, but it has proven difficult to to apply models trained in one region to\nimages collected in different geographic areas. In some cases, accuracy falls\noff catastrophically in new region, due to both changes in background and the\npresence of previously-unseen species. We propose a pipeline that takes\nadvantage of a pre-trained general animal detector and a smaller set of labeled\nimages to train a classification model that can efficiently achieve accurate\nresults in a new region.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 22:15:00 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Beery", "Sara", ""], ["Morris", "Dan", ""], ["Yang", "Siyu", ""]]}, {"id": "1907.06777", "submitter": "Jason Ku", "authors": "Jason Ku, Alex D. Pon, Sean Walsh, and Steven L. Waslander", "title": "Improving 3D Object Detection for Pedestrians with Virtual Multi-View\n  Synthesis Orientation Estimation", "comments": "Accepted in IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurately estimating the orientation of pedestrians is an important and\nchallenging task for autonomous driving because this information is essential\nfor tracking and predicting pedestrian behavior. This paper presents a flexible\nVirtual Multi-View Synthesis module that can be adopted into 3D object\ndetection methods to improve orientation estimation. The module uses a\nmulti-step process to acquire the fine-grained semantic information required\nfor accurate orientation estimation. First, the scene's point cloud is\ndensified using a structure preserving depth completion algorithm and each\npoint is colorized using its corresponding RGB pixel. Next, virtual cameras are\nplaced around each object in the densified point cloud to generate novel\nviewpoints, which preserve the object's appearance. We show that this module\ngreatly improves the orientation estimation on the challenging pedestrian class\non the KITTI benchmark. When used with the open-source 3D detector AVOD-FPN, we\noutperform all other published methods on the pedestrian Orientation, 3D, and\nBird's Eye View benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 22:27:16 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Ku", "Jason", ""], ["Pon", "Alex D.", ""], ["Walsh", "Sean", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1907.06781", "submitter": "Deng-Ping Fan", "authors": "Deng-Ping Fan, Zheng Lin, Jia-Xing Zhao, Yun Liu, Zhao Zhang, Qibin\n  Hou, Menglong Zhu, Ming-Ming Cheng", "title": "Rethinking RGB-D Salient Object Detection: Models, Data Sets, and\n  Large-Scale Benchmarks", "comments": "Accepted in TNNLS20. 15 pages, 12 figures. Code:\n  https://github.com/DengPingFan/D3NetBenchmark", "journal-ref": null, "doi": "10.1109/TNNLS.2020.2996406", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of RGB-D information for salient object detection has been\nextensively explored in recent years. However, relatively few efforts have been\nput towards modeling salient object detection in real-world human activity\nscenes with RGBD. In this work, we fill the gap by making the following\ncontributions to RGB-D salient object detection. (1) We carefully collect a new\nSIP (salient person) dataset, which consists of ~1K high-resolution images that\ncover diverse real-world scenes from various viewpoints, poses, occlusions,\nilluminations, and backgrounds. (2) We conduct a large-scale (and, so far, the\nmost comprehensive) benchmark comparing contemporary methods, which has long\nbeen missing in the field and can serve as a baseline for future research. We\nsystematically summarize 32 popular models and evaluate 18 parts of 32 models\non seven datasets containing a total of about 97K images. (3) We propose a\nsimple general architecture, called Deep Depth-Depurator Network (D3Net). It\nconsists of a depth depurator unit (DDU) and a three-stream feature learning\nmodule (FLM), which performs low-quality depth map filtering and cross-modal\nfeature learning respectively. These components form a nested structure and are\nelaborately designed to be learned jointly. D3Net exceeds the performance of\nany prior contenders across all five metrics under consideration, thus serving\nas a strong model to advance research in this field. We also demonstrate that\nD3Net can be used to efficiently extract salient object masks from real scenes,\nenabling effective background changing application with a speed of 65fps on a\nsingle GPU. All the saliency maps, our new SIP dataset, the D3Net model, and\nthe evaluation tools are publicly available at\nhttps://github.com/DengPingFan/D3NetBenchmark.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 22:43:20 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 17:32:35 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Lin", "Zheng", ""], ["Zhao", "Jia-Xing", ""], ["Liu", "Yun", ""], ["Zhang", "Zhao", ""], ["Hou", "Qibin", ""], ["Zhu", "Menglong", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "1907.06794", "submitter": "Ji Zhang", "authors": "Shijie Geng and Ji Zhang and Hang Zhang and Ahmed Elgammal and\n  Dimitris N. Metaxas", "title": "2nd Place Solution to the GQA Challenge 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple method that achieves unexpectedly superior performance\nfor Complex Reasoning involved Visual Question Answering. Our solution collects\nstatistical features from high-frequency words of all the questions asked about\nan image and use them as accurate knowledge for answering further questions of\nthe same image. We are fully aware that this setting is not ubiquitously\napplicable, and in a more common setting one should assume the questions are\nasked separately and they cannot be gathered to obtain a knowledge base.\nNonetheless, we use this method as an evidence to demonstrate our observation\nthat the bottleneck effect is more severe on the feature extraction part than\nit is on the knowledge reasoning part. We show significant gaps when using the\nsame reasoning model with 1) ground-truth features; 2) statistical features; 3)\ndetected features from completely learned detectors, and analyze what these\ngaps mean to researches on visual reasoning topics. Our model with the\nstatistical features achieves the 2nd place in the GQA Challenge 2019.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 00:09:09 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 22:04:53 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Geng", "Shijie", ""], ["Zhang", "Ji", ""], ["Zhang", "Hang", ""], ["Elgammal", "Ahmed", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1907.06796", "submitter": "Jianing Wei", "authors": "Jianing Wei, Genzhi Ye, Tyler Mullen, Matthias Grundmann, Adel\n  Ahmadyan, Tingbo Hou", "title": "Instant Motion Tracking and Its Applications to Augmented Reality", "comments": "CVPR Workshop on Computer Vision for Augmented and Virtual Reality,\n  Long Beach, CA, 2019", "journal-ref": "CVPR Workshop on Computer Vision for Augmented and Virtual\n  Reality, Long Beach, CA, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) brings immersive experiences to users. With recent\nadvances in computer vision and mobile computing, AR has scaled across\nplatforms, and has increased adoption in major products. One of the key\nchallenges in enabling AR features is proper anchoring of the virtual content\nto the real world, a process referred to as tracking. In this paper, we present\na system for motion tracking, which is capable of robustly tracking planar\ntargets and performing relative-scale 6DoF tracking without calibration. Our\nsystem runs in real-time on mobile phones and has been deployed in multiple\nmajor products on hundreds of millions of devices.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 00:13:09 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Wei", "Jianing", ""], ["Ye", "Genzhi", ""], ["Mullen", "Tyler", ""], ["Grundmann", "Matthias", ""], ["Ahmadyan", "Adel", ""], ["Hou", "Tingbo", ""]]}, {"id": "1907.06823", "submitter": "Aras Dargazany", "authors": "Aras R. Dargazany", "title": "Stereo-based terrain traversability analysis using normal-based\n  segmentation and superpixel surface analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an stereo-based traversability analysis approach for all\nterrains in off-road mobile robotics, e.g. Unmanned Ground Vehicles (UGVs) is\nproposed. This approach reformulates the problem of terrain traversability\nanalysis into two main problems: (1) 3D terrain reconstruction and (2) terrain\nall surfaces detection and analysis. The proposed approach is using stereo\ncamera for perception and 3D reconstruction of the terrain. In order to detect\nall the existing surfaces in the 3D reconstructed terrain as superpixel\nsurfaces (i.e. segments), an image segmentation technique is applied using\ngeometry-based features (pixel-based surface normals). Having detected all the\nsurfaces, Superpixel Surface Traversability Analysis approach (SSTA) is applied\non all of the detected surfaces (superpixel segments) in order to classify them\nbased on their traversability index. The proposed SSTA approach is based on:\n(1) Superpixel surface normal and plane estimation, (2) Traversability analysis\nusing superpixel surface planes. Having analyzed all the superpixel surfaces\nbased on their traversability, these surfaces are finally classified into five\nmain categories as following: traversable, semi-traversable, non-traversable,\nunknown and undecided.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 03:48:18 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Dargazany", "Aras R.", ""]]}, {"id": "1907.06826", "submitter": "Yulong Cao", "authors": "Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara\n  Rampazzi, Qi Alfred Chen, Kevin Fu, Z. Morley Mao", "title": "Adversarial Sensor Attack on LiDAR-based Perception in Autonomous\n  Driving", "comments": "Accepted at the ACM Conference on Computer and Communications\n  Security (CCS), 2019", "journal-ref": null, "doi": "10.1145/3319535.3339815", "report-no": null, "categories": "cs.CR cs.CV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Autonomous Vehicles (AVs), one fundamental pillar is perception, which\nleverages sensors like cameras and LiDARs (Light Detection and Ranging) to\nunderstand the driving environment. Due to its direct impact on road safety,\nmultiple prior efforts have been made to study its the security of perception\nsystems. In contrast to prior work that concentrates on camera-based\nperception, in this work we perform the first security study of LiDAR-based\nperception in AV settings, which is highly important but unexplored. We\nconsider LiDAR spoofing attacks as the threat model and set the attack goal as\nspoofing obstacles close to the front of a victim AV. We find that blindly\napplying LiDAR spoofing is insufficient to achieve this goal due to the machine\nlearning-based object detection process. Thus, we then explore the possibility\nof strategically controlling the spoofed attack to fool the machine learning\nmodel. We formulate this task as an optimization problem and design modeling\nmethods for the input perturbation function and the objective function. We also\nidentify the inherent limitations of directly solving the problem using\noptimization and design an algorithm that combines optimization and global\nsampling, which improves the attack success rates to around 75%. As a case\nstudy to understand the attack impact at the AV driving decision level, we\nconstruct and evaluate two attack scenarios that may damage road safety and\nmobility. We also discuss defense directions at the AV system, sensor, and\nmachine learning model levels.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 04:00:56 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 13:26:03 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Cao", "Yulong", ""], ["Xiao", "Chaowei", ""], ["Cyr", "Benjamin", ""], ["Zhou", "Yimeng", ""], ["Park", "Won", ""], ["Rampazzi", "Sara", ""], ["Chen", "Qi Alfred", ""], ["Fu", "Kevin", ""], ["Mao", "Z. Morley", ""]]}, {"id": "1907.06835", "submitter": "Sung-Ho Bae", "authors": "Kang-Ho Lee, JoonHyun Jeong, and Sung-Ho Bae", "title": "An Inter-Layer Weight Prediction and Quantization for Deep Neural\n  Networks based on a Smoothly Varying Weight Hypothesis", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to a resource-constrained environment, network compression has become an\nimportant part of deep neural networks research. In this paper, we propose a\nnew compression method, \\textit{Inter-Layer Weight Prediction} (ILWP) and\nquantization method which quantize the predicted residuals between the weights\nin all convolution layers based on an inter-frame prediction method in\nconventional video coding schemes. Furthermore, we found a phenomenon\n\\textit{Smoothly Varying Weight Hypothesis} (SVWH) which is that the weights in\nadjacent convolution layers share strong similarity in shapes and values, i.e.,\nthe weights tend to vary smoothly along with the layers. Based on SVWH, we\npropose a second ILWP and quantization method which quantize the predicted\nresiduals between the weights in adjacent convolution layers. Since the\npredicted weight residuals tend to follow Laplace distributions with very low\nvariance, the weight quantization can more effectively be applied, thus\nproducing more zero weights and enhancing the weight compression ratio. In\naddition, we propose a new \\textit{inter-layer loss} for eliminating\nnon-texture bits, which enabled us to more effectively store only texture bits.\nThat is, the proposed loss regularizes the weights such that the collocated\nweights between the adjacent two layers have the same values. Finally, we\npropose an ILWP with an inter-layer loss and quantization method. Our\ncomprehensive experiments show that the proposed method achieves a much higher\nweight compression rate at the same accuracy level compared with the previous\nquantization-based compression methods in deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 04:44:59 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 02:32:12 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Lee", "Kang-Ho", ""], ["Jeong", "JoonHyun", ""], ["Bae", "Sung-Ho", ""]]}, {"id": "1907.06838", "submitter": "Tianqi Wang", "authors": "Tianqi Wang, Dong Eui Chang", "title": "Improved Reinforcement Learning through Imitation Learning Pretraining\n  Towards Image-based Autonomous Driving", "comments": "5 pages, 2019 19th International Conference on Control, Automation\n  and Systems (ICCAS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a training pipeline for the autonomous driving task given the\ncurrent camera image and vehicle speed as the input to produce the throttle,\nbrake, and steering control output. The simulator Airsim's convenient weather\nand lighting API provides a sufficient diversity during training which can be\nvery helpful to increase the trained policy's robustness. In order to not limit\nthe possible policy's performance, we use a continuous and deterministic\ncontrol policy setting. We utilize ResNet-34 as our actor and critic networks\nwith some slight changes in the fully connected layers. Considering human's\nmastery of this task and the high-complexity nature of this task, we first use\nimitation learning to mimic the given human policy and leverage the trained\npolicy and its weights to the reinforcement learning phase for which we use\nDDPG. This combination shows a considerable performance boost comparing to both\npure imitation learning and pure DDPG for the autonomous driving task.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 04:48:52 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Wang", "Tianqi", ""], ["Chang", "Dong Eui", ""]]}, {"id": "1907.06844", "submitter": "Liangchen Liu", "authors": "Liangchen Liu, Teng Zhang, Kun Zhao, Arnold Wiliem, Kieren\n  Astin-Walmsley, Brian Lovell", "title": "Deep inspection: an electrical distribution pole parts study via deep\n  neural networks", "comments": "electrical distribution pole inspection, integrated inspection\n  system, object detection, imbalanced data classification, To appear in\n  Proceeding of ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrical distribution poles are important assets in electricity supply.\nThese poles need to be maintained in good condition to ensure they protect\ncommunity safety, maintain reliability of supply, and meet legislative\nobligations. However, maintaining such a large volumes of assets is an\nexpensive and challenging task. To address this, recent approaches utilise\nimagery data captured from helicopter and/or drone inspections. Whilst reducing\nthe cost for manual inspection, manual analysis on each image is still\nrequired. As such, several image-based automated inspection systems have been\nproposed. In this paper, we target two major challenges: tiny object detection\nand extremely imbalanced datasets, which currently hinder the wide deployment\nof the automatic inspection. We propose a novel two-stage zoom-in detection\nmethod to gradually focus on the object of interest. To address the imbalanced\ndataset problem, we propose the resampling as well as reweighting schemes to\niteratively adapt the model to the large intra-class variation of major class\nand balance the contributions to the loss from each class. Finally, we\nintegrate these components together and devise a novel automatic inspection\nframework. Extensive experiments demonstrate that our proposed approaches are\neffective and can boost the performance compared to the baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 05:10:38 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Liu", "Liangchen", ""], ["Zhang", "Teng", ""], ["Zhao", "Kun", ""], ["Wiliem", "Arnold", ""], ["Astin-Walmsley", "Kieren", ""], ["Lovell", "Brian", ""]]}, {"id": "1907.06852", "submitter": "Yulei Qin", "authors": "Yulei Qin, Mingjian Chen, Hao Zheng, Yun Gu, Mali Shen, Jie Yang,\n  Xiaolin Huang, Yue-Min Zhu, Guang-Zhong Yang", "title": "AirwayNet: A Voxel-Connectivity Aware Approach for Accurate Airway\n  Segmentation Using Convolutional Neural Networks", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Airway segmentation on CT scans is critical for pulmonary disease diagnosis\nand endobronchial navigation. Manual extraction of airway requires strenuous\nefforts due to the complicated structure and various appearance of airway. For\nautomatic airway extraction, convolutional neural networks (CNNs) based methods\nhave recently become the state-of-the-art approach. However, there still\nremains a challenge for CNNs to perceive the tree-like pattern and comprehend\nthe connectivity of airway. To address this, we propose a voxel-connectivity\naware approach named AirwayNet for accurate airway segmentation. By\nconnectivity modeling, conventional binary segmentation task is transformed\ninto 26 tasks of connectivity prediction. Thus, our AirwayNet learns both\nairway structure and relationship between neighboring voxels. To take advantage\nof context knowledge, lung distance map and voxel coordinates are fed into\nAirwayNet as additional semantic information. Compared to existing approaches,\nAirwayNet achieved superior performance, demonstrating the effectiveness of the\nnetwork's awareness of voxel connectivity.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 05:54:22 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Qin", "Yulei", ""], ["Chen", "Mingjian", ""], ["Zheng", "Hao", ""], ["Gu", "Yun", ""], ["Shen", "Mali", ""], ["Yang", "Jie", ""], ["Huang", "Xiaolin", ""], ["Zhu", "Yue-Min", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1907.06876", "submitter": "Andreas Pfeuffer", "authors": "Andreas Pfeuffer and Klaus Dietmayer", "title": "Separable Convolutional LSTMs for Faster Video Segmentation", "comments": null, "journal-ref": "2019 22st International Conference on Intelligent Transportation\n  Systems (ITSC)", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Segmentation is an important module for autonomous robots such as\nself-driving cars. The advantage of video segmentation approaches compared to\nsingle image segmentation is that temporal image information is considered, and\ntheir performance increases due to this. Hence, single image segmentation\napproaches are extended by recurrent units such as convolutional LSTM\n(convLSTM) cells, which are placed at suitable positions in the basic network\narchitecture. However, a major critique of video segmentation approaches based\non recurrent neural networks is their large parameter count and their\ncomputational complexity, and so, their inference time of one video frame takes\nup to 66 percent longer than their basic version. Inspired by the success of\nthe spatial and depthwise separable convolutional neural networks, we\ngeneralize these techniques for convLSTMs in this work, so that the number of\nparameters and the required FLOPs are reduced significantly. Experiments on\ndifferent datasets show that the segmentation approaches using the proposed,\nmodified convLSTM cells achieve similar or slightly worse accuracy, but are up\nto 15 percent faster on a GPU than the ones using the standard convLSTM cells.\nFurthermore, a new evaluation metric is introduced, which measures the amount\nof flickering pixels in the segmented video sequence.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 07:52:52 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Pfeuffer", "Andreas", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1907.06881", "submitter": "Hongkai Zhang", "authors": "Hongkai Zhang, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen", "title": "Cascade RetinaNet: Maintaining Consistency for Single-Stage Object\n  Detection", "comments": "BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches attempt to improve the detection performance by adopting\nthe idea of cascade for single-stage detectors. In this paper, we analyze and\ndiscover that inconsistency is the major factor limiting the performance. The\nrefined anchors are associated with the feature extracted from the previous\nlocation and the classifier is confused by misaligned classification and\nlocalization. Further, we point out two main designing rules for the cascade\nmanner: improving consistency between classification confidence and\nlocalization performance, and maintaining feature consistency between different\nstages. A multistage object detector named Cas-RetinaNet, is then proposed for\nreducing the misalignments. It consists of sequential stages trained with\nincreasing IoU thresholds for improving the correlation, and a novel Feature\nConsistency Module for mitigating the feature inconsistency. Experiments show\nthat our proposed Cas-RetinaNet achieves stable performance gains across\ndifferent models and input scales. Specifically, our method improves RetinaNet\nfrom 39.1 AP to 41.1 AP on the challenging MS COCO dataset without any bells or\nwhistles.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 08:01:56 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Zhang", "Hongkai", ""], ["Chang", "Hong", ""], ["Ma", "Bingpeng", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1907.06882", "submitter": "Yipeng Mou", "authors": "Yipeng Mou, Mingming Gong, Huan Fu, Kayhan Batmanghelich, Kun Zhang,\n  Dacheng Tao", "title": "Learning Depth from Monocular Videos Using Synthetic Data: A\n  Temporally-Consistent Domain Adaptation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of state-of-the-art monocular depth estimation methods are\nsupervised learning approaches. The success of such approaches heavily depends\non the high-quality depth labels which are expensive to obtain. Some recent\nmethods try to learn depth networks by leveraging unsupervised cues from\nmonocular videos which are easier to acquire but less reliable. In this paper,\nwe propose to resolve this dilemma by transferring knowledge from synthetic\nvideos with easily obtainable ground-truth depth labels. Due to the stylish\ndifference between synthetic and real images, we propose a\ntemporally-consistent domain adaptation (TCDA) approach that simultaneously\nexplores labels in the synthetic domain and temporal constraints in the videos\nto improve style transfer and depth prediction. Furthermore, we make use of the\nground-truth optical flow and pose information in the synthetic data to learn\nmoving mask and pose prediction networks. The learned moving masks can filter\nout moving regions that produces erroneous temporal constraints and the\nestimated poses provide better initializations for estimating temporal\nconstraints. Experimental results demonstrate the effectiveness of our method\nand comparable performance against state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 08:05:26 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 18:04:56 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Mou", "Yipeng", ""], ["Gong", "Mingming", ""], ["Fu", "Huan", ""], ["Batmanghelich", "Kayhan", ""], ["Zhang", "Kun", ""], ["Tao", "Dacheng", ""]]}, {"id": "1907.06890", "submitter": "Antonio Loquercio", "authors": "Antonio Loquercio, Mattia Seg\\`u, Davide Scaramuzza", "title": "A General Framework for Uncertainty Estimation in Deep Learning", "comments": "Accepted for publication in the Robotics and Automation Letters 2020,\n  and for presentation at the International Conference on Robotics and\n  Automation (ICRA) 2020", "journal-ref": "IEEE Robotics and Automation Letters 2020", "doi": "10.1109/LRA.2020.2974682", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks predictions are unreliable when the input sample is out of\nthe training distribution or corrupted by noise. Being able to detect such\nfailures automatically is fundamental to integrate deep learning algorithms\ninto robotics. Current approaches for uncertainty estimation of neural networks\nrequire changes to the network and optimization process, typically ignore prior\nknowledge about the data, and tend to make over-simplifying assumptions which\nunderestimate uncertainty. To address these limitations, we propose a novel\nframework for uncertainty estimation. Based on Bayesian belief networks and\nMonte-Carlo sampling, our framework not only fully models the different sources\nof prediction uncertainty, but also incorporates prior data information, e.g.\nsensor noise. We show theoretically that this gives us the ability to capture\nuncertainty better than existing methods. In addition, our framework has\nseveral desirable properties: (i) it is agnostic to the network architecture\nand task; (ii) it does not require changes in the optimization process; (iii)\nit can be applied to already trained architectures. We thoroughly validate the\nproposed framework through extensive experiments on both computer vision and\ncontrol tasks, where we outperform previous methods by up to 23% in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 08:46:03 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 13:56:53 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 10:45:59 GMT"}, {"version": "v4", "created": "Fri, 7 Feb 2020 12:10:56 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Loquercio", "Antonio", ""], ["Seg\u00f9", "Mattia", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1907.06915", "submitter": "Vikas Agaradahalli Gurumurthy", "authors": "Vikas Agaradahalli Gurumurthy, Ramesh Kestur, Omkar Narasipura", "title": "Mango Tree Net -- A fully convolutional network for semantic\n  segmentation and individual crown detection of mango trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a method for semantic segmentation of mango trees in high\nresolution aerial imagery, and, a novel method for individual crown detection\nof mango trees using segmentation output. Mango Tree Net, a fully convolutional\nneural network (FCN), is trained using supervised learning to perform semantic\nsegmentation of mango trees in imagery acquired using an unmanned aerial\nvehicle (UAV). The proposed network is retrained to separate\ntouching/overlapping tree crowns in segmentation output. Contour based\nconnected object detection is performed on the segmentation output from\nretrained network. Bounding boxes are drawn on the original images using\ncoordinates of connected objects to achieve individual crown detection. The\ntraining dataset consists of 8,824 image patches of size 240 x 240. The\napproach is tested for performance on segmentation and individual crown\ndetection tasks using test datasets containing 36 and 4 images respectively.\nThe performance is analyzed using standard metrics precision, recall, f1-score\nand accuracy. Results obtained demonstrate the robustness of the proposed\nmethods despite variations in factors such as scale, occlusion, lighting\nconditions and surrounding vegetation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 09:41:13 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Gurumurthy", "Vikas Agaradahalli", ""], ["Kestur", "Ramesh", ""], ["Narasipura", "Omkar", ""]]}, {"id": "1907.06916", "submitter": "Mark McDonnell", "authors": "Mark D. McDonnell, Hesham Mostafa, Runchun Wang and Andre van Schaik", "title": "Single-bit-per-weight deep convolutional neural networks without\n  batch-normalization layers for embedded systems", "comments": "8 pages, published IEEE conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch-normalization (BN) layers are thought to be an integrally important\nlayer type in today's state-of-the-art deep convolutional neural networks for\ncomputer vision tasks such as classification and detection. However, BN layers\nintroduce complexity and computational overheads that are highly undesirable\nfor training and/or inference on low-power custom hardware implementations of\nreal-time embedded vision systems such as UAVs, robots and Internet of Things\n(IoT) devices. They are also problematic when batch sizes need to be very small\nduring training, and innovations such as residual connections introduced more\nrecently than BN layers could potentially have lessened their impact. In this\npaper we aim to quantify the benefits BN layers offer in image classification\nnetworks, in comparison with alternative choices. In particular, we study\nnetworks that use shifted-ReLU layers instead of BN layers. We found, following\nexperiments with wide residual networks applied to the ImageNet, CIFAR 10 and\nCIFAR 100 image classification datasets, that BN layers do not consistently\noffer a significant advantage. We found that the accuracy margin offered by BN\nlayers depends on the data set, the network size, and the bit-depth of weights.\nWe conclude that in situations where BN layers are undesirable due to speed,\nmemory or complexity costs, that using shifted-ReLU layers instead should be\nconsidered; we found they can offer advantages in all these areas, and often do\nnot impose a significant accuracy cost.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 09:42:02 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 13:04:27 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["McDonnell", "Mark D.", ""], ["Mostafa", "Hesham", ""], ["Wang", "Runchun", ""], ["van Schaik", "Andre", ""]]}, {"id": "1907.06922", "submitter": "Thomas Golda", "authors": "Thomas Golda, Tobias Kalb, Arne Schumann, J\\\"urgen Beyerer", "title": "Human Pose Estimation for Real-World Crowded Scenarios", "comments": "Accepted for the 16th IEEE International Conference on Advanced Video\n  and Signal-based Surveillance (AVSS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation has recently made significant progress with the\nadoption of deep convolutional neural networks. Its many applications have\nattracted tremendous interest in recent years. However, many practical\napplications require pose estimation for human crowds, which still is a rarely\naddressed problem. In this work, we explore methods to optimize pose estimation\nfor human crowds, focusing on challenges introduced with dense crowds, such as\nocclusions, people in close proximity to each other, and partial visibility of\npeople. In order to address these challenges, we evaluate three aspects of a\npose detection approach: i) a data augmentation method to introduce robustness\nto occlusions, ii) the explicit detection of occluded body parts, and iii) the\nuse of the synthetic generated datasets. The first approach to improve the\naccuracy in crowded scenarios is to generate occlusions at training time using\nperson and object cutouts from the object recognition dataset COCO (Common\nObjects in Context). Furthermore, the synthetically generated dataset JTA\n(Joint Track Auto) is evaluated for the use in real-world crowd applications.\nIn order to overcome the transfer gap of JTA originating from a low pose\nvariety and less dense crowds, an extension dataset is created to ease the use\nfor real-world applications. Additionally, the occlusion flags provided with\nJTA are utilized to train a model, which explicitly distinguishes between\noccluded and visible body parts in two distinct branches. The combination of\nthe proposed additions to the baseline method help to improve the overall\naccuracy by 4.7% AP and thereby provide comparable results to current\nstate-of-the-art approaches on the respective dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 09:53:27 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Golda", "Thomas", ""], ["Kalb", "Tobias", ""], ["Schumann", "Arne", ""], ["Beyerer", "J\u00fcrgen", ""]]}, {"id": "1907.06941", "submitter": "Sihong Chen", "authors": "Sihong Chen and Weiping Yu and Kai Ma and Xinlong Sun and Xiaona Lin\n  and Desheng Sun and Yefeng Zheng", "title": "Semi-supervised Breast Lesion Detection in Ultrasound Video Based on\n  Temporal Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast lesion detection in ultrasound video is critical for computer-aided\ndiagnosis. However, detecting lesion in video is quite challenging due to the\nblurred lesion boundary, high similarity to soft tissue and lack of video\nannotations. In this paper, we propose a semi-supervised breast lesion\ndetection method based on temporal coherence which can detect the lesion more\naccurately. We aggregate features extracted from the historical key frames with\nadaptive key-frame scheduling strategy. Our proposed method accomplishes the\nunlabeled videos detection task by leveraging the supervision information from\na different set of labeled images. In addition, a new WarpNet is designed to\nreplace both the traditional spatial warping and feature aggregation operation,\nleading to a tremendous increase in speed. Experiments on 1,060 2D ultrasound\nsequences demonstrate that our proposed method achieves state-of-the-art video\ndetection result as 91.3% in mean average precision and 19 ms per frame on GPU,\ncompared to a RetinaNet based detection method in 86.6% and 32 ms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 11:16:52 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Chen", "Sihong", ""], ["Yu", "Weiping", ""], ["Ma", "Kai", ""], ["Sun", "Xinlong", ""], ["Lin", "Xiaona", ""], ["Sun", "Desheng", ""], ["Zheng", "Yefeng", ""]]}, {"id": "1907.06955", "submitter": "Thomas Kurmann", "authors": "Thomas Kurmann and Pablo M\\'arquez-Neila and Siqing Yu and Marion Munk\n  and Sebastian Wolf and Raphael Sznitman", "title": "Fused Detection of Retinal Biomarkers in OCT Volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography (OCT) is the primary imaging modality for\ndetecting pathological biomarkers associated to retinal diseases such as\nAge-Related Macular Degeneration. In practice, clinical diagnosis and treatment\nstrategies are closely linked to biomarkers visible in OCT volumes and the\nability to identify these plays an important role in the development of\nophthalmic pharmaceutical products. In this context, we present a method that\nautomatically predicts the presence of biomarkers in OCT cross-sections by\nincorporating information from the entire volume. We do so by adding a\nbidirectional LSTM to fuse the outputs of a Convolutional Neural Network that\npredicts individual biomarkers. We thus avoid the need to use pixel-wise\nannotations to train our method, and instead provide fine-grained biomarker\ninformation regardless. On a dataset of 416 volumes, we show that our approach\nimposes coherence between biomarker predictions across volume slices and our\npredictions are superior to several existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 12:12:57 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Kurmann", "Thomas", ""], ["M\u00e1rquez-Neila", "Pablo", ""], ["Yu", "Siqing", ""], ["Munk", "Marion", ""], ["Wolf", "Sebastian", ""], ["Sznitman", "Raphael", ""]]}, {"id": "1907.06968", "submitter": "Huy-Hieu Pham", "authors": "Huy Hieu Pham, Houssam Salmane, Louahdi Khoudour, Alain Crouzil, Pablo\n  Zegers, Sergio A Velastin", "title": "A Unified Deep Framework for Joint 3D Pose Estimation and Action\n  Recognition from a Single RGB Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning-based multitask framework for joint 3D human pose\nestimation and action recognition from RGB video sequences. Our approach\nproceeds along two stages. In the first, we run a real-time 2D pose detector to\ndetermine the precise pixel location of important keypoints of the body. A\ntwo-stream neural network is then designed and trained to map detected 2D\nkeypoints into 3D poses. In the second, we deploy the Efficient Neural\nArchitecture Search (ENAS) algorithm to find an optimal network architecture\nthat is used for modeling the spatio-temporal evolution of the estimated 3D\nposes via an image-based intermediate representation and performing action\nrecognition. Experiments on Human3.6M, MSR Action3D and SBU Kinect Interaction\ndatasets verify the effectiveness of the proposed method on the targeted tasks.\nMoreover, we show that our method requires a low computational budget for\ntraining and inference.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 12:50:42 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Pham", "Huy Hieu", ""], ["Salmane", "Houssam", ""], ["Khoudour", "Louahdi", ""], ["Crouzil", "Alain", ""], ["Zegers", "Pablo", ""], ["Velastin", "Sergio A", ""]]}, {"id": "1907.06987", "submitter": "Joao Carreira", "authors": "Joao Carreira, Eric Noland, Chloe Hillier, Andrew Zisserman", "title": "A Short Note on the Kinetics-700 Human Action Dataset", "comments": "arXiv admin note: substantial text overlap with arXiv:1808.01340", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We describe an extension of the DeepMind Kinetics human action dataset from\n600 classes to 700 classes, where for each class there are at least 600 video\nclips from different YouTube videos. This paper details the changes introduced\nfor this new release of the dataset, and includes a comprehensive set of\nstatistics as well as baseline results using the I3D neural network\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 12:58:21 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Carreira", "Joao", ""], ["Noland", "Eric", ""], ["Hillier", "Chloe", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1907.06989", "submitter": "R\\'obert Rill", "authors": "R\\'obert-Adrian Rill", "title": "Speed estimation evaluation on the KITTI benchmark based on motion and\n  monocular depth information", "comments": "technical report with 16 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report we investigate speed estimation of the ego-vehicle\non the KITTI benchmark using state-of-the-art deep neural network based optical\nflow and single-view depth prediction methods. Using a straightforward\nintuitive approach and approximating a single scale factor, we evaluate several\napplication schemes of the deep networks and formulate meaningful conclusions\nsuch as: combining depth information with optical flow improves speed\nestimation accuracy as opposed to using optical flow alone; the quality of the\ndeep neural network methods influences speed estimation performance; using the\ndepth and optical flow results from smaller crops of wide images degrades\nperformance. With these observations in mind, we achieve a RMSE of less than 1\nm/s for vehicle speed estimation using monocular images as input from\nrecordings of the KITTI benchmark. Limitations and possible future directions\nare discussed as well.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:36:25 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Rill", "R\u00f3bert-Adrian", ""]]}, {"id": "1907.06996", "submitter": "Alberto Testolin Dr.", "authors": "Alberto Testolin, Serena Dolfi, Mathijs Rochus and Marco Zorzi", "title": "Perception of visual numerosity in humans and machines", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerosity perception is foundational to mathematical learning, but its\ncomputational bases are strongly debated. Some investigators argue that humans\nare endowed with a specialized system supporting numerical representation;\nothers argue that visual numerosity is estimated using continuous magnitudes,\nsuch as density or area, which usually co-vary with number. Here we reconcile\nthese contrasting perspectives by testing deep networks on the same numerosity\ncomparison task that was administered to humans, using a stimulus space that\nallows to measure the contribution of non-numerical features. Our model\naccurately simulated the psychophysics of numerosity perception and the\nassociated developmental changes: discrimination was driven by numerosity\ninformation, but non-numerical features had a significant impact, especially\nearly during development. Representational similarity analysis further\nhighlighted that both numerosity and continuous magnitudes were spontaneously\nencoded even when no task had to be carried out, demonstrating that numerosity\nis a major, salient property of our visual environment.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:45:18 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Testolin", "Alberto", ""], ["Dolfi", "Serena", ""], ["Rochus", "Mathijs", ""], ["Zorzi", "Marco", ""]]}, {"id": "1907.07000", "submitter": "Kehan Qi", "authors": "Kehan Qi, Hao Yang, Cheng Li, Zaiyi Liu, Meiyun Wang, Qiegen Liu and\n  Shanshan Wang", "title": "X-Net: Brain Stroke Lesion Segmentation Based on Depthwise Separable\n  Convolution and Long-range Dependencies", "comments": "MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32248-9_28", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The morbidity of brain stroke increased rapidly in the past few years. To\nhelp specialists in lesion measurements and treatment planning, automatic\nsegmentation methods are critically required for clinical practices. Recently,\napproaches based on deep learning and methods for contextual information\nextraction have served in many image segmentation tasks. However, their\nperformances are limited due to the insufficient training of a large number of\nparameters, which sometimes fail in capturing long-range dependencies. To\naddress these issues, we propose a depthwise separable convolution based X-Net\nthat designs a nonlocal operation namely Feature Similarity Module (FSM) to\ncapture long-range dependencies. The adopted depthwise convolution allows to\nreduce the network size, while the developed FSM provides a more effective,\ndense contextual information extraction and thus facilitates better\nsegmentation. The effectiveness of X-Net was evaluated on an open dataset\nAnatomical Tracings of Lesions After Stroke (ATLAS) with superior performance\nachieved compared to other six state-of-the-art approaches. We make our code\nand models available at https://github.com/Andrewsher/X-Net.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:48:41 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 06:52:15 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Qi", "Kehan", ""], ["Yang", "Hao", ""], ["Li", "Cheng", ""], ["Liu", "Zaiyi", ""], ["Wang", "Meiyun", ""], ["Liu", "Qiegen", ""], ["Wang", "Shanshan", ""]]}, {"id": "1907.07008", "submitter": "Hao Yang", "authors": "Hao Yang, Weijian Huang, Kehan Qi, Cheng Li, Xinfeng Liu, Meiyun Wang,\n  Hairong Zheng, Shanshan Wang", "title": "CLCI-Net: Cross-Level fusion and Context Inference Networks for Lesion\n  Segmentation of Chronic Stroke", "comments": null, "journal-ref": "Medical Image Computing and Computer Assisted Intervention 2019:\n  266-274", "doi": "10.1007/978-3-030-32248-9_30", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting stroke lesions from T1-weighted MR images is of great value for\nlarge-scale stroke rehabilitation neuroimaging analyses. Nevertheless, there\nare great challenges with this task, such as large range of stroke lesion\nscales and the tissue intensity similarity. The famous encoder-decoder\nconvolutional neural network, which although has made great achievements in\nmedical image segmentation areas, may fail to address these challenges due to\nthe insufficient uses of multi-scale features and context information. To\naddress these challenges, this paper proposes a Cross-Level fusion and Context\nInference Network (CLCI-Net) for the chronic stroke lesion segmentation from\nT1-weighted MR images. Specifically, a Cross-Level feature Fusion (CLF)\nstrategy was developed to make full use of different scale features across\ndifferent levels; Extending Atrous Spatial Pyramid Pooling (ASPP) with CLF, we\nhave enriched multi-scale features to handle the different lesion sizes; In\naddition, convolutional long short-term memory (ConvLSTM) is employed to infer\ncontext information and thus capture fine structures to address the intensity\nsimilarity issue. The proposed approach was evaluated on an open-source\ndataset, the Anatomical Tracings of Lesions After Stroke (ATLAS) with the\nresults showing that our network outperforms five state-of-the-art methods. We\nmake our code and models available at https://github.com/YH0517/CLCI_Net.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:54:50 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 02:26:04 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Yang", "Hao", ""], ["Huang", "Weijian", ""], ["Qi", "Kehan", ""], ["Li", "Cheng", ""], ["Liu", "Xinfeng", ""], ["Wang", "Meiyun", ""], ["Zheng", "Hairong", ""], ["Wang", "Shanshan", ""]]}, {"id": "1907.07011", "submitter": "Boxi Wu", "authors": "Boxi Wu, Shuai Zhao, Wenqing Chu, Zheng Yang, Deng Cai", "title": "Improving Semantic Segmentation via Dilated Affinity", "comments": "10 pages, 5 figures, under review of NIPS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introducing explicit constraints on the structural predictions has been an\neffective way to improve the performance of semantic segmentation models.\nExisting methods are mainly based on insufficient hand-crafted rules that only\npartially capture the image structure, and some methods can also suffer from\nthe efficiency issue. As a result, most of the state-of-the-art fully\nconvolutional networks did not adopt these techniques. In this work, we propose\na simple, fast yet effective method that exploits structural information\nthrough direct supervision with minor additional expense. To be specific, our\nmethod explicitly requires the network to predict semantic segmentation as well\nas dilated affinity, which is a sparse version of pair-wise pixel affinity. The\ncapability of telling the relationships between pixels are directly built into\nthe model and enhance the quality of segmentation in two stages. 1) Joint\ntraining with dilated affinity can provide robust feature representations and\nthus lead to finer segmentation results. 2) The extra output of affinity\ninformation can be further utilized to refine the original segmentation with a\nfast propagation process. Consistent improvements are observed on various\nbenchmark datasets when applying our framework to the existing state-of-the-art\nmodel. Codes will be released soon.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:59:22 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 02:00:03 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Wu", "Boxi", ""], ["Zhao", "Shuai", ""], ["Chu", "Wenqing", ""], ["Yang", "Zheng", ""], ["Cai", "Deng", ""]]}, {"id": "1907.07023", "submitter": "Panagiotis Meletis", "authors": "Panagiotis Meletis and Rob Romijnders and Gijs Dubbelman", "title": "Data Selection for training Semantic Segmentation CNNs with\n  cross-dataset weak supervision", "comments": "IEEE ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Training convolutional networks for semantic segmentation with strong\n(per-pixel) and weak (per-bounding-box) supervision requires a large amount of\nweakly labeled data. We propose two methods for selecting the most relevant\ndata with weak supervision. The first method is designed for finding visually\nsimilar images without the need of labels and is based on modeling image\nrepresentations with a Gaussian Mixture Model (GMM). As a byproduct of GMM\nmodeling, we present useful insights on characterizing the data generating\ndistribution. The second method aims at finding images with high object\ndiversity and requires only the bounding box labels. Both methods are developed\nin the context of automated driving and experimentation is conducted on\nCityscapes and Open Images datasets. We demonstrate performance gains by\nreducing the amount of employed weakly labeled images up to 100 times for Open\nImages and up to 20 times for Cityscapes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:17:06 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Meletis", "Panagiotis", ""], ["Romijnders", "Rob", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "1907.07034", "submitter": "Lequan Yu", "authors": "Lequan Yu, Shujun Wang, Xiaomeng Li, Chi-Wing Fu, Pheng-Ann Heng", "title": "Uncertainty-aware Self-ensembling Model for Semi-supervised 3D Left\n  Atrium Segmentation", "comments": "Accepted by MICCAI2019; Code is available in\n  https://github.com/yulequan/UA-MT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep convolutional neural networks usually requires a large amount\nof labeled data. However, it is expensive and time-consuming to annotate data\nfor medical image segmentation tasks. In this paper, we present a novel\nuncertainty-aware semi-supervised framework for left atrium segmentation from\n3D MR images. Our framework can effectively leverage the unlabeled data by\nencouraging consistent predictions of the same input under different\nperturbations. Concretely, the framework consists of a student model and a\nteacher model, and the student model learns from the teacher model by\nminimizing a segmentation loss and a consistency loss with respect to the\ntargets of the teacher model. We design a novel uncertainty-aware scheme to\nenable the student model to gradually learn from the meaningful and reliable\ntargets by exploiting the uncertainty information. Experiments show that our\nmethod achieves high performance gains by incorporating the unlabeled data. Our\nmethod outperforms the state-of-the-art semi-supervised methods, demonstrating\nthe potential of our framework for the challenging semi-supervised problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:34:11 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Yu", "Lequan", ""], ["Wang", "Shujun", ""], ["Li", "Xiaomeng", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1907.07045", "submitter": "Borna Bi\\'cani\\'c", "authors": "Borna Bi\\'cani\\'c, Marin Or\\v{s}i\\'c, Ivan Markovi\\'c, Sini\\v{s}a\n  \\v{S}egvi\\'c, Ivan Petrovi\\'c", "title": "Pedestrian Tracking by Probabilistic Data Association and Correspondence\n  Embeddings", "comments": null, "journal-ref": "22nd International Conference on Information Fusion (FUSION)\n  (2019)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the interplay between kinematics (position and velocity)\nand appearance cues for establishing correspondences in multi-target pedestrian\ntracking. We investigate tracking-by-detection approaches based on a deep\nlearning detector, joint integrated probabilistic data association (JIPDA), and\nappearance-based tracking of deep correspondence embeddings. We first addressed\nthe fixed-camera setup by fine-tuning a convolutional detector for accurate\npedestrian detection and combining it with kinematic-only JIPDA. The resulting\nsubmission ranked first on the 3DMOT2015 benchmark. However, in sequences with\na moving camera and unknown ego-motion, we achieved the best results by\nreplacing kinematic cues with global nearest neighbor tracking of deep\ncorrespondence embeddings. We trained the embeddings by fine-tuning features\nfrom the second block of ResNet-18 using angular loss extended by a margin\nterm. We note that integrating deep correspondence embeddings directly in JIPDA\ndid not bring significant improvement. It appears that geometry of deep\ncorrespondence embeddings for soft data association needs further investigation\nin order to obtain the best from both worlds.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:58:37 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Bi\u0107ani\u0107", "Borna", ""], ["Or\u0161i\u0107", "Marin", ""], ["Markovi\u0107", "Ivan", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""], ["Petrovi\u0107", "Ivan", ""]]}, {"id": "1907.07061", "submitter": "Farzan Erlik Nowruzi", "authors": "Farzan Erlik Nowruzi and Prince Kapoor and Dhanvin Kolhatkar and Fahed\n  Al Hassanat and Robert Laganiere and Julien Rebut", "title": "How much real data do we actually need: Analyzing object detection\n  performance using synthetic and real data", "comments": "Accepted in International Conference on Machine Learning (ICML 2019)\n  Workshop on AI for Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning models have resulted in a huge amount of\nprogress in various areas, including computer vision. By nature, the supervised\ntraining of deep models requires a large amount of data to be available. This\nideal case is usually not tractable as the data annotation is a tremendously\nexhausting and costly task to perform. An alternative is to use synthetic data.\nIn this paper, we take a comprehensive look into the effects of replacing real\ndata with synthetic data. We further analyze the effects of having a limited\namount of real data. We use multiple synthetic and real datasets along with a\nsimulation tool to create large amounts of cheaply annotated synthetic data. We\nanalyze the domain similarity of each of these datasets. We provide insights\nabout designing a methodological procedure for training deep networks using\nthese datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:21:38 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Nowruzi", "Farzan Erlik", ""], ["Kapoor", "Prince", ""], ["Kolhatkar", "Dhanvin", ""], ["Hassanat", "Fahed Al", ""], ["Laganiere", "Robert", ""], ["Rebut", "Julien", ""]]}, {"id": "1907.07077", "submitter": "Giulia Bert\\`o", "authors": "Giulia Bert\\`o, Paolo Avesani, Franco Pestilli, Daniel Bullock,\n  Bradley Caron and Emanuele Olivetti", "title": "Anatomically-Informed Multiple Linear Assignment Problems for White\n  Matter Bundle Segmentation", "comments": null, "journal-ref": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019)", "doi": "10.1109/ISBI.2019.8759174", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting white matter bundles from human tractograms is a task of interest\nfor several applications. Current methods for bundle segmentation consider\neither only prior knowledge about the relative anatomical position of a bundle,\nor only its geometrical properties. Our aim is to improve the results of\nsegmentation by proposing a method that takes into account information about\nboth the underlying anatomy and the geometry of bundles at the same time. To\nachieve this goal, we extend a state-of-the-art example-based method based on\nthe Linear Assignment Problem (LAP) by including prior anatomical information\nwithin the optimization process. The proposed method shows a significant\nimprovement with respect to the original method, in particular on small\nbundles.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:42:21 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Bert\u00f2", "Giulia", ""], ["Avesani", "Paolo", ""], ["Pestilli", "Franco", ""], ["Bullock", "Daniel", ""], ["Caron", "Bradley", ""], ["Olivetti", "Emanuele", ""]]}, {"id": "1907.07131", "submitter": "Ying Da Wang", "authors": "Ying Da Wang, Ryan T. Armstrong, Peyman Mostaghimi", "title": "Boosting Resolution and Recovering Texture of micro-CT Images with Deep\n  Learning", "comments": "\\keywords{Digital Rock Imaging \\and Super Resolution \\and\n  Convolutional Neural Networks \\and Generative Adversarial Networks}", "journal-ref": null, "doi": "10.1029/2019WR026052", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Rock Imaging is constrained by detector hardware, and a trade-off\nbetween the image field of view (FOV) and the image resolution must be made.\nThis can be compensated for with super resolution (SR) techniques that take a\nwide FOV, low resolution (LR) image, and super resolve a high resolution (HR),\nhigh FOV image. The Enhanced Deep Super Resolution Generative Adversarial\nNetwork (EDSRGAN) is trained on the Deep Learning Digital Rock Super Resolution\nDataset, a diverse compilation 12000 of raw and processed uCT images. The\nnetwork shows comparable performance of 50% to 70% reduction in relative error\nover bicubic interpolation. GAN performance in recovering texture shows\nsuperior visual similarity compared to SRCNN and other methods. Difference maps\nindicate that the SRCNN section of the SRGAN network recovers large scale edge\n(grain boundaries) features while the GAN network regenerates perceptually\nindistinguishable high frequency texture. Network performance is generalised\nwith augmentation, showing high adaptability to noise and blur. HR images are\nfed into the network, generating HR-SR images to extrapolate network\nperformance to sub-resolution features present in the HR images themselves.\nResults show that under-resolution features such as dissolved minerals and thin\nfractures are regenerated despite the network operating outside of trained\nspecifications. Comparison with Scanning Electron Microscope images shows\ndetails are consistent with the underlying geometry of the sample. Recovery of\ntextures benefits the characterisation of digital rocks with a high proportion\nof under-resolution micro-porous features, such as carbonate and coal samples.\nImages that are normally constrained by the mineralogy of the rock (coal), by\nfast transient imaging (waterflooding), or by the energy of the source\n(microporosity), can be super resolved accurately for further analysis\ndownstream.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 04:32:50 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 06:17:34 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2019 01:37:33 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Da Wang", "Ying", ""], ["Armstrong", "Ryan T.", ""], ["Mostaghimi", "Peyman", ""]]}, {"id": "1907.07156", "submitter": "Dmitrii Marin", "authors": "Dmitrii Marin, Zijian He, Peter Vajda, Priyam Chatterjee, Sam Tsai,\n  Fei Yang, Yuri Boykov", "title": "Efficient Segmentation: Learning Downsampling Near Semantic Boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many automated processes such as auto-piloting rely on a good semantic\nsegmentation as a critical component. To speed up performance, it is common to\ndownsample the input frame. However, this comes at the cost of missed small\nobjects and reduced accuracy at semantic boundaries. To address this problem,\nwe propose a new content-adaptive downsampling technique that learns to favor\nsampling locations near semantic boundaries of target classes. Cost-performance\nanalysis shows that our method consistently outperforms the uniform sampling\nimproving balance between accuracy and computational efficiency. Our adaptive\nsampling gives segmentation with better quality of boundaries and more reliable\nsupport for smaller-size objects.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:27:21 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Marin", "Dmitrii", ""], ["He", "Zijian", ""], ["Vajda", "Peter", ""], ["Chatterjee", "Priyam", ""], ["Tsai", "Sam", ""], ["Yang", "Fei", ""], ["Boykov", "Yuri", ""]]}, {"id": "1907.07160", "submitter": "Guan Wang", "authors": "Yu Chen and Guan Wang", "title": "EnforceNet: Monocular Camera Localization in Large Scale Indoor Sparse\n  LiDAR Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Pose estimation is a fundamental building block for robotic applications such\nas autonomous vehicles, UAV, and large scale augmented reality. It is also a\nprohibitive factor for those applications to be in mass production, since the\nstate-of-the-art, centimeter-level pose estimation often requires long mapping\nprocedures and expensive localization sensors, e.g. LiDAR and high precision\nGPS/IMU, etc. To overcome the cost barrier, we propose a neural network based\nsolution to localize a consumer degree RGB camera within a prior sparse LiDAR\nmap with comparable centimeter-level precision. We achieved it by introducing a\nnovel network module, which we call resistor module, to enforce the network\ngeneralize better, predicts more accurately, and converge faster. Such results\nare benchmarked by several datasets we collected in the large scale indoor\nparking garage scenes. We plan to open both the data and the code for the\ncommunity to join the effort to advance this field.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:35:53 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Chen", "Yu", ""], ["Wang", "Guan", ""]]}, {"id": "1907.07161", "submitter": "Yusan Lin", "authors": "Yusan Lin and Hao Yang", "title": "Predicting Next-Season Designs on High Fashion Runway", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fashion is a large and fast-changing industry. Foreseeing the upcoming\nfashion trends is beneficial for fashion designers, consumers, and retailers.\nHowever, fashion trends are often perceived as unpredictable due to the\nenormous amount of factors involved into designers' subjectivity. In this\npaper, we propose a fashion trend prediction framework and design neural\nnetwork models to leverage structured fashion runway show data, learn the\nfashion collection embedding, and further train RNN/LSTM models to capture the\ndesigners' style evolution. Our proposed framework consists of (1) a runway\nembedding learning model that uses fashion runway images to learn every\nseason's collection embedding, and (2) a next-season fashion design prediction\nmodel that leverage the concept of designer style and trend to predict\nnext-season design given designers. Through experiments on a collected dataset\nacross 32 years of fashion shows, our framework can achieve the best\nperformance of 78.42% AUC on average and 95% for an individual designer when\npredicting the next season's design.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:38:45 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Lin", "Yusan", ""], ["Yang", "Hao", ""]]}, {"id": "1907.07165", "submitter": "Yash Goyal", "authors": "Yash Goyal, Amir Feder, Uri Shalit, Been Kim", "title": "Explaining Classifiers with Causal Concept Effect (CaCE)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we understand classification decisions made by deep neural networks?\nMany existing explainability methods rely solely on correlations and fail to\naccount for confounding, which may result in potentially misleading\nexplanations. To overcome this problem, we define the Causal Concept Effect\n(CaCE) as the causal effect of (the presence or absence of) a\nhuman-interpretable concept on a deep neural net's predictions. We show that\nthe CaCE measure can avoid errors stemming from confounding. Estimating CaCE is\ndifficult in situations where we cannot easily simulate the do-operator. To\nmitigate this problem, we use a generative model, specifically a Variational\nAutoEncoder (VAE), to measure VAE-CaCE. In an extensive experimental analysis,\nwe show that the VAE-CaCE is able to estimate the true concept causal effect,\ncompared to baselines for a number of datasets including high dimensional\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:47:43 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 18:56:14 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Goyal", "Yash", ""], ["Feder", "Amir", ""], ["Shalit", "Uri", ""], ["Kim", "Been", ""]]}, {"id": "1907.07171", "submitter": "Lucy Chai", "authors": "Ali Jahanian, Lucy Chai, Phillip Isola", "title": "On the \"steerability\" of generative adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open secret in contemporary machine learning is that many models work\nbeautifully on standard benchmarks but fail to generalize outside the lab. This\nhas been attributed to biased training data, which provide poor coverage over\nreal world events. Generative models are no exception, but recent advances in\ngenerative adversarial networks (GANs) suggest otherwise - these models can now\nsynthesize strikingly realistic and diverse images. Is generative modeling of\nphotos a solved problem? We show that although current GANs can fit standard\ndatasets very well, they still fall short of being comprehensive models of the\nvisual manifold. In particular, we study their ability to fit simple\ntransformations such as camera movements and color changes. We find that the\nmodels reflect the biases of the datasets on which they are trained (e.g.,\ncentered objects), but that they also exhibit some capacity for generalization:\nby \"steering\" in latent space, we can shift the distribution while still\ncreating realistic images. We hypothesize that the degree of distributional\nshift is related to the breadth of the training data distribution. Thus, we\nconduct experiments to quantify the limits of GAN transformations and introduce\ntechniques to mitigate the problem. Code is released on our project page:\nhttps://ali-design.github.io/gan_steerability/\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:55:07 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 21:28:26 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 05:45:54 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 01:13:18 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Jahanian", "Ali", ""], ["Chai", "Lucy", ""], ["Isola", "Phillip", ""]]}, {"id": "1907.07174", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, Dawn Song", "title": "Natural Adversarial Examples", "comments": "CVPR 2021; dataset and code available at\n  https://github.com/hendrycks/natural-adv-examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two challenging datasets that reliably cause machine learning\nmodel performance to substantially degrade. The datasets are collected with a\nsimple adversarial filtration technique to create datasets with limited\nspurious cues. Our datasets' real-world, unmodified examples transfer to\nvarious unseen models reliably, demonstrating that computer vision models have\nshared weaknesses. The first dataset is called ImageNet-A and is like the\nImageNet test set, but it is far more challenging for existing models. We also\ncurate an adversarial out-of-distribution detection dataset called ImageNet-O,\nwhich is the first out-of-distribution detection dataset created for ImageNet\nmodels. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy\ndrop of approximately 90%, and its out-of-distribution detection performance on\nImageNet-O is near random chance levels. We find that existing data\naugmentation techniques hardly boost performance, and using other public\ntraining datasets provides improvements that are limited. However, we find that\nimprovements to computer vision architectures provide a promising path towards\nrobust models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:56:30 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 16:32:28 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 01:30:54 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2021 21:56:19 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Hendrycks", "Dan", ""], ["Zhao", "Kevin", ""], ["Basart", "Steven", ""], ["Steinhardt", "Jacob", ""], ["Song", "Dawn", ""]]}, {"id": "1907.07198", "submitter": "Avik Pal", "authors": "Avik Pal", "title": "RayTracer.jl: A Differentiable Renderer that supports Parameter\n  Optimization for Scene Reconstruction", "comments": "Proceedings of the JuliaCon Conferences 2019", "journal-ref": null, "doi": "10.5281/zenodo.1442780", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present RayTracer.jl, a renderer in Julia that is fully\ndifferentiable using source-to-source Automatic Differentiation (AD). This\nmeans that RayTracer not only renders 2D images from 3D scene parameters, but\nit can be used to optimize for model parameters that generate a target image in\na Differentiable Programming (DP) pipeline. We interface our renderer with the\ndeep learning library Flux for use in combination with neural networks. We\ndemonstrate the use of this differentiable renderer in rendering tasks and in\nsolving inverse graphics problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 18:01:44 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 19:43:00 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 15:06:21 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Pal", "Avik", ""]]}, {"id": "1907.07210", "submitter": "Kirill Muravyev", "authors": "Andrey Bokovoy, Kirill Muravyev and Konstantin Yakovlev", "title": "Real-time Vision-based Depth Reconstruction with NVidia Jetson", "comments": "Camera-ready version as submitted to ECMR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based depth reconstruction is a challenging problem extensively\nstudied in computer vision but still lacking universal solution. Reconstructing\ndepth from single image is particularly valuable to mobile robotics as it can\nbe embedded to the modern vision-based simultaneous localization and mapping\n(vSLAM) methods providing them with the metric information needed to construct\naccurate maps in real scale. Typically, depth reconstruction is done nowadays\nvia fully-convolutional neural networks (FCNNs). In this work we experiment\nwith several FCNN architectures and introduce a few enhancements aimed at\nincreasing both the effectiveness and the efficiency of the inference. We\nexperimentally determine the solution that provides the best\nperformance/accuracy tradeoff and is able to run on NVidia Jetson with the\nframerates exceeding 16FPS for 320 x 240 input. We also evaluate the suggested\nmodels by conducting monocular vSLAM of unknown indoor environment on NVidia\nJetson TX2 in real-time. Open-source implementation of the models and the\ninference node for Robot Operating System (ROS) are available at\nhttps://github.com/CnnDepth/tx2_fcnn_node.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 18:33:20 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Bokovoy", "Andrey", ""], ["Muravyev", "Kirill", ""], ["Yakovlev", "Konstantin", ""]]}, {"id": "1907.07220", "submitter": "Fabian Timm", "authors": "Lukas Enderich and Fabian Timm and Lars Rosenbaum and Wolfram Burgard", "title": "Learning Multimodal Fixed-Point Weights using Gradient Descent", "comments": "presented at ESANN 2019 (European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning)", "journal-ref": "https://www.elen.ucl.ac.be/esann/proceedings/papers.php?ann=2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their high computational complexity, deep neural networks are still\nlimited to powerful processing units. To promote a reduced model complexity by\ndint of low-bit fixed-point quantization, we propose a gradient-based\noptimization strategy to generate a symmetric mixture of Gaussian modes (SGM)\nwhere each mode belongs to a particular quantization stage. We achieve 2-bit\nstate-of-the-art performance and illustrate the model's ability for\nself-dependent weight adaptation during training.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 19:11:01 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Enderich", "Lukas", ""], ["Timm", "Fabian", ""], ["Rosenbaum", "Lars", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1907.07227", "submitter": "Igror Slinko", "authors": "Igor Slinko, Anna Vorontsova, Filipp Konokhov, Olga Barinova, Anton\n  Konushin", "title": "Scene Motion Decomposition for Learnable Visual Odometry", "comments": null, "journal-ref": "CVPR 2019 Workshop", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Flow (OF) and depth are commonly used for visual odometry since they\nprovide sufficient information about camera ego-motion in a rigid scene. We\nreformulate the problem of ego-motion estimation as a problem of motion\nestimation of a 3D-scene with respect to a static camera. The entire scene\nmotion can be represented as a combination of motions of its visible points.\nUsing OF and depth we estimate a motion of each point in terms of 6DoF and\nrepresent results in the form of motion maps, each one addressing single degree\nof freedom. In this work we provide motion maps as inputs to a deep neural\nnetwork that predicts 6DoF of scene motion. Through our evaluation on outdoor\nand indoor datasets we show that utilizing motion maps leads to accuracy\nimprovement in comparison with naive stacking of depth and OF. Another\ncontribution of our work is a novel network architecture that efficiently\nexploits motion maps and outperforms learnable RGB/RGB-D baselines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 19:38:32 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Slinko", "Igor", ""], ["Vorontsova", "Anna", ""], ["Konokhov", "Filipp", ""], ["Barinova", "Olga", ""], ["Konushin", "Anton", ""]]}, {"id": "1907.07270", "submitter": "Alessandro Lameiras Koerich", "authors": "Israel A. Laurensi R., Luciana T. Menon, Manoel Camillo O. Penna N.,\n  Alessandro L. Koerich, Alceu S. Britto Jr", "title": "Style Transfer Applied to Face Liveness Detection with User-Centered\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a face anti-spoofing user-centered model (FAS-UCM). The\nmajor difficulty, in this case, is obtaining fraudulent images from all users\nto train the models. To overcome this problem, the proposed method is divided\nin three main parts: generation of new spoof images, based on style transfer\nand spoof image representation models; training of a Convolutional Neural\nNetwork (CNN) for liveness detection; evaluation of the live and spoof testing\nimages for each subject. The generalization of the CNN to perform style\ntransfer has shown promising qualitative results. Preliminary results have\nshown that the proposed method is capable of distinguishing between live and\nspoof images on the SiW database, with an average classification error rate of\n0.22.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 21:40:29 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["R.", "Israel A. Laurensi", ""], ["Menon", "Luciana T.", ""], ["N.", "Manoel Camillo O. Penna", ""], ["Koerich", "Alessandro L.", ""], ["Britto", "Alceu S.", "Jr"]]}, {"id": "1907.07274", "submitter": "Yuansheng Hua", "authors": "Yuansheng Hua, Lichao Mou, Xiao Xiang Zhu", "title": "Relation Network for Multi-label Aerial Image Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2019.2963364", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification plays a momentous role in perceiving intricate\ncontents of an aerial image and triggers several related studies over the last\nyears. However, most of them deploy few efforts in exploiting label relations,\nwhile such dependencies are crucial for making accurate predictions. Although\nan LSTM layer can be introduced to modeling such label dependencies in a chain\npropagation manner, the efficiency might be questioned when certain labels are\nimproperly inferred. To address this, we propose a novel aerial image\nmulti-label classification network, attention-aware label relational reasoning\nnetwork. Particularly, our network consists of three elemental modules: 1) a\nlabel-wise feature parcel learning module, 2) an attentional region extraction\nmodule, and 3) a label relational inference module. To be more specific, the\nlabel-wise feature parcel learning module is designed for extracting high-level\nlabel-specific features. The attentional region extraction module aims at\nlocalizing discriminative regions in these features and yielding attentional\nlabel-specific features. The label relational inference module finally predicts\nlabel existences using label relations reasoned from outputs of the previous\nmodule. The proposed network is characterized by its capacities of extracting\ndiscriminative label-wise features in a proposal-free way and reasoning about\nlabel relations naturally and interpretably. In our experiments, we evaluate\nthe proposed model on the UCM multi-label dataset and a newly produced dataset,\nAID multi-label dataset. Quantitative and qualitative results on these two\ndatasets demonstrate the effectiveness of our model. To facilitate progress in\nthe multi-label aerial image classification, the AID multi-label dataset will\nbe made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 22:00:47 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 18:11:45 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 03:44:21 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hua", "Yuansheng", ""], ["Mou", "Lichao", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1907.07287", "submitter": "Simon Guiroy", "authors": "Simon Guiroy, Vikas Verma, Christopher Pal", "title": "Towards Understanding Generalization in Gradient-Based Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study generalization of neural networks in gradient-based\nmeta-learning by analyzing various properties of the objective landscapes. We\nexperimentally demonstrate that as meta-training progresses, the meta-test\nsolutions, obtained after adapting the meta-train solution of the model, to new\ntasks via few steps of gradient-based fine-tuning, become flatter, lower in\nloss, and further away from the meta-train solution. We also show that those\nmeta-test solutions become flatter even as generalization starts to degrade,\nthus providing an experimental evidence against the correlation between\ngeneralization and flat minima in the paradigm of gradient-based meta-leaning.\nFurthermore, we provide empirical evidence that generalization to new tasks is\ncorrelated with the coherence between their adaptation trajectories in\nparameter space, measured by the average cosine similarity between\ntask-specific trajectory directions, starting from a same meta-train solution.\nWe also show that coherence of meta-test gradients, measured by the average\ninner product between the task-specific gradient vectors evaluated at\nmeta-train solution, is also correlated with generalization. Based on these\nobservations, we propose a novel regularizer for MAML and provide experimental\nevidence for its effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 23:22:14 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Guiroy", "Simon", ""], ["Verma", "Vikas", ""], ["Pal", "Christopher", ""]]}, {"id": "1907.07315", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Jiacheng Zhu, Wenshuo Wang, Ding Zhao", "title": "A General Framework of Learning Multi-Vehicle Interaction Patterns from\n  Videos", "comments": "2019 IEEE Intelligent Transportation Systems Conference (ITSC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic learning and understanding of multi-vehicle interaction patterns in\na cluttered driving environment are essential but challenging for autonomous\nvehicles to make proper decisions. This paper presents a general framework to\ngain insights into intricate multi-vehicle interaction patterns from bird's-eye\nview traffic videos. We adopt a Gaussian velocity field to describe the\ntime-varying multi-vehicle interaction behaviors and then use deep autoencoders\nto learn associated latent representations for each temporal frame. Then, we\nutilize a hidden semi-Markov model with a hierarchical Dirichlet process as a\nprior to segment these sequential representations into granular components,\nalso called traffic primitives, corresponding to interaction patterns.\nExperimental results demonstrate that our proposed framework can extract\ntraffic primitives from videos, thus providing a semantic way to analyze\nmulti-vehicle interaction patterns, even for cluttered driving scenarios that\nare far messier than human beings can cope with.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 03:41:51 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Zhu", "Jiacheng", ""], ["Wang", "Wenshuo", ""], ["Zhao", "Ding", ""]]}, {"id": "1907.07319", "submitter": "Benjamin Kellenberger", "authors": "Benjamin Kellenberger, Diego Marcos, Sylvain Lobry, Devis Tuia", "title": "Half a Percent of Labels is Enough: Efficient Animal Detection in UAV\n  Imagery using Deep CNNs and Active Learning", "comments": "In press at IEEE Transactions on Geoscience and Remote Sensing (TGRS)", "journal-ref": null, "doi": "10.1109/TGRS.2019.2927393", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an Active Learning (AL) strategy for re-using a deep Convolutional\nNeural Network (CNN)-based object detector on a new dataset. This is of\nparticular interest for wildlife conservation: given a set of images acquired\nwith an Unmanned Aerial Vehicle (UAV) and manually labeled gound truth, our\ngoal is to train an animal detector that can be re-used for repeated\nacquisitions, e.g. in follow-up years. Domain shifts between datasets typically\nprevent such a direct model application. We thus propose to bridge this gap\nusing AL and introduce a new criterion called Transfer Sampling (TS). TS uses\nOptimal Transport to find corresponding regions between the source and the\ntarget datasets in the space of CNN activations. The CNN scores in the source\ndataset are used to rank the samples according to their likelihood of being\nanimals, and this ranking is transferred to the target dataset. Unlike\nconventional AL criteria that exploit model uncertainty, TS focuses on very\nconfident samples, thus allowing a quick retrieval of true positives in the\ntarget dataset, where positives are typically extremely rare and difficult to\nfind by visual inspection. We extend TS with a new window cropping strategy\nthat further accelerates sample retrieval. Our experiments show that with both\nstrategies combined, less than half a percent of oracle-provided labels are\nenough to find almost 80% of the animals in challenging sets of UAV images,\nbeating all baselines by a margin.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 04:06:17 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kellenberger", "Benjamin", ""], ["Marcos", "Diego", ""], ["Lobry", "Sylvain", ""], ["Tuia", "Devis", ""]]}, {"id": "1907.07324", "submitter": "Andr\\'e Goo{\\ss}en", "authors": "Andr\\'e Goo{\\ss}en, Hrishikesh Deshpande, Tim Harder, Evan Schwab, Ivo\n  Baltruschat, Thusitha Mabotuwana, Nathan Cross, Axel Saalbach", "title": "Deep Learning for Pneumothorax Detection and Localization in Chest\n  Radiographs", "comments": "MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/SkxvPEqIwV", "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pneumothorax is a critical condition that requires timely communication and\nimmediate action. In order to prevent significant morbidity or patient death,\nearly detection is crucial. For the task of pneumothorax detection, we study\nthe characteristics of three different deep learning techniques: (i)\nconvolutional neural networks, (ii) multiple-instance learning, and (iii) fully\nconvolutional networks. We perform a five-fold cross-validation on a dataset\nconsisting of 1003 chest X-ray images. ROC analysis yields AUCs of 0.96, 0.93,\nand 0.92 for the three methods, respectively. We review the classification and\nlocalization performance of these approaches as well as an ensemble of the\nthree aforementioned techniques.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 11:06:48 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Goo\u00dfen", "Andr\u00e9", ""], ["Deshpande", "Hrishikesh", ""], ["Harder", "Tim", ""], ["Schwab", "Evan", ""], ["Baltruschat", "Ivo", ""], ["Mabotuwana", "Thusitha", ""], ["Cross", "Nathan", ""], ["Saalbach", "Axel", ""]]}, {"id": "1907.07345", "submitter": "Sergey Podlesnyy", "authors": "Sergey Podlesnyy", "title": "Towards Data-Driven Automatic Video Editing", "comments": "2019 15th International Conference on Natural Computation, Fuzzy\n  Systems and Knowledge Discovery, Kunming, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic video editing involving at least the steps of selecting the most\nvaluable footage from points of view of visual quality and the importance of\naction filmed; and cutting the footage into a brief and coherent visual story\nthat would be interesting to watch is implemented in a purely data-driven\nmanner. Visual semantic and aesthetic features are extracted by the\nImageNet-trained convolutional neural network, and the editing controller is\ntrained by an imitation learning algorithm. As a result, at test time the\ncontroller shows the signs of observing basic cinematography editing rules\nlearned from the corpus of motion pictures masterpieces.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 05:54:37 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Podlesnyy", "Sergey", ""]]}, {"id": "1907.07388", "submitter": "Samarth Manoj Brahmbhatt", "authors": "Samarth Brahmbhatt, Charles C. Kemp and James Hays", "title": "Towards Markerless Grasp Capture", "comments": "Third Workshop on Computer Vision for AR/VR, CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans excel at grasping objects and manipulating them. Capturing human\ngrasps is important for understanding grasping behavior and reconstructing it\nrealistically in Virtual Reality (VR). However, grasp capture - capturing the\npose of a hand grasping an object, and orienting it w.r.t. the object - is\ndifficult because of the complexity and diversity of the human hand, and\nocclusion. Reflective markers and magnetic trackers traditionally used to\nmitigate this difficulty introduce undesirable artifacts in images and can\ninterfere with natural grasping behavior. We present preliminary work on a\ncompletely marker-less algorithm for grasp capture from a video depicting a\ngrasp. We show how recent advances in 2D hand pose estimation can be used with\nwell-established optimization techniques. Uniquely, our algorithm can also\ncapture hand-object contact in detail and integrate it in the grasp capture\nprocess. This is work in progress, find more details at https://contactdb.\ncc.gatech.edu/grasp_capture.html.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 08:41:21 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Brahmbhatt", "Samarth", ""], ["Kemp", "Charles C.", ""], ["Hays", "James", ""]]}, {"id": "1907.07408", "submitter": "Risheng Liu", "authors": "Risheng Liu, Long Ma, Yuxi Zhang, Xin Fan, Zhongxuan Luo", "title": "Underexposed Image Correction via Hybrid Priors Navigated Deep\n  Propagation", "comments": "Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS). Project page: http://dutmedia.org/HPNDP/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancing visual qualities for underexposed images is an extensively\nconcerned task that plays important roles in various areas of multimedia and\ncomputer vision. Most existing methods often fail to generate high-quality\nresults with appropriate luminance and abundant details. To address these\nissues, we in this work develop a novel framework, integrating both knowledge\nfrom physical principles and implicit distributions from data to solve the\nunderexposed image correction task. More concretely, we propose a new\nperspective to formulate this task as an energy-inspired model with advanced\nhybrid priors. A propagation procedure navigated by the hybrid priors is well\ndesigned for simultaneously propagating the reflectance and illumination toward\ndesired results. We conduct extensive experiments to verify the necessity of\nintegrating both underlying principles (i.e., with knowledge) and distributions\n(i.e., from data) as navigated deep propagation. Plenty of experimental results\nof underexposed image correction demonstrate that our proposed method performs\nfavorably against the state-of-the-art methods on both subjective and objective\nassessments. Additionally, we execute the task of face detection to further\nverify the naturalness and practical value of underexposed image correction.\nWhat's more, we employ our method to single image haze removal whose\nexperimental results further demonstrate its superiorities.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 09:27:05 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Liu", "Risheng", ""], ["Ma", "Long", ""], ["Zhang", "Yuxi", ""], ["Fan", "Xin", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1907.07449", "submitter": "Lanyun Zhu", "authors": "Shiping Zhu, Lanyun Zhu", "title": "OGNet: Salient Object Detection with Output-guided Attention Module", "comments": "submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms are widely used in salient object detection models based\non deep learning, which can effectively promote the extraction and utilization\nof useful information by neural networks. However, most of the existing\nattention modules used in salient object detection are input with the processed\nfeature map itself, which easily leads to the problem of `blind\noverconfidence'. In this paper, instead of applying the widely used\nself-attention module, we present an output-guided attention module built with\nmulti-scale outputs to overcome the problem of `blind overconfidence'. We also\nconstruct a new loss function, the intractable area F-measure loss function,\nwhich is based on the F-measure of the hard-to-handle area to improve the\ndetection effect of the model in the edge areas and confusing areas of an\nimage. Extensive experiments and abundant ablation studies are conducted to\nevaluate the effect of our methods and to explore the most suitable structure\nfor the model. Tests on several data sets show that our model performs very\nwell, even though it is very lightweight.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 11:36:37 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Zhu", "Shiping", ""], ["Zhu", "Lanyun", ""]]}, {"id": "1907.07477", "submitter": "Murari Mandal", "authors": "Murari Mandal, Manal Shah, Prashant Meena, Sanhita Devi, Santosh Kumar\n  Vipparthi", "title": "AVDNet: A Small-Sized Vehicle Detection Network for Aerial Visual Data", "comments": "IEEE Geoscience and Remote Sensing Letters, doi:\n  10.1109/LGRS.2019.2923564", "journal-ref": "IEEE Geoscience and Remote Sensing Letters, doi:\n  10.1109/LGRS.2019.2923564", "doi": "10.1109/LGRS.2019.2923564", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of small-sized targets in aerial views is a challenging task due to\nthe smallness of vehicle size, complex background, and monotonic object\nappearances. In this letter, we propose a one-stage vehicle detection network\n(AVDNet) to robustly detect small-sized vehicles in aerial scenes. In AVDNet,\nwe introduced ConvRes residual blocks at multiple scales to alleviate the\nproblem of vanishing features for smaller objects caused because of the\ninclusion of deeper convolutional layers. These residual blocks, along with\nenlarged output feature map, ensure the robust representation of the salient\nfeatures for small sized objects. Furthermore, we proposed a recurrent-feature\naware visualization (RFAV) technique to analyze the network behavior. We also\ncreated a new airborne image data set (ABD) by annotating 1396 new objects in\n79 aerial images for our experiments. The effectiveness of AVDNet is validated\non VEDAI, DLR- 3K, DOTA, and the combined (VEDAI, DLR-3K, DOTA, and ABD) data\nset. Experimental results demonstrate the significant performance improvement\nof the proposed method over state-of-the-art detection techniques in terms of\nmAP, computation, and space complexity.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 12:42:55 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Mandal", "Murari", ""], ["Shah", "Manal", ""], ["Meena", "Prashant", ""], ["Devi", "Sanhita", ""], ["Vipparthi", "Santosh Kumar", ""]]}, {"id": "1907.07484", "submitter": "Robert Geirhos", "authors": "Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak,\n  Oliver Bringmann, Alexander S. Ecker, Matthias Bethge, Wieland Brendel", "title": "Benchmarking Robustness in Object Detection: Autonomous Driving when\n  Winter is Coming", "comments": "21 pages, 10 figures, 1 dragon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to detect objects regardless of image distortions or weather\nconditions is crucial for real-world applications of deep learning like\nautonomous driving. We here provide an easy-to-use benchmark to assess how\nobject detection models perform when image quality degrades. The three\nresulting benchmark datasets, termed Pascal-C, Coco-C and Cityscapes-C, contain\na large variety of image corruptions. We show that a range of standard object\ndetection models suffer a severe performance loss on corrupted images (down to\n30--60\\% of the original performance). However, a simple data augmentation\ntrick---stylizing the training images---leads to a substantial increase in\nrobustness across corruption type, severity and dataset. We envision our\ncomprehensive benchmark to track future progress towards building robust object\ndetection models. Benchmark, code and data are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 12:51:10 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 08:42:46 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Michaelis", "Claudio", ""], ["Mitzkus", "Benjamin", ""], ["Geirhos", "Robert", ""], ["Rusak", "Evgenia", ""], ["Bringmann", "Oliver", ""], ["Ecker", "Alexander S.", ""], ["Bethge", "Matthias", ""], ["Brendel", "Wieland", ""]]}, {"id": "1907.07485", "submitter": "Chenglong Li", "authors": "Chenglong Li, Andong Lu, Aihua Zheng, Zhengzheng Tu and Jin Tang", "title": "Multi-Adapter RGBT Tracking", "comments": "Code site: https://github.com/Alexadlu/MANet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of RGBT tracking aims to take the complementary advantages from\nvisible spectrum and thermal infrared data to achieve robust visual tracking,\nand receives more and more attention in recent years. Existing works focus on\nmodality-specific information integration by introducing modality weights to\nachieve adaptive fusion or learning robust feature representations of different\nmodalities. Although these methods could effectively deploy the\nmodality-specific properties, they ignore the potential values of\nmodality-shared cues as well as instance-aware information, which are crucial\nfor effective fusion of different modalities in RGBT tracking. In this paper,\nwe propose a novel Multi-Adapter convolutional Network (MANet) to jointly\nperform modality-shared, modality-specific and instance-aware feature learning\nin an end-to-end trained deep framework for RGBT tracking. We design three\nkinds of adapters within our network. In a specific, the generality adapter is\nto extract shared object representations, the modality adapter aims at encoding\nmodality-specific information to deploy their complementary advantages, and the\ninstance adapter is to model the appearance properties and temporal variations\nof a certain object. Moreover, to reduce computational complexity for real-time\ndemand of visual tracking, we design a parallel structure of generic adapter\nand modality adapter. Extensive experiments on two RGBT tracking benchmark\ndatasets demonstrate the outstanding performance of the proposed tracker\nagainst other state-of-the-art RGB and RGBT tracking algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 12:51:37 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Li", "Chenglong", ""], ["Lu", "Andong", ""], ["Zheng", "Aihua", ""], ["Tu", "Zhengzheng", ""], ["Tang", "Jin", ""]]}, {"id": "1907.07509", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI, SIGPH@iPRI)", "title": "A Link Between the Multiplicative and Additive Functional Asplund's\n  Metrics", "comments": null, "journal-ref": "14th International Symposium on Mathematical Morphology, Saarland\n  University, Jul 2019, Saarbr\\\"ucken, Germany. pp.41-53", "doi": "10.1007/978-3-030-20867-7_4", "report-no": null, "categories": "cs.CV math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Asplund's metrics were recently introduced to perform pattern\nmatching robust to lighting changes thanks to double-sided probing in the\nLogarithmic Image Processing (LIP) framework. Two metrics were defined, namely\nthe LIP-multiplicative Asplund's metric which is robust to variations of object\nthickness (or opacity) and the LIP-additive Asplund's metric which is robust to\nvariations of camera exposure-time (or light intensity). Maps of distances-i.e.\nmaps of these metric values-were also computed between a reference template and\nan image. Recently, it was proven that the map of LIP-multiplicative As-plund's\ndistances corresponds to mathematical morphology operations. In this paper, the\nlink between both metrics and between their corresponding distance maps will be\ndemonstrated. It will be shown that the map of LIP-additive Asplund's distances\nof an image can be computed from the map of the LIP-multiplicative Asplund's\ndistance of a transform of this image and vice-versa. Both maps will be related\nby the LIP isomorphism which will allow to pass from the image space of the\nLIP-additive distance map to the positive real function space of the\nLIP-multiplicative distance map. Experiments will illustrate this relation and\nthe robustness of the LIP-additive Asplund's metric to lighting changes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:32:58 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI, SIGPH@iPRI"]]}, {"id": "1907.07518", "submitter": "Antea Hadviger", "authors": "Antea Hadviger, Ivan Markovi\\'c, Ivan Petrovi\\'c", "title": "Stereo Event Lifetime and Disparity Estimation for Dynamic Vision\n  Sensors", "comments": "Accepted to European Conference on Mobile Robots (ECMR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras are biologically inspired sensors that output\nasynchronous pixel-wise brightness changes in the scene called events. They\nhave a high dynamic range and temporal resolution of a microsecond, opposed to\nstandard cameras that output frames at fixed frame rates and suffer from motion\nblur. Forming stereo pairs of such cameras can open novel application\npossibilities, since for each event depth can be readily estimated; however, to\nfully exploit asynchronous nature of the sensor and avoid fixed time interval\nevent accumulation, stereo event lifetime estimation should be employed. In\nthis paper, we propose a novel method for event lifetime estimation of stereo\nevent-cameras, allowing generation of sharp gradient images of events that\nserve as input to disparity estimation methods. Since a single brightness\nchange triggers events in both event-camera sensors, we propose a method for\nsingle shot event lifetime and disparity estimation, with association via\nstereo matching. The proposed method is approximately twice as fast and more\naccurate than if lifetimes were estimated separately for each sensor and then\nstereo matched. Results are validated on real-world data through multiple\nstereo event-camera experiments.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:46:22 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Hadviger", "Antea", ""], ["Markovi\u0107", "Ivan", ""], ["Petrovi\u0107", "Ivan", ""]]}, {"id": "1907.07570", "submitter": "Hongje Seong", "authors": "Hongje Seong, Junhyuk Hyun and Euntai Kim", "title": "FOSNet: An End-to-End Trainable Deep Neural Network for Scene\n  Recognition", "comments": "2019 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene recognition is an image recognition problem aimed at predicting the\ncategory of the place at which the image is taken. In this paper, a new scene\nrecognition method using the convolutional neural network (CNN) is proposed.\nThe proposed method is based on the fusion of the object and the scene\ninformation in the given image and the CNN framework is named as FOS (fusion of\nobject and scene) Net. In addition, a new loss named scene coherence loss (SCL)\nis developed to train the FOSNet and to improve the scene recognition\nperformance. The proposed SCL is based on the unique traits of the scene that\nthe 'sceneness' spreads and the scene class does not change all over the image.\nThe proposed FOSNet was experimented with three most popular scene recognition\ndatasets, and their state-of-the-art performance is obtained in two sets:\n60.14% on Places 2 and 90.37% on MIT indoor 67. The second highest performance\nof 77.28% is obtained on SUN 397.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:10:24 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 10:15:06 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Seong", "Hongje", ""], ["Hyun", "Junhyuk", ""], ["Kim", "Euntai", ""]]}, {"id": "1907.07573", "submitter": "Ankit Gupta", "authors": "Ankit Gupta, Elliott Ruebush", "title": "AquaSight: Automatic Water Impurity Detection Utilizing Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the United Nations World Water Assessment Programme, every day,\n2 million tons of sewage and industrial and agricultural waste are discharged\ninto the worlds water. In order to address this pervasive issue of increasing\nwater pollution, while ensuring that the global population has an efficient,\naccurate, and low cost method to assess whether the water they drink is\ncontaminated, we propose AquaSight, a novel mobile application that utilizes\ndeep learning methods, specifically Convolutional Neural Networks, for\nautomated water impurity detection. After comprehensive training with a dataset\nof 105 images representing varying magnitudes of contamination, the deep\nlearning algorithm achieved a 96 percent accuracy and loss of 0.108.\nFurthermore, the machine learning model uses efficient analysis of the\nturbidity and transparency levels of water to estimate a particular sample of\nwaters level of contamination. When deployed, the AquaSight system will provide\nan efficient way for individuals to secure an estimation of water quality,\nalerting local and national government to take action and potentially saving\nmillions of lives worldwide.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:17:21 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Gupta", "Ankit", ""], ["Ruebush", "Elliott", ""]]}, {"id": "1907.07581", "submitter": "Shuang Zhao", "authors": "Zixun Sun and Shuang Zhao and Chengwei Zhu and Xiao Chen", "title": "News Cover Assessment via Multi-task Learning", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online personalized news product needs a suitable cover for the article. The\nnews cover demands to be with high image quality, and draw readers' attention\nat same time, which is extraordinary challenging due to the subjectivity of the\ntask. In this paper, we assess the news cover from image clarity and object\nsalience perspective. We propose an end-to-end multi-task learning network for\nimage clarity assessment and semantic segmentation simultaneously, the results\nof which can be guided for news cover assessment. The proposed network is based\non a modified DeepLabv3+ model. The network backbone is used for multiple scale\nspatial features exaction, followed by two branches for image clarity\nassessment and semantic segmentation, respectively. The experiment results show\nthat the proposed model is able to capture important content in images and\nperforms better than single-task learning baselines on our proposed game\ncontent based CIA dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:24:01 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 02:26:58 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Sun", "Zixun", ""], ["Zhao", "Shuang", ""], ["Zhu", "Chengwei", ""], ["Chen", "Xiao", ""]]}, {"id": "1907.07585", "submitter": "O\\u{g}ul Can", "authors": "O\\u{g}ul Can, Yeti Ziya G\\\"urb\\\"uz and A. Ayd{\\i}n Alatan", "title": "Deep Metric Learning with Alternating Projections onto Feasible Sets", "comments": "10 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the training of networks for distance metric learning, minimizers of\nthe typical loss functions can be considered as \"feasible points\" satisfying a\nset of constraints imposed by the training data. To this end, we reformulate\ndistance metric learning problem as finding a feasible point of a constraint\nset where the embedding vectors of the training data satisfy desired\nintra-class and inter-class proximity. The feasible set induced by the\nconstraint set is expressed as the intersection of the relaxed feasible sets\nwhich enforce the proximity constraints only for particular samples (a sample\nfrom each class) of the training data. Then, the feasible point problem is to\nbe approximately solved by performing alternating projections onto those\nfeasible sets. Such an approach introduces a regularization term and results in\nminimizing a typical loss function with a systematic batch set construction\nwhere these batches are constrained to contain the same sample from each class\nfor a certain number of iterations. Moreover, these particular samples can be\nconsidered as the class representatives, allowing efficient utilization of hard\nclass mining during batch construction. The proposed technique is applied with\nthe well-accepted losses and evaluated on Stanford Online Products, CAR196 and\nCUB200-2011 datasets for image retrieval and clustering. Outperforming\nstate-of-the-art, the proposed approach consistently improves the performance\nof the integrated loss functions with no additional computational cost and\nboosts the performance further by hard negative class mining.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:29:19 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 13:42:00 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Can", "O\u011ful", ""], ["G\u00fcrb\u00fcz", "Yeti Ziya", ""], ["Alatan", "A. Ayd\u0131n", ""]]}, {"id": "1907.07613", "submitter": "Tianyu Yang", "authors": "Tianyu Yang and Antoni B. Chan", "title": "Visual Tracking via Dynamic Memory Networks", "comments": "accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), 2019. arXiv admin note: substantial text overlap with\n  arXiv:1803.07268", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template-matching methods for visual tracking have gained popularity recently\ndue to their good performance and fast speed. However, they lack effective ways\nto adapt to changes in the target object's appearance, making their tracking\naccuracy still far from state-of-the-art. In this paper, we propose a dynamic\nmemory network to adapt the template to the target's appearance variations\nduring tracking. The reading and writing process of the external memory is\ncontrolled by an LSTM network with the search feature map as input. A spatial\nattention mechanism is applied to concentrate the LSTM input on the potential\ntarget as the location of the target is at first unknown. To prevent aggressive\nmodel adaptivity, we apply gated residual template learning to control the\namount of retrieved memory that is used to combine with the initial template.\nIn order to alleviate the drift problem, we also design a \"negative\" memory\nunit that stores templates for distractors, which are used to cancel out wrong\nresponses from the object template. To further boost the tracking performance,\nan auxiliary classification loss is added after the feature extractor part.\nUnlike tracking-by-detection methods where the object's information is\nmaintained by the weight parameters of neural networks, which requires\nexpensive online fine-tuning to be adaptable, our tracker runs completely\nfeed-forward and adapts to the target's appearance changes by updating the\nexternal memory. Moreover, the capacity of our model is not determined by the\nnetwork size as with other trackers --- the capacity can be easily enlarged as\nthe memory requirements of a task increase, which is favorable for memorizing\nlong-term object information. Extensive experiments on the OTB and VOT datasets\ndemonstrate that our trackers perform favorably against state-of-the-art\ntracking methods while retaining real-time speed.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 14:07:24 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 08:25:01 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2019 06:21:40 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Yang", "Tianyu", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1907.07617", "submitter": "Sara Beery", "authors": "Sara Beery, Dan Morris, Pietro Perona", "title": "The iWildCam 2019 Challenge Dataset", "comments": "From the Sixth Fine-Grained Visual Categorization Workshop at CVPR19.\n  arXiv admin note: text overlap with arXiv:1904.05986", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera Traps (or Wild Cams) enable the automatic collection of large\nquantities of image data. Biologists all over the world use camera traps to\nmonitor biodiversity and population density of animal species. The computer\nvision community has been making strides towards automating the species\nclassification challenge in camera traps, but as we try to expand the scope of\nthese models from specific regions where we have collected training data to\ndifferent areas we are faced with an interesting problem: how do you classify a\nspecies in a new region that you may not have seen in previous training data?\n  In order to tackle this problem, we have prepared a dataset and challenge\nwhere the training data and test data are from different regions, namely The\nAmerican Southwest and the American Northwest. We use the Caltech Camera Traps\ndataset, collected from the American Southwest, as training data. We add a new\ndataset from the American Northwest, curated from data provided by the Idaho\nDepartment of Fish and Game (IDFG), as our test dataset. The test data has some\nclass overlap with the training data, some species are found in both datasets,\nbut there are both species seen during training that are not seen during test\nand vice versa. To help fill the gaps in the training species, we allow\ncompetitors to utilize transfer learning from two alternate domains:\nhuman-curated images from iNaturalist and synthetic images from Microsoft's\nTrapCam-AirSim simulation environment.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 21:57:00 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Beery", "Sara", ""], ["Morris", "Dan", ""], ["Perona", "Pietro", ""]]}, {"id": "1907.07640", "submitter": "Emin Orhan", "authors": "A. Emin Orhan", "title": "Robustness properties of Facebook's ResNeXt WSL models", "comments": "10 pages, 4 figures, 4 tables; v5 corrects the ImageNet-A results and\n  revises the discussion accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the robustness properties of ResNeXt class image recognition\nmodels trained with billion scale weakly supervised data (ResNeXt WSL models).\nThese models, recently made public by Facebook AI, were trained with ~1B images\nfrom Instagram and fine-tuned on ImageNet. We show that these models display an\nunprecedented degree of robustness against common image corruptions and\nperturbations, as measured by the ImageNet-C and ImageNet-P benchmarks. They\nalso achieve substantially improved accuracies on the recently introduced\n\"natural adversarial examples\" benchmark (ImageNet-A). The largest of the\nreleased models, in particular, achieves state-of-the-art results on\nImageNet-C, ImageNet-P, and ImageNet-A by a large margin. The gains on\nImageNet-C, ImageNet-P, and ImageNet-A far outpace the gains on ImageNet\nvalidation accuracy, suggesting the former as more useful benchmarks to measure\nfurther progress in image recognition. Remarkably, the ResNeXt WSL models even\nachieve a limited degree of adversarial robustness against state-of-the-art\nwhite-box attacks (10-step PGD attacks). However, in contrast to adversarially\ntrained models, the robustness of the ResNeXt WSL models rapidly declines with\nthe number of PGD steps, suggesting that these models do not achieve genuine\nadversarial robustness. Visualization of the learned features also confirms\nthis conclusion. Finally, we show that although the ResNeXt WSL models are more\nshape-biased than comparable ImageNet-trained models in a shape-texture cue\nconflict experiment, they still remain much more texture-biased than humans,\nsuggesting that they share some of the underlying characteristics of\nImageNet-trained models that make this benchmark challenging.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 17:03:52 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 17:59:19 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 15:52:53 GMT"}, {"version": "v4", "created": "Fri, 2 Aug 2019 16:30:13 GMT"}, {"version": "v5", "created": "Mon, 9 Dec 2019 16:28:47 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Orhan", "A. Emin", ""]]}, {"id": "1907.07660", "submitter": "Lynn Kaack", "authors": "Lynn H. Kaack and George H. Chen and M. Granger Morgan", "title": "Truck Traffic Monitoring with Satellite Images", "comments": "31 pages, 15 figures, to be published in ACM SIGCAS Conference on\n  Computing and Sustainable Societies (COMPASS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The road freight sector is responsible for a large and growing share of\ngreenhouse gas emissions, but reliable data on the amount of freight that is\nmoved on roads in many parts of the world are scarce. Many low- and\nmiddle-income countries have limited ground-based traffic monitoring and\nfreight surveying activities. In this proof of concept, we show that we can use\nan object detection network to count trucks in satellite images and predict\naverage annual daily truck traffic from those counts. We describe a complete\nmodel, test the uncertainty of the estimation, and discuss the transfer to\ndeveloping countries.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 17:45:40 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kaack", "Lynn H.", ""], ["Chen", "George H.", ""], ["Morgan", "M. Granger", ""]]}, {"id": "1907.07676", "submitter": "Guy Engelhard", "authors": "Evi Kopelowitz, Guy Engelhard", "title": "Lung Nodules Detection and Segmentation Using 3D Mask-RCNN", "comments": "MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/Hkxqw5ilcV", "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate assessment of Lung nodules is a time consuming and error prone\ningredient of the radiologist interpretation work. Automating 3D volume\ndetection and segmentation can improve workflow as well as patient care.\nPrevious works have focused either on detecting lung nodules from a full CT\nscan or on segmenting them from a small ROI. We adapt the state of the art\narchitecture for 2D object detection and segmentation, MaskRCNN, to handle 3D\nimages and employ it to detect and segment lung nodules from CT scans. We\nreport on competitive results for the lung nodule detection on LUNA16 data set.\nThe added value of our method is that in addition to lung nodule detection, our\nframework produces 3D segmentations of the detected nodules.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 09:51:11 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Kopelowitz", "Evi", ""], ["Engelhard", "Guy", ""]]}, {"id": "1907.07677", "submitter": "Fanhua Shang", "authors": "Hongying Liu, Xiongjie Shen, Fanhua Shang, Fei Wang", "title": "CU-Net: Cascaded U-Net with Loss Weighted Sampling for Brain Tumor\n  Segmentation", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel cascaded U-Net for brain tumor segmentation.\nInspired by the distinct hierarchical structure of brain tumor, we design a\ncascaded deep network framework, in which the whole tumor is segmented firstly\nand then the tumor internal substructures are further segmented. Considering\nthat the increase of the network depth brought by cascade structures leads to a\nloss of accurate localization information in deeper layers, we construct many\nskip connections to link features at the same resolution and transmit detailed\ninformation from shallow layers to the deeper layers. Then we present a loss\nweighted sampling (LWS) scheme to eliminate the issue of imbalanced data during\ntraining the network. Experimental results on BraTS 2017 data show that our\narchitecture framework outperforms the state-of-the-art segmentation\nalgorithms, especially in terms of segmentation sensitivity.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 10:16:04 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Liu", "Hongying", ""], ["Shen", "Xiongjie", ""], ["Shang", "Fanhua", ""], ["Wang", "Fei", ""]]}, {"id": "1907.07739", "submitter": "Heather Couture", "authors": "Heather D. Couture, Roland Kwitt, J.S. Marron, Melissa Troester,\n  Charles M. Perou, Marc Niethammer", "title": "Deep Multi-View Learning via Task-Optimal CCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical Correlation Analysis (CCA) is widely used for multimodal data\nanalysis and, more recently, for discriminative tasks such as multi-view\nlearning; however, it makes no use of class labels. Recent CCA methods have\nstarted to address this weakness but are limited in that they do not\nsimultaneously optimize the CCA projection for discrimination and the CCA\nprojection itself, or they are linear only. We address these deficiencies by\nsimultaneously optimizing a CCA-based and a task objective in an end-to-end\nmanner. Together, these two objectives learn a non-linear CCA projection to a\nshared latent space that is highly correlated and discriminative. Our method\nshows a significant improvement over previous state-of-the-art (including deep\nsupervised approaches) for cross-view classification, regularization with a\nsecond view, and semi-supervised learning on real data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 20:06:47 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Couture", "Heather D.", ""], ["Kwitt", "Roland", ""], ["Marron", "J. S.", ""], ["Troester", "Melissa", ""], ["Perou", "Charles M.", ""], ["Niethammer", "Marc", ""]]}, {"id": "1907.07745", "submitter": "Oscar Rahnama", "authors": "Oscar Rahnama, Tommaso Cavallari, Stuart Golodetz, Alessio Tonioni,\n  Thomas Joy, Luigi Di Stefano, Simon Walker, Philip H. S. Torr", "title": "Real-Time Highly Accurate Dense Depth on a Power Budget using an\n  FPGA-CPU Hybrid SoC", "comments": "6 pages, 7 figures, 2 tables, journal", "journal-ref": "IEEE Transactions on Circuits and Systems II: Express Briefs, vol.\n  66, no. 5, pp. 773-777, May 2019", "doi": "10.1109/TCSII.2019.2909169", "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining highly accurate depth from stereo images in real time has many\napplications across computer vision and robotics, but in some contexts, upper\nbounds on power consumption constrain the feasible hardware to embedded\nplatforms such as FPGAs. Whilst various stereo algorithms have been deployed on\nthese platforms, usually cut down to better match the embedded architecture,\ncertain key parts of the more advanced algorithms, e.g. those that rely on\nunpredictable access to memory or are highly iterative in nature, are difficult\nto deploy efficiently on FPGAs, and thus the depth quality that can be achieved\nis limited. In this paper, we leverage a FPGA-CPU chip to propose a novel,\nsophisticated, stereo approach that combines the best features of SGM and\nELAS-based methods to compute highly accurate dense depth in real time. Our\napproach achieves an 8.7% error rate on the challenging KITTI 2015 dataset at\nover 50 FPS, with a power consumption of only 5W.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 20:22:47 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Rahnama", "Oscar", ""], ["Cavallari", "Tommaso", ""], ["Golodetz", "Stuart", ""], ["Tonioni", "Alessio", ""], ["Joy", "Thomas", ""], ["Di Stefano", "Luigi", ""], ["Walker", "Simon", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1907.07748", "submitter": "Khaled Elmadawy Khaledelmadawi", "authors": "Khaled Elmadawi, Moemen Abdelrazek, Mohamed Elsobky, Hesham M. Eraqi,\n  and Mohamed Zahran", "title": "End-to-end sensor modeling for LiDAR Point Cloud", "comments": "Accepted in IEEE Intelligent Transportation Systems Conference - ITSC\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced sensors are a key to enable self-driving cars technology. Laser\nscanner sensors (LiDAR, Light Detection And Ranging) became a fundamental\nchoice due to its long-range and robustness to low light driving conditions.\nThe problem of designing a control software for self-driving cars is a complex\ntask to explicitly formulate in rule-based systems, thus recent approaches rely\non machine learning that can learn those rules from data. The major problem\nwith such approaches is that the amount of training data required for\ngeneralizing a machine learning model is big, and on the other hand LiDAR data\nannotation is very costly compared to other car sensors. An accurate LiDAR\nsensor model can cope with such problem. Moreover, its value goes beyond this\nbecause existing LiDAR development, validation, and evaluation platforms and\nprocesses are very costly, and virtual testing and development environments are\nstill immature in terms of physical properties representation. In this work we\npropose a novel Deep Learning-based LiDAR sensor model. This method models the\nsensor echos, using a Deep Neural Network to model echo pulse widths learned\nfrom real data using Polar Grid Maps (PGM). We benchmark our model performance\nagainst comprehensive real sensor data and very promising results are achieved\nthat sets a baseline for future works.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 20:34:14 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Elmadawi", "Khaled", ""], ["Abdelrazek", "Moemen", ""], ["Elsobky", "Mohamed", ""], ["Eraqi", "Hesham M.", ""], ["Zahran", "Mohamed", ""]]}, {"id": "1907.07783", "submitter": "Bernhard Egger", "authors": "Bernhard Egger, Markus D. Schirmer, Florian Dubost, Marco J. Nardin,\n  Natalia S. Rost, Polina Golland", "title": "Patient-specific Conditional Joint Models of Shape, Image Features and\n  Clinical Indicators", "comments": "Supplementary material: https://www.youtube.com/watch?v=gPoHP_iFQIA", "journal-ref": "MICCAI 2019, the 22nd International Conference on Medical Image\n  Computing and Computer Assisted Intervention, in Shenzhen, China", "doi": null, "report-no": null, "categories": "eess.IV cs.CG cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose and demonstrate a joint model of anatomical shapes, image features\nand clinical indicators for statistical shape modeling and medical image\nanalysis. The key idea is to employ a copula model to separate the joint\ndependency structure from the marginal distributions of variables of interest.\nThis separation provides flexibility on the assumptions made during the\nmodeling process. The proposed method can handle binary, discrete, ordinal and\ncontinuous variables. We demonstrate a simple and efficient way to include\nbinary, discrete and ordinal variables into the modeling. We build Bayesian\nconditional models based on observed partial clinical indicators, features or\nshape based on Gaussian processes capturing the dependency structure. We apply\nthe proposed method on a stroke dataset to jointly model the shape of the\nlateral ventricles, the spatial distribution of the white matter hyperintensity\nassociated with periventricular white matter disease, and clinical indicators.\nThe proposed method yields interpretable joint models for data exploration and\npatient-specific statistical shape models for medical image analysis.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 21:49:29 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Egger", "Bernhard", ""], ["Schirmer", "Markus D.", ""], ["Dubost", "Florian", ""], ["Nardin", "Marco J.", ""], ["Rost", "Natalia S.", ""], ["Golland", "Polina", ""]]}, {"id": "1907.07786", "submitter": "Alex Burnap", "authors": "Alex Burnap, John R. Hauser, Artem Timoshenko", "title": "Design and Evaluation of Product Aesthetics: A Human-Machine Hybrid\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aesthetics are critically important to market acceptance in many product\ncategories. In the automotive industry in particular, an improved aesthetic\ndesign can boost sales by 30% or more. Firms invest heavily in designing and\ntesting new product aesthetics. A single automotive \"theme clinic\" costs\nbetween \\$100,000 and \\$1,000,000, and hundreds are conducted annually. We use\nmachine learning to augment human judgment when designing and testing new\nproduct aesthetics. The model combines a probabilistic variational autoencoder\n(VAE) and adversarial components from generative adversarial networks (GAN),\nalong with modeling assumptions that address managerial requirements for firm\nadoption. We train our model with data from an automotive partner-7,000 images\nevaluated by targeted consumers and 180,000 high-quality unrated images. Our\nmodel predicts well the appeal of new aesthetic designs-38% improvement\nrelative to a baseline and substantial improvement over both conventional\nmachine learning models and pretrained deep learning models. New automotive\ndesigns are generated in a controllable manner for the design team to consider,\nwhich we also empirically verify are appealing to consumers. These results,\ncombining human and machine inputs for practical managerial usage, suggest that\nmachine learning offers significant opportunity to augment aesthetic design.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 21:56:55 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Burnap", "Alex", ""], ["Hauser", "John R.", ""], ["Timoshenko", "Artem", ""]]}, {"id": "1907.07792", "submitter": "Xin Li", "authors": "Xin Li, Xiaowen Ying, Mooi Choo Chuah", "title": "GRIP++: Enhanced Graph-based Interaction-aware Trajectory Prediction for\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advancement in the technology of autonomous driving cars, the\nsafety of a self-driving car is still a challenging problem that has not been\nwell studied. Motion prediction is one of the core functions of an autonomous\ndriving car. Previously, we propose a novel scheme called GRIP which is\ndesigned to predict trajectories for traffic agents around an autonomous car\nefficiently. GRIP uses a graph to represent the interactions of close objects,\napplies several graph convolutional blocks to extract features, and\nsubsequently uses an encoder-decoder long short-term memory (LSTM) model to\nmake predictions. Even though our experimental results show that GRIP improves\nthe prediction accuracy of the state-of-the-art solution by 30%, GRIP still has\nsome limitations. GRIP uses a fixed graph to describe the relationships between\ndifferent traffic agents and hence may suffer some performance degradations\nwhen it is being used in urban traffic scenarios. Hence, in this paper, we\ndescribe an improved scheme called GRIP++ where we use both fixed and dynamic\ngraphs for trajectory predictions of different types of traffic agents. Such an\nimprovement can help autonomous driving cars avoid many traffic accidents. Our\nevaluations using a recently released urban traffic dataset, namely ApolloScape\nshowed that GRIP++ achieves better prediction accuracy than state-of-the-art\nschemes. GRIP++ ranked #1 on the leaderboard of the ApolloScape trajectory\ncompetition in October 2019. In addition, GRIP++ runs 21.7 times faster than a\nstate-of-the-art scheme, CS-LSTM.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 22:10:16 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 23:27:53 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Li", "Xin", ""], ["Ying", "Xiaowen", ""], ["Chuah", "Mooi Choo", ""]]}, {"id": "1907.07804", "submitter": "Subhojeet Pramanik", "authors": "Subhojeet Pramanik, Priyanka Agrawal, Aman Hussain", "title": "OmniNet: A unified architecture for multi-modal multi-task learning", "comments": "Source code available at: https://github.com/subho406/OmniNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer is a popularly used neural network architecture, especially for\nlanguage understanding. We introduce an extended and unified architecture that\ncan be used for tasks involving a variety of modalities like image, text,\nvideos, etc. We propose a spatio-temporal cache mechanism that enables learning\nspatial dimension of the input in addition to the hidden states corresponding\nto the temporal input sequence. The proposed architecture further enables a\nsingle model to support tasks with multiple input modalities as well as\nasynchronous multi-task learning, thus we refer to it as OmniNet. For example,\na single instance of OmniNet can concurrently learn to perform the tasks of\npart-of-speech tagging, image captioning, visual question answering and video\nactivity recognition. We demonstrate that training these four tasks together\nresults in about three times compressed model while retaining the performance\nin comparison to training them individually. We also show that using this\nneural network pre-trained on some modalities assists in learning unseen tasks\nsuch as video captioning and video question answering. This illustrates the\ngeneralization capacity of the self-attention mechanism on the spatio-temporal\ncache present in OmniNet.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 22:59:56 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 09:59:06 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Pramanik", "Subhojeet", ""], ["Agrawal", "Priyanka", ""], ["Hussain", "Aman", ""]]}, {"id": "1907.07807", "submitter": "Usman Roshan", "authors": "Yunzhe Xue, Meiyan Xie, Fadi G. Farhat, Olga Boukrina, A. M. Barrett,\n  Jeffrey R. Binder, Usman W. Roshan, William W. Graves", "title": "A fully 3D multi-path convolutional neural network with feature fusion\n  and feature weighting for automatic lesion identification in brain MRI images", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully 3D multi-path convolutional network to predict stroke\nlesions from 3D brain MRI images. Our multi-path model has independent encoders\nfor different modalities containing residual convolutional blocks, weighted\nmulti-path feature fusion from different modalities, and weighted fusion\nmodules to combine encoder and decoder features. Compared to existing 3D CNNs\nlike DeepMedic, 3D U-Net, and AnatomyNet, our networks achieves the highest\nstatistically significant cross-validation accuracy of 60.5% on the large ATLAS\nbenchmark of 220 patients. We also test our model on multi-modal images from\nthe Kessler Foundation and Medical College Wisconsin and achieve a\nstatistically significant cross-validation accuracy of 65%, significantly\noutperforming the multi-modal 3D U-Net and DeepMedic. Overall our model offers\na principled, extensible multi-path approach that outperforms multi-channel\nalternatives and achieves high Dice accuracies on existing benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 23:21:42 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 18:30:20 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Xue", "Yunzhe", ""], ["Xie", "Meiyan", ""], ["Farhat", "Fadi G.", ""], ["Boukrina", "Olga", ""], ["Barrett", "A. M.", ""], ["Binder", "Jeffrey R.", ""], ["Roshan", "Usman W.", ""], ["Graves", "William W.", ""]]}, {"id": "1907.07816", "submitter": "Gabriel Maicas", "authors": "Gabriel Maicas, Cuong Nguyen, Farbod Motlagh, Jacinto C. Nascimento,\n  Gustavo Carneiro", "title": "Unsupervised Task Design to Meta-Train Medical Image Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-training has been empirically demonstrated to be the most effective\npre-training method for few-shot learning of medical image classifiers (i.e.,\nclassifiers modeled with small training sets). However, the effectiveness of\nmeta-training relies on the availability of a reasonable number of\nhand-designed classification tasks, which are costly to obtain, and\nconsequently rarely available. In this paper, we propose a new method to\nunsupervisedly design a large number of classification tasks to meta-train\nmedical image classifiers. We evaluate our method on a breast dynamically\ncontrast enhanced magnetic resonance imaging (DCE-MRI) data set that has been\nused to benchmark few-shot training methods of medical image classifiers. Our\nresults show that the proposed unsupervised task design to meta-train medical\nimage classifiers builds a pre-trained model that, after fine-tuning, produces\nbetter classification results than other unsupervised and supervised\npre-training methods, and competitive results with respect to meta-training\nthat relies on hand-designed classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 23:51:24 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Maicas", "Gabriel", ""], ["Nguyen", "Cuong", ""], ["Motlagh", "Farbod", ""], ["Nascimento", "Jacinto C.", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1907.07835", "submitter": "Peixiang Zhong", "authors": "Peixiang Zhong, Di Wang, and Chunyan Miao", "title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) measures the neuronal activities in different\nbrain regions via electrodes. Many existing studies on EEG-based emotion\nrecognition do not fully exploit the topology of EEG channels. In this paper,\nwe propose a regularized graph neural network (RGNN) for EEG-based emotion\nrecognition. RGNN considers the biological topology among different brain\nregions to capture both local and global relations among different EEG\nchannels. Specifically, we model the inter-channel relations in EEG signals via\nan adjacency matrix in a graph neural network where the connection and\nsparseness of the adjacency matrix are inspired by neuroscience theories of\nhuman brain organization. In addition, we propose two regularizers, namely\nnode-wise domain adversarial training (NodeDAT) and emotion-aware distribution\nlearning (EmotionDL), to better handle cross-subject EEG variations and noisy\nlabels, respectively. Extensive experiments on two public datasets, SEED and\nSEED-IV, demonstrate the superior performance of our model than\nstate-of-the-art models in most experimental settings. Moreover, ablation\nstudies show that the proposed adjacency matrix and two regularizers contribute\nconsistent and significant gain to the performance of our RGNN model. Finally,\ninvestigations on the neuronal activities reveal important brain regions and\ninter-channel relations for EEG-based emotion recognition.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 01:44:44 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 10:57:39 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 07:02:59 GMT"}, {"version": "v4", "created": "Wed, 13 May 2020 03:19:26 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Zhong", "Peixiang", ""], ["Wang", "Di", ""], ["Miao", "Chunyan", ""]]}, {"id": "1907.07844", "submitter": "Yu-Xiong Wang", "authors": "Yu-Xiong Wang, Deva Ramanan, Martial Hebert", "title": "Growing a Brain: Fine-Tuning by Increasing Model Capacity", "comments": "CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs have made an undeniable impact on computer vision through the ability to\nlearn high-capacity models with large annotated training sets. One of their\nremarkable properties is the ability to transfer knowledge from a large source\ndataset to a (typically smaller) target dataset. This is usually accomplished\nthrough fine-tuning a fixed-size network on new target data. Indeed, virtually\nevery contemporary visual recognition system makes use of fine-tuning to\ntransfer knowledge from ImageNet. In this work, we analyze what components and\nparameters change during fine-tuning, and discover that increasing model\ncapacity allows for more natural model adaptation through fine-tuning. By\nmaking an analogy to developmental learning, we demonstrate that \"growing\" a\nCNN with additional units, either by widening existing layers or deepening the\noverall network, significantly outperforms classic fine-tuning approaches. But\nin order to properly grow a network, we show that newly-added units must be\nappropriately normalized to allow for a pace of learning that is consistent\nwith existing units. We empirically validate our approach on several benchmark\ndatasets, producing state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 02:20:18 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Wang", "Yu-Xiong", ""], ["Ramanan", "Deva", ""], ["Hebert", "Martial", ""]]}, {"id": "1907.07853", "submitter": "Saeed Afshar", "authors": "Saeed Afshar, Ying Xu, Jonathan Tapson, Andr\\'e van Schaik, Gregory\n  Cohen", "title": "Event-based Feature Extraction Using Adaptive Selection Thresholds", "comments": "15 Pages. 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised feature extraction algorithms form one of the most important\nbuilding blocks in machine learning systems. These algorithms are often adapted\nto the event-based domain to perform online learning in neuromorphic hardware.\nHowever, not designed for the purpose, such algorithms typically require\nsignificant simplification during implementation to meet hardware constraints,\ncreating trade offs with performance. Furthermore, conventional feature\nextraction algorithms are not designed to generate useful intermediary signals\nwhich are valuable only in the context of neuromorphic hardware limitations. In\nthis work a novel event-based feature extraction method is proposed that\nfocuses on these issues. The algorithm operates via simple adaptive selection\nthresholds which allow a simpler implementation of network homeostasis than\nprevious works by trading off a small amount of information loss in the form of\nmissed events that fall outside the selection thresholds. The behavior of the\nselection thresholds and the output of the network as a whole are shown to\nprovide uniquely useful signals indicating network weight convergence without\nthe need to access network weights. A novel heuristic method for network size\nselection is proposed which makes use of noise events and their feature\nrepresentations. The use of selection thresholds is shown to produce network\nactivation patterns that predict classification accuracy allowing rapid\nevaluation and optimization of system parameters without the need to run\nback-end classifiers. The feature extraction method is tested on both the\nN-MNIST benchmarking dataset and a dataset of airplanes passing through the\nfield of view. Multiple configurations with different classifiers are tested\nwith the results quantifying the resultant performance gains at each processing\nstage.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 03:15:09 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 04:41:30 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Afshar", "Saeed", ""], ["Xu", "Ying", ""], ["Tapson", "Jonathan", ""], ["van Schaik", "Andr\u00e9", ""], ["Cohen", "Gregory", ""]]}, {"id": "1907.07854", "submitter": "Xiao Chen", "authors": "Wentao Yao, Zixun Sun, Xiao Chen", "title": "Understanding Video Content: Efficient Hero Detection and Recognition\n  for the Game \"Honor of Kings\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to understand content and automatically extract labels for videos of\nthe game \"Honor of Kings\", it is necessary to detect and recognize characters\n(called \"hero\") together with their camps in the game video. In this paper, we\npropose an efficient two-stage algorithm to detect and recognize heros in game\nvideos. First, we detect all heros in a video frame based on blood bar\ntemplate-matching method, and classify them according to their camps (self/\nfriend/ enemy). Then we recognize the name of each hero using one or more deep\nconvolution neural networks. Our method needs almost no work for labelling\ntraining and testing samples in the recognition stage. Experiments show its\nefficiency and accuracy in the task of hero detection and recognition in game\nvideos.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 03:23:53 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Yao", "Wentao", ""], ["Sun", "Zixun", ""], ["Chen", "Xiao", ""]]}, {"id": "1907.07863", "submitter": "Saeed Anwar", "authors": "Saeed Anwar, Chongyi Li", "title": "Diving Deeper into Underwater Image Enhancement: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The powerful representation capacity of deep learning has made it inevitable\nfor the underwater image enhancement community to employ its potential. The\nexploration of deep underwater image enhancement networks is increasing over\ntime, and hence; a comprehensive survey is the need of the hour. In this paper,\nour main aim is two-fold, 1): to provide a comprehensive and in-depth survey of\nthe deep learning-based underwater image enhancement, which covers various\nperspectives ranging from algorithms to open issues, and 2): to conduct a\nqualitative and quantitative comparison of the deep algorithms on diverse\ndatasets to serve as a benchmark, which has been barely explored before. To be\nspecific, we first introduce the underwater image formation models, which are\nthe base of training data synthesis and design of deep networks, and also\nhelpful for understanding the process of underwater image degradation. Then, we\nreview deep underwater image enhancement algorithms, and a glimpse of some of\nthe aspects of the current networks is presented including network\narchitecture, network parameters, training data, loss function, and training\nconfigurations. We also summarize the evaluation metrics and underwater image\ndatasets. Following that, a systematically experimental comparison is carried\nout to analyze the robustness and effectiveness of deep algorithms. Meanwhile,\nwe point out the shortcomings of current benchmark datasets and evaluation\nmetrics. Finally, we discuss several unsolved open issues and suggest possible\nresearch directions. We hope that all efforts done in this paper might serve as\na comprehensive reference for future research and call for the development of\ndeep learning-based underwater image enhancement.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 06:45:25 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Anwar", "Saeed", ""], ["Li", "Chongyi", ""]]}, {"id": "1907.07877", "submitter": "Harish Mulchandani", "authors": "Dhananjay Nahata, Harish Kumar Mulchandani, Suraj Bansal, G Muthukumar", "title": "Post-Earthquake Assessment of Buildings Using Deep Learning", "comments": "8 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification of the extent of damage suffered by a building in a seismic\nevent is crucial from the safety perspective and repairing work. In this study,\nauthors have proposed a CNN based autonomous damage detection model. Over 1200\nimages of different types of buildings-1000 for training and 200 for testing\nclassified into 4 categories according to the extent of damage suffered.\nCategories are namely, no damage, minor damage, major damage, and collapse.\nTrained network tested by the application of various algorithms with different\nlearning rates. The most optimum results were obtained on the application of\nVGG16 transfer learning model with a learning rate of 1e-5 as it gave a\ntraining accuracy of 97.85% and validation accuracy of up to 89.38%. The model\ndeveloped has real-time application in the event of an earthquake.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 05:22:25 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Nahata", "Dhananjay", ""], ["Mulchandani", "Harish Kumar", ""], ["Bansal", "Suraj", ""], ["Muthukumar", "G", ""]]}, {"id": "1907.07880", "submitter": "Zhipeng Zhou", "authors": "Zhipeng Zhou, Rui Zhang, Dong Yin", "title": "A Strong Feature Representation for Siamese Network Tracker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking has important application in assistive technologies for\npersonalized monitoring. Recent trackers choosing AlexNet as their backbone to\nextract features have gained great success. However, AlexNet is too shallow to\nform a strong feature representation, the tracker based on the Siamese network\nhave an accuracy gap compared with state-of-the-art algorithms. To solve this\nproblem, this paper proposes a tracker called SiamPF. Firstly, the modified\npre-trained VGG16 network is fine-tuned as the backbone. Secondly, an\nAlexNet-like branch is added after the third convolutional layer and merged\nwith the response map of the backbone network to form a preliminary strong\nfeature representation. And then, a channel attention block is designed to\nadaptively select the contribution features. Finally, the APCE is modified to\nprocess the response map to reduce interference and focus the tracker on the\ntarget. Our SiamPF only used ILSVRC2015-VID for training, but it achieved\nexcellent performance on OTB-2013 / OTB-2015 / VOT2015 / VOT2017, while\nmaintaining the real-time performance of 41FPS on the GTX 1080Ti.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 05:26:08 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Zhou", "Zhipeng", ""], ["Zhang", "Rui", ""], ["Yin", "Dong", ""]]}, {"id": "1907.07890", "submitter": "Julia Schulte", "authors": "Julia Schulte, Daniel Staps, Alexander Lampe", "title": "A feasibility study of deep neural networks for the recognition of\n  banknotes regarding central bank requirements", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contains a feasibility study of deep neural networks for the\nclassification of Euro banknotes with respect to requirements of central banks\non the ATM and high speed sorting industry. Instead of concentrating on the\naccuracy for a large number of classes as in the famous ImageNet Challenge we\nfocus thus on conditions with few classes and the requirement of rejection of\nimages belonging clearly to neither of the trained classes (i.e. classification\nin a so-called 0-class). These special requirements are part of frameworks\ndefined by central banks as the European Central Bank and are met by current\nATMs and high speed sorting machines. We also consider training and\nclassification time on state of the art GPU hardware. The study concentrates on\nthe banknote recognition whereas banknote class dependent authenticity and\nfitness checks are a topic of its own which is not considered in this work.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 06:29:31 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 08:21:15 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Schulte", "Julia", ""], ["Staps", "Daniel", ""], ["Lampe", "Alexander", ""]]}, {"id": "1907.07899", "submitter": "Yueming Jin", "authors": "Yueming Jin, Keyun Cheng, Qi Dou, Pheng-Ann Heng", "title": "Incorporating Temporal Prior from Motion Flow for Instrument\n  Segmentation in Minimally Invasive Surgery Video", "comments": "Accepted by MICCAI 2019; Code is available in\n  https://github.com/keyuncheng/MF-TAPNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic instrument segmentation in video is an essentially fundamental yet\nchallenging problem for robot-assisted minimally invasive surgery. In this\npaper, we propose a novel framework to leverage instrument motion information,\nby incorporating a derived temporal prior to an attention pyramid network for\naccurate segmentation. Our inferred prior can provide reliable indication of\nthe instrument location and shape, which is propagated from the previous frame\nto the current frame according to inter-frame motion flow. This prior is\ninjected to the middle of an encoder-decoder segmentation network as an\ninitialization of a pyramid of attention modules, to explicitly guide\nsegmentation output from coarse to fine. In this way, the temporal dynamics and\nthe attention network can effectively complement and benefit each other. As\nadditional usage, our temporal prior enables semi-supervised learning with\nperiodically unlabeled video frames, simply by reverse execution. We\nextensively validate our method on the public 2017 MICCAI EndoVis Robotic\nInstrument Segmentation Challenge dataset with three different tasks. Our\nmethod consistently exceeds the state-of-the-art results across all three tasks\nby a large margin. Our semi-supervised variant also demonstrates a promising\npotential for reducing annotation cost in the clinical practice.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 06:51:05 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Jin", "Yueming", ""], ["Cheng", "Keyun", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1907.07901", "submitter": "Jacob Spoelstra", "authors": "Tingting Zhao, Hang Zhang, Jacob Spoelstra", "title": "A Computer Vision Application for Assessing Facial Acne Severity from\n  Selfie Images", "comments": "4 pages, 6 figures. To be presented at the 2019 KDD workshop on\n  Applied data science in Healthcare: bridging the gap between data and\n  knowledge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We worked with Nestle SHIELD (Skin Health, Innovation, Education, and\nLongevity Development, NSH) to develop a deep learning model that is able to\nassess acne severity from selfie images as accurate as dermatologists. The\nmodel was deployed as a mobile application, providing patients an easy way to\nassess and track the progress of their acne treatment. NSH acquired 4,700\nselfie images for this study and recruited 11 internal dermatologists to label\nthem in five categories: 1-Clear, 2- Almost Clear, 3-Mild, 4-Moderate,\n5-Severe. Using OpenCV to detect facial landmarks we cut specific skin patches\nfrom the selfie images in order to minimize irrelevant background. We then\napplied a transfer learning approach by extracting features from the patches\nusing a ResNet 152 pre-trained model, followed by a fully connected layer\ntrained to approximate the desired severity rating. To address the problem of\nspatial sensitivity of CNN models, we introduce a new image rolling data\naugmentation approach, effectively causing acne lesions appeared in more\nlocations in the training images. Our results demonstrate that this approach\nimproved the generalization of the CNN model, outperforming more than half of\nthe panel of human dermatologists on test images. To our knowledge, this is the\nfirst deep learning-based solution for acne assessment using selfie images.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 06:51:33 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 06:24:24 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 16:34:46 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Zhao", "Tingting", ""], ["Zhang", "Hang", ""], ["Spoelstra", "Jacob", ""]]}, {"id": "1907.07911", "submitter": "Yanyan Fang", "authors": "Yanyan Fang, Biyun Zhan, Wandi Cai, Shenghua Gao, Bo Hu", "title": "Locality-constrained Spatial Transformer Network for Video Crowd\n  Counting", "comments": "Accepted by ICME2019(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with single image based crowd counting, video provides the\nspatial-temporal information of the crowd that would help improve the\nrobustness of crowd counting. But translation, rotation and scaling of people\nlead to the change of density map of heads between neighbouring frames.\nMeanwhile, people walking in/out or being occluded in dynamic scenes leads to\nthe change of head counts. To alleviate these issues in video crowd counting, a\nLocality-constrained Spatial Transformer Network (LSTN) is proposed.\nSpecifically, we first leverage a Convolutional Neural Networks to estimate the\ndensity map for each frame. Then to relate the density maps between\nneighbouring frames, a Locality-constrained Spatial Transformer (LST) module is\nintroduced to estimate the density map of next frame with that of current\nframe. To facilitate the performance evaluation, a large-scale video crowd\ncounting dataset is collected, which contains 15K frames with about 394K\nannotated heads captured from 13 different scenes. As far as we know, it is the\nlargest video crowd counting dataset. Extensive experiments on our dataset and\nother crowd counting datasets validate the effectiveness of our LSTN for crowd\ncounting.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 07:25:26 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Fang", "Yanyan", ""], ["Zhan", "Biyun", ""], ["Cai", "Wandi", ""], ["Gao", "Shenghua", ""], ["Hu", "Bo", ""]]}, {"id": "1907.07945", "submitter": "Yang Song", "authors": "Yang Song and Chenlin Meng and Stefano Ermon", "title": "MintNet: Building Invertible Neural Networks with Masked Convolutions", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new way of constructing invertible neural networks by combining\nsimple building blocks with a novel set of composition rules. This leads to a\nrich set of invertible architectures, including those similar to ResNets.\nInversion is achieved with a locally convergent iterative procedure that is\nparallelizable and very fast in practice. Additionally, the determinant of the\nJacobian can be computed analytically and efficiently, enabling their\ngenerative use as flow models. To demonstrate their flexibility, we show that\nour invertible neural networks are competitive with ResNets on MNIST and\nCIFAR-10 classification. When trained as generative models, our invertible\nnetworks achieve competitive likelihoods on MNIST, CIFAR-10 and ImageNet 32x32,\nwith bits per dimension of 0.98, 3.32 and 4.06 respectively.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 09:24:55 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 07:20:45 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Song", "Yang", ""], ["Meng", "Chenlin", ""], ["Ermon", "Stefano", ""]]}, {"id": "1907.07951", "submitter": "Mohammad Eslami", "authors": "Mohammad Eslami, Christiane Neuschaefer-Rube, Antoine Serrurier", "title": "Automatic vocal tract landmark localization from midsagittal MRI data", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-020-58103-6", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The various speech sounds of a language are obtained by varying the shape and\nposition of the articulators surrounding the vocal tract. Analyzing their\nvariations is crucial for understanding speech production, diagnosing speech\ndisorders and planning therapy. Identifying key anatomical landmarks of these\nstructures on medical images is a pre-requisite for any quantitative analysis\nand the rising amount of data generated in the field calls for an automatic\nsolution. The challenge lies in the high inter- and intra-speaker variability,\nthe mutual interaction between the articulators and the moderate quality of the\nimages. This study addresses this issue for the first time and tackles it by\nmeans by means of Deep Learning. It proposes a dedicated network architecture\nnamed Flat-net and its performance are evaluated and compared with eleven\nstate-of-the-art methods from the literature. The dataset contains midsagittal\nanatomical Magnetic Resonance Images for 9 speakers sustaining 62 articulations\nwith 21 annotated anatomical landmarks per image. Results show that the\nFlat-net approach outperforms the former methods, leading to an overall Root\nMean Square Error of 3.6 pixels/0.36 cm obtained in a leave-one-out procedure\nover the speakers. The implementation codes are also shared publicly on GitHub.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 09:38:09 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 16:37:46 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Eslami", "Mohammad", ""], ["Neuschaefer-Rube", "Christiane", ""], ["Serrurier", "Antoine", ""]]}, {"id": "1907.07980", "submitter": "Wouter Bulten", "authors": "Wouter Bulten, Hans Pinckaers, Hester van Boven, Robert Vink, Thomas\n  de Bel, Bram van Ginneken, Jeroen van der Laak, Christina Hulsbergen-van de\n  Kaa, Geert Litjens", "title": "Automated Gleason Grading of Prostate Biopsies using Deep Learning", "comments": "13 pages, 6 figures", "journal-ref": "The Lancet Oncology, Available online 8 January 2020", "doi": "10.1016/S1470-2045(19)30739-9", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gleason score is the most important prognostic marker for prostate cancer\npatients but suffers from significant inter-observer variability. We developed\na fully automated deep learning system to grade prostate biopsies. The system\nwas developed using 5834 biopsies from 1243 patients. A semi-automatic labeling\ntechnique was used to circumvent the need for full manual annotation by\npathologists. The developed system achieved a high agreement with the reference\nstandard. In a separate observer experiment, the deep learning system\noutperformed 10 out of 15 pathologists. The system has the potential to improve\nprostate cancer prognostics by acting as a first or second reader.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 10:47:26 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Bulten", "Wouter", ""], ["Pinckaers", "Hans", ""], ["van Boven", "Hester", ""], ["Vink", "Robert", ""], ["de Bel", "Thomas", ""], ["van Ginneken", "Bram", ""], ["van der Laak", "Jeroen", ""], ["de Kaa", "Christina Hulsbergen-van", ""], ["Litjens", "Geert", ""]]}, {"id": "1907.08009", "submitter": "Neslihan Kose Cihangir", "authors": "Neslihan Kose, Okan Kopuklu, Alexander Unnervik, Gerhard Rigoll", "title": "Real-Time Driver State Monitoring Using a CNN Based Spatio-Temporal\n  Approach", "comments": "Accepted for publication by the IEEE Intelligent Transportation\n  Systems Conference (ITSC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many road accidents occur due to distracted drivers. Today, driver monitoring\nis essential even for the latest autonomous vehicles to alert distracted\ndrivers in order to take over control of the vehicle in case of emergency. In\nthis paper, a spatio-temporal approach is applied to classify drivers'\ndistraction level and movement decisions using convolutional neural networks\n(CNNs). We approach this problem as action recognition to benefit from temporal\ninformation in addition to spatial information. Our approach relies on features\nextracted from sparsely selected frames of an action using a pre-trained\nBN-Inception network. Experiments show that our approach outperforms the\nstate-of-the art results on the Distracted Driver Dataset (96.31%), with an\naccuracy of 99.10% for 10-class classification while providing real-time\nperformance. We also analyzed the impact of fusion using RGB and optical flow\nmodalities with a very recent data level fusion strategy. The results on the\nDistracted Driver and Brain4Cars datasets show that fusion of these modalities\nfurther increases the accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 12:03:12 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Kose", "Neslihan", ""], ["Kopuklu", "Okan", ""], ["Unnervik", "Alexander", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1907.08020", "submitter": "Aleksei Tiulpin", "authors": "Aleksei Tiulpin and Simo Saarakkala", "title": "Automatic Grading of Individual Knee Osteoarthritis Features in Plain\n  Radiographs using Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knee osteoarthritis (OA) is the most common musculoskeletal disease in the\nworld. In primary healthcare, knee OA is diagnosed using clinical examination\nand radiographic assessment. Osteoarthritis Research Society International\n(OARSI) atlas of OA radiographic features allows to perform independent\nassessment of knee osteophytes, joint space narrowing and other knee features.\nThis provides a fine-grained OA severity assessment of the knee, compared to\nthe gold standard and most commonly used Kellgren-Lawrence (KL) composite\nscore. However, both OARSI and KL grading systems suffer from moderate\ninter-rater agreement, and therefore, the use of computer-aided methods could\nhelp to improve the reliability of the process. In this study, we developed a\nrobust, automatic method to simultaneously predict KL and OARSI grades in knee\nradiographs. Our method is based on Deep Learning and leverages an ensemble of\ndeep residual networks with 50 layers, squeeze-excitation and ResNeXt blocks.\nHere, we used transfer learning from ImageNet with a fine-tuning on the whole\nOsteoarthritis Initiative (OAI) dataset. An independent testing of our model\nwas performed on the whole Multicenter Osteoarthritis Study (MOST) dataset. Our\nmulti-task method yielded Cohen's kappa coefficients of 0.82 for KL-grade and\n0.79, 0.84, 0.94, 0.83, 0.84, 0.90 for femoral osteophytes, tibial osteophytes\nand joint space narrowing for lateral and medial compartments respectively.\nFurthermore, our method yielded area under the ROC curve of 0.98 and average\nprecision of 0.98 for detecting the presence of radiographic OA (KL $\\geq 2$),\nwhich is better than the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 12:52:32 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tiulpin", "Aleksei", ""], ["Saarakkala", "Simo", ""]]}, {"id": "1907.08051", "submitter": "Isinsu Katircioglu", "authors": "Isinsu Katircioglu, Helge Rhodin, Victor Constantin, J\\\"org Sp\\\"orri,\n  Mathieu Salzmann and Pascal Fua", "title": "Self-supervised Training of Proposal-based Segmentation via Background\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While supervised object detection methods achieve impressive accuracy, they\ngeneralize poorly to images whose appearance significantly differs from the\ndata they have been trained on. To address this in scenarios where annotating\ndata is prohibitively expensive, we introduce a self-supervised approach to\nobject detection and segmentation, able to work with monocular images captured\nwith a moving camera. At the heart of our approach lies the observation that\nsegmentation and background reconstruction are linked tasks, and the idea that,\nbecause we observe a structured scene, background regions can be re-synthesized\nfrom their surroundings, whereas regions depicting the object cannot. We\ntherefore encode this intuition as a self-supervised loss function that we\nexploit to train a proposal-based segmentation network. To account for the\ndiscrete nature of object proposals, we develop a Monte Carlo-based training\nstrategy that allows us to explore the large space of object proposals. Our\nexperiments demonstrate that our approach yields accurate detections and\nsegmentations in images that visually depart from those of standard benchmarks,\noutperforming existing self-supervised methods and approaching weakly\nsupervised ones that exploit large annotated datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 13:52:06 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Katircioglu", "Isinsu", ""], ["Rhodin", "Helge", ""], ["Constantin", "Victor", ""], ["Sp\u00f6rri", "J\u00f6rg", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1907.08068", "submitter": "Giulia Orr\\`u", "authors": "Giulia Orr\\`u, Pierluigi Tuveri, Luca Ghiani, Gian Luca Marcialis", "title": "Analysis of \"User-Specific Effect\" and Impact of Operator Skills on\n  Fingerprint PAD Systems", "comments": "Preprint version of a paper accepted at BioFor 2019", "journal-ref": "New Trends in Image Analysis and Processing - ICIAP 2019. Lecture\n  Notes in Computer Science, vol 11808. Springer, Cham", "doi": "10.1007/978-3-030-30754-7_6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint Liveness detection, or presentation attacks detection (PAD), that\nis, the ability of detecting if a fingerprint submitted to an electronic\ncapture device is authentic or made up of some artificial materials, boosted\nthe attention of the scientific community and recently machine learning\napproaches based on deep networks opened novel scenarios. A significant step\nahead was due thanks to the public availability of large sets of data; in\nparticular, the ones released during the International Fingerprint Liveness\nDetection Competition (LivDet). Among others, the fifth edition carried on in\n2017, challenged the participants in two more challenges which were not\ndetailed in the official report. In this paper, we want to extend that report\nby focusing on them: the first one was aimed at exploring the case in which the\nPAD is integrated into a fingerprint verification systems, where templates of\nusers are available too and the designer is not constrained to refer only to a\ngeneric users population for the PAD settings. The second one faces with the\nexploitation ability of attackers of the provided fakes, and how this ability\nimpacts on the final performance. These two challenges together may set at\nwhich extent the fingerprint presentation attacks are an actual threat and how\nto exploit additional information to make the PAD more effective.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:19:06 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Orr\u00f9", "Giulia", ""], ["Tuveri", "Pierluigi", ""], ["Ghiani", "Luca", ""], ["Marcialis", "Gian Luca", ""]]}, {"id": "1907.08069", "submitter": "Yuan Cao", "authors": "Yuan Cao, Qiuying Li, Hongming Shan, Zhizhong Huang, Lei Chen, Leiming\n  Ma, Junping Zhang", "title": "Precipitation Nowcasting with Star-Bridge Networks", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Precipitation nowcasting, which aims to precisely predict the short-term\nrainfall intensity of a local region, is gaining increasing attention in the\nartificial intelligence community. Existing deep learning-based algorithms use\na single network to process various rainfall intensities together, compromising\nthe predictive accuracy. Therefore, this paper proposes a novel recurrent\nneural network (RNN) based star-bridge network (StarBriNet) for precipitation\nnowcasting. The novelty of this work lies in the following three aspects.\nFirst, the proposed network comprises multiple sub-networks to deal with\ndifferent rainfall intensities and duration separately, which can significantly\nimprove the model performance. Second, we propose a star-shaped information\nbridge to enhance the information flow across RNN layers. Third, we introduce a\nmulti-sigmoid loss function to take the precipitation nowcasting criterion into\naccount. Experimental results demonstrate superior performance for\nprecipitation nowcasting over existing algorithms, including the\nstate-of-the-art one, on a natural radar echo dataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:19:28 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 05:06:50 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Cao", "Yuan", ""], ["Li", "Qiuying", ""], ["Shan", "Hongming", ""], ["Huang", "Zhizhong", ""], ["Chen", "Lei", ""], ["Ma", "Leiming", ""], ["Zhang", "Junping", ""]]}, {"id": "1907.08070", "submitter": "Ying Shi", "authors": "Ying Shi, Wei Wei, and Zhiming Zheng", "title": "Discriminative Embedding Autoencoder with a Regressor Feedback for\n  Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize the novel object categories using\nthe semantic representation of categories, and the key idea is to explore the\nknowledge of how the novel class is semantically related to the familiar\nclasses. Some typical models are to learn the proper embedding between the\nimage feature space and the semantic space, whilst it is important to learn\ndiscriminative features and comprise the coarse-to-fine image feature and\nsemantic information. In this paper, we propose a discriminative embedding\nautoencoder with a regressor feedback model for ZSL. The encoder learns a\nmapping from the image feature space to the discriminative embedding space,\nwhich regulates both inter-class and intra-class distances between the learned\nfeatures by a margin, making the learned features be discriminative for object\nrecognition. The regressor feedback learns to map the reconstructed samples\nback to the the discriminative embedding and the semantic embedding, assisting\nthe decoder to improve the quality of the samples and provide a generalization\nto the unseen classes. The proposed model is validated extensively on four\nbenchmark datasets: SUN, CUB, AWA1, AWA2, the experiment results show that our\nproposed model outperforms the state-of-the-art models, and especially in the\ngeneralized zero-shot learning (GZSL), significant improvements are achieved.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:19:49 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Shi", "Ying", ""], ["Wei", "Wei", ""], ["Zheng", "Zhiming", ""]]}, {"id": "1907.08088", "submitter": "Claudio Zito", "authors": "Brice Denoun, Beatriz Leon, Claudio Zito, Rustam Stolkin, Lorenzo\n  Jamone and Miles Hansard", "title": "Robust and fast generation of top and side grasps for unknown objects", "comments": "Extended abstract", "journal-ref": "Workshop on Task-Informed Grasping (TIG-II): From Perception to\n  Physical Interaction, Robotics: Science and Systems (RSS), 2019", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this work, we present a geometry-based grasping algorithm that is capable\nof efficiently generating both top and side grasps for unknown objects, using a\nsingle view RGB-D camera, and of selecting the most promising one. We\ndemonstrate the effectiveness of our approach on a picking scenario on a real\nrobot platform. Our approach has shown to be more reliable than another recent\ngeometry-based method considered as baseline [7] in terms of grasp stability,\nby increasing the successful grasp attempts by a factor of six.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:44:18 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Denoun", "Brice", ""], ["Leon", "Beatriz", ""], ["Zito", "Claudio", ""], ["Stolkin", "Rustam", ""], ["Jamone", "Lorenzo", ""], ["Hansard", "Miles", ""]]}, {"id": "1907.08136", "submitter": "Jake Sganga", "authors": "Jake Sganga, David Eng, Chauncey Graetzel, David B. Camarillo", "title": "Autonomous Driving in the Lung using Deep Learning for Localization", "comments": "10 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:1903.10554", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is the leading cause of cancer-related death worldwide, and early\ndiagnosis is critical to improving patient outcomes. To diagnose cancer, a\nhighly trained pulmonologist must navigate a flexible bronchoscope deep into\nthe branched structure of the lung for biopsy. The biopsy fails to sample the\ntarget tissue in 26-33% of cases largely because of poor registration with the\npreoperative CT map. To improve intraoperative registration, we develop two\ndeep learning approaches to localize the bronchoscope in the preoperative CT\nmap based on the bronchoscopic video in real-time, called AirwayNet and\nBifurcationNet. The networks are trained entirely on simulated images derived\nfrom the patient-specific CT. When evaluated on recorded bronchoscopy videos in\na phantom lung, AirwayNet outperforms other deep learning localization\nalgorithms with an area under the precision-recall curve of 0.97. Using\nAirwayNet, we demonstrate autonomous driving in the phantom lung based on video\nfeedback alone. The robot reaches four targets in the left and right lungs in\n95% of the trials. On recorded videos in eight human cadaver lungs, AirwayNet\nachieves areas under the precision-recall curve ranging from 0.82 to 0.997.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 20:07:13 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Sganga", "Jake", ""], ["Eng", "David", ""], ["Graetzel", "Chauncey", ""], ["Camarillo", "David B.", ""]]}, {"id": "1907.08175", "submitter": "Terrance DeVries", "authors": "Terrance DeVries, Adriana Romero, Luis Pineda, Graham W. Taylor,\n  Michal Drozdzal", "title": "On the Evaluation of Conditional GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Generative Adversarial Networks (cGANs) are finding increasingly\nwidespread use in many application domains. Despite outstanding progress,\nquantitative evaluation of such models often involves multiple distinct metrics\nto assess different desirable properties, such as image quality, conditional\nconsistency, and intra-conditioning diversity. In this setting, model\nbenchmarking becomes a challenge, as each metric may indicate a different\n\"best\" model. In this paper, we propose the Frechet Joint Distance (FJD), which\nis defined as the Frechet distance between joint distributions of images and\nconditioning, allowing it to implicitly capture the aforementioned properties\nin a single metric. We conduct proof-of-concept experiments on a controllable\nsynthetic dataset, which consistently highlight the benefits of FJD when\ncompared to currently established metrics. Moreover, we use the newly\nintroduced metric to compare existing cGAN-based models for a variety of\nconditioning modalities (e.g. class labels, object masks, bounding boxes,\nimages, and text captions). We show that FJD can be used as a promising single\nmetric for cGAN benchmarking and model selection. Code can be found at\nhttps://github.com/facebookresearch/fjd.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 17:41:57 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 21:02:23 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2019 02:53:54 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["DeVries", "Terrance", ""], ["Romero", "Adriana", ""], ["Pineda", "Luis", ""], ["Taylor", "Graham W.", ""], ["Drozdzal", "Michal", ""]]}, {"id": "1907.08195", "submitter": "Armin Mustafa", "authors": "Armin Mustafa, Marco Volino, Hansung Kim, Jean-Yves Guillemaut, Adrian\n  Hilton", "title": "Temporally Coherent General Dynamic Scene Reconstruction", "comments": "Submitted to IJCV 2019. arXiv admin note: substantial text overlap\n  with arXiv:1603.03381", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques for dynamic scene reconstruction from multiple\nwide-baseline cameras primarily focus on reconstruction in controlled\nenvironments, with fixed calibrated cameras and strong prior constraints. This\npaper introduces a general approach to obtain a 4D representation of complex\ndynamic scenes from multi-view wide-baseline static or moving cameras without\nprior knowledge of the scene structure, appearance, or illumination.\nContributions of the work are: An automatic method for initial coarse\nreconstruction to initialize joint estimation; Sparse-to-dense temporal\ncorrespondence integrated with joint multi-view segmentation and reconstruction\nto introduce temporal coherence; and a general robust approach for joint\nsegmentation refinement and dense reconstruction of dynamic scenes by\nintroducing shape constraint. Comparison with state-of-the-art approaches on a\nvariety of complex indoor and outdoor scenes, demonstrates improved accuracy in\nboth multi-view segmentation and dense reconstruction. This paper demonstrates\nunsupervised reconstruction of complete temporally coherent 4D scene models\nwith improved non-rigid object segmentation and shape reconstruction and its\napplication to free-viewpoint rendering and virtual reality.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 12:33:25 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 13:34:35 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Mustafa", "Armin", ""], ["Volino", "Marco", ""], ["Kim", "Hansung", ""], ["Guillemaut", "Jean-Yves", ""], ["Hilton", "Adrian", ""]]}, {"id": "1907.08196", "submitter": "Tanya Schmah", "authors": "Kevin Raina, Uladzimir Yahorau, Tanya Schmah", "title": "Exploiting bilateral symmetry in brain lesion segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain lesions, including stroke and tumours, have a high degree of\nvariability in terms of location, size, intensity and form, making automatic\nsegmentation difficult. We propose an improvement to existing segmentation\nmethods by exploiting the bilateral quasi-symmetry of healthy brains, which\nbreaks down when lesions are present. Specifically, we use nonlinear\nregistration of a neuroimage to a reflected version of itself (\"reflective\nregistration\") to determine for each voxel its homologous (corresponding) voxel\nin the other hemisphere. A patch around the homologous voxel is added as a set\nof new features to the segmentation algorithm. To evaluate this method, we\nimplemented two different CNN-based multimodal MRI stroke lesion segmentation\nalgorithms, and then augmented them by adding extra symmetry features using the\nreflective registration method described above. For each architecture, we\ncompared the performance with and without symmetry augmentation, on the SISS\nTraining dataset of the Ischemic Stroke Lesion Segmentation Challenge (ISLES)\n2015 challenge. Using affine reflective registration improves performance over\nbaseline, but nonlinear reflective registration gives significantly better\nresults: an improvement in Dice coefficient of 13 percentage points over\nbaseline for one architecture and 9 points for the other. We argue for the\nbroad applicability of adding symmetric features to existing segmentation\nalgorithms, specifically using nonlinear, template-free methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 13:42:22 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Raina", "Kevin", ""], ["Yahorau", "Uladzimir", ""], ["Schmah", "Tanya", ""]]}, {"id": "1907.08225", "submitter": "Kristian Hartikainen", "authors": "Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, Sergey Levine", "title": "Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill\n  Discovery", "comments": "11+6 pages, 6+2 figures, last two authors (Tuomas Haarnoja, Sergey\n  Levine) advised equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning requires manual specification of a reward function to\nlearn a task. While in principle this reward function only needs to specify the\ntask goal, in practice reinforcement learning can be very time-consuming or\neven infeasible unless the reward function is shaped so as to provide a smooth\ngradient towards a successful outcome. This shaping is difficult to specify by\nhand, particularly when the task is learned from raw observations, such as\nimages. In this paper, we study how we can automatically learn dynamical\ndistances: a measure of the expected number of time steps to reach a given goal\nstate from any other state. These dynamical distances can be used to provide\nwell-shaped reward functions for reaching new goals, making it possible to\nlearn complex tasks efficiently. We show that dynamical distances can be used\nin a semi-supervised regime, where unsupervised interaction with the\nenvironment is used to learn the dynamical distances, while a small amount of\npreference supervision is used to determine the task goal, without any manually\nengineered reward function or goal examples. We evaluate our method both on a\nreal-world robot and in simulation. We show that our method can learn to turn a\nvalve with a real-world 9-DoF hand, using raw image observations and just ten\npreference labels, without any other supervision. Videos of the learned skills\ncan be found on the project website:\nhttps://sites.google.com/view/dynamical-distance-learning.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:07:47 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 18:25:04 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 14:15:46 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2020 10:16:54 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Hartikainen", "Kristian", ""], ["Geng", "Xinyang", ""], ["Haarnoja", "Tuomas", ""], ["Levine", "Sergey", ""]]}, {"id": "1907.08288", "submitter": "Canyi Lu", "authors": "Canyi Lu and Pan Zhou", "title": "Exact Recovery of Tensor Robust Principal Component Analysis under\n  Linear Transforms", "comments": "arXiv admin note: text overlap with arXiv:1804.03728; text overlap\n  with arXiv:1311.6182 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the Tensor Robust Principal Component Analysis (TRPCA)\nproblem, which aims to exactly recover the low-rank and sparse components from\ntheir sum. Our model is motivated by the recently proposed linear transforms\nbased tensor-tensor product and tensor SVD. We define a new transforms depended\ntensor rank and the corresponding tensor nuclear norm. Then we solve the TRPCA\nproblem by convex optimization whose objective is a weighted combination of the\nnew tensor nuclear norm and the $\\ell_1$-norm. In theory, we show that under\ncertain incoherence conditions, the convex program exactly recovers the\nunderlying low-rank and sparse components. It is of great interest that our new\nTRPCA model generalizes existing works. In particular, if the studied tensor\nreduces to a matrix, our TRPCA model reduces to the known matrix RPCA. Our new\nTRPCA which is allowed to use general linear transforms can be regarded as an\nextension of our former TRPCA work which uses the discrete Fourier transform.\nBut their proof of the recovery guarantee is different. Numerical experiments\nverify our results and the application on image recovery demonstrates the\nsuperiority of our method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 19:05:15 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Lu", "Canyi", ""], ["Zhou", "Pan", ""]]}, {"id": "1907.08303", "submitter": "Jakub Nalepa", "authors": "Jakub Nalepa, Pablo Ribalta Lorenzo, Michal Marcinkiewicz, Barbara\n  Bobek-Billewicz, Pawel Wawrzyniak, Maksym Walczak, Michal Kawulok, Wojciech\n  Dudzik, Grzegorz Mrukwa, Pawel Ulrych, Michael P. Hayball", "title": "Fully-automated deep learning-powered system for DCE-MRI analysis of\n  brain tumors", "comments": "Submitted for publication in Artificial Intelligence in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an\nimportant role in diagnosis and grading of brain tumor. Although manual DCE\nbiomarker extraction algorithms boost the diagnostic yield of DCE-MRI by\nproviding quantitative information on tumor prognosis and prediction, they are\ntime-consuming and prone to human error. In this paper, we propose a\nfully-automated, end-to-end system for DCE-MRI analysis of brain tumors. Our\ndeep learning-powered technique does not require any user interaction, it\nyields reproducible results, and it is rigorously validated against benchmark\n(BraTS'17 for tumor segmentation, and a test dataset released by the\nQuantitative Imaging Biomarkers Alliance for the contrast-concentration\nfitting) and clinical (44 low-grade glioma patients) data. Also, we introduce a\ncubic model of the vascular input function used for pharmacokinetic modeling\nwhich significantly decreases the fitting error when compared with the state of\nthe art, alongside a real-time algorithm for determination of the vascular\ninput region. An extensive experimental study, backed up with statistical\ntests, showed that our system delivers state-of-the-art results (in terms of\nsegmentation accuracy and contrast-concentration fitting) while requiring less\nthan 3 minutes to process an entire input DCE-MRI study using a single GPU.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 21:57:02 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Nalepa", "Jakub", ""], ["Lorenzo", "Pablo Ribalta", ""], ["Marcinkiewicz", "Michal", ""], ["Bobek-Billewicz", "Barbara", ""], ["Wawrzyniak", "Pawel", ""], ["Walczak", "Maksym", ""], ["Kawulok", "Michal", ""], ["Dudzik", "Wojciech", ""], ["Mrukwa", "Grzegorz", ""], ["Ulrych", "Pawel", ""], ["Hayball", "Michael P.", ""]]}, {"id": "1907.08307", "submitter": "Martin Wistuba", "authors": "Martin Wistuba", "title": "XferNAS: Transfer Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term Neural Architecture Search (NAS) refers to the automatic\noptimization of network architectures for a new, previously unknown task. Since\ntesting an architecture is computationally very expensive, many optimizers need\ndays or even weeks to find suitable architectures. However, this search time\ncan be significantly reduced if knowledge from previous searches on different\ntasks is reused. In this work, we propose a generally applicable framework that\nintroduces only minor changes to existing optimizers to leverage this feature.\nAs an example, we select an existing optimizer and demonstrate the complexity\nof the integration of the framework as well as its impact. In experiments on\nCIFAR-10 and CIFAR-100, we observe a reduction in the search time from 200 to\nonly 6 GPU days, a speed up by a factor of 33. In addition, we observe new\nrecords of 1.99 and 14.06 for NAS optimizers on the CIFAR benchmarks,\nrespectively. In a separate study, we analyze the impact of the amount of\nsource and target data. Empirically, we demonstrate that the proposed framework\ngenerally gives better results and, in the worst case, is just as good as the\nunmodified optimizer.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 22:05:49 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Wistuba", "Martin", ""]]}, {"id": "1907.08310", "submitter": "Yash Patel", "authors": "Yash Patel, Srikar Appalaraju, R. Manmatha", "title": "Deep Perceptual Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several deep learned lossy compression techniques have been proposed in the\nrecent literature. Most of these are optimized by using either MS-SSIM\n(multi-scale structural similarity) or MSE (mean squared error) as a loss\nfunction. Unfortunately, neither of these correlate well with human perception\nand this is clearly visible from the resulting compressed images. In several\ncases, the MS-SSIM for deep learned techniques is higher than say a\nconventional, non-deep learned codec such as JPEG-2000 or BPG. However, the\nimages produced by these deep learned techniques are in many cases clearly\nworse to human eyes than those produced by JPEG-2000 or BPG.\n  We propose the use of an alternative, deep perceptual metric, which has been\nshown to align better with human perceptual similarity. We then propose Deep\nPerceptual Compression (DPC) which makes use of an encoder-decoder based image\ncompression model to jointly optimize on the deep perceptual metric and\nMS-SSIM. Via extensive human evaluations, we show that the proposed method\ngenerates visually better results than previous learning based compression\nmethods and JPEG-2000, and is comparable to BPG. Furthermore, we demonstrate\nthat for tasks like object-detection, images compressed with DPC give better\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 22:17:52 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 21:17:27 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Patel", "Yash", ""], ["Appalaraju", "Srikar", ""], ["Manmatha", "R.", ""]]}, {"id": "1907.08320", "submitter": "Bruna Maciel-Pearson", "authors": "Bruna G. Maciel-Pearson, Samet Akcay, Amir Atapour-Abarghouei,\n  Christopher Holder and Toby P. Breckon", "title": "Multi-Task Regression-based Learning for Autonomous Unmanned Aerial\n  Vehicle Flight Control within Unstructured Outdoor Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased growth in the global Unmanned Aerial Vehicles (UAV) (drone)\nindustry has expanded possibilities for fully autonomous UAV applications. A\nparticular application which has in part motivated this research is the use of\nUAV in wide area search and surveillance operations in unstructured outdoor\nenvironments. The critical issue with such environments is the lack of\nstructured features that could aid in autonomous flight, such as road lines or\npaths. In this paper, we propose an End-to-End Multi-Task Regression-based\nLearning approach capable of defining flight commands for navigation and\nexploration under the forest canopy, regardless of the presence of trails or\nadditional sensors (i.e. GPS). Training and testing are performed using a\nsoftware in the loop pipeline which allows for a detailed evaluation against\nstate-of-the-art pose estimation techniques. Our extensive experiments\ndemonstrate that our approach excels in performing dense exploration within the\nrequired search perimeter, is capable of covering wider search regions,\ngeneralises to previously unseen and unexplored environments and outperforms\ncontemporary state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 23:45:05 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Maciel-Pearson", "Bruna G.", ""], ["Akcay", "Samet", ""], ["Atapour-Abarghouei", "Amir", ""], ["Holder", "Christopher", ""], ["Breckon", "Toby P.", ""]]}, {"id": "1907.08328", "submitter": "Anthony Reeves", "authors": "Sergei V. Fotin, David F. Yankelevitz, Claudia I. Henschke, Anthony P.\n  Reeves", "title": "A multiscale Laplacian of Gaussian (LoG) filtering approach to pulmonary\n  nodule detection from whole-lung CT scans", "comments": "16 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Candidate generation, the first stage for most computer aided detection (CAD)\nsystems, rapidly scans the entire image data for any possible abnormality\nlocations, while the subsequent stages of the CAD system refine the candidates\nlist to determine the most probable or significant of these candidates. The\ncandidate generator creates a list of the locations and provides a size\nestimate for each candidate. A multiscale scale-normalized Laplacian of\nGaussian (LoG) filtering method for detecting pulmonary nodules in whole-lung\nCT scans, presented in this paper, achieves a high sensitivity for both solid\nand nonsolid pulmonary nodules. The pulmonary nodule LoG filtering method was\nvalidated on a size-enriched database of 706 whole-lung low-dose CT scans\ncontaining 499 solid (>= 4 mm) and 107 nonsolid (>= 6 mm) pulmonary nodules.\nThe method achieved a sensitivity of 0.998 (498/499) for solid nodules and a\nsensitivity of 1.000 (107/107) for nonsolid nodules. Furthermore, compared to\nradiologist measurements, the method provided low average nodule size\nestimation error of 0.12 mm for solid and 1.27 mm for nonsolid nodules. The\naverage distance between automatically and manually determined nodule centroids\nwere 1.41 mm and 1.43 mm, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 01:14:29 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Fotin", "Sergei V.", ""], ["Yankelevitz", "David F.", ""], ["Henschke", "Claudia I.", ""], ["Reeves", "Anthony P.", ""]]}, {"id": "1907.08340", "submitter": "Shengxin Zha", "authors": "Laura Sevilla-Lara, Shengxin Zha, Zhicheng Yan, Vedanuj Goswami, Matt\n  Feiszli, Lorenzo Torresani", "title": "Only Time Can Tell: Discovering Temporal Data for Temporal Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding temporal information and how the visual world changes over time\nis a fundamental ability of intelligent systems. In video understanding,\ntemporal information is at the core of many current challenges, including\ncompression, efficient inference, motion estimation or summarization. However,\nin current video datasets it has been observed that action classes can often be\nrecognized without any temporal information from a single frame of video. As a\nresult, both benchmarking and training in these datasets may give an\nunintentional advantage to models with strong image understanding capabilities,\nas opposed to those with strong temporal understanding. In this paper we\naddress this problem head on by identifying action classes where temporal\ninformation is actually necessary to recognize them and call these \"temporal\nclasses\". Selecting temporal classes using a computational method would bias\nthe process. Instead, we propose a methodology based on a simple and effective\nhuman annotation experiment. We remove just the temporal information by\nshuffling frames in time and measure if the action can still be recognized.\nClasses that cannot be recognized when frames are not in order are included in\nthe temporal Dataset. We observe that this set is statistically different from\nother static classes, and that performance in it correlates with a network's\nability to capture temporal information. Thus we use it as a benchmark on\ncurrent popular networks, which reveals a series of interesting facts. We also\nexplore the effect of training on the temporal dataset, and observe that this\nleads to better generalization in unseen classes, demonstrating the need for\nmore temporal data. We hope that the proposed dataset of temporal categories\nwill help guide future research in temporal modeling for better video\nunderstanding.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 02:00:23 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 19:20:18 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Sevilla-Lara", "Laura", ""], ["Zha", "Shengxin", ""], ["Yan", "Zhicheng", ""], ["Goswami", "Vedanuj", ""], ["Feiszli", "Matt", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1907.08388", "submitter": "Sangil Lee", "authors": "Sangil Lee, Clark Youngdong Son, and H. Jin Kim", "title": "Robust Real-time RGB-D Visual Odometry in Dynamic Environments via Rigid\n  Motion Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the paper, we propose a robust real-time visual odometry in dynamic\nenvironments via rigid-motion model updated by scene flow. The proposed\nalgorithm consists of spatial motion segmentation and temporal motion tracking.\nThe spatial segmentation first generates several motion hypotheses by using a\ngrid-based scene flow and clusters the extracted motion hypotheses, separating\nobjects that move independently of one another. Further, we use a dual-mode\nmotion model to consistently distinguish between the static and dynamic parts\nin the temporal motion tracking stage. Finally, the proposed algorithm\nestimates the pose of a camera by taking advantage of the region classified as\nstatic parts. In order to evaluate the performance of visual odometry under the\nexistence of dynamic rigid objects, we use self-collected dataset containing\nRGB-D images and motion capture data for ground-truth. We compare our algorithm\nwith state-of-the-art visual odometry algorithms. The validation results\nsuggest that the proposed algorithm can estimate the pose of a camera robustly\nand accurately in dynamic environments.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 06:53:33 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Lee", "Sangil", ""], ["Son", "Clark Youngdong", ""], ["Kim", "H. Jin", ""]]}, {"id": "1907.08427", "submitter": "Ruibing Hou", "authors": "Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, Xilin\n  Chen", "title": "VRSTC: Occlusion-Free Video Person Re-Identification", "comments": "10 pages, 6 figures, 5 tables. Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video person re-identification (re-ID) plays an important role in\nsurveillance video analysis. However, the performance of video re-ID\ndegenerates severely under partial occlusion. In this paper, we propose a novel\nnetwork, called Spatio-Temporal Completion network (STCnet), to explicitly\nhandle partial occlusion problem. Different from most previous works that\ndiscard the occluded frames, STCnet can recover the appearance of the occluded\nparts. For one thing, the spatial structure of a pedestrian frame can be used\nto predict the occluded body parts from the unoccluded body parts of this\nframe. For another, the temporal patterns of pedestrian sequence provide\nimportant clues to generate the contents of occluded parts. With the\nSpatio-temporal information, STCnet can recover the appearance for the occluded\nparts, which could be leveraged with those unoccluded parts for more accurate\nvideo re-ID. By combining a re-ID network with STCnet, a video re-ID framework\nrobust to partial occlusion (VRSTC) is proposed. Experiments on three\nchallenging video re-ID databases demonstrate that the proposed approach\noutperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 09:38:27 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Hou", "Ruibing", ""], ["Ma", "Bingpeng", ""], ["Chang", "Hong", ""], ["Gu", "Xinqian", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1907.08435", "submitter": "Ruibing Hou", "authors": "Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, Xilin\n  Chen", "title": "Interaction-and-Aggregation Network for Person Re-identification", "comments": "10 pages, 8 figures, accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (reID) benefits greatly from deep convolutional\nneural networks (CNNs) which learn robust feature embeddings. However, CNNs are\ninherently limited in modeling the large variations in person pose and scale\ndue to their fixed geometric structures. In this paper, we propose a novel\nnetwork structure, Interaction-and-Aggregation (IA), to enhance the feature\nrepresentation capability of CNNs. Firstly, Spatial IA (SIA) module is\nintroduced. It models the interdependencies between spatial features and then\naggregates the correlated features corresponding to the same body parts. Unlike\nCNNs which extract features from fixed rectangle regions, SIA can adaptively\ndetermine the receptive fields according to the input person pose and scale.\nSecondly, we introduce Channel IA (CIA) module which selectively aggregates\nchannel features to enhance the feature representation, especially for\nsmallscale visual cues. Further, IA network can be constructed by inserting IA\nblocks into CNNs at any depth. We validate the effectiveness of our model for\nperson reID by demonstrating its superiority over state-of-the-art methods on\nthree benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 09:47:58 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Hou", "Ruibing", ""], ["Ma", "Bingpeng", ""], ["Chang", "Hong", ""], ["Gu", "Xinqian", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1907.08448", "submitter": "Diego Valsesia", "authors": "Diego Valsesia, Giulia Fracastoro, Enrico Magli", "title": "Deep Graph-Convolutional Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local self-similarity is well-known to be an effective prior for the\nimage denoising problem. However, little work has been done to incorporate it\nin convolutional neural networks, which surpass non-local model-based methods\ndespite only exploiting local information. In this paper, we propose a novel\nend-to-end trainable neural network architecture employing layers based on\ngraph convolution operations, thereby creating neurons with non-local receptive\nfields. The graph convolution operation generalizes the classic convolution to\narbitrary graphs. In this work, the graph is dynamically computed from\nsimilarities among the hidden features of the network, so that the powerful\nrepresentation learning capabilities of the network are exploited to uncover\nself-similar patterns. We introduce a lightweight Edge-Conditioned Convolution\nwhich addresses vanishing gradient and over-parameterization issues of this\nparticular graph convolution. Extensive experiments show state-of-the-art\nperformance with improved qualitative and quantitative results on both\nsynthetic Gaussian noise and real noise.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 10:27:41 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Valsesia", "Diego", ""], ["Fracastoro", "Giulia", ""], ["Magli", "Enrico", ""]]}, {"id": "1907.08451", "submitter": "Mathis Hoffmann", "authors": "Mathis Hoffmann, Bernd Doll, Florian Talkenberg, Christoph J. Brabec,\n  Andreas K. Maier, Vincent Christlein", "title": "Fast and robust detection of solar modules in electroluminescence images", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-29891-3_46", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast, non-destructive and on-site quality control tools, mainly high\nsensitive imaging techniques, are important to assess the reliability of\nphotovoltaic plants. To minimize the risk of further damages and electrical\nyield losses, electroluminescence (EL) imaging is used to detect local defects\nin an early stage, which might cause future electric losses. For an automated\ndefect recognition on EL measurements, a robust detection and rectification of\nmodules, as well as an optional segmentation into cells is required. This paper\nintroduces a method to detect solar modules and crossing points between solar\ncells in EL images. We only require 1-D image statistics for the detection,\nresulting in an approach that is computationally efficient. In addition, the\nmethod is able to detect the modules under perspective distortion and in\nscenarios, where multiple modules are visible in the image. We compare our\nmethod to the state of the art and show that it is superior in presence of\nperspective distortion while the performance on images, where the module is\nroughly coplanar to the detector, is similar to the reference method. Finally,\nwe show that we greatly improve in terms of computational time in comparison to\nthe reference method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 10:32:35 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Hoffmann", "Mathis", ""], ["Doll", "Bernd", ""], ["Talkenberg", "Florian", ""], ["Brabec", "Christoph J.", ""], ["Maier", "Andreas K.", ""], ["Christlein", "Vincent", ""]]}, {"id": "1907.08511", "submitter": "Adrien Lagrange", "authors": "Adrien Lagrange, Mathieu Fauvel, St\\'ephane May and Nicolas Dobigeon", "title": "Matrix cofactorization for joint spatial-spectral unmixing of\n  hyperspectral images", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.2968541", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral unmixing aims at identifying a set of elementary spectra and\nthe corresponding mixture coefficients for each pixel of an image. As the\nelementary spectra correspond to the reflectance spectra of real materials,\nthey are often very correlated yielding an ill-conditioned problem. To enrich\nthe model and to reduce ambiguity due to the high correlation, it is common to\nintroduce spatial information to complement the spectral information. The most\ncommon way to introduce spatial information is to rely on a spatial\nregularization of the abundance maps. In this paper, instead of considering a\nsimple but limited regularization process, spatial information is directly\nincorporated through the newly proposed context of spatial unmixing. Contextual\nfeatures are extracted for each pixel and this additional set of observations\nis decomposed according to a linear model. Finally the spatial and spectral\nobservations are unmixed jointly through a cofactorization model. In\nparticular, this model introduces a coupling term used to identify clusters of\nshared spatial and spectral signatures. An evaluation of the proposed method is\nconducted on synthetic and real data and shows that results are accurate and\nalso very meaningful since they describe both spatially and spectrally the\nvarious areas of the scene.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 13:43:08 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 10:10:41 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Lagrange", "Adrien", ""], ["Fauvel", "Mathieu", ""], ["May", "St\u00e9phane", ""], ["Dobigeon", "Nicolas", ""]]}, {"id": "1907.08514", "submitter": "Cameron Kyle-Davidson Mr", "authors": "Cameron Kyle-Davidson, Adrian Bors, Karla Evans", "title": "Predicting Visual Memory Schemas with Variational Autoencoders", "comments": "Accepted to BMVC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual memory schema (VMS) maps show which regions of an image cause that\nimage to be remembered or falsely remembered. Previous work has succeeded in\ngenerating low resolution VMS maps using convolutional neural networks. We\ninstead approach this problem as an image-to-image translation task making use\nof a variational autoencoder. This approach allows us to generate higher\nresolution dual channel images that represent visual memory schemas, allowing\nus to evaluate predicted true memorability and false memorability separately.\nWe also evaluate the relationship between VMS maps, predicted VMS maps, ground\ntruth memorability scores, and predicted memorability scores.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 13:48:21 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Kyle-Davidson", "Cameron", ""], ["Bors", "Adrian", ""], ["Evans", "Karla", ""]]}, {"id": "1907.08533", "submitter": "Anders Eklund", "authors": "David Abramian, Anders Eklund", "title": "Generating fMRI volumes from T1-weighted volumes using 3D CycleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Registration between an fMRI volume and a T1-weighted volume is challenging,\nsince fMRI volumes contain geometric distortions. Here we present preliminary\nresults showing that 3D CycleGAN can be used to synthesize fMRI volumes from\nT1-weighted volumes, and vice versa, which can facilitate registration.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 14:47:18 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 16:56:06 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Abramian", "David", ""], ["Eklund", "Anders", ""]]}, {"id": "1907.08612", "submitter": "Tom Vercauteren", "authors": "M. Jorge Cardoso, Aasa Feragen, Ben Glocker, Ender Konukoglu, Ipek\n  Oguz, Gozde Unal, Tom Vercauteren", "title": "Medical Imaging with Deep Learning: MIDL 2019 -- Extended Abstract Track", "comments": "Accepted extended abstracts can also be found at\n  https://openreview.net/group?id=MIDL.io/2019/Conference#abstract-accept-papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This compendium gathers all the accepted extended abstracts from the Second\nInternational Conference on Medical Imaging with Deep Learning (MIDL 2019),\nheld in London, UK, 8-10 July 2019. Note that only accepted extended abstracts\nare listed here, the Proceedings of the MIDL 2019 Full Paper Track are\npublished as Volume 102 of the Proceedings of Machine Learning Research (PMLR)\nhttp://proceedings.mlr.press/v102/.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 16:27:29 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 13:14:56 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Cardoso", "M. Jorge", ""], ["Feragen", "Aasa", ""], ["Glocker", "Ben", ""], ["Konukoglu", "Ender", ""], ["Oguz", "Ipek", ""], ["Unal", "Gozde", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1907.08719", "submitter": "Vinicius Arruda", "authors": "Vinicius F. Arruda, Thiago M. Paix\\~ao, Rodrigo F. Berriel, Alberto F.\n  De Souza, Claudine Badue, Nicu Sebe and Thiago Oliveira-Santos", "title": "Cross-Domain Car Detection Using Unsupervised Image-to-Image\n  Translation: From Day to Night", "comments": "8 pages, 8 figures,\n  https://github.com/viniciusarruda/cross-domain-car-detection and accepted at\n  IJCNN 2019", "journal-ref": null, "doi": "10.1109/IJCNN.2019.8852008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have enabled the emergence of state-of-the-art\nmodels to address object detection tasks. However, these techniques are\ndata-driven, delegating the accuracy to the training dataset which must\nresemble the images in the target task. The acquisition of a dataset involves\nannotating images, an arduous and expensive process, generally requiring time\nand manual effort. Thus, a challenging scenario arises when the target domain\nof application has no annotated dataset available, making tasks in such\nsituation to lean on a training dataset of a different domain. Sharing this\nissue, object detection is a vital task for autonomous vehicles where the large\namount of driving scenarios yields several domains of application requiring\nannotated data for the training process. In this work, a method for training a\ncar detection system with annotated data from a source domain (day images)\nwithout requiring the image annotations of the target domain (night images) is\npresented. For that, a model based on Generative Adversarial Networks (GANs) is\nexplored to enable the generation of an artificial dataset with its respective\nannotations. The artificial dataset (fake dataset) is created translating\nimages from day-time domain to night-time domain. The fake dataset, which\ncomprises annotated images of only the target domain (night images), is then\nused to train the car detector model. Experimental results showed that the\nproposed method achieved significant and consistent improvements, including the\nincreasing by more than 10% of the detection performance when compared to the\ntraining with only the available annotated data (i.e., day images).\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 22:37:28 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Arruda", "Vinicius F.", ""], ["Paix\u00e3o", "Thiago M.", ""], ["Berriel", "Rodrigo F.", ""], ["De Souza", "Alberto F.", ""], ["Badue", "Claudine", ""], ["Sebe", "Nicu", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "1907.08769", "submitter": "Lin Zhu", "authors": "Lin Zhu and Siwei Dong and Tiejun Huang and Yonghong Tian", "title": "A Retina-inspired Sampling Method for Visual Texture Reconstruction", "comments": "Published in ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional frame-based camera is not able to meet the demand of rapid\nreaction for real-time applications, while the emerging dynamic vision sensor\n(DVS) can realize high speed capturing for moving objects. However, to achieve\nvisual texture reconstruction, DVS need extra information apart from the output\nspikes. This paper introduces a fovea-like sampling method inspired by the\nneuron signal processing in retina, which aims at visual texture reconstruction\nonly taking advantage of the properties of spikes. In the proposed method, the\npixels independently respond to the luminance changes with temporal\nasynchronous spikes. Analyzing the arrivals of spikes makes it possible to\nrestore the luminance information, enabling reconstructing the natural scene\nfor visualization. Three decoding methods of spike stream for texture\nreconstruction are proposed for high-speed motion and stationary scenes.\nCompared to conventional frame-based camera and DVS, our model can achieve\nbetter image quality and higher flexibility, which is capable of changing the\nway that demanding machine vision applications are built.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 06:54:14 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zhu", "Lin", ""], ["Dong", "Siwei", ""], ["Huang", "Tiejun", ""], ["Tian", "Yonghong", ""]]}, {"id": "1907.08770", "submitter": "Andrew Price", "authors": "Andrew Price and Linyi Jin and Dmitry Berenson", "title": "Inferring Occluded Geometry Improves Performance when Retrieving an\n  Object from Dense Clutter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object search -- the problem of finding a target object in a cluttered scene\n-- is essential to solve for many robotics applications in warehouse and\nhousehold environments. However, cluttered environments entail that objects\noften occlude one another, making it difficult to segment objects and infer\ntheir shapes and properties. Instead of relying on the availability of CAD or\nother explicit models of scene objects, we augment a manipulation planner for\ncluttered environments with a state-of-the-art deep neural network for shape\ncompletion as well as a volumetric memory system, allowing the robot to reason\nabout what may be contained in occluded areas. We test the system in a variety\nof tabletop manipulation scenes composed of household items, highlighting its\napplicability to realistic domains. Our results suggest that incorporating both\ncomponents into a manipulation planning framework significantly reduces the\nnumber of actions needed to find a hidden object in dense clutter.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 06:57:52 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 20:59:28 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Price", "Andrew", ""], ["Jin", "Linyi", ""], ["Berenson", "Dmitry", ""]]}, {"id": "1907.08816", "submitter": "Jianhui Chen Mr", "authors": "Jikai Lu, Jianhui Chen, James J. Little", "title": "Pan-tilt-zoom SLAM for Sports Videos", "comments": "10+3 pages, BMVC 2019 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online SLAM system specifically designed to track pan-tilt-zoom\n(PTZ) cameras in highly dynamic sports such as basketball and soccer games. In\nthese games, PTZ cameras rotate very fast and players cover large image areas.\nTo overcome these challenges, we propose to use a novel camera model for\ntracking and to use rays as landmarks in mapping. Rays overcome the missing\ndepth in pure-rotation cameras. We also develop an online pan-tilt forest for\nmapping and introduce moving objects (players) detection to mitigate negative\nimpacts from foreground objects. We test our method on both synthetic and real\ndatasets. The experimental results show the superior performance of our method\nover previous methods for online PTZ camera pose estimation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 14:25:16 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Lu", "Jikai", ""], ["Chen", "Jianhui", ""], ["Little", "James J.", ""]]}, {"id": "1907.08822", "submitter": "Bo Jiang", "authors": "Bo Jiang, Xixi Wang, Bin Luo", "title": "PH-GCN: Person Re-identification with Part-based Hierarchical Graph\n  Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The person re-identification (Re-ID) task requires to robustly extract\nfeature representations for person images. Recently, part-based representation\nmodels have been widely studied for extracting the more compact and robust\nfeature representations for person images to improve person Re-ID results.\nHowever, existing part-based representation models mostly extract the features\nof different parts independently which ignore the relationship information\nbetween different parts. To overcome this limitation, in this paper we propose\na novel deep learning framework, named Part-based Hierarchical Graph\nConvolutional Network (PH-GCN) for person Re-ID problem. Given a person image,\nPH-GCN first constructs a hierarchical graph to represent the pairwise\nrelationships among different parts. Then, both local and global feature\nlearning are performed by the messages passing in PH-GCN, which takes other\nnodes information into account for part feature representation. Finally, a\nperceptron layer is adopted for the final person part label prediction and\nre-identification. The proposed framework provides a general solution that\nintegrates local, global and structural feature learning simultaneously in a\nunified end-to-end network. Extensive experiments on several benchmark datasets\ndemonstrate the effectiveness of the proposed PH-GCN based Re-ID approach.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 15:18:39 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Jiang", "Bo", ""], ["Wang", "Xixi", ""], ["Luo", "Bin", ""]]}, {"id": "1907.08825", "submitter": "Robert DiPietro", "authors": "Robert DiPietro, Gregory D. Hager", "title": "Automated Surgical Activity Recognition with One Labeled Sequence", "comments": "Accepted for publication at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work has demonstrated the feasibility of automated activity recognition\nin robot-assisted surgery from motion data. However, these efforts have assumed\nthe availability of a large number of densely-annotated sequences, which must\nbe provided manually by experts. This process is tedious, expensive, and\nerror-prone. In this paper, we present the first analysis under the assumption\nof scarce annotations, where as little as one annotated sequence is available\nfor training. We demonstrate feasibility of automated recognition in this\nchallenging setting, and we show that learning representations in an\nunsupervised fashion, before the recognition phase, leads to significant gains\nin performance. In addition, our paper poses a new challenge to the community:\nhow much further can we push performance in this important yet relatively\nunexplored regime?\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 15:26:51 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["DiPietro", "Robert", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1907.08831", "submitter": "Markus Roland Ernst", "authors": "Markus Roland Ernst, Jochen Triesch, Thomas Burwick", "title": "Recurrent Connections Aid Occluded Object Recognition by Discounting\n  Occluders", "comments": "13 pages, 5 figures, accepted at the 28th International Conference on\n  Artificial Neural Networks, published in Springer Lecture Notes in Computer\n  Science vol 11729", "journal-ref": "In: Tetko, I. V. et al. (eds.) ICANN 2019. LNCS, vol 11729.\n  Springer, Cham, pp 294-305", "doi": "10.1007/978-3-030-30508-6_24", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent connections in the visual cortex are thought to aid object\nrecognition when part of the stimulus is occluded. Here we investigate if and\nhow recurrent connections in artificial neural networks similarly aid object\nrecognition. We systematically test and compare architectures comprised of\nbottom-up (B), lateral (L) and top-down (T) connections. Performance is\nevaluated on a novel stereoscopic occluded object recognition dataset. The task\nconsists of recognizing one target digit occluded by multiple occluder digits\nin a pseudo-3D environment. We find that recurrent models perform significantly\nbetter than their feedforward counterparts, which were matched in parametric\ncomplexity. Furthermore, we analyze how the network's representation of the\nstimuli evolves over time due to recurrent connections. We show that the\nrecurrent connections tend to move the network's representation of an occluded\ndigit towards its un-occluded version. Our results suggest that both the brain\nand artificial neural networks can exploit recurrent connectivity to aid\noccluded object recognition.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 16:10:04 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 09:36:29 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ernst", "Markus Roland", ""], ["Triesch", "Jochen", ""], ["Burwick", "Thomas", ""]]}, {"id": "1907.08845", "submitter": "Bingzhang Hu", "authors": "Junyan Wang and Bingzhang Hu and Yang Long and Yu Guan", "title": "Order Matters: Shuffling Sequence Generation for Video Prediction", "comments": "This manuscript has been accepted at BMVC 2019. See the project at\n  https://github.com/andrewjywang/SEENet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future frames in natural video sequences is a new challenge that\nis receiving increasing attention in the computer vision community. However,\nexisting models suffer from severe loss of temporal information when the\npredicted sequence is long. Compared to previous methods focusing on generating\nmore realistic contents, this paper extensively studies the importance of\nsequential order information for video generation. A novel Shuffling sEquence\ngEneration network (SEE-Net) is proposed that can learn to discriminate\nunnatural sequential orders by shuffling the video frames and comparing them to\nthe real video sequence. Systematic experiments on three datasets with both\nsynthetic and real-world videos manifest the effectiveness of shuffling\nsequence generation for video prediction in our proposed model and demonstrate\nstate-of-the-art performance by both qualitative and quantitative evaluations.\nThe source code is available at https://github.com/andrewjywang/SEENet.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 17:25:25 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Wang", "Junyan", ""], ["Hu", "Bingzhang", ""], ["Long", "Yang", ""], ["Guan", "Yu", ""]]}, {"id": "1907.08870", "submitter": "Jakub Nalepa", "authors": "Jakub Nalepa, Michal Myller, Yasuteru Imai, Ken-ichi Honda, Tomomi\n  Takeda, Marek Antoniak", "title": "Unsupervised Segmentation of Hyperspectral Images Using 3D Convolutional\n  Autoencoders", "comments": "Submitted to IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": "10.1109/LGRS.2019.2960945", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image analysis has become an important topic widely researched\nby the remote sensing community. Classification and segmentation of such\nimagery help understand the underlying materials within a scanned scene, since\nhyperspectral images convey a detailed information captured in a number of\nspectral bands. Although deep learning has established the state of the art in\nthe field, it still remains challenging to train well-generalizing models due\nto the lack of ground-truth data. In this letter, we tackle this problem and\npropose an end-to-end approach to segment hyperspectral images in a fully\nunsupervised way. We introduce a new deep architecture which couples 3D\nconvolutional autoencoders with clustering. Our multi-faceted experimental\nstudy---performed over benchmark and real-life data---revealed that our\napproach delivers high-quality segmentation without any prior class labels.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 22:17:10 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Nalepa", "Jakub", ""], ["Myller", "Michal", ""], ["Imai", "Yasuteru", ""], ["Honda", "Ken-ichi", ""], ["Takeda", "Tomomi", ""], ["Antoniak", "Marek", ""]]}, {"id": "1907.08871", "submitter": "Yuxiao Chen", "authors": "Yuxiao Chen, Long Zhao, Xi Peng, Jianbo Yuan, and Dimitris N. Metaxas", "title": "Construct Dynamic Graphs for Hand Gesture Recognition via\n  Spatial-Temporal Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Dynamic Graph-Based Spatial-Temporal Attention (DG-STA) method\nfor hand gesture recognition. The key idea is to first construct a\nfully-connected graph from a hand skeleton, where the node features and edges\nare then automatically learned via a self-attention mechanism that performs in\nboth spatial and temporal domains. We further propose to leverage the\nspatial-temporal cues of joint positions to guarantee robust recognition in\nchallenging conditions. In addition, a novel spatial-temporal mask is applied\nto significantly cut down the computational cost by 99%. We carry out extensive\nexperiments on benchmarks (DHG-14/28 and SHREC'17) and prove the superior\nperformance of our method compared with the state-of-the-art methods. The\nsource code can be found at https://github.com/yuxiaochen1103/DG-STA.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 22:24:01 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chen", "Yuxiao", ""], ["Zhao", "Long", ""], ["Peng", "Xi", ""], ["Yuan", "Jianbo", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1907.08884", "submitter": "Minkesh Asati", "authors": "Asati Minkesh, Kraittipong Worranitta, Miyachi Taizo", "title": "Human Extraction and Scene Transition utilizing Mask R-CNN", "comments": "6 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a trendy branch of computer vision, especially on human\nrecognition and pedestrian detection. Recognizing the complete body of a person\nhas always been a difficult problem. Over the years, researchers proposed\nvarious methods, and recently, Mask R-CNN has made a breakthrough for instance\nsegmentation. Based on Faster R-CNN, Mask R-CNN has been able to generate a\nsegmentation mask for each instance. We propose an application to extracts\nmultiple persons from images and videos for pleasant life scenes to grouping\nhappy moments of people such as family or friends and a community for QOL\n(Quality Of Life). We likewise propose a methodology to put extracted images of\npersons into the new background. This enables a user to make a pleasant\ncollection of happy facial expressions and actions of his/her family and\nfriends in his/her life. Mask R-CNN detects all types of object masks from\nimages. Then our algorithm considers only the target person and extracts a\nperson only without obstacles, such as dogs in front of the person, and the\nuser also can select multiple persons as their expectations. Our algorithm is\neffective for both an image and a video irrespective of the length of it. Our\nalgorithm does not add any overhead to Mask R-CNN, running at 5 fps. We show\nexamples of yoga-person in an image and a dancer in a dance-video frame. We\nhope our simple and effective approach would serve as a baseline for replacing\nthe image background and help ease future research.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 23:57:46 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 14:31:25 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Minkesh", "Asati", ""], ["Worranitta", "Kraittipong", ""], ["Taizo", "Miyachi", ""]]}, {"id": "1907.08895", "submitter": "Chen Chen", "authors": "Rui Hou, Chen Chen, Rahul Sukthankar, Mubarak Shah", "title": "An Efficient 3D CNN for Action/Object Segmentation in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) based image segmentation has made great\nprogress in recent years. However, video object segmentation remains a\nchallenging task due to its high computational complexity. Most of the previous\nmethods employ a two-stream CNN framework to handle spatial and motion features\nseparately. In this paper, we propose an end-to-end encoder-decoder style 3D\nCNN to aggregate spatial and temporal information simultaneously for video\nobject segmentation. To efficiently process video, we propose 3D separable\nconvolution for the pyramid pooling module and decoder, which dramatically\nreduces the number of operations while maintaining the performance. Moreover,\nwe also extend our framework to video action segmentation by adding an extra\nclassifier to predict the action label for actors in videos. Extensive\nexperiments on several video datasets demonstrate the superior performance of\nthe proposed approach for action and object segmentation compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 02:05:38 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Hou", "Rui", ""], ["Chen", "Chen", ""], ["Sukthankar", "Rahul", ""], ["Shah", "Mubarak", ""]]}, {"id": "1907.08915", "submitter": "Yuta Hiasa", "authors": "Yuta Hiasa, Yoshito Otake, Masaki Takao, Takeshi Ogawa, Nobuhiko\n  Sugano, Yoshinobu Sato", "title": "Automated Muscle Segmentation from Clinical CT using Bayesian U-Net for\n  Personalized Musculoskeletal Modeling", "comments": "11 pages, 10 figures, and supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for automatic segmentation of individual muscles from a\nclinical CT. The method uses Bayesian convolutional neural networks with the\nU-Net architecture, using Monte Carlo dropout that infers an uncertainty metric\nin addition to the segmentation label. We evaluated the performance of the\nproposed method using two data sets: 20 fully annotated CTs of the hip and\nthigh regions and 18 partially annotated CTs that are publicly available from\nThe Cancer Imaging Archive (TCIA) database. The experiments showed a Dice\ncoefficient (DC) of 0.891 +/- 0.016 (mean +/- std) and an average symmetric\nsurface distance (ASD) of 0.994 +/- 0.230 mm over 19 muscles in the set of 20\nCTs. These results were statistically significant improvements compared to the\nstate-of-the-art hierarchical multi-atlas method which resulted in 0.845 +/-\n0.031 DC and 1.556 +/- 0.444 mm ASD. We evaluated validity of the uncertainty\nmetric in the multi-class organ segmentation problem and demonstrated a\ncorrelation between the pixels with high uncertainty and the segmentation\nfailure. One application of the uncertainty metric in active-learning is\ndemonstrated, and the proposed query pixel selection method considerably\nreduced the manual annotation cost for expanding the training data set. The\nproposed method allows an accurate patient-specific analysis of individual\nmuscle shapes in a clinical routine. This would open up various applications\nincluding personalization of biomechanical simulation and quantitative\nevaluation of muscle atrophy.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 05:04:16 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 12:00:17 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Hiasa", "Yuta", ""], ["Otake", "Yoshito", ""], ["Takao", "Masaki", ""], ["Ogawa", "Takeshi", ""], ["Sugano", "Nobuhiko", ""], ["Sato", "Yoshinobu", ""]]}, {"id": "1907.08924", "submitter": "Edward Fry", "authors": "Edward W. S. Fry, Sophie Triantaphillidou, Robin B. Jenkin, John R.\n  Jarvis, Ralph E. Jacobson", "title": "Validation of Modulation Transfer Functions and Noise Power Spectra from\n  Natural Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Modulation Transfer Function (MTF) and the Noise Power Spectrum (NPS)\ncharacterize imaging system sharpness/resolution and noise, respectively. Both\nmeasures are based on linear system theory but are applied routinely to systems\nemploying non-linear, content-aware image processing. For such systems,\nMTFs/NPSs are derived inaccurately from traditional test charts containing\nedges, sinusoids, noise or uniform tone signals, which are unrepresentative of\nnatural scene signals. The dead leaves test chart delivers improved\nmeasurements, but still has limitations when describing the performance of\nscene-dependent systems. In this paper, we validate several novel\nscene-and-process-dependent MTF (SPD-MTF) and NPS (SPD-NPS) measures that\ncharacterize, either: i) system performance concerning one scene, or ii)\naverage real-world performance concerning many scenes, or iii) the level of\nsystem scene-dependency. We also derive novel SPD-NPS and SPD-MTF measures\nusing the dead leaves chart. We demonstrate that all the proposed measures are\nrobust and preferable for scene-dependent systems than current measures.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 07:01:55 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fry", "Edward W. S.", ""], ["Triantaphillidou", "Sophie", ""], ["Jenkin", "Robin B.", ""], ["Jarvis", "John R.", ""], ["Jacobson", "Ralph E.", ""]]}, {"id": "1907.08926", "submitter": "Edward Fry", "authors": "Edward W. S. Fry, Sophie Triantaphillidou, Robin B. Jenkin, Ralph E.\n  Jacobson, John R. Jarvis", "title": "Scene-and-Process-Dependent Spatial Image Quality Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial image quality metrics designed for camera systems generally employ\nthe Modulation Transfer Function (MTF), the Noise Power Spectrum (NPS), and a\nvisual contrast detection model. Prior art indicates that scene-dependent\ncharacteristics of non-linear, content-aware image processing are unaccounted\nfor by MTFs and NPSs measured using traditional methods. We present two novel\nmetrics: the log Noise Equivalent Quanta (log NEQ) and Visual log NEQ. They\nboth employ scene-and-process-dependent MTF (SPD-MTF) and NPS (SPD-NPS)\nmeasures, which account for signal-transfer and noise scene-dependency,\nrespectively. We also investigate implementing contrast detection and\ndiscrimination models that account for scene-dependent visual masking. Also,\nthree leading camera metrics are revised that use the above scene-dependent\nmeasures. All metrics are validated by examining correlations with the\nperceived quality of images produced by simulated camera pipelines. Metric\naccuracy improved consistently when the SPD-MTFs and SPD-NPSs were implemented.\nThe novel metrics outperformed existing metrics of the same genre.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 07:13:43 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fry", "Edward W. S.", ""], ["Triantaphillidou", "Sophie", ""], ["Jenkin", "Robin B.", ""], ["Jacobson", "Ralph E.", ""], ["Jarvis", "John R.", ""]]}, {"id": "1907.08952", "submitter": "Shang-Ho Tsai", "authors": "Tzu-Wei Tseng, Kai-Jiun Yang, C.-C. Jay Kuo and Shang-Ho (Lawrence)\n  Tsai", "title": "An Interpretable Compression and Classification System: Theory and\n  Applications", "comments": "12 pages, 12 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a low-complexity interpretable classification system. The\nproposed system contains three main modules including feature extraction,\nfeature reduction, and classification. All of them are linear. Thanks to the\nlinear property, the extracted and reduced features can be inversed to original\ndata, like a linear transform such as Fourier transform, so that one can\nquantify and visualize the contribution of individual features towards the\noriginal data. Also, the reduced features and reversibility naturally endure\nthe proposed system ability of data compression. This system can significantly\ncompress data with a small percent deviation between the compressed and the\noriginal data. At the same time, when the compressed data is used for\nclassification, it still achieves high testing accuracy. Furthermore, we\nobserve that the extracted features of the proposed system can be approximated\nto uncorrelated Gaussian random variables. Hence, classical theory in\nestimation and detection can be applied for classification. This motivates us\nto propose using a MAP (maximum a posteriori) based classification method. As a\nresult, the extracted features and the corresponding performance have\nstatistical meaning and mathematically interpretable. Simulation results show\nthat the proposed classification system not only enjoys significant reduced\ntraining and testing time but also high testing accuracy compared to the\nconventional schemes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 10:16:56 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 13:13:31 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Tseng", "Tzu-Wei", "", "Lawrence"], ["Yang", "Kai-Jiun", "", "Lawrence"], ["Kuo", "C. -C. Jay", "", "Lawrence"], ["Shang-Ho", "", "", "Lawrence"], ["Tsai", "", ""]]}, {"id": "1907.09000", "submitter": "Boris Knyazev", "authors": "Boris Knyazev, Xiao Lin, Mohamed R. Amer, Graham W. Taylor", "title": "Image Classification with Hierarchical Multigraph Networks", "comments": "13 pages, BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) are a class of general models that can\nlearn from graph structured data. Despite being general, GCNs are admittedly\ninferior to convolutional neural networks (CNNs) when applied to vision tasks,\nmainly due to the lack of domain knowledge that is hardcoded into CNNs, such as\nspatially oriented translation invariant filters. However, a great advantage of\nGCNs is the ability to work on irregular inputs, such as superpixels of images.\nThis could significantly reduce the computational cost of image reasoning\ntasks. Another key advantage inherent to GCNs is the natural ability to model\nmultirelational data. Building upon these two promising properties, in this\nwork, we show best practices for designing GCNs for image classification; in\nsome cases even outperforming CNNs on the MNIST, CIFAR-10 and PASCAL image\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 16:30:32 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Knyazev", "Boris", ""], ["Lin", "Xiao", ""], ["Amer", "Mohamed R.", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1907.09008", "submitter": "Fanhua Shang", "authors": "Dong Wang, Yicheng Liu, Wenwo Tang, Fanhua Shang, Hongying Liu, Qigong\n  Sun, Licheng Jiao", "title": "signADAM: Learning Confidences for Deep Neural Networks", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new first-order gradient-based algorithm to train\ndeep neural networks. We first introduce the sign operation of stochastic\ngradients (as in sign-based methods, e.g., SIGN-SGD) into ADAM, which is called\nas signADAM. Moreover, in order to make the rate of fitting each feature\ncloser, we define a confidence function to distinguish different components of\ngradients and apply it to our algorithm. It can generate more sparse gradients\nthan existing algorithms do. We call this new algorithm signADAM++. In\nparticular, both our algorithms are easy to implement and can speed up training\nof various deep neural networks. The motivation of signADAM++ is preferably\nlearning features from the most different samples by updating large and useful\ngradients regardless of useless information in stochastic gradients. We also\nestablish theoretical convergence guarantees for our algorithms. Empirical\nresults on various datasets and models show that our algorithms yield much\nbetter performance than many state-of-the-art algorithms including SIGN-SGD,\nSIGNUM and ADAM. We also analyze the performance from multiple perspectives\nincluding the loss landscape and develop an adaptive method to further improve\ngeneralization. The source code is available at\nhttps://github.com/DongWanginxdu/signADAM-Learn-by-Confidence.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 17:08:50 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Wang", "Dong", ""], ["Liu", "Yicheng", ""], ["Tang", "Wenwo", ""], ["Shang", "Fanhua", ""], ["Liu", "Hongying", ""], ["Sun", "Qigong", ""], ["Jiao", "Licheng", ""]]}, {"id": "1907.09019", "submitter": "Eric Sun", "authors": "Eric D. Sun and Ron Dekel", "title": "ImageNet-trained deep neural network exhibits illusion-like response to\n  the Scintillating Grid", "comments": "Supplementary material at end of document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) models for computer vision are now capable of\nhuman-level object recognition. Consequently, similarities in the performance\nand vulnerabilities of DNN and human vision are of great interest. Here we\ncharacterize the response of the VGG-19 DNN to images of the Scintillating Grid\nvisual illusion, in which white dots are perceived to be partially black. We\nobserved a significant deviation from the expected monotonic relation between\nVGG-19 representational dissimilarity and dot whiteness in the Scintillating\nGrid. That is, a linear increase in dot whiteness leads to a non-linear\nincrease and then, remarkably, a decrease (non-monotonicity) in\nrepresentational dissimilarity. In control images, mostly monotonic relations\nbetween representational dissimilarity and dot whiteness were observed.\nFurthermore, the dot whiteness level corresponding to the maximal\nrepresentational dissimilarity (i.e. onset of non-monotonic dissimilarity)\nmatched closely with that corresponding to the onset of illusion perception in\nhuman observers. As such, the non-monotonic response in the DNN is a potential\nmodel correlate for human illusion perception.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 19:14:47 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 02:13:38 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Sun", "Eric D.", ""], ["Dekel", "Ron", ""]]}, {"id": "1907.09021", "submitter": "Mina Bishay", "authors": "Mina Bishay, Georgios Zoumpourlis, Ioannis Patras", "title": "TARN: Temporal Attentive Relation Network for Few-Shot and Zero-Shot\n  Action Recognition", "comments": "14 pages, IEEE Transactions on Affective Computing", "journal-ref": null, "doi": null, "report-no": "British Machine Vision Conference (BMVC) 2019", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel Temporal Attentive Relation Network (TARN)\nfor the problems of few-shot and zero-shot action recognition. At the heart of\nour network is a meta-learning approach that learns to compare representations\nof variable temporal length, that is, either two videos of different length (in\nthe case of few-shot action recognition) or a video and a semantic\nrepresentation such as word vector (in the case of zero-shot action\nrecognition). By contrast to other works in few-shot and zero-shot action\nrecognition, we a) utilise attention mechanisms so as to perform temporal\nalignment, and b) learn a deep-distance measure on the aligned representations\nat video segment level. We adopt an episode-based training scheme and train our\nnetwork in an end-to-end manner. The proposed method does not require any\nfine-tuning in the target domain or maintaining additional representations as\nis the case of memory networks. Experimental results show that the proposed\narchitecture outperforms the state of the art in few-shot action recognition,\nand achieves competitive results in zero-shot action recognition.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 19:52:24 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Bishay", "Mina", ""], ["Zoumpourlis", "Georgios", ""], ["Patras", "Ioannis", ""]]}, {"id": "1907.09050", "submitter": "Richard Jiang", "authors": "Richard Jiang and Danny Crookes", "title": "Shallow Unorganized Neural Networks using Smart Neuron Model for Visual\n  Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent success of Deep Neural Networks (DNNs) has revealed the\nsignificant capability of neural computing in many challenging applications.\nAlthough DNNs are derived from emulating biological neurons, there still exist\ndoubts over whether or not DNNs are the final and best model to emulate the\nmechanism of human intelligence. In particular, there are two discrepancies\nbetween computational DNN models and the observed facts of biological neurons.\nFirst, human neurons are interconnected randomly, while DNNs need\ncarefully-designed architectures to work properly. Second, human neurons\nusually have a long spiking latency (~100ms) which implies that not many layers\ncan be involved in making a decision, while DNNs could have hundreds of layers\nto guarantee high accuracy. In this paper, we propose a new computational\nmodel, namely shallow unorganized neural networks (SUNNs), in contrast to\nANNs/DNNs. The proposed SUNNs differ from standard ANNs or DNNs in three\nfundamental aspects: 1) SUNNs are based on an adaptive neuron cell model, Smart\nNeurons, that allows each artificial neuron cell to adaptively respond to its\ninputs rather than carrying out a fixed weighted-sum operation like the classic\nneuron model in ANNs/DNNs; 2) SUNNs can cope with computational tasks with very\nshallow architectures; 3) SUNNs have a natural topology with random\ninterconnections, as the human brain does, and as proposed by Turing's B-type\nunorganized machines. We implemented the proposed SUNN architecture and tested\nit on a number of unsupervised early stage visual perception tasks.\nSurprisingly, such simple shallow architectures achieved very good results in\nour experiments. The success of our new computational model makes it the first\nworkable example of Turing's B-Type unorganized machine that can achieve\ncomparable or better performance against the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 23:09:35 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 23:50:49 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Jiang", "Richard", ""], ["Crookes", "Danny", ""]]}, {"id": "1907.09081", "submitter": "Amir Hossein Raffiee", "authors": "Amir Hossein Raffiee, Humayun Irshad", "title": "Class-specific Anchoring Proposal for 3D Object Recognition in LIDAR and\n  RGB Images", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting objects in a two-dimensional setting is often insufficient in the\ncontext of real-life applications where the surrounding environment needs to be\naccurately recognized and oriented in three-dimension (3D), such as in the case\nof autonomous driving vehicles. Therefore, accurately and efficiently detecting\nobjects in the three-dimensional setting is becoming increasingly relevant to a\nwide range of industrial applications, and thus is progressively attracting the\nattention of researchers. Building systems to detect objects in 3D is a\nchallenging task though, because it relies on the multi-modal fusion of data\nderived from different sources. In this paper, we study the effects of\nanchoring using the current state-of-the-art 3D object detector and propose\nClass-specific Anchoring Proposal (CAP) strategy based on object sizes and\naspect ratios based clustering of anchors. The proposed anchoring strategy\nsignificantly increased detection accuracy's by 7.19%, 8.13% and 8.8% on Easy,\nModerate and Hard setting of the pedestrian class, 2.19%, 2.17% and 1.27% on\nEasy, Moderate and Hard setting of the car class and 12.1% on Easy setting of\ncyclist class. We also show that the clustering in anchoring process also\nenhances the performance of the regional proposal network in proposing regions\nof interests significantly. Finally, we propose the best cluster numbers for\neach class of objects in KITTI dataset that improves the performance of\ndetection model significantly.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 02:02:06 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Raffiee", "Amir Hossein", ""], ["Irshad", "Humayun", ""]]}, {"id": "1907.09085", "submitter": "Jianbo Yuan", "authors": "Jianbo Yuan, Haofu Liao, Rui Luo, Jiebo Luo", "title": "Automatic Radiology Report Generation based on Multi-view Image Fusion\n  and Medical Concept Enrichment", "comments": null, "journal-ref": "MICCAI 2019", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating radiology reports is time-consuming and requires extensive\nexpertise in practice. Therefore, reliable automatic radiology report\ngeneration is highly desired to alleviate the workload. Although deep learning\ntechniques have been successfully applied to image classification and image\ncaptioning tasks, radiology report generation remains challenging in regards to\nunderstanding and linking complicated medical visual contents with accurate\nnatural language descriptions. In addition, the data scales of open-access\ndatasets that contain paired medical images and reports remain very limited. To\ncope with these practical challenges, we propose a generative encoder-decoder\nmodel and focus on chest x-ray images and reports with the following\nimprovements. First, we pretrain the encoder with a large number of chest x-ray\nimages to accurately recognize 14 common radiographic observations, while\ntaking advantage of the multi-view images by enforcing the cross-view\nconsistency. Second, we synthesize multi-view visual features based on a\nsentence-level attention mechanism in a late fusion fashion. In addition, in\norder to enrich the decoder with descriptive semantics and enforce the\ncorrectness of the deterministic medical-related contents such as mentions of\norgans or diagnoses, we extract medical concepts based on the radiology reports\nin the training data and fine-tune the encoder to extract the most frequent\nmedical concepts from the x-ray images. Such concepts are fused with each\ndecoding step by a word-level attention model. The experimental results\nconducted on the Indiana University Chest X-Ray dataset demonstrate that the\nproposed model achieves the state-of-the-art performance compared with other\nbaseline approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 02:25:33 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 00:45:21 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Yuan", "Jianbo", ""], ["Liao", "Haofu", ""], ["Luo", "Rui", ""], ["Luo", "Jiebo", ""]]}, {"id": "1907.09127", "submitter": "Ryo Hachiuma", "authors": "Ryo Hachiuma, Christian Pirchheim, Dieter Schmalstieg, Hideo Saito", "title": "DetectFusion: Detecting and Segmenting Both Known and Unknown Dynamic\n  Objects in Real-time SLAM", "comments": "12 pages, 4 figures, 4 tables, accepted by BMVC 2019 spotlight\n  session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DetectFusion, an RGB-D SLAM system that runs in real-time and can\nrobustly handle semantically known and unknown objects that can move\ndynamically in the scene. Our system detects, segments and assigns semantic\nclass labels to known objects in the scene, while tracking and reconstructing\nthem even when they move independently in front of the monocular camera. In\ncontrast to related work, we achieve real-time computational performance on\nsemantic instance segmentation with a novel method combining 2D object\ndetection and 3D geometric segmentation. In addition, we propose a method for\ndetecting and segmenting the motion of semantically unknown objects, thus\nfurther improving the accuracy of camera tracking and map reconstruction. We\nshow that our method performs on par or better than previous work in terms of\nlocalization and object reconstruction accuracy, while achieving about 20 FPS\neven if the objects are segmented in each frame.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 04:29:32 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Hachiuma", "Ryo", ""], ["Pirchheim", "Christian", ""], ["Schmalstieg", "Dieter", ""], ["Saito", "Hideo", ""]]}, {"id": "1907.09128", "submitter": "Mang Shao", "authors": "Mang Shao, Danhang Tang, Tae-Kyun Kim", "title": "Real-time Background-aware 3D Textureless Object Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a modified fuzzy decision forest for real-time 3D\nobject pose estimation based on typical template representation. We employ an\nextra preemptive background rejector node in the decision forest framework to\nterminate the examination of background locations as early as possible, result\nin a significantly improvement on efficiency. Our approach is also scalable to\nlarge dataset since the tree structure naturally provides a logarithm time\ncomplexity to the number of objects. Finally we further reduce the validation\nstage with a fast breadth-first scheme. The results show that our approach\noutperform the state-of-the-arts on the efficiency while maintaining a\ncomparable accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 04:31:45 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Shao", "Mang", ""], ["Tang", "Danhang", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1907.09138", "submitter": "Xiang Gao", "authors": "Wei Hu, Xiang Gao, Gene Cheung, Zongming Guo", "title": "Feature Graph Learning for 3D Point Cloud Denoising", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.2978617", "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying an appropriate underlying graph kernel that reflects pairwise\nsimilarities is critical in many recent graph spectral signal restoration\nschemes, including image denoising, dequantization, and contrast enhancement.\nExisting graph learning algorithms compute the most likely entries of a\nproperly defined graph Laplacian matrix $\\mathbf{L}$, but require a large\nnumber of signal observations $\\mathbf{z}$'s for a stable estimate. In this\nwork, we assume instead the availability of a relevant feature vector\n$\\mathbf{f}_i$ per node $i$, from which we compute an optimal feature graph via\noptimization of a feature metric. Specifically, we alternately optimize the\ndiagonal and off-diagonal entries of a Mahalanobis distance matrix $\\mathbf{M}$\nby minimizing the graph Laplacian regularizer (GLR) $\\mathbf{z}^{\\top}\n\\mathbf{L} \\mathbf{z}$, where edge weight is $w_{i,j} = \\exp\\{-(\\mathbf{f}_i -\n\\mathbf{f}_j)^{\\top} \\mathbf{M} (\\mathbf{f}_i - \\mathbf{f}_j) \\}$, given a\nsingle observation $\\mathbf{z}$. We optimize diagonal entries via proximal\ngradient (PG), where we constrain $\\mathbf{M}$ to be positive definite (PD) via\nlinear inequalities derived from the Gershgorin circle theorem. To optimize\noff-diagonal entries, we design a block descent algorithm that iteratively\noptimizes one row and column of $\\mathbf{M}$. To keep $\\mathbf{M}$ PD, we\nconstrain the Schur complement of sub-matrix $\\mathbf{M}_{2,2}$ of $\\mathbf{M}$\nto be PD when optimizing via PG. Our algorithm mitigates full\neigen-decomposition of $\\mathbf{M}$, thus ensuring fast computation speed even\nwhen feature vector $\\mathbf{f}_i$ has high dimension. To validate its\nusefulness, we apply our feature graph learning algorithm to the problem of 3D\npoint cloud denoising, resulting in state-of-the-art performance compared to\ncompeting schemes in extensive experiments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 05:02:12 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 02:11:27 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Hu", "Wei", ""], ["Gao", "Xiang", ""], ["Cheung", "Gene", ""], ["Guo", "Zongming", ""]]}, {"id": "1907.09140", "submitter": "Jingru Yi", "authors": "Jingru Yi, Pengxiang Wu, Qiaoying Huang, Hui Qu, Bo Liu, Daniel J.\n  Hoeppner, Dimitris N. Metaxas", "title": "Multi-scale Cell Instance Segmentation with Keypoint Graph based\n  Bounding Boxes", "comments": "accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing methods handle cell instance segmentation problems directly\nwithout relying on additional detection boxes. These methods generally fails to\nseparate touching cells due to the lack of global understanding of the objects.\nIn contrast, box-based instance segmentation solves this problem by combining\nobject detection with segmentation. However, existing methods typically utilize\nanchor box-based detectors, which would lead to inferior instance segmentation\nperformance due to the class imbalance issue. In this paper, we propose a new\nbox-based cell instance segmentation method. In particular, we first detect the\nfive pre-defined points of a cell via keypoints detection. Then we group these\npoints according to a keypoint graph and subsequently extract the bounding box\nfor each cell. Finally, cell segmentation is performed on feature maps within\nthe bounding boxes. We validate our method on two cell datasets with distinct\nobject shapes, and empirically demonstrate the superiority of our method\ncompared to other instance segmentation techniques. Code is available at:\nhttps://github.com/yijingru/KG_Instance_Segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 05:25:01 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 18:01:59 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Yi", "Jingru", ""], ["Wu", "Pengxiang", ""], ["Huang", "Qiaoying", ""], ["Qu", "Hui", ""], ["Liu", "Bo", ""], ["Hoeppner", "Daniel J.", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1907.09160", "submitter": "Li Liu", "authors": "Chengyu Guo, Jingyun Liang, Geng Zhan, Zhong Liu, Matti Pietik\\\"ainen,\n  and Li Liu", "title": "Extended Local Binary Patterns for Efficient and Robust Spontaneous\n  Facial Micro-Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Micro-Expressions (MEs) are spontaneous, involuntary facial movements\nwhen a person experiences an emotion but deliberately or unconsciously attempts\nto conceal his or her genuine emotions. Recently, ME recognition has attracted\nincreasing attention due to its potential applications such as clinical\ndiagnosis, business negotiation, interrogations, and security. However, it is\nexpensive to build large scale ME datasets, mainly due to the difficulty of\ninducing spontaneous MEs. This limits the application of deep learning\ntechniques which require lots of training data. In this paper, we propose a\nsimple, efficient yet robust descriptor called Extended Local Binary Patterns\non Three Orthogonal Planes (ELBPTOP) for ME recognition. ELBPTOP consists of\nthree complementary binary descriptors: LBPTOP and two novel ones Radial\nDifference LBPTOP (RDLBPTOP) and Angular Difference LBPTOP (ADLBPTOP), which\nexplore the local second order information along the radial and angular\ndirections contained in ME video sequences. ELBPTOP is a novel ME descriptor\ninspired by unique and subtle facial movements. It is computationally efficient\nand only marginally increases the cost of computing LBPTOP, yet is extremely\neffective for ME recognition. In addition, by firstly introducing Whitened\nPrincipal Component Analysis (WPCA) to ME recognition, we can further obtain\nmore compact and discriminative feature representations, then achieve\nsignificantly computational savings. Extensive experimental evaluation on three\npopular spontaneous ME datasets SMIC, CASME II and SAMM show that our proposed\nELBPTOP approach significantly outperforms the previous state-of-the-art on all\nthree single evaluated datasets and achieves promising results on\ncross-database recognition.Our code will be made available.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 07:15:53 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 12:34:48 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Guo", "Chengyu", ""], ["Liang", "Jingyun", ""], ["Zhan", "Geng", ""], ["Liu", "Zhong", ""], ["Pietik\u00e4inen", "Matti", ""], ["Liu", "Li", ""]]}, {"id": "1907.09167", "submitter": "Dmitri Kovalenko", "authors": "Dmitri Kovalenko, Mikhail Korobkin, Andrey Minin", "title": "Sensor Aware Lidar Odometry", "comments": "to appear in European Conference on Mobile Robots 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lidar odometry method, integrating into the computation the knowledge about\nthe physics of the sensor, is proposed. A model of measurement error enables\nhigher precision in estimation of the point normal covariance. Adjacent laser\nbeams are used in an outlier correspondence rejection scheme. The method is\nranked in the KITTI's leaderboard with 1.37% positioning error. 3.67% is\nachieved in comparison with the LOAM method on the internal dataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 07:39:12 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 15:36:09 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 10:26:52 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Kovalenko", "Dmitri", ""], ["Korobkin", "Mikhail", ""], ["Minin", "Andrey", ""]]}, {"id": "1907.09180", "submitter": "Hemin Ali Qadir", "authors": "Hemin Ali Qadir, Younghak Shin, Johannes Solhusvik, Jacob Bergsland,\n  Lars Aabakken, Ilangko Balasingham", "title": "Polyp Detection and Segmentation using Mask R-CNN: Does a Deeper Feature\n  Extractor CNN Always Perform Better?", "comments": "6", "journal-ref": "2019 13th International Symposium on Medical Information and\n  Communication Technology (ISMICT)", "doi": "10.1109/ISMICT.2019.8743694", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic polyp detection and segmentation are highly desirable for colon\nscreening due to polyp miss rate by physicians during colonoscopy, which is\nabout 25%. However, this computerization is still an unsolved problem due to\nvarious polyp-like structures in the colon and high interclass polyp variations\nin terms of size, color, shape, and texture. In this paper, we adapt Mask R-CNN\nand evaluate its performance with different modern convolutional neural\nnetworks (CNN) as its feature extractor for polyp detection and segmentation.\nWe investigate the performance improvement of each feature extractor by adding\nextra polyp images to the training dataset to answer whether we need deeper and\nmore complex CNNs or better dataset for training in automatic polyp detection\nand segmentation. Finally, we propose an ensemble method for further\nperformance improvement. We evaluate the performance on the 2015 MICCAI polyp\ndetection dataset. The best results achieved are 72.59% recall, 80% precision,\n70.42% dice, and 61.24% Jaccard. The model achieved state-of-the-art\nsegmentation performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 08:34:47 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Qadir", "Hemin Ali", ""], ["Shin", "Younghak", ""], ["Solhusvik", "Johannes", ""], ["Bergsland", "Jacob", ""], ["Aabakken", "Lars", ""], ["Balasingham", "Ilangko", ""]]}, {"id": "1907.09194", "submitter": "Binbin Yang", "authors": "Binbin Yang and Weiwei Zhang", "title": "FD-FCN: 3D Fully Dense and Fully Convolutional Network for Semantic\n  Segmentation of Brain Anatomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a 3D patch-based fully dense and fully convolutional network\n(FD-FCN) is proposed for fast and accurate segmentation of subcortical\nstructures in T1-weighted magnetic resonance images. Developed from the seminal\nFCN with an end-to-end learning-based approach and constructed by newly\ndesigned dense blocks including a dense fully-connected layer, the proposed\nFD-FCN is different from other FCN-based methods and leads to an outperformance\nin the perspective of both efficiency and accuracy. Compared with the U-shaped\narchitecture, FD-FCN discards the upsampling path for model fitness. To\nalleviate the problem of parameter explosion, the inputs of dense blocks are no\nlonger directly passed to subsequent layers. This architecture of FD-FCN brings\na great reduction on both memory and time consumption in training process.\nAlthough FD-FCN is slimmed down, in model competence it gains better capability\nof dense inference than other conventional networks. This benefits from the\nconstruction of network architecture and the incorporation of redesigned dense\nblocks. The multi-scale FD-FCN models both local and global context by\nembedding intermediate-layer outputs in the final prediction, which encourages\nconsistency between features extracted at different scales and embeds\nfine-grained information directly in the segmentation process. In addition,\ndense blocks are rebuilt to enlarge the receptive fields without significantly\nincreasing parameters, and spectral coordinates are exploited for spatial\ncontext of the original input patch. The experiments were performed over the\nIBSR dataset, and FD-FCN produced an accurate segmentation result of overall\nDice overlap value of 89.81% for 11 brain structures in 53 seconds, with at\nleast 3.66% absolute improvement of dice accuracy than state-of-the-art 3D\nFCN-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 09:19:05 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 10:39:17 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Yang", "Binbin", ""], ["Zhang", "Weiwei", ""]]}, {"id": "1907.09200", "submitter": "Ben Glocker", "authors": "Matthew C.H. Lee, Ozan Oktay, Andreas Schuh, Michiel Schaap, Ben\n  Glocker", "title": "Image-and-Spatial Transformer Networks for Structure-Guided Image\n  Registration", "comments": "Accepted at MICCAI 2019. Code available on\n  https://github.com/biomedia-mira/istn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration with deep neural networks has become an active field of\nresearch and exciting avenue for a long standing problem in medical imaging.\nThe goal is to learn a complex function that maps the appearance of input image\npairs to parameters of a spatial transformation in order to align corresponding\nanatomical structures. We argue and show that the current direct, non-iterative\napproaches are sub-optimal, in particular if we seek accurate alignment of\nStructures-of-Interest (SoI). Information about SoI is often available at\ntraining time, for example, in form of segmentations or landmarks. We introduce\na novel, generic framework, Image-and-Spatial Transformer Networks (ISTNs), to\nleverage SoI information allowing us to learn new image representations that\nare optimised for the downstream registration task. Thanks to these\nrepresentations we can employ a test-specific, iterative refinement over the\ntransformation parameters which yields highly accurate registration even with\nvery limited training data. Performance is demonstrated on pairwise 3D brain\nregistration and illustrative synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 09:39:53 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Lee", "Matthew C. H.", ""], ["Oktay", "Ozan", ""], ["Schuh", "Andreas", ""], ["Schaap", "Michiel", ""], ["Glocker", "Ben", ""]]}, {"id": "1907.09217", "submitter": "Hui Yuan", "authors": "Hui Yuan, Mengyu Li, Junhui Hou, Jimin Xiao", "title": "Single Image based Head Pose Estimation with Spherical Parameterization\n  and 3D Morphing", "comments": "34pages, 5figures, Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head pose estimation plays a vital role in various applications, e.g.,\ndriverassistance systems, human-computer interaction, virtual reality\ntechnology, and so on. We propose a novel geometry based algorithm for\naccurately estimating the head pose from a single 2D face image at a very low\ncomputational cost. Specifically, the rectangular coordinates of only four\nnon-coplanar feature points from a predefined 3D facial model as well as the\ncorresponding ones automatically/ manually extracted from a 2D face image are\nfirst normalized to exclude the effect of external factors (i.e., scale factor\nand translation parameters). Then, the four normalized 3D feature points are\nrepresented in spherical coordinates with reference to the uniquely determined\nsphere by themselves. Due to the spherical parameterization, the coordinates of\nfeature points can then be morphed along all the three directions in the\nrectangular coordinates effectively. Finally, the rotation matrix indicating\nthe head pose is obtained by minimizing the Euclidean distance between the\nnormalized 2D feature points and the 2D re-projections of morphed 3D feature\npoints. Comprehensive experimental results over two popular databases, i.e.,\nPointing'04 and Biwi Kinect, demonstrate that the proposed algorithm can\nestimate head poses with higher accuracy and lower run time than\nstate-of-the-art geometry based methods. Even compared with start-of-the-art\nlearning based methods or geometry based methods with additional depth\ninformation, our algorithm still produces comparable performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 10:16:30 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 00:54:54 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2020 14:44:18 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Yuan", "Hui", ""], ["Li", "Mengyu", ""], ["Hou", "Junhui", ""], ["Xiao", "Jimin", ""]]}, {"id": "1907.09233", "submitter": "Hannes Fassold", "authors": "Hannes Fassold", "title": "Adapting Computer Vision Algorithms for Omnidirectional Video", "comments": "Accepted for 27th ACM International Conference on Multimedia (ACMM MM\n  2019, Nice, France)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional (360{\\deg}) video has got quite popular because it provides a\nhighly immersive viewing experience. For computer vision algorithms, it poses\nseveral challenges, like the special (equirectangular) projection commonly\nemployed and the huge image size. In this work, we give a high-level overview\nof these challenges and outline strategies how to adapt computer vision\nalgorithm for the specifics of omnidirectional video.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 11:12:35 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fassold", "Hannes", ""]]}, {"id": "1907.09236", "submitter": "Isaac Ronald Ward", "authors": "Isaac Ronald Ward, Hamid Laga, Mohammed Bennamoun", "title": "RGB-D image-based Object Detection: from Traditional Methods to Deep\n  Learning Techniques", "comments": "Chapter in the book 'RGB-D Image Analysis and Processing' (Paul\n  Rosin)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection from RGB images is a long-standing problem in image\nprocessing and computer vision. It has applications in various domains\nincluding robotics, surveillance, human-computer interaction, and medical\ndiagnosis. With the availability of low cost 3D scanners, a large number of\nRGB-D object detection approaches have been proposed in the past years. This\nchapter provides a comprehensive survey of the recent developments in this\nfield. We structure the chapter into two parts; the focus of the first part is\non techniques that are based on hand-crafted features combined with machine\nlearning algorithms. The focus of the second part is on the more recent work,\nwhich is based on deep learning. Deep learning techniques, coupled with the\navailability of large training datasets, have now revolutionized the field of\ncomputer vision, including RGB-D object detection, achieving an unprecedented\nlevel of performance. We survey the key contributions, summarize the most\ncommonly used pipelines, discuss their benefits and limitations, and highlight\nsome important directions for future research.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 11:18:01 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Ward", "Isaac Ronald", ""], ["Laga", "Hamid", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "1907.09245", "submitter": "Kaan Karaman", "authors": "Kaan Karaman, Erhan Gundogdu, Aykut Koc, A. Aydin Alatan", "title": "Quadruplet Selection Methods for Deep Embedding Learning", "comments": "6 pages, 2 figures, accepted by IEEE ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of objects with subtle differences has been used in many\npractical applications, such as car model recognition and maritime vessel\nidentification. For discrimination of the objects in fine-grained detail, we\nfocus on deep embedding learning by using a multi-task learning framework, in\nwhich the hierarchical labels (coarse and fine labels) of the samples are\nutilized both for classification and a quadruplet-based loss function. In order\nto improve the recognition strength of the learned features, we present a novel\nfeature selection method specifically designed for four training samples of a\nquadruplet. By experiments, it is observed that the selection of very hard\nnegative samples with relatively easy positive ones from the same coarse and\nfine classes significantly increases some performance metrics in a fine-grained\ndataset when compared to selecting the quadruplet samples randomly. The feature\nembedding learned by the proposed method achieves favorable performance against\nits state-of-the-art counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 11:39:15 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Karaman", "Kaan", ""], ["Gundogdu", "Erhan", ""], ["Koc", "Aykut", ""], ["Alatan", "A. Aydin", ""]]}, {"id": "1907.09254", "submitter": "Anjany Kumar Sekuboyina", "authors": "Anjany Sekuboyina, Markus Rempfler, Alexander Valentinitsch,\n  Maximilian Loeffler, Jan S. Kirschke, and Bjoern H. Menze", "title": "Probabilistic Point Cloud Reconstructions for Vertebral Shape Analysis", "comments": "Accepted at Medical Image Computing and Computer-Assisted\n  Intervention (MICCAI), 2019; JSK and BHM are joint supervising authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an auto-encoding network architecture for point clouds (PC)\ncapable of extracting shape signatures without supervision. Building on this,\nwe (i) design a loss function capable of modelling data variance on PCs which\nare unstructured, and (ii) regularise the latent space as in a variational\nauto-encoder, both of which increase the auto-encoders' descriptive capacity\nwhile making them probabilistic. Evaluating the reconstruction quality of our\narchitectures, we employ them for detecting vertebral fractures without any\nsupervision. By learning to efficiently reconstruct only healthy vertebrae,\nfractures are detected as anomalous reconstructions. Evaluating on a dataset\ncontaining $\\sim$1500 vertebrae, we achieve area-under-ROC curve of $>$75%,\nwithout using intensity-based features.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 12:08:19 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 19:23:18 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Sekuboyina", "Anjany", ""], ["Rempfler", "Markus", ""], ["Valentinitsch", "Alexander", ""], ["Loeffler", "Maximilian", ""], ["Kirschke", "Jan S.", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1907.09296", "submitter": "Alfonso Alba", "authors": "Edgar R. Arce-Santana, Alfonso Alba, Martin O. Mendez, Valdemar\n  Arce-Guevara", "title": "A-Phase classification using convolutional neural networks", "comments": "19 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A series of short events, called A-phases, can be observed in the human\nelectroencephalogram during NREM sleep. These events can be classified in three\ngroups (A1, A2 and A3) according to their spectral contents, and are thought to\nplay a role in the transitions between the different sleep stages. A-phase\ndetection and classification is usually performed manually by a trained expert,\nbut it is a tedious and time-consuming task. In the past two decades, various\nresearchers have designed algorithms to automatically detect and classify the\nA-phases with varying degrees of success, but the problem remains open. In this\npaper, a different approach is proposed: instead of attempting to design a\ngeneral classifier for all subjects, we propose to train ad-hoc classifiers for\neach subject using as little data as possible, in order to drastically reduce\nthe amount of time required from the expert. The proposed classifiers are based\non deep convolutional neural networks using the log-spectrogram of the EEG\nsignal as input data. Results are encouraging, achieving average accuracies of\n80.31% when discriminating between A-phases and non A-phases, and 71.87% when\nclassifying among A-phase sub-types, with only 25% of the total A-phases used\nfor training. When additional expert-validated data is considered, the sub-type\nclassification accuracy increases to 78.92%. These results show that a\nsemi-automatic annotation system with assistance from an expert could provide a\nbetter alternative to fully automatic classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 13:10:38 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Arce-Santana", "Edgar R.", ""], ["Alba", "Alfonso", ""], ["Mendez", "Martin O.", ""], ["Arce-Guevara", "Valdemar", ""]]}, {"id": "1907.09314", "submitter": "Prashanth B", "authors": "B.U.V Prashanth, Mohammed Riyaz Ahmed", "title": "Artificial Neural Network Algorithm based Skyrmion Material Design of\n  Chiral Crystals", "comments": "8 Pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model presented in this research predicts ideal chiral crystal and\npropose a new direction of designing chiral crystals. Skyrmions are\ntopologically protected and structurally assymetric materials with an exotic\nspin composition. This work presents deep learning method for skyrmion material\ndesign of chiral crystals. This paper presents an approach to construct a\nprobabilistic classifier and an Artificial Neural Network(ANN) from a true or\nfalse chirality dataset consisting of chiral and achiral compounds with 'A' and\n'B' type elements. A quantitative predictor for accuracy of forming the chiral\ncrystals is illustrated. The feasibility of ANN method is tested in a\ncomprehensive manner by comparing with probalistic classifier method.\nThroughout this manuscript we present deep learnig algorithm design with\nmodelling and simulations of materials. This research work elucidated paves a\nway to develop sophisticated software tool to make an indicator of crystal\ndesign.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 09:07:09 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Prashanth", "B. U. V", ""], ["Ahmed", "Mohammed Riyaz", ""]]}, {"id": "1907.09320", "submitter": "Guangcun Shan", "authors": "Hongyu Wang, Wei Liang, Guangcun Shan", "title": "An Efficient Method of Detection and Recognition in Remote Sensing Image\n  Based on multi-angle Region of Interests", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presently, deep learning technology has been widely used in the field of\nimage recognition. However, it mainly aims at the recognition and detection of\nordinary pictures and common scenes. As special images, remote sensing images\nhave different shooting angles and shooting methods compared with ordinary\nones, which makes remote sensing images play an irreplaceable role in some\nareas. In this paper, based on a deep convolution neural network for providing\nmulti-level information of images and combines RPN (Region Proposal Network)\nfor generating multi-angle ROIs (Region of Interest), a new model for object\ndetection and recognition in remote sensing images is proposed. In the\nexperiment, it achieves better results than traditional ways, which demonstrate\nthat the model proposed here would have a huge potential application in remote\nsensing image recognition.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 13:48:05 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Wang", "Hongyu", ""], ["Liang", "Wei", ""], ["Shan", "Guangcun", ""]]}, {"id": "1907.09340", "submitter": "Pranava Madhyastha", "authors": "Pranava Madhyastha, Josiah Wang, Lucia Specia", "title": "VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions", "comments": "Accepted for publication at ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the task of evaluating image description generation systems. We\npropose a novel image-aware metric for this task: VIFIDEL. It estimates the\nfaithfulness of a generated caption with respect to the content of the actual\nimage, based on the semantic similarity between labels of objects depicted in\nimages and words in the description. The metric is also able to take into\naccount the relative importance of objects mentioned in human reference\ndescriptions during evaluation. Even if these human reference descriptions are\nnot available, VIFIDEL can still reliably evaluate system descriptions. The\nmetric achieves high correlation with human judgments on two well-known\ndatasets and is competitive with metrics that depend on human references\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 14:33:43 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Madhyastha", "Pranava", ""], ["Wang", "Josiah", ""], ["Specia", "Lucia", ""]]}, {"id": "1907.09358", "submitter": "Aditya Mogadala", "authors": "Aditya Mogadala and Marimuthu Kalimuthu and Dietrich Klakow", "title": "Trends in Integration of Vision and Language Research: A Survey of\n  Tasks, Datasets, and Methods", "comments": "Accepted at Journal of Artificial Intelligence Research (JAIR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest in Artificial Intelligence (AI) and its applications has seen\nunprecedented growth in the last few years. This success can be partly\nattributed to the advancements made in the sub-fields of AI such as Machine\nLearning (ML), Computer Vision (CV), and Natural Language Processing (NLP). The\nlargest of the growths in these fields has been made possible with deep\nlearning, a sub-area of machine learning, which uses the principles of\nartificial neural networks. This has created significant interest in the\nintegration of vision and language. The tasks are designed such that they\nperfectly embrace the ideas of deep learning. In this survey, we focus on ten\nprominent tasks that integrate language and vision by discussing their problem\nformulations, methods, existing datasets, evaluation measures, and compare the\nresults obtained with corresponding state-of-the-art methods. Our efforts go\nbeyond earlier surveys which are either task-specific or concentrate only on\none type of visual content, i.e., image or video. Furthermore, we also provide\nsome potential future directions in this field of research with an anticipation\nthat this survey brings in innovative thoughts and ideas to address the\nexisting challenges and build new applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 14:53:48 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 13:26:29 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Mogadala", "Aditya", ""], ["Kalimuthu", "Marimuthu", ""], ["Klakow", "Dietrich", ""]]}, {"id": "1907.09380", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Amirali Abdolrashidi", "title": "DeepIris: Iris Recognition Using A Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris recognition has been an active research area during last few decades,\nbecause of its wide applications in security, from airports to homeland\nsecurity border control. Different features and algorithms have been proposed\nfor iris recognition in the past. In this paper, we propose an end-to-end deep\nlearning framework for iris recognition based on residual convolutional neural\nnetwork (CNN), which can jointly learn the feature representation and perform\nrecognition. We train our model on a well-known iris recognition dataset using\nonly a few training images from each class, and show promising results and\nimprovements over previous approaches. We also present a visualization\ntechnique which is able to detect the important areas in iris images which can\nmostly impact the recognition results. We believe this framework can be widely\nused for other biometrics recognition tasks, helping to have a more scalable\nand accurate systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 15:48:48 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "Amirali", ""]]}, {"id": "1907.09381", "submitter": "Wenxi Liu", "authors": "Xiaosheng Yan, Yuanlong Yu, Feigege Wang, Wenxi Liu, Shengfeng He, Jia\n  Pan", "title": "Visualizing the Invisible: Occluded Vehicle Segmentation and Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel iterative multi-task framework to complete\nthe segmentation mask of an occluded vehicle and recover the appearance of its\ninvisible parts. In particular, to improve the quality of the segmentation\ncompletion, we present two coupled discriminators and introduce an auxiliary 3D\nmodel pool for sampling authentic silhouettes as adversarial samples. In\naddition, we propose a two-path structure with a shared network to enhance the\nappearance recovery capability. By iteratively performing the segmentation\ncompletion and the appearance recovery, the results will be progressively\nrefined. To evaluate our method, we present a dataset, the Occluded Vehicle\ndataset, containing synthetic and real-world occluded vehicle images. We\nconduct comparison experiments on this dataset and demonstrate that our model\noutperforms the state-of-the-art in tasks of recovering segmentation mask and\nappearance for occluded vehicles. Moreover, we also demonstrate that our\nappearance recovery approach can benefit the occluded vehicle tracking in\nreal-world videos.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 15:49:03 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Yan", "Xiaosheng", ""], ["Yu", "Yuanlong", ""], ["Wang", "Feigege", ""], ["Liu", "Wenxi", ""], ["He", "Shengfeng", ""], ["Pan", "Jia", ""]]}, {"id": "1907.09382", "submitter": "Huseyin Coskun", "authors": "Huseyin Coskun, Zeeshan Zia, Bugra Tekin, Federica Bogo, Nassir Navab,\n  Federico Tombari, Harpreet Sawhney", "title": "Domain-Specific Priors and Meta Learning for Few-Shot First-Person\n  Action Recognition", "comments": "Paper has been accepted in Transactions on Pattern Analysis and\n  Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of large-scale real datasets with annotations makes transfer\nlearning a necessity for video activity understanding. We aim to develop an\neffective method for few-shot transfer learning for first-person action\nclassification. We leverage independently trained local visual cues to learn\nrepresentations that can be transferred from a source domain, which provides\nprimitive action labels, to a different target domain -- using only a handful\nof examples. Visual cues we employ include object-object interactions, hand\ngrasps and motion within regions that are a function of hand locations. We\nemploy a framework based on meta-learning to extract the distinctive and domain\ninvariant components of the deployed visual cues. This enables transfer of\naction classification models across public datasets captured with diverse scene\nand action configurations. We present comparative results of our transfer\nlearning methodology and report superior results over state-of-the-art action\nclassification approaches for both inter-class and inter-dataset transfer.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 15:52:21 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 20:38:16 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Coskun", "Huseyin", ""], ["Zia", "Zeeshan", ""], ["Tekin", "Bugra", ""], ["Bogo", "Federica", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""], ["Sawhney", "Harpreet", ""]]}, {"id": "1907.09394", "submitter": "Divyaa Ravichandran", "authors": "Hallee E. Wong and Osman Akar and Emmanuel Antonio Cuevas and Iuliana\n  Tabian and Divyaa Ravichandran and Iris Fu and Cambron Carter", "title": "Markerless Augmented Advertising for Sports Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Markerless augmented reality can be a challenging computer vision task,\nespecially in live broadcast settings and in the absence of information related\nto the video capture such as the intrinsic camera parameters. This typically\nrequires the assistance of a skilled artist, along with the use of advanced\nvideo editing tools in a post-production environment. We present an automated\nvideo augmentation pipeline that identifies textures of interest and overlays\nan advertisement onto these regions. We constrain the advertisement to be\nplaced in a way that is aesthetic and natural. The aim is to augment the scene\nsuch that there is no longer a need for commercial breaks. In order to achieve\nseamless integration of the advertisement with the original video we build a 3D\nrepresentation of the scene, place the advertisement in 3D, and then project it\nback onto the image plane. After successful placement in a single frame, we use\nhomography-based, shape-preserving tracking such that the advertisement appears\nperspective correct for the duration of a video clip. The tracker is designed\nto handle smooth camera motion and shot boundaries.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 16:10:34 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Wong", "Hallee E.", ""], ["Akar", "Osman", ""], ["Cuevas", "Emmanuel Antonio", ""], ["Tabian", "Iuliana", ""], ["Ravichandran", "Divyaa", ""], ["Fu", "Iris", ""], ["Carter", "Cambron", ""]]}, {"id": "1907.09404", "submitter": "Alessandro Lameiras Koerich", "authors": "Kelly Lais Wiggers, Alceu de Souza Britto Junior, Alessandro Lameiras\n  Koerich, Laurent Heutte, Luiz Eduardo Soares de Oliveira", "title": "Deep Learning Approaches for Image Retrieval and Pattern Spotting in\n  Ancient Documents", "comments": "The paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes two approaches for content-based image retrieval and\npattern spotting in document images using deep learning. The first approach\nuses a pre-trained CNN model to cope with the lack of training data, which is\nfine-tuned to achieve a compact yet discriminant representation of queries and\nimage candidates. The second approach uses a Siamese Convolution Neural Network\ntrained on a previously prepared subset of image pairs from the ImageNet\ndataset to provide the similarity-based feature maps. In both methods, the\nlearned representation scheme considers feature maps of different sizes which\nare evaluated in terms of retrieval performance. A robust experimental protocol\nusing two public datasets (Tobacoo-800 and DocExplore) has shown that the\nproposed methods compare favorably against state-of-the-art document image\nretrieval and pattern spotting methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 16:27:19 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Wiggers", "Kelly Lais", ""], ["Junior", "Alceu de Souza Britto", ""], ["Koerich", "Alessandro Lameiras", ""], ["Heutte", "Laurent", ""], ["de Oliveira", "Luiz Eduardo Soares", ""]]}, {"id": "1907.09408", "submitter": "Fan Zhang", "authors": "Licheng Jiao, Fan Zhang, Fang Liu, Shuyuan Yang, Lingling Li, Zhixi\n  Feng, and Rong Qu", "title": "A Survey of Deep Learning-based Object Detection", "comments": "30 pages,12 figures", "journal-ref": "05 September 2019", "doi": "10.1109/ACCESS.2019.2939201", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is one of the most important and challenging branches of\ncomputer vision, which has been widely applied in peoples life, such as\nmonitoring security, autonomous driving and so on, with the purpose of locating\ninstances of semantic objects of a certain class. With the rapid development of\ndeep learning networks for detection tasks, the performance of object detectors\nhas been greatly improved. In order to understand the main development status\nof object detection pipeline, thoroughly and deeply, in this survey, we first\nanalyze the methods of existing typical detection models and describe the\nbenchmark datasets. Afterwards and primarily, we provide a comprehensive\noverview of a variety of object detection methods in a systematic manner,\ncovering the one-stage and two-stage detectors. Moreover, we list the\ntraditional and new applications. Some representative branches of object\ndetection are analyzed as well. Finally, we discuss the architecture of\nexploiting these object detection methods to build an effective and efficient\nsystem and point out a set of development trends to better follow the\nstate-of-the-art algorithms and further research.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 04:53:09 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 17:09:59 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Jiao", "Licheng", ""], ["Zhang", "Fan", ""], ["Liu", "Fang", ""], ["Yang", "Shuyuan", ""], ["Li", "Lingling", ""], ["Feng", "Zhixi", ""], ["Qu", "Rong", ""]]}, {"id": "1907.09423", "submitter": "Eleonora Bernasconi", "authors": "Eleonora Bernasconi, Francesco Pugliese, Diego Zardetto, Monica\n  Scannapieco", "title": "Satellite-Net: Automatic Extraction of Land Cover Indicators from\n  Satellite Imagery by Deep Learning", "comments": "New Techniques and Technologies for Statistics 2019, Brussels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we address the challenge of land cover classification for\nsatellite images via Deep Learning (DL). Land Cover aims to detect the physical\ncharacteristics of the territory and estimate the percentage of land occupied\nby a certain category of entities: vegetation, residential buildings,\nindustrial areas, forest areas, rivers, lakes, etc. DL is a new paradigm for\nBig Data analytics and in particular for Computer Vision. The application of DL\nin images classification for land cover purposes has a great potential owing to\nthe high degree of automation and computing performance. In particular, the\ninvention of Convolution Neural Networks (CNNs) was a fundament for the\nadvancements in this field. In [1], the Satellite Task Team of the UN Global\nWorking Group describes the results achieved so far with respect to the use of\nearth observation for Official Statistics. However, in that study, CNNs have\nnot yet been explored for automatic classification of imagery. This work\ninvestigates the usage of CNNs for the estimation of land cover indicators,\nproviding evidence of the first promising results. In particular, the paper\nproposes a customized model, called Satellite-Net, able to reach an accuracy\nlevel up to 98% on test sets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 16:50:35 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Bernasconi", "Eleonora", ""], ["Pugliese", "Francesco", ""], ["Zardetto", "Diego", ""], ["Scannapieco", "Monica", ""]]}, {"id": "1907.09425", "submitter": "Chen Qin", "authors": "Chen Qin, Jo Schlemper, Jinming Duan, Gavin Seegoolam, Anthony Price,\n  Joseph Hajnal, Daniel Rueckert", "title": "k-t NEXT: Dynamic MR Image Reconstruction Exploiting Spatio-temporal\n  Correlations", "comments": "This paper is accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic magnetic resonance imaging (MRI) exhibits high correlations in\nk-space and time. In order to accelerate the dynamic MR imaging and to exploit\nk-t correlations from highly undersampled data, here we propose a novel deep\nlearning based approach for dynamic MR image reconstruction, termed k-t NEXT\n(k-t NEtwork with X-f Transform). In particular, inspired by traditional\nmethods such as k-t BLAST and k-t FOCUSS, we propose to reconstruct the true\nsignals from aliased signals in x-f domain to exploit the spatio-temporal\nredundancies. Building on that, the proposed method then learns to recover the\nsignals by alternating the reconstruction process between the x-f space and\nimage space in an iterative fashion. This enables the network to effectively\ncapture useful information and jointly exploit spatio-temporal correlations\nfrom both complementary domains. Experiments conducted on highly undersampled\nshort-axis cardiac cine MRI scans demonstrate that our proposed method\noutperforms the current state-of-the-art dynamic MR reconstruction approaches\nboth quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 16:51:59 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Qin", "Chen", ""], ["Schlemper", "Jo", ""], ["Duan", "Jinming", ""], ["Seegoolam", "Gavin", ""], ["Price", "Anthony", ""], ["Hajnal", "Joseph", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1907.09438", "submitter": "Shao-Yuan Lo", "authors": "Shao-Yuan Lo, Hsueh-Ming Hang, Sheng-Wei Chan, Jing-Jhih Lin", "title": "Multi-Class Lane Semantic Segmentation using Efficient Convolutional\n  Networks", "comments": "Accepted in IEEE International Workshop on Multimedia Signal\n  Processing (MMSP) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane detection plays an important role in a self-driving vehicle. Several\nstudies leverage a semantic segmentation network to extract robust lane\nfeatures, but few of them can distinguish different types of lanes. In this\npaper, we focus on the problem of multi-class lane semantic segmentation. Based\non the observation that the lane is a small-size and narrow-width object in a\nroad scene image, we propose two techniques, Feature Size Selection (FSS) and\nDegressive Dilation Block (DD Block). The FSS allows a network to extract thin\nlane features using appropriate feature sizes. To acquire fine-grained spatial\ninformation, the DD Block is made of a series of dilated convolutions with\ndegressive dilation rates. Experimental results show that the proposed\ntechniques provide obvious improvement in accuracy, while they achieve the same\nor faster inference speed compared to the baseline system, and can run at\nreal-time on high-resolution images.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 17:22:03 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Lo", "Shao-Yuan", ""], ["Hang", "Hsueh-Ming", ""], ["Chan", "Sheng-Wei", ""], ["Lin", "Jing-Jhih", ""]]}, {"id": "1907.09449", "submitter": "Gwenole Quellec", "authors": "Gwenol\\'e Quellec, Mathieu Lamard, Pierre-Henri Conze, Pascale Massin,\n  B\\'eatrice Cochener", "title": "Automatic detection of rare pathologies in fundus photographs using\n  few-shot learning", "comments": null, "journal-ref": "Medical Image Analysis, Volume 61, April 2020, 101660", "doi": "10.1016/j.media.2020.101660", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decades, large datasets of fundus photographs have been collected\nin diabetic retinopathy (DR) screening networks. Through deep learning, these\ndatasets were used to train automatic detectors for DR and a few other frequent\npathologies, with the goal to automate screening. One challenge limits the\nadoption of such systems so far: automatic detectors ignore rare conditions\nthat ophthalmologists currently detect, such as papilledema or anterior\nischemic optic neuropathy. The reason is that standard deep learning requires\ntoo many examples of these conditions. However, this limitation can be\naddressed with few-shot learning, a machine learning paradigm where a\nclassifier has to generalize to a new category not seen in training, given only\na few examples of this category. This paper presents a new few-shot learning\nframework that extends convolutional neural networks (CNNs), trained for\nfrequent conditions, with an unsupervised probabilistic model for rare\ncondition detection. It is based on the observation that CNNs often perceive\nphotographs containing the same anomalies as similar, even though these CNNs\nwere trained to detect unrelated conditions. This observation was based on the\nt-SNE visualization tool, which we decided to incorporate in our probabilistic\nmodel. Experiments on a dataset of 164,660 screening examinations from the\nOPHDIAT screening network show that 37 conditions, out of 41, can be detected\nwith an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938).\nIn particular, this framework significantly outperforms other frameworks for\ndetecting rare conditions, including multitask learning, transfer learning and\nSiamese networks, another few-shot learning solution. We expect these richer\npredictions to trigger the adoption of automated eye pathology screening, which\nwill revolutionize clinical practice in ophthalmology.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 17:48:27 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 13:00:24 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 17:41:05 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Quellec", "Gwenol\u00e9", ""], ["Lamard", "Mathieu", ""], ["Conze", "Pierre-Henri", ""], ["Massin", "Pascale", ""], ["Cochener", "B\u00e9atrice", ""]]}, {"id": "1907.09500", "submitter": "Keming Zhang", "authors": "Keming Zhang and Joshua S. Bloom", "title": "deepCR: Cosmic Ray Rejection with Deep Learning", "comments": "Accepted for publication in ApJ. 12 pages, 6 figures. An open-source\n  Python package, deepCR, which implements the approach in this paper is at\n  https://github.com/profjsb/deepCR. Figures and benchmarks can be reproduced\n  at https://github.com/kmzzhang/deepCR-paper. Blog post describing this work\n  is at https://medium.com/@kemingzhang/deepcr-2fa610655655", "journal-ref": null, "doi": "10.3847/1538-4357/ab3fa6", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosmic ray (CR) identification and replacement are critical components of\nimaging and spectroscopic reduction pipelines involving solid-state detectors.\nWe present deepCR, a deep learning based framework for CR identification and\nsubsequent image inpainting based on the predicted CR mask. To demonstrate the\neffectiveness of this framework, we train and evaluate models on Hubble Space\nTelescope ACS/WFC images of sparse extragalactic fields, globular clusters, and\nresolved galaxies. We demonstrate that at a false positive rate of 0.5%, deepCR\nachieves close to 100% detection rates in both extragalactic and globular\ncluster fields, and 91% in resolved galaxy fields, which is a significant\nimprovement over the current state-of-the-art method LACosmic. Compared to a\nmulticore CPU implementation of LACosmic, deepCR CR mask predictions run up to\n6.5 times faster on CPU and 90 times faster on a single GPU. For image\ninpainting, the mean squared errors of deepCR predictions are 20 times lower in\nglobular cluster fields, 5 times lower in resolved galaxy fields, and 2.5 times\nlower in extragalactic fields, compared to the best performing non-neural\ntechnique tested. We present our framework and the trained models as an\nopen-source Python project, with a simple-to-use API. To facilitate\nreproducibility of the results we also provide a benchmarking codebase.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 18:02:19 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 06:04:47 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Zhang", "Keming", ""], ["Bloom", "Joshua S.", ""]]}, {"id": "1907.09511", "submitter": "Xu Lan", "authors": "Xu Lan, Xiatian Zhu, Shaogang Gong", "title": "Universal Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art person re-identification (re-id) methods depend on\nsupervised model learning with a large set of cross-view identity labelled\ntraining data. Even worse, such trained models are limited to only the\nsame-domain deployment with significantly degraded cross-domain generalization\ncapability, i.e. \"domain specific\". To solve this limitation, there are a\nnumber of recent unsupervised domain adaptation and unsupervised learning\nmethods that leverage unlabelled target domain training data. However, these\nmethods need to train a separate model for each target domain as supervised\nlearning methods. This conventional \"{\\em train once, run once}\" pattern is\nunscalable to a large number of target domains typically encountered in\nreal-world deployments. We address this problem by presenting a \"train once,\nrun everywhere\" pattern industry-scale systems are desperate for. We formulate\na \"universal model learning' approach enabling domain-generic person re-id\nusing only limited training data of a \"{\\em single}\" seed domain. Specifically,\nwe train a universal re-id deep model to discriminate between a set of\ntransformed person identity classes. Each of such classes is formed by applying\na variety of random appearance transformations to the images of that class,\nwhere the transformations simulate the camera viewing conditions of any domains\nfor making the model training domain generic. Extensive evaluations show the\nsuperiority of our method for universal person re-id over a wide variety of\nstate-of-the-art unsupervised domain adaptation and unsupervised learning re-id\nmethods on five standard benchmarks: Market-1501, DukeMTMC, CUHK03, MSMT17, and\nVIPeR.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 18:17:56 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Lan", "Xu", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1907.09554", "submitter": "Ankita Shukla", "authors": "Ankita Shukla, Sarthak Bhagat, Shagun Uppal, Saket Anand, Pavan Turaga", "title": "Product of Orthogonal Spheres Parameterization for Disentangled\n  Representation Learning", "comments": "Accepted at British Machine Vision Conference (BMVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representations that can disentangle explanatory attributes\nunderlying the data improves interpretabilty as well as provides control on\ndata generation. Various learning frameworks such as VAEs, GANs and\nauto-encoders have been used in the literature to learn such representations.\nMost often, the latent space is constrained to a partitioned representation or\nstructured by a prior to impose disentangling. In this work, we advance the use\nof a latent representation based on a product space of Orthogonal Spheres\nPrOSe. The PrOSe model is motivated by the reasoning that latent-variables\nrelated to the physics of image-formation can under certain relaxed assumptions\nlead to spherical-spaces. Orthogonality between the spheres is motivated via\nphysical independence models. Imposing the orthogonal-sphere constraint is much\nsimpler than other complicated physical models, is fairly general and flexible,\nand extensible beyond the factors used to motivate its development. Under\nfurther relaxed assumptions of equal-sized latent blocks per factor, the\nconstraint can be written down in closed form as an ortho-normality term in the\nloss function. We show that our approach improves the quality of\ndisentanglement significantly. We find consistent improvement in\ndisentanglement compared to several state-of-the-art approaches, across several\nbenchmarks and metrics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:20:00 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Shukla", "Ankita", ""], ["Bhagat", "Sarthak", ""], ["Uppal", "Shagun", ""], ["Anand", "Saket", ""], ["Turaga", "Pavan", ""]]}, {"id": "1907.09567", "submitter": "Naftali Cohen", "authors": "Naftali Cohen, Tucker Balch, and Manuela Veloso", "title": "The Effect of Visual Design in Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC q-fin.CP q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial companies continuously analyze the state of the markets to rethink\nand adjust their investment strategies. While the analysis is done on the\ndigital form of data, decisions are often made based on graphical\nrepresentations in white papers or presentation slides. In this study, we\nexamine whether binary decisions are better to be decided based on the numeric\nor the visual representation of the same data. Using two data sets, a matrix of\nnumerical data with spatial dependencies and financial data describing the\nstate of the S&P index, we compare the results of supervised classification\nbased on the original numerical representation and the visual transformation of\nthe same data. We show that, for these data sets, the visual transformation\nresults in higher predictability skill compared to the original form of the\ndata. We suggest thinking of the visual representation of numeric data,\neffectively, as a combination of dimensional reduction and feature engineering\ntechniques. In particular, if the visual layout encapsulates the full\ncomplexity of the data. In this view, thoughtful visual design can guard\nagainst overfitting, or introduce new features -- all of which benefit the\nlearning process, and effectively lead to better recognition of meaningful\npatterns.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:47:56 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 14:41:00 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Cohen", "Naftali", ""], ["Balch", "Tucker", ""], ["Veloso", "Manuela", ""]]}, {"id": "1907.09569", "submitter": "Peiye Liu", "authors": "Peiye Liu, Bo Wu, Huadong Ma, Mingoo Seok", "title": "MemNet: Memory-Efficiency Guided Neural Architecture Search with\n  Augment-Trim learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on automatic neural architectures search have demonstrated\nsignificant performance, competitive to or even better than hand-crafted neural\narchitectures. However, most of the existing network architecture tend to use\nresidual, parallel structures and concatenation block between shallow and deep\nfeatures to construct a large network. This requires large amounts of memory\nfor storing both weights and feature maps. This is challenging for mobile and\nembedded devices since they may not have enough memory to perform inference\nwith the designed large network model. To close this gap, we propose MemNet, an\naugment-trim learning-based neural network search framework that optimizes not\nonly performance but also memory requirement. Specifically, it employs memory\nconsumption based ranking score which forces an upper bound on memory\nconsumption for navigating the search process. Experiment results show that, as\ncompared to the state-of-the-art efficient designing methods, MemNet can find\nan architecture which can achieve competitive accuracy and save an average of\n24.17% on the total memory needed.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:49:53 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 20:12:57 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Liu", "Peiye", ""], ["Wu", "Bo", ""], ["Ma", "Huadong", ""], ["Seok", "Mingoo", ""]]}, {"id": "1907.09578", "submitter": "Andrey Zhmoginov", "authors": "Andrey Zhmoginov, Ian Fischer, Mark Sandler", "title": "Information-Bottleneck Approach to Salient Region Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for learning image attention masks in a\nsemi-supervised setting based on the Information Bottleneck principle. Provided\nwith a set of labeled images, the mask generation model is minimizing mutual\ninformation between the input and the masked image while maximizing the mutual\ninformation between the same masked image and the image label. In contrast with\nother approaches, our attention model produces a Boolean rather than a\ncontinuous mask, entirely concealing the information in masked-out pixels.\nUsing a set of synthetic datasets based on MNIST and CIFAR10 and the SVHN\ndatasets, we demonstrate that our method can successfully attend to features\nknown to define the image class.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:13:30 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 22:14:54 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhmoginov", "Andrey", ""], ["Fischer", "Ian", ""], ["Sandler", "Mark", ""]]}, {"id": "1907.09594", "submitter": "Jungseock Joo", "authors": "Nan Xi, Di Ma, Marcus Liou, Zachary C. Steinert-Threlkeld, Jason\n  Anastasopoulos, Jungseock Joo", "title": "Understanding the Political Ideology of Legislators from Social Media\n  Images", "comments": "To appear in the Proceedings of International AAAI Conference on Web\n  and Social Media (ICWSM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.HC cs.MM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we seek to understand how politicians use images to express\nideological rhetoric through Facebook images posted by members of the U.S.\nHouse and Senate. In the era of social media, politics has become saturated\nwith imagery, a potent and emotionally salient form of political rhetoric which\nhas been used by politicians and political organizations to influence public\nsentiment and voting behavior for well over a century. To date, however, little\nis known about how images are used as political rhetoric. Using deep learning\ntechniques to automatically predict Republican or Democratic party affiliation\nsolely from the Facebook photographs of the members of the 114th U.S. Congress,\nwe demonstrate that predicted class probabilities from our model function as an\naccurate proxy of the political ideology of images along a left-right\n(liberal-conservative) dimension. After controlling for the gender and race of\npoliticians, our method achieves an accuracy of 59.28% from single photographs\nand 82.35% when aggregating scores from multiple photographs (up to 150) of the\nsame person. To better understand image content distinguishing liberal from\nconservative images, we also perform in-depth content analyses of the\nphotographs. Our findings suggest that conservatives tend to use more images\nsupporting status quo political institutions and hierarchy maintenance,\nfeaturing individuals from dominant social groups, and displaying greater\nhappiness than liberals.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:43:49 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Xi", "Nan", ""], ["Ma", "Di", ""], ["Liou", "Marcus", ""], ["Steinert-Threlkeld", "Zachary C.", ""], ["Anastasopoulos", "Jason", ""], ["Joo", "Jungseock", ""]]}, {"id": "1907.09595", "submitter": "Mingxing Tan", "authors": "Mingxing Tan, Quoc V. Le", "title": "MixConv: Mixed Depthwise Convolutional Kernels", "comments": "BMVC 2019", "journal-ref": "BMVC 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depthwise convolution is becoming increasingly popular in modern efficient\nConvNets, but its kernel size is often overlooked. In this paper, we\nsystematically study the impact of different kernel sizes, and observe that\ncombining the benefits of multiple kernel sizes can lead to better accuracy and\nefficiency. Based on this observation, we propose a new mixed depthwise\nconvolution (MixConv), which naturally mixes up multiple kernel sizes in a\nsingle convolution. As a simple drop-in replacement of vanilla depthwise\nconvolution, our MixConv improves the accuracy and efficiency for existing\nMobileNets on both ImageNet classification and COCO object detection. To\ndemonstrate the effectiveness of MixConv, we integrate it into AutoML search\nspace and develop a new family of models, named as MixNets, which outperform\nprevious mobile models including MobileNetV2 [20] (ImageNet top-1 accuracy\n+4.2%), ShuffleNetV2 [16] (+3.5%), MnasNet [26] (+1.3%), ProxylessNAS [2]\n(+2.2%), and FBNet [27] (+2.0%). In particular, our MixNet-L achieves a new\nstate-of-the-art 78.9% ImageNet top-1 accuracy under typical mobile settings\n(<600M FLOPS). Code is at https://github.com/\ntensorflow/tpu/tree/master/models/official/mnasnet/mixnet\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:49:25 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 01:43:48 GMT"}, {"version": "v3", "created": "Sun, 1 Dec 2019 06:29:57 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Tan", "Mingxing", ""], ["Le", "Quoc V.", ""]]}, {"id": "1907.09624", "submitter": "Sarkhan Badirli", "authors": "Sarkhan Badirli, Zeynep Akata, Murat Dundar", "title": "Bayesian Zero-Shot Learning", "comments": "Accepted to ECCV 2020, TASK-CV Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object classes that surround us have a natural tendency to emerge at varying\nlevels of abstraction. We propose a Bayesian approach to zero-shot learning\n(ZSL) that introduces the notion of meta-classes and implements a Bayesian\nhierarchy around these classes to effectively blend data likelihood with local\nand global priors. Local priors driven by data from seen classes, i.e. classes\nthat are available at training time, become instrumental in recovering unseen\nclasses, i.e. classes that are missing at training time, in a generalized ZSL\nsetting. Hyperparameters of the Bayesian model offer a convenient way to\noptimize the trade-off between seen and unseen class accuracy in addition to\nguiding other aspects of model fitting. We conduct experiments on seven\nbenchmark datasets including the large scale ImageNet and show that our model\nimproves the current state of the art in the challenging generalized ZSL\nsetting.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 23:14:51 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 18:25:48 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 18:58:55 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Badirli", "Sarkhan", ""], ["Akata", "Zeynep", ""], ["Dundar", "Murat", ""]]}, {"id": "1907.09640", "submitter": "Jing Jin", "authors": "Jing Jin and Junhui Hou and Jie Chen and Sam Kwong and Jingyi Yu", "title": "Light Field Super-resolution via Attention-Guided Fusion of Hybrid\n  Lenses", "comments": "This paper was accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the problem of reconstructing high-resolution light field\n(LF) images from hybrid lenses, including a high-resolution camera surrounded\nby multiple low-resolution cameras. To tackle this challenge, we propose a\nnovel end-to-end learning-based approach, which can comprehensively utilize the\nspecific characteristics of the input from two complementary and parallel\nperspectives. Specifically, one module regresses a spatially consistent\nintermediate estimation by learning a deep multidimensional and cross-domain\nfeature representation; the other one constructs another intermediate\nestimation, which maintains the high-frequency textures, by propagating the\ninformation of the high-resolution view. We finally leverage the advantages of\nthe two intermediate estimations via the learned attention maps, leading to the\nfinal high-resolution LF image. Extensive experiments demonstrate the\nsignificant superiority of our approach over state-of-the-art ones. That is,\nour method not only improves the PSNR by more than 2 dB, but also preserves the\nLF structure much better. To the best of our knowledge, this is the first\nend-to-end deep learning method for reconstructing a high-resolution LF image\nwith a hybrid input. We believe our framework could potentially decrease the\ncost of high-resolution LF data acquisition and also be beneficial to LF data\nstorage and transmission. The code is available at\nhttps://github.com/jingjin25/LFhybridSR-Fusion.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 00:40:39 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 03:02:51 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Jin", "Jing", ""], ["Hou", "Junhui", ""], ["Chen", "Jie", ""], ["Kwong", "Sam", ""], ["Yu", "Jingyi", ""]]}, {"id": "1907.09642", "submitter": "Wei Liu", "authors": "Wei Liu, Pingping Zhang, Yinjie Lei, Xiaolin Huang, Jie Yang and Ian\n  Reid", "title": "A Generalized Framework for Edge-preserving and Structure-preserving\n  Image Smoothing", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, a non-convex\nnon-smooth optimization framework is proposed to achieve diverse smoothing\nnatures where even contradictive smoothing behaviors can be achieved. To this\nend, we first introduce the truncated Huber penalty function which has seldom\nbeen used in image smoothing. A robust framework is then proposed. When\ncombined with the strong flexibility of the truncated Huber penalty function,\nour framework is capable of a range of applications and can outperform the\nstate-of-the-art approaches in several tasks. In addition, an efficient\nnumerical solution is provided and its convergence is theoretically guaranteed\neven the optimization framework is non-convex and non-smooth. The effectiveness\nand superior performance of our approach are validated through comprehensive\nexperimental results in a range of applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 00:51:21 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 07:06:25 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 07:34:08 GMT"}, {"version": "v4", "created": "Wed, 27 Nov 2019 04:58:19 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Liu", "Wei", ""], ["Zhang", "Pingping", ""], ["Lei", "Yinjie", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""], ["Reid", "Ian", ""]]}, {"id": "1907.09643", "submitter": "Oceangroup Ouc", "authors": "Haoran Zhao, Xin Sun, Junyu Dong, Changrui Chen and Zihe Dong", "title": "Highlight Every Step: Knowledge Distillation via Collaborative Teaching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High storage and computational costs obstruct deep neural networks to be\ndeployed on resource-constrained devices. Knowledge distillation aims to train\na compact student network by transferring knowledge from a larger pre-trained\nteacher model. However, most existing methods on knowledge distillation ignore\nthe valuable information among training process associated with training\nresults. In this paper, we provide a new Collaborative Teaching Knowledge\nDistillation (CTKD) strategy which employs two special teachers. Specifically,\none teacher trained from scratch (i.e., scratch teacher) assists the student\nstep by step using its temporary outputs. It forces the student to approach the\noptimal path towards the final logits with high accuracy. The other pre-trained\nteacher (i.e., expert teacher) guides the student to focus on a critical region\nwhich is more useful for the task. The combination of the knowledge from two\nspecial teachers can significantly improve the performance of the student\nnetwork in knowledge distillation. The results of experiments on CIFAR-10,\nCIFAR-100, SVHN and Tiny ImageNet datasets verify that the proposed knowledge\ndistillation method is efficient and achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 01:02:39 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Zhao", "Haoran", ""], ["Sun", "Xin", ""], ["Dong", "Junyu", ""], ["Chen", "Changrui", ""], ["Dong", "Zihe", ""]]}, {"id": "1907.09647", "submitter": "Hongwei Xv", "authors": "Xin Sun, Hongwei Xv, Junyu Dong, Qiong Li, Changrui Chen", "title": "Few-shot Learning for Domain-specific Fine-grained Image Classification", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to recognize novel visual categories from a few examples is a\nchallenging task for machines in real-world industrial applications. In\ncontrast, humans have the ability to discriminate even similar objects with\nlittle supervision. This paper attempts to address the few shot fine-grained\nimage classification problem. We propose a feature fusion model to explore\ndiscriminative features by focusing on key regions. The model utilizes the\nfocus area location mechanism to discover the perceptually similar regions\namong objects. High-order integration is employed to capture the interaction\ninformation among intra-parts. We also design a Center Neighbor Loss to form\nrobust embedding space distributions. Furthermore, we build a typical\nfine-grained and few-shot learning dataset miniPPlankton from the real-world\napplication in the area of marine ecological environments. Extensive\nexperiments are carried out to validate the performance of our method. The\nresults demonstrate that our model achieves competitive performance compared\nwith state-of-the-art models. Our work is a valuable complement to the model\ndomain-specific industrial applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 01:21:30 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 03:38:45 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 05:18:20 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Sun", "Xin", ""], ["Xv", "Hongwei", ""], ["Dong", "Junyu", ""], ["Li", "Qiong", ""], ["Chen", "Changrui", ""]]}, {"id": "1907.09653", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Chuhui Xue, Shijian Lu", "title": "GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text\n  Detection and Recognition", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent adversarial learning research has achieved very impressive progress\nfor modelling cross-domain data shifts in appearance space but its counterpart\nin modelling cross-domain shifts in geometry space lags far behind. This paper\npresents an innovative Geometry-Aware Domain Adaptation Network (GA-DAN) that\nis capable of modelling cross-domain shifts concurrently in both geometry space\nand appearance space and realistically converting images across domains with\nvery different characteristics. In the proposed GA-DAN, a novel multi-modal\nspatial learning technique is designed which converts a source-domain image\ninto multiple images of different spatial views as in the target domain. A new\ndisentangled cycle-consistency loss is introduced which balances the cycle\nconsistency in appearance and geometry spaces and improves the learning of the\nwhole network greatly. The proposed GA-DAN has been evaluated for the classic\nscene text detection and recognition tasks, and experiments show that the\ndomain-adapted images achieve superior scene text detection and recognition\nperformance while applied to network training.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 01:56:06 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Zhan", "Fangneng", ""], ["Xue", "Chuhui", ""], ["Lu", "Shijian", ""]]}, {"id": "1907.09656", "submitter": "Masoud Baghbahari", "authors": "Masoud Baghbahari, Aman Behal", "title": "Grasping Using Tactile Sensing and Deep Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile perception is an essential ability of intelligent robots in\ninteraction with their surrounding environments. This perception as an\nintermediate level acts between sensation and action and has to be defined\nproperly to generate suitable action in response to sensed data. In this paper,\nwe propose a feedback approach to address robot grasping task using\nforce-torque tactile sensing. While visual perception is an essential part for\ngross reaching, constant utilization of this sensing modality can negatively\naffect the grasping process with overwhelming computation. In such case, human\nbeing utilizes tactile sensing to interact with objects. Inspired by, the\nproposed approach is presented and evaluated on a real robot to demonstrate the\neffectiveness of the suggested framework. Moreover, we utilize a deep learning\nframework called Deep Calibration in order to eliminate the effect of bias in\nthe collected data from the robot sensors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 02:06:43 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Baghbahari", "Masoud", ""], ["Behal", "Aman", ""]]}, {"id": "1907.09658", "submitter": "Fan Yang", "authors": "Fan Yang, Sakriani Sakti, Yang Wu, Satoshi Nakamura", "title": "Make Skeleton-based Action Recognition Model Smaller, Faster and Better", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although skeleton-based action recognition has achieved great success in\nrecent years, most of the existing methods may suffer from a large model size\nand slow execution speed. To alleviate this issue, we analyze skeleton sequence\nproperties to propose a Double-feature Double-motion Network (DD-Net) for\nskeleton-based action recognition. By using a lightweight network structure\n(i.e., 0.15 million parameters), DD-Net can reach a super fast speed, as 3,500\nFPS on one GPU, or, 2,000 FPS on one CPU. By employing robust features, DD-Net\nachieves the state-of-the-art performance on our experimental datasets: SHREC\n(i.e., hand actions) and JHMDB (i.e., body actions). Our code will be released\nwith this paper later.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 02:06:51 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 02:17:39 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 09:44:58 GMT"}, {"version": "v4", "created": "Sat, 14 Sep 2019 09:15:16 GMT"}, {"version": "v5", "created": "Tue, 29 Oct 2019 03:26:20 GMT"}, {"version": "v6", "created": "Thu, 21 Nov 2019 14:19:55 GMT"}, {"version": "v7", "created": "Mon, 9 Dec 2019 03:28:46 GMT"}, {"version": "v8", "created": "Wed, 18 Mar 2020 08:58:07 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Yang", "Fan", ""], ["Sakti", "Sakriani", ""], ["Wu", "Yang", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1907.09659", "submitter": "Haijun Liu", "authors": "Haijun Liu and Jian Cheng", "title": "Enhancing the Discriminative Feature Learning for Visible-Thermal\n  Cross-Modality Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person re-identification has achieved great progress in the visible\ndomain, capturing all the person images with visible cameras. However, in a\n24-hour intelligent surveillance system, the visible cameras may be\nnoneffective at night. In this situation, thermal cameras are the best\nsupplemental components, which capture images without depending on visible\nlight. Therefore, in this paper, we investigate the visible-thermal\ncross-modality person re-identification (VT Re-ID) problem. In VT Re-ID, there\nare two knotty problems should be well handled, cross-modality discrepancy and\nintra-modality variations. To address these two issues, we propose focusing on\nenhancing the discriminative feature learning (EDFL) with two extreme simple\nmeans from two core aspects, (1) skip-connection for mid-level features\nincorporation to improve the person features with more discriminability and\nrobustness, and (2) dual-modality triplet loss to guide the training procedures\nby simultaneously considering the cross-modality discrepancy and intra-modality\nvariations. Additionally, the two-stream CNN structure is adopted to learn the\nmulti-modality sharable person features. The experimental results on two\ndatasets show that our proposed EDFL approach distinctly outperforms\nstate-of-the-art methods by large margins, demonstrating the effectiveness of\nour EDFL to enhance the discriminative feature learning for VT Re-ID.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 02:09:25 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Liu", "Haijun", ""], ["Cheng", "Jian", ""]]}, {"id": "1907.09665", "submitter": "Xiangyu He", "authors": "Xiangyu He, Ke Cheng, Qiang Chen, Qinghao Hu, Peisong Wang, Jian Cheng", "title": "Compact Global Descriptor for Neural Networks", "comments": "This paper will be included in our future works as a subsection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-range dependencies modeling, widely used in capturing spatiotemporal\ncorrelation, has shown to be effective in CNN dominated computer vision tasks.\nYet neither stacks of convolutional operations to enlarge receptive fields nor\nrecent nonlocal modules is computationally efficient. In this paper, we present\na generic family of lightweight global descriptors for modeling the\ninteractions between positions across different dimensions (e.g., channels,\nframes). This descriptor enables subsequent convolutions to access the\ninformative global features with negligible computational complexity and\nparameters. Benchmark experiments show that the proposed method can complete\nstate-of-the-art long-range mechanisms with a significant reduction in extra\ncomputing cost. Code available at\nhttps://github.com/HolmesShuan/Compact-Global-Descriptor.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 02:51:47 GMT"}, {"version": "v10", "created": "Wed, 5 Aug 2020 00:51:57 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 01:33:41 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 13:19:45 GMT"}, {"version": "v4", "created": "Sun, 26 Jul 2020 15:02:41 GMT"}, {"version": "v5", "created": "Wed, 29 Jul 2020 03:06:33 GMT"}, {"version": "v6", "created": "Thu, 30 Jul 2020 01:03:40 GMT"}, {"version": "v7", "created": "Fri, 31 Jul 2020 07:15:41 GMT"}, {"version": "v8", "created": "Mon, 3 Aug 2020 03:28:00 GMT"}, {"version": "v9", "created": "Tue, 4 Aug 2020 04:54:47 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["He", "Xiangyu", ""], ["Cheng", "Ke", ""], ["Chen", "Qiang", ""], ["Hu", "Qinghao", ""], ["Wang", "Peisong", ""], ["Cheng", "Jian", ""]]}, {"id": "1907.09670", "submitter": "Yongpei Zhu", "authors": "Yongpei Zhu, Zicong Zhou, Guojun Liao, Kehong Yuan", "title": "Deformable Registration Using Average Geometric Transformations for\n  Brain MR Images", "comments": "9 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate registration of medical images is vital for doctor's diagnosis and\nquantitative analysis. In this paper, we propose a new deformable medical image\nregistration method based on average geometric transformations and VoxelMorph\nCNN architecture. We compute the differential geometric information including\nJacobian determinant(JD) and the curl vector(CV) of diffeomorphic registration\nfield and use them as multi-channel of VoxelMorph CNN for second train. In\naddition, we use the average transformation to construct a standard brain MRI\natlas which can be used as fixed image. We verify our method on two datasets\nincluding ADNI dataset and MRBrainS18 Challenge dataset, and obtain excellent\nimprovement on MR image registration with average Dice scores and non-negative\nJacobian locations compared with MIT's original method. The experimental\nresults show the method can achieve better performance in brain MRI diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 03:05:53 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Zhu", "Yongpei", ""], ["Zhou", "Zicong", ""], ["Liao", "Guojun", ""], ["Yuan", "Kehong", ""]]}, {"id": "1907.09679", "submitter": "Lucas Tabelini Torres", "authors": "Lucas Tabelini Torres, Thiago M. Paix\\~ao, Rodrigo F. Berriel, Alberto\n  F. De Souza, Claudine Badue, Nicu Sebe and Thiago Oliveira-Santos", "title": "Effortless Deep Training for Traffic Sign Detection Using Templates and\n  Arbitrary Natural Images", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN.2019.8852086", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been successfully applied to several problems related to\nautonomous driving. Often, these solutions rely on large networks that require\ndatabases of real image samples of the problem (i.e., real world) for proper\ntraining. The acquisition of such real-world data sets is not always possible\nin the autonomous driving context, and sometimes their annotation is not\nfeasible (e.g., takes too long or is too expensive). Moreover, in many tasks,\nthere is an intrinsic data imbalance that most learning-based methods struggle\nto cope with. It turns out that traffic sign detection is a problem in which\nthese three issues are seen altogether. In this work, we propose a novel\ndatabase generation method that requires only (i) arbitrary natural images,\ni.e., requires no real image from the domain of interest, and (ii) templates of\nthe traffic signs, i.e., templates synthetically created to illustrate the\nappearance of the category of a traffic sign. The effortlessly generated\ntraining database is shown to be effective for the training of a deep detector\n(such as Faster R-CNN) on German traffic signs, achieving 95.66% of mAP on\naverage. In addition, the proposed method is able to detect traffic signs with\nan average precision, recall and F1-score of about 94%, 91% and 93%,\nrespectively. The experiments surprisingly show that detectors can be trained\nwith simple data generation methods and without problem domain data for the\nbackground, which is in the opposite direction of the common sense for deep\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 03:43:00 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Torres", "Lucas Tabelini", ""], ["Paix\u00e3o", "Thiago M.", ""], ["Berriel", "Rodrigo F.", ""], ["De Souza", "Alberto F.", ""], ["Badue", "Claudine", ""], ["Sebe", "Nicu", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "1907.09682", "submitter": "Frederick Tung", "authors": "Frederick Tung, Greg Mori", "title": "Similarity-Preserving Knowledge Distillation", "comments": "ICCV 2019 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation is a widely applicable technique for training a\nstudent neural network under the guidance of a trained teacher network. For\nexample, in neural network compression, a high-capacity teacher is distilled to\ntrain a compact student; in privileged learning, a teacher trained with\nprivileged data is distilled to train a student without access to that data.\nThe distillation loss determines how a teacher's knowledge is captured and\ntransferred to the student. In this paper, we propose a new form of knowledge\ndistillation loss that is inspired by the observation that semantically similar\ninputs tend to elicit similar activation patterns in a trained network.\nSimilarity-preserving knowledge distillation guides the training of a student\nnetwork such that input pairs that produce similar (dissimilar) activations in\nthe teacher network produce similar (dissimilar) activations in the student\nnetwork. In contrast to previous distillation methods, the student is not\nrequired to mimic the representation space of the teacher, but rather to\npreserve the pairwise similarities in its own representation space. Experiments\non three public datasets demonstrate the potential of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 03:56:14 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 22:48:24 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Tung", "Frederick", ""], ["Mori", "Greg", ""]]}, {"id": "1907.09691", "submitter": "Lan Hu", "authors": "Lan Hu, Wanting Xu, Kun Huang, Laurent Kneip", "title": "Deep-SLAM++: Object-level RGBD SLAM based on class-specific deep shape\n  priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to increase the capabilities of SLAM systems and produce\nobject-level representations, the community increasingly investigates the\nimposition of higher-level priors into the estimation process. One such example\nis given by employing object detectors to load and register full CAD models.\nOur work extends this idea to environments with unknown objects and imposes\nobject priors by employing modern class-specific neural networks to generate\ncomplete model geometry proposals. The difficulty of using such predictions in\na real SLAM scenario is that the prediction performance depends on the\nview-point and measurement quality, with even small changes of the input data\nsometimes leading to a large variability in the network output. We propose a\ndiscrete selection strategy that finds the best among multiple proposals from\ndifferent registered views by re-enforcing the agreement with the online depth\nmeasurements. The result is an effective object-level RGBD SLAM system that\nproduces compact, high-fidelity, and dense 3D maps with semantic annotations.\nIt outperforms traditional fusion strategies in terms of map completeness and\nresilience against degrading measurement quality.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 04:24:25 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 13:25:38 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Hu", "Lan", ""], ["Xu", "Wanting", ""], ["Huang", "Kun", ""], ["Kneip", "Laurent", ""]]}, {"id": "1907.09695", "submitter": "Shivangi Srivastava", "authors": "Shivangi Srivastava, Maxim Berman, Matthew B. Blaschko, Devis Tuia", "title": "Adaptive Compression-based Lifelong Learning", "comments": "Accepted at BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of a deep learning model losing performance on a previously\nlearned task when fine-tuned to a new one is a phenomenon known as Catastrophic\nforgetting. There are two major ways to mitigate this problem: either\npreserving activations of the initial network during training with a new task;\nor restricting the new network activations to remain close to the initial ones.\nThe latter approach falls under the denomination of lifelong learning, where\nthe model is updated in a way that it performs well on both old and new tasks,\nwithout having access to the old task's training samples anymore. Recently,\napproaches like pruning networks for freeing network capacity during sequential\nlearning of tasks have been gaining in popularity. Such approaches allow\nlearning small networks while making redundant parameters available for the\nnext tasks. The common problem encountered with these approaches is that the\npruning percentage is hard-coded, irrespective of the number of samples, of the\ncomplexity of the learning task and of the number of classes in the dataset. We\npropose a method based on Bayesian optimization to perform adaptive\ncompression/pruning of the network and show its effectiveness in lifelong\nlearning. Our method learns to perform heavy pruning for small and/or simple\ndatasets while using milder compression rates for large and/or complex data.\nExperiments on classification and semantic segmentation demonstrate the\napplicability of learning network compression, where we are able to effectively\npreserve performances along sequences of tasks of varying complexity.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 04:58:52 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Srivastava", "Shivangi", ""], ["Berman", "Maxim", ""], ["Blaschko", "Matthew B.", ""], ["Tuia", "Devis", ""]]}, {"id": "1907.09702", "submitter": "Tianwei Lin", "authors": "Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, Shilei Wen", "title": "BMN: Boundary-Matching Network for Temporal Action Proposal Generation", "comments": "This paper is accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action proposal generation is an challenging and promising task\nwhich aims to locate temporal regions in real-world videos where action or\nevent may occur. Current bottom-up proposal generation methods can generate\nproposals with precise boundary, but cannot efficiently generate adequately\nreliable confidence scores for retrieving proposals. To address these\ndifficulties, we introduce the Boundary-Matching (BM) mechanism to evaluate\nconfidence scores of densely distributed proposals, which denote a proposal as\na matching pair of starting and ending boundaries and combine all densely\ndistributed BM pairs into the BM confidence map. Based on BM mechanism, we\npropose an effective, efficient and end-to-end proposal generation method,\nnamed Boundary-Matching Network (BMN), which generates proposals with precise\ntemporal boundaries as well as reliable confidence scores simultaneously. The\ntwo-branches of BMN are jointly trained in an unified framework. We conduct\nexperiments on two challenging datasets: THUMOS-14 and ActivityNet-1.3, where\nBMN shows significant performance improvement with remarkable efficiency and\ngeneralizability. Further, combining with existing action classifier, BMN can\nachieve state-of-the-art temporal action detection performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 05:53:28 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Lin", "Tianwei", ""], ["Liu", "Xiao", ""], ["Li", "Xin", ""], ["Ding", "Errui", ""], ["Wen", "Shilei", ""]]}, {"id": "1907.09705", "submitter": "Zhaoyi Wan", "authors": "Zhaoyi Wan, Fengming Xie, Yibo Liu, Xiang Bai, Cong Yao", "title": "2D-CTC for Scene Text Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition has been an important, active research topic in\ncomputer vision for years. Previous approaches mainly consider text as 1D\nsignals and cast scene text recognition as a sequence prediction problem, by\nfeat of CTC or attention based encoder-decoder framework, which is originally\ndesigned for speech recognition. However, different from speech voices, which\nare 1D signals, text instances are essentially distributed in 2D image spaces.\nTo adhere to and make use of the 2D nature of text for higher recognition\naccuracy, we extend the vanilla CTC model to a second dimension, thus creating\n2D-CTC. 2D-CTC can adaptively concentrate on most relevant features while\nexcluding the impact from clutters and noises in the background; It can also\nnaturally handle text instances with various forms (horizontal, oriented and\ncurved) while giving more interpretable intermediate predictions. The\nexperiments on standard benchmarks for scene text recognition, such as IIIT-5K,\nICDAR 2015, SVP-Perspective, and CUTE80, demonstrate that the proposed 2D-CTC\nmodel outperforms state-of-the-art methods on the text of both regular and\nirregular shapes. Moreover, 2D-CTC exhibits its superiority over prior art on\ntraining and testing speed. Our implementation and models of 2D-CTC will be\nmade publicly available soon later.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 05:55:28 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Wan", "Zhaoyi", ""], ["Xie", "Fengming", ""], ["Liu", "Yibo", ""], ["Bai", "Xiang", ""], ["Yao", "Cong", ""]]}, {"id": "1907.09706", "submitter": "Heon Lee", "authors": "Samuel Yu, Heon Lee, John Kim", "title": "LYTNet: A Convolutional Neural Network for Real-Time Pedestrian Traffic\n  Lights and Zebra Crossing Recognition for the Visually Impaired", "comments": "12 pages, 5 figures, 6 tables, International Conference on Computer\n  Analysis of Images and Patterns (CAIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Currently, the visually impaired rely on either a sighted human, guide dog,\nor white cane to safely navigate. However, the training of guide dogs is\nextremely expensive, and canes cannot provide essential information regarding\nthe color of traffic lights and direction of crosswalks. In this paper, we\npropose a deep learning based solution that provides information regarding the\ntraffic light mode and the position of the zebra crossing. Previous solutions\nthat utilize machine learning only provide one piece of information and are\nmostly binary: only detecting red or green lights. The proposed convolutional\nneural network, LYTNet, is designed for comprehensiveness, accuracy, and\ncomputational efficiency. LYTNet delivers both of the two most important pieces\nof information for the visually impaired to cross the road. We provide five\nclasses of pedestrian traffic lights rather than the commonly seen three or\nfour, and a direction vector representing the midline of the zebra crossing\nthat is converted from the 2D image plane to real-world positions. We created\nour own dataset of pedestrian traffic lights containing over 5000 photos taken\nat hundreds of intersections in Shanghai. The experiments carried out achieve a\nclassification accuracy of 94%, average angle error of 6.35 degrees, with a\nframe rate of 20 frames per second when testing the network on an iPhone 7 with\nadditional post-processing steps.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 05:56:43 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Yu", "Samuel", ""], ["Lee", "Heon", ""], ["Kim", "John", ""]]}, {"id": "1907.09707", "submitter": "Sangyun Oh", "authors": "Sangyun Oh, Hye-Jin S. Kim, Jongeun Lee and Junmo Kim", "title": "RRNet: Repetition-Reduction Network for Energy Efficient Decoder of\n  Depth Estimation", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Repetition-Reduction network (RRNet) for resource-constrained\ndepth estimation, offering significantly improved efficiency in terms of\ncomputation, memory and energy consumption. The proposed method is based on\nrepetition-reduction (RR) blocks. The RR blocks consist of the set of repeated\nconvolutions and the residual connection layer that take place of the pointwise\nreduction layer with linear connection to the decoder. The RRNet help reduce\nmemory usage and power consumption in the residual connections to the decoder\nlayers. RRNet consumes approximately 3.84 times less energy and 3.06 times less\nmeory and is approaximately 2.21 times faster, without increasing the demand on\nhardware resource relative to the baseline network (Godard et al, CVPR'17),\noutperforming current state-of-the-art lightweight architectures such as\nSqueezeNet, ShuffleNet, MobileNet and PyDNet.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 05:59:05 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 05:26:19 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Oh", "Sangyun", ""], ["Kim", "Hye-Jin S.", ""], ["Lee", "Jongeun", ""], ["Kim", "Junmo", ""]]}, {"id": "1907.09729", "submitter": "Juntang Zhuang", "authors": "Juntang Zhuang, Nicha C. Dvornek, Xiaoxiao Li, Pamela Ventola, James\n  S. Duncan", "title": "Invertible Network for Classification and Biomarker Selection for ASD", "comments": null, "journal-ref": "Medical Image Computing and Computer-Assisted Intervention 2019,\n  MICCAI", "doi": "10.1007/978-3-030-32248-9_78", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining biomarkers for autism spectrum disorder (ASD) is crucial to\nunderstanding its mechanisms. Recently deep learning methods have achieved\nsuccess in the classification task of ASD using fMRI data. However, due to the\nblack-box nature of most deep learning models, it's hard to perform biomarker\nselection and interpret model decisions. The recently proposed invertible\nnetworks can accurately reconstruct the input from its output, and have the\npotential to unravel the black-box representation. Therefore, we propose a\nnovel method to classify ASD and identify biomarkers for ASD using the\nconnectivity matrix calculated from fMRI as the input. Specifically, with\ninvertible networks, we explicitly determine the decision boundary and the\nprojection of data points onto the boundary. Like linear classifiers, the\ndifference between a point and its projection onto the decision boundary can be\nviewed as the explanation. We then define the importance as the explanation\nweighted by the gradient of prediction $w.r.t$ the input, and identify\nbiomarkers based on this importance measure. We perform a regression task to\nfurther validate our biomarker selection: compared to using all edges in the\nconnectivity matrix, using the top 10\\% important edges we generate a lower\nregression error on 6 different severity scores. Our experiments show that the\ninvertible network is both effective at ASD classification and interpretable,\nallowing for discovery of reliable biomarkers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 07:31:39 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Zhuang", "Juntang", ""], ["Dvornek", "Nicha C.", ""], ["Li", "Xiaoxiao", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S.", ""]]}, {"id": "1907.09732", "submitter": "Kai Brehmer", "authors": "Kai Brehmer, Hari Om Aggrawal, Stefan Heldmann, and Jan Modersitzki", "title": "Variational Registration of Multiple Images with the SVD based SqN\n  Distance Measure", "comments": "12 pages, 5 figures, accepted at the conference \"Scale Space and\n  Variational Methods\" in Hofgeismar, Germany 2019", "journal-ref": null, "doi": "10.1007/978-3-030-22368-7_20", "report-no": null, "categories": "eess.IV cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration, especially the quantification of image similarity, is an\nimportant task in image processing. Various approaches for the comparison of\ntwo images are discussed in the literature. However, although most of these\napproaches perform very well in a two image scenario, an extension to a\nmultiple images scenario deserves attention. In this article, we discuss and\ncompare registration methods for multiple images. Our key assumption is, that\ninformation about the singular values of a feature matrix of images can be used\nfor alignment. We introduce, discuss and relate three recent approaches from\nthe literature: the Schatten q-norm based SqN distance measure, a rank based\napproach, and a feature volume based approach. We also present results for\ntypical applications such as dynamic image sequences or stacks of histological\nsections. Our results indicate that the SqN approach is in fact a suitable\ndistance measure for image registration. Moreover, our examples also indicate\nthat the results obtained by SqN are superior to those obtained by its\ncompetitors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 07:41:22 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Brehmer", "Kai", ""], ["Aggrawal", "Hari Om", ""], ["Heldmann", "Stefan", ""], ["Modersitzki", "Jan", ""]]}, {"id": "1907.09747", "submitter": "Ming Yin", "authors": "Ming Yin, Weitian Huang, Junbin Gao", "title": "Shared Generative Latent Representation Learning for Multi-view\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering multi-view data has been a fundamental research topic in the\ncomputer vision community. It has been shown that a better accuracy can be\nachieved by integrating information of all the views than just using one view\nindividually. However, the existing methods often struggle with the issues of\ndealing with the large-scale datasets and the poor performance in\nreconstructing samples. This paper proposes a novel multi-view clustering\nmethod by learning a shared generative latent representation that obeys a\nmixture of Gaussian distributions. The motivation is based on the fact that the\nmulti-view data share a common latent embedding despite the diversity among the\nviews. Specifically, benefited from the success of the deep generative\nlearning, the proposed model not only can extract the nonlinear features from\nthe views, but render a powerful ability in capturing the correlations among\nall the views. The extensive experimental results, on several datasets with\ndifferent scales, demonstrate that the proposed method outperforms the\nstate-of-the-art methods under a range of performance criteria.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 08:20:38 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Yin", "Ming", ""], ["Huang", "Weitian", ""], ["Gao", "Junbin", ""]]}, {"id": "1907.09754", "submitter": "Yaxing Wang", "authors": "Yaxing Wang, Abel Gonzalez-Garcia, Joost van de Weijer, Luis Herranz", "title": "Controlling biases and diversity in diverse image-to-image translation", "comments": "The paper is under consideration at Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of unpaired image-to-image translation is highly challenging due to\nthe lack of explicit cross-domain pairs of instances. We consider here diverse\nimage translation (DIT), an even more challenging setting in which an image can\nhave multiple plausible translations. This is normally achieved by explicitly\ndisentangling content and style in the latent representation and sampling\ndifferent styles codes while maintaining the image content. Despite the success\nof current DIT models, they are prone to suffer from bias. In this paper, we\nstudy the problem of bias in image-to-image translation. Biased datasets may\nadd undesired changes (e.g. change gender or race in face images) to the output\ntranslations as a consequence of the particular underlying visual distribution\nin the target domain. In order to alleviate the effects of this problem we\npropose the use of semantic constraints that enforce the preservation of\ndesired image properties. Our proposed model is a step towards unbiased diverse\nimage-to-image translation (UDIT), and results in less unwanted changes in the\ntranslated images while still performing the wanted transformation. Experiments\non several heavily biased datasets show the effectiveness of the proposed\ntechniques in different domains such as faces, objects, and scenes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 08:35:23 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Wang", "Yaxing", ""], ["Gonzalez-Garcia", "Abel", ""], ["van de Weijer", "Joost", ""], ["Herranz", "Luis", ""]]}, {"id": "1907.09760", "submitter": "Hyeonwoo Yu", "authors": "Hyeonwoo Yu", "title": "Not Only Look But Observe: Variational Observation Model of Scene-Level\n  3D Multi-Object Understanding for Probabilistic SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NOLBO, a variational observation model estimation for 3D\nmulti-object from 2D single shot. Previous probabilistic instance-level\nunderstandings mainly consider the single-object image, not single shot with\nmulti-object; relations between objects and the entire scene are out of their\nfocus. The objectness of each observation also hardly join their model.\nTherefore, we propose a method to approximate the Bayesian observation model of\nscene-level 3D multi-object understanding. By exploiting variational\nauto-encoder (VAE), we estimate latent variables from the entire scene, which\nfollow tractable distributions and concurrently imply 3D full shape and pose.\nTo perform object-oriented data association and probabilistic simultaneous\nlocalization and mapping (SLAM), our observation models can easily be adopted\nto probabilistic inference by replacing object-oriented features with latent\nvariables.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 08:45:24 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 03:33:52 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2020 03:27:35 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Yu", "Hyeonwoo", ""]]}, {"id": "1907.09775", "submitter": "Claudio Zito", "authors": "A. Barsky, C. Zito, H. Mori, T. Ogata and J. L. Wyatt", "title": "Multisensory Learning Framework for Robot Drumming", "comments": "Extended abstract", "journal-ref": "Workshop on Crossmodal Learning for Intelligent Robotics 2nd\n  Edition. IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2018", "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SD", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The hype about sensorimotor learning is currently reaching high fever, thanks\nto the latest advancement in deep learning. In this paper, we present an\nopen-source framework for collecting large-scale, time-synchronised synthetic\ndata from highly disparate sensory modalities, such as audio, video, and\nproprioception, for learning robot manipulation tasks. We demonstrate the\nlearning of non-linear sensorimotor mappings for a humanoid drumming robot that\ngenerates novel motion sequences from desired audio data using cross-modal\ncorrespondences. We evaluate our system through the quality of its cross-modal\nretrieval, for generating suitable motion sequences to match desired unseen\naudio or video sequences.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 09:09:18 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Barsky", "A.", ""], ["Zito", "C.", ""], ["Mori", "H.", ""], ["Ogata", "T.", ""], ["Wyatt", "J. L.", ""]]}, {"id": "1907.09786", "submitter": "Chenyang Lu", "authors": "Chenyang Lu, Gijs Dubbelman", "title": "Hallucinating Beyond Observation: Learning to Complete with Partial\n  Observation and Unpaired Prior Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel single-step training strategy that allows convolutional\nencoder-decoder networks that use skip connections, to complete partially\nobserved data by means of hallucination. This strategy is demonstrated for the\ntask of completing 2-D road layouts as well as 3-D vehicle shapes. As input, it\ntakes data from a partially observed domain, for which no ground truth is\navailable, and data from an unpaired prior knowledge domain and trains the\nnetwork in an end-to-end manner. Our single-step training strategy is compared\nagainst two state-of-the-art baselines, one using a two-step auto-encoder\ntraining strategy and one using an adversarial strategy. Our novel strategy\nachieves an improvement up to +12.2% F-measure on the Cityscapes dataset. The\nlearned network intrinsically generalizes better than the baselines on unseen\ndatasets, which is demonstrated by an improvement up to +23.8% F-measure on the\nunseen KITTI dataset. Moreover, our approach outperforms the baselines using\nthe same backbone network on the 3-D shape completion benchmark by a margin of\n0.006 Hamming distance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 09:40:43 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 13:34:21 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Lu", "Chenyang", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "1907.09798", "submitter": "Liang Pan", "authors": "Liang Pan, Chee-Meng Chew and Gim Hee Lee", "title": "PointAtrousGraph: Deep Hierarchical Encoder-Decoder with Point Atrous\n  Convolution for Unorganized 3D Points", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the success of encoding multi-scale contextual information for\nimage analysis, we propose our PointAtrousGraph (PAG) - a deep\npermutation-invariant hierarchical encoder-decoder for efficiently exploiting\nmulti-scale edge features in point clouds. Our PAG is constructed by several\nnovel modules, such as Point Atrous Convolution (PAC), Edge-preserved Pooling\n(EP) and Edge-preserved Unpooling (EU). Similar with atrous convolution, our\nPAC can effectively enlarge receptive fields of filters and thus densely learn\nmulti-scale point features. Following the idea of non-overlapping max-pooling\noperations, we propose our EP to preserve critical edge features during\nsubsampling. Correspondingly, our EU modules gradually recover spatial\ninformation for edge features. In addition, we introduce chained skip\nsubsampling/upsampling modules that directly propagate edge features to the\nfinal stage. Particularly, our proposed auxiliary loss functions can further\nimprove our performance. Experimental results show that our PAG outperform\nprevious state-of-the-art methods on various 3D semantic perception\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 10:16:58 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 09:41:25 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Pan", "Liang", ""], ["Chew", "Chee-Meng", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1907.09811", "submitter": "Lei Wang", "authors": "Xiurui Geng, Lei Wang", "title": "NPSA: Nonorthogonal Principal Skewness Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2984849", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal skewness analysis (PSA) has been introduced for feature extraction\nin hyperspectral imagery. As a third-order generalization of principal\ncomponent analysis (PCA), its solution of searching for the locally maximum\nskewness direction is transformed into the problem of calculating the\neigenpairs (the eigenvalues and the corresponding eigenvectors) of a coskewness\ntensor. By combining a fixed-point method with an orthogonal constraint, it can\nprevent the new eigenpairs from converging to the same maxima that has been\ndetermined before. However, the eigenvectors of the supersymmetric tensor are\nnot inherently orthogonal in general, which implies that the results obtained\nby the search strategy used in PSA may unavoidably deviate from the actual\neigenpairs. In this paper, we propose a new nonorthogonal search strategy to\nsolve this problem and the new algorithm is named nonorthogonal principal\nskewness analysis (NPSA). The contribution of NPSA lies in the finding that the\nsearch space of the eigenvector to be determined can be enlarged by using the\northogonal complement of the Kronecker product of the previous one, instead of\nits orthogonal complement space. We give a detailed theoretical proof to\nillustrate why the new strategy can result in the more accurate eigenpairs. In\naddition, after some algebraic derivations, the complexity of the presented\nalgorithm is also greatly reduced. Experiments with both simulated data and\nreal multi/hyperspectral imagery demonstrate its validity in feature\nextraction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 10:48:15 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Geng", "Xiurui", ""], ["Wang", "Lei", ""]]}, {"id": "1907.09815", "submitter": "Dalu Guo Mr.", "authors": "Dalu Guo, Chang Xu, Dacheng Tao", "title": "Bilinear Graph Networks for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the bilinear attention networks in the visual question\nanswering task from a graph perspective. The classical bilinear attention\nnetworks build a bilinear attention map to extract the joint representation of\nwords in the question and objects in the image but lack fully exploring the\nrelationship between words for complex reasoning. In contrast, we develop\nbilinear graph networks to model the context of the joint embeddings of words\nand objects. Two kinds of graphs are investigated, namely image-graph and\nquestion-graph. The image-graph transfers features of the detected objects to\ntheir related query words, enabling the output nodes to have both semantic and\nfactual information. The question-graph exchanges information between these\noutput nodes from image-graph to amplify the implicit yet important\nrelationship between objects. These two kinds of graphs cooperate with each\nother, and thus our resulting model can model the relationship and dependency\nbetween objects, which leads to the realization of multi-step reasoning.\nExperimental results on the VQA v2.0 validation dataset demonstrate the ability\nof our method to handle the complex questions. On the test-std set, our best\nsingle model achieves state-of-the-art performance, boosting the overall\naccuracy to 72.41%.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 10:59:54 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 11:26:12 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Guo", "Dalu", ""], ["Xu", "Chang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1907.09828", "submitter": "Da Chen", "authors": "Da Chen and Laurent D. Cohen", "title": "From Active Contours to Minimal Geodesic Paths: New Solutions to Active\n  Contours Problems by Eikonal Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we give an overview of part of our previous work based on\nthe minimal path framework and the Eikonal partial differential equation (PDE).\nWe show that by designing adequate Riemannian and Randers geodesic metrics the\nminimal paths can be utilized to search for solutions to almost all of the\nactive contour problems and to the Euler-Mumford elastica problem, which allows\nto blend the advantages from minimal geodesic paths and those original\napproaches, i.e. the active contours and elastica curves. The proposed minimal\npath-based models can be applied to deal with a broad variety of image analysis\ntasks such as boundary detection, image segmentation and tubular structure\nextraction. The numerical implementations for the computation of minimal paths\nare known to be quite efficient thanks to the Eikonal solvers such as the\nFinsler variant of the fast marching method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 11:44:51 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 06:06:07 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Chen", "Da", ""], ["Cohen", "Laurent D.", ""]]}, {"id": "1907.09831", "submitter": "Ning Wang", "authors": "Ning Wang, Wengang Zhou, Yibing Song, Chao Ma and Houqiang Li", "title": "Real-Time Correlation Tracking via Joint Model Compression and Transfer", "comments": "12 pages, 12 figures, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2020.2989544", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filters (CF) have received considerable attention in visual\ntracking because of their computational efficiency. Leveraging deep features\nvia off-the-shelf CNN models (e.g., VGG), CF trackers achieve state-of-the-art\nperformance while consuming a large number of computing resources. This limits\ndeep CF trackers to be deployed to many mobile platforms on which only a\nsingle-core CPU is available. In this paper, we propose to jointly compress and\ntransfer off-the-shelf CNN models within a knowledge distillation framework. We\nformulate a CNN model pretrained from the image classification task as a\nteacher network, and distill this teacher network into a lightweight student\nnetwork as the feature extractor to speed up CF trackers. In the distillation\nprocess, we propose a fidelity loss to enable the student network to maintain\nthe representation capability of the teacher network. Meanwhile, we design a\ntracking loss to adapt the objective of the student network from object\nrecognition to visual tracking. The distillation process is performed offline\non multiple layers and adaptively updates the student network using a\nbackground-aware online learning scheme. Extensive experiments on five\nchallenging datasets demonstrate that the lightweight student network\naccelerates the speed of state-of-the-art deep CF trackers to real-time on a\nsingle-core CPU while maintaining almost the same tracking accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 11:57:42 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Wang", "Ning", ""], ["Zhou", "Wengang", ""], ["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Li", "Houqiang", ""]]}, {"id": "1907.09837", "submitter": "Patricia Vitoria", "authors": "Patricia Vitoria, Lara Raad, Coloma Ballester", "title": "ChromaGAN: Adversarial Picture Colorization with Semantic Class\n  Distribution", "comments": "8 pages + references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The colorization of grayscale images is an ill-posed problem, with multiple\ncorrect solutions. In this paper, we propose an adversarial learning\ncolorization approach coupled with semantic information. A generative network\nis used to infer the chromaticity of a given grayscale image conditioned to\nsemantic clues. This network is framed in an adversarial model that learns to\ncolorize by incorporating perceptual and semantic understanding of color and\nclass distributions. The model is trained via a fully self-supervised strategy.\nQualitative and quantitative results show the capacity of the proposed method\nto colorize images in a realistic way achieving state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 12:20:27 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 21:38:56 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Vitoria", "Patricia", ""], ["Raad", "Lara", ""], ["Ballester", "Coloma", ""]]}, {"id": "1907.09896", "submitter": "Jonny O'Dwyer", "authors": "Jonny O'Dwyer, Niall Murray, Ronan Flynn", "title": "Eye-based Continuous Affect Prediction", "comments": "Accepted paper (pre-print) for 2019 8th International Conference on\n  Affective Computing and Intelligent Interaction (ACII)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye-based information channels include the pupils, gaze, saccades, fixational\nmovements, and numerous forms of eye opening and closure. Pupil size variation\nindicates cognitive load and emotion, while a person's gaze direction is said\nto be congruent with the motivation to approach or avoid stimuli. The eyelids\nare involved in facial expressions that can encode basic emotions.\nAdditionally, eye-based cues can have implications for human annotators of\nemotions or feelings. Despite these facts, the use of eye-based cues in\naffective computing is in its infancy, however, and this work is intended to\nstart to address this. Eye-based feature sets, incorporating data from all of\nthe aforementioned information channels, that can be estimated from video are\nproposed. Feature set refinement is provided by way of continuous arousal and\nvalence learning and prediction experiments on the RECOLA validation set. The\neye-based features are then combined with a speech feature set to provide\nconfirmation of their usefulness and assess affect prediction performance\ncompared with group-of-humans-level performance on the RECOLA test set. The\ncore contribution of this paper, a refined eye-based feature set, is shown to\nprovide benefits for affect prediction. It is hoped that this work stimulates\nfurther research into eye-based affective computing.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 14:18:30 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 15:50:03 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["O'Dwyer", "Jonny", ""], ["Murray", "Niall", ""], ["Flynn", "Ronan", ""]]}, {"id": "1907.09905", "submitter": "Armin Mustafa", "authors": "Armin Mustafa, Chris Russell and Adrian Hilton", "title": "U4D: Unsupervised 4D Dynamic Scene Understanding", "comments": "To appear in IEEE International Conference in Computer Vision ICCV\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first approach to solve the challenging problem of\nunsupervised 4D visual scene understanding for complex dynamic scenes with\nmultiple interacting people from multi-view video. Our approach simultaneously\nestimates a detailed model that includes a per-pixel semantically and\ntemporally coherent reconstruction, together with instance-level segmentation\nexploiting photo-consistency, semantic and motion information. We further\nleverage recent advances in 3D pose estimation to constrain the joint semantic\ninstance segmentation and 4D temporally coherent reconstruction. This enables\nper person semantic instance segmentation of multiple interacting people in\ncomplex dynamic scenes. Extensive evaluation of the joint visual scene\nunderstanding framework against state-of-the-art methods on challenging indoor\nand outdoor sequences demonstrates a significant (approx 40%) improvement in\nsemantic segmentation, reconstruction and scene flow accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 14:25:27 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Mustafa", "Armin", ""], ["Russell", "Chris", ""], ["Hilton", "Adrian", ""]]}, {"id": "1907.09919", "submitter": "Jonny O'Dwyer", "authors": "Jonny O'Dwyer", "title": "Speech, Head, and Eye-based Cues for Continuous Affect Prediction", "comments": "Accepted paper (pre-print) for 2019 8th International Conference on\n  Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous affect prediction involves the discrete time-continuous regression\nof affect dimensions. Dimensions to be predicted often include arousal and\nvalence. Continuous affect prediction researchers are now embracing multimodal\nmodel input. This provides motivation for researchers to investigate previously\nunexplored affective cues. Speech-based cues have traditionally received the\nmost attention for affect prediction, however, non-verbal inputs have\nsignificant potential to increase the performance of affective computing\nsystems and in addition, allow affect modelling in the absence of speech.\nHowever, non-verbal inputs that have received little attention for continuous\naffect prediction include eye and head-based cues. The eyes are involved in\nemotion displays and perception while head-based cues have been shown to\ncontribute to emotion conveyance and perception. Additionally, these cues can\nbe estimated non-invasively from video, using modern computer vision tools.\nThis work exploits this gap by comprehensively investigating head and eye-based\nfeatures and their combination with speech for continuous affect prediction.\nHand-crafted, automatically generated and CNN-learned features from these\nmodalities will be investigated for continuous affect prediction. The highest\nperforming feature sets and feature set combinations will answer how effective\nthese features are for the prediction of an individual's affective state.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 14:46:51 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 15:59:13 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["O'Dwyer", "Jonny", ""]]}, {"id": "1907.09945", "submitter": "Cristiano Massaroni", "authors": "Danilo Avola, Luigi Cinque, Alessio Fagioli, Gian Luca Foresti and\n  Cristiano Massaroni", "title": "Deep Temporal Analysis for Non-Acted Body Affect Recognition", "comments": null, "journal-ref": "IEEE Transactions on Affective Computing 2020", "doi": "10.1109/TAFFC.2020.3003816", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Affective computing is a field of great interest in many computer vision\napplications, including video surveillance, behaviour analysis, and human-robot\ninteraction. Most of the existing literature has addressed this field by\nanalysing different sets of face features. However, in the last decade, several\nstudies have shown how body movements can play a key role even in emotion\nrecognition. The majority of these experiments on the body are performed by\ntrained actors whose aim is to simulate emotional reactions. These unnatural\nexpressions differ from the more challenging genuine emotions, thus\ninvalidating the obtained results. In this paper, a solution for basic\nnon-acted emotion recognition based on 3D skeleton and Deep Neural Networks\n(DNNs) is provided. The proposed work introduces three majors contributions.\nFirst, unlike the current state-of-the-art in non-acted body affect\nrecognition, where only static or global body features are considered, in this\nwork also temporal local movements performed by subjects in each frame are\nexamined. Second, an original set of global and time-dependent features for\nbody movement description is provided. Third, to the best of out knowledge,\nthis is the first attempt to use deep learning methods for non-acted body\naffect recognition. Due to the novelty of the topic, only the UCLIC dataset is\ncurrently considered the benchmark for comparative tests. On the latter, the\nproposed method outperforms all the competitors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 15:10:12 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Avola", "Danilo", ""], ["Cinque", "Luigi", ""], ["Fagioli", "Alessio", ""], ["Foresti", "Gian Luca", ""], ["Massaroni", "Cristiano", ""]]}, {"id": "1907.09974", "submitter": "Lydia Neary-Zajiczek", "authors": "Lydia Neary-Zajiczek, Clara Essmann, Neil Clancy, Aiman Haider, Elena\n  Miranda, Michael Shaw, Amir Gander, Brian Davidson, Delmiro Fernandez-Reyes,\n  Vijay Pawar and Danail Stoyanov", "title": "Whole-Sample Mapping of Cancerous and Benign Tissue Properties", "comments": "Accepted at MICCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural and mechanical differences between cancerous and healthy tissue\ngive rise to variations in macroscopic properties such as visual appearance and\nelastic modulus that show promise as signatures for early cancer detection.\nAtomic force microscopy (AFM) has been used to measure significant differences\nin stiffness between cancerous and healthy cells owing to its high force\nsensitivity and spatial resolution, however due to absorption and scattering of\nlight, it is often challenging to accurately locate where AFM measurements have\nbeen made on a bulk tissue sample. In this paper we describe an image\nregistration method that localizes AFM elastic stiffness measurements with\nhigh-resolution images of haematoxylin and eosin (H\\&E)-stained tissue to\nwithin 1.5 microns. Color RGB images are segmented into three structure types\n(lumen, cells and stroma) by a neural network classifier trained on\nground-truth pixel data obtained through k-means clustering in HSV color space.\nUsing the localized stiffness maps and corresponding structural information, a\nwhole-sample stiffness map is generated with a region matching and\ninterpolation algorithm that associates similar structures with measured\nstiffness values. We present results showing significant differences in\nstiffness between healthy and cancerous liver tissue and discuss potential\napplications of this technique.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 16:00:52 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Neary-Zajiczek", "Lydia", ""], ["Essmann", "Clara", ""], ["Clancy", "Neil", ""], ["Haider", "Aiman", ""], ["Miranda", "Elena", ""], ["Shaw", "Michael", ""], ["Gander", "Amir", ""], ["Davidson", "Brian", ""], ["Fernandez-Reyes", "Delmiro", ""], ["Pawar", "Vijay", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1907.09983", "submitter": "Chen Chen", "authors": "Chen Chen, Carlo Biffi, Giacomo Tarroni, Steffen Petersen, Wenjia Bai,\n  Daniel Rueckert", "title": "Learning Shape Priors for Robust Cardiac MR Segmentation from Multi-view\n  Images", "comments": "11 pages, 5 figures, accepted at MICCAI 2019, Camera-ready version", "journal-ref": null, "doi": "10.1007/978-3-030-32245-8_58", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cardiac MR image segmentation is essential for the morphological and\nfunctional analysis of the heart. Inspired by how experienced clinicians assess\nthe cardiac morphology and function across multiple standard views (i.e. long-\nand short-axis views), we propose a novel approach which learns anatomical\nshape priors across different 2D standard views and leverages these priors to\nsegment the left ventricular (LV) myocardium from short-axis MR image stacks.\nThe proposed segmentation method has the advantage of being a 2D network but at\nthe same time incorporates spatial context from multiple, complementary views\nthat span a 3D space. Our method achieves accurate and robust segmentation of\nthe myocardium across different short-axis slices (from apex to base),\noutperforming baseline models (e.g. 2D U-Net, 3D U-Net) while achieving higher\ndata efficiency. Compared to the 2D U-Net, the proposed method reduces the mean\nHausdorff distance (mm) from 3.24 to 2.49 on the apical slices, from 2.34 to\n2.09 on the middle slices and from 3.62 to 2.76 on the basal slices on the test\nset, when only 10% of the training data was used.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 16:22:43 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 17:06:11 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Chen", "Chen", ""], ["Biffi", "Carlo", ""], ["Tarroni", "Giacomo", ""], ["Petersen", "Steffen", ""], ["Bai", "Wenjia", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1907.09997", "submitter": "Zhongming Xiang", "authors": "Zhongming Xiang, Abbas Rashidi, and Ge (Gaby) Ou", "title": "An Improved Convolutional Neural Network System for Automatically\n  Detecting Rebar in GPR Data", "comments": null, "journal-ref": null, "doi": "10.1061/9780784482438.054", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a mature technology, Ground Penetration Radar (GPR) is now widely employed\nin detecting rebar and other embedded elements in concrete structures. Manually\nrecognizing rebar from GPR data is a time-consuming and error-prone procedure.\nAlthough there are several approaches to automatically detect rebar, it is\nstill challenging to find a high resolution and efficient method for different\nrebar arrangements, especially for closely spaced rebar meshes. As an improved\nConvolution Neural Network (CNN), AlexNet shows superiority over traditional\nmethods in image recognition domain. Thus, this paper introduces AlexNet as an\nalternative solution for automatically detecting rebar within GPR data. In\norder to show the efficiency of the proposed approach, a traditional CNN is\nbuilt as the comparative option. Moreover, this research evaluates the impacts\nof different rebar arrangements and different window sizes on the accuracy of\nresults. The results revealed that: (1) AlexNet outperforms the traditional CNN\napproach, and its superiority is more notable when the rebar meshes are densely\ndistributed; (2) the detection accuracy significantly varies with changing the\nsize of splitting window, and a proper window should contain enough information\nabout rebar; (3) uniformly and sparsely distributed rebar meshes are more\nrecognizable than densely or unevenly distributed items, due to lower chances\nof signal interferences.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 16:46:50 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Xiang", "Zhongming", "", "Gaby"], ["Rashidi", "Abbas", "", "Gaby"], ["Ge", "", "", "Gaby"], ["Ou", "", ""]]}, {"id": "1907.10004", "submitter": "Tal Hakim", "authors": "Tal Hakim and Ilan Shimshoni", "title": "A-MAL: Automatic Movement Assessment Learning from Properly Performed\n  Movements in 3D Skeleton Videos", "comments": "Accepted and orally presented in CVPM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of assessing movement quality has recently gained high demand in a\nvariety of domains. The ability to automatically assess subject movement in\nvideos that were captured by affordable devices, such as Kinect cameras, is\nessential for monitoring clinical rehabilitation processes, for improving motor\nskills and for movement learning tasks. The need to pay attention to low-level\ndetails while accurately tracking the movement stages, makes this task very\nchallenging. In this work, we introduce A-MAL, an automatic, strong movement\nassessment learning algorithm that only learns from properly-performed movement\nvideos without further annotations, powered by a deviation time-segmentation\nalgorithm, a parameter relevance detection algorithm, a novel time-warping\nalgorithm that is based on automatic detection of common temporal\npoints-of-interest and a textual-feedback generation mechanism. We demonstrate\nour method on movements from the Fugl-Meyer Assessment (FMA) test, which is\ntypically held by occupational therapists in order to monitor patients'\nrecovery processes after strokes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 16:57:26 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 15:03:02 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 10:42:28 GMT"}, {"version": "v4", "created": "Wed, 15 Jul 2020 16:47:15 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Hakim", "Tal", ""], ["Shimshoni", "Ilan", ""]]}, {"id": "1907.10008", "submitter": "Yoshikatsu Nakajima", "authors": "Yoshikatsu Nakajima, Byeongkeun Kang, Hideo Saito and Kris Kitani", "title": "Incremental Class Discovery for Semantic Segmentation with RGBD Sensing", "comments": "10 pages, To appear at IEEE International Conference on Computer\n  Vision (ICCV 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the task of open world semantic segmentation using RGBD\nsensing to discover new semantic classes over time. Although there are many\ntypes of objects in the real-word, current semantic segmentation methods make a\nclosed world assumption and are trained only to segment a limited number of\nobject classes. Towards a more open world approach, we propose a novel method\nthat incrementally learns new classes for image segmentation. The proposed\nsystem first segments each RGBD frame using both color and geometric\ninformation, and then aggregates that information to build a single segmented\ndense 3D map of the environment. The segmented 3D map representation is a key\ncomponent of our approach as it is used to discover new object classes by\nidentifying coherent regions in the 3D map that have no semantic label. The use\nof coherent region in the 3D map as a primitive element, rather than\ntraditional elements such as surfels or voxels, also significantly reduces the\ncomputational complexity and memory use of our method. It thus leads to\nsemi-real-time performance at {10.7}Hz when incrementally updating the dense 3D\nmap at every frame. Through experiments on the NYUDv2 dataset, we demonstrate\nthat the proposed method is able to correctly cluster objects of both known and\nunseen classes. We also show the quantitative comparison with the\nstate-of-the-art supervised methods, the processing time of each step, and the\ninfluences of each component.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:02:46 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Nakajima", "Yoshikatsu", ""], ["Kang", "Byeongkeun", ""], ["Saito", "Hideo", ""], ["Kitani", "Kris", ""]]}, {"id": "1907.10014", "submitter": "Florian Kluger", "authors": "Florian Kluger, Hanno Ackermann, Michael Ying Yang, Bodo Rosenhahn", "title": "Temporally Consistent Horizon Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The horizon line is an important geometric feature for many image processing\nand scene understanding tasks in computer vision. For instance, in navigation\nof autonomous vehicles or driver assistance, it can be used to improve 3D\nreconstruction as well as for semantic interpretation of dynamic environments.\nWhile both algorithms and datasets exist for single images, the problem of\nhorizon line estimation from video sequences has not gained attention. In this\npaper, we show how convolutional neural networks are able to utilise the\ntemporal consistency imposed by video sequences in order to increase the\naccuracy and reduce the variance of horizon line estimates. A novel CNN\narchitecture with an improved residual convolutional LSTM is presented for\ntemporally consistent horizon line estimation. We propose an adaptive loss\nfunction that ensures stable training as well as accurate results. Furthermore,\nwe introduce an extension of the KITTI dataset which contains precise horizon\nline labels for 43699 images across 72 video sequences. A comprehensive\nevaluation shows that the proposed approach consistently achieves superior\nperformance compared with existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:18:04 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 14:09:20 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Kluger", "Florian", ""], ["Ackermann", "Hanno", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1907.10015", "submitter": "Shao-Yuan Lo", "authors": "Shao-Yuan Lo, Hsueh-Ming Hang", "title": "Exploring Semantic Segmentation on the DCT Representation", "comments": "Accepted in ACM International Conference on Multimedia in Asia\n  (MMAsia) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical convolutional networks are trained and conducted on RGB images.\nHowever, images are often compressed for memory savings and efficient\ntransmission in real-world applications. In this paper, we explore methods for\nperforming semantic segmentation on the discrete cosine transform (DCT)\nrepresentation defined by the JPEG standard. We first rearrange the DCT\ncoefficients to form a preferred input type, then we tailor an existing network\nto the DCT inputs. The proposed method has an accuracy close to the RGB model\nat about the same network complexity. Moreover, we investigate the impact of\nselecting different DCT components on segmentation performance. With a proper\nselection, one can achieve the same level accuracy using only 36% of the DCT\ncoefficients. We further show the robustness of our method under the\nquantization errors. To our knowledge, this paper is the first to explore\nsemantic segmentation on the DCT representation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:18:39 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 07:31:45 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lo", "Shao-Yuan", ""], ["Hang", "Hsueh-Ming", ""]]}, {"id": "1907.10032", "submitter": "Dong Zhang", "authors": "Dong Zhang, Guang Yang, Shu Zhao, Yanping Zhang, Heye Zhang, Shuo Li", "title": "Direct Quantification for Coronary Artery Stenosis Using Multiview\n  Learning", "comments": "Refine some text in paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantification of the coronary artery stenosis is of significant clinical\nimportance in coronary artery disease diagnosis and intervention treatment. It\naims to quantify the morphological indices of the coronary artery lesions such\nas minimum lumen diameter, reference vessel diameter, lesion length, and these\nindices are the reference of the interventional stent placement. In this study,\nwe propose a direct multiview quantitative coronary angiography (DMQCA) model\nas an automatic clinical tool to quantify the coronary artery stenosis from\nX-ray coronary angiography images. The proposed DMQCA model consists of a\nmultiview module with two attention mechanisms, a key-frame module, and a\nregression module, to achieve direct accurate multiple-index estimation. The\nmulti-view module comprehensively learns the Spatio-temporal features of\ncoronary arteries through a three-dimensional convolution. The attention\nmechanisms of each view focus on the subtle feature of the lesion region and\ncapture the important context information. The key-frame module learns the\nsubtle features of the stenosis through successive dilated residual blocks. The\nregression module finally generates the indices estimation from multiple\nfeatures.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 16:12:18 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 06:45:26 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 02:51:47 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhang", "Dong", ""], ["Yang", "Guang", ""], ["Zhao", "Shu", ""], ["Zhang", "Yanping", ""], ["Zhang", "Heye", ""], ["Li", "Shuo", ""]]}, {"id": "1907.10033", "submitter": "Jinming Duan", "authors": "Jinming Duan, Jo Schlemper, Chen Qin, Cheng Ouyang, Wenjia Bai, Carlo\n  Biffi, Ghalib Bello, Ben Statton, Declan P O'Regan, Daniel Rueckert", "title": "VS-Net: Variable splitting network for accelerated parallel MRI\n  reconstruction", "comments": "Accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a deep learning approach for parallel magnetic\nresonance imaging (MRI) reconstruction, termed a variable splitting network\n(VS-Net), for an efficient, high-quality reconstruction of undersampled\nmulti-coil MR data. We formulate the generalized parallel compressed sensing\nreconstruction as an energy minimization problem, for which a variable\nsplitting optimization method is derived. Based on this formulation we propose\na novel, end-to-end trainable deep neural network architecture by unrolling the\nresulting iterative process of such variable splitting scheme. VS-Net is\nevaluated on complex valued multi-coil knee images for 4-fold and 6-fold\nacceleration factors. We show that VS-Net outperforms state-of-the-art deep\nlearning reconstruction algorithms, in terms of reconstruction accuracy and\nperceptual quality. Our code is publicly available at\nhttps://github.com/j-duan/VS-Net.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 21:04:53 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Duan", "Jinming", ""], ["Schlemper", "Jo", ""], ["Qin", "Chen", ""], ["Ouyang", "Cheng", ""], ["Bai", "Wenjia", ""], ["Biffi", "Carlo", ""], ["Bello", "Ghalib", ""], ["Statton", "Ben", ""], ["O'Regan", "Declan P", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1907.10043", "submitter": "Nilesh Kulkarni", "authors": "Nilesh Kulkarni, Abhinav Gupta, Shubham Tulsiani", "title": "Canonical Surface Mapping via Geometric Cycle Consistency", "comments": "To appear at ICCV 2019. Project page:\n  https://nileshkulkarni.github.io/csm/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the task of Canonical Surface Mapping (CSM). Specifically, given\nan image, we learn to map pixels on the object to their corresponding locations\non an abstract 3D model of the category. But how do we learn such a mapping? A\nsupervised approach would require extensive manual labeling which is not\nscalable beyond a few hand-picked categories. Our key insight is that the CSM\ntask (pixel to 3D), when combined with 3D projection (3D to pixel), completes a\ncycle. Hence, we can exploit a geometric cycle consistency loss, thereby\nallowing us to forgo the dense manual supervision. Our approach allows us to\ntrain a CSM model for a diverse set of classes, without sparse or dense\nkeypoint annotation, by leveraging only foreground mask labels for training. We\nshow that our predictions also allow us to infer dense correspondence between\ntwo images, and compare the performance of our approach against several methods\nthat predict correspondence by leveraging varying amount of supervision.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:55:12 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 17:42:54 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Kulkarni", "Nilesh", ""], ["Gupta", "Abhinav", ""], ["Tulsiani", "Shubham", ""]]}, {"id": "1907.10045", "submitter": "Denis Tome", "authors": "Denis Tome, Patrick Peluse, Lourdes Agapito and Hernan Badino", "title": "xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new solution to egocentric 3D body pose estimation from\nmonocular images captured from a downward looking fish-eye camera installed on\nthe rim of a head mounted virtual reality device. This unusual viewpoint, just\n2 cm. away from the user's face, leads to images with unique visual appearance,\ncharacterized by severe self-occlusions and strong perspective distortions that\nresult in a drastic difference in resolution between lower and upper body. Our\ncontribution is two-fold. Firstly, we propose a new encoder-decoder\narchitecture with a novel dual branch decoder designed specifically to account\nfor the varying uncertainty in the 2D joint locations. Our quantitative\nevaluation, both on synthetic and real-world datasets, shows that our strategy\nleads to substantial improvements in accuracy over state of the art egocentric\npose estimation approaches. Our second contribution is a new large-scale\nphotorealistic synthetic dataset - xR-EgoPose - offering 383K frames of high\nquality renderings of people with a diversity of skin tones, body shapes,\nclothing, in a variety of backgrounds and lighting conditions, performing a\nrange of actions. Our experiments show that the high variability in our new\nsynthetic training corpus leads to good generalization to real world footage\nand to state of the art results on real world datasets with ground truth.\nMoreover, an evaluation on the Human3.6M benchmark shows that the performance\nof our method is on par with top performing approaches on the more classic\nproblem of 3D human pose from a third person viewpoint.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:58:03 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Tome", "Denis", ""], ["Peluse", "Patrick", ""], ["Agapito", "Lourdes", ""], ["Badino", "Hernan", ""]]}, {"id": "1907.10046", "submitter": "Naftali Cohen", "authors": "Naftali Cohen, Tucker Balch, and Manuela Veloso", "title": "Trading via Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-fin.CP q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The art of systematic financial trading evolved with an array of approaches,\nranging from simple strategies to complex algorithms all relying, primary, on\naspects of time-series analysis. Recently, after visiting the trading floor of\na leading financial institution, we noticed that traders always execute their\ntrade orders while observing images of financial time-series on their screens.\nIn this work, we built upon the success in image recognition and examine the\nvalue in transforming the traditional time-series analysis to that of image\nclassification. We create a large sample of financial time-series images\nencoded as candlestick (Box and Whisker) charts and label the samples following\nthree algebraically-defined binary trade strategies. Using the images, we train\nover a dozen machine-learning classification models and find that the\nalgorithms are very efficient in recovering the complicated, multiscale\nlabel-generating rules when the data is represented visually. We suggest that\nthe transformation of continuous numeric time-series classification problem to\na vision problem is useful for recovering signals typical of technical\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:58:10 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 14:02:59 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 05:01:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Cohen", "Naftali", ""], ["Balch", "Tucker", ""], ["Veloso", "Manuela", ""]]}, {"id": "1907.10081", "submitter": "Dogucan Yaman", "authors": "Dogucan Yaman, Fevziye Irem Eyiokur, Haz{\\i}m Kemal Ekenel", "title": "Multimodal Age and Gender Classification Using Ear and Profile Face\n  Images", "comments": "8 pages, 4 figures, accepted for CVPR 2019 - Workshop on Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present multimodal deep neural network frameworks for age\nand gender classification, which take input a profile face image as well as an\near image. Our main objective is to enhance the accuracy of soft biometric\ntrait extraction from profile face images by additionally utilizing a promising\nbiometric modality: ear appearance. For this purpose, we provided end-to-end\nmultimodal deep learning frameworks. We explored different multimodal\nstrategies by employing data, feature, and score level fusion. To increase\nrepresentation and discrimination capability of the deep neural networks, we\nbenefited from domain adaptation and employed center loss besides softmax loss.\nWe conducted extensive experiments on the UND-F, UND-J2, and FERET datasets.\nExperimental results indicated that profile face images contain a rich source\nof information for age and gender classification. We found that the presented\nmultimodal system achieves very high age and gender classification accuracies.\nMoreover, we attained superior results compared to the state-of-the-art profile\nface image or ear image-based age and gender classification methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 18:04:16 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Yaman", "Dogucan", ""], ["Eyiokur", "Fevziye Irem", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "1907.10085", "submitter": "Angelica I. Aviles-Rivero", "authors": "Angelica I. Aviles-Rivero, Nicolas Papadakis, Ruoteng Li, Philip\n  Sellars, Qingnan Fan, Robby T. Tan, Carola-Bibiane Sch\\\"onlieb", "title": "GraphX$^{NET}-$ Chest X-Ray Classification Under Extreme Minimal\n  Supervision", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of classifying X-ray data is a problem of both theoretical and\nclinical interest. Whilst supervised deep learning methods rely upon huge\namounts of labelled data, the critical problem of achieving a good\nclassification accuracy when an extremely small amount of labelled data is\navailable has yet to be tackled. In this work, we introduce a novel\nsemi-supervised framework for X-ray classification which is based on a\ngraph-based optimisation model. To the best of our knowledge, this is the first\nmethod that exploits graph-based semi-supervised learning for X-ray data\nclassification. Furthermore, we introduce a new multi-class classification\nfunctional with carefully selected class priors which allows for a smooth\nsolution that strengthens the synergy between the limited number of labels and\nthe huge amount of unlabelled data. We demonstrate, through a set of numerical\nand visual experiments, that our method produces highly competitive results on\nthe ChestX-ray14 data set whilst drastically reducing the need for annotated\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 18:10:32 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 12:39:10 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 21:42:19 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Aviles-Rivero", "Angelica I.", ""], ["Papadakis", "Nicolas", ""], ["Li", "Ruoteng", ""], ["Sellars", "Philip", ""], ["Fan", "Qingnan", ""], ["Tan", "Robby T.", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1907.10087", "submitter": "Mohammed Daoudi", "authors": "Naima Otberdout, Mohamed Daoudi, Anis Kacem, Lahoucine Ballihi,\n  Stefano Berretti", "title": "Dynamic Facial Expression Generation on Hilbert Hypersphere with\n  Conditional Wasserstein Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2020.3002500", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel approach for generating videos of the six\nbasic facial expressions given a neutral face image. We propose to exploit the\nface geometry by modeling the facial landmarks motion as curves encoded as\npoints on a hypersphere. By proposing a conditional version of manifold-valued\nWasserstein generative adversarial network (GAN) for motion generation on the\nhypersphere, we learn the distribution of facial expression dynamics of\ndifferent classes, from which we synthesize new facial expression motions. The\nresulting motions can be transformed to sequences of landmarks and then to\nimages sequences by editing the texture information using another conditional\nGenerative Adversarial Network. To the best of our knowledge, this is the first\nwork that explores manifold-valued representations with GAN to address the\nproblem of dynamic facial expression generation. We evaluate our proposed\napproach both quantitatively and qualitatively on two public datasets;\nOulu-CASIA and MUG Facial Expression. Our experimental results demonstrate the\neffectiveness of our approach in generating realistic videos with continuous\nmotion, realistic appearance and identity preservation. We also show the\nefficiency of our framework for dynamic facial expressions generation, dynamic\nfacial expression transfer and data augmentation for training improved emotion\nrecognition models.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 18:22:18 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 18:14:42 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Otberdout", "Naima", ""], ["Daoudi", "Mohamed", ""], ["Kacem", "Anis", ""], ["Ballihi", "Lahoucine", ""], ["Berretti", "Stefano", ""]]}, {"id": "1907.10104", "submitter": "Omid Abdollahi Aghdam", "authors": "Omid Abdollahi Aghdam, Behzad Bozorgtabar, Haz{\\i}m Kemal Ekenel and\n  Jean-Philippe Thiran", "title": "Exploring Factors for Improving Low Resolution Face Recognition", "comments": "CVPR Workshop on Biometrics 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep face recognition approaches report near perfect\nperformance on popular benchmarks, e.g., Labeled Faces in the Wild. However,\ntheir performance deteriorates significantly when they are applied on low\nquality images, such as those acquired by surveillance cameras. A further\nchallenge for low resolution face recognition for surveillance applications is\nthe matching of recorded low resolution probe face images with high resolution\nreference images, which could be the case in watchlist scenarios. In this\npaper, we have addressed these problems and investigated the factors that would\ncontribute to the identification performance of the state-of-the-art deep face\nrecognition models when they are applied to low resolution face recognition\nunder mismatched conditions. We have observed that the following factors affect\nperformance in a positive way: appearance variety and resolution distribution\nof the training dataset, resolution matching between the gallery and probe\nimages, and the amount of information included in the probe images. By\nleveraging this information, we have utilized deep face models trained on\nMS-Celeb-1M and fine-tuned on VGGFace2 dataset and achieved state-of-the-art\naccuracies on the SCFace and ICB-RW benchmarks, even without using any training\ndata from the datasets of these benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 19:16:48 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 17:22:08 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Aghdam", "Omid Abdollahi", ""], ["Bozorgtabar", "Behzad", ""], ["Ekenel", "Haz\u0131m Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1907.10107", "submitter": "Mengyao Zhai", "authors": "Mengyao Zhai, Lei Chen, Fred Tung, Jiawei He, Megha Nawhal, Greg Mori", "title": "Lifelong GAN: Continual Learning for Conditional Image Generation", "comments": "accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning is challenging for deep neural networks due to their\nsusceptibility to catastrophic forgetting. Catastrophic forgetting occurs when\na trained network is not able to maintain its ability to accomplish previously\nlearned tasks when it is trained to perform new tasks. We study the problem of\nlifelong learning for generative models, extending a trained network to new\nconditional generation tasks without forgetting previous tasks, while assuming\naccess to the training data for the current task only. In contrast to\nstate-of-the-art memory replay based approaches which are limited to\nlabel-conditioned image generation tasks, a more generic framework for\ncontinual learning of generative models under different conditional image\ngeneration settings is proposed in this paper. Lifelong GAN employs knowledge\ndistillation to transfer learned knowledge from previous networks to the new\nnetwork. This makes it possible to perform image-conditioned generation tasks\nin a lifelong learning setting. We validate Lifelong GAN for both\nimage-conditioned and label-conditioned generation tasks, and provide\nqualitative and quantitative results to show the generality and effectiveness\nof our method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 19:25:15 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 07:44:02 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Zhai", "Mengyao", ""], ["Chen", "Lei", ""], ["Tung", "Fred", ""], ["He", "Jiawei", ""], ["Nawhal", "Megha", ""], ["Mori", "Greg", ""]]}, {"id": "1907.10128", "submitter": "Siddhant Sahu", "authors": "Siddhant Sahu, Manoj Kumar Lenka and Pankaj Kumar Sa", "title": "Blind Deblurring using Deep Learning: A Survey", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We inspect all the deep learning based solutions and provide holistic\nunderstanding of various architectures that have evolved over the past few\nyears to solve blind deblurring. The introductory work used deep learning to\nestimate some features of the blur kernel and then moved onto predicting the\nblur kernel entirely, which converts the problem into non-blind deblurring. The\nrecent state of the art techniques are end to end, i.e., they don't estimate\nthe blur kernel rather try to estimate the latent sharp image directly from the\nblurred image. The benchmarking PSNR and SSIM values on standard datasets of\nGOPRO and Kohler using various architectures are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 21:04:29 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Sahu", "Siddhant", ""], ["Lenka", "Manoj Kumar", ""], ["Sa", "Pankaj Kumar", ""]]}, {"id": "1907.10132", "submitter": "Sebastian Niehaus", "authors": "Marie Kloenne, Sebastian Niehaus, Leonie Lampe, Alberto Merola, Janis\n  Reinelt, Ingo Roeder, Nico Scherf", "title": "Domain specific cues improve robustness of deep learning based\n  segmentation of ct volumes", "comments": null, "journal-ref": "Scientific Reports 10, 10712 (2020)", "doi": "10.1038/s41598-020-67544-y", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning has considerably improved medical image analysis in the past\nyears. Although data-driven approaches are intrinsically adaptive and thus,\ngeneric, they often do not perform the same way on data from different imaging\nmodalities. In particular Computed tomography (CT) data poses many challenges\nto medical image segmentation based on convolutional neural networks (CNNs),\nmostly due to the broad dynamic range of intensities and the varying number of\nrecorded slices of CT volumes. In this paper, we address these issues with a\nframework that combines domain-specific data preprocessing and augmentation\nwith state-of-the-art CNN architectures. The focus is not limited to optimise\nthe score, but also to stabilise the prediction performance since this is a\nmandatory requirement for use in automated and semi-automated workflows in the\nclinical environment.\n  The framework is validated with an architecture comparison to show CNN\narchitecture-independent effects of our framework functionality. We compare a\nmodified U-Net and a modified Mixed-Scale Dense Network (MS-D Net) to compare\ndilated convolutions for parallel multi-scale processing to the U-Net approach\nbased on traditional scaling operations. Finally, we propose an ensemble model\ncombining the strengths of different individual methods. The framework performs\nwell on a range of tasks such as liver and kidney segmentation, without\nsignificant differences in prediction performance on strongly differing volume\nsizes and varying slice thickness. Thus our framework is an essential step\ntowards performing robust segmentation of unknown real-world samples.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 21:11:22 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 14:37:34 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 09:26:58 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Kloenne", "Marie", ""], ["Niehaus", "Sebastian", ""], ["Lampe", "Leonie", ""], ["Merola", "Alberto", ""], ["Reinelt", "Janis", ""], ["Roeder", "Ingo", ""], ["Scherf", "Nico", ""]]}, {"id": "1907.10138", "submitter": "Javad Fotouhi", "authors": "Javad Fotouhi, Tianyu Song, Arian Mehrfard, Giacomo Taylor, Qiaochu\n  Wang, Fengfang Xian, Alejandro Martin-Gomez, Bernhard Fuerst, Mehran Armand,\n  Mathias Unberath, Nassir Navab", "title": "Reflective-AR Display: An Interaction Methodology for Virtual-Real\n  Alignment in Medical Robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot-assisted minimally invasive surgery has shown to improve patient\noutcomes, as well as reduce complications and recovery time for several\nclinical applications. While increasingly configurable robotic arms can\nmaximize reach and avoid collisions in cluttered environments, positioning them\nappropriately during surgery is complicated because safety regulations prevent\nautomatic driving. We propose a head-mounted display (HMD) based augmented\nreality (AR) system designed to guide optimal surgical arm set up. The staff\nequipped with HMD aligns the robot with its planned virtual counterpart. In\nthis user-centric setting, the main challenge is the perspective ambiguities\nhindering such collaborative robotic solution. To overcome this challenge, we\nintroduce a novel registration concept for intuitive alignment of AR content to\nits physical counterpart by providing a multi-view AR experience via\nreflective-AR displays that simultaneously show the augmentations from multiple\nviewpoints. Using this system, users can visualize different perspectives while\nactively adjusting the pose to determine the registration transformation that\nmost closely superimposes the virtual onto the real. The experimental results\ndemonstrate improvement in the interactive alignment of a virtual and real\nrobot when using a reflective-AR display. We also present measurements from\nconfiguring a robotic manipulator in a simulated trocar placement surgery using\nthe AR guidance methodology.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 21:27:44 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 19:16:19 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Fotouhi", "Javad", ""], ["Song", "Tianyu", ""], ["Mehrfard", "Arian", ""], ["Taylor", "Giacomo", ""], ["Wang", "Qiaochu", ""], ["Xian", "Fengfang", ""], ["Martin-Gomez", "Alejandro", ""], ["Fuerst", "Bernhard", ""], ["Armand", "Mehran", ""], ["Unberath", "Mathias", ""], ["Navab", "Nassir", ""]]}, {"id": "1907.10148", "submitter": "Hamid Hekmatian", "authors": "Hamid Hekmatian, Jingfu Jin and Samir Al-Stouhi", "title": "Conf-Net: Toward High-Confidence Dense 3D Point-Cloud with Error-Map\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a method for depth completion of sparse LiDAR data using a\nconvolutional neural network which can be used to generate semi-dense depth\nmaps and \"almost\" full 3D point-clouds with significantly lower root mean\nsquared error (RMSE) over state-of-the-art methods. We add an \"Error\nPrediction\" unit to our network and present a novel and simple end-to-end\nmethod that learns to predict an error-map of depth regression task. An\n\"almost\" dense high-confidence/low-variance point-cloud is more valuable for\nsafety-critical applications specifically real-world autonomous driving than a\nfull point-cloud with high error rate and high error variance. Using our\npredicted error-map, we demonstrate that by up-filling a LiDAR point cloud from\n18,000 points to 285,000 points, versus 300,000 points for full depth, we can\nreduce the RMSE error from 1004 to 399. This error is approximately 60% less\nthan the state-of-the-art and 50% less than the state-of-the-art with RGB\nguidance (we did not use RGB guidance in our algorithm). In addition to\nanalyzing our results on Kitti depth completion dataset, we also demonstrate\nthe ability of our proposed method to extend to new tasks by deploying our\n\"Error Prediction\" unit to improve upon the state-of-the-art for monocular\ndepth estimation. Codes and demo videos are available at\nhttp://github.com/hekmak/Conf-net.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 21:41:54 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 15:11:43 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 18:52:58 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Hekmatian", "Hamid", ""], ["Jin", "Jingfu", ""], ["Al-Stouhi", "Samir", ""]]}, {"id": "1907.10156", "submitter": "Qi Qian", "authors": "Qi Qian, Lei Chen, Hao Li, Rong Jin", "title": "DR Loss: Improving Object Detection by Distributional Ranking", "comments": "accepted by CVPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of object detection algorithms can be categorized into two classes:\ntwo-stage detectors and one-stage detectors. Recently, many efforts have been\ndevoted to one-stage detectors for the simple yet effective architecture.\nDifferent from two-stage detectors, one-stage detectors aim to identify\nforeground objects from all candidates in a single stage. This architecture is\nefficient but can suffer from the imbalance issue with respect to two aspects:\nthe inter-class imbalance between the number of candidates from foreground and\nbackground classes and the intra-class imbalance in the hardness of background\ncandidates, where only a few candidates are hard to be identified. In this\nwork, we propose a novel distributional ranking (DR) loss to handle the\nchallenge. For each image, we convert the classification problem to a ranking\nproblem, which considers pairs of candidates within the image, to address the\ninter-class imbalance problem. Then, we push the distributions of confidence\nscores for foreground and background towards the decision boundary. After that,\nwe optimize the rank of the expectations of derived distributions in lieu of\noriginal pairs. Our method not only mitigates the intra-class imbalance issue\nin background candidates but also improves the efficiency for the ranking\nalgorithm. By merely replacing the focal loss in RetinaNet with the developed\nDR loss and applying ResNet-101 as the backbone, mAP of the single-scale test\non COCO can be improved from 39.1% to 41.7% without bells and whistles, which\ndemonstrates the effectiveness of the proposed loss function. Code is available\nat \\url{https://github.com/idstcv/DR_loss}.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 22:14:38 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 18:13:55 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 17:14:36 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Qian", "Qi", ""], ["Chen", "Lei", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "1907.10164", "submitter": "Keren Ye", "authors": "Keren Ye, Mingda Zhang, Adriana Kovashka, Wei Li, Danfeng Qin, Jesse\n  Berent", "title": "Cap2Det: Learning to Amplify Weak Caption Supervision for Object\n  Detection", "comments": "To appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to localize and name object instances is a fundamental problem in\nvision, but state-of-the-art approaches rely on expensive bounding box\nsupervision. While weakly supervised detection (WSOD) methods relax the need\nfor boxes to that of image-level annotations, even cheaper supervision is\nnaturally available in the form of unstructured textual descriptions that users\nmay freely provide when uploading image content. However, straightforward\napproaches to using such data for WSOD wastefully discard captions that do not\nexactly match object names. Instead, we show how to squeeze the most\ninformation out of these captions by training a text-only classifier that\ngeneralizes beyond dataset boundaries. Our discovery provides an opportunity\nfor learning detection models from noisy but more abundant and freely-available\ncaption data. We also validate our model on three classic object detection\nbenchmarks and achieve state-of-the-art WSOD performance. Our code is available\nat https://github.com/yekeren/Cap2Det.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 22:39:37 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 21:44:59 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 08:35:36 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Ye", "Keren", ""], ["Zhang", "Mingda", ""], ["Kovashka", "Adriana", ""], ["Li", "Wei", ""], ["Qin", "Danfeng", ""], ["Berent", "Jesse", ""]]}, {"id": "1907.10178", "submitter": "Luca Anthony Thiede", "authors": "Luca Anthony Thiede and Pratik Prabhanjan Brahma", "title": "Analyzing the Variety Loss in the Context of Probabilistic Trajectory\n  Prediction", "comments": "Accepted for publication at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory or behavior prediction of traffic agents is an important component\nof autonomous driving and robot planning in general. It can be framed as a\nprobabilistic future sequence generation problem and recent literature has\nstudied the applicability of generative models in this context. The variety or\nMinimum over N (MoN) loss, which tries to minimize the error between the ground\ntruth and the closest of N output predictions, has been used in these recent\nlearning models to improve the diversity of predictions. In this work, we\npresent a proof to show that the MoN loss does not lead to the ground truth\nprobability density function, but approximately to its square root instead. We\nvalidate this finding with extensive experiments on both simulated toy as well\nas real world datasets. We also propose multiple solutions to compensate for\nthe dilation to show improvement of log likelihood of the ground truth samples\nin the corrected probability density function.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 23:56:02 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Thiede", "Luca Anthony", ""], ["Brahma", "Pratik Prabhanjan", ""]]}, {"id": "1907.10202", "submitter": "Feng-Ju Chang", "authors": "Feng-Ju Chang, Xiang Yu, Ram Nevatia, Manmohan Chandraker", "title": "Pose-variant 3D Facial Attribute Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging problem of generating facial attributes using a\nsingle image in an unconstrained pose. In contrast to prior works that largely\nconsider generation on 2D near-frontal images, we propose a GAN-based framework\nto generate attributes directly on a dense 3D representation given by UV\ntexture and position maps, resulting in photorealistic,\ngeometrically-consistent and identity-preserving outputs. Starting from a\nself-occluded UV texture map obtained by applying an off-the-shelf 3D\nreconstruction method, we propose two novel components. First, a texture\ncompletion generative adversarial network (TC-GAN) completes the partial UV\ntexture map. Second, a 3D attribute generation GAN (3DA-GAN) synthesizes the\ntarget attribute while obtaining an appearance consistent with 3D face geometry\nand preserving identity. Extensive experiments on CelebA, LFW and IJB-A show\nthat our method achieves consistently better attribute generation accuracy than\nprior methods, a higher degree of qualitative photorealism and preserves face\nidentity information.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 01:52:33 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Chang", "Feng-Ju", ""], ["Yu", "Xiang", ""], ["Nevatia", "Ram", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1907.10209", "submitter": "Yu Cheng", "authors": "Duo Wang, Ming Li, Nir Ben-Shlomo, C. Eduardo Corrales, Yu Cheng, Tao\n  Zhang, Jagadeesan Jayender", "title": "Mixed-Supervised Dual-Network for Medical Image Segmentation", "comments": "To appear in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based medical image segmentation models usually require large\ndatasets with high-quality dense segmentations to train, which are very\ntime-consuming and expensive to prepare. One way to tackle this challenge is by\nusing the mixed-supervised learning framework, in which only a part of data is\ndensely annotated with segmentation label and the rest is weakly labeled with\nbounding boxes. The model is trained jointly in a multi-task learning setting.\nIn this paper, we propose Mixed-Supervised Dual-Network (MSDN), a novel\narchitecture which consists of two separate networks for the detection and\nsegmentation tasks respectively, and a series of connection modules between the\nlayers of the two networks. These connection modules are used to transfer\nuseful information from the auxiliary detection task to help the segmentation\ntask. We propose to use a recent technique called \"Squeeze and Excitation\" in\nthe connection module to boost the transfer. We conduct experiments on two\nmedical image segmentation datasets. The proposed MSDN model outperforms\nmultiple baselines.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 02:31:45 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 05:45:09 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Wang", "Duo", ""], ["Li", "Ming", ""], ["Ben-Shlomo", "Nir", ""], ["Corrales", "C. Eduardo", ""], ["Cheng", "Yu", ""], ["Zhang", "Tao", ""], ["Jayender", "Jagadeesan", ""]]}, {"id": "1907.10210", "submitter": "Jian Zhu", "authors": "Jian Zhu, Will Styler and Ian Calloway", "title": "A CNN-based tool for automatic tongue contour tracking in ultrasound\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For speech research, ultrasound tongue imaging provides a non-invasive means\nfor visualizing tongue position and movement during articulation. Extracting\ntongue contours from ultrasound images is a basic step in analyzing ultrasound\ndata but this task often requires non-trivial manual annotation. This study\npresents an open source tool for fully automatic tracking of tongue contours in\nultrasound frames using neural network based methods. We have implemented and\nsystematically compared two convolutional neural networks, U-Net and\nDenseU-Net, under different conditions. Though both models can perform\nautomatic contour tracking with comparable accuracy, Dense U-Net architecture\nseems more generalizable across test datasets while U-Net has faster extraction\nspeed. Our comparison also shows that the choice of loss function and data\naugmentation have a greater effect on tracking performance in this task. This\npublic available segmentation tool shows considerable promise for the automated\ntongue contour annotation of ultrasound images in speech research.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 02:34:58 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zhu", "Jian", ""], ["Styler", "Will", ""], ["Calloway", "Ian", ""]]}, {"id": "1907.10211", "submitter": "Yi Zhu", "authors": "Yi Zhu and Shawn Newsam", "title": "Motion-Aware Feature for Improved Video Anomaly Detection", "comments": "BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by our observation that motion information is the key to good\nanomaly detection performance in video, we propose a temporal augmented network\nto learn a motion-aware feature. This feature alone can achieve competitive\nperformance with previous state-of-the-art methods, and when combined with\nthem, can achieve significant performance improvements. Furthermore, we\nincorporate temporal context into the Multiple Instance Learning (MIL) ranking\nmodel by using an attention block. The learned attention weights can help to\ndifferentiate between anomalous and normal video segments better. With the\nproposed motion-aware feature and the temporal MIL ranking model, we outperform\nprevious approaches by a large margin on both anomaly detection and anomalous\naction recognition tasks in the UCF Crime dataset.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 02:36:28 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zhu", "Yi", ""], ["Newsam", "Shawn", ""]]}, {"id": "1907.10213", "submitter": "Qi Zhang", "authors": "Qi Zhang, Huafeng Wang, Sichen Yang", "title": "Image Super-Resolution Using a Wavelet-based Generative Adversarial\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of super-resolution recons-truction.\nThis is a hot topic because super-resolution reconstruction has a wide range of\napplications in the medical field, remote sensing monitoring, and criminal\ninvestigation. Compared with traditional algorithms, the current\nsuper-resolution reconstruction algorithm based on deep learning greatly\nimproves the clarity of reconstructed pictures. Existing work like\nSuper-Resolution Using a Generative Adversarial Network (SRGAN) can effectively\nrestore the texture details of the image. However, experimentally verified that\nthe texture details of the image recovered by the SRGAN are not robust. In\norder to get super-resolution reconstructed images with richer high-frequency\ndetails, we improve the network structure and propose a super-resolution\nreconstruction algorithm combining wavelet transform and Generative Adversarial\nNetwork. The proposed algorithm can efficiently reconstruct high-resolution\nimages with rich global information and local texture details. We have trained\nour model by PyTorch framework and VOC2012 dataset, and tested it by Set5,\nSet14, BSD100 and Urban100 test datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 02:44:41 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zhang", "Qi", ""], ["Wang", "Huafeng", ""], ["Yang", "Sichen", ""]]}, {"id": "1907.10219", "submitter": "Fulin Tang", "authors": "Fulin Tang and Yihong Wu", "title": "Efficient Circle-Based Camera Pose Tracking Free of PnP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera pose tracking attracts much interest both from academic and industrial\ncommunities, of which the methods based on planar markers are easy to be\nimplemented. However, most of the existing methods need to identify multiple\npoints in the marker images for matching to space points. Then, PnP and RANSAC\nare used to compute the camera pose. If cameras move fast or are far away from\nmarkers, matching is easy to generate errors and even RANSAC cannot remove\nincorrect matching. Then, the result by PnP cannot have good performance. To\nsolve this problem, we design circular markers and represent 6D camera pose\nanalytically and unifiedly as very concise forms from each of the marker by\nprojective invariance. Afterwards, the pose is further optimized by a proposed\nnonlinear cost function based on a polar-n-direction geometric distance. The\nmethod is from imaged circle edges and without PnP/RANSAC, making pose tracking\nrobust and accurate. Experimental results show that the proposed method\noutperforms the state of the arts in terms of noise, blur, and distance from\ncamera to marker. Simultaneously, it can still run at about 100 FPS on a\nconsumer computer with only CPU.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 03:07:45 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Tang", "Fulin", ""], ["Wu", "Yihong", ""]]}, {"id": "1907.10226", "submitter": "Nidhi Seethapathi", "authors": "Nidhi Seethapathi, Shaofei Wang, Rachit Saluja, Gunnar Blohm, Konrad\n  P. Kording", "title": "Movement science needs different pose tracking algorithms", "comments": "13 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, computer science has made progress towards extracting\nbody pose from single camera photographs or videos. This promises to enable\nmovement science to detect disease, quantify movement performance, and take the\nscience out of the lab into the real world. However, current pose tracking\nalgorithms fall short of the needs of movement science; the types of movement\ndata that matter are poorly estimated. For instance, the metrics currently used\nfor evaluating pose tracking algorithms use noisy hand-labeled ground truth\ndata and do not prioritize precision of relevant variables like\nthree-dimensional position, velocity, acceleration, and forces which are\ncrucial for movement science. Here, we introduce the scientific disciplines\nthat use movement data, the types of data they need, and discuss the changes\nneeded to make pose tracking truly transformative for movement science.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 03:54:22 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Seethapathi", "Nidhi", ""], ["Wang", "Shaofei", ""], ["Saluja", "Rachit", ""], ["Blohm", "Gunnar", ""], ["Kording", "Konrad P.", ""]]}, {"id": "1907.10233", "submitter": "Lidan Zhang", "authors": "Lidan Zhang, Qi She, Ping Guo", "title": "Stochastic trajectory prediction with social graph network", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian trajectory prediction is a challenging task because of the\ncomplexity of real-world human social behaviors and uncertainty of the future\nmotion. For the first issue, existing methods adopt fully connected topology\nfor modeling the social behaviors, while ignoring non-symmetric pairwise\nrelationships. To effectively capture social behaviors of relevant pedestrians,\nwe utilize a directed social graph which is dynamically constructed on timely\nlocation and speed direction. Based on the social graph, we further propose a\nnetwork to collect social effects and accumulate with individual\nrepresentation, in order to generate destination-oriented and social-aware\nrepresentations. For the second issue, instead of modeling the uncertainty of\nthe entire future as a whole, we utilize a temporal stochastic method for\nsequentially learning a prior model of uncertainty during social interactions.\nThe prediction on the next step is then generated by sampling on the prior\nmodel and progressively decoding with a hierarchical LSTMs. Experimental\nresults on two public datasets show the effectiveness of our method, especially\nwhen predicting trajectories in very crowded scenes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 04:44:34 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zhang", "Lidan", ""], ["She", "Qi", ""], ["Guo", "Ping", ""]]}, {"id": "1907.10244", "submitter": "Hyeongmin Lee", "authors": "Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun Pak, Yuseok Ban,\n  Sangyoun Lee", "title": "AdaCoF: Adaptive Collaboration of Flows for Video Frame Interpolation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame interpolation is one of the most challenging tasks in video\nprocessing research. Recently, many studies based on deep learning have been\nsuggested. Most of these methods focus on finding locations with useful\ninformation to estimate each output pixel using their own frame warping\noperations. However, many of them have Degrees of Freedom (DoF) limitations and\nfail to deal with the complex motions found in real world videos. To solve this\nproblem, we propose a new warping module named Adaptive Collaboration of Flows\n(AdaCoF). Our method estimates both kernel weights and offset vectors for each\ntarget pixel to synthesize the output frame. AdaCoF is one of the most\ngeneralized warping modules compared to other approaches, and covers most of\nthem as special cases of it. Therefore, it can deal with a significantly wide\ndomain of complex motions. To further improve our framework and synthesize more\nrealistic outputs, we introduce dual-frame adversarial loss which is applicable\nonly to video frame interpolation tasks. The experimental results show that our\nmethod outperforms the state-of-the-art methods for both fixed training set\nenvironments and the Middlebury benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 05:40:53 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 06:54:17 GMT"}, {"version": "v3", "created": "Sun, 8 Mar 2020 13:19:57 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Lee", "Hyeongmin", ""], ["Kim", "Taeoh", ""], ["Chung", "Tae-young", ""], ["Pak", "Daehyun", ""], ["Ban", "Yuseok", ""], ["Lee", "Sangyoun", ""]]}, {"id": "1907.10250", "submitter": "Nitin Agarwal", "authors": "Nitin Agarwal, Sung-eui Yoon, M Gopi", "title": "Learning Embedding of 3D models with Quadric Loss", "comments": "Accepted to BMVC 2019 for Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharp features such as edges and corners play an important role in the\nperception of 3D models. In order to capture them better, we propose quadric\nloss, a point-surface loss function, which minimizes the quadric error between\nthe reconstructed points and the input surface. Computation of Quadric loss is\neasy, efficient since the quadric matrices can be computed apriori, and is\nfully differentiable, making quadric loss suitable for training point and mesh\nbased architectures. Through extensive experiments we show the merits and\ndemerits of quadric loss. When combined with Chamfer loss, quadric loss\nachieves better reconstruction results as compared to any one of them or other\npoint-surface loss functions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 05:57:27 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Agarwal", "Nitin", ""], ["Yoon", "Sung-eui", ""], ["Gopi", "M", ""]]}, {"id": "1907.10255", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A. Sindagi and Vishal M. Patel", "title": "HA-CCN: Hierarchical Attention-based Crowd Counting Network", "comments": "Accepted for publication at IEEE Transactions on Image Processing\n  (TIP) 2019", "journal-ref": null, "doi": "10.1109/TIP.2019.2928634", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image-based crowd counting has recently witnessed increased focus, but\nmany leading methods are far from optimal, especially in highly congested\nscenes. In this paper, we present Hierarchical Attention-based Crowd Counting\nNetwork (HA-CCN) that employs attention mechanisms at various levels to\nselectively enhance the features of the network. The proposed method, which is\nbased on the VGG16 network, consists of a spatial attention module (SAM) and a\nset of global attention modules (GAM). SAM enhances low-level features in the\nnetwork by infusing spatial segmentation information, whereas the GAM focuses\non enhancing channel-wise information in the higher level layers. The proposed\nmethod is a single-step training framework, simple to implement and achieves\nstate-of-the-art results on different datasets.\n  Furthermore, we extend the proposed counting network by introducing a novel\nset-up to adapt the network to different scenes and datasets via weak\nsupervision using image-level labels. This new set up reduces the burden of\nacquiring labour intensive point-wise annotations for new datasets while\nimproving the cross-dataset performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 06:20:14 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1907.10257", "submitter": "Jong Chul Ye", "authors": "Shujaat Khan, Jaeyoung Huh, Jong Chul Ye", "title": "Adaptive and Compressive Beamforming Using Deep Learning for Medical\n  Ultrasound", "comments": "This is a significantly extended version of the original paper in\n  arXiv:1901.01706. This paper is accepted for IEEE Transactions on\n  Ultrasonics, Ferroelectrics, and Frequency Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ultrasound (US) imaging, various types of adaptive beamforming techniques\nhave been investigated to improve the resolution and contrast-to-noise ratio of\nthe delay and sum (DAS) beamformers. Unfortunately, the performance of these\nadaptive beamforming approaches degrade when the underlying model is not\nsufficiently accurate and the number of channels decreases. To address this\nproblem, here we propose a deep learning-based beamformer to generate\nsignificantly improved images over widely varying measurement conditions and\nchannel subsampling patterns. In particular, our deep neural network is\ndesigned to directly process full or sub-sampled radio-frequency (RF) data\nacquired at various subsampling rates and detector configurations so that it\ncan generate high quality ultrasound images using a single beamformer. The\norigin of such input-dependent adaptivity is also theoretically analyzed.\nExperimental results using B-mode focused ultrasound confirm the efficacy of\nthe proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 06:30:30 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 10:54:20 GMT"}, {"version": "v3", "created": "Sun, 23 Feb 2020 16:15:20 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Khan", "Shujaat", ""], ["Huh", "Jaeyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1907.10267", "submitter": "Jun Chen", "authors": "Jun Chen, Heye Zhang, Yanping Zhang, Shu Zhao, Raad Mohiaddin, Tom\n  Wong, David Firmin, Guang Yang, Jennifer Keegan", "title": "Discriminative Consistent Domain Generation for Semi-supervised Learning", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based task systems normally rely on a large amount of manually\nlabeled training data, which is expensive to obtain and subject to operator\nvariations. Moreover, it does not always hold that the manually labeled data\nand the unlabeled data are sitting in the same distribution. In this paper, we\nalleviate these problems by proposing a discriminative consistent domain\ngeneration (DCDG) approach to achieve a semi-supervised learning. The\ndiscriminative consistent domain is achieved by a double-sided domain\nadaptation. The double-sided domain adaptation aims to make a fusion of the\nfeature spaces of labeled data and unlabeled data. In this way, we can fit the\ndifferences of various distributions between labeled data and unlabeled data.\nIn order to keep the discriminativeness of generated consistent domain for the\ntask learning, we apply an indirect learning for the double-sided domain\nadaptation. Based on the generated discriminative consistent domain, we can use\nthe unlabeled data to learn the task model along with the labeled data via a\nconsistent image generation. We demonstrate the performance of our proposed\nDCDG on the late gadolinium enhancement cardiac MRI (LGE-CMRI) images acquired\nfrom patients with atrial fibrillation in two clinical centers for the\nsegmentation of the left atrium anatomy (LA) and proximal pulmonary veins\n(PVs). The experiments show that our semi-supervised approach achieves\ncompelling segmentation results, which can prove the robustness of DCDG for the\nsemi-supervised learning using the unlabeled data along with labeled data\nacquired from a single center or multicenter studies.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 06:59:23 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Chen", "Jun", ""], ["Zhang", "Heye", ""], ["Zhang", "Yanping", ""], ["Zhao", "Shu", ""], ["Mohiaddin", "Raad", ""], ["Wong", "Tom", ""], ["Firmin", "David", ""], ["Yang", "Guang", ""], ["Keegan", "Jennifer", ""]]}, {"id": "1907.10270", "submitter": "Shaodi You", "authors": "Shaodi You, Erqi Huang, Shuaizhe Liang, Yongrong Zheng, Yunxiang Li,\n  Fan Wang, Sen Lin, Qiu Shen, Xun Cao, Diming Zhang, Yuanjiang Li, Yu Li, Ying\n  Fu, Boxin Shi, Feng Lu, Yinqiang Zheng, Robby T. Tan", "title": "Hyperspectral City V1.0 Dataset and Benchmark", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document introduces the background and the usage of the Hyperspectral\nCity Dataset and the benchmark. The documentation first starts with the\nbackground and motivation of the dataset. Follow it, we briefly describe the\nmethod of collecting the dataset and the processing method from raw dataset to\nthe final release dataset, specifically, the version 1.0. We also provide the\ndetailed usage of the dataset and the evaluation metric for submitted the\nresult for the 2019 Hyperspectral City Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:13:27 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 09:03:49 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 15:24:16 GMT"}, {"version": "v4", "created": "Wed, 26 Feb 2020 12:50:23 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["You", "Shaodi", ""], ["Huang", "Erqi", ""], ["Liang", "Shuaizhe", ""], ["Zheng", "Yongrong", ""], ["Li", "Yunxiang", ""], ["Wang", "Fan", ""], ["Lin", "Sen", ""], ["Shen", "Qiu", ""], ["Cao", "Xun", ""], ["Zhang", "Diming", ""], ["Li", "Yuanjiang", ""], ["Li", "Yu", ""], ["Fu", "Ying", ""], ["Shi", "Boxin", ""], ["Lu", "Feng", ""], ["Zheng", "Yinqiang", ""], ["Tan", "Robby T.", ""]]}, {"id": "1907.10274", "submitter": "Ying Qu", "authors": "Ying Qu and Zhenzhou Shao and Hairong Qi", "title": "Non-Local Representation based Mutual Affine-Transfer Network for\n  Photorealistic Stylization", "comments": "Accepted by IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE\n  INTELLIGENCE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic stylization aims to transfer the style of a reference photo\nonto a content photo in a natural fashion, such that the stylized image looks\nlike a real photo taken by a camera. State-of-the-art methods stylize the image\nlocally within each matched semantic region and are prone to global color\ninconsistency across semantic objects/parts, making the stylized image less\nphotorealistic. To tackle the challenging issues, we propose a non-local\nrepresentation scheme, constrained with a mutual affine-transfer network\n(NL-MAT). Through a dictionary-based decomposition, NL-MAT is able to\nsuccessfully decouple matched non-local representations and color information\nof the image pair, such that the context correspondence between the image pair\nis incorporated naturally, which largely facilitates local style transfer in a\nglobal-consistent fashion. To the best of our knowledge, this is the first\nattempt to address the photorealistic stylization problem with a non-local\nrepresentation scheme, such that no additional models or steps for semantic\nmatching are required during stylization. Experimental results demonstrate that\nthe proposed method is able to generate photorealistic results with local style\ntransfer while preserving both the spatial structure and global color\nconsistency of the content image.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:21:44 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 03:02:07 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Qu", "Ying", ""], ["Shao", "Zhenzhou", ""], ["Qi", "Hairong", ""]]}, {"id": "1907.10282", "submitter": "Dibyasundar Das", "authors": "Dibyasundar Das, Deepak Ranjan Nayak, Ratnakar Dash, Banshidhar Majhi", "title": "Backward-Forward Algorithm: An Improvement towards Extreme Learning\n  Machine", "comments": "12 Pages, 11 figures, to be submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extreme learning machine needs a large number of hidden nodes to\ngeneralize a single hidden layer neural network for a given training data-set.\nThe need for more number of hidden nodes suggests that the neural-network is\nmemorizing rather than generalizing the model. Hence, a supervised learning\nmethod is described here that uses Moore-Penrose approximation to determine\nboth input-weight and output-weight in two epochs, namely, backward-pass and\nforward-pass. The proposed technique has an advantage over the back-propagation\nmethod in terms of iterations required and is superior to the extreme learning\nmachine in terms of the number of hidden units necessary for generalization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:52:57 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 08:37:30 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 12:48:12 GMT"}, {"version": "v4", "created": "Mon, 7 Oct 2019 08:37:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Das", "Dibyasundar", ""], ["Nayak", "Deepak Ranjan", ""], ["Dash", "Ratnakar", ""], ["Majhi", "Banshidhar", ""]]}, {"id": "1907.10283", "submitter": "Hang Yin", "authors": "Chia-Hung Huang, Hang Yin, Yu-Wing Tai, Chi-Keung Tang", "title": "StableNet: Semi-Online, Multi-Scale Deep Video Stabilization", "comments": "Chia-Hung and Hang have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video stabilization algorithms are of greater importance nowadays with the\nprevalence of hand-held devices which unavoidably produce videos with\nundesirable shaky motions. In this paper we propose a data-driven online video\nstabilization method along with a paired dataset for deep learning. The network\nprocesses each unsteady frame progressively in a multi-scale manner, from low\nresolution to high resolution, and then outputs an affine transformation to\nstabilize the frame. Different from conventional methods which require explicit\nfeature tracking or optical flow estimation, the underlying stabilization\nprocess is learned implicitly from the training data, and the stabilization\nprocess can be done online. Since there are limited public video stabilization\ndatasets available, we synthesized unstable videos with different extent of\nshake that simulate real-life camera movement. Experiments show that our method\nis able to outperform other stabilization methods in several unstable samples\nwhile remaining comparable in general. Also, our method is tested on complex\ncontents and found robust enough to dampen these samples to some extent even it\nwas not explicitly trained in the contents.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:54:45 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Huang", "Chia-Hung", ""], ["Yin", "Hang", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1907.10292", "submitter": "Yunus Can Bilge", "authors": "Yunus Can Bilge, Nazli Ikizler-Cinbis, Ramazan Gokberk Cinbis", "title": "Zero-Shot Sign Language Recognition: Can Textual Data Uncover Sign\n  Languages?", "comments": "To appear in British Machine Vision Conference (BMVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of zero-shot sign language recognition (ZSSLR),\nwhere the goal is to leverage models learned over the seen sign class examples\nto recognize the instances of unseen signs. To this end, we propose to utilize\nthe readily available descriptions in sign language dictionaries as an\nintermediate-level semantic representation for knowledge transfer. We introduce\na new benchmark dataset called ASL-Text that consists of 250 sign language\nclasses and their accompanying textual descriptions. Compared to the ZSL\ndatasets in other domains (such as object recognition), our dataset consists of\nlimited number of training examples for a large number of classes, which\nimposes a significant challenge. We propose a framework that operates over the\nbody and hand regions by means of 3D-CNNs, and models longer temporal\nrelationships via bidirectional LSTMs. By leveraging the descriptive text\nembeddings along with these spatio-temporal representations within a zero-shot\nlearning framework, we show that textual data can indeed be useful in\nuncovering sign languages. We anticipate that the introduced approach and the\naccompanying dataset will provide a basis for further exploration of this new\nzero-shot learning problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 08:12:41 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Bilge", "Yunus Can", ""], ["Ikizler-Cinbis", "Nazli", ""], ["Cinbis", "Ramazan Gokberk", ""]]}, {"id": "1907.10303", "submitter": "Chenglong Li", "authors": "Chenglong Li, Wei Xia, Yan Yan, Bin Luo, and Jin Tang", "title": "Segmenting Objects in Day and Night:Edge-Conditioned CNN for Thermal\n  Image Semantic Segmentation", "comments": "Submitted it to IEEE TNNLS. arXiv admin note: text overlap with\n  arXiv:1806.01013 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite much research progress in image semantic segmentation, it remains\nchallenging under adverse environmental conditions caused by imaging\nlimitations of visible spectrum. While thermal infrared cameras have several\nadvantages over cameras for the visible spectrum, such as operating in total\ndarkness, insensitive to illumination variations, robust to shadow effects and\nstrong ability to penetrate haze and smog. These advantages of thermal infrared\ncameras make the segmentation of semantic objects in day and night. In this\npaper, we propose a novel network architecture, called edge-conditioned\nconvolutional neural network (EC-CNN), for thermal image semantic segmentation.\nParticularly, we elaborately design a gated feature-wise transform layer in\nEC-CNN to adaptively incorporate edge prior knowledge. The whole EC-CNN is\nend-to-end trained, and can generate high-quality segmentation results with the\nedge guidance. Meanwhile, we also introduce a new benchmark dataset named\n\"Segment Objects in Day And night\"(SODA) for comprehensive evaluations in\nthermal image semantic segmentation. SODA contains over 7,168 manually\nannotated and synthetically generated thermal images with 20 semantic region\nlabels and from a broad range of viewpoints and scene complexities. Extensive\nexperiments on SODA demonstrate the effectiveness of the proposed EC-CNN\nagainst the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 08:49:27 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Li", "Chenglong", ""], ["Xia", "Wei", ""], ["Yan", "Yan", ""], ["Luo", "Bin", ""], ["Tang", "Jin", ""]]}, {"id": "1907.10310", "submitter": "Haichao Zhang", "authors": "Haichao Zhang, Jianyu Wang", "title": "Towards Adversarially Robust Object Detection", "comments": "ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an important vision task and has emerged as an\nindispensable component in many vision system, rendering its robustness as an\nincreasingly important performance factor for practical applications. While\nobject detection models have been demonstrated to be vulnerable against\nadversarial attacks by many recent works, very few efforts have been devoted to\nimproving their robustness. In this work, we take an initial attempt towards\nthis direction. We first revisit and systematically analyze object detectors\nand many recently developed attacks from the perspective of model robustness.\nWe then present a multi-task learning perspective of object detection and\nidentify an asymmetric role of task losses. We further develop an adversarial\ntraining approach which can leverage the multiple sources of attacks for\nimproving the robustness of detection models. Extensive experiments on\nPASCAL-VOC and MS-COCO verified the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 09:04:23 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zhang", "Haichao", ""], ["Wang", "Jianyu", ""]]}, {"id": "1907.10326", "submitter": "Jin Han Lee", "authors": "Jin Han Lee, Myung-Kyu Han, Dong Wook Ko and Il Hong Suh", "title": "From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating accurate depth from a single image is challenging because it is an\nill-posed problem as infinitely many 3D scenes can be projected to the same 2D\nscene. However, recent works based on deep convolutional neural networks show\ngreat progress with plausible results. The convolutional neural networks are\ngenerally composed of two parts: an encoder for dense feature extraction and a\ndecoder for predicting the desired depth. In the encoder-decoder schemes,\nrepeated strided convolution and spatial pooling layers lower the spatial\nresolution of transitional outputs, and several techniques such as skip\nconnections or multi-layer deconvolutional networks are adopted to recover back\nto the original resolution for effective dense prediction. In this paper, for\nmore effective guidance of densely encoded features to the desired depth\nprediction, we propose a network architecture that utilizes novel local planar\nguidance layers located at multiple stages in the decoding phase. We show that\nthe proposed method outperforms the state-of-the-art works with significant\nmargin evaluating on challenging benchmarks. We also provide results from an\nablation study to validate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 09:31:24 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 00:51:50 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 04:58:39 GMT"}, {"version": "v4", "created": "Mon, 26 Aug 2019 08:10:54 GMT"}, {"version": "v5", "created": "Fri, 6 Mar 2020 05:23:27 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Lee", "Jin Han", ""], ["Han", "Myung-Kyu", ""], ["Ko", "Dong Wook", ""], ["Suh", "Il Hong", ""]]}, {"id": "1907.10343", "submitter": "Zhenwei He", "authors": "Zhenwei He, Lei Zhang", "title": "Multi-adversarial Faster-RCNN for Unrestricted Object Detection", "comments": "This paper is accepted in ICCV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional object detection methods essentially suppose that the training\nand testing data are collected from a restricted target domain with expensive\nlabeling cost. For alleviating the problem of domain dependency and cumbersome\nlabeling, this paper proposes to detect objects in an unrestricted environment\nby leveraging domain knowledge trained from an auxiliary source domain with\nsufficient labels. Specifically, we propose a multi-adversarial Faster-RCNN\n(MAF) framework for unrestricted object detection, which inherently addresses\ndomain disparity minimization for domain adaptation in feature representation.\nThe paper merits are in three-fold: 1) With the idea that object detectors\noften becomes domain incompatible when image distribution resulted domain\ndisparity appears, we propose a hierarchical domain feature alignment module,\nin which multiple adversarial domain classifier submodules for layer-wise\ndomain feature confusion are designed; 2) An information invariant scale\nreduction module (SRM) for hierarchical feature map resizing is proposed for\npromoting the training efficiency of adversarial domain adaptation; 3) In order\nto improve the domain adaptability, the aggregated proposal features with\ndetection results are feed into a proposed weighted gradient reversal layer\n(WGRL) for characterizing hard confused domain samples. We evaluate our MAF on\nunrestricted tasks, including Cityscapes, KITTI, Sim10k, etc. and the\nexperiments show the state-of-the-art performance over the existing detectors.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 10:12:36 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 02:08:35 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["He", "Zhenwei", ""], ["Zhang", "Lei", ""]]}, {"id": "1907.10346", "submitter": "Jiechao Ma", "authors": "Jiechao Ma, Yingqian Chen, Yu Chen, Fengkai Wan, Sumin Xue, Ziping Li,\n  Shiting Feng", "title": "Delving Deep into Liver Focal Lesion Detection: A Preliminary Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hepatocellular carcinoma (HCC) is the second most frequent cause of\nmalignancy-related death and is one of the diseases with the highest incidence\nin the world. Because the liver is the only organ in the human body that is\nsupplied by two major vessels: the hepatic artery and the portal vein, various\ntypes of malignant tumors can spread from other organs to the liver. And due to\nthe liver masses' heterogeneous and diffusive shape, the tumor lesions are very\ndifficult to be recognized, thus automatic lesion detection is necessary for\nthe doctors with huge workloads. To assist doctors, this work uses the existing\nlarge-scale annotation medical image data to delve deep into liver lesion\ndetection from multiple directions. To solve technical difficulties, such as\nthe image-recognition task, traditional deep learning with convolution neural\nnetworks (CNNs) has been widely applied in recent years. However, this kind of\nneural network, such as Faster Regions with CNN features (R-CNN), cannot\nleverage the spatial information because it is applied in natural images (2D)\nrather than medical images (3D), such as computed tomography (CT) images. To\naddress this issue, we propose a novel algorithm that is appropriate for liver\nCT imaging. Furthermore, according to radiologists' experience in clinical\ndiagnosis and the characteristics of CT images of liver cancer, a liver\ncancer-detection framework with CNN, including image processing, feature\nextraction, region proposal, image registration, and classification\nrecognition, was proposed to facilitate the effective detection of liver\nlesions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 10:24:41 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Ma", "Jiechao", ""], ["Chen", "Yingqian", ""], ["Chen", "Yu", ""], ["Wan", "Fengkai", ""], ["Xue", "Sumin", ""], ["Li", "Ziping", ""], ["Feng", "Shiting", ""]]}, {"id": "1907.10354", "submitter": "Ricardo Ara\\'ujo", "authors": "Ricardo J. Ara\\'ujo, Vera Garrido, Catarina A. Bara\\c{c}as, Maria A.\n  Vasconcelos, Carlos Mavioso, Jo\\~ao C. Anacleto, Maria J. Cardoso, H\\'elder\n  P. Oliveira", "title": "Computer Aided Detection of Deep Inferior Epigastric Perforators in\n  Computed Tomography Angiography scans", "comments": "26 pages, 2 tables, 9 figures, submitted to Computerized Medical\n  Imaging and Graphics journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep inferior epigastric artery perforator (DIEAP) flap is the most\ncommon free flap used for breast reconstruction after a mastectomy. It makes\nuse of the skin and fat of the lower abdomen to build a new breast mound either\nat the same time of the mastectomy or in a second surgery. This operation\nrequires preoperative imaging studies to evaluate the branches - the\nperforators - that irrigate the tissue that will be used to reconstruct the\nbreast mound. These branches will support tissue viability after the\nmicrosurgical ligation of the inferior epigastric vessels to the receptor\nvessels in the thorax. Usually through a Computed Tomography Angiography (CTA),\neach perforator, diameter and direction is manually identified by the imaging\nteam, who will subsequently draw a map for the identification of the best\nvascular support for the reconstruction. In the current work we propose a\nsemi-automatic methodology that aims at reducing the time and subjectivity\ninherent to the manual annotation. In 21 CTAs from patients proposed for breast\nreconstruction with DIEAP flaps, the subcutaneous region of each perforator was\nextracted, by means of a tracking procedure, whereas the intramuscular portion\nwas detected through a minimum cost approach. Both were subsequently compared\nwith the radiologist manual annotation. Results showed that the semi-automatic\nprocedure was able to correctly detect the course of the DIEAPs with a minimum\nerror (average error of 0.64 mm and 0.50 mm regarding the extraction of\nsubcutaneous and intramuscular paths, respectively). The objective methodology\nis a promising tool in the automatic detection of perforators in CTA and can\ncontribute to spare human resources and reduce subjectivity in the\naforementioned task.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 10:44:30 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Ara\u00fajo", "Ricardo J.", ""], ["Garrido", "Vera", ""], ["Bara\u00e7as", "Catarina A.", ""], ["Vasconcelos", "Maria A.", ""], ["Mavioso", "Carlos", ""], ["Anacleto", "Jo\u00e3o C.", ""], ["Cardoso", "Maria J.", ""], ["Oliveira", "H\u00e9lder P.", ""]]}, {"id": "1907.10367", "submitter": "Soshi Shimada", "authors": "Soshi Shimada, Vladislav Golyanik, Edgar Tretschk, Didier Stricker,\n  Christian Theobalt", "title": "DispVoxNets: Non-Rigid Point Set Alignment with Supervised Learning\n  Proxies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a supervised-learning framework for non-rigid point set\nalignment of a new kind - Displacements on Voxels Networks (DispVoxNets) -\nwhich abstracts away from the point set representation and regresses 3D\ndisplacement fields on regularly sampled proxy 3D voxel grids. Thanks to\nrecently released collections of deformable objects with known intra-state\ncorrespondences, DispVoxNets learn a deformation model and further priors\n(e.g., weak point topology preservation) for different object categories such\nas cloths, human bodies and faces. DispVoxNets cope with large deformations,\nnoise and clustered outliers more robustly than the state-of-the-art. At test\ntime, our approach runs orders of magnitude faster than previous techniques.\nAll properties of DispVoxNets are ascertained numerically and qualitatively in\nextensive experiments and comparisons to several previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 11:14:10 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 07:11:55 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Shimada", "Soshi", ""], ["Golyanik", "Vladislav", ""], ["Tretschk", "Edgar", ""], ["Stricker", "Didier", ""], ["Theobalt", "Christian", ""]]}, {"id": "1907.10370", "submitter": "Vishal Srivastava Dr", "authors": "Kavita Dubey, Anant Agarwal, Astitwa Sarthak Lathe, Ranjeet Kumar and\n  Vishal Srivastava", "title": "Self-attention based BiLSTM-CNN classifier for the prediction of\n  ischemic and non-ischemic cardiomyopathy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart Failure is a major component of healthcare expenditure and a leading\ncause of mortality worldwide. Despite higher inter-rater variability,\nendomyocardial biopsy (EMB) is still regarded as the standard technique, used\nto identify the cause (e.g. ischemic or non-ischemic cardiomyopathy, coronary\nartery disease, myocardial infarction etc.) of unexplained heart failure. In\nthis paper, we focus on identifying cardiomyopathy as ischemic or non-ischemic.\nFor this, we propose and implement a new unified architecture comprising CNN\n(inception-V3 model) and bidirectional LSTM (BiLSTM) with self-attention\nmechanism to predict the ischemic or non-ischemic to classify cardiomyopathy\nusing histopathological images. The proposed model is based on self-attention\nthat implicitly focuses on the information outputted from the hidden layers of\nBiLSTM. Through our results we demonstrate that this framework carries a high\nlearning capacity and is able to improve the classification performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 11:34:44 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 11:39:52 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 06:46:33 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Dubey", "Kavita", ""], ["Agarwal", "Anant", ""], ["Lathe", "Astitwa Sarthak", ""], ["Kumar", "Ranjeet", ""], ["Srivastava", "Vishal", ""]]}, {"id": "1907.10384", "submitter": "Chirag Raman", "authors": "Chirag Raman, Hayley Hung", "title": "Towards automatic estimation of conversation floors within F-formations", "comments": "8th International Conference on Affective Computing & Intelligent\n  Interaction EMERGent Workshop, 7 pages, 4 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of free-standing conversing groups has received significant\nattention in recent years. In the absence of a formal definition, most studies\noperationalize the notion of a conversation group either through a spatial or a\ntemporal lens. Spatially, the most commonly used representation is the\nF-formation, defined by social scientists as the configuration in which people\narrange themselves to sustain an interaction. However, the use of this\nrepresentation is often accompanied with the simplifying assumption that a\nsingle conversation occurs within an F-formation. Temporally, various\ncategories have been used to organize conversational units; these include,\namong others, turn, topic, and floor. Some of these concepts are hard to define\nobjectively by themselves. The present work constitutes an initial exploration\ninto unifying these perspectives by primarily posing the question: can we use\nthe observation of simultaneous speaker turns to infer whether multiple\nconversation floors exist within an F-formation? We motivate a metric for the\nexistence of distinct conversation floors based on simultaneous speaker turns,\nand provide an analysis using this metric to characterize conversations across\nF-formations of varying cardinality. We contribute two key findings: firstly,\nat the average speaking turn duration of about two seconds for humans, there is\nevidence for the existence of multiple floors within an F-formation; and\nsecondly, an increase in the cardinality of an F-formation correlates with a\ndecrease in duration of simultaneous speaking turns.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 09:16:05 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 09:31:16 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Raman", "Chirag", ""], ["Hung", "Hayley", ""]]}, {"id": "1907.10388", "submitter": "Eric Mitchell", "authors": "Eric Mitchell, Selim Engin, Volkan Isler, Daniel D Lee", "title": "Higher-Order Function Networks for Learning Composable 3D Object\n  Representations", "comments": "To be published in International Conference on Learning\n  Representations (ICLR 2020) [https://openreview.net/forum?id=HJgfDREKDB]; 19\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to 3D object representation where a neural network\nencodes the geometry of an object directly into the weights and biases of a\nsecond 'mapping' network. This mapping network can be used to reconstruct an\nobject by applying its encoded transformation to points randomly sampled from a\nsimple geometric space, such as the unit sphere. We study the effectiveness of\nour method through various experiments on subsets of the ShapeNet dataset. We\nfind that the proposed approach can reconstruct encoded objects with accuracy\nequal to or exceeding state-of-the-art methods with orders of magnitude fewer\nparameters. Our smallest mapping network has only about 7000 parameters and\nshows reconstruction quality on par with state-of-the-art object decoder\narchitectures with millions of parameters. Further experiments on feature\nmixing through the composition of learned functions show that the encoding\ncaptures a meaningful subspace of objects.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 12:31:16 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 05:18:09 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Mitchell", "Eric", ""], ["Engin", "Selim", ""], ["Isler", "Volkan", ""], ["Lee", "Daniel D", ""]]}, {"id": "1907.10399", "submitter": "Zheng Hui", "authors": "Zheng Hui, Jie Li, Xinbo Gao, Xiumei Wang", "title": "Progressive Perception-Oriented Network for Single Image\n  Super-Resolution", "comments": "Information Sciences 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been demonstrated that deep neural networks can\nsignificantly improve the performance of single image super-resolution (SISR).\nNumerous studies have concentrated on raising the quantitative quality of\nsuper-resolved (SR) images. However, these methods that target PSNR\nmaximization usually produce blurred images at large upscaling factor. The\nintroduction of generative adversarial networks (GANs) can mitigate this issue\nand show impressive results with synthetic high-frequency textures.\nNevertheless, these GAN-based approaches always have a tendency to add fake\ntextures and even artifacts to make the SR image of visually higher-resolution.\nIn this paper, we propose a novel perceptual image super-resolution method that\nprogressively generates visually high-quality results by constructing a\nstage-wise network. Specifically, the first phase concentrates on minimizing\npixel-wise error, and the second stage utilizes the features extracted by the\nprevious stage to pursue results with better structural retention. The final\nstage employs fine structure features distilled by the second phase to produce\nmore realistic results. In this way, we can maintain the pixel, and structural\nlevel information in the perceptual image as much as possible. It is useful to\nnote that the proposed method can build three types of images in a feed-forward\nprocess. Also, we explore a new generator that adopts multi-scale hierarchical\nfeatures fusion. Extensive experiments on benchmark datasets show that our\napproach is superior to the state-of-the-art methods. Code is available at\nhttps://github.com/Zheng222/PPON.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 12:43:35 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 15:07:03 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Hui", "Zheng", ""], ["Li", "Jie", ""], ["Gao", "Xinbo", ""], ["Wang", "Xiumei", ""]]}, {"id": "1907.10450", "submitter": "Kader Pustu-Iren", "authors": "Kader Pustu-Iren and Markus M\\\"uhling and Nikolaus Korfhage and Joanna\n  Bars and Sabrina Bernh\\\"oft and Angelika H\\\"orth and Bernd Freisleben and\n  Ralph Ewerth", "title": "Investigating Correlations of Inter-coder Agreement and Machine\n  Annotation Performance for Historical Video Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video indexing approaches such as visual concept classification and person\nrecognition are essential to enable fine-grained semantic search in large-scale\nvideo archives such as the historical video collection of former German\nDemocratic Republic (GDR) maintained by the German Broadcasting Archive (DRA).\nTypically, a lexicon of visual concepts has to be defined for semantic search.\nHowever, the definition of visual concepts can be more or less subjective due\nto individually differing judgments of annotators, which may have an impact on\nannotation quality and subsequently training of supervised machine learning\nmethods. In this paper, we analyze the inter-coder agreement for historical TV\ndata of the former GDR for visual concept classification and person\nrecognition. The inter-coder agreement is evaluated for a group of expert as\nwell as non-expert annotators in order to determine differences in annotation\nhomogeneity. Furthermore, correlations between visual recognition performance\nand inter-annotator agreement are measured. In this context, information about\nimage quantity and agreement are used to predict average precision for concept\nclassification. Finally, the influence of expert vs. non-expert annotations\nacquired in the study are used to evaluate person recognition.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 13:50:20 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Pustu-Iren", "Kader", ""], ["M\u00fchling", "Markus", ""], ["Korfhage", "Nikolaus", ""], ["Bars", "Joanna", ""], ["Bernh\u00f6ft", "Sabrina", ""], ["H\u00f6rth", "Angelika", ""], ["Freisleben", "Bernd", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1907.10451", "submitter": "Yabin Zhu", "authors": "Yabin Zhu, Chenglong Li, Bin Luo, Jin Tang, Xiao Wang", "title": "Dense Feature Aggregation and Pruning for RGBT Tracking", "comments": "arXiv admin note: text overlap with arXiv:1811.09855", "journal-ref": "ACM International Conference on Multimedia.2019", "doi": "10.1145/3343031.3350928", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to perform effective information fusion of different modalities is a core\nfactor in boosting the performance of RGBT tracking. This paper presents a\nnovel deep fusion algorithm based on the representations from an end-to-end\ntrained convolutional neural network. To deploy the complementarity of features\nof all layers, we propose a recursive strategy to densely aggregate these\nfeatures that yield robust representations of target objects in each modality.\nIn different modalities, we propose to prune the densely aggregated features of\nall modalities in a collaborative way. In a specific, we employ the operations\nof global average pooling and weighted random selection to perform channel\nscoring and selection, which could remove redundant and noisy features to\nachieve more robust feature representation. Experimental results on two RGBT\ntracking benchmark datasets suggest that our tracker achieves clear\nstate-of-the-art against other RGB and RGBT tracking methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 13:51:24 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Zhu", "Yabin", ""], ["Li", "Chenglong", ""], ["Luo", "Bin", ""], ["Tang", "Jin", ""], ["Wang", "Xiao", ""]]}, {"id": "1907.10456", "submitter": "Yuhao Niu", "authors": "Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey,\n  Feng Lu", "title": "Understanding Adversarial Attacks on Deep Learning Based Medical Image\n  Analysis Systems", "comments": "15 pages, 11 figures, to appear in Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2020.107332", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have become popular for medical image analysis\ntasks like cancer diagnosis and lesion detection. However, a recent study\ndemonstrates that medical deep learning systems can be compromised by\ncarefully-engineered adversarial examples/attacks with small imperceptible\nperturbations. This raises safety concerns about the deployment of these\nsystems in clinical settings. In this paper, we provide a deeper understanding\nof adversarial examples in the context of medical images. We find that medical\nDNN models can be more vulnerable to adversarial attacks compared to models for\nnatural images, according to two different viewpoints. Surprisingly, we also\nfind that medical adversarial attacks can be easily detected, i.e., simple\ndetectors can achieve over 98% detection AUC against state-of-the-art attacks,\ndue to fundamental feature differences compared to normal examples. We believe\nthese findings may be a useful basis to approach the design of more explainable\nand secure medical deep learning systems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 14:04:13 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 15:57:31 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Ma", "Xingjun", ""], ["Niu", "Yuhao", ""], ["Gu", "Lin", ""], ["Wang", "Yisen", ""], ["Zhao", "Yitian", ""], ["Bailey", "James", ""], ["Lu", "Feng", ""]]}, {"id": "1907.10465", "submitter": "Florian Kordon", "authors": "Florian Kordon (1 and 2 and 3), Peter Fischer (1 and 2), Maxim\n  Privalov (4), Benedict Swartman (4), Marc Schnetzke (4), Jochen Franke (4),\n  Ruxandra Lasowski (3), Andreas Maier (1), Holger Kunze (2) ((1) Pattern\n  Recognition Lab, Department of Computer Science,\n  Friedrich-Alexander-Universit\\\"at Erlangen-N\\\"urnberg, Erlangen, Germany, (2)\n  Advanced Therapies, Siemens Healthcare GmbH, Forchheim, Germany, (3) Faculty\n  of Digital Media, Hochschule Furtwangen, Furtwangen, Germany, (4) Department\n  for Trauma and Orthopaedic Surgery, BG Trauma Center Ludwigshafen,\n  Ludwigshafen, Germany)", "title": "Multi-task Localization and Segmentation for X-ray Guided Planning in\n  Knee Surgery", "comments": "Accepted for MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray based measurement and guidance are commonly used tools in orthopaedic\nsurgery to facilitate a minimally invasive workflow. Typically, a surgical\nplanning is first performed using knowledge of bone morphology and anatomical\nlandmarks. Information about bone location then serves as a prior for\nregistration during overlay of the planning on intra-operative X-ray images.\nPerforming these steps manually however is prone to intra-rater/inter-rater\nvariability and increases task complexity for the surgeon. To remedy these\nissues, we propose an automatic framework for planning and subsequent overlay.\nWe evaluate it on the example of femoral drill site planning for medial\npatellofemoral ligament reconstruction surgery. A deep multi-task stacked\nhourglass network is trained on 149 conventional lateral X-ray images to\njointly localize two femoral landmarks, to predict a region of interest for the\nposterior femoral cortex tangent line, and to perform semantic segmentation of\nthe femur, patella, tibia, and fibula with adaptive task complexity weighting.\nOn 38 clinical test images the framework achieves a median localization error\nof 1.50 mm for the femoral drill site and mean IOU scores of 0.99, 0.97, 0.98,\nand 0.96 for the femur, patella, tibia, and fibula respectively. The\ndemonstrated approach consistently performs surgical planning at expert-level\nprecision without the need for manual correction.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 14:32:10 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Kordon", "Florian", "", "1 and 2 and 3"], ["Fischer", "Peter", "", "1 and 2"], ["Privalov", "Maxim", ""], ["Swartman", "Benedict", ""], ["Schnetzke", "Marc", ""], ["Franke", "Jochen", ""], ["Lasowski", "Ruxandra", ""], ["Maier", "Andreas", ""], ["Kunze", "Holger", ""]]}, {"id": "1907.10471", "submitter": "Zetong Yang", "authors": "Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, Jiaya Jia", "title": "STD: Sparse-to-Dense 3D Object Detector for Point Cloud", "comments": "arXiv admin note: text overlap with arXiv:1812.05276", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new two-stage 3D object detection framework, named\nsparse-to-dense 3D Object Detector (STD). The first stage is a bottom-up\nproposal generation network that uses raw point cloud as input to generate\naccurate proposals by seeding each point with a new spherical anchor. It\nachieves a high recall with less computation compared with prior works. Then,\nPointsPool is applied for generating proposal features by transforming their\ninterior point features from sparse expression to compact representation, which\nsaves even more computation time. In box prediction, which is the second stage,\nwe implement a parallel intersection-over-union (IoU) branch to increase\nawareness of localization accuracy, resulting in further improved performance.\nWe conduct experiments on KITTI dataset, and evaluate our method in terms of 3D\nobject and Bird's Eye View (BEV) detection. Our method outperforms other\nstate-of-the-arts by a large margin, especially on the hard set, with inference\nspeed more than 10 FPS.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 16:20:44 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Yang", "Zetong", ""], ["Sun", "Yanan", ""], ["Liu", "Shu", ""], ["Shen", "Xiaoyong", ""], ["Jia", "Jiaya", ""]]}, {"id": "1907.10473", "submitter": "Ruimao Zhang", "authors": "Ping Luo, Ruimao Zhang, Jiamin Ren, Zhanglin Peng, Jingyu Li", "title": "Switchable Normalization for Learning-to-Normalize Deep Representation", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 18\n  pages, 15 figures, 11 tables. arXiv admin note: substantial text overlap with\n  arXiv:1806.10779", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a learning-to-normalize problem by proposing Switchable\nNormalization (SN), which learns to select different normalizers for different\nnormalization layers of a deep neural network. SN employs three distinct scopes\nto compute statistics (means and variances) including a channel, a layer, and a\nminibatch. SN switches between them by learning their importance weights in an\nend-to-end manner. It has several good properties. First, it adapts to various\nnetwork architectures and tasks. Second, it is robust to a wide range of batch\nsizes, maintaining high performance even when small minibatch is presented\n(e.g. 2 images/GPU). Third, SN does not have sensitive hyper-parameter, unlike\ngroup normalization that searches the number of groups as a hyper-parameter.\nWithout bells and whistles, SN outperforms its counterparts on various\nchallenging benchmarks, such as ImageNet, COCO, CityScapes, ADE20K, MegaFace,\nand Kinetics. Analyses of SN are also presented to answer the following three\nquestions: (a) Is it useful to allow each normalization layer to select its own\nnormalizer? (b) What impacts the choices of normalizers? (c) Do different tasks\nand datasets prefer different normalizers? We hope SN will help ease the usage\nand understand the normalization techniques in deep learning. The code of SN\nhas been released at https://github.com/switchablenorms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 17:50:31 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Luo", "Ping", ""], ["Zhang", "Ruimao", ""], ["Ren", "Jiamin", ""], ["Peng", "Zhanglin", ""], ["Li", "Jingyu", ""]]}, {"id": "1907.10526", "submitter": "Kai Zhang", "authors": "Kai Zhang, Alireza Entezari", "title": "A Convolutional Forward and Back-Projection Model for Fan-Beam Geometry", "comments": "This paper was submitted to IEEE-TMI, and it's an extension of our\n  ISBI paper (https://ieeexplore.ieee.org/abstract/document/8759285)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Iterative methods for tomographic image reconstruction have great potential\nfor enabling high quality imaging from low-dose projection data. The\ncomputational burden of iterative reconstruction algorithms, however, has been\nan impediment in their adoption in practical CT reconstruction problems. We\npresent an approach for highly efficient and accurate computation of forward\nmodel for image reconstruction in fan-beam geometry in X-ray CT. The efficiency\nof computations makes this approach suitable for large-scale optimization\nalgorithms with on-the-fly, memory-less, computations of the forward and\nback-projection. Our experiments demonstrate the improvements in accuracy as\nwell as efficiency of our model, specifically for first-order box splines\n(i.e., pixel-basis) compared to recently developed methods for this purpose,\nnamely Look-up Table-based Ray Integration (LTRI) and Separable Footprints (SF)\nin 2-D.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 15:39:41 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zhang", "Kai", ""], ["Entezari", "Alireza", ""]]}, {"id": "1907.10545", "submitter": "S\\'ergio Agostinho", "authors": "S\\'ergio Agostinho, Jo\\~ao Gomes, Alessio Del Bue", "title": "CvxPnPL: A Unified Convex Solution to the Absolute Pose Estimation\n  Problem from Point and Line Correspondences", "comments": "Main paper and supplemental material included. References added and\n  minor change to fig 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new convex method to estimate 3D pose from mixed combinations of\n2D-3D point and line correspondences, the Perspective-n-Points-and-Lines\nproblem (PnPL). We merge the contributions of each point and line into a\nunified Quadratic Constrained Quadratic Problem (QCQP) and then relax it into a\nSemi Definite Program (SDP) through Shor's relaxation. This makes it possible\nto gracefully handle mixed configurations of points and lines. Furthermore, the\nproposed relaxation allows us to recover a finite number of solutions under\nambiguous configurations. In such cases, the 3D pose candidates are found by\nfurther enforcing geometric constraints on the solution space and then\nretrieving such poses from the intersections of multiple quadrics. Experiments\nprovide results in line with the best performing state of the art methods while\nproviding the flexibility of solving for an arbitrary number of points and\nlines.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 16:28:52 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 07:42:09 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Agostinho", "S\u00e9rgio", ""], ["Gomes", "Jo\u00e3o", ""], ["Del Bue", "Alessio", ""]]}, {"id": "1907.10557", "submitter": "Matwey Kornilov", "authors": "Matwey V. Kornilov", "title": "Maximum likelihood estimation for disk image parameters", "comments": "13 pages, 4 figures. in IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2020.3017371", "report-no": null, "categories": "eess.IV astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique for estimating disk parameters (the centre and\nthe radius) from its 2D image. It is based on the maximal likelihood approach\nutilising both edge pixels coordinates and the image intensity gradients. We\nemphasise the following advantages of our likelihood model. It has closed-form\nformulae for parameter estimating, requiring less computational resources than\niterative algorithms therefore. The likelihood model naturally distinguishes\nthe outer and inner annulus edges. The proposed technique was evaluated on both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 16:55:29 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 10:02:54 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Kornilov", "Matwey V.", ""]]}, {"id": "1907.10559", "submitter": "Hayder Hamandi", "authors": "Hayder Hamandi, Nabil Sarhan", "title": "QRMODA and BRMODA: Novel Models for Face Recognition Accuracy in\n  Computer Vision Systems with Adapted Video Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge facing Computer Vision systems is providing the ability to\naccurately detect threats and recognize subjects and/or objects under\ndynamically changing network conditions. We propose two novel models that\ncharacterize the face recognition accuracy in terms of video encoding\nparameters. Specifically, we model the accuracy in terms of video resolution,\nquantization, and actual bit rate. We validate the models using two distinct\nvideo datasets and a large image dataset by conducting 1, 668 experiments that\ninvolve simultaneously varying combinations of encoding parameters. We show\nthat both models hold true for the deep learning and statistical based face\nrecognition. Furthermore, we show that the models can be used to capture\ndifferent accuracy metrics, specifically the recall, precision, and F1-score.\nUltimately, we provide meaningful insights on the factors affecting the\nconstants of each proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 16:59:56 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Hamandi", "Hayder", ""], ["Sarhan", "Nabil", ""]]}, {"id": "1907.10586", "submitter": "Jianbing Shen", "authors": "Yuanpei Liu and Xingping Dong and Xiankai Lu and Fahad Shahbaz Khan\n  and Jianbing Shen and Steven Hoi", "title": "Teacher-Students Knowledge Distillation for Siamese Trackers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Siamese network based trackers have significantly advanced\nthe state-of-the-art in real-time tracking. However, state-of-the-art Siamese\ntrackers suffer from high memory cost which restricts their applicability in\nmobile applications having strict constraints on memory budget. To address this\nissue, we propose a novel distilled Siamese tracking framework to learn small,\nfast yet accurate trackers (students), which capture critical knowledge from\nlarge Siamese trackers (teachers) by a teacher-students knowledge distillation\nmodel. This model is intuitively inspired by a one-teacher vs multi-students\nlearning mechanism, which is the most usual teaching method in the school. In\nparticular, it contains a single teacher-student distillation model and a\nstudent-student knowledge sharing mechanism. The first one is designed by a\ntracking-specific distillation strategy to transfer knowledge from the teacher\nto students. The later is utilized for mutual learning between students to\nenable an in-depth knowledge understanding. To the best of our knowledge, we\nare the first to investigate knowledge distillation for Siamese trackers and\npropose a distilled Siamese tracking framework. We demonstrate the generality\nand effectiveness of our framework by conducting a theoretical analysis and\nextensive empirical evaluations on several popular Siamese trackers. The\nresults on five tracking benchmarks clearly show that the proposed distilled\ntrackers achieve compression rates up to 18$\\times$ and frame-rates of $265$\nFPS with speedups of 3$\\times$, while obtaining similar or even slightly\nimproved tracking accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 17:46:44 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 16:19:10 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Liu", "Yuanpei", ""], ["Dong", "Xingping", ""], ["Lu", "Xiankai", ""], ["Khan", "Fahad Shahbaz", ""], ["Shen", "Jianbing", ""], ["Hoi", "Steven", ""]]}, {"id": "1907.10589", "submitter": "Richard Jiang", "authors": "Bing Xu, Tobechukwu Agbele and Richard Jiang", "title": "Biometric Blockchain: A Better Solution for the Security and Trust of\n  Food Logistics", "comments": null, "journal-ref": "2019 3rd International Conference on Artificial Intelligence\n  Applications and Technologies (AIAAT 2019)", "doi": "10.1088/1757-899X/646/1/012009", "report-no": null, "categories": "cs.CR cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has been emerging as a promising technology that could totally\nchange the landscape of data security in the coming years, particularly for\ndata access over Internet-of-Things and cloud servers. However, blockchain\nitself, though secured by its protocol, does not identify who owns the data and\nwho uses the data. Other than simply encrypting data into keys, in this paper,\nwe proposed a protocol called Biometric Blockchain (BBC) that explicitly\nincorporate the biometric cues of individuals to unambiguously identify the\ncreators and users in a blockchain-based system, particularly to address the\nincreasing needs to secure the food logistics, following the recently widely\nreported incident on wrongly labelled foods that caused the death of a customer\non a flight. The advantage of using BBC in the food logistics is clear: it can\nnot only identify if the data or labels are authentic, but also clearly record\nwho is responsible for the secured data or labels. As a result, such a\nBBC-based solution can great ease the difficulty to control the risks\naccompanying the food logistics, such as faked foods or wrong gradient labels.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 23:07:58 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 22:18:11 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Xu", "Bing", ""], ["Agbele", "Tobechukwu", ""], ["Jiang", "Richard", ""]]}, {"id": "1907.10597", "submitter": "Roy Schwartz", "authors": "Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni", "title": "Green AI", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computations required for deep learning research have been doubling every\nfew months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].\nThese computations have a surprisingly large carbon footprint [38]. Ironically,\ndeep learning was inspired by the human brain, which is remarkably energy\nefficient. Moreover, the financial cost of the computations can make it\ndifficult for academics, students, and researchers, in particular those from\nemerging economies, to engage in deep learning research.\n  This position paper advocates a practical solution by making efficiency an\nevaluation criterion for research alongside accuracy and related measures. In\naddition, we propose reporting the financial cost or \"price tag\" of developing,\ntraining, and running models to provide baselines for the investigation of\nincreasingly efficient methods. Our goal is to make AI both greener and more\ninclusive---enabling any inspired undergraduate with a laptop to write\nhigh-quality research papers. Green AI is an emerging focus at the Allen\nInstitute for AI.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 19:36:18 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 02:54:44 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 20:09:57 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Schwartz", "Roy", ""], ["Dodge", "Jesse", ""], ["Smith", "Noah A.", ""], ["Etzioni", "Oren", ""]]}, {"id": "1907.10628", "submitter": "Vinod Kumar Kurmi", "authors": "Vinod Kumar Kurmi, Vipul Bajaj, Venkatesh K Subramanian, Vinay P\n  Namboodiri", "title": "Curriculum based Dropout Discriminator for Domain Adaptation", "comments": "BMVC 2019 Accepted, Project Page:\n  https://delta-lab-iitk.github.io/CD3A/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is essential to enable wide usage of deep learning based\nnetworks trained using large labeled datasets. Adversarial learning based\ntechniques have shown their utility towards solving this problem using a\ndiscriminator that ensures source and target distributions are close. However,\nhere we suggest that rather than using a point estimate, it would be useful if\na distribution based discriminator could be used to bridge this gap. This could\nbe achieved using multiple classifiers or using traditional ensemble methods.\nIn contrast, we suggest that a Monte Carlo dropout based ensemble discriminator\ncould suffice to obtain the distribution based discriminator. Specifically, we\npropose a curriculum based dropout discriminator that gradually increases the\nvariance of the sample based distribution and the corresponding reverse\ngradients are used to align the source and target feature representations. The\ndetailed results and thorough ablation analysis show that our model outperforms\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 18:00:12 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 19:43:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Kurmi", "Vinod Kumar", ""], ["Bajaj", "Vipul", ""], ["Subramanian", "Venkatesh K", ""], ["Namboodiri", "Vinay P", ""]]}, {"id": "1907.10634", "submitter": "Andrea Palazzi", "authors": "Andrea Palazzi, Luca Bergamini, Simone Calderara, Rita Cucchiara", "title": "Warp and Learn: Novel Views Generation for Vehicles and Other Objects", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a new self-supervised, semi-parametric approach for\nsynthesizing novel views of a vehicle starting from a single monocular image.\nDifferently from parametric (i.e. entirely learning-based) methods, we show how\na-priori geometric knowledge about the object and the 3D world can be\nsuccessfully integrated into a deep learning based image generation framework.\nAs this geometric component is not learnt, we call our approach\nsemi-parametric. In particular, we exploit man-made object symmetry and\npiece-wise planarity to integrate rich a-priori visual information into the\nnovel viewpoint synthesis process. An Image Completion Network (ICN) is then\ntrained to generate a realistic image starting from this geometric guidance.\nThis careful blend between parametric and non-parametric components allows us\nto i) operate in a real-world scenario, ii) preserve high-frequency visual\ninformation such as textures, iii) handle truly arbitrary 3D roto-translations\nof the input and iv) perform shape transfer to completely different 3D models.\nEventually, we show that our approach can be easily complemented with synthetic\ndata and extended to other rigid objects with completely different topology,\neven in presence of concave structures and holes (e.g. chairs). A comprehensive\nexperimental analysis against state-of-the-art competitors shows the efficacy\nof our method both from a quantitative and a perceptive point of view.\nSupplementary material, animated results, code and data are available at:\nhttps://github.com/ndrplz/semiparametric\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 18:01:51 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 12:44:51 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 15:21:58 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Palazzi", "Andrea", ""], ["Bergamini", "Luca", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1907.10655", "submitter": "Yuan Xue", "authors": "Yuan Xue, Qianying Zhou, Jiarong Ye, L. Rodney Long, Sameer Antani,\n  Carl Cornwell, Zhiyun Xue, Xiaolei Huang", "title": "Synthetic Augmentation and Feature-based Filtering for Improved Cervical\n  Histopathology Image Classification", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cervical intraepithelial neoplasia (CIN) grade of histopathology images is a\ncrucial indicator in cervical biopsy results. Accurate CIN grading of\nepithelium regions helps pathologists with precancerous lesion diagnosis and\ntreatment planning. Although an automated CIN grading system has been desired,\nsupervised training of such a system would require a large amount of expert\nannotations, which are expensive and time-consuming to collect. In this paper,\nwe investigate the CIN grade classification problem on segmented epithelium\npatches. We propose to use conditional Generative Adversarial Networks (cGANs)\nto expand the limited training dataset, by synthesizing realistic cervical\nhistopathology images. While the synthetic images are visually appealing, they\nare not guaranteed to contain meaningful features for data augmentation. To\ntackle this issue, we propose a synthetic-image filtering mechanism based on\nthe divergence in feature space between generated images and class centroids in\norder to control the feature quality of selected synthetic images for data\naugmentation. Our models are evaluated on a cervical histopathology image\ndataset with a limited number of patch-level CIN grade annotations. Extensive\nexperimental results show a significant improvement of classification accuracy\nfrom 66.3% to 71.7% using the same ResNet18 baseline classifier after\nleveraging our cGAN generated images with feature-based filtering, which\ndemonstrates the effectiveness of our models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 18:54:11 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Xue", "Yuan", ""], ["Zhou", "Qianying", ""], ["Ye", "Jiarong", ""], ["Long", "L. Rodney", ""], ["Antani", "Sameer", ""], ["Cornwell", "Carl", ""], ["Xue", "Zhiyun", ""], ["Huang", "Xiaolei", ""]]}, {"id": "1907.10659", "submitter": "Matthias Ochs", "authors": "Matthias Ochs, Adrian Kretz, Rudolf Mester", "title": "SDNet: Semantically Guided Depth Estimation Network", "comments": "Paper is accepted at German Conference on Pattern Recognition (GCPR),\n  Dortmund, Germany, September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles and robots require a full scene understanding of the\nenvironment to interact with it. Such a perception typically incorporates\npixel-wise knowledge of the depths and semantic labels for each image from a\nvideo sensor. Recent learning-based methods estimate both types of information\nindependently using two separate CNNs. In this paper, we propose a model that\nis able to predict both outputs simultaneously, which leads to improved results\nand even reduced computational costs compared to independent estimation of\ndepth and semantics. We also empirically prove that the CNN is capable of\nlearning more meaningful and semantically richer features. Furthermore, our\nSDNet estimates the depth based on ordinal classification. On the basis of\nthese two enhancements, our proposed method achieves state-of-the-art results\nin semantic segmentation and depth estimation from single monocular input\nimages on two challenging datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 19:08:31 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Ochs", "Matthias", ""], ["Kretz", "Adrian", ""], ["Mester", "Rudolf", ""]]}, {"id": "1907.10665", "submitter": "Wei Shen", "authors": "Wei Shen, Yilu Guo, Yan Wang, Kai Zhao, Bo Wang, and Alan Yuille", "title": "Deep Differentiable Random Forests for Age Estimation", "comments": "Accepted by TPAMI. arXiv admin note: substantial text overlap with\n  arXiv:1712.07195", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age estimation from facial images is typically cast as a label distribution\nlearning or regression problem, since aging is a gradual progress. Its main\nchallenge is the facial feature space w.r.t. ages is inhomogeneous, due to the\nlarge variation in facial appearance across different persons of the same age\nand the non-stationary property of aging. In this paper, we propose two Deep\nDifferentiable Random Forests methods, Deep Label Distribution Learning Forest\n(DLDLF) and Deep Regression Forest (DRF), for age estimation. Both of them\nconnect split nodes to the top layer of convolutional neural networks (CNNs)\nand deal with inhomogeneous data by jointly learning input-dependent data\npartitions at the split nodes and age distributions at the leaf nodes. This\njoint learning follows an alternating strategy: (1) Fixing the leaf nodes and\noptimizing the split nodes and the CNN parameters by Back-propagation; (2)\nFixing the split nodes and optimizing the leaf nodes by Variational Bounding.\nTwo Deterministic Annealing processes are introduced into the learning of the\nsplit and leaf nodes, respectively, to avoid poor local optima and obtain\nbetter estimates of tree parameters free of initial values. Experimental\nresults show that DLDLF and DRF achieve state-of-the-art performance on three\nage estimation datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 02:44:38 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 15:18:01 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Shen", "Wei", ""], ["Guo", "Yilu", ""], ["Wang", "Yan", ""], ["Zhao", "Kai", ""], ["Wang", "Bo", ""], ["Yuille", "Alan", ""]]}, {"id": "1907.10695", "submitter": "Chengde Wan Mr", "authors": "Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao", "title": "Dual Grid Net: hand mesh vertex regression from single depth maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for recovering the dense 3D surface of the hand by\nregressing the vertex coordinates of a mesh model from a single depth map. To\nthis end, we use a two-stage 2D fully convolutional network architecture. In\nthe first stage, the network estimates a dense correspondence field for every\npixel on the depth map or image grid to the mesh grid. In the second stage, we\ndesign a differentiable operator to map features learned from the previous\nstage and regress a 3D coordinate map on the mesh grid. Finally, we sample from\nthe mesh grid to recover the mesh vertices, and fit it an articulated template\nmesh in closed form. During inference, the network can predict all the mesh\nvertices, transformation matrices for every joint and the joint coordinates in\na single forward pass. When given supervision on the sparse key-point\ncoordinates, our method achieves state-of-the-art accuracy on NYU dataset for\nkey point localization while recovering mesh vertices and a dense\ncorrespondence map. Our framework can also be learned through self-supervision\nby minimizing a set of data fitting and kinematic prior terms. With\nmulti-camera rig during training to resolve self-occlusion, it can perform\ncompetitively with strongly supervised methods Without any human annotation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:07:58 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Wan", "Chengde", ""], ["Probst", "Thomas", ""], ["Van Gool", "Luc", ""], ["Yao", "Angela", ""]]}, {"id": "1907.10700", "submitter": "Florian Willomitzer", "authors": "Florian Willomitzer, Chia-Kai Yeh, Vikas Gupta, William Spies, Florian\n  Schiffers, Marc Walton, Oliver Cossairt", "title": "Uncalibrated Deflectometry with a Mobile Device on Extended Specular\n  Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a system and methods for the three-dimensional measurement of\nextended specular surfaces with high surface normal variations. Our system\nconsists only of a mobile hand held device and exploits screen and front camera\nfor Deflectometry-based surface measurements. We demonstrate high quality\nmeasurements without the need for an offline calibration procedure. In\naddition, we develop a multi-view technique to compensate for the small screen\nof a mobile device so that large surfaces can be densely reconstructed in their\nentirety. This work is a first step towards developing a self-calibrating\nDeflectometry procedure capable of taking 3D surface measurements of specular\nobjects in the wild and accessible to users with little to no technical imaging\nexperience.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:17:32 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Willomitzer", "Florian", ""], ["Yeh", "Chia-Kai", ""], ["Gupta", "Vikas", ""], ["Spies", "William", ""], ["Schiffers", "Florian", ""], ["Walton", "Marc", ""], ["Cossairt", "Oliver", ""]]}, {"id": "1907.10719", "submitter": "Akash Abdu Jyothi", "authors": "Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, Greg Mori", "title": "LayoutVAE: Stochastic Scene Layout Generation From a Label Set", "comments": "20 pages, 24 figures, accepted in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there is an increasing interest in scene generation within the\nresearch community. However, models used for generating scene layouts from\ntextual description largely ignore plausible visual variations within the\nstructure dictated by the text. We propose LayoutVAE, a variational autoencoder\nbased framework for generating stochastic scene layouts. LayoutVAE is a\nversatile modeling framework that allows for generating full image layouts\ngiven a label set, or per label layouts for an existing image given a new\nlabel. In addition, it is also capable of detecting unusual layouts,\npotentially providing a way to evaluate layout generation problem. Extensive\nexperiments on MNIST-Layouts and challenging COCO 2017 Panoptic dataset\nverifies the effectiveness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:53:55 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 21:49:56 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 06:25:20 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Jyothi", "Akash Abdu", ""], ["Durand", "Thibaut", ""], ["He", "Jiawei", ""], ["Sigal", "Leonid", ""], ["Mori", "Greg", ""]]}, {"id": "1907.10737", "submitter": "Haichao Zhang", "authors": "Haichao Zhang, Jianyu Wang", "title": "Joint Adversarial Training: Incorporating both Spatial and Pixel Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional adversarial training methods using attacks that manipulate the\npixel value directly and individually, leading to models that are less robust\nin face of spatial transformation-based attacks. In this paper, we propose a\njoint adversarial training method that incorporates both spatial\ntransformation-based and pixel-value based attacks for improving model\nrobustness. We introduce a spatial transformation-based attack with an explicit\nnotion of budget and develop an algorithm for spatial attack generation. We\nfurther integrate both pixel and spatial attacks into one generation model and\nshow how to leverage the complementary strengths of each other in training for\nimproving the overall model robustness. Extensive experimental results on\ndifferent benchmark datasets compared with state-of-the-art methods verified\nthe effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 21:36:27 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 05:28:27 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Zhang", "Haichao", ""], ["Wang", "Jianyu", ""]]}, {"id": "1907.10763", "submitter": "Xiao-Yun Zhou", "authors": "Xiao-Yun Zhou, Zhao-Yang Wang, Peichao Li, Jian-Qing Zheng,\n  Guang-Zhong Yang", "title": "One-stage Shape Instantiation from a Single 2D Image to 3D Point Cloud", "comments": "8.5 pages, 5 figures, MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape instantiation which predicts the 3D shape of a dynamic target from one\nor more 2D images is important for real-time intra-operative navigation.\nPreviously, a general shape instantiation framework was proposed with manual\nimage segmentation to generate a 2D Statistical Shape Model (SSM) and with\nKernel Partial Least Square Regression (KPLSR) to learn the relationship\nbetween the 2D and 3D SSM for 3D shape prediction. In this paper, the two-stage\nshape instantiation is improved to be one-stage. PointOutNet with 19\nconvolutional layers and three fully-connected layers is used as the network\nstructure and Chamfer distance is used as the loss function to predict the 3D\ntarget point cloud from a single 2D image. With the proposed one-stage shape\ninstantiation algorithm, a spontaneous image-to-point cloud training and\ninference can be achieved. A dataset from 27 Right Ventricle (RV) subjects,\nindicating 609 experiments, were used to validate the proposed one-stage shape\ninstantiation algorithm. An average point cloud-to-point cloud (PC-to-PC) error\nof 1.72mm has been achieved, which is comparable to the PLSR-based (1.42mm) and\nKPLSR-based (1.31mm) two-stage shape instantiation algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 22:41:46 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Zhou", "Xiao-Yun", ""], ["Wang", "Zhao-Yang", ""], ["Li", "Peichao", ""], ["Zheng", "Jian-Qing", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1907.10764", "submitter": "Haichao Zhang", "authors": "Haichao Zhang, Jianyu Wang", "title": "Defense Against Adversarial Attacks Using Feature Scattering-based\n  Adversarial Training", "comments": "Published at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a feature scattering-based adversarial training approach for\nimproving model robustness against adversarial attacks. Conventional\nadversarial training approaches leverage a supervised scheme (either targeted\nor non-targeted) in generating attacks for training, which typically suffer\nfrom issues such as label leaking as noted in recent works. Differently, the\nproposed approach generates adversarial images for training through feature\nscattering in the latent space, which is unsupervised in nature and avoids\nlabel leaking. More importantly, this new approach generates perturbed images\nin a collaborative fashion, taking the inter-sample relationships into\nconsideration. We conduct analysis on model robustness and demonstrate the\neffectiveness of the proposed approach through extensively experiments on\ndifferent datasets compared with state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 22:43:55 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 07:20:14 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 00:45:19 GMT"}, {"version": "v4", "created": "Thu, 21 Nov 2019 21:51:21 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zhang", "Haichao", ""], ["Wang", "Jianyu", ""]]}, {"id": "1907.10786", "submitter": "Yujun Shen", "authors": "Yujun Shen, Jinjin Gu, Xiaoou Tang, Bolei Zhou", "title": "Interpreting the Latent Space of GANs for Semantic Face Editing", "comments": "CVPR2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advance of Generative Adversarial Networks (GANs) in\nhigh-fidelity image synthesis, there lacks enough understanding of how GANs are\nable to map a latent code sampled from a random distribution to a\nphoto-realistic image. Previous work assumes the latent space learned by GANs\nfollows a distributed representation but observes the vector arithmetic\nphenomenon. In this work, we propose a novel framework, called InterFaceGAN,\nfor semantic face editing by interpreting the latent semantics learned by GANs.\nIn this framework, we conduct a detailed study on how different semantics are\nencoded in the latent space of GANs for face synthesis. We find that the latent\ncode of well-trained generative models actually learns a disentangled\nrepresentation after linear transformations. We explore the disentanglement\nbetween various semantics and manage to decouple some entangled semantics with\nsubspace projection, leading to more precise control of facial attributes.\nBesides manipulating gender, age, expression, and the presence of eyeglasses,\nwe can even vary the face pose as well as fix the artifacts accidentally\ngenerated by GAN models. The proposed method is further applied to achieve real\nimage manipulation when combined with GAN inversion methods or some\nencoder-involved models. Extensive results suggest that learning to synthesize\nfaces spontaneously brings a disentangled and controllable facial attribute\nrepresentation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 01:30:16 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 03:33:06 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 10:24:42 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Shen", "Yujun", ""], ["Gu", "Jinjin", ""], ["Tang", "Xiaoou", ""], ["Zhou", "Bolei", ""]]}, {"id": "1907.10801", "submitter": "Dong Liu", "authors": "Dong Liu, Rohit Puri, Nagendra Kamath, Subhabrata Bhattachary", "title": "Composition-Aware Image Aesthetics Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image aesthetics assessment is important for a wide variety of\napplications such as on-line photo suggestion, photo album management and image\nretrieval. Previous methods have focused on mapping the holistic image content\nto a high or low aesthetics rating. However, the composition information of an\nimage characterizes the harmony of its visual elements according to the\nprinciples of art, and provides richer information for learning aesthetics. In\nthis work, we propose to model the image composition information as the mutual\ndependency of its local regions, and design a novel architecture to leverage\nsuch information to boost the performance of aesthetics assessment. To achieve\nthis, we densely partition an image into local regions and compute\naesthetics-preserving features over the regions to characterize the aesthetics\nproperties of image content. With the feature representation of local regions,\nwe build a region composition graph in which each node denotes one region and\nany two nodes are connected by an edge weighted by the similarity of the region\nfeatures. We perform reasoning on this graph via graph convolution, in which\nthe activation of each node is determined by its highly correlated neighbors.\nOur method naturally uncovers the mutual dependency of local regions in the\nnetwork training procedure, and achieves the state-of-the-art performance on\nthe benchmark visual aesthetics datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 02:22:16 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Liu", "Dong", ""], ["Puri", "Rohit", ""], ["Kamath", "Nagendra", ""], ["Bhattachary", "Subhabrata", ""]]}, {"id": "1907.10804", "submitter": "Han Shu", "authors": "Han Shu, Yunhe Wang, Xu Jia, Kai Han, Hanting Chen, Chunjing Xu, Qi\n  Tian, Chang Xu", "title": "Co-Evolutionary Compression for Unpaired Image Translation", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have been successfully used for\nconsiderable computer vision tasks, especially the image-to-image translation.\nHowever, generators in these networks are of complicated architectures with\nlarge number of parameters and huge computational complexities. Existing\nmethods are mainly designed for compressing and speeding-up deep neural\nnetworks in the classification task, and cannot be directly applied on GANs for\nimage translation, due to their different objectives and training procedures.\nTo this end, we develop a novel co-evolutionary approach for reducing their\nmemory usage and FLOPs simultaneously. In practice, generators for two image\ndomains are encoded as two populations and synergistically optimized for\ninvestigating the most important convolution filters iteratively. Fitness of\neach individual is calculated using the number of parameters, a\ndiscriminator-aware regularization, and the cycle consistency. Extensive\nexperiments conducted on benchmark datasets demonstrate the effectiveness of\nthe proposed method for obtaining compact and effective generators.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 02:26:14 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Shu", "Han", ""], ["Wang", "Yunhe", ""], ["Jia", "Xu", ""], ["Han", "Kai", ""], ["Chen", "Hanting", ""], ["Xu", "Chunjing", ""], ["Tian", "Qi", ""], ["Xu", "Chang", ""]]}, {"id": "1907.10815", "submitter": "Jae Shin Yoon", "authors": "Jae Shin Yoon, Takaaki Shiratori, Shoou-I Yu, and Hyun Soo Park", "title": "Self-Supervised Adaptation of High-Fidelity Face Models for Monocular\n  Performance Tracking", "comments": "This work is accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improvements in data-capture and face modeling techniques have enabled us to\ncreate high-fidelity realistic face models. However, driving these realistic\nface models requires special input data, e.g. 3D meshes and unwrapped textures.\nAlso, these face models expect clean input data taken under controlled lab\nenvironments, which is very different from data collected in the wild. All\nthese constraints make it challenging to use the high-fidelity models in\ntracking for commodity cameras. In this paper, we propose a self-supervised\ndomain adaptation approach to enable the animation of high-fidelity face models\nfrom a commodity camera. Our approach first circumvents the requirement for\nspecial input data by training a new network that can directly drive a face\nmodel just from a single 2D image. Then, we overcome the domain mismatch\nbetween lab and uncontrolled environments by performing self-supervised domain\nadaptation based on \"consecutive frame texture consistency\" based on the\nassumption that the appearance of the face is consistent over consecutive\nframes, avoiding the necessity of modeling the new environment such as lighting\nor background. Experiments show that we are able to drive a high-fidelity face\nmodel to perform complex facial motion from a cellphone camera without\nrequiring any labeled data from the new domain.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 03:20:21 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Yoon", "Jae Shin", ""], ["Shiratori", "Takaaki", ""], ["Yu", "Shoou-I", ""], ["Park", "Hyun Soo", ""]]}, {"id": "1907.10823", "submitter": "Qian Huang", "authors": "Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, Ser-Nam\n  Lim", "title": "Enhancing Adversarial Example Transferability with an Intermediate Level\n  Attack", "comments": "ICCV 2019 camera-ready. Imagenet results are updated after fixing the\n  normalization. arXiv admin note: text overlap with arXiv:1811.08458", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to adversarial examples, malicious inputs\ncrafted to fool trained models. Adversarial examples often exhibit black-box\ntransfer, meaning that adversarial examples for one model can fool another\nmodel. However, adversarial examples are typically overfit to exploit the\nparticular architecture and feature representation of a source model, resulting\nin sub-optimal black-box transfer attacks to other target models. We introduce\nthe Intermediate Level Attack (ILA), which attempts to fine-tune an existing\nadversarial example for greater black-box transferability by increasing its\nperturbation on a pre-specified layer of the source model, improving upon\nstate-of-the-art methods. We show that we can select a layer of the source\nmodel to perturb without any knowledge of the target models while achieving\nhigh transferability. Additionally, we provide some explanatory insights\nregarding our method and the effect of optimizing for adversarial examples\nusing intermediate feature maps. Our code is available at\nhttps://github.com/CUVL/Intermediate-Level-Attack.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 23:37:15 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 00:45:51 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 22:43:49 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Huang", "Qian", ""], ["Katsman", "Isay", ""], ["He", "Horace", ""], ["Gu", "Zeqi", ""], ["Belongie", "Serge", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "1907.10830", "submitter": "Junho Kim", "authors": "Junho Kim, Minjae Kim, Hyeonwoo Kang, Kwanghee Lee", "title": "U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive\n  Layer-Instance Normalization for Image-to-Image Translation", "comments": "Accepted to ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for unsupervised image-to-image translation, which\nincorporates a new attention module and a new learnable normalization function\nin an end-to-end manner. The attention module guides our model to focus on more\nimportant regions distinguishing between source and target domains based on the\nattention map obtained by the auxiliary classifier. Unlike previous\nattention-based method which cannot handle the geometric changes between\ndomains, our model can translate both images requiring holistic changes and\nimages requiring large shape changes. Moreover, our new AdaLIN (Adaptive\nLayer-Instance Normalization) function helps our attention-guided model to\nflexibly control the amount of change in shape and texture by learned\nparameters depending on datasets. Experimental results show the superiority of\nthe proposed method compared to the existing state-of-the-art models with a\nfixed network architecture and hyper-parameters. Our code and datasets are\navailable at https://github.com/taki0112/UGATIT or\nhttps://github.com/znxlwm/UGATIT-pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 04:17:25 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 08:58:07 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 10:14:18 GMT"}, {"version": "v4", "created": "Wed, 8 Apr 2020 15:36:10 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Kim", "Junho", ""], ["Kim", "Minjae", ""], ["Kang", "Hyeonwoo", ""], ["Lee", "Kwanghee", ""]]}, {"id": "1907.10831", "submitter": "James Folberth", "authors": "James Folberth, Stephen Becker", "title": "Safe Feature Elimination for Non-Negativity Constrained Convex\n  Optimization", "comments": "v2: added pointer to version to appear in Journal of Optimization\n  Theory and Applications", "journal-ref": null, "doi": "10.1007/s10957-019-01612-w", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent work on safe feature elimination for $1$-norm regularized\nleast-squares, we develop strategies to eliminate features from convex\noptimization problems with non-negativity constraints. Our strategy is safe in\nthe sense that it will only remove features/coordinates from the problem when\nthey are guaranteed to be zero at a solution. To perform feature elimination we\nuse an accurate, but not optimal, primal-dual feasible pair, making our methods\nrobust and able to be used on ill-conditioned problems. We supplement our\nfeature elimination problem with a method to construct an accurate dual\nfeasible point from an accurate primal feasible point; this allows us to use a\nfirst-order method to find an accurate primal feasible point, then use that\npoint to construct an accurate dual feasible point and perform feature\nelimination. Under reasonable conditions, our feature elimination strategy will\neventually eliminate all zero features from the problem. As an application of\nour methods we show how safe feature elimination can be used to robustly\ncertify the uniqueness of non-negative least-squares (NNLS) problems. We give\nnumerical examples on a well-conditioned synthetic NNLS problem and a on set of\n40000 extremely ill-conditioned NNLS problems arising in a microscopy\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 04:24:20 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 18:43:20 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Folberth", "James", ""], ["Becker", "Stephen", ""]]}, {"id": "1907.10837", "submitter": "Chunfei Ma", "authors": "Chunfei Ma, Joonhyang Choi, Byeongwon Lee, Seungji Yang", "title": "Submission to ActivityNet Challenge 2019: Task B Spatio-temporal Action\n  Localization", "comments": "4 pages, 2 fighures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report present an overview of our system proposed for the\nspatio-temporal action localization(SAL) task in ActivityNet Challenge 2019.\nUnlike previous two-streams-based works, we focus on exploring the end-to-end\ntrainable architecture using only RGB sequential images. To this end, we employ\na previously proposed simple yet effective two-branches network called SlowFast\nNetworks which is capable of capturing both short- and long-term spatiotemporal\nfeatures. Moreover, to handle the severe class imbalance and overfitting\nproblems, we propose a correlation-preserving data augmentation method and a\nrandom label subsampling method which have been proven to be able to reduce\noverfitting and improve the performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 04:48:10 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Ma", "Chunfei", ""], ["Choi", "Joonhyang", ""], ["Lee", "Byeongwon", ""], ["Yang", "Seungji", ""]]}, {"id": "1907.10838", "submitter": "Wenxuan Wang", "authors": "Wenxuan Wang, Qiang Sun, Tao Chen, Chenjie Cao, Ziqi Zheng, Guoqiang\n  Xu, Han Qiu, Yanwei Fu", "title": "A Fine-Grained Facial Expression Database for End-to-End Multi-Pose\n  Facial Expression Recognition", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent research of facial expression recognition has made a lot of\nprogress due to the development of deep learning technologies, but some typical\nchallenging problems such as the variety of rich facial expressions and poses\nare still not resolved. To solve these problems, we develop a new Facial\nExpression Recognition (FER) framework by involving the facial poses into our\nimage synthesizing and classification process. There are two major novelties in\nthis work. First, we create a new facial expression dataset of more than 200k\nimages with 119 persons, 4 poses and 54 expressions. To our knowledge this is\nthe first dataset to label faces with subtle emotion changes for expression\nrecognition purpose. It is also the first dataset that is large enough to\nvalidate the FER task on unbalanced poses, expressions, and zero-shot subject\nIDs. Second, we propose a facial pose generative adversarial network (FaPE-GAN)\nto synthesize new facial expression images to augment the data set for training\npurpose, and then learn a LightCNN based Fa-Net model for expression\nclassification. Finally, we advocate four novel learning tasks on this dataset.\nThe experimental results well validate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 04:58:33 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Wang", "Wenxuan", ""], ["Sun", "Qiang", ""], ["Chen", "Tao", ""], ["Cao", "Chenjie", ""], ["Zheng", "Ziqi", ""], ["Xu", "Guoqiang", ""], ["Qiu", "Han", ""], ["Fu", "Yanwei", ""]]}, {"id": "1907.10839", "submitter": "Yun Ye", "authors": "Yun Ye, Yixin Li, Bo Wu, Wei Zhang, Lingyu Duan, Tao Mei", "title": "Hard-Aware Fashion Attribute Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion attribute classification is of great importance to many high-level\ntasks such as fashion item search, fashion trend analysis, fashion\nrecommendation, etc. The task is challenging due to the extremely imbalanced\ndata distribution, particularly the attributes with only a few positive\nsamples. In this paper, we introduce a hard-aware pipeline to make full use of\n\"hard\" samples/attributes. We first propose Hard-Aware BackPropagation (HABP)\nto efficiently and adaptively focus on training \"hard\" data. Then for the\nidentified hard labels, we propose to synthesize more complementary samples for\ntraining. To stabilize training, we extend semi-supervised GAN by directly\ndeactivating outputs for synthetic complementary samples (Deact). In general,\nour method is more effective in addressing \"hard\" cases. HABP weights more on\n\"hard\" samples. For \"hard\" attributes with insufficient training data, Deact\nbrings more stable synthetic samples for training and further improve the\nperformance. Our method is verified on large scale fashion dataset,\noutperforming other state-of-the-art without any additional supervisions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 05:02:02 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Ye", "Yun", ""], ["Li", "Yixin", ""], ["Wu", "Bo", ""], ["Zhang", "Wei", ""], ["Duan", "Lingyu", ""], ["Mei", "Tao", ""]]}, {"id": "1907.10843", "submitter": "Yu-Jhe Li", "authors": "Yun-Chun Chen, Yu-Jhe Li, Xiaofei Du, Yu-Chiang Frank Wang", "title": "Learning Resolution-Invariant Deep Representations for Person\n  Re-Identification", "comments": "Accepted to AAAI 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) solves the task of matching images across\ncameras and is among the research topics in vision community. Since query\nimages in real-world scenarios might suffer from resolution loss, how to solve\nthe resolution mismatch problem during person re-ID becomes a practical\nproblem. Instead of applying separate image super-resolution models, we propose\na novel network architecture of Resolution Adaptation and re-Identification\nNetwork (RAIN) to solve cross-resolution person re-ID. Advancing the strategy\nof adversarial learning, we aim at extracting resolution-invariant\nrepresentations for re-ID, while the proposed model is learned in an end-to-end\ntraining fashion. Our experiments confirm that the use of our model can\nrecognize low-resolution query images, even if the resolution is not seen\nduring training. Moreover, the extension of our model for semi-supervised re-ID\nfurther confirms the scalability of our proposed method for real-world\nscenarios and applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 05:24:05 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Chen", "Yun-Chun", ""], ["Li", "Yu-Jhe", ""], ["Du", "Xiaofei", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1907.10844", "submitter": "Ruihui Li", "authors": "Ruihui Li and Xianzhi Li and Chi-Wing Fu and Daniel Cohen-Or and\n  Pheng-Ann Heng", "title": "PU-GAN: a Point Cloud Upsampling Adversarial Network", "comments": "accepted by ICCV2019, project page at\n  https://liruihui.github.io/publication/PU-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds acquired from range scans are often sparse, noisy, and\nnon-uniform. This paper presents a new point cloud upsampling network called\nPU-GAN, which is formulated based on a generative adversarial network (GAN), to\nlearn a rich variety of point distributions from the latent space and upsample\npoints over patches on object surfaces. To realize a working GAN network, we\nconstruct an up-down-up expansion unit in the generator for upsampling point\nfeatures with error feedback and self-correction, and formulate a\nself-attention unit to enhance the feature integration. Further, we design a\ncompound loss with adversarial, uniform and reconstruction terms, to encourage\nthe discriminator to learn more latent patterns and enhance the output point\ndistribution uniformity. Qualitative and quantitative evaluations demonstrate\nthe quality of our results over the state-of-the-arts in terms of distribution\nuniformity, proximity-to-surface, and 3D reconstruction quality.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 05:28:56 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Li", "Ruihui", ""], ["Li", "Xianzhi", ""], ["Fu", "Chi-Wing", ""], ["Cohen-Or", "Daniel", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1907.10880", "submitter": "Yuriy Anisimov", "authors": "Yuriy Anisimov, Oliver Wasenm\\\"uller, Didier Stricker", "title": "A Compact Light Field Camera for Real-Time Depth Estimation", "comments": "The 18th International Conference on Computer Analysis of Images and\n  Patterns, Salerno, 2-6 September, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Depth cameras are utilized in many applications. Recently light field\napproaches are increasingly being used for depth computation. While these\napproaches demonstrate the technical feasibility, they can not be brought into\nreal-world application, since they have both a high computation time as well as\na large design. Exactly these two drawbacks are overcome in this paper. For the\nfirst time, we present a depth camera based on the light field principle, which\nprovides real-time depth information as well as a compact design.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 07:48:11 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Anisimov", "Yuriy", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "1907.10882", "submitter": "Max Losch", "authors": "Max Losch, Mario Fritz, Bernt Schiele", "title": "Interpretability Beyond Classification Output: Semantic Bottleneck\n  Networks", "comments": "Correct figures in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Today's deep learning systems deliver high performance based on end-to-end\ntraining. While they deliver strong performance, these systems are hard to\ninterpret. To address this issue, we propose Semantic Bottleneck Networks\n(SBN): deep networks with semantically interpretable intermediate layers that\nall downstream results are based on. As a consequence, the analysis on what the\nfinal prediction is based on is transparent to the engineer and failure cases\nand modes can be analyzed and avoided by high-level reasoning. We present a\ncase study on street scene segmentation to demonstrate the feasibility and\npower of SBN. In particular, we start from a well performing classic deep\nnetwork which we adapt to house a SB-Layer containing task related semantic\nconcepts (such as object-parts and materials). Importantly, we can recover\nstate of the art performance despite a drastic dimensionality reduction from\n1000s (non-semantic feature) to 10s (semantic concept) channels. Additionally\nwe show how the activations of the SB-Layer can be used for both the\ninterpretation of failure cases of the network as well as for confidence\nprediction of the resulting output. For the first time, e.g., we show\ninterpretable segmentation results for most predictions at over 99% accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 07:53:00 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 11:40:03 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Losch", "Max", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1907.10889", "submitter": "Hiroyuki Kobayashi", "authors": "Hiroyuki Kobayashi and Hitoshi Kiya", "title": "Performance Evaluation of Two-layer lossless HDR Coding using Histogram\n  Packing Technique under Various Tone-mapping Operators", "comments": "to appear in 2019 IEEE 8th Global Conference on Consumer Electronics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a lossless two-layer HDR coding method using a histogram packing\ntechnique. The proposed method was demonstrated to outperform the normative\nJPEG XT encoder, under the use of the default tone-mapping operator. However,\nthe performance under various tone-mapping operators has not been discussed. In\nthis paper, we aim to compare the performance of the proposed method with that\nof the JPEG XT encoder under the use of various tone-mapping operators to\nclearly show the characteristic difference between them.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 08:08:13 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Kobayashi", "Hiroyuki", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1907.10892", "submitter": "Ahmed Nassar", "authors": "Ahmed Samy Nassar, Sebastien Lefevre, Jan D. Wegner", "title": "Simultaneous multi-view instance detection with learned geometric\n  soft-constraints", "comments": "Internationcal Conference on Computer Vision 2019 (ICCV 19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to jointly learn multi-view geometry and warping between views of\nthe same object instances for robust cross-view object detection. What makes\nmulti-view object instance detection difficult are strong changes in viewpoint,\nlighting conditions, high similarity of neighbouring objects, and strong\nvariability in scale. By turning object detection and instance\nre-identification in different views into a joint learning task, we are able to\nincorporate both image appearance and geometric soft constraints into a single,\nmulti-view detection process that is learnable end-to-end. We validate our\nmethod on a new, large data set of street-level panoramas of urban objects and\nshow superior performance compared to various baselines. Our contribution is\nthreefold: a large-scale, publicly available data set for multi-view instance\ndetection and re-identification; an annotation tool custom-tailored for\nmulti-view instance detection; and a novel, holistic multi-view instance\ndetection and re-identification method that jointly models geometry and\nappearance across views.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 08:11:22 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Nassar", "Ahmed Samy", ""], ["Lefevre", "Sebastien", ""], ["Wegner", "Jan D.", ""]]}, {"id": "1907.10901", "submitter": "Tom Viering", "authors": "Tom Viering, Ziqi Wang, Marco Loog, Elmar Eisemann", "title": "How to Manipulate CNNs to Make Them Lie: the GradCAM Case", "comments": "Presented at BMVC 2019: Workshop on Interpretable and Explainable\n  Machine Vision, Cardiff, UK. Updated to BMVC template", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently many methods have been introduced to explain CNN decisions. However,\nit has been shown that some methods can be sensitive to manipulation of the\ninput. We continue this line of work and investigate the explanation method\nGradCAM. Instead of manipulating the input, we consider an adversary that\nmanipulates the model itself to attack the explanation. By changing weights and\narchitecture, we demonstrate that it is possible to generate any desired\nexplanation, while leaving the model's accuracy essentially unchanged. This\nillustrates that GradCAM cannot explain the decision of every CNN and provides\na proof of concept showing that it is possible to obfuscate the inner workings\nof a CNN. Finally, we combine input and model manipulation. To this end we put\na backdoor in the network: the explanation is correct unless there is a\nspecific pattern present in the input, which triggers a malicious explanation.\nOur work raises new security concerns, especially in settings where\nexplanations of models may be used to make decisions, such as in the medical\ndomain.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 08:51:08 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 14:12:21 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Viering", "Tom", ""], ["Wang", "Ziqi", ""], ["Loog", "Marco", ""], ["Eisemann", "Elmar", ""]]}, {"id": "1907.10915", "submitter": "Jiaolong Xu", "authors": "Jiaolong Xu and Liang Xiao and Antonio M. Lopez", "title": "Self-supervised Domain Adaptation for Computer Vision Tasks", "comments": "Accepted by IEEE Access", "journal-ref": "IEEE Access. 7 (2019) 156694-156706", "doi": "10.1109/ACCESS.2019.2949697", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress of self-supervised visual representation learning has\nachieved remarkable success on many challenging computer vision benchmarks.\nHowever, whether these techniques can be used for domain adaptation has not\nbeen explored. In this work, we propose a generic method for self-supervised\ndomain adaptation, using object recognition and semantic segmentation of urban\nscenes as use cases. Focusing on simple pretext/auxiliary tasks (e.g. image\nrotation prediction), we assess different learning strategies to improve domain\nadaptation effectiveness by self-supervision. Additionally, we propose two\ncomplementary strategies to further boost the domain adaptation accuracy on\nsemantic segmentation within our method, consisting of prediction layer\nalignment and batch normalization calibration. The experimental results show\nadaptation levels comparable to most studied domain adaptation methods, thus,\nbringing self-supervision as a new alternative for reaching domain adaptation.\nThe code is available at https://github.com/Jiaolong/self-supervised-da.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:20:29 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 05:17:19 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 23:55:34 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Xu", "Jiaolong", ""], ["Xiao", "Liang", ""], ["Lopez", "Antonio M.", ""]]}, {"id": "1907.10931", "submitter": "Mattias Heinrich", "authors": "Mattias P. Heinrich", "title": "Closing the Gap between Deep and Conventional Image Registration using\n  Probabilistic Dense Displacement Networks", "comments": "accepted for publication at MICCAI 2019, open source code available\n  at https://github.com/multimodallearning/pdd_net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear image registration continues to be a fundamentally important tool\nin medical image analysis. Diagnostic tasks, image-guided surgery and\nradiotherapy as well as motion analysis all rely heavily on accurate\nintra-patient alignment. Furthermore, inter-patient registration enables\natlas-based segmentation or landmark localisation and shape analysis. When\nlabelled scans are scarce and anatomical differences large, conventional\nregistration has often remained superior to deep learning methods that have so\nfar mainly dealt with relatively small or low-complexity deformations. We\naddress this shortcoming by leveraging ideas from probabilistic dense\ndisplacement optimisation that has excelled in many registration tasks with\nlarge deformations. We propose to design a network with approximate\nmin-convolutions and mean field inference for differentiable displacement\nregularisation within a discrete weakly-supervised registration setting. By\nemploying these meaningful and theoretically proven constraints, our learnable\nregistration algorithm contains very few trainable weights (primarily for\nfeature extraction) and is easier to train with few labelled scans. It is very\nfast in training and inference and achieves state-of-the-art accuracies for the\nchallenging inter-patient registration of abdominal CT outperforming previous\ndeep learning approaches by 15% Dice overlap.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:44:12 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Heinrich", "Mattias P.", ""]]}, {"id": "1907.10935", "submitter": "Cristian Ivan", "authors": "Cristian Ivan", "title": "Convolutional Neural Networks on Randomized Data", "comments": "8 pages, 17 figures, presented at Deep-Vision workshop, CVPR 2019", "journal-ref": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition Workshops, pp. 1-8. 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are build specifically for computer\nvision tasks for which it is known that the input data is a hierarchical\nstructure based on locally correlated elements. The question that naturally\narises is what happens with the performance of CNNs if one of the basic\nproperties of the data is removed, e.g. what happens if the image pixels are\nrandomly permuted? Intuitively one expects that the convolutional network\nperforms poorly in these circumstances in contrast to a multilayer perceptron\n(MLPs) whose classification accuracy should not be affected by the pixel\nrandomization. This work shows that by randomizing image pixels the\nhierarchical structure of the data is destroyed and long range correlations are\nintroduced which standard CNNs are not able to capture. We show that their\nclassification accuracy is heavily dependent on the class similarities as well\nas the pixel randomization process. We also indicate that dilated convolutions\nare able to recover some of the pixel correlations and improve the performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:59:27 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Ivan", "Cristian", ""]]}, {"id": "1907.10936", "submitter": "Zhijie Zhang", "authors": "Zhijie Zhang and Huazhu Fu and Hang Dai and Jianbing Shen and Yanwei\n  Pang and Ling Shao", "title": "ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image\n  Segmentation", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is a fundamental task in medical image analysis. However, most\nexisting methods focus on primary region extraction and ignore edge\ninformation, which is useful for obtaining accurate segmentation. In this\npaper, we propose a generic medical segmentation method, called Edge-aTtention\nguidance Network (ET-Net), which embeds edge-attention representations to guide\nthe segmentation network. Specifically, an edge guidance module is utilized to\nlearn the edge-attention representations in the early encoding layers, which\nare then transferred to the multi-scale decoding layers, fused using a weighted\naggregation module. The experimental results on four segmentation tasks (i.e.,\noptic disc/cup and vessel segmentation in retinal images, and lung segmentation\nin chest X-Ray and CT images) demonstrate that preserving edge-attention\nrepresentations contributes to the final segmentation accuracy, and our\nproposed method outperforms current state-of-the-art segmentation methods. The\nsource code of our method is available at https://github.com/ZzzJzzZ/ETNet.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 10:00:08 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Zhang", "Zhijie", ""], ["Fu", "Huazhu", ""], ["Dai", "Hang", ""], ["Shen", "Jianbing", ""], ["Pang", "Yanwei", ""], ["Shao", "Ling", ""]]}, {"id": "1907.10949", "submitter": "Massimiliano Patacchiola PhD", "authors": "Massimiliano Patacchiola and Patrick Fox-Roberts and Edward Rosten", "title": "Y-Autoencoders: disentangling latent representations via\n  sequential-encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years there have been important advancements in generative\nmodels with the two dominant approaches being Generative Adversarial Networks\n(GANs) and Variational Autoencoders (VAEs). However, standard Autoencoders\n(AEs) and closely related structures have remained popular because they are\neasy to train and adapt to different tasks. An interesting question is if we\ncan achieve state-of-the-art performance with AEs while retaining their good\nproperties. We propose an answer to this question by introducing a new model\ncalled Y-Autoencoder (Y-AE). The structure and training procedure of a Y-AE\nenclose a representation into an implicit and an explicit part. The implicit\npart is similar to the output of an autoencoder and the explicit part is\nstrongly correlated with labels in the training set. The two parts are\nseparated in the latent space by splitting the output of the encoder into two\npaths (forming a Y shape) before decoding and re-encoding. We then impose a\nnumber of losses, such as reconstruction loss, and a loss on dependence between\nthe implicit and explicit parts. Additionally, the projection in the explicit\nmanifold is monitored by a predictor, that is embedded in the encoder and\ntrained end-to-end with no adversarial losses. We provide significant\nexperimental results on various domains, such as separation of style and\ncontent, image-to-image translation, and inverse graphics.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 10:28:15 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Patacchiola", "Massimiliano", ""], ["Fox-Roberts", "Patrick", ""], ["Rosten", "Edward", ""]]}, {"id": "1907.10958", "submitter": "Mengyu Liu", "authors": "Mengyu Liu, Hujun Yin", "title": "Cross Attention Network for Semantic Segmentation", "comments": "Accepted in ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the semantic segmentation task with a deep network\nthat combines contextual features and spatial information. The proposed Cross\nAttention Network is composed of two branches and a Feature Cross Attention\n(FCA) module. Specifically, a shallow branch is used to preserve low-level\nspatial information and a deep branch is employed to extract high-level\ncontextual features. Then the FCA module is introduced to combine these two\nbranches. Different from most existing attention mechanisms, the FCA module\nobtains spatial attention map and channel attention map from two branches\nseparately, and then fuses them. The contextual features are used to provide\nglobal contextual guidance in fused feature maps, and spatial features are used\nto refine localizations. The proposed network outperforms other real-time\nmethods with improved speed on the Cityscapes and CamVid datasets with\nlightweight backbones, and achieves state-of-the-art performance with a deep\nbackbone.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 10:46:03 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Liu", "Mengyu", ""], ["Yin", "Hujun", ""]]}, {"id": "1907.10961", "submitter": "Nick Pawlowski", "authors": "Nick Pawlowski, Ben Glocker", "title": "Is Texture Predictive for Age and Sex in Brain MRI?", "comments": "MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/BJxgXfab94", "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep learning builds the foundation for many medical image analysis tasks\nwhere neuralnetworks are often designed to have a large receptive field to\nincorporate long spatialdependencies. Recent work has shown that large\nreceptive fields are not always necessaryfor computer vision tasks on natural\nimages. We explore whether this translates to certainmedical imaging tasks such\nas age and sex prediction from a T1-weighted brain MRI scans.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 10:53:28 GMT"}], "update_date": "2019-07-27", "authors_parsed": [["Pawlowski", "Nick", ""], ["Glocker", "Ben", ""]]}, {"id": "1907.10982", "submitter": "Zeju Li", "authors": "Zeju Li and Konstantinos Kamnitsas and Ben Glocker", "title": "Overfitting of neural nets under class imbalance: Analysis and\n  improvements for segmentation", "comments": "Accepted at MICCAI 2019; typo corrected in Table 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overfitting in deep learning has been the focus of a number of recent works,\nyet its exact impact on the behavior of neural networks is not well understood.\nThis study analyzes overfitting by examining how the distribution of logits\nalters in relation to how much the model overfits. Specifically, we find that\nwhen training with few data samples, the distribution of logit activations when\nprocessing unseen test samples of an under-represented class tends to shift\ntowards and even across the decision boundary, while the over-represented class\nseems unaffected. In image segmentation, foreground samples are often heavily\nunder-represented. We observe that sensitivity of the model drops as a result\nof overfitting, while precision remains mostly stable. Based on our analysis,\nwe derive asymmetric modifications of existing loss functions and regularizers\nincluding a large margin loss, focal loss, adversarial training and mixup,\nwhich specifically aim at reducing the shift observed when embedding unseen\nsamples of the under-represented class. We study the case of binary\nsegmentation of brain tumor core and show that our proposed simple\nmodifications lead to significantly improved segmentation performance over the\nsymmetric variants.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 11:47:12 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 13:22:58 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Li", "Zeju", ""], ["Kamnitsas", "Konstantinos", ""], ["Glocker", "Ben", ""]]}, {"id": "1907.10992", "submitter": "Qing Zhang", "authors": "Qing Zhang, Yongwei Nie, Lei Zhu, Chunxia Xiao, Wei-Shi Zheng", "title": "Enhancing Underexposed Photos using Perceptually Bidirectional\n  Similarity", "comments": "Aceepted to IEEE Transactions on Multimedia (TMM)", "journal-ref": null, "doi": "10.1109/TMM.2020.2982045", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although remarkable progress has been made, existing methods for enhancing\nunderexposed photos tend to produce visually unpleasing results due to the\nexistence of visual artifacts (e.g., color distortion, loss of details and\nuneven exposure). We observed that this is because they fail to ensure the\nperceptual consistency of visual information between the source underexposed\nimage and its enhanced output. To obtain high-quality results free of these\nartifacts, we present a novel underexposed photo enhancement approach that is\nable to maintain the perceptual consistency. We achieve this by proposing an\neffective criterion, referred to as perceptually bidirectional similarity,\nwhich explicitly describes how to ensure the perceptual consistency.\nParticularly, we adopt the Retinex theory and cast the enhancement problem as a\nconstrained illumination estimation optimization, where we formulate\nperceptually bidirectional similarity as constraints on illumination and solve\nfor the illumination which can recover the desired artifact-free enhancement\nresults. In addition, we describe a video enhancement framework that adopts the\npresented illumination estimation for handling underexposed videos. To this\nend, a probabilistic approach is introduced to propagate illuminations of\nsampled keyframes to the entire video by tackling a Bayesian Maximum A\nPosteriori problem. Extensive experiments demonstrate the superiority of our\nmethod over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 12:01:59 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 05:02:12 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 12:56:44 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Zhang", "Qing", ""], ["Nie", "Yongwei", ""], ["Zhu", "Lei", ""], ["Xiao", "Chunxia", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1907.10993", "submitter": "Beatrice van Amsterdam", "authors": "Beatrice van Amsterdam, Hirenkumar Nakawala, Elena De Momi, Danail\n  Stoyanov", "title": "Weakly Supervised Recognition of Surgical Gestures", "comments": "2019 IEEE International Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinematic trajectories recorded from surgical robots contain information\nabout surgical gestures and potentially encode cues about surgeon's skill\nlevels. Automatic segmentation of these trajectories into meaningful action\nunits could help to develop new metrics for surgical skill assessment as well\nas to simplify surgical automation. State-of-the-art methods for action\nrecognition relied on manual labelling of large datasets, which is time\nconsuming and error prone. Unsupervised methods have been developed to overcome\nthese limitations. However, they often rely on tedious parameter tuning and\nperform less well than supervised approaches, especially on data with high\nvariability such as surgical trajectories. Hence, the potential of weak\nsupervision could be to improve unsupervised learning while avoiding manual\nannotation of large datasets. In this paper, we used at a minimum one expert\ndemonstration and its ground truth annotations to generate an appropriate\ninitialization for a GMM-based algorithm for gesture recognition. We showed on\nreal surgical demonstrations that the latter significantly outperforms standard\ntask-agnostic initialization methods. We also demonstrated how to improve the\nrecognition accuracy further by redefining the actions and optimising the\ninputs.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 12:07:33 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["van Amsterdam", "Beatrice", ""], ["Nakawala", "Hirenkumar", ""], ["De Momi", "Elena", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1907.11004", "submitter": "Horia Porav", "authors": "Horia Porav, Tom Bruls and Paul Newman", "title": "Don't Worry About the Weather: Unsupervised Condition-Dependent Domain\n  Adaptation", "comments": "Presented at ITSC2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern models that perform system-critical tasks such as segmentation and\nlocalization exhibit good performance and robustness under ideal conditions\n(i.e. daytime, overcast) but performance degrades quickly and often\ncatastrophically when input conditions change. In this work, we present a\ndomain adaptation system that uses light-weight input adapters to pre-processes\ninput images, irrespective of their appearance, in a way that makes them\ncompatible with off-the-shelf computer vision tasks that are trained only on\ninputs with ideal conditions. No fine-tuning is performed on the off-the-shelf\nmodels, and the system is capable of incrementally training new input adapters\nin a self-supervised fashion, using the computer vision tasks as supervisors,\nwhen the input domain differs significantly from previously seen domains. We\nreport large improvements in semantic segmentation and topological localization\nperformance on two popular datasets, RobotCar and BDD.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 12:28:35 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Porav", "Horia", ""], ["Bruls", "Tom", ""], ["Newman", "Paul", ""]]}, {"id": "1907.11025", "submitter": "Patrick Wenzel", "authors": "Qadeer Khan, Patrick Wenzel, Daniel Cremers, Laura Leal-Taix\\'e", "title": "Towards Generalizing Sensorimotor Control Across Weather Conditions", "comments": "Accepted for publication in 2019 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of deep learning models to generalize well across different\nscenarios depends primarily on the quality and quantity of annotated data.\nLabeling large amounts of data for all possible scenarios that a model may\nencounter would not be feasible; if even possible. We propose a framework to\ndeal with limited labeled training data and demonstrate it on the application\nof vision-based vehicle control. We show how limited steering angle data\navailable for only one condition can be transferred to multiple different\nweather scenarios. This is done by leveraging unlabeled images in a\nteacher-student learning paradigm complemented with an image-to-image\ntranslation network. The translation network transfers the images to a new\ndomain, whereas the teacher provides soft supervised targets to train the\nstudent on this domain. Furthermore, we demonstrate how utilization of\nauxiliary networks can reduce the size of a model at inference time, without\naffecting the accuracy. The experiments show that our approach generalizes well\nacross multiple different weather conditions using only ground truth labels\nfrom one domain.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:22:05 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Khan", "Qadeer", ""], ["Wenzel", "Patrick", ""], ["Cremers", "Daniel", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "1907.11066", "submitter": "Kaite Xiang", "authors": "Kaite Xiang, Kaiwei Wang and Kailun Yang", "title": "Importance-Aware Semantic Segmentation with Efficient Pyramidal Context\n  Network for Navigational Assistant Systems", "comments": "7 pages, 22 figures, IEEE Intelligent Transportation Systems\n  Conference - ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Segmentation (SS) is a task to assign semantic label to each pixel\nof the images, which is of immense significance for autonomous vehicles,\nrobotics and assisted navigation of vulnerable road users. It is obvious that\nin different application scenarios, different objects possess hierarchical\nimportance and safety-relevance, but conventional loss functions like cross\nentropy have not taken the different levels of importance of diverse traffic\nelements into consideration. To address this dilemma, we leverage and re-design\nan importance-aware loss function, throwing insightful hints on how importance\nof semantics are assigned for real-world applications. To customize semantic\nsegmentation networks for different navigational tasks, we extend ERF-PSPNet, a\nreal-time segmenter designed for wearable device aiding visually impaired\npedestrians, and propose BiERF-PSPNet, which can yield high-quality\nsegmentation maps with finer spatial details exceptionally suitable for\nautonomous vehicles. A comprehensive variety of experiments with these\nefficient pyramidal context networks on CamVid and Cityscapes datasets\ndemonstrates the effectiveness of our proposal to support diverse navigational\nassistant systems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:04:22 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 12:08:24 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Xiang", "Kaite", ""], ["Wang", "Kaiwei", ""], ["Yang", "Kailun", ""]]}, {"id": "1907.11093", "submitter": "Pengyi Zhangpy", "authors": "Pengyi Zhang, Yunxin Zhong, Xiaoqiong Li", "title": "SlimYOLOv3: Narrower, Faster and Better for Real-Time UAV Applications", "comments": null, "journal-ref": null, "doi": "10.1109/ICCVW.2019.00011", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Drones or general Unmanned Aerial Vehicles (UAVs), endowed with computer\nvision function by on-board cameras and embedded systems, have become popular\nin a wide range of applications. However, real-time scene parsing through\nobject detection running on a UAV platform is very challenging, due to limited\nmemory and computing power of embedded devices. To deal with these challenges,\nin this paper we propose to learn efficient deep object detectors through\nchannel pruning of convolutional layers. To this end, we enforce channel-level\nsparsity of convolutional layers by imposing L1 regularization on channel\nscaling factors and prune less informative feature channels to obtain \"slim\"\nobject detectors. Based on such approach, we present SlimYOLOv3 with fewer\ntrainable parameters and floating point operations (FLOPs) in comparison of\noriginal YOLOv3 (Joseph Redmon et al., 2018) as a promising solution for\nreal-time object detection on UAVs. We evaluate SlimYOLOv3 on VisDrone2018-Det\nbenchmark dataset; compelling results are achieved by SlimYOLOv3 in comparison\nof unpruned counterpart, including ~90.8% decrease of FLOPs, ~92.0% decline of\nparameter size, running ~2 times faster and comparable detection accuracy as\nYOLOv3. Experimental results with different pruning ratios consistently verify\nthat proposed SlimYOLOv3 with narrower structure are more efficient, faster and\nbetter than YOLOv3, and thus are more suitable for real-time object detection\non UAVs. Our codes are made publicly available at\nhttps://github.com/PengyiZhang/SlimYOLOv3.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:22:43 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Zhang", "Pengyi", ""], ["Zhong", "Yunxin", ""], ["Li", "Xiaoqiong", ""]]}, {"id": "1907.11106", "submitter": "Mihai B\\^ace", "authors": "Mihai B\\^ace, Sander Staal, Andreas Bulling", "title": "How far are we from quantifying visual attention in mobile HCI?", "comments": "7 pages, 4 figures", "journal-ref": "IEEE Pervasive Computing, April-June 2020", "doi": "10.1109/MPRV.2020.2967736", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an ever-increasing number of mobile devices competing for our attention,\nquantifying when, how often, or for how long users visually attend to their\ndevices has emerged as a core challenge in mobile human-computer interaction.\nEncouraged by recent advances in automatic eye contact detection using machine\nlearning and device-integrated cameras, we provide a fundamental investigation\ninto the feasibility of quantifying visual attention during everyday mobile\ninteractions. We identify core challenges and sources of errors associated with\nsensing attention on mobile devices in the wild, including the impact of face\nand eye visibility, the importance of robust head pose estimation, and the need\nfor accurate gaze estimation. Based on this analysis, we propose future\nresearch directions and discuss how eye contact detection represents the\nfoundation for exciting new applications towards next-generation pervasive\nattentive user interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:38:52 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["B\u00e2ce", "Mihai", ""], ["Staal", "Sander", ""], ["Bulling", "Andreas", ""]]}, {"id": "1907.11111", "submitter": "Lukas Liebel", "authors": "Lukas Liebel, Marco K\\\"orner", "title": "MultiDepth: Single-Image Depth Estimation via Multi-Task Regression and\n  Classification", "comments": "Accepted for presentation at the IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce MultiDepth, a novel training strategy and convolutional neural\nnetwork (CNN) architecture that allows approaching single-image depth\nestimation (SIDE) as a multi-task problem. SIDE is an important part of road\nscene understanding. It, thus, plays a vital role in advanced driver assistance\nsystems and autonomous vehicles. Best results for the SIDE task so far have\nbeen achieved using deep CNNs. However, optimization of regression problems,\nsuch as estimating depth, is still a challenging task. For the related tasks of\nimage classification and semantic segmentation, numerous CNN-based methods with\nrobust training behavior have been proposed. Hence, in order to overcome the\nnotorious instability and slow convergence of depth value regression during\ntraining, MultiDepth makes use of depth interval classification as an auxiliary\ntask. The auxiliary task can be disabled at test-time to predict continuous\ndepth values using the main regression branch more efficiently. We applied\nMultiDepth to road scenes and present results on the KITTI depth prediction\ndataset. In experiments, we were able to show that end-to-end multi-task\nlearning with both, regression and classification, is able to considerably\nimprove training and yield more accurate results.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:43:31 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Liebel", "Lukas", ""], ["K\u00f6rner", "Marco", ""]]}, {"id": "1907.11115", "submitter": "Mihai B\\^ace", "authors": "Mihai B\\^ace, Sander Staal, Andreas Bulling", "title": "Accurate and Robust Eye Contact Detection During Everyday Mobile Device\n  Interactions", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of human attention is key to several tasks in mobile\nhuman-computer interaction (HCI), such as predicting user interruptibility,\nestimating noticeability of user interface content, or measuring user\nengagement. Previous works to study mobile attentive behaviour required\nspecial-purpose eye tracking equipment or constrained users' mobility. We\npropose a novel method to sense and analyse visual attention on mobile devices\nduring everyday interactions. We demonstrate the capabilities of our method on\nthe sample task of eye contact detection that has recently attracted increasing\nresearch interest in mobile HCI. Our method builds on a state-of-the-art method\nfor unsupervised eye contact detection and extends it to address challenges\nspecific to mobile interactive scenarios. Through evaluation on two current\ndatasets, we demonstrate significant performance improvements for eye contact\ndetection across mobile devices, users, or environmental conditions. Moreover,\nwe discuss how our method enables the calculation of additional attention\nmetrics that, for the first time, enable researchers from different domains to\nstudy and quantify attention allocation during mobile interactions in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:55:16 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["B\u00e2ce", "Mihai", ""], ["Staal", "Sander", ""], ["Bulling", "Andreas", ""]]}, {"id": "1907.11117", "submitter": "Michael Wray", "authors": "Michael Wray and Dima Damen", "title": "Learning Visual Actions Using Multiple Verb-Only Labels", "comments": "Accepted at BMVC 2019. More information can be found at\n  https://mwray.github.io/MVOL/. Annotations can be found at\n  https://github.com/mwray/Multi-Verb-Labels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces verb-only representations for both recognition and\nretrieval of visual actions, in video. Current methods neglect legitimate\nsemantic ambiguities between verbs, instead choosing unambiguous subsets of\nverbs along with objects to disambiguate the actions. We instead propose\nmultiple verb-only labels, which we learn through hard or soft assignment as a\nregression. This enables learning a much larger vocabulary of verbs, including\ncontextual overlaps of these verbs. We collect multi-verb annotations for three\naction video datasets and evaluate the verb-only labelling representations for\naction recognition and cross-modal retrieval (video-to-text and text-to-video).\nWe demonstrate that multi-label verb-only representations outperform\nconventional single verb labels. We also explore other benefits of a multi-verb\nrepresentation including cross-dataset retrieval and verb type manner and\nresult verb types) retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:58:34 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 14:13:08 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Wray", "Michael", ""], ["Damen", "Dima", ""]]}, {"id": "1907.11150", "submitter": "Reuben Dorent", "authors": "Reuben Dorent, Samuel Joutard, Marc Modat, S\\'ebastien Ourselin, Tom\n  Vercauteren", "title": "Hetero-Modal Variational Encoder-Decoder for Joint Modality Completion\n  and Segmentation", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32245-8_9", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep learning method for tumour segmentation when dealing\nwith missing imaging modalities. Instead of producing one network for each\npossible subset of observed modalities or using arithmetic operations to\ncombine feature maps, our hetero-modal variational 3D encoder-decoder\nindependently embeds all observed modalities into a shared latent\nrepresentation. Missing data and tumour segmentation can be then generated from\nthis embedding. In our scenario, the input is a random subset of modalities. We\ndemonstrate that the optimisation problem can be seen as a mixture sampling. In\naddition to this, we introduce a new network architecture building upon both\nthe 3D U-Net and the Multi-Modal Variational Auto-Encoder (MVAE). Finally, we\nevaluate our method on BraTS2018 using subsets of the imaging modalities as\ninput. Our model outperforms the current state-of-the-art method for dealing\nwith missing modalities and achieves similar performance to the subset-specific\nequivalent networks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 15:49:11 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Dorent", "Reuben", ""], ["Joutard", "Samuel", ""], ["Modat", "Marc", ""], ["Ourselin", "S\u00e9bastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1907.11202", "submitter": "Ligong Han", "authors": "Ligong Han, Yang Zou, Ruijiang Gao, Lezi Wang, Dimitris Metaxas", "title": "Unsupervised Domain Adaptation via Calibrating Uncertainties", "comments": "4 pages", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) Workshops, 2019, pp. 99-102", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims at inferring class labels for\nunlabeled target domain given a related labeled source dataset. Intuitively, a\nmodel trained on source domain normally produces higher uncertainties for\nunseen data. In this work, we build on this assumption and propose to adapt\nfrom source to target domain via calibrating their predictive uncertainties.\nThe uncertainty is quantified as the Renyi entropy, from which we propose a\ngeneral Renyi entropy regularization (RER) framework. We further employ\nvariational Bayes learning for reliable uncertainty estimation. In addition,\ncalibrating the sample variance of network parameters serves as a plug-in\nregularizer for training. We discuss the theoretical properties of the proposed\nmethod and demonstrate its effectiveness on three domain-adaptation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 17:02:51 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Han", "Ligong", ""], ["Zou", "Yang", ""], ["Gao", "Ruijiang", ""], ["Wang", "Lezi", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1907.11210", "submitter": "Feng Shi", "authors": "Feng Shi, Ziheng Xu, Tao Yuan, Song-Chun Zhu", "title": "HUGE2: a Highly Untangled Generative-model Engine for Edge-computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a type of prominent studies in deep learning, generative models have been\nwidely investigated in research recently. Two research branches of the deep\nlearning models, the Generative Networks (GANs, VAE) and the Semantic\nSegmentation, rely highly on the upsampling operations, especially the\ntransposed convolution and the dilated convolution. However, these two types of\nconvolutions are intrinsically different from standard convolution regarding\nthe insertion of zeros in input feature maps or in kernels respectively. This\ndistinct nature severely degrades the performance of the existing deep learning\nengine or frameworks, such as Darknet, Tensorflow, and PyTorch, which are\nmainly developed for the standard convolution. Another trend in deep learning\nrealm is to deploy the model onto edge/ embedded devices, in which the memory\nresource is scarce. In this work, we propose a Highly Untangled\nGenerative-model Engine for Edge-computing or HUGE2 for accelerating these two\nspecial convolutions on the edge-computing platform by decomposing the kernels\nand untangling these smaller convolutions by performing basic matrix\nmultiplications. The methods we propose use much smaller memory footprint,\nhence much fewer memory accesses, and the data access patterns also\ndramatically increase the reusability of the data already fetched in caches,\nhence increasing the localities of caches. Our engine achieves a speedup of\nnearly 5x on embedded CPUs, and around 10x on embedded GPUs, and more than 50%\nreduction of memory access.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 17:21:52 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 17:07:28 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Shi", "Feng", ""], ["Xu", "Ziheng", ""], ["Yuan", "Tao", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1907.11272", "submitter": "Omar Elharrouss", "authors": "Noor Almaadeed, Omar Elharrouss, Somaya Al-Maadeed, Ahmed Bouridane,\n  Azeddine Beghdadi", "title": "A Novel Approach for Robust Multi Human Action Recognition and\n  Summarization based on 3D Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human actions in videos are 3D signals. However, there are a few methods\navailable for multiple human action recognition. For long videos, it's\ndifficult to search within a video for a specific action and/or person. For\nthat, this paper proposes a new technic for multiple human action recognition\nand summarization for surveillance videos. The proposed approach proposes a new\nrepresentation of the data by extracting the sequence of each person from the\nscene. This is followed by an analysis of each sequence to detect and recognize\nthe corresponding actions using 3D convolutional neural networks (3DCNNs).\nAction-based video summarization is performed by saving each person's action at\neach time of the video. Results of this work revealed that the proposed method\nprovides accurate multi human action recognition that easily used for\nsummarization of any action. Further, for other videos that can be collected\nfrom the internet, which are complex and not built for surveillance\napplications, the proposed model was evaluated on some datasets like UCF101 and\nYouTube without any preprocessing. For this category of videos, the\nsummarization is performed on the video sequences by summarizing the actions in\neach subsequence. The results obtained demonstrate its efficiency compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 18:48:59 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 22:04:40 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 22:38:41 GMT"}, {"version": "v4", "created": "Mon, 15 Mar 2021 08:56:57 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Almaadeed", "Noor", ""], ["Elharrouss", "Omar", ""], ["Al-Maadeed", "Somaya", ""], ["Bouridane", "Ahmed", ""], ["Beghdadi", "Azeddine", ""]]}, {"id": "1907.11292", "submitter": "Ming Li", "authors": "Ming Li, Weiwei Zhang, Guang Yang, Chengjia Wang, Heye Zhang, Huafeng\n  Liu, Wei Zheng, Shuo Li", "title": "Recurrent Aggregation Learning for Multi-View Echocardiographic\n  Sequences Segmentation", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view echocardiographic sequences segmentation is crucial for clinical\ndiagnosis. However, this task is challenging due to limited labeled data, huge\nnoise, and large gaps across views. Here we propose a recurrent aggregation\nlearning method to tackle this challenging task. By pyramid ConvBlocks,\nmulti-level and multi-scale features are extracted efficiently. Hierarchical\nConvLSTMs next fuse these features and capture spatial-temporal information in\nmulti-level and multi-scale space. We further introduce a double-branch\naggregation mechanism for segmentation and classification which are mutually\npromoted by deep aggregation of multi-level and multi-scale features. The\nsegmentation branch provides information to guide the classification while the\nclassification branch affords multi-view regularization to refine segmentations\nand further lessen gaps across views. Our method is built as an end-to-end\nframework for segmentation and classification. Adequate experiments on our\nmulti-view dataset (9000 labeled images) and the CAMUS dataset (1800 labeled\nimages) corroborate that our method achieves not only superior segmentation and\nclassification accuracy but also prominent temporal stability.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 14:43:18 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Li", "Ming", ""], ["Zhang", "Weiwei", ""], ["Yang", "Guang", ""], ["Wang", "Chengjia", ""], ["Zhang", "Heye", ""], ["Liu", "Huafeng", ""], ["Zheng", "Wei", ""], ["Li", "Shuo", ""]]}, {"id": "1907.11308", "submitter": "Yang Zhou", "authors": "Yang Zhou, Zachary While, Evangelos Kalogerakis", "title": "SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation", "comments": "8 pages, 8 figures, to appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a neural message passing approach to augment an\ninput 3D indoor scene with new objects matching their surroundings. Given an\ninput, potentially incomplete, 3D scene and a query location, our method\npredicts a probability distribution over object types that fit well in that\nlocation. Our distribution is predicted though passing learned messages in a\ndense graph whose nodes represent objects in the input scene and edges\nrepresent spatial and structural relationships. By weighting messages through\nan attention mechanism, our method learns to focus on the most relevant\nsurrounding scene context to predict new scene objects. We found that our\nmethod significantly outperforms state-of-the-art approaches in terms of\ncorrectly predicting objects missing in a scene based on our experiments in the\nSUNCG dataset. We also demonstrate other applications of our method, including\ncontext-based 3D object recognition and iterative scene generation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 21:03:15 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Zhou", "Yang", ""], ["While", "Zachary", ""], ["Kalogerakis", "Evangelos", ""]]}, {"id": "1907.11320", "submitter": "Hao Tang", "authors": "Hao Tang, Chupeng Zhang, and Xiaohui Xie", "title": "NoduleNet: Decoupled False Positive Reductionfor Pulmonary Nodule\n  Detection and Segmentation", "comments": "Accepted to MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pulmonary nodule detection, false positive reduction and segmentation\nrepresent three of the most common tasks in the computeraided analysis of chest\nCT images. Methods have been proposed for eachtask with deep learning based\nmethods heavily favored recently. However training deep learning models to\nsolve each task separately may be sub-optimal - resource intensive and without\nthe benefit of feature sharing. Here, we propose a new end-to-end 3D deep\nconvolutional neural net (DCNN), called NoduleNet, to solve nodule detection,\nfalse positive reduction and nodule segmentation jointly in a multi-task\nfashion. To avoid friction between different tasks and encourage feature\ndiversification, we incorporate two major design tricks: 1) decoupled feature\nmaps for nodule detection and false positive reduction, and 2) a segmentation\nrefinement subnet for increasing the precision of nodule segmentation.\nExtensive experiments on the large-scale LIDC dataset demonstrate that the\nmulti-task training is highly beneficial, improving the nodule detection\naccuracy by 10.27%, compared to the baseline model trained to only solve the\nnodule detection task. We also carry out systematic ablation studies to\nhighlight contributions from each of the added components. Code is available at\nhttps://github.com/uci-cbcl/NoduleNet.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 22:05:21 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Tang", "Hao", ""], ["Zhang", "Chupeng", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1907.11341", "submitter": "Saem Park", "authors": "Saem Park, Nojun Kwak", "title": "Image Enhancement by Recurrently-trained Super-resolution Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new learning strategy for image enhancement by recurrently\ntraining the same simple superresolution (SR) network multiple times. After\ninitially training an SR network by using pairs of a corrupted low resolution\n(LR) image and an original image, the proposed method makes use of the trained\nSR network to generate new high resolution (HR) images with a doubled\nresolution from the original uncorrupted images. Then, the new HR images are\ndownscaled to the original resolution, which work as target images for the SR\nnetwork in the next stage. The newly generated HR images by the repeatedly\ntrained SR network show better image quality and this strategy of training LR\nto mimic new HR can lead to a more efficient SR network. Up to a certain point,\nby repeating this process multiple times, better and better images are\nobtained. This recurrent leaning strategy for SR can be a good solution for\ndownsizing convolution networks and making a more efficient SR network. To\nmeasure the enhanced image quality, for the first time in this area of\nsuper-resolution and image enhancement, we use VIQET MOS score which reflects\nhuman visual quality more accurately than the conventional MSE measure.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 00:30:36 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Park", "Saem", ""], ["Kwak", "Nojun", ""]]}, {"id": "1907.11346", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee", "title": "Camera Distance-aware Top-down Approach for 3D Multi-person Pose\n  Estimation from a Single RGB Image", "comments": "Published at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although significant improvement has been achieved recently in 3D human pose\nestimation, most of the previous methods only treat a single-person case. In\nthis work, we firstly propose a fully learning-based, camera distance-aware\ntop-down approach for 3D multi-person pose estimation from a single RGB image.\nThe pipeline of the proposed system consists of human detection, absolute 3D\nhuman root localization, and root-relative 3D single-person pose estimation\nmodules. Our system achieves comparable results with the state-of-the-art 3D\nsingle-person pose estimation models without any groundtruth information and\nsignificantly outperforms previous 3D multi-person pose estimation methods on\npublicly available datasets.\n  The code is available in https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE ,\nhttps://github.com/mks0601/3DMPPE_POSENET_RELEASE.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 01:02:45 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 06:39:36 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Chang", "Ju Yong", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1907.11350", "submitter": "Qiang Zhai", "authors": "Qiang Zhai", "title": "Learning Quintuplet Loss for Large-scale Visual Geo-Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the maturity of Artificial Intelligence (AI) technology, Large Scale\nVisual Geo-Localization (LSVGL) is increasingly important in urban computing,\nwhere the task is to accurately and efficiently recognize the geo-location of a\ngiven query image. The main challenge of LSVGL faced by many experiments due to\nthe appearance of real-word places may differ in various ways. While\nperspective deviation almost inevitably exists between training images and\nquery images because of the arbitrary perspective. To cope with this situation,\nin this paper, we in-depth analyze the limitation of triplet loss which is the\nmost commonly used metric learning loss in state-of-the-art LSVGL framework,\nand propose a new QUInTuplet Loss (QUITLoss) by embedding all the potential\npositive samples to the primitive triplet loss. Extensive experiments have been\nconducted to verify the effectiveness of the proposed approach and the results\ndemonstrate that our new loss can enhance various LSVGL methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 01:08:30 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 02:42:02 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Zhai", "Qiang", ""]]}, {"id": "1907.11357", "submitter": "Gen Li", "authors": "Gen Li, Inyoung Yun, Jonghyun Kim, Joongkyu Kim", "title": "DABNet: Depth-wise Asymmetric Bottleneck for Real-time Semantic\n  Segmentation", "comments": "Accepted to BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a pixel-level prediction task, semantic segmentation needs large\ncomputational cost with enormous parameters to obtain high performance.\nRecently, due to the increasing demand for autonomous systems and robots, it is\nsignificant to make a tradeoff between accuracy and inference speed. In this\npaper, we propose a novel Depthwise Asymmetric Bottleneck (DAB) module to\naddress this dilemma, which efficiently adopts depth-wise asymmetric\nconvolution and dilated convolution to build a bottleneck structure. Based on\nthe DAB module, we design a Depth-wise Asymmetric Bottleneck Network (DABNet)\nespecially for real-time semantic segmentation, which creates sufficient\nreceptive field and densely utilizes the contextual information. Experiments on\nCityscapes and CamVid datasets demonstrate that the proposed DABNet achieves a\nbalance between speed and precision. Specifically, without any pretrained model\nand postprocessing, it achieves 70.1% Mean IoU on the Cityscapes test dataset\nwith only 0.76 million parameters and a speed of 104 FPS on a single GTX 1080Ti\ncard.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 01:50:31 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 01:29:58 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Li", "Gen", ""], ["Yun", "Inyoung", ""], ["Kim", "Jonghyun", ""], ["Kim", "Joongkyu", ""]]}, {"id": "1907.11366", "submitter": "Dong Li", "authors": "Zhulin Zhang, Dong Li, Jinhua Wu, Yunda Sun, and Li Zhang", "title": "MVB: A Large-Scale Dataset for Baggage Re-Identification and Merged\n  Siamese Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel dataset named MVB (Multi View Baggage) for\nbaggage ReID task which has some essential differences from person ReID. The\nfeatures of MVB are three-fold. First, MVB is the first publicly released\nlarge-scale dataset that contains 4519 baggage identities and 22660 annotated\nbaggage images as well as its surface material labels. Second, all baggage\nimages are captured by specially-designed multi-view camera system to handle\npose variation and occlusion, in order to obtain the 3D information of baggage\nsurface as complete as possible. Third, MVB has remarkable inter-class\nsimilarity and intra-class dissimilarity, considering the fact that baggage\nmight have very similar appearance while the data is collected in two real\nairport environments, where imaging factors varies significantly from each\nother. Moreover, we proposed a merged Siamese network as baseline model and\nevaluated its performance. Experiments and case study are conducted on MVB.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 02:47:43 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Zhang", "Zhulin", ""], ["Li", "Dong", ""], ["Wu", "Jinhua", ""], ["Sun", "Yunda", ""], ["Zhang", "Li", ""]]}, {"id": "1907.11371", "submitter": "Ozan Tezcan", "authors": "M. Ozan Tezcan, Prakash Ishwar, Janusz Konrad", "title": "BSUV-Net: A Fully-Convolutional Neural Network for Background\n  Subtraction of Unseen Videos", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background subtraction is a basic task in computer vision and video\nprocessing often applied as a pre-processing step for object tracking, people\nrecognition, etc. Recently, a number of successful background-subtraction\nalgorithms have been proposed, however nearly all of the top-performing ones\nare supervised. Crucially, their success relies upon the availability of some\nannotated frames of the test video during training. Consequently, their\nperformance on completely \"unseen\" videos is undocumented in the literature. In\nthis work, we propose a new, supervised, background-subtraction algorithm for\nunseen videos (BSUV-Net) based on a fully-convolutional neural network. The\ninput to our network consists of the current frame and two background frames\ncaptured at different time scales along with their semantic segmentation maps.\nIn order to reduce the chance of overfitting, we also introduce a new\ndata-augmentation technique which mitigates the impact of illumination\ndifference between the background frames and the current frame. On the\nCDNet-2014 dataset, BSUV-Net outperforms state-of-the-art algorithms evaluated\non unseen videos in terms of several metrics including F-measure, recall and\nprecision.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 03:05:00 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 16:30:38 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Tezcan", "M. Ozan", ""], ["Ishwar", "Prakash", ""], ["Konrad", "Janusz", ""]]}, {"id": "1907.11375", "submitter": "Pei Yan", "authors": "Pei Yan, Yihua Tan, Yuan Xiao, Yuan Tai, Cai Wen", "title": "Unsupervised Learning Framework of Interest Point Via Properties\n  Optimization", "comments": "16 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an entirely unsupervised interest point training\nframework by jointly learning detector and descriptor, which takes an image as\ninput and outputs a probability and a description for every image point. The\nobjective of the training framework is formulated as joint probability\ndistribution of the properties of the extracted points. The essential\nproperties are selected as sparsity, repeatability and discriminability which\nare formulated by the probabilities. To maximize the objective efficiently,\nlatent variable is introduced to represent the probability of that a point\nsatisfies the required properties. Therefore, original maximization can be\noptimized with Expectation Maximization algorithm (EM). Considering high\ncomputation cost of EM on large scale image set, we implement the optimization\nprocess with an efficient strategy as Mini-Batch approximation of EM (MBEM). In\nthe experiments both detector and descriptor are instantiated with fully\nconvolutional network which is named as Property Network (PN). The experiments\ndemonstrate that PN outperforms state-of-the-art methods on a number of image\nmatching benchmarks without need of retraining. PN also reveals that the\nproposed training framework has high flexibility to adapt to diverse types of\nscenes.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 03:31:27 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Yan", "Pei", ""], ["Tan", "Yihua", ""], ["Xiao", "Yuan", ""], ["Tai", "Yuan", ""], ["Wen", "Cai", ""]]}, {"id": "1907.11384", "submitter": "Xiaojiang Peng", "authors": "Qing Li, Xiaojiang Peng, Liangliang Cao, Wenbin Du, Hao Xing, Yu Qiao", "title": "Product Image Recognition with Guidance Learning and Noisy Supervision", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers recognizing products from daily photos, which is an\nimportant problem in real-world applications but also challenging due to\nbackground clutters, category diversities, noisy labels, etc. We address this\nproblem by two contributions. First, we introduce a novel large-scale product\nimage dataset, termed as Product-90. Instead of collecting product images by\nlabor-and time-intensive image capturing, we take advantage of the web and\ndownload images from the reviews of several e-commerce websites where the\nimages are casually captured by consumers. Labels are assigned automatically by\nthe categories of e-commerce websites. Totally the Product-90 consists of more\nthan 140K images with 90 categories. Due to the fact that consumers may upload\nunrelated images, it is inevitable that our Product-90 introduces noisy labels.\nAs the second contribution, we develop a simple yet efficient \\textit{guidance\nlearning} (GL) method for training convolutional neural networks (CNNs) with\nnoisy supervision. The GL method first trains an initial teacher network with\nthe full noisy dataset, and then trains a target/student network with both\nlarge-scale noisy set and small manually-verified clean set in a multi-task\nmanner. Specifically, in the stage of student network training, the large-scale\nnoisy data is supervised by its guidance knowledge which is the combination of\nits given noisy label and the soften label from the teacher network. We conduct\nextensive experiments on our Products-90 and public datasets, namely Food101,\nFood-101N, and Clothing1M. Our guidance learning method achieves performance\nsuperior to state-of-the-art methods on these datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 05:13:13 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Li", "Qing", ""], ["Peng", "Xiaojiang", ""], ["Cao", "Liangliang", ""], ["Du", "Wenbin", ""], ["Xing", "Hao", ""], ["Qiao", "Yu", ""]]}, {"id": "1907.11392", "submitter": "Jiechao Ma", "authors": "Jiechao Ma, Rongguo Zhang", "title": "Automatic Calcium Scoring in Cardiac and Chest CT Using DenseRAUnet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular disease (CVD) is a common and strong threat to human beings,\nfeaturing high prevalence, disability and mortality. The amount of coronary\nartery calcification (CAC) is an effective factor for CVD risk evaluation.\nConventionally, CAC is quantified using ECG-synchronized cardiac CT but rarely\nfrom general chest CT scans. However, compared with ECG-synchronized cardiac\nCT, chest CT is more prevalent and economical in clinical practice. To address\nthis, we propose an automatic method based on Dense U-Net to segment coronary\ncalcium pixels on both types of CT scans. Our contribution is two-fold. First,\nwe propose a novel network called DenseRAUnet, which takes advantage of Dense\nU-net, ResNet and atrous convolutions. We prove the robustness and\ngeneralizability of our model by training it exclusively on chest CT while test\non both types of CT scans. Second, we design a loss function combining\nbootstrap with IoU function to balance foreground and background classes.\nDenseRAUnet is trained in a 2.5D fashion and tested on a private dataset\nconsisting of 144 scans. Results show an F1-score of 0.75, with 0.83 accuracy\nof predicting cardiovascular disease risk.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 06:30:44 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Ma", "Jiechao", ""], ["Zhang", "Rongguo", ""]]}, {"id": "1907.11394", "submitter": "Kaite Xiang", "authors": "Kaite Xiang, Kaiwei Wang and Kailun Yang", "title": "A Comparative Study of High-Recall Real-Time Semantic Segmentation Based\n  on Swift Factorized Network", "comments": "14 pages, 11figures, SPIE Security + Defence 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Segmentation (SS) is the task to assign a semantic label to each\npixel of the observed images, which is of crucial significance for autonomous\nvehicles, navigation assistance systems for the visually impaired, and\naugmented reality devices. However, there is still a long way for SS to be put\ninto practice as there are two essential challenges that need to be addressed:\nefficiency and evaluation criterions for practical application. For specific\napplication scenarios, different criterions need to be adopted. Recall rate is\nan important criterion for many tasks like autonomous vehicles. For autonomous\nvehicles, we need to focus on the detection of the traffic objects like cars,\nbuses, and pedestrians, which should be detected with high recall rates. In\nother words, it is preferable to detect it wrongly than miss it, because the\nother traffic objects will be dangerous if the algorithm miss them and segment\nthem as safe roadways. In this paper, our main goal is to explore possible\nmethods to attain high recall rate. Firstly, we propose a real-time SS network\nnamed Swift Factorized Network (SFN). The proposed network is adapted from\nSwiftNet, whose structure is a typical U-shape structure with lateral\nconnections. Inspired by ERFNet and Global convolution Networks (GCNet), we\npropose two different blocks to enlarge valid receptive field. They do not take\nup too much calculation resources, but significantly enhance the performance\ncompared with the baseline network. Secondly, we explore three ways to achieve\nhigher recall rate, i.e. loss function, classifier and decision rules. We\nperform a comprehensive set of experiments on state-of-the-art datasets\nincluding CamVid and Cityscapes. We demonstrate that our SS convolutional\nneural networks reach excellent performance. Furthermore, we make a detailed\nanalysis and comparison of the three proposed methods on the promotion of\nrecall rate.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 06:36:18 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Xiang", "Kaite", ""], ["Wang", "Kaiwei", ""], ["Yang", "Kailun", ""]]}, {"id": "1907.11397", "submitter": "Xiaofeng Xu", "authors": "Xiaofeng Xu, Ivor W. Tsang, and Chuancai Liu", "title": "Improving Generalization via Attribute Selection on Out-of-the-box Data", "comments": "Accepted by Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize unseen objects (test classes)\ngiven some other seen objects (training classes), by sharing information of\nattributes between different objects. Attributes are artificially annotated for\nobjects and treated equally in recent ZSL tasks. However, some inferior\nattributes with poor predictability or poor discriminability may have negative\nimpacts on the ZSL system performance. This paper first derives a\ngeneralization error bound for ZSL tasks. Our theoretical analysis verifies\nthat selecting the subset of key attributes can improve the generalization\nperformance of the original ZSL model, which utilizes all the attributes.\nUnfortunately, previous attribute selection methods are conducted based on the\nseen data, and their selected attributes have poor generalization capability to\nthe unseen data, which is unavailable in the training stage of ZSL tasks.\nInspired by learning from pseudo relevance feedback, this paper introduces the\nout-of-the-box data, which is pseudo data generated by an attribute-guided\ngenerative model, to mimic the unseen data. After that, we present an iterative\nattribute selection (IAS) strategy which iteratively selects key attributes\nbased on the out-of-the-box data. Since the distribution of the generated\nout-of-the-box data is similar to the test data, the key attributes selected by\nIAS can be effectively generalized to test data. Extensive experiments\ndemonstrate that IAS can significantly improve existing attribute-based ZSL\nmethods and achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 06:57:27 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 05:28:53 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Xu", "Xiaofeng", ""], ["Tsang", "Ivor W.", ""], ["Liu", "Chuancai", ""]]}, {"id": "1907.11418", "submitter": "Defa Zhu", "authors": "Defa Zhu, Si Liu, Wentao Jiang, Chen Gao, Tianyi Wu, Qaingchang Wang,\n  Guodong Guo", "title": "UGAN: Untraceable GAN for Multi-Domain Face Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-domain image-to-image translation is a challenging task where the\ngoal is to translate an image into multiple different domains. The target-only\ncharacteristics are desired for translated images, while the source-only\ncharacteristics should be erased. However, recent methods often suffer from\nretaining the characteristics of the source domain, which are incompatible with\nthe target domain. To address this issue, we propose a method called\nUntraceable GAN, which has a novel source classifier to differentiate which\ndomain an image is translated from, and determines whether the translated image\nstill retains the characteristics of the source domain. Furthermore, we take\nthe prototype of the target domain as the guidance for the translator to\neffectively synthesize the target-only characteristics. The translator is\nlearned to synthesize the target-only characteristics and make the source\ndomain untraceable for the discriminator, so that the source-only\ncharacteristics are erased. Finally, extensive experiments on three face\nediting tasks, including face aging, makeup, and expression editing, show that\nthe proposed UGAN can produce superior results over the state-of-the-art\nmodels. The source code will be released.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 08:05:13 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 02:23:45 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Zhu", "Defa", ""], ["Liu", "Si", ""], ["Jiang", "Wentao", ""], ["Gao", "Chen", ""], ["Wu", "Tianyi", ""], ["Wang", "Qaingchang", ""], ["Guo", "Guodong", ""]]}, {"id": "1907.11432", "submitter": "Kumara Kahatapitiya", "authors": "Kumara Kahatapitiya and Ranga Rodrigo", "title": "Exploiting the Redundancy in Convolutional Filters for Parameter\n  Reduction", "comments": "Accepted to be published in Proceedings of IEEE Winter Conference on\n  Applications of Computer Vision (WACV) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have achieved state-of-the-art\nperformance in many computer vision tasks over the years. However, this comes\nat the cost of heavy computation and memory intensive network designs,\nsuggesting potential improvements in efficiency. Convolutional layers of CNNs\npartly account for such an inefficiency, as they are known to learn redundant\nfeatures. In this work, we exploit this redundancy, observing it as the\ncorrelation between convolutional filters of a layer, and propose an\nalternative approach to reproduce it efficiently. The proposed 'LinearConv'\nlayer learns a set of orthogonal filters, and a set of coefficients that\nlinearly combines them to introduce a controlled redundancy. We introduce a\ncorrelation-based regularization loss to achieve such flexibility over\nredundancy, and control the number of parameters in turn. This is designed as a\nplug-and-play layer to conveniently replace a conventional convolutional layer,\nwithout any additional changes required in the network architecture or the\nhyperparameter settings. Our experiments verify that LinearConv models achieve\na performance on-par with their counterparts, with almost a 50% reduction in\nparameters on average, and the same computational requirement and speed at\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 08:39:31 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 05:45:08 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 16:53:03 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kahatapitiya", "Kumara", ""], ["Rodrigo", "Ranga", ""]]}, {"id": "1907.11436", "submitter": "Sebastian Kleinschmidt", "authors": "Sebastian P. Kleinschmidt and Bernardo Wagner", "title": "Semantic Deep Intermodal Feature Transfer: Transferring Feature\n  Descriptors Between Imaging Modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under difficult environmental conditions, the view of RGB cameras may be\nrestricted by fog, dust or difficult lighting situations. Because thermal\ncameras visualize thermal radiation, they are not subject to the same\nlimitations as RGB cameras. However, because RGB and thermal imaging differ\nsignificantly in appearance, common, state-of-the-art feature descriptors are\nunsuitable for intermodal feature matching between these imaging modalities. As\na consequence, visual maps created with an RGB camera can currently not be used\nfor localization using a thermal camera. In this paper, we introduce the\nSemantic Deep Intermodal Feature Transfer (Se-DIFT), an approach for\ntransferring image feature descriptors from the visual to the thermal spectrum\nand vice versa. For this purpose, we predict potential feature appearance in\nvarying imaging modalities using a deep convolutional encoder-decoder\narchitecture in combination with a global feature vector. Since the\nrepresentation of a thermal image is not only affected by features which can be\nextracted from an RGB image, we introduce the global feature vector which\naugments the auto encoder's coding. The global feature vector contains\nadditional information about the thermal history of a scene which is\nautomatically extracted from external data sources. By augmenting the encoder's\ncoding, we decrease the L1 error of the prediction by more than 7% compared to\nthe prediction of a traditional U-Net architecture. To evaluate our approach,\nwe match image feature descriptors detected in RGB and thermal images using\nSe-DIFT. Subsequently, we make a competitive comparison on the intermodal\ntransferability of SIFT, SURF, and ORB features using our approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 08:42:38 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Kleinschmidt", "Sebastian P.", ""], ["Wagner", "Bernardo", ""]]}, {"id": "1907.11440", "submitter": "Junhyuk Hyun", "authors": "Junhyuk Hyun, Hongje Seong and Euntai Kim", "title": "Universal Pooling -- A New Pooling Method for Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling is one of the main elements in convolutional neural networks. The\npooling reduces the size of the feature map, enabling training and testing with\na limited amount of computation. This paper proposes a new pooling method named\nuniversal pooling. Unlike the existing pooling methods such as average pooling,\nmax pooling, and stride pooling with fixed pooling function, universal pooling\ngenerates any pooling function, depending on a given problem and dataset.\nUniversal pooling was inspired by attention methods and can be considered as a\nchannel-wise form of local spatial attention. Universal pooling is trained\njointly with the main network and it is shown that it includes the existing\npooling methods. Finally, when applied to two benchmark problems, the proposed\nmethod outperformed the existing pooling methods and performed with the\nexpected diversity, adapting to the given problem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 09:00:00 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Hyun", "Junhyuk", ""], ["Seong", "Hongje", ""], ["Kim", "Euntai", ""]]}, {"id": "1907.11449", "submitter": "Nicolas Boizot", "authors": "Nicolas Boizot (LIS), Ludovic Sacchelli", "title": "A bisector line field approach to interpolation of orientation fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to the problem of global reconstruction of an\norientation field. The method is based on a geometric model called \"bisector\nline fields\", which maps a pair of vector fields to an orientation field,\neffectively generalizing the notion of doubling phase vector fields. Endowed\nwith a well chosen energy minimization problem, we provide a polynomial\ninterpolation of a target orientation field while bypassing the doubling phase\nstep. The procedure is then illustrated with examples from fingerprint\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 09:18:36 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 09:12:14 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 16:34:41 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 08:23:48 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Boizot", "Nicolas", "", "LIS"], ["Sacchelli", "Ludovic", ""]]}, {"id": "1907.11454", "submitter": "Isabel Funke", "authors": "Isabel Funke, Sebastian Bodenstedt, Florian Oehme, Felix von\n  Bechtolsheim, J\\\"urgen Weitz, and Stefanie Speidel", "title": "Using 3D Convolutional Neural Networks to Learn Spatiotemporal Features\n  for Automatic Surgical Gesture Recognition in Video", "comments": "Accepted at MICCAI 2019. Source code will be made available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically recognizing surgical gestures is a crucial step towards a\nthorough understanding of surgical skill. Possible areas of application include\nautomatic skill assessment, intra-operative monitoring of critical surgical\nsteps, and semi-automation of surgical tasks. Solutions that rely only on the\nlaparoscopic video and do not require additional sensor hardware are especially\nattractive as they can be implemented at low cost in many scenarios. However,\nsurgical gesture recognition based only on video is a challenging problem that\nrequires effective means to extract both visual and temporal information from\nthe video. Previous approaches mainly rely on frame-wise feature extractors,\neither handcrafted or learned, which fail to capture the dynamics in surgical\nvideo. To address this issue, we propose to use a 3D Convolutional Neural\nNetwork (CNN) to learn spatiotemporal features from consecutive video frames.\nWe evaluate our approach on recordings of robot-assisted suturing on a\nbench-top model, which are taken from the publicly available JIGSAWS dataset.\nOur approach achieves high frame-wise surgical gesture recognition accuracies\nof more than 84%, outperforming comparable models that either extract only\nspatial features or model spatial and low-level temporal information\nseparately. For the first time, these results demonstrate the benefit of\nspatiotemporal CNNs for video-based surgical gesture recognition.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 09:34:09 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Funke", "Isabel", ""], ["Bodenstedt", "Sebastian", ""], ["Oehme", "Florian", ""], ["von Bechtolsheim", "Felix", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""]]}, {"id": "1907.11458", "submitter": "Ruize Han", "authors": "Ruize Han, Yujun Zhang, Wei Feng, Chenxing Gong, Xiaoyu Zhang, Jiewen\n  Zhao, Liang Wan, Song Wang", "title": "Multiple Human Association between Top and Horizontal Views by Matching\n  Subjects' Spatial Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video surveillance can be significantly enhanced by using both top-view data,\ne.g., those from drone-mounted cameras in the air, and horizontal-view data,\ne.g., those from wearable cameras on the ground. Collaborative analysis of\ndifferent-view data can facilitate various kinds of applications, such as human\ntracking, person identification, and human activity recognition. However, for\nsuch collaborative analysis, the first step is to associate people, referred to\nas subjects in this paper, across these two views. This is a very challenging\nproblem due to large human-appearance difference between top and horizontal\nviews. In this paper, we present a new approach to address this problem by\nexploring and matching the subjects' spatial distributions between the two\nviews. More specifically, on the top-view image, we model and match subjects'\nrelative positions to the horizontal-view camera in both views and define a\nmatching cost to decide the actual location of horizontal-view camera and its\nview angle in the top-view image. We collect a new dataset consisting of\ntop-view and horizontal-view image pairs for performance evaluation and the\nexperimental results show the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 09:47:17 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Han", "Ruize", ""], ["Zhang", "Yujun", ""], ["Feng", "Wei", ""], ["Gong", "Chenxing", ""], ["Zhang", "Xiaoyu", ""], ["Zhao", "Jiewen", ""], ["Wan", "Liang", ""], ["Wang", "Song", ""]]}, {"id": "1907.11474", "submitter": "Wenxuan Tu", "authors": "Bin Jiang, Wenxuan Tu, Chao Yang, Junsong Yuan", "title": "Context-Integrated and Feature-Refined Network for Lightweight Object\n  Parsing", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2978583", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation for lightweight object parsing is a very challenging\ntask, because both accuracy and efficiency (e.g., execution speed, memory\nfootprint or computational complexity) should all be taken into account.\nHowever, most previous works pay too much attention to one-sided perspective,\neither accuracy or speed, and ignore others, which poses a great limitation to\nactual demands of intelligent devices. To tackle this dilemma, we propose a\nnovel lightweight architecture named Context-Integrated and Feature-Refined\nNetwork (CIFReNet). The core components of CIFReNet are the Long-skip\nRefinement Module (LRM) and the Multi-scale Context Integration Module (MCIM).\nThe LRM is designed to ease the propagation of spatial information between\nlow-level and high-level stages. Furthermore, channel attention mechanism is\nintroduced into the process of long-skip learning to boost the quality of\nlow-level feature refinement. Meanwhile, the MCIM consists of three cascaded\nDense Semantic Pyramid (DSP) blocks with image-level features, which is\npresented to encode multiple context information and enlarge the field of view.\nSpecifically, the proposed DSP block exploits a dense feature sampling strategy\nto enhance the information representations without significantly increasing the\ncomputation cost. Comprehensive experiments are conducted on three benchmark\ndatasets for object parsing including Cityscapes, CamVid, and Helen. As\nindicated, the proposed method reaches a better trade-off between accuracy and\nefficiency compared with the other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 10:50:30 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 07:08:36 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 04:12:16 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Jiang", "Bin", ""], ["Tu", "Wenxuan", ""], ["Yang", "Chao", ""], ["Yuan", "Junsong", ""]]}, {"id": "1907.11475", "submitter": "Josip \\v{S}ari\\'c", "authors": "Josip \\v{S}ari\\'c, Marin Or\\v{s}i\\'c, Ton\\'ci Antunovi\\'c, Sacha\n  Vra\\v{z}i\\'c, Sini\\v{s}a \\v{S}egvi\\'c", "title": "Single Level Feature-to-Feature Forecasting with Deformable Convolutions", "comments": "Accepted to German Conference on Pattern Recognition 2019. 19 pages,\n  8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future anticipation is of vital importance in autonomous driving and other\ndecision-making systems. We present a method to anticipate semantic\nsegmentation of future frames in driving scenarios based on feature-to-feature\nforecasting. Our method is based on a semantic segmentation model without\nlateral connections within the upsampling path. Such design ensures that the\nforecasting addresses only the most abstract features on a very coarse\nresolution. We further propose to express feature-to-feature forecasting with\ndeformable convolutions. This increases the modelling power due to being able\nto represent different motion patterns within a single feature map. Experiments\nshow that our models with deformable convolutions outperform their regular and\ndilated counterparts while minimally increasing the number of parameters. Our\nmethod achieves state of the art performance on the Cityscapes validation set\nwhen forecasting nine timesteps into the future.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 10:51:24 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["\u0160ari\u0107", "Josip", ""], ["Or\u0161i\u0107", "Marin", ""], ["Antunovi\u0107", "Ton\u0107i", ""], ["Vra\u017ei\u0107", "Sacha", ""], ["\u0160egvi\u0107", "Sini\u0161a", ""]]}, {"id": "1907.11483", "submitter": "Fei Yu", "authors": "Fei Yu, Jie Zhao, Yanjun Gong, Zhi Wang, Yuxi Li, Fan Yang, Bin Dong,\n  Quanzheng Li, Li Zhang", "title": "Annotation-Free Cardiac Vessel Segmentation via Knowledge Transfer from\n  Retinal Images", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting coronary arteries is challenging, as classic unsupervised methods\nfail to produce satisfactory results and modern supervised learning (deep\nlearning) requires manual annotation which is often time-consuming and can some\ntime be infeasible. To solve this problem, we propose a knowledge transfer\nbased shape-consistent generative adversarial network (SC-GAN), which is an\nannotation-free approach that uses the knowledge from publicly available\nannotated fundus dataset to segment coronary arteries. The proposed network is\ntrained in an end-to-end fashion, generating and segmenting synthetic images\nthat maintain the background of coronary angiography and preserve the vascular\nstructures of retinal vessels and coronary arteries. We train and evaluate the\nproposed model on a dataset of 1092 digital subtraction angiography images, and\nexperiments demonstrate the supreme accuracy of the proposed method on coronary\narteries segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 11:08:32 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Yu", "Fei", ""], ["Zhao", "Jie", ""], ["Gong", "Yanjun", ""], ["Wang", "Zhi", ""], ["Li", "Yuxi", ""], ["Yang", "Fan", ""], ["Dong", "Bin", ""], ["Li", "Quanzheng", ""], ["Zhang", "Li", ""]]}, {"id": "1907.11484", "submitter": "Fei Yu", "authors": "Rongchang Xie, Fei Yu, Jiachao Wang, Yizhou Wang, Li Zhang", "title": "Multi-level Domain Adaptive learning for Cross-Domain Detection", "comments": "Accepted to the TASK-CV workshop at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, object detection has shown impressive results using\nsupervised deep learning, but it remains challenging in a cross-domain\nenvironment. The variations of illumination, style, scale, and appearance in\ndifferent domains can seriously affect the performance of detection models.\nPrevious works use adversarial training to align global features across the\ndomain shift and to achieve image information transfer. However, such methods\ndo not effectively match the distribution of local features, resulting in\nlimited improvement in cross-domain object detection. To solve this problem, we\npropose a multi-level domain adaptive model to simultaneously align the\ndistributions of local-level features and global-level features. We evaluate\nour method with multiple experiments, including adverse weather adaptation,\nsynthetic data adaptation, and cross camera adaptation. In most object\ncategories, the proposed method achieves superior performance against\nstate-of-the-art techniques, which demonstrates the effectiveness and\nrobustness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 11:09:24 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 05:32:19 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Xie", "Rongchang", ""], ["Yu", "Fei", ""], ["Wang", "Jiachao", ""], ["Wang", "Yizhou", ""], ["Zhang", "Li", ""]]}, {"id": "1907.11496", "submitter": "Xin Wang", "authors": "Xin Wang, Bo Wu, Yun Ye, Yueqi Zhong", "title": "Outfit Compatibility Prediction and Diagnosis with Multi-Layered\n  Comparison Network", "comments": "9 pages, 6 figures, Proceedings of the 27th ACM International\n  Conference on Multimedia", "journal-ref": null, "doi": "10.1145/3343031.3350909", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works about fashion outfit compatibility focus on predicting the\noverall compatibility of a set of fashion items with their information from\ndifferent modalities. However, there are few works explore how to explain the\nprediction, which limits the persuasiveness and effectiveness of the model. In\nthis work, we propose an approach to not only predict but also diagnose the\noutfit compatibility. We introduce an end-to-end framework for this goal, which\nfeatures for: (1) The overall compatibility is learned from all type-specified\npairwise similarities between items, and the backpropagation gradients are used\nto diagnose the incompatible factors. (2) We leverage the hierarchy of CNN and\ncompare the features at different layers to take into account the\ncompatibilities of different aspects from the low level (such as color,\ntexture) to the high level (such as style). To support the proposed method, we\nbuild a new type-specified outfit dataset named Polyvore-T based on Polyvore\ndataset. We compare our method with the prior state-of-the-art in two tasks:\noutfit compatibility prediction and fill-in-the-blank. Experiments show that\nour approach has advantages in both prediction performance and diagnosis\nability.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 11:39:15 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 03:56:30 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Wang", "Xin", ""], ["Wu", "Bo", ""], ["Ye", "Yun", ""], ["Zhong", "Yueqi", ""]]}, {"id": "1907.11503", "submitter": "Dr. Mohammed Javed", "authors": "Bulla Rajesh and Mohammed Javed and Ratnesh and Shubham Srivastava", "title": "DCT-CompCNN: A Novel Image Classification Network Using JPEG Compressed\n  DCT Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of Convolutional Neural Network (CNN) in the field of Image\nProcessing and Computer Vision has motivated researchers and industrialist\nexperts across the globe to solve different challenges with high accuracy. The\nsimplest way to train a CNN classifier is to directly feed the original RGB\npixels images into the network. However, if we intend to classify images\ndirectly with its compressed data, the same approach may not work better, like\nin case of JPEG compressed images. This research paper investigates the issues\nof modifying the input representation of the JPEG compressed data, and then\nfeeding into the CNN. The architecture is termed as DCT-CompCNN. This novel\napproach has shown that CNNs can also be trained with JPEG compressed DCT\ncoefficients, and subsequently can produce a better performance in comparison\nwith the conventional CNN approach. The efficiency of the modified input\nrepresentation is tested with the existing ResNet-50 architecture and the\nproposed DCT-CompCNN architecture on a public image classification datasets\nlike Dog Vs Cat and CIFAR-10 datasets, reporting a better performance\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 12:01:21 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Rajesh", "Bulla", ""], ["Javed", "Mohammed", ""], ["Ratnesh", "", ""], ["Srivastava", "Shubham", ""]]}, {"id": "1907.11510", "submitter": "Fabien Ringeval", "authors": "Fabien Ringeval, Bj\\\"orn Schuller, Michel Valstar, NIcholas Cummins,\n  Roddy Cowie, Leili Tavabi, Maximilian Schmitt, Sina Alisamir, Shahin\n  Amiriparian, Eva-Maria Messner, Siyang Song, Shuo Liu, Ziping Zhao, Adria\n  Mallol-Ragolta, Zhao Ren, Mohammad Soleymani, Maja Pantic", "title": "AVEC 2019 Workshop and Challenge: State-of-Mind, Detecting Depression\n  with AI, and Cross-Cultural Affect Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Audio/Visual Emotion Challenge and Workshop (AVEC 2019) \"State-of-Mind,\nDetecting Depression with AI, and Cross-cultural Affect Recognition\" is the\nninth competition event aimed at the comparison of multimedia processing and\nmachine learning methods for automatic audiovisual health and emotion analysis,\nwith all participants competing strictly under the same conditions. The goal of\nthe Challenge is to provide a common benchmark test set for multimodal\ninformation processing and to bring together the health and emotion recognition\ncommunities, as well as the audiovisual processing communities, to compare the\nrelative merits of various approaches to health and emotion recognition from\nreal-life data. This paper presents the major novelties introduced this year,\nthe challenge guidelines, the data used, and the performance of the baseline\nsystems on the three proposed tasks: state-of-mind recognition, depression\nassessment with AI, and cross-cultural affect sensing, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 13:41:42 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Ringeval", "Fabien", ""], ["Schuller", "Bj\u00f6rn", ""], ["Valstar", "Michel", ""], ["Cummins", "NIcholas", ""], ["Cowie", "Roddy", ""], ["Tavabi", "Leili", ""], ["Schmitt", "Maximilian", ""], ["Alisamir", "Sina", ""], ["Amiriparian", "Shahin", ""], ["Messner", "Eva-Maria", ""], ["Song", "Siyang", ""], ["Liu", "Shuo", ""], ["Zhao", "Ziping", ""], ["Mallol-Ragolta", "Adria", ""], ["Ren", "Zhao", ""], ["Soleymani", "Mohammad", ""], ["Pantic", "Maja", ""]]}, {"id": "1907.11519", "submitter": "Dumindu Tissera", "authors": "Dumindu Tissera, Kumara Kahatapitiya, Rukshan Wijesinghe, Subha\n  Fernando, Ranga Rodrigo", "title": "Context-Aware Multipath Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making a single network effectively address diverse contexts---learning the\nvariations within a dataset or multiple datasets---is an intriguing step\ntowards achieving generalized intelligence. Existing approaches of deepening,\nwidening, and assembling networks are not cost effective in general. In view of\nthis, networks which can allocate resources according to the context of the\ninput and regulate flow of information across the network are effective. In\nthis paper, we present Context-Aware Multipath Network (CAMNet), a multi-path\nneural network with data-dependant routing between parallel tensors. We show\nthat our model performs as a generalized model capturing variations in\nindividual datasets and multiple different datasets, both simultaneously and\nsequentially. CAMNet surpasses the performance of classification and\npixel-labeling tasks in comparison with the equivalent single-path, multi-path,\nand deeper single-path networks, considering datasets individually,\nsequentially, and in combination. The data-dependent routing between tensors in\nCAMNet enables the model to control the flow of information end-to-end,\ndeciding which resources to be common or domain-specific.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 12:34:31 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Tissera", "Dumindu", ""], ["Kahatapitiya", "Kumara", ""], ["Wijesinghe", "Rukshan", ""], ["Fernando", "Subha", ""], ["Rodrigo", "Ranga", ""]]}, {"id": "1907.11529", "submitter": "Rosaura VidalMata", "authors": "Sreya Banerjee, Rosaura G. VidalMata, Zhangyang Wang, and Walter J.\n  Scheirer", "title": "Report on UG^2+ Challenge Track 1: Assessing Algorithms to Improve Video\n  Object Detection and Classification from Unconstrained Mobility Platforms", "comments": "Supplemental material: http://bit.ly/UG2Supp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we effectively engineer a computer vision system that is able to\ninterpret videos from unconstrained mobility platforms like UAVs? One promising\noption is to make use of image restoration and enhancement algorithms from the\narea of computational photography to improve the quality of the underlying\nframes in a way that also improves automatic visual recognition. Along these\nlines, exploratory work is needed to find out which image pre-processing\nalgorithms, in combination with the strongest features and supervised machine\nlearning approaches, are good candidates for difficult scenarios like motion\nblur, weather, and mis-focus -- all common artifacts in UAV acquired images.\nThis paper summarizes the protocols and results of Track 1 of the UG^2+\nChallenge held in conjunction with IEEE/CVF CVPR 2019. The challenge looked at\ntwo separate problems: (1) object detection improvement in video, and (2)\nobject classification improvement in video. The challenge made use of the UG^2\n(UAV, Glider, Ground) dataset, which is an established benchmark for assessing\nthe interplay between image restoration and enhancement and visual recognition.\n16 algorithms were submitted by academic and corporate teams, and a detailed\nanalysis of how they performed on each challenge problem is reported here.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 12:40:51 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 22:13:44 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 18:33:35 GMT"}, {"version": "v4", "created": "Fri, 20 Nov 2020 04:16:10 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Banerjee", "Sreya", ""], ["VidalMata", "Rosaura G.", ""], ["Wang", "Zhangyang", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1907.11539", "submitter": "Yaroslava Lochman", "authors": "James Pritts, Zuzana Kukelova, Viktor Larsson, Yaroslava Lochman,\n  Ond\\v{r}ej Chum", "title": "Minimal Solvers for Rectifying from Radially-Distorted Scales and Change\n  of Scales", "comments": "arXiv admin note: text overlap with arXiv:1807.06110", "journal-ref": "International Journal of Computer Vision (2020) 1-19", "doi": "10.1007/s11263-019-01216-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first minimal solvers that jointly estimate lens\ndistortion and affine rectification from the image of rigidly-transformed\ncoplanar features. The solvers work on scenes without straight lines and, in\ngeneral, relax strong assumptions about scene content made by the state of the\nart. The proposed solvers use the affine invariant that coplanar repeats have\nthe same scale in rectified space. The solvers are separated into two groups\nthat differ by how the equal scale invariant of rectified space is used to\nplace constraints on the lens undistortion and rectification parameters. We\ndemonstrate a principled approach for generating stable minimal solvers by the\nGr\\\"obner basis method, which is accomplished by sampling feasible monomial\nbases to maximize numerical stability. Synthetic and real-image experiments\nconfirm that the proposed solvers demonstrate superior robustness to noise\ncompared to the state of the art. Accurate rectifications on imagery taken with\nnarrow to fisheye field-of-view lenses demonstrate the wide applicability of\nthe proposed method. The method is fully automatic.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 17:16:32 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 23:34:52 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Pritts", "James", ""], ["Kukelova", "Zuzana", ""], ["Larsson", "Viktor", ""], ["Lochman", "Yaroslava", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1907.11544", "submitter": "Guanying Chen", "authors": "Guanying Chen, Kai Han, Kwan-Yee K. Wong", "title": "Learning Transparent Object Matting", "comments": "To appear in International Journal of Computer Vision, Project Page:\n  https://guanyingc.github.io/TOM-Net. arXiv admin note: substantial text\n  overlap with arXiv:1803.04636", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of image matting for transparent objects.\nExisting approaches often require tedious capturing procedures and long\nprocessing time, which limit their practical use. In this paper, we formulate\ntransparent object matting as a refractive flow estimation problem, and propose\na deep learning framework, called TOM-Net, for learning the refractive flow.\nOur framework comprises two parts, namely a multi-scale encoder-decoder network\nfor producing a coarse prediction, and a residual network for refinement. At\ntest time, TOM-Net takes a single image as input, and outputs a matte\n(consisting of an object mask, an attenuation mask and a refractive flow field)\nin a fast feed-forward pass. As no off-the-shelf dataset is available for\ntransparent object matting, we create a large-scale synthetic dataset\nconsisting of $178K$ images of transparent objects rendered in front of images\nsampled from the Microsoft COCO dataset. We also capture a real dataset\nconsisting of $876$ samples using $14$ transparent objects and $60$ background\nimages. Besides, we show that our method can be easily extended to handle the\ncases where a trimap or a background image is available.Promising experimental\nresults have been achieved on both synthetic and real data, which clearly\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 11:14:46 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Chen", "Guanying", ""], ["Han", "Kai", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "1907.11559", "submitter": "Guilherme Pombo", "authors": "Guilherme Pombo, Robert Gray, Tom Varsavsky, John Ashburner, Parashkev\n  Nachev", "title": "Bayesian Volumetric Autoregressive generative models for better\n  semisupervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are rapidly gaining traction in medical imaging.\nNonetheless, most generative architectures struggle to capture the underlying\nprobability distributions of volumetric data, exhibit convergence problems, and\noffer no robust indices of model uncertainty. By comparison, the autoregressive\ngenerative model PixelCNN can be extended to volumetric data with relative\nease, it readily attempts to learn the true underlying probability distribution\nand it still admits a Bayesian reformulation that provides a principled\nframework for reasoning about model uncertainty. Our contributions in this\npaper are two fold: first, we extend PixelCNN to work with volumetric brain\nmagnetic resonance imaging data. Second, we show that reformulating this model\nto approximate a deep Gaussian process yields a measure of uncertainty that\nimproves the performance of semi-supervised learning, in particular\nclassification performance in settings where the proportion of labelled data is\nlow. We quantify this improvement across classification, regression, and\nsemantic segmentation tasks, training and testing on clinical magnetic\nresonance brain imaging data comprising T1-weighted and diffusion-weighted\nsequences.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 13:08:36 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Pombo", "Guilherme", ""], ["Gray", "Robert", ""], ["Varsavsky", "Tom", ""], ["Ashburner", "John", ""], ["Nachev", "Parashkev", ""]]}, {"id": "1907.11561", "submitter": "Jos\\'e Esgario", "authors": "J. G. M. Esgario, R. A. Krohling, J. A. Ventura", "title": "Deep Learning for Classification and Severity Estimation of Coffee Leaf\n  Biotic Stress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biotic stress consists of damage to plants through other living organisms.\nEfficient control of biotic agents such as pests and pathogens (viruses, fungi,\nbacteria, etc.) is closely related to the concept of agricultural\nsustainability. Agricultural sustainability promotes the development of new\ntechnologies that allow the reduction of environmental impacts, greater\naccessibility to farmers and, consequently, increase on productivity. The use\nof computer vision with deep learning methods allows the early and correct\nidentification of the stress-causing agent. So, corrective measures can be\napplied as soon as possible to mitigate the problem. The objective of this work\nis to design an effective and practical system capable of identifying and\nestimating the stress severity caused by biotic agents on coffee leaves. The\nproposed approach consists of a multi-task system based on convolutional neural\nnetworks. In addition, we have explored the use of data augmentation techniques\nto make the system more robust and accurate. The experimental results obtained\nfor classification as well as for severity estimation indicate that the\nproposed system might be a suitable tool to assist both experts and farmers in\nthe identification and quantification of biotic stresses in coffee plantations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 13:12:44 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Esgario", "J. G. M.", ""], ["Krohling", "R. A.", ""], ["Ventura", "J. A.", ""]]}, {"id": "1907.11565", "submitter": "Gilad Vered", "authors": "Gilad Vered, Gal Oren, Yuval Atzmon, Gal Chechik", "title": "Cooperative image captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When describing images with natural language, the descriptions can be made\nmore informative if tuned using downstream tasks. This is often achieved by\ntraining two networks: a \"speaker network\" that generates sentences given an\nimage, and a \"listener network\" that uses them to perform a task.\nUnfortunately, training multiple networks jointly to communicate to achieve a\njoint task, faces two major challenges. First, the descriptions generated by a\nspeaker network are discrete and stochastic, making optimization very hard and\ninefficient. Second, joint training usually causes the vocabulary used during\ncommunication to drift and diverge from natural language.\n  We describe an approach that addresses both challenges. We first develop a\nnew effective optimization based on partial-sampling from a multinomial\ndistribution combined with straight-through gradient updates, which we name\nPSST for Partial-Sampling Straight-Through. Second, we show that the generated\ndescriptions can be kept close to natural by constraining them to be similar to\nhuman descriptions. Together, this approach creates descriptions that are both\nmore discriminative and more natural than previous approaches. Evaluations on\nthe standard COCO benchmark show that PSST Multinomial dramatically improve the\nrecall@10 from 60% to 86% maintaining comparable language naturalness, and\nhuman evaluations show that it also increases naturalness while keeping the\ndiscriminative power of generated captions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 13:18:56 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Vered", "Gilad", ""], ["Oren", "Gal", ""], ["Atzmon", "Yuval", ""], ["Chechik", "Gal", ""]]}, {"id": "1907.11587", "submitter": "Susana Lai-Yuen", "authors": "Maria G. Baldeon Calisto and Susana K. Lai-Yuen", "title": "Self-Adaptive 2D-3D Ensemble of Fully Convolutional Networks for Medical\n  Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is a critical step in medical image analysis. Fully\nConvolutional Networks (FCNs) have emerged as powerful segmentation models\nachieving state-of-the-art results in various medical image datasets. Network\narchitectures are usually designed manually for a specific segmentation task so\napplying them to other medical datasets requires extensive experience and time.\nMoreover, the segmentation requires handling large volumetric data that results\nin big and complex architectures. Recently, methods that automatically design\nneural networks for medical image segmentation have been presented; however,\nmost approaches either do not fully consider volumetric information or do not\noptimize the size of the network. In this paper, we propose a novel\nself-adaptive 2D-3D ensemble of FCNs for medical image segmentation that\nincorporates volumetric information and optimizes both the model's performance\nand size. The model is composed of an ensemble of a 2D FCN that extracts\nintra-slice information, and a 3D FCN that exploits inter-slice information.\nThe architectures of the 2D and 3D FCNs are automatically adapted to a medical\nimage dataset using a multiobjective evolutionary based algorithm that\nminimizes both the segmentation error and number of parameters in the network.\nThe proposed 2D-3D FCN ensemble was tested on the task of prostate segmentation\non the image dataset from the PROMISE12 Grand Challenge. The resulting network\nis ranked in the top 10 submissions, surpassing the performance of other\nautomatically-designed architectures while being considerably smaller in size.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 14:14:53 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Calisto", "Maria G. Baldeon", ""], ["Lai-Yuen", "Susana K.", ""]]}, {"id": "1907.11628", "submitter": "Shuosen Guan", "authors": "Shuosen Guan, Haoxin Li, Wei-Shi Zheng", "title": "Unsupervised Learning for Optical Flow Estimation Using Pyramid\n  Convolution LSTM", "comments": "IEEE International Conference on Multimedia and Expo(ICME). 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of current Convolution Neural Network (CNN) based methods for optical\nflow estimation focus on learning optical flow on synthetic datasets with\ngroundtruth, which is not practical. In this paper, we propose an unsupervised\noptical flow estimation framework named PCLNet. It uses pyramid Convolution\nLSTM (ConvLSTM) with the constraint of adjacent frame reconstruction, which\nallows flexibly estimating multi-frame optical flows from any video clip.\nBesides, by decoupling motion feature learning and optical flow representation,\nour method avoids complex short-cut connections used in existing frameworks\nwhile improving accuracy of optical flow estimation. Moreover, different from\nthose methods using specialized CNN architectures for capturing motion, our\nframework directly learns optical flow from the features of generic CNNs and\nthus can be easily embedded in any CNN based frameworks for other tasks.\nExtensive experiments have verified that our method not only estimates optical\nflow effectively and accurately, but also obtains comparable performance on\naction recognition.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:28:59 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Guan", "Shuosen", ""], ["Li", "Haoxin", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1907.11629", "submitter": "Stefano B. Blumberg", "authors": "Stefano B. Blumberg, Marco Palombo, Can Son Khoo, Chantal M. W. Tax,\n  Ryutaro Tanno, and Daniel C. Alexander", "title": "Multi-Stage Prediction Networks for Data Harmonization", "comments": "Accepted In Medical Image Computing and Computer Assisted\n  Intervention (MICCAI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce multi-task learning (MTL) to data harmonization\n(DH); where we aim to harmonize images across different acquisition platforms\nand sites. This allows us to integrate information from multiple acquisitions\nand improve the predictive performance and learning efficiency of the\nharmonization model. Specifically, we introduce the Multi Stage Prediction\n(MSP) Network, a MTL framework that incorporates neural networks of potentially\ndisparate architectures, trained for different individual acquisition\nplatforms, into a larger architecture that is refined in unison. The MSP\nutilizes high-level features of single networks for individual tasks, as inputs\nof additional neural networks to inform the final prediction, therefore\nexploiting redundancy across tasks to make the most of limited training data.\nWe validate our methods on a dMRI harmonization challenge dataset, where we\npredict three modern platform types, from one obtained from an old scanner. We\nshow how MTL architectures, such as the MSP, produce around 20\\% improvement of\npatch-based mean-squared error over current state-of-the-art methods and that\nour MSP outperforms off-the-shelf MTL networks. Our code is available\nhttps://github.com/sbb-gh/ .\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:29:46 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Blumberg", "Stefano B.", ""], ["Palombo", "Marco", ""], ["Khoo", "Can Son", ""], ["Tax", "Chantal M. W.", ""], ["Tanno", "Ryutaro", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "1907.11637", "submitter": "Sizhuo Ma", "authors": "Sizhuo Ma, Brandon M. Smith, Mohit Gupta", "title": "Differential Scene Flow from Light Field Gradients", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-019-01230-z", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents novel techniques for recovering 3D dense scene flow,\nbased on differential analysis of 4D light fields. The key enabling result is a\nper-ray linear equation, called the ray flow equation, that relates 3D scene\nflow to 4D light field gradients. The ray flow equation is invariant to 3D\nscene structure and applicable to a general class of scenes, but is\nunder-constrained (3 unknowns per equation). Thus, additional constraints must\nbe imposed to recover motion. We develop two families of scene flow algorithms\nby leveraging the structural similarity between ray flow and optical flow\nequations: local 'Lucas-Kanade' ray flow and global 'Horn-Schunck' ray flow,\ninspired by corresponding optical flow methods. We also develop a combined\nlocal-global method by utilizing the correspondence structure in the light\nfields. We demonstrate high precision 3D scene flow recovery for a wide range\nof scenarios, including rotation and non-rigid motion. We analyze the\ntheoretical and practical performance limits of the proposed techniques via the\nlight field structure tensor, a 3x3 matrix that encodes the local structure of\nlight fields. We envision that the proposed analysis and algorithms will lead\nto design of future light-field cameras that are optimized for motion sensing,\nin addition to depth sensing.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:47:14 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 18:01:08 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ma", "Sizhuo", ""], ["Smith", "Brandon M.", ""], ["Gupta", "Mohit", ""]]}, {"id": "1907.11684", "submitter": "Pu Zhao", "authors": "Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, Bhavya\n  Kailkhura, Xue Lin", "title": "On the Design of Black-box Adversarial Examples by Leveraging\n  Gradient-free Optimization and Operator Splitting Method", "comments": "accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust machine learning is currently one of the most prominent topics which\ncould potentially help shaping a future of advanced AI platforms that not only\nperform well in average cases but also in worst cases or adverse situations.\nDespite the long-term vision, however, existing studies on black-box\nadversarial attacks are still restricted to very specific settings of threat\nmodels (e.g., single distortion metric and restrictive assumption on target\nmodel's feedback to queries) and/or suffer from prohibitively high query\ncomplexity. To push for further advances in this field, we introduce a general\nframework based on an operator splitting method, the alternating direction\nmethod of multipliers (ADMM) to devise efficient, robust black-box attacks that\nwork with various distortion metrics and feedback settings without incurring\nhigh query complexity. Due to the black-box nature of the threat model, the\nproposed ADMM solution framework is integrated with zeroth-order (ZO)\noptimization and Bayesian optimization (BO), and thus is applicable to the\ngradient-free regime. This results in two new black-box adversarial attack\ngeneration methods, ZO-ADMM and BO-ADMM. Our empirical evaluations on image\nclassification datasets show that our proposed approaches have much lower\nfunction query complexities compared to state-of-the-art attack methods, but\nachieve very competitive attack success rates.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 17:29:52 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 19:34:54 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 16:35:58 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 21:05:19 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Zhao", "Pu", ""], ["Liu", "Sijia", ""], ["Chen", "Pin-Yu", ""], ["Hoang", "Nghia", ""], ["Xu", "Kaidi", ""], ["Kailkhura", "Bhavya", ""], ["Lin", "Xue", ""]]}, {"id": "1907.11704", "submitter": "Jingya Liu", "authors": "Jingya Liu, Liangliang Cao, Oguz Akin, Yingli Tian", "title": "Accurate and Robust Pulmonary Nodule Detection by 3D Feature Pyramid\n  Network with Self-supervised Feature Learning", "comments": "15 pages, 8 figures, 5 tables, under review by Medical Image\n  Analysis. arXiv admin note: substantial text overlap with arXiv:1906.03467", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of pulmonary nodules with high sensitivity and specificity\nis essential for automatic lung cancer diagnosis from CT scans. Although many\ndeep learning-based algorithms make great progress for improving the accuracy\nof nodule detection, the high false positive rate is still a challenging\nproblem which limits the automatic diagnosis in routine clinical practice.\nMoreover, the CT scans collected from multiple manufacturers may affect the\nrobustness of Computer-aided diagnosis (CAD) due to the differences in\nintensity scales and machine noises. In this paper, we propose a novel\nself-supervised learning assisted pulmonary nodule detection framework based on\na 3D Feature Pyramid Network (3DFPN) to improve the sensitivity of nodule\ndetection by employing multi-scale features to increase the resolution of\nnodules, as well as a parallel top-down path to transit the high-level semantic\nfeatures to complement low-level general features. Furthermore, a High\nSensitivity and Specificity (HS2) network is introduced to eliminate the false\npositive nodule candidates by tracking the appearance changes in continuous CT\nslices of each nodule candidate on Location History Images (LHI). In addition,\nin order to improve the performance consistency of the proposed framework\nacross data captured by different CT scanners without using additional\nannotations, an effective self-supervised learning schema is applied to learn\nspatiotemporal features of CT scans from large-scale unlabeled data. The\nperformance and robustness of our method are evaluated on several publicly\navailable datasets with significant performance improvements. The proposed\nframework is able to accurately detect pulmonary nodules with high sensitivity\nand specificity and achieves 90.6% sensitivity with 1/8 false positive per scan\nwhich outperforms the state-of-the-art results 15.8% on LUNA16 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 21:00:29 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Liu", "Jingya", ""], ["Cao", "Liangliang", ""], ["Akin", "Oguz", ""], ["Tian", "Yingli", ""]]}, {"id": "1907.11711", "submitter": "Jing Cheng", "authors": "Dong Liang, Jing Cheng, Ziwen Ke, Leslie Ying", "title": "Deep MRI Reconstruction: Unrolled Optimization Algorithms Meet Neural\n  Networks", "comments": "a review paper on deep learning MR reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image reconstruction from undersampled k-space data has been playing an\nimportant role for fast MRI. Recently, deep learning has demonstrated\ntremendous success in various fields and also shown potential to significantly\nspeed up MR reconstruction with reduced measurements. This article gives an\noverview of deep learning-based image reconstruction methods for MRI. Three\ntypes of deep learning-based approaches are reviewed, the data-driven,\nmodel-driven and integrated approaches. The main structure of each network in\nthree approaches is explained and the analysis of common parts of reviewed\nnetworks and differences in-between are highlighted. Based on the review, a\nnumber of signal processing issues are discussed for maximizing the potential\nof deep reconstruction for fast MRI. the discussion may facilitate further\ndevelopment of \"optimal\" network and performance analysis from a theoretical\npoint of view.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 08:22:53 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Liang", "Dong", ""], ["Cheng", "Jing", ""], ["Ke", "Ziwen", ""], ["Ying", "Leslie", ""]]}, {"id": "1907.11751", "submitter": "Qi Feng", "authors": "Qi Feng, Vitaly Ablavsky, Qinxun Bai, Guorong Li, and Stan Sclaroff", "title": "Real-time Visual Object Tracking with Natural Language Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep-learning-based visual object trackers have been studied\nthoroughly, but handling occlusions and/or rapid motion of the target remains\nchallenging. In this work, we argue that conditioning on the natural language\n(NL) description of a target provides information for longer-term invariance,\nand thus helps cope with typical tracking challenges. However, deriving a\nformulation to combine the strengths of appearance-based tracking with the\nlanguage modality is not straightforward. We propose a novel deep\ntracking-by-detection formulation that can take advantage of NL descriptions.\nRegions that are related to the given NL description are generated by a\nproposal network during the detection phase of the tracker. Our LSTM based\ntracker then predicts the update of the target from regions proposed by the NL\nbased detection phase. In benchmarks, our method is competitive with state of\nthe art trackers, while it outperforms all other trackers on targets with\nunambiguous and precise language annotations. It also beats the\nstate-of-the-art NL tracker when initializing without a bounding box. Our\nmethod runs at over 30 fps on a single GPU.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 18:44:17 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 18:40:54 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 22:08:53 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Feng", "Qi", ""], ["Ablavsky", "Vitaly", ""], ["Bai", "Qinxun", ""], ["Li", "Guorong", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1907.11770", "submitter": "Noriyuki Kojima", "authors": "Noriyuki Kojima, Jia Deng", "title": "To Learn or Not to Learn: Analyzing the Role of Learning for Navigation\n  in Virtual Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we compare learning-based methods and classical methods for\nnavigation in virtual environments. We construct classical navigation agents\nand demonstrate that they outperform state-of-the-art learning-based agents on\ntwo standard benchmarks: MINOS and Stanford Large-Scale 3D Indoor Spaces. We\nperform detailed analysis to study the strengths and weaknesses of learned\nagents and classical agents, as well as how characteristics of the virtual\nenvironment impact navigation performance. Our results show that learned agents\nhave inferior collision avoidance and memory management, but are superior in\nhandling ambiguity and noise. These results can inform future design of\nnavigation agents.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 19:45:23 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Kojima", "Noriyuki", ""], ["Deng", "Jia", ""]]}, {"id": "1907.11804", "submitter": "Kartikeya Bhardwaj", "authors": "Kartikeya Bhardwaj, Chingyi Lin, Anderson Sartor, Radu Marculescu", "title": "Memory- and Communication-Aware Model Compression for Distributed Deep\n  Learning Inference on IoT", "comments": "This preprint is for personal use only. The official article will\n  appear as part of the ESWEEK-TECS special issue and will be presented in the\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression has emerged as an important area of research for deploying\ndeep learning models on Internet-of-Things (IoT). However, for extremely\nmemory-constrained scenarios, even the compressed models cannot fit within the\nmemory of a single device and, as a result, must be distributed across multiple\ndevices. This leads to a distributed inference paradigm in which memory and\ncommunication costs represent a major bottleneck. Yet, existing model\ncompression techniques are not communication-aware. Therefore, we propose\nNetwork of Neural Networks (NoNN), a new distributed IoT learning paradigm that\ncompresses a large pretrained 'teacher' deep network into several disjoint and\nhighly-compressed 'student' modules, without loss of accuracy. Moreover, we\npropose a network science-based knowledge partitioning algorithm for the\nteacher model, and then train individual students on the resulting disjoint\npartitions. Extensive experimentation on five image classification datasets,\nfor user-defined memory/performance budgets, show that NoNN achieves higher\naccuracy than several baselines and similar accuracy as the teacher model,\nwhile using minimal communication among students. Finally, as a case study, we\ndeploy the proposed model for CIFAR-10 dataset on edge devices and demonstrate\nsignificant improvements in memory footprint (up to 24x), performance (up to\n12x), and energy per node (up to 14x) compared to the large teacher model. We\nfurther show that for distributed inference on multiple edge devices, our\nproposed NoNN model results in up to 33x reduction in total latency w.r.t. a\nstate-of-the-art model compression baseline.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 22:17:42 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Bhardwaj", "Kartikeya", ""], ["Lin", "Chingyi", ""], ["Sartor", "Anderson", ""], ["Marculescu", "Radu", ""]]}, {"id": "1907.11811", "submitter": "Chengjiang Long", "authors": "Tao Hu, Chengjiang Long, Leheng Zhang, Chunxia Xiao", "title": "VITAL: A Visual Interpretation on Text with Adversarial Learning for\n  Image Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel way to interpret text information by\nextracting visual feature presentation from multiple high-resolution and\nphoto-realistic synthetic images generated by Text-to-image Generative\nAdversarial Network (GAN) to improve the performance of image labeling.\nFirstly, we design a stacked Generative Multi-Adversarial Network (GMAN),\nStackGMAN++, a modified version of the current state-of-the-art Text-to-image\nGAN, StackGAN++, to generate multiple synthetic images with various prior\nnoises conditioned on a text. And then we extract deep visual features from the\ngenerated synthetic images to explore the underlying visual concepts for text.\nFinally, we combine image-level visual feature, text-level feature and visual\nfeatures based on synthetic images together to predict labels for images. We\nconduct experiments on two benchmark datasets and the experimental results\nclearly demonstrate the efficacy of our proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 22:52:45 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 18:57:53 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Hu", "Tao", ""], ["Long", "Chengjiang", ""], ["Zhang", "Leheng", ""], ["Xiao", "Chunxia", ""]]}, {"id": "1907.11818", "submitter": "Il Yong Chun", "authors": "Il Yong Chun, Zhengyu Huang, Hongki Lim, Jeffrey A. Fessler", "title": "Momentum-Net: Fast and convergent iterative neural network for inverse\n  problems", "comments": "28 pages, 13 figures, 3 algorithms, 4 tables, submitted revision to\n  IEEE T-PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative neural networks (INN) are rapidly gaining attention for solving\ninverse problems in imaging, image processing, and computer vision. INNs\ncombine regression NNs and an iterative model-based image reconstruction (MBIR)\nalgorithm, often leading to both good generalization capability and\noutperforming reconstruction quality over existing MBIR optimization models.\nThis paper proposes the first fast and convergent INN architecture,\nMomentum-Net, by generalizing a block-wise MBIR algorithm that uses momentum\nand majorizers with regression NNs. For fast MBIR, Momentum-Net uses momentum\nterms in extrapolation modules, and noniterative MBIR modules at each iteration\nby using majorizers, where each iteration of Momentum-Net consists of three\ncore modules: image refining, extrapolation, and MBIR. Momentum-Net guarantees\nconvergence to a fixed-point for general differentiable (non)convex MBIR\nfunctions (or data-fit terms) and convex feasible sets, under two asymptomatic\nconditions. To consider data-fit variations across training and testing\nsamples, we also propose a regularization parameter selection scheme based on\nthe \"spectral spread\" of majorization matrices. Numerical experiments for\nlight-field photography using a focal stack and sparse-view computational\ntomography demonstrate that, given identical regression NN architectures,\nMomentum-Net significantly improves MBIR speed and accuracy over several\nexisting INNs; it significantly improves reconstruction quality compared to a\nstate-of-the-art MBIR method in each application.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 23:42:37 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 01:54:14 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 09:42:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Chun", "Il Yong", ""], ["Huang", "Zhengyu", ""], ["Lim", "Hongki", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1907.11819", "submitter": "Thiago Santos", "authors": "Thiago T. Santos, Leonardo L. de Souza, Andreza A. dos Santos and\n  Sandra Avila", "title": "Grape detection, segmentation and tracking using deep neural networks\n  and three-dimensional association", "comments": null, "journal-ref": "Computers and Electronics in Agriculture, 170, 105-247 (2020)", "doi": "10.1016/J.COMPAG.2020.105247", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agricultural applications such as yield prediction, precision agriculture and\nautomated harvesting need systems able to infer the crop state from low-cost\nsensing devices. Proximal sensing using affordable cameras combined with\ncomputer vision has seen a promising alternative, strengthened after the advent\nof convolutional neural networks (CNNs) as an alternative for challenging\npattern recognition problems in natural images. Considering fruit growing\nmonitoring and automation, a fundamental problem is the detection, segmentation\nand counting of individual fruits in orchards. Here we show that for wine\ngrapes, a crop presenting large variability in shape, color, size and\ncompactness, grape clusters can be successfully detected, segmented and tracked\nusing state-of-the-art CNNs. In a test set containing 408 grape clusters from\nimages taken on a trellis-system based vineyard, we have reached an F 1 -score\nup to 0.91 for instance segmentation, a fine separation of each cluster from\nother structures in the image that allows a more accurate assessment of fruit\nsize and shape. We have also shown as clusters can be identified and tracked\nalong video sequences recording orchard rows. We also present a public dataset\ncontaining grape clusters properly annotated in 300 images and a novel\nannotation methodology for segmentation of complex objects in natural images.\nThe presented pipeline for annotation, training, evaluation and tracking of\nagricultural patterns in images can be replicated for different crops and\nproduction systems. It can be employed in the development of sensing components\nfor several agricultural and environmental applications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 23:45:51 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 20:47:46 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 11:36:18 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Santos", "Thiago T.", ""], ["de Souza", "Leonardo L.", ""], ["Santos", "Andreza A. dos", ""], ["Avila", "Sandra", ""]]}, {"id": "1907.11821", "submitter": "Kashyap Chitta", "authors": "Kashyap Chitta, Jose M. Alvarez, Martial Hebert", "title": "Quadtree Generating Networks: Efficient Hierarchical Scene Parsing with\n  Sparse Convolutions", "comments": "Accepted for IEEE Winter Conference on Applications of Computer\n  Vision (WACV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation with Convolutional Neural Networks is a\nmemory-intensive task due to the high spatial resolution of feature maps and\noutput predictions. In this paper, we present Quadtree Generating Networks\n(QGNs), a novel approach able to drastically reduce the memory footprint of\nmodern semantic segmentation networks. The key idea is to use quadtrees to\nrepresent the predictions and target segmentation masks instead of dense pixel\ngrids. Our quadtree representation enables hierarchical processing of an input\nimage, with the most computationally demanding layers only being used at\nregions in the image containing boundaries between classes. In addition, given\na trained model, our representation enables flexible inference schemes to\ntrade-off accuracy and computational cost, allowing the network to adapt in\nconstrained situations such as embedded devices. We demonstrate the benefits of\nour approach on the Cityscapes, SUN-RGBD and ADE20k datasets. On Cityscapes, we\nobtain an relative 3% mIoU improvement compared to a dilated network with\nsimilar memory consumption; and only receive a 3% relative mIoU drop compared\nto a large dilated network, while reducing memory consumption by over\n4$\\times$.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 00:20:12 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 18:58:50 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Chitta", "Kashyap", ""], ["Alvarez", "Jose M.", ""], ["Hebert", "Martial", ""]]}, {"id": "1907.11824", "submitter": "Akshay Rangesh", "authors": "Akshay Rangesh and Mohan M. Trivedi", "title": "Forced Spatial Attention for Driver Foot Activity Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a simple solution for reliably solving image\nclassification tasks tied to spatial locations of salient objects in the scene.\nUnlike conventional image classification approaches that are designed to be\ninvariant to translations of objects in the scene, we focus on tasks where the\noutput classes vary with respect to where an object of interest is situated\nwithin an image. To handle this variant of the image classification task, we\npropose augmenting the standard cross-entropy (classification) loss with a\ndomain dependent Forced Spatial Attention (FSA) loss, which in essence compels\nthe network to attend to specific regions in the image associated with the\ndesired output class. To demonstrate the utility of this loss function, we\nconsider the task of driver foot activity classification - where each activity\nis strongly correlated with where the driver's foot is in the scene. Training\nwith our proposed loss function results in significantly improved accuracies,\nbetter generalization, and robustness against noise, while obviating the need\nfor very large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 01:36:09 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 00:36:55 GMT"}, {"version": "v3", "created": "Sun, 20 Oct 2019 20:53:43 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Rangesh", "Akshay", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1907.11830", "submitter": "Py Zhao", "authors": "Pengyu Zhao, Ansheng You, Yuanxing Zhang, Jiaying Liu, Kaigui Bian,\n  Yunhai Tong", "title": "Reprojection R-CNN: A Fast and Accurate Object Detector for 360{\\deg}\n  Images", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360{\\deg} images are usually represented in either equirectangular projection\n(ERP) or multiple perspective projections. Different from the flat 2D images,\nthe detection task is challenging for 360{\\deg} images due to the distortion of\nERP and the inefficiency of perspective projections. However, existing methods\nmostly focus on one of the above representations instead of both, leading to\nlimited detection performance. Moreover, the lack of appropriate bounding-box\nannotations as well as the annotated datasets further increases the\ndifficulties of the detection task. In this paper, we present a standard object\ndetection framework for 360{\\deg} images. Specifically, we adapt the\nterminologies of the traditional object detection task to the omnidirectional\nscenarios, and propose a novel two-stage object detector, i.e., Reprojection\nR-CNN by combining both ERP and perspective projection. Owing to the\nomnidirectional field-of-view of ERP, Reprojection R-CNN first generates coarse\nregion proposals efficiently by a distortion-aware spherical region proposal\nnetwork. Then, it leverages the distortion-free perspective projection and\nrefines the proposed regions by a novel reprojection network. We construct two\nnovel synthetic datasets for training and evaluation. Experiments reveal that\nReprojection R-CNN outperforms the previous state-of-the-art methods on the mAP\nmetric. In addition, the proposed detector could run at 178ms per image in the\npanoramic datasets, which implies its practicability in real-world\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 02:12:41 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhao", "Pengyu", ""], ["You", "Ansheng", ""], ["Zhang", "Yuanxing", ""], ["Liu", "Jiaying", ""], ["Bian", "Kaigui", ""], ["Tong", "Yunhai", ""]]}, {"id": "1907.11832", "submitter": "Binghui Chen", "authors": "Binghui Chen, Weihong Deng", "title": "Hybrid-Attention based Decoupled Metric Learning for Zero-Shot Image\n  Retrieval", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In zero-shot image retrieval (ZSIR) task, embedding learning becomes more\nattractive, however, many methods follow the traditional metric learning idea\nand omit the problems behind zero-shot settings. In this paper, we first\nemphasize the importance of learning visual discriminative metric and\npreventing the partial/selective learning behavior of learner in ZSIR, and then\npropose the Decoupled Metric Learning (DeML) framework to achieve these\nindividually. Instead of coarsely optimizing an unified metric, we decouple it\ninto multiple attention-specific parts so as to recurrently induce the\ndiscrimination and explicitly enhance the generalization. And they are mainly\nachieved by our object-attention module based on random walk graph propagation\nand the channel-attention module based on the adversary constraint,\nrespectively. We demonstrate the necessity of addressing the vital problems in\nZSIR on the popular benchmarks, outperforming the state-of-theart methods by a\nsignificant margin. Code is available at http://www.bhchen.cn\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 02:27:10 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Chen", "Binghui", ""], ["Deng", "Weihong", ""]]}, {"id": "1907.11835", "submitter": "Haidong Zhu", "authors": "Haidong Zhu, Jialin Shi and Ji Wu", "title": "Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image\n  Segmentation", "comments": "Accepted for MICCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have achieved promising performance in many areas, but\nthey are still struggling with noisy-labeled images during the training\nprocess. Considering that the annotation quality indispensably relies on great\nexpertise, the problem is even more crucial in the medical image domain. How to\neliminate the disturbance from noisy labels for segmentation tasks without\nfurther annotations is still a significant challenge. In this paper, we\nintroduce our label quality evaluation strategy for deep neural networks\nautomatically assessing the quality of each label, which is not explicitly\nprovided, and training on clean-annotated ones. We propose a solution for\nnetwork automatically evaluating the relative quality of the labels in the\ntraining set and using good ones to tune the network parameters. We also design\nan overfitting control module to let the network maximally learn from the\nprecise annotations during the training process. Experiments on the public\nbiomedical image segmentation dataset have proved the method outperforms\nbaseline methods and retains both high accuracy and good generalization at\ndifferent noise levels.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 02:34:30 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhu", "Haidong", ""], ["Shi", "Jialin", ""], ["Wu", "Ji", ""]]}, {"id": "1907.11837", "submitter": "Kai Han", "authors": "Kai Han, Yunhe Wang, Han Shu, Chuanjian Liu, Chunjing Xu, Chang Xu", "title": "Attribute Aware Pooling for Pedestrian Attribute Recognition", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper expands the strength of deep convolutional neural networks (CNNs)\nto the pedestrian attribute recognition problem by devising a novel attribute\naware pooling algorithm. Existing vanilla CNNs cannot be straightforwardly\napplied to handle multi-attribute data because of the larger label space as\nwell as the attribute entanglement and correlations. We tackle these challenges\nthat hampers the development of CNNs for multi-attribute classification by\nfully exploiting the correlation between different attributes. The multi-branch\narchitecture is adopted for fucusing on attributes at different regions.\nBesides the prediction based on each branch itself, context information of each\nbranch are employed for decision as well. The attribute aware pooling is\ndeveloped to integrate both kinds of information. Therefore, attributes which\nare indistinct or tangled with others can be accurately recognized by\nexploiting the context information. Experiments on benchmark datasets\ndemonstrate that the proposed pooling method appropriately explores and\nexploits the correlations between attributes for the pedestrian attribute\nrecognition.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 02:45:32 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Shu", "Han", ""], ["Liu", "Chuanjian", ""], ["Xu", "Chunjing", ""], ["Xu", "Chang", ""]]}, {"id": "1907.11840", "submitter": "Kai Han", "authors": "Chuanjian Liu, Yunhe Wang, Kai Han, Chunjing Xu, Chang Xu", "title": "Learning Instance-wise Sparsity for Accelerating Deep Models", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring deep convolutional neural networks of high efficiency and low\nmemory usage is very essential for a wide variety of machine learning tasks.\nMost of existing approaches used to accelerate deep models by manipulating\nparameters or filters without data, e.g., pruning and decomposition. In\ncontrast, we study this problem from a different perspective by respecting the\ndifference between data. An instance-wise feature pruning is developed by\nidentifying informative features for different instances. Specifically, by\ninvestigating a feature decay regularization, we expect intermediate feature\nmaps of each instance in deep neural networks to be sparse while preserving the\noverall network performance. During online inference, subtle features of input\nimages extracted by intermediate layers of a well-trained neural network can be\neliminated to accelerate the subsequent calculations. We further take\ncoefficient of variation as a measure to select the layers that are appropriate\nfor acceleration. Extensive experiments conducted on benchmark datasets and\nnetworks demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 02:59:38 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Liu", "Chuanjian", ""], ["Wang", "Yunhe", ""], ["Han", "Kai", ""], ["Xu", "Chunjing", ""], ["Xu", "Chang", ""]]}, {"id": "1907.11845", "submitter": "Tianyi Chen", "authors": "Bo Ji, Tianyi Chen", "title": "Generative Adversarial Network for Handwritten Text", "comments": "12 pages, 7 figures, submitted for WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have proven hugely successful in\nvariety of applications of image processing. However, generative adversarial\nnetworks for handwriting is relatively rare somehow because of difficulty of\nhandling sequential handwriting data by Convolutional Neural Network (CNN). In\nthis paper, we propose a handwriting generative adversarial network framework\n(HWGANs) for synthesizing handwritten stroke data. The main features of the new\nframework include: (i) A discriminator consists of an integrated\nCNN-Long-Short-Term- Memory (LSTM) based feature extraction with Path Signature\nFeatures (PSF) as input and a Feedforward Neural Network (FNN) based binary\nclassifier; (ii) A recurrent latent variable model as generator for\nsynthesizing sequential handwritten data. The numerical experiments show the\neffectivity of the new model. Moreover, comparing with sole handwriting\ngenerator, the HWGANs synthesize more natural and realistic handwritten text.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 04:15:10 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 05:11:09 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 07:17:16 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Ji", "Bo", ""], ["Chen", "Tianyi", ""]]}, {"id": "1907.11849", "submitter": "Connor Monahan", "authors": "Hunter Park, Connor Monahan", "title": "Genetic Deep Learning for Lung Cancer Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown great promise in improving\ncomputer aided detection (CADe). From classifying tumors found via mammography\nas benign or malignant to automated detection of colorectal polyps in CT\ncolonography, these advances have helped reduce the need for further evaluation\nwith invasive testing and prevent errors from missed diagnoses by acting as a\nsecond observer in today's fast paced and high volume clinical environment.\nCADe methods have become faster and more precise thanks to innovations in deep\nlearning over the past several years. With advancements such as the inception\nmodule and utilization of residual connections, the approach to designing CNN\narchitectures has become an art. It is customary to use proven models and fine\ntune them for particular tasks given a dataset, often requiring tedious work.\nWe investigated using a genetic algorithm (GA) to conduct a neural\narchitectural search (NAS) to generate a novel CNN architecture to find early\nstage lung cancer in chest x-rays (CXR). Using a dataset of over twelve\nthousand biopsy proven cases of lung cancer, the trained classification model\nachieved an accuracy of 97.15% with a PPV of 99.88% and a NPV of 94.81%,\nbeating models such as Inception-V3 and ResNet-152 while simultaneously\nreducing the number of parameters a factor of 4 and 14, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 04:47:35 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Park", "Hunter", ""], ["Monahan", "Connor", ""]]}, {"id": "1907.11854", "submitter": "ByungSoo Ko", "authors": "Byungsoo Ko, Minchul Shin, Geonmo Gu, HeeJae Jun, Tae Kwan Lee,\n  Youngjoon Kim", "title": "A Benchmark on Tricks for Large-scale Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have been performed on metric learning, which has become a key\ningredient in top-performing methods of instance-level image retrieval.\nMeanwhile, less attention has been paid to pre-processing and post-processing\ntricks that can significantly boost performance. Furthermore, we found that\nmost previous studies used small scale datasets to simplify processing. Because\nthe behavior of a feature representation in a deep learning model depends on\nboth domain and data, it is important to understand how model behave in\nlarge-scale environments when a proper combination of retrieval tricks is used.\nIn this paper, we extensively analyze the effect of well-known pre-processing,\npost-processing tricks, and their combination for large-scale image retrieval.\nWe found that proper use of these tricks can significantly improve model\nperformance without necessitating complex architecture or introducing loss, as\nconfirmed by achieving a competitive result on the Google Landmark Retrieval\nChallenge 2019.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 05:58:00 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 06:29:25 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ko", "Byungsoo", ""], ["Shin", "Minchul", ""], ["Gu", "Geonmo", ""], ["Jun", "HeeJae", ""], ["Lee", "Tae Kwan", ""], ["Kim", "Youngjoon", ""]]}, {"id": "1907.11880", "submitter": "Manoj Lenka", "authors": "Manoj Kumar Lenka, Anubha Pandey and Anurag Mittal", "title": "Blind Deblurring Using GANs", "comments": "15 Pages (including reference and appendices). 6 Figures. 5 Tables\n  (including appendices). Work done as a part of the Summer Research Fellowship\n  Program by the Indian Academy of Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deblurring is the task of restoring a blurred image to a sharp one,\nretrieving the information lost due to the blur. In blind deblurring we have no\ninformation regarding the blur kernel. As deblurring can be considered as an\nimage to image translation task, deep learning based solutions, including the\nones which use GAN (Generative Adversarial Network), have been proven effective\nfor deblurring. Most of them have an encoder-decoder structure. Our objective\nis to try different GAN structures and improve its performance through various\nmodifications to the existing structure for supervised deblurring. In\nsupervised deblurring we have pairs of blurred and their corresponding sharp\nimages, while in the unsupervised case we have a set of blurred and sharp\nimages but their is no correspondence between them. Modifications to the\nstructures is done to improve the global perception of the model. As blur is\nnon-uniform in nature, for deblurring we require global information of the\nentire image, whereas convolution used in CNN is able to provide only local\nperception. Deep models can be used to improve global perception but due to\nlarge number of parameters it becomes difficult for it to converge and\ninference time increases, to solve this we propose the use of attention module\n(non-local block) which was previously used in language translation and other\nimage to image translation tasks in deblurring. Use of residual connection also\nimproves the performance of deblurring as features from the lower layers are\nadded to the upper layers of the model. It has been found that classical losses\nlike L1, L2, and perceptual loss also help in training of GANs when added\ntogether with adversarial loss. We also concatenate edge information of the\nimage to observe its effects on deblurring. We also use feedback modules to\nretain long term dependencies\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 09:30:21 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lenka", "Manoj Kumar", ""], ["Pandey", "Anubha", ""], ["Mittal", "Anurag", ""]]}, {"id": "1907.11881", "submitter": "Satyajit Neogi", "authors": "Satyajit Neogi, Michael Hoy, Kang Dang, Hang Yu and Justin Dauwels", "title": "Context Model for Pedestrian Intention Prediction using Factored\n  Latent-Dynamic Conditional Random Fields", "comments": "Accepted by IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2020.2995166", "report-no": null, "categories": "cs.CV cs.RO eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smooth handling of pedestrian interactions is a key requirement for\nAutonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS). Such\nsystems call for early and accurate prediction of a pedestrian's\ncrossing/not-crossing behaviour in front of the vehicle. Existing approaches to\npedestrian behaviour prediction make use of pedestrian motion, his/her location\nin a scene and static context variables such as traffic lights, zebra crossings\netc. We stress on the necessity of early prediction for smooth operation of\nsuch systems. We introduce the influence of vehicle interactions on pedestrian\nintention for this purpose. In this paper, we show a discernible advance in\nprediction time aided by the inclusion of such vehicle interaction context. We\napply our methods to two different datasets, one in-house collected - NTU\ndataset and another public real-life benchmark - JAAD dataset. We also propose\na generic graphical model Factored Latent-Dynamic Conditional Random Fields\n(FLDCRF) for single and multi-label sequence prediction as well as joint\ninteraction modeling tasks. FLDCRF outperforms Long Short-Term Memory (LSTM)\nnetworks across the datasets ($\\sim$100 sequences per dataset) over identical\ntime-series features. While the existing best system predicts pedestrian\nstopping behaviour with 70\\% accuracy 0.38 seconds before the actual events,\nour system achieves such accuracy at least 0.9 seconds on an average before the\nactual events across datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 09:34:12 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 16:51:20 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 10:24:04 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2020 11:19:23 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Neogi", "Satyajit", ""], ["Hoy", "Michael", ""], ["Dang", "Kang", ""], ["Yu", "Hang", ""], ["Dauwels", "Justin", ""]]}, {"id": "1907.11885", "submitter": "Kai Qiao", "authors": "Kai Qiao, Chi Zhang, Jian Chen, Linyuan Wang, Li Tong, Bin Yan", "title": "Effective and efficient ROI-wise visual encoding using an end-to-end CNN\n  regression model and selective optimization", "comments": "under review in Computational Intelligence and Neuroscience", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, visual encoding based on functional magnetic resonance imaging\n(fMRI) have realized many achievements with the rapid development of deep\nnetwork computation. Visual encoding model is aimed at predicting brain\nactivity in response to presented image stimuli. Currently, visual encoding is\naccomplished mainly by firstly extracting image features through convolutional\nneural network (CNN) model pre-trained on computer vision task, and secondly\ntraining a linear regression model to map specific layer of CNN features to\neach voxel, namely voxel-wise encoding. However, the two-step manner model,\nessentially, is hard to determine which kind of well features are well linearly\nmatched for beforehand unknown fMRI data with little understanding of human\nvisual representation. Analogizing computer vision mostly related human vision,\nwe proposed the end-to-end convolution regression model (ETECRM) in the region\nof interest (ROI)-wise manner to accomplish effective and efficient visual\nencoding. The end-to-end manner was introduced to make the model automatically\nlearn better matching features to improve encoding performance. The ROI-wise\nmanner was used to improve the encoding efficiency for many voxels. In\naddition, we designed the selective optimization including self-adapting weight\nlearning and weighted correlation loss, noise regularization to avoid\ninterfering of ineffective voxels in ROI-wise encoding. Experiment demonstrated\nthat the proposed model obtained better predicting accuracy than the two-step\nmanner of encoding models. Comparative analysis implied that end-to-end manner\nand large volume of fMRI data may drive the future development of visual\nencoding.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 10:09:05 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Qiao", "Kai", ""], ["Zhang", "Chi", ""], ["Chen", "Jian", ""], ["Wang", "Linyuan", ""], ["Tong", "Li", ""], ["Yan", "Bin", ""]]}, {"id": "1907.11899", "submitter": "Cian M. Scannell", "authors": "Cian M. Scannell, Piet van den Bosch, Amedeo Chiribiri, Jack Lee,\n  Marcel Breeuwer and Mitko Veta", "title": "Deep learning-based prediction of kinetic parameters from myocardial\n  perfusion MRI", "comments": "Medical Imaging with Deep Learning: MIDL 2019 Extended Abstract\n  Track. MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/HyxyNmQAF4", "categories": "eess.IV cs.CV physics.med-ph q-bio.QM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The quantification of myocardial perfusion MRI has the potential to provide a\nfast, automated and user-independent assessment of myocardial ischaemia.\nHowever, due to the relatively high noise level and low temporal resolution of\nthe acquired data and the complexity of the tracer-kinetic models, the model\nfitting can yield unreliable parameter estimates. A solution to this problem is\nthe use of Bayesian inference which can incorporate prior knowledge and improve\nthe reliability of the parameter estimation. This, however, uses Markov chain\nMonte Carlo sampling to approximate the posterior distribution of the kinetic\nparameters which is extremely time intensive. This work proposes training\nconvolutional networks to directly predict the kinetic parameters from the\nsignal-intensity curves that are trained using estimates obtained from the\nBayesian inference. This allows fast estimation of the kinetic parameters with\na similar performance to the Bayesian inference.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 11:58:43 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Scannell", "Cian M.", ""], ["Bosch", "Piet van den", ""], ["Chiribiri", "Amedeo", ""], ["Lee", "Jack", ""], ["Breeuwer", "Marcel", ""], ["Veta", "Mitko", ""]]}, {"id": "1907.11912", "submitter": "Yunfei Liu", "authors": "Yunfei Liu, Yu Li, Shaodi You and Feng Lu", "title": "Semantic Guided Single Image Reflection Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflection is common in images capturing scenes behind a glass window, which\nis not only a disturbance visually but also influence the performance of other\ncomputer vision algorithms. Single image reflection removal is an ill-posed\nproblem because the color at each pixel needs to be separated into two values,\ni.e., the desired clear background and the reflection. To solve it, existing\nmethods propose priors such as smoothness, color consistency. However, the\nlow-level priors are not reliable in complex scenes, for instance, when\ncapturing a real outdoor scene through a window, both the foreground and\nbackground contain both smooth and sharp area and a variety of color. In this\npaper, inspired by the fact that human can separate the two layers easily by\nrecognizing the objects, we use the object semantic as guidance to force the\nsame semantic object belong to the same layer. Extensive experiments on\ndifferent datasets show that adding the semantic information offers a\nsignificant improvement to reflection separation. We also demonstrate the\napplications of the proposed method to other computer vision tasks.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 13:52:00 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 06:44:22 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Liu", "Yunfei", ""], ["Li", "Yu", ""], ["You", "Shaodi", ""], ["Lu", "Feng", ""]]}, {"id": "1907.11914", "submitter": "Ang Li", "authors": "Ang Li, Xue Yang, Chongyang Zhang", "title": "Rethinking Classification and Localization for Cascade R-CNN", "comments": "BMVC 2019 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the state-of-the-art Cascade R-CNN with a simple feature sharing\nmechanism. Our approach focuses on the performance increases on high IoU but\ndecreases on low IoU thresholds--a key problem this detector suffers from.\nFeature sharing is extremely helpful, our results show that given this\nmechanism embedded into all stages, we can easily narrow the gap between the\nlast stage and preceding stages on low IoU thresholds without resorting to the\ncommonly used testing ensemble but the network itself. We also observe obvious\nimprovements on all IoU thresholds benefited from feature sharing, and the\nresulting cascade structure can easily match or exceed its counterparts, only\nwith negligible extra parameters introduced. To push the envelope, we\ndemonstrate 43.2 AP on COCO object detection without any bells and whistles\nincluding testing ensemble, surpassing previous Cascade R-CNN by a large\nmargin. Our framework is easy to implement and we hope it can serve as a\ngeneral and strong baseline for future research.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 13:57:05 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Li", "Ang", ""], ["Yang", "Xue", ""], ["Zhang", "Chongyang", ""]]}, {"id": "1907.11917", "submitter": "Seong Hun Lee", "authors": "Seong Hun Lee, Javier Civera", "title": "Triangulation: Why Optimize?", "comments": "Accepted to BMVC2019 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, it has been widely accepted that the gold standard for two-view\ntriangulation is to minimize the cost based on reprojection errors. In this\nwork, we challenge this idea. We propose a novel alternative to the classic\nmidpoint method that leads to significantly lower 2D errors and parallax\nerrors. It provides a numerically stable closed-form solution based solely on a\npair of backprojected rays. Since our solution is rotationally invariant, it\ncan also be applied for fisheye and omnidirectional cameras. We show that for\nsmall parallax angles, our method outperforms the state-of-the-art in terms of\ncombined 2D, 3D and parallax accuracy, while achieving comparable speed.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 14:08:09 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 10:28:20 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Lee", "Seong Hun", ""], ["Civera", "Javier", ""]]}, {"id": "1907.11921", "submitter": "Wei Peng", "authors": "Zitong Yu, Wei Peng, Xiaobai Li, Xiaopeng Hong, Guoying Zhao", "title": "Remote Heart Rate Measurement from Highly Compressed Facial Videos: an\n  End-to-end Deep Learning Solution with Video Enhancement", "comments": "IEEE ICCV2019, accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Remote photoplethysmography (rPPG), which aims at measuring heart activities\nwithout any contact, has great potential in many applications (e.g., remote\nhealthcare). Existing rPPG approaches rely on analyzing very fine details of\nfacial videos, which are prone to be affected by video compression. Here we\npropose a two-stage, end-to-end method using hidden rPPG information\nenhancement and attention networks, which is the first attempt to counter video\ncompression loss and recover rPPG signals from highly compressed videos. The\nmethod includes two parts: 1) a Spatio-Temporal Video Enhancement Network\n(STVEN) for video enhancement, and 2) an rPPG network (rPPGNet) for rPPG signal\nrecovery. The rPPGNet can work on its own for robust rPPG measurement, and the\nSTVEN network can be added and jointly trained to further boost the performance\nespecially on highly compressed videos. Comprehensive experiments are performed\non two benchmark datasets to show that, 1) the proposed method not only\nachieves superior performance on compressed videos with high-quality videos\npair, 2) it also generalizes well on novel data with only compressed videos\navailable, which implies the promising potential for real world applications.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 14:22:57 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Yu", "Zitong", ""], ["Peng", "Wei", ""], ["Li", "Xiaobai", ""], ["Hong", "Xiaopeng", ""], ["Zhao", "Guoying", ""]]}, {"id": "1907.11922", "submitter": "Ziwei Liu", "authors": "Cheng-Han Lee, Ziwei Liu, Lingyun Wu, Ping Luo", "title": "MaskGAN: Towards Diverse and Interactive Facial Image Manipulation", "comments": "To appear in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2020. The code, models and dataset are available at:\n  https://github.com/switchablenorms/CelebAMask-HQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial image manipulation has achieved great progress in recent years.\nHowever, previous methods either operate on a predefined set of face attributes\nor leave users little freedom to interactively manipulate images. To overcome\nthese drawbacks, we propose a novel framework termed MaskGAN, enabling diverse\nand interactive face manipulation. Our key insight is that semantic masks serve\nas a suitable intermediate representation for flexible face manipulation with\nfidelity preservation. MaskGAN has two main components: 1) Dense Mapping\nNetwork (DMN) and 2) Editing Behavior Simulated Training (EBST). Specifically,\nDMN learns style mapping between a free-form user modified mask and a target\nimage, enabling diverse generation results. EBST models the user editing\nbehavior on the source mask, making the overall framework more robust to\nvarious manipulated inputs. Specifically, it introduces dual-editing\nconsistency as the auxiliary supervision signal. To facilitate extensive\nstudies, we construct a large-scale high-resolution face dataset with\nfine-grained mask annotations named CelebAMask-HQ. MaskGAN is comprehensively\nevaluated on two challenging tasks: attribute transfer and style copy,\ndemonstrating superior performance over other state-of-the-art methods. The\ncode, models, and dataset are available at\nhttps://github.com/switchablenorms/CelebAMask-HQ.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 14:23:19 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 05:34:29 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Lee", "Cheng-Han", ""], ["Liu", "Ziwei", ""], ["Wu", "Lingyun", ""], ["Luo", "Ping", ""]]}, {"id": "1907.11935", "submitter": "Jakub Nalepa", "authors": "Jakub Nalepa, Lukasz Tulczyjew, Michal Myller, Michal Kawulok", "title": "Segmenting Hyperspectral Images Using Spectral-Spatial Convolutional\n  Neural Networks With Training-Time Data Augmentation", "comments": "Submitted to IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging provides detailed information about the scanned\nobjects, as it captures their spectral characteristics within a large number of\nwavelength bands. Classification of such data has become an active research\ntopic due to its wide applicability in a variety of fields. Deep learning has\nestablished the state of the art in the area, and it constitutes the current\nresearch mainstream. In this letter, we introduce a new spectral-spatial\nconvolutional neural network, benefitting from a battery of data augmentation\ntechniques which help deal with a real-life problem of lacking ground-truth\ntraining data. Our rigorous experiments showed that the proposed method\noutperforms other spectral-spatial techniques from the literature, and delivers\nprecise hyperspectral classification in real time.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 15:32:10 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Nalepa", "Jakub", ""], ["Tulczyjew", "Lukasz", ""], ["Myller", "Michal", ""], ["Kawulok", "Michal", ""]]}, {"id": "1907.11943", "submitter": "Guangcong Wang", "authors": "Guangcong Wang and Jianhuang Lai and Wenqi Liang and Guangrun Wang", "title": "Learnable Parameter Similarity", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing approaches focus on specific visual tasks while ignoring\nthe relations between them. Estimating task relation sheds light on the\nlearning of high-order semantic concepts, e.g., transfer learning. How to\nreveal the underlying relations between different visual tasks remains largely\nunexplored. In this paper, we propose a novel \\textbf{L}earnable\n\\textbf{P}arameter \\textbf{S}imilarity (\\textbf{LPS}) method that learns an\neffective metric to measure the similarity of second-order semantics hidden in\ntrained models. LPS is achieved by using a second-order neural network to align\nhigh-dimensional model parameters and learning second-order similarity in an\nend-to-end way. In addition, we create a model set called ModelSet500 as a\nparameter similarity learning benchmark that contains 500 trained models.\nExtensive experiments on ModelSet500 validate the effectiveness of the proposed\nmethod. Code will be released at\n\\url{https://github.com/Wanggcong/learnable-parameter-similarity}.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 16:14:08 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Wang", "Guangcong", ""], ["Lai", "Jianhuang", ""], ["Liang", "Wenqi", ""], ["Wang", "Guangrun", ""]]}, {"id": "1907.11955", "submitter": "Yusuke Yoshiyasu", "authors": "Yusuke Yoshiyasu and Lucas Gamez", "title": "Learning Body Shape and Pose from Dense Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of learning 3D human pose and body\nshape from 2D image dataset, without having to use 3D dataset (body shape and\npose). The idea is to use dense correspondences between image points and a body\nsurface, which can be annotated on in-the wild 2D images, and extract and\naggregate 3D information from them. To do so, we propose a training strategy\ncalled ``deform-and-learn\" where we alternate deformable surface registration\nand training of deep convolutional neural networks (ConvNets). Unlike previous\napproaches, our method does not require 3D pose annotations from a motion\ncapture (MoCap) system or human intervention to validate 3D pose annotations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 17:42:46 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Yoshiyasu", "Yusuke", ""], ["Gamez", "Lucas", ""]]}, {"id": "1907.11980", "submitter": "Seyed Mehdi Iranmanesh", "authors": "Seyed Mehdi Iranmanesh, Nasser M. Nasrabadi", "title": "Attribute-Guided Deep Polarimetric Thermal-to-visible Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an attribute-guided deep coupled learning framework\nto address the problem of matching polarimetric thermal face photos against a\ngallery of visible faces. The coupled framework contains two sub-networks, one\ndedicated to the visible spectrum and the second sub-network dedicated to the\npolarimetric thermal spectrum. Each sub-network is made of a generative\nadversarial network (GAN) architecture. We propose a novel Attribute-Guided\nCoupled Generative Adversarial Network (AGC-GAN) architecture which utilizes\nfacial attributes to improve the thermal-to-visible face recognition\nperformance. The proposed AGC-GAN exploits the facial attributes and leverages\nmultiple loss functions in order to learn rich discriminative features in a\ncommon embedding subspace. To achieve a realistic photo reconstruction while\npreserving the discriminative information, we also add a perceptual loss term\nto the coupling loss function. An ablation study is performed to show the\neffectiveness of different loss functions for optimizing the proposed method.\nMoreover, the superiority of the model compared to the state-of-the-art models\nis demonstrated using polarimetric dataset.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 21:14:30 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Iranmanesh", "Seyed Mehdi", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1907.12005", "submitter": "Xavier Francis", "authors": "Xavier Francis, Hamid Sharifzadeh, Angus Newton, Nilufar Baghaei,\n  Soheil Varastehpour", "title": "Learning Wear Patterns on Footwear Outsoles Using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Footwear outsoles acquire characteristics unique to the individual wearing\nthem over time. Forensic scientists largely rely on their skills and knowledge,\ngained through years of experience, to analyse such characteristics on a\nshoeprint. In this work, we present a convolutional neural network model that\ncan predict the wear pattern on a unique dataset of shoeprints that captures\nthe life and wear of a pair of shoes. We present an additional architecture\nable to reconstruct the outsole back to its original state on a given week, and\nprovide empirical evaluations of the performance of both models.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 03:42:23 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Francis", "Xavier", ""], ["Sharifzadeh", "Hamid", ""], ["Newton", "Angus", ""], ["Baghaei", "Nilufar", ""], ["Varastehpour", "Soheil", ""]]}, {"id": "1907.12006", "submitter": "Tianyu Yang", "authors": "Tianyu Yang, Pengfei Xu, Runbo Hu, Hua Chai and Antoni B. Chan", "title": "ROAM: Recurrently Optimizing Tracking Model", "comments": "CVPR2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a tracking model consisting of response generation\nand bounding box regression, where the first component produces a heat map to\nindicate the presence of the object at different positions and the second part\nregresses the relative bounding box shifts to anchors mounted on sliding-window\nlocations. Thanks to the resizable convolutional filters used in both\ncomponents to adapt to the shape changes of objects, our tracking model does\nnot need to enumerate different sized anchors, thus saving model parameters. To\neffectively adapt the model to appearance variations, we propose to offline\ntrain a recurrent neural optimizer to update tracking model in a meta-learning\nsetting, which can converge the model in a few gradient steps. This improves\nthe convergence speed of updating the tracking model while achieving better\nperformance. We extensively evaluate our trackers, ROAM and ROAM++, on the OTB,\nVOT, LaSOT, GOT-10K and TrackingNet benchmark and our methods perform favorably\nagainst state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 03:50:05 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 08:29:13 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 06:28:26 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Yang", "Tianyu", ""], ["Xu", "Pengfei", ""], ["Hu", "Runbo", ""], ["Chai", "Hua", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1907.12013", "submitter": "Thomas Vandal", "authors": "Thomas Vandal and Ramakrishna Nemani", "title": "Temporal Interpolation of Geostationary Satellite Imagery with Task\n  Specific Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of satellite data in areas such as weather tracking and\nmodeling, ecosystem monitoring, wildfire detection, and land-cover change are\nheavily dependent on the trade-offs to spatial, spectral and temporal\nresolutions of observations. In weather tracking, high-frequency temporal\nobservations are critical and used to improve forecasts, study severe events,\nand extract atmospheric motion, among others. However, while the current\ngeneration of geostationary satellites have hemispheric coverage at 10-15\nminute intervals, higher temporal frequency observations are ideal for studying\nmesoscale severe weather events. In this work, we apply a task specific optical\nflow approach to temporal up-sampling using deep convolutional neural networks.\nWe apply this technique to 16-bands of GOES-R/Advanced Baseline Imager\nmesoscale dataset to temporally enhance full disk hemispheric snapshots of\ndifferent spatial resolutions from 15 minutes to 1 minute. Experiments show the\neffectiveness of task specific optical flow and multi-scale blocks for\ninterpolating high-frequency severe weather events relative to bilinear and\nglobal optical flow baselines. Lastly, we demonstrate strong performance in\ncapturing variability during a convective precipitation events.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 04:44:50 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 22:47:30 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 01:34:09 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Vandal", "Thomas", ""], ["Nemani", "Ramakrishna", ""]]}, {"id": "1907.12016", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Parthipan Siva, Paul Marchwica, Alexander Wong", "title": "Fairest of Them All: Establishing a Strong Baseline for Cross-Domain\n  Person ReID", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) remains a very difficult challenge in\ncomputer vision, and critical for large-scale video surveillance scenarios\nwhere an individual could appear in different camera views at different times.\nThere has been recent interest in tackling this challenge using cross-domain\napproaches, which leverages data from source domains that are different than\nthe target domain. Such approaches are more practical for real-world widespread\ndeployment given that they don't require on-site training (as with unsupervised\nor domain transfer approaches) or on-site manual annotation and training (as\nwith supervised approaches). In this study, we take a systematic approach to\nestablishing a large baseline source domain and target domain for cross-domain\nperson ReID. We accomplish this by conducting a comprehensive analysis to study\nthe similarities between source domains proposed in literature, and studying\nthe effects of incrementally increasing the size of the source domain. This\nallows us to establish a balanced source domain and target domain split that\npromotes variety in both source and target domains. Furthermore, using lessons\nlearned from the state-of-the-art supervised person re-identification methods,\nwe establish a strong baseline method for cross-domain person ReID. Experiments\nshow that a source domain composed of two of the largest person ReID domains\n(SYSU and MSMT) performs well across six commonly-used target domains.\nFurthermore, we show that, surprisingly, two of the recent commonly-used\ndomains (PRID and GRID) have too few query images to provide meaningful\ninsights. As such, based on our findings, we propose the following balanced\nbaseline for cross-domain person ReID consisting of: i) a fixed multi-source\ndomain consisting of SYSU, MSMT, Airport and 3DPeS, and ii) a multi-target\ndomain consisting of Market-1501, DukeMTMC-reID, CUHK03, PRID, GRID and VIPeR.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 05:20:34 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 18:25:34 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Kumar", "Devinder", ""], ["Siva", "Parthipan", ""], ["Marchwica", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "1907.12021", "submitter": "William Yang Wang", "authors": "Pushkar Shukla, Carlos Elmadjian, Richika Sharan, Vivek Kulkarni,\n  Matthew Turk, William Yang Wang", "title": "What Should I Ask? Using Conversationally Informative Rewards for\n  Goal-Oriented Visual Dialog", "comments": "Accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to engage in goal-oriented conversations has allowed humans to\ngain knowledge, reduce uncertainty, and perform tasks more efficiently.\nArtificial agents, however, are still far behind humans in having goal-driven\nconversations. In this work, we focus on the task of goal-oriented visual\ndialogue, aiming to automatically generate a series of questions about an image\nwith a single objective. This task is challenging since these questions must\nnot only be consistent with a strategy to achieve a goal, but also consider the\ncontextual information in the image. We propose an end-to-end goal-oriented\nvisual dialogue system, that combines reinforcement learning with regularized\ninformation gain. Unlike previous approaches that have been proposed for the\ntask, our work is motivated by the Rational Speech Act framework, which models\nthe process of human inquiry to reach a goal. We test the two versions of our\nmodel on the GuessWhat?! dataset, obtaining significant results that outperform\nthe current state-of-the-art models in the task of generating questions to find\nan undisclosed object in an image.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 06:15:35 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Shukla", "Pushkar", ""], ["Elmadjian", "Carlos", ""], ["Sharan", "Richika", ""], ["Kulkarni", "Vivek", ""], ["Turk", "Matthew", ""], ["Wang", "William Yang", ""]]}, {"id": "1907.12022", "submitter": "Zongyue Zhao", "authors": "Zongyue Zhao, Min Liu, Karthik Ramani", "title": "DAR-Net: Dynamic Aggregation Network for Semantic Scene Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional grid/neighbor-based static pooling has become a constraint for\npoint cloud geometry analysis. In this paper, we propose DAR-Net, a novel\nnetwork architecture that focuses on dynamic feature aggregation. The central\nidea of DAR-Net is generating a self-adaptive pooling skeleton that considers\nboth scene complexity and local geometry features. Providing variable\nsemi-local receptive fields and weights, the skeleton serves as a bridge that\nconnect local convolutional feature extractors and a global recurrent feature\nintegrator. Experimental results on indoor scene datasets show advantages of\nthe proposed approach compared to state-of-the-art architectures that adopt\nstatic pooling methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 06:23:19 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 03:13:59 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Zhao", "Zongyue", ""], ["Liu", "Min", ""], ["Ramani", "Karthik", ""]]}, {"id": "1907.12023", "submitter": "Weisen Wang", "authors": "Weisen Wang, Zhiyan Xu, Weihong Yu, Jianchun Zhao, Jingyuan Yang, Feng\n  He, Zhikun Yang, Di Chen, Dayong Ding, Youxin Chen, Xirong Li", "title": "Two-Stream CNN with Loose Pair Training for Multi-modal AMD\n  Categorization", "comments": "accepted by MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies automated categorization of age-related macular\ndegeneration (AMD) given a multi-modal input, which consists of a color fundus\nimage and an optical coherence tomography (OCT) image from a specific eye.\nPrevious work uses a traditional method, comprised of feature extraction and\nclassifier training that cannot be optimized jointly. By contrast, we propose a\ntwo-stream convolutional neural network (CNN) that is end-to-end. The CNN's\nfusion layer is tailored to the need of fusing information from the fundus and\nOCT streams. For generating more multi-modal training instances, we introduce\nLoose Pair training, where a fundus image and an OCT image are paired based on\nclass labels rather than eyes. Moreover, for a visual interpretation of how the\nindividual modalities make contributions, we extend the class activation\nmapping technique to the multi-modal scenario. Experiments on a real-world\ndataset collected from an outpatient clinic justify the viability of our\nproposal for multi-modal AMD categorization.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 06:27:01 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Wang", "Weisen", ""], ["Xu", "Zhiyan", ""], ["Yu", "Weihong", ""], ["Zhao", "Jianchun", ""], ["Yang", "Jingyuan", ""], ["He", "Feng", ""], ["Yang", "Zhikun", ""], ["Chen", "Di", ""], ["Ding", "Dayong", ""], ["Chen", "Youxin", ""], ["Li", "Xirong", ""]]}, {"id": "1907.12046", "submitter": "Francis Engelmann", "authors": "Francis Engelmann, Theodora Kontogianni, Bastian Leibe", "title": "Dilated Point Convolutions: On the Receptive Field Size of Point\n  Convolutions on 3D Point Clouds", "comments": "ICRA 2020 Video https://www.youtube.com/watch?v=JDfFmuOvMkM Project\n  https://francisengelmann.github.io/DPC/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose Dilated Point Convolutions (DPC). In a thorough\nablation study, we show that the receptive field size is directly related to\nthe performance of 3D point cloud processing tasks, including semantic\nsegmentation and object classification. Point convolutions are widely used to\nefficiently process 3D data representations such as point clouds or graphs.\nHowever, we observe that the receptive field size of recent point convolutional\nnetworks is inherently limited. Our dilated point convolutions alleviate this\nissue, they significantly increase the receptive field size of point\nconvolutions. Importantly, our dilation mechanism can easily be integrated into\nmost existing point convolutional networks. To evaluate the resulting network\narchitectures, we visualize the receptive field and report competitive scores\non popular point cloud benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 08:52:41 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 22:09:54 GMT"}, {"version": "v3", "created": "Sat, 23 May 2020 16:11:30 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Engelmann", "Francis", ""], ["Kontogianni", "Theodora", ""], ["Leibe", "Bastian", ""]]}, {"id": "1907.12056", "submitter": "Yunhe Gao", "authors": "Yunhe Gao, Rui Huang, Ming Chen, Zhe Wang, Jincheng Deng, Yuanyuan\n  Chen, Yiwei Yang, Jie Zhang, Chanjuan Tao and Hongsheng Li", "title": "FocusNet: Imbalanced Large and Small Organ Segmentation with an\n  End-to-End Deep Neural Network for Head and Neck CT Images", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end deep neural network for solving the\nproblem of imbalanced large and small organ segmentation in head and neck (HaN)\nCT images. To conduct radiotherapy planning for nasopharyngeal cancer, more\nthan 10 organs-at-risk (normal organs) need to be precisely segmented in\nadvance. However, the size ratio between large and small organs in the head\ncould reach hundreds. Directly using such imbalanced organ annotations to train\ndeep neural networks generally leads to inaccurate small-organ label maps. We\npropose a novel end-to-end deep neural network to solve this challenging\nproblem by automatically locating, ROI-pooling, and segmenting small organs\nwith specifically designed small-organ sub-networks while maintaining the\naccuracy of large organ segmentation. A strong main network with densely\nconnected atrous spatial pyramid pooling and squeeze-and-excitation modules is\nused for segmenting large organs, where large organs' label maps are directly\noutput. For small organs, their probabilistic locations instead of label maps\nare estimated by the main network. High-resolution and multi-scale feature\nvolumes for each small organ are ROI-pooled according to their locations and\nare fed into small-organ networks for accurate segmenting small organs. Our\nproposed network is extensively tested on both collected real data and the\n\\emph{MICCAI Head and Neck Auto Segmentation Challenge 2015} dataset, and shows\nsuperior performance compared with state-of-the-art segmentation methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 09:45:19 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Gao", "Yunhe", ""], ["Huang", "Rui", ""], ["Chen", "Ming", ""], ["Wang", "Zhe", ""], ["Deng", "Jincheng", ""], ["Chen", "Yuanyuan", ""], ["Yang", "Yiwei", ""], ["Zhang", "Jie", ""], ["Tao", "Chanjuan", ""], ["Li", "Hongsheng", ""]]}, {"id": "1907.12087", "submitter": "Nupur Kumari", "authors": "Puneet Mangla, Mayank Singh, Abhishek Sinha, Nupur Kumari, Vineeth N\n  Balasubramanian, Balaji Krishnamurthy", "title": "Charting the Right Manifold: Manifold Mixup for Few-shot Learning", "comments": "WACV 2020, Code: https://github.com/nupurkmr9/S2M2_fewshot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning algorithms aim to learn model parameters capable of\nadapting to unseen classes with the help of only a few labeled examples. A\nrecent regularization technique - Manifold Mixup focuses on learning a\ngeneral-purpose representation, robust to small changes in the data\ndistribution. Since the goal of few-shot learning is closely linked to robust\nrepresentation learning, we study Manifold Mixup in this problem setting.\nSelf-supervised learning is another technique that learns semantically\nmeaningful features, using only the inherent structure of the data. This work\ninvestigates the role of learning relevant feature manifold for few-shot tasks\nusing self-supervision and regularization techniques. We observe that\nregularizing the feature manifold, enriched via self-supervised techniques,\nwith Manifold Mixup significantly improves few-shot learning performance. We\nshow that our proposed method S2M2 beats the current state-of-the-art accuracy\non standard few-shot learning datasets like CIFAR-FS, CUB, mini-ImageNet and\ntiered-ImageNet by 3-8 %. Through extensive experimentation, we show that the\nfeatures learned using our approach generalize to complex few-shot evaluation\ntasks, cross-domain scenarios and are robust against slight changes to data\ndistribution.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 14:14:55 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 19:40:53 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 08:02:11 GMT"}, {"version": "v4", "created": "Sat, 18 Jan 2020 20:01:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mangla", "Puneet", ""], ["Singh", "Mayank", ""], ["Sinha", "Abhishek", ""], ["Kumari", "Nupur", ""], ["Balasubramanian", "Vineeth N", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1907.12112", "submitter": "Marco Carraro", "authors": "Alessandro Malaguti, Marco Carraro, Mattia Guidolin, Luca\n  Tagliapietra, Emanuele Menegatti, Stefano Ghidoni", "title": "Real-time Tracking-by-Detection of Human Motion in RGB-D Camera Networks", "comments": "Accepted to IEEE SMC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel real-time tracking system capable of improving\nbody pose estimation algorithms in distributed camera networks. The first stage\nof our approach introduces a linear Kalman filter operating at the body joints\nlevel, used to fuse single-view body poses coming from different detection\nnodes of the network and to ensure temporal consistency between them. The\nsecond stage, instead, refines the Kalman filter estimates by fitting a\nhierarchical model of the human body having constrained link sizes in order to\nensure the physical consistency of the tracking. The effectiveness of the\nproposed approach is demonstrated through a broad experimental validation,\nperformed on a set of sequences whose ground truth references are generated by\na commercial marker-based motion capture system. The obtained results show how\nthe proposed system outperforms the considered state-of-the-art approaches,\ngranting accurate and reliable estimates. Moreover, the developed methodology\nconstrains neither the number of persons to track, nor the number, position,\nsynchronization, frame-rate, and manufacturer of the RGB-D cameras used.\nFinally, the real-time performances of the system are of paramount importance\nfor a large number of real-world applications.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 17:23:50 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Malaguti", "Alessandro", ""], ["Carraro", "Marco", ""], ["Guidolin", "Mattia", ""], ["Tagliapietra", "Luca", ""], ["Menegatti", "Emanuele", ""], ["Ghidoni", "Stefano", ""]]}, {"id": "1907.12122", "submitter": "Elad Richardson", "authors": "Elad Richardson, Yaniv Azar, Or Avioz, Niv Geron, Tomer Ronen, Zach\n  Avraham, Stav Shapiro", "title": "It's All About The Scale -- Efficient Text Detection Using Adaptive\n  Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Text can appear anywhere\". This property requires us to carefully process\nall the pixels in an image in order to accurately localize all text instances.\nIn particular, for the more difficult task of localizing small text regions,\nmany methods use an enlarged image or even several rescaled ones as their\ninput. This significantly increases the processing time of the entire image and\nneedlessly enlarges background regions. If we were to have a prior telling us\nthe coarse location of text instances in the image and their approximate scale,\nwe could have adaptively chosen which regions to process and how to rescale\nthem, thus significantly reducing the processing time. To estimate this prior\nwe propose a segmentation-based network with an additional \"scale predictor\",\nan output channel that predicts the scale of each text segment. The network is\napplied on a scaled down image to efficiently approximate the desired prior,\nwithout processing all the pixels of the original image. The approximated prior\nis then used to create a compact image containing only text regions, resized to\na canonical scale, which is fed again to the segmentation network for\nfine-grained detection. We show that our approach offers a powerful alternative\nto fixed scaling schemes, achieving an equivalent accuracy to larger input\nscales while processing far fewer pixels. Qualitative and quantitative results\nare presented on the ICDAR15 and ICDAR17 MLT benchmarks to validate our\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 18:32:08 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Richardson", "Elad", ""], ["Azar", "Yaniv", ""], ["Avioz", "Or", ""], ["Geron", "Niv", ""], ["Ronen", "Tomer", ""], ["Avraham", "Zach", ""], ["Shapiro", "Stav", ""]]}, {"id": "1907.12133", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Wei-Lun Chao, Dong Xuan", "title": "An Empirical Study on Leveraging Scene Graphs for Visual Question\n  Answering", "comments": "Accepted as oral presentation at BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (Visual QA) has attracted significant attention\nthese years. While a variety of algorithms have been proposed, most of them are\nbuilt upon different combinations of image and language features as well as\nmulti-modal attention and fusion. In this paper, we investigate an alternative\napproach inspired by conventional QA systems that operate on knowledge graphs.\nSpecifically, we investigate the use of scene graphs derived from images for\nVisual QA: an image is abstractly represented by a graph with nodes\ncorresponding to object entities and edges to object relationships. We adapt\nthe recently proposed graph network (GN) to encode the scene graph and perform\nstructured reasoning according to the input question. Our empirical studies\ndemonstrate that scene graphs can already capture essential information of\nimages and graph networks have the potential to outperform state-of-the-art\nVisual QA algorithms but with a much cleaner architecture. By analyzing the\nfeatures generated by GNs we can further interpret the reasoning process,\nsuggesting a promising direction towards explainable Visual QA.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 19:59:20 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhang", "Cheng", ""], ["Chao", "Wei-Lun", ""], ["Xuan", "Dong", ""]]}, {"id": "1907.12145", "submitter": "Mahdi Salarian mr", "authors": "Shideh Homayon, Mahdi Salarian", "title": "Iris Recognition for Personal Identification using LAMSTAR neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris recognition is one of the most important biometric recognition method.\nThis is because the iris texture provides many features such as freckles,\ncoronas, stripes, furrows, crypts, etc. Those features are unique for different\npeople and distinguishable. Such unique features in the anatomical structure of\nthe iris make it possible the differentiation among individuals. So during last\nyears huge number of people have been trying to improve its performance. In\nthis article first different common steps for the Iris recognition system is\nexplained. Then a special type of neural network is used for recognition part.\nExperimental results show high accuracy can be obtained especially when the\nprimary steps are done well.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 22:04:48 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Homayon", "Shideh", ""], ["Salarian", "Mahdi", ""]]}, {"id": "1907.12165", "submitter": "Hugh Kennedy Dr.", "authors": "Hugh L Kennedy", "title": "On the Realization and Analysis of Circular Harmonic Transforms for\n  Feature Detection", "comments": "A new section on parallel software implementation (MATLAB, C++ and\n  CUDA) was added to this draft version. Manuscript was then accepted for\n  publication in Journal of Real-Time Image Processing, special issue on\n  Real-Time Statistical Image and Video Processing for Remote Sensing and\n  Surveillance Applications", "journal-ref": null, "doi": "10.1007/s11554-020-01040-4", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circular-harmonic spectra are a compact representation of local image\nfeatures in two dimensions. It is well known that the computational complexity\nof such transforms is greatly reduced when polar separability is exploited in\nsteerable filter-banks. Further simplifications are possible when Cartesian\nseparability is incorporated using the radial apodization (i.e. weight, window,\nor taper) described here, as a consequence of the Laguerre/Hermite\ncorrespondence over polar/Cartesian coordinates. The chosen form also mitigates\nundesirable discretization artefacts due to angular aliasing. The possible\nutility of circular-harmonic spectra for the description of simple features is\nillustrated using real data from an airborne electro-optic sensor. The spectrum\nis deployed in a test-statistic to detect and characterize corners of arbitrary\nangle and orientation (i.e. wedges). The test-statistic considers uncertainty\ndue to finite sampling and clutter/noise.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 00:20:17 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 23:17:10 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 03:50:55 GMT"}, {"version": "v4", "created": "Fri, 10 Jan 2020 06:10:05 GMT"}, {"version": "v5", "created": "Fri, 6 Nov 2020 10:25:09 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Kennedy", "Hugh L", ""]]}, {"id": "1907.12176", "submitter": "Jun Xiang", "authors": "Jun Xiang, Ma Chao, Guohan Xu, Jianhua Hou", "title": "End-to-End Learning Deep CRF models for Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep multi-object tracking (MOT) approaches first learn a deep\nrepresentation to describe target objects and then associate detection results\nby optimizing a linear assignment problem. Despite demonstrated successes, it\nis challenging to discriminate target objects under mutual occlusion or to\nreduce identity switches in crowded scenes. In this paper, we propose learning\ndeep conditional random field (CRF) networks, aiming to model the assignment\ncosts as unary potentials and the long-term dependencies among detection\nresults as pairwise potentials. Specifically, we use a bidirectional long\nshort-term memory (LSTM) network to encode the long-term dependencies. We pose\nthe CRF inference as a recurrent neural network learning process using the\nstandard gradient descent algorithm, where unary and pairwise potentials are\njointly optimized in an end-to-end manner. Extensive experimental results on\nthe challenging MOT datasets including MOT-2015 and MOT-2016, demonstrate that\nour approach achieves the state of the art performances in comparison with\npublished works on both benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 02:27:18 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Xiang", "Jun", ""], ["Chao", "Ma", ""], ["Xu", "Guohan", ""], ["Hou", "Jianhua", ""]]}, {"id": "1907.12192", "submitter": "Jia Peng", "authors": "Peng Jia and Yi Huang and Bojun Cai and Dongmei Cai", "title": "Solar Image Restoration with the Cycle-GAN Based on Multi-Fractal\n  Properties of Texture Features", "comments": "Accepted by APJ Letters", "journal-ref": null, "doi": "10.3847/2041-8213/ab365f", "report-no": null, "categories": "astro-ph.IM astro-ph.SR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is one of the most obvious characteristics in solar images and it is\nnormally described by texture features. Because textures from solar images of\nthe same wavelength are similar, we assume texture features of solar images are\nmulti-fractals. Based on this assumption, we propose a pure data-based image\nrestoration method: with several high resolution solar images as references, we\nuse the Cycle-Consistent Adversarial Network to restore burred images of the\nsame steady physical process, in the same wavelength obtained by the same\ntelescope. We test our method with simulated and real observation data and find\nthat our method can improve the spatial resolution of solar images, without\nloss of any frames. Because our method does not need paired training set or\nadditional instruments, it can be used as a post-processing method for solar\nimages obtained by either seeing limited telescopes or telescopes with ground\nlayer adaptive optic system.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 03:09:32 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 08:33:44 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Jia", "Peng", ""], ["Huang", "Yi", ""], ["Cai", "Bojun", ""], ["Cai", "Dongmei", ""]]}, {"id": "1907.12193", "submitter": "Jun Wan", "authors": "Jun Wan, Chi Lin, Longyin Wen, Yunan Li, Qiguang Miao, Sergio\n  Escalera, Gholamreza Anbarjafari, Isabelle Guyon, Guodong Guo, Stan Z. Li", "title": "ChaLearn Looking at People: IsoGD and ConGD Large-scale RGB-D Gesture\n  Recognition", "comments": "14 pages, 8 figures, 6 tables", "journal-ref": "IEEE Transactions on Cybernetics 2020", "doi": "10.1109/TCYB.2020.3012092", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ChaLearn large-scale gesture recognition challenge has been run twice in\ntwo workshops in conjunction with the International Conference on Pattern\nRecognition (ICPR) 2016 and International Conference on Computer Vision (ICCV)\n2017, attracting more than $200$ teams round the world. This challenge has two\ntracks, focusing on isolated and continuous gesture recognition, respectively.\nThis paper describes the creation of both benchmark datasets and analyzes the\nadvances in large-scale gesture recognition based on these two datasets. We\ndiscuss the challenges of collecting large-scale ground-truth annotations of\ngesture recognition, and provide a detailed analysis of the current\nstate-of-the-art methods for large-scale isolated and continuous gesture\nrecognition based on RGB-D video sequences. In addition to recognition rate and\nmean jaccard index (MJI) as evaluation metrics used in our previous challenges,\nwe also introduce the corrected segmentation rate (CSR) metric to evaluate the\nperformance of temporal segmentation for continuous gesture recognition.\nFurthermore, we propose a bidirectional long short-term memory (Bi-LSTM)\nbaseline method, determining the video division points based on the skeleton\npoints extracted by convolutional pose machine (CPM). Experiments demonstrate\nthat the proposed Bi-LSTM outperforms the state-of-the-art methods with an\nabsolute improvement of $8.1\\%$ (from $0.8917$ to $0.9639$) of CSR.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 03:09:40 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wan", "Jun", ""], ["Lin", "Chi", ""], ["Wen", "Longyin", ""], ["Li", "Yunan", ""], ["Miao", "Qiguang", ""], ["Escalera", "Sergio", ""], ["Anbarjafari", "Gholamreza", ""], ["Guyon", "Isabelle", ""], ["Guo", "Guodong", ""], ["Li", "Stan Z.", ""]]}, {"id": "1907.12195", "submitter": "Thomas Dag\\`es", "authors": "Thomas Dag\\`es, Michael Lindenbaum, Alfred M. Bruckstein", "title": "Seeing Things in Random-Dot Videos", "comments": "Technical report ; Resubmission: Corrected typos. Rephrased some\n  sentences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans possess an intricate and powerful visual system in order to perceive\nand understand the environing world. Human perception can effortlessly detect\nand correctly group features in visual data and can even interpret random-dot\nvideos induced by imaging natural dynamic scenes with highly noisy sensors such\nas ultrasound imaging. Remarkably, this happens even if perception completely\nfails when the same information is presented frame by frame rather than in a\nvideo sequence. We study this property of surprising dynamic perception with\nthe first goal of proposing a new detection and spatio-temporal grouping\nalgorithm for such signals when, per frame, the information on objects is both\nrandom and sparse and embedded in random noise. The algorithm is based on the\nsuccession of temporal integration and spatial statistical tests of\nunlikeliness, the a contrario framework. The algorithm not only manages to\nhandle such signals but the striking similarity in its performance to the\nperception by human observers, as witnessed by a series of psychophysical\nexperiments on image and video data, leads us to see in it a simple\ncomputational Gestalt model of human perception with only two parameters: the\ntime integration and the visual angle for candidate shapes to be detected.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 03:12:28 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 16:39:13 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Dag\u00e8s", "Thomas", ""], ["Lindenbaum", "Michael", ""], ["Bruckstein", "Alfred M.", ""]]}, {"id": "1907.12209", "submitter": "Chunhua Shen", "authors": "Wei Yin, Yifan Liu, Chunhua Shen, Youliang Yan", "title": "Enforcing geometric constraints of virtual normal for depth prediction", "comments": "Fixed typos. Appearing in Proc. Int. Conf. Computer Vision 2019. Code\n  is available at: https://tinyurl.com/virtualnormal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Monocular depth prediction plays a crucial role in understanding 3D scene\ngeometry. Although recent methods have achieved impressive progress in\nevaluation metrics such as the pixel-wise relative error, most methods neglect\nthe geometric constraints in the 3D space. In this work, we show the importance\nof the high-order 3D geometric constraints for depth prediction. By designing a\nloss term that enforces one simple type of geometric constraints, namely,\nvirtual normal directions determined by randomly sampled three points in the\nreconstructed 3D space, we can considerably improve the depth prediction\naccuracy. Significantly, the byproduct of this predicted depth being\nsufficiently accurate is that we are now able to recover good 3D structures of\nthe scene such as the point cloud and surface normal directly from the depth,\neliminating the necessity of training new sub-models as was previously done.\nExperiments on two benchmarks: NYU Depth-V2 and KITTI demonstrate the\neffectiveness of our method and state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 04:41:53 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 06:15:30 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Yin", "Wei", ""], ["Liu", "Yifan", ""], ["Shen", "Chunhua", ""], ["Yan", "Youliang", ""]]}, {"id": "1907.12219", "submitter": "Dr. Mohammed Javed", "authors": "Bulla Rajesh and Mohammed Javed and P Nagabhushan", "title": "Automatic Text Line Segmentation Directly in JPEG Compressed Document\n  Images", "comments": "Accepted in GCCE2019, Okinawa, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JPEG is one of the popular image compression algorithms that provide\nefficient storage and transmission capabilities in consumer electronics, and\nhence it is the most preferred image format over the internet world. In the\npresent digital and Big-data era, a huge volume of JPEG compressed document\nimages are being archived and communicated through consumer electronics on\ndaily basis. Though it is advantageous to have data in the compressed form on\none side, however, on the other side processing with off-the-shelf methods\nbecomes computationally expensive because it requires decompression and\nrecompression operations. Therefore, it would be novel and efficient, if the\ncompressed data are processed directly in their respective compressed domains\nof consumer electronics. In the present research paper, we propose to\ndemonstrate this idea taking the case study of printed text line segmentation.\nSince, JPEG achieves compression by dividing the image into non overlapping 8x8\nblocks in the pixel domain and using Discrete Cosine Transform (DCT); it is\nvery likely that the partitioned 8x8 DCT blocks overlap the contents of two\nadjacent text-lines without leaving any clue for the line separator, thus\nmaking text-line segmentation a challenging problem. Two approaches of\nsegmentation have been proposed here using the DC projection profile and AC\ncoefficients of each 8x8 DCT block. The first approach is based on the strategy\nof partial decompression of selected DCT blocks, and the second approach is\nwith intelligent analysis of F10 and F11 AC coefficients and without using any\ntype of decompression. The proposed methods have been tested with variable font\nsizes, font style and spacing between lines, and a good performance is\nreported.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 05:32:31 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Rajesh", "Bulla", ""], ["Javed", "Mohammed", ""], ["Nagabhushan", "P", ""]]}, {"id": "1907.12223", "submitter": "Haisheng Su", "authors": "Haisheng Su and Xu Zhao and Shuming Liu", "title": "Multi-Granularity Fusion Network for Proposal and Activity Localization:\n  Submission to ActivityNet Challenge 2019 Task 1 and Task 2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report presents an overview of our solution used in the\nsubmission to ActivityNet Challenge 2019 Task 1 (\\textbf{temporal action\nproposal generation}) and Task 2 (\\textbf{temporal action\nlocalization/detection}). Temporal action proposal indicates the temporal\nintervals containing the actions and plays an important role in temporal action\nlocalization. Top-down and bottom-up methods are the two main categories used\nfor proposal generation in the existing literature. In this paper, we devise a\nnovel Multi-Granularity Fusion Network (MGFN) to combine the proposals\ngenerated from different frameworks for complementary filtering and confidence\nre-ranking. Specifically, we consider the diversity comprehensively from\nmultiple perspectives, e.g. the characteristic aspect, the data aspect, the\nmodel aspect and the result aspect. Our MGFN achieves the state-of-the-art\nperformance on the temporal action proposal task with 69.85 AUC score and the\ntemporal action localization task with 38.90 mAP on the challenge testing set.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 06:10:51 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Su", "Haisheng", ""], ["Zhao", "Xu", ""], ["Liu", "Shuming", ""]]}, {"id": "1907.12237", "submitter": "Aleksei Tiulpin", "authors": "Aleksei Tiulpin and Iaroslav Melekhov and Simo Saarakkala", "title": "KNEEL: Knee Anatomical Landmark Localization Using Hourglass Networks", "comments": "Accepted for Publication at ICCV 2019 VRMI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenge of localization of anatomical landmarks in\nknee X-ray images at different stages of osteoarthritis (OA). Landmark\nlocalization can be viewed as regression problem, where the landmark position\nis directly predicted by using the region of interest or even full-size images\nleading to large memory footprint, especially in case of high resolution\nmedical images. In this work, we propose an efficient deep neural networks\nframework with an hourglass architecture utilizing a soft-argmax layer to\ndirectly predict normalized coordinates of the landmark points. We provide an\nextensive evaluation of different regularization techniques and various loss\nfunctions to understand their influence on the localization performance.\nFurthermore, we introduce the concept of transfer learning from low-budget\nannotations, and experimentally demonstrate that such approach is improving the\naccuracy of landmark localization. Compared to the prior methods, we validate\nour model on two datasets that are independent from the train data and assess\nthe performance of the method for different stages of OA severity. The proposed\napproach demonstrates better generalization performance compared to the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 07:18:54 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 18:54:20 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tiulpin", "Aleksei", ""], ["Melekhov", "Iaroslav", ""], ["Saarakkala", "Simo", ""]]}, {"id": "1907.12244", "submitter": "Rongzhao Zhang", "authors": "Rongzhao Zhang and Albert C.S. Chung", "title": "A Fine-Grain Error Map Prediction and Segmentation Quality Assessment\n  Framework for Whole-Heart Segmentation", "comments": "9 pages, accepted by MICCAI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When introducing advanced image computing algorithms, e.g., whole-heart\nsegmentation, into clinical practice, a common suspicion is how reliable the\nautomatically computed results are. In fact, it is important to find out the\nfailure cases and identify the misclassified pixels so that they can be\nexcluded or corrected for the subsequent analysis or diagnosis. However, it is\nnot a trivial problem to predict the errors in a segmentation mask when ground\ntruth (usually annotated by experts) is absent. In this work, we attempt to\naddress the pixel-wise error map prediction problem and the per-case mask\nquality assessment problem using a unified deep learning (DL) framework.\nSpecifically, we first formalize an error map prediction problem, then we\nconvert it to a segmentation problem and build a DL network to tackle it. We\nalso derive a quality indicator (QI) from a predicted error map to measure the\noverall quality of a segmentation mask. To evaluate the proposed framework, we\nperform extensive experiments on a public whole-heart segmentation dataset,\ni.e., MICCAI 2017 MMWHS. By 5-fold cross validation, we obtain an overall Dice\nscore of 0.626 for the error map prediction task, and observe a high Pearson\ncorrelation coefficient (PCC) of 0.972 between QI and the actual segmentation\naccuracy (Acc), as well as a low mean absolute error (MAE) of 0.0048 between\nthem, which evidences the efficacy of our method in both error map prediction\nand quality assessment.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 07:35:00 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhang", "Rongzhao", ""], ["Chung", "Albert C. S.", ""]]}, {"id": "1907.12250", "submitter": "Minyoung Chung", "authors": "Minyoung Chung, Jingyu Lee, Wisoo Song, Youngchan Song, Il-Hyung Yang,\n  Jeongjin Lee, Yeong-Gil Shin", "title": "Automatic Registration between Cone-Beam CT and Scanned Surface via\n  Deep-Pose Regression Neural Networks and Clustered Similarities", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TMI.2020.3007520", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computerized registration between maxillofacial cone-beam computed tomography\n(CT) images and a scanned dental model is an essential prerequisite in surgical\nplanning for dental implants or orthognathic surgery. We propose a novel method\nthat performs fully automatic registration between a cone-beam CT image and an\noptically scanned model. To build a robust and automatic initial registration\nmethod, our method applies deep-pose regression neural networks in a reduced\ndomain (i.e., 2-dimensional image). Subsequently, fine registration is\nperformed via optimal clusters. Majority voting system achieves globally\noptimal transformations while each cluster attempts to optimize local\ntransformation parameters. The coherency of clusters determines their candidacy\nfor the optimal cluster set. The outlying regions in the iso-surface are\neffectively removed based on the consensus among the optimal clusters. The\naccuracy of registration was evaluated by the Euclidean distance of 10\nlandmarks on a scanned model which were annotated by the experts in the field.\nThe experiments show that the proposed method's registration accuracy, measured\nin landmark distance, outperforms other existing methods by 30.77% to 70%. In\naddition to achieving high accuracy, our proposed method requires neither\nhuman-interactions nor priors (e.g., iso-surface extraction). The main\nsignificance of our study is twofold: 1) the employment of light-weighted\nneural networks which indicates the applicability of neural network in\nextracting pose cues that can be easily obtained and 2) the introduction of an\noptimal cluster-based registration method that can avoid metal artifacts during\nthe matching procedures.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 07:46:01 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chung", "Minyoung", ""], ["Lee", "Jingyu", ""], ["Song", "Wisoo", ""], ["Song", "Youngchan", ""], ["Yang", "Il-Hyung", ""], ["Lee", "Jeongjin", ""], ["Shin", "Yeong-Gil", ""]]}, {"id": "1907.12253", "submitter": "Chuhang Zou", "authors": "Chuhang Zou, Derek Hoiem", "title": "Silhouette Guided Point Cloud Reconstruction beyond Occlusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major challenge in 3D reconstruction is to infer the complete shape\ngeometry from partial foreground occlusions. In this paper, we propose a method\nto reconstruct the complete 3D shape of an object from a single RGB image, with\nrobustness to occlusion. Given the image and a silhouette of the visible\nregion, our approach completes the silhouette of the occluded region and then\ngenerates a point cloud. We show improvements for reconstruction of\nnon-occluded and partially occluded objects by providing the predicted complete\nsilhouette as guidance. We also improve state-of-the-art for 3D shape\nprediction with a 2D reprojection loss from multiple synthetic views and a\nsurface-based smoothing and refinement step. Experiments demonstrate the\nefficacy of our approach both quantitatively and qualitatively on synthetic and\nreal scene datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:00:08 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zou", "Chuhang", ""], ["Hoiem", "Derek", ""]]}, {"id": "1907.12256", "submitter": "Xianyang Li", "authors": "Xianyang Li, Feng Wang, Qinghao Hu, Cong Leng", "title": "AirFace: Lightweight and Efficient Model for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of convolutional neural network, significant progress\nhas been made in computer vision tasks. However, the commonly used loss\nfunction softmax loss and highly efficient network architecture for common\nvisual tasks are not as effective for face recognition. In this paper, we\npropose a novel loss function named Li-ArcFace based on ArcFace. Li-ArcFace\ntakes the value of the angle through linear function as the target logit rather\nthan through cosine function, which has better convergence and performance on\nlow dimensional embedding feature learning for face recognition. In terms of\nnetwork architecture, we improved the the perfomance of MobileFaceNet by\nincreasing the network depth, width and adding attention module. Besides, we\nfound some useful training tricks for face recognition. With all the above\nresults, we won the second place in the deepglint-light challenge of LFR2019.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:04:37 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 05:19:59 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 01:04:20 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Li", "Xianyang", ""], ["Wang", "Feng", ""], ["Hu", "Qinghao", ""], ["Leng", "Cong", ""]]}, {"id": "1907.12271", "submitter": "Damien Teney", "authors": "Damien Teney, Peng Wang, Jiewei Cao, Lingqiao Liu, Chunhua Shen, Anton\n  van den Hengel", "title": "V-PROM: A Benchmark for Visual Reasoning Using Visual Progressive\n  Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary challenges faced by deep learning is the degree to which\ncurrent methods exploit superficial statistics and dataset bias, rather than\nlearning to generalise over the specific representations they have experienced.\nThis is a critical concern because generalisation enables robust reasoning over\nunseen data, whereas leveraging superficial statistics is fragile to even small\nchanges in data distribution. To illuminate the issue and drive progress\ntowards a solution, we propose a test that explicitly evaluates abstract\nreasoning over visual data. We introduce a large-scale benchmark of visual\nquestions that involve operations fundamental to many high-level vision tasks,\nsuch as comparisons of counts and logical operations on complex visual\nproperties. The benchmark directly measures a method's ability to infer\nhigh-level relationships and to generalise them over image-based concepts. It\nincludes multiple training/test splits that require controlled levels of\ngeneralization. We evaluate a range of deep learning architectures, and find\nthat existing models, including those popular for vision-and-language tasks,\nare unable to solve seemingly-simple instances. Models using relational\nnetworks fare better but leave substantial room for improvement.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:28:33 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Teney", "Damien", ""], ["Wang", "Peng", ""], ["Cao", "Jiewei", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1907.12273", "submitter": "Yuhui Yuan", "authors": "Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang, Xilin Chen, Jingdong\n  Wang", "title": "Interlaced Sparse Self-Attention for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a so-called interlaced sparse self-attention\napproach to improve the efficiency of the \\emph{self-attention} mechanism for\nsemantic segmentation. The main idea is that we factorize the dense affinity\nmatrix as the product of two sparse affinity matrices. There are two successive\nattention modules each estimating a sparse affinity matrix. The first attention\nmodule is used to estimate the affinities within a subset of positions that\nhave long spatial interval distances and the second attention module is used to\nestimate the affinities within a subset of positions that have short spatial\ninterval distances. These two attention modules are designed so that each\nposition is able to receive the information from all the other positions. In\ncontrast to the original self-attention module, our approach decreases the\ncomputation and memory complexity substantially especially when processing\nhigh-resolution feature maps. We empirically verify the effectiveness of our\napproach on six challenging semantic segmentation benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:33:32 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 06:33:46 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Huang", "Lang", ""], ["Yuan", "Yuhui", ""], ["Guo", "Jianyuan", ""], ["Zhang", "Chao", ""], ["Chen", "Xilin", ""], ["Wang", "Jingdong", ""]]}, {"id": "1907.12282", "submitter": "Tong Shen", "authors": "Tong Shen, Dong Gong, Wei Zhang, Chunhua Shen, Tao Mei", "title": "Regularizing Proxies with Multi-Adversarial Training for Unsupervised\n  Domain-Adaptive Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a semantic segmentation model requires a large amount of pixel-level\nannotation, hampering its application at scale. With computer graphics, we can\ngenerate almost unlimited training data with precise annotation. However,a deep\nmodel trained with synthetic data usually cannot directly generalize well to\nrealistic images due to domain shift. It has been observed that highly\nconfident labels for the unlabeled real images may be predicted relying on the\nlabeled synthetic data. To tackle the unsupervised domain adaptation problem,\nwe explore the possibilities to generate high-quality labels as proxy labels to\nsupervise the training on target data. Specifically, we propose a novel\nproxy-based method using multi-adversarial training. We first train the model\nusing synthetic data (source domain). Multiple discriminators are used to align\nthe features be-tween the source and target domain (real images) at different\nlevels. Then we focus on obtaining and selecting high-quality proxy labels by\nincorporating both the confidence of the class predictor and that from the\nadversarial discriminators. Our discriminators not only work as a regularizer\nto encourage feature alignment but also provide an alternative confidence\nmeasure for generating proxy labels. Relying on the generated high-quality\nproxies, our model can be trained in a \"supervised manner\" on the target\ndo-main. On two major tasks, GTA5->Cityscapes and SYNTHIA->Cityscapes, our\nmethod achieves state-of-the-art results, outperforming the previous by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:55:22 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Shen", "Tong", ""], ["Gong", "Dong", ""], ["Zhang", "Wei", ""], ["Shen", "Chunhua", ""], ["Mei", "Tao", ""]]}, {"id": "1907.12296", "submitter": "Simone Bonechi", "authors": "Paolo Andreini, Simone Bonechi, Monica Bianchini, Alessandro Mecocci,\n  Franco Scarselli, Andrea Sodi", "title": "A Two Stage GAN for High Resolution Retinal Image Generation and\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the use of deep learning is becoming increasingly popular in\ncomputer vision. However, the effective training of deep architectures usually\nrelies on huge sets of annotated data. This is critical in the medical field\nwhere it is difficult and expensive to obtain annotated images. In this paper,\nwe use Generative Adversarial Networks (GANs) for synthesizing high quality\nretinal images, along with the corresponding semantic label-maps, to be used\ninstead of real images during the training process. Differently from other\nprevious proposals, we suggest a two step approach: first, a progressively\ngrowing GAN is trained to generate the semantic label-maps, which describe the\nblood vessel structure (i.e. vasculature); second, an image-to-image\ntranslation approach is used to obtain realistic retinal images from the\ngenerated vasculature. By using only a handful of training samples, our\napproach generates realistic high resolution images, that can be effectively\nused to enlarge small available datasets. Comparable results have been obtained\nemploying the generated images in place of real data during training. The\npractical viability of the proposed approach has been demonstrated by applying\nit on two well established benchmark sets for retinal vessel segmentation, both\ncontaining a very small number of training samples. Our method obtained better\nperformances with respect to state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 09:37:18 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Andreini", "Paolo", ""], ["Bonechi", "Simone", ""], ["Bianchini", "Monica", ""], ["Mecocci", "Alessandro", ""], ["Scarselli", "Franco", ""], ["Sodi", "Andrea", ""]]}, {"id": "1907.12303", "submitter": "Shuai Chen", "authors": "Shuai Chen, Gerda Bortsova, Antonio Garcia-Uceda Juarez, Gijs van\n  Tulder, and Marleen de Bruijne", "title": "Multi-Task Attention-Based Semi-Supervised Learning for Medical Image\n  Segmentation", "comments": "Accepted at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel semi-supervised image segmentation method that\nsimultaneously optimizes a supervised segmentation and an unsupervised\nreconstruction objectives. The reconstruction objective uses an attention\nmechanism that separates the reconstruction of image areas corresponding to\ndifferent classes. The proposed approach was evaluated on two applications:\nbrain tumor and white matter hyperintensities segmentation. Our method, trained\non unlabeled and a small number of labeled images, outperformed supervised CNNs\ntrained with the same number of images and CNNs pre-trained on unlabeled data.\nIn ablation experiments, we observed that the proposed attention mechanism\nsubstantially improves segmentation performance. We explore two multi-task\ntraining strategies: joint training and alternating training. Alternating\ntraining requires fewer hyperparameters and achieves a better, more stable\nperformance than joint training. Finally, we analyze the features learned by\ndifferent methods and find that the attention mechanism helps to learn more\ndiscriminative features in the deeper layers of encoders.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 09:44:20 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Chen", "Shuai", ""], ["Bortsova", "Gerda", ""], ["Juarez", "Antonio Garcia-Uceda", ""], ["van Tulder", "Gijs", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1907.12336", "submitter": "Umar Riaz Muhammad", "authors": "Umar Riaz Muhammad, Yongxin Yang, Timothy M. Hospedales, Tao Xiang and\n  Yi-Zhe Song", "title": "Goal-Driven Sequential Data Abstraction", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic data abstraction is an important capability for both benchmarking\nmachine intelligence and supporting summarization applications. In the former\none asks whether a machine can `understand' enough about the meaning of input\ndata to produce a meaningful but more compact abstraction. In the latter this\ncapability is exploited for saving space or human time by summarizing the\nessence of input data. In this paper we study a general reinforcement learning\nbased framework for learning to abstract sequential data in a goal-driven way.\nThe ability to define different abstraction goals uniquely allows different\naspects of the input data to be preserved according to the ultimate purpose of\nthe abstraction. Our reinforcement learning objective does not require\nhuman-defined examples of ideal abstraction. Importantly our model processes\nthe input sequence holistically without being constrained by the original input\norder. Our framework is also domain agnostic -- we demonstrate applications to\nsketch, video and text data and achieve promising results in all domains.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 11:24:51 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 10:01:40 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Muhammad", "Umar Riaz", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "1907.12342", "submitter": "Yongsheng Dong", "authors": "Xuelong Li, Hongli Li, and Yongsheng Dong", "title": "Meta Learning for Task-Driven Video Summarization", "comments": "9 pages, 6 figures", "journal-ref": "IEEE Transactions on Industrial Electronics, 2019", "doi": "10.1109/TIE.2019.2931283.", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing video summarization approaches mainly concentrate on sequential or\nstructural characteristic of video data. However, they do not pay enough\nattention to the video summarization task itself. In this paper, we propose a\nmeta learning method for performing task-driven video summarization, denoted by\nMetaL-TDVS, to explicitly explore the video summarization mechanism among\nsummarizing processes on different videos. Particularly, MetaL-TDVS aims to\nexcavate the latent mechanism for summarizing video by reformulating video\nsummarization as a meta learning problem and promote generalization ability of\nthe trained model. MetaL-TDVS regards summarizing each video as a single task\nto make better use of the experience and knowledge learned from processes of\nsummarizing other videos to summarize new ones. Furthermore, MetaL-TDVS updates\nmodels via a two-fold back propagation which forces the model optimized on one\nvideo to obtain high accuracy on another video in every training step.\nExtensive experiments on benchmark datasets demonstrate the superiority and\nbetter generalization ability of MetaL-TDVS against several state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 11:35:27 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Li", "Xuelong", ""], ["Li", "Hongli", ""], ["Dong", "Yongsheng", ""]]}, {"id": "1907.12347", "submitter": "Xiang Li", "authors": "Xiang Li, Tianhan Wei, Yau Pun Chen, Yu-Wing Tai, Chi-Keung Tang", "title": "FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00294", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, we have witnessed the success of deep learning in\nimage recognition thanks to the availability of large-scale human-annotated\ndatasets such as PASCAL VOC, ImageNet, and COCO. Although these datasets have\ncovered a wide range of object categories, there are still a significant number\nof objects that are not included. Can we perform the same task without a lot of\nhuman annotations? In this paper, we are interested in few-shot object\nsegmentation where the number of annotated training examples are limited to 5\nonly. To evaluate and validate the performance of our approach, we have built a\nfew-shot segmentation dataset, FSS-1000, which consists of 1000 object classes\nwith pixelwise annotation of ground-truth segmentation. Unique in FSS-1000, our\ndataset contains significant number of objects that have never been seen or\nannotated in previous datasets, such as tiny daily objects, merchandise,\ncartoon characters, logos, etc. We build our baseline model using standard\nbackbone networks such as VGG-16, ResNet-101, and Inception. To our surprise,\nwe found that training our model from scratch using FSS-1000 achieves\ncomparable and even better results than training with weights pre-trained by\nImageNet which is more than 100 times larger than FSS-1000. Both our approach\nand dataset are simple, effective, and easily extensible to learn segmentation\nof new object classes given very few annotated training examples. Dataset is\navailable at https://github.com/HKUSTCV/FSS-1000.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 11:47:25 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 22:35:58 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Li", "Xiang", ""], ["Wei", "Tianhan", ""], ["Chen", "Yau Pun", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1907.12353", "submitter": "Shengyu Zhao", "authors": "Shengyu Zhao, Yue Dong, Eric I-Chao Chang, Yan Xu", "title": "Recursive Cascaded Networks for Unsupervised Medical Image Registration", "comments": "Accepted to ICCV 2019", "journal-ref": "IEEE International Conference on Computer Vision (ICCV), 2019, pp.\n  10600-10610", "doi": "10.1109/ICCV.2019.01070", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present recursive cascaded networks, a general architecture that enables\nlearning deep cascades, for deformable image registration. The proposed\narchitecture is simple in design and can be built on any base network. The\nmoving image is warped successively by each cascade and finally aligned to the\nfixed image; this procedure is recursive in a way that every cascade learns to\nperform a progressive deformation for the current warped image. The entire\nsystem is end-to-end and jointly trained in an unsupervised manner. In\naddition, enabled by the recursive architecture, one cascade can be iteratively\napplied for multiple times during testing, which approaches a better fit\nbetween each of the image pairs. We evaluate our method on 3D medical images,\nwhere deformable registration is most commonly applied. We demonstrate that\nrecursive cascaded networks achieve consistent, significant gains and\noutperform state-of-the-art methods. The performance reveals an increasing\ntrend as long as more cascades are trained, while the limit is not observed.\nCode is available at https://github.com/microsoft/Recursive-Cascaded-Networks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 12:10:03 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 09:58:08 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 12:39:08 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Zhao", "Shengyu", ""], ["Dong", "Yue", ""], ["Chang", "Eric I-Chao", ""], ["Xu", "Yan", ""]]}, {"id": "1907.12400", "submitter": "Akinori F Ebihara", "authors": "Akinori F. Ebihara, Kazuyuki Sakurai and Hitoshi Imaoka", "title": "Specular- and Diffuse-reflection-based Face Spoofing Detection for\n  Mobile Devices", "comments": "International Joint Conference on Biometrics (IJCB) 2020 Google PC\n  Chairs Choice Best Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of the rising demand for biometric-authentication systems,\npreventing face spoofing attacks is a critical issue for the safe deployment of\nface recognition systems. Here, we propose an efficient face presentation\nattack detection (PAD) algorithm that requires minimal hardware and only a\nsmall database, making it suitable for resource-constrained devices such as\nmobile phones. Utilizing one monocular visible light camera, the proposed\nalgorithm takes two facial photos, one taken with a flash, the other without a\nflash. The proposed $SpecDiff$ descriptor is constructed by leveraging two\ntypes of reflection: (i) specular reflections from the iris region that have a\nspecific intensity distribution depending on liveness, and (ii) diffuse\nreflections from the entire face region that represents the 3D structure of a\nsubject's face. Classifiers trained with $SpecDiff$ descriptor outperforms\nother flash-based PAD algorithms on both an in-house database and on publicly\navailable NUAA, Replay-Attack, and SiW databases. Moreover, the proposed\nalgorithm achieves statistically significantly better accuracy to that of an\nend-to-end, deep neural network classifier, while being approximately six-times\nfaster execution speed. The code is publicly available at\nhttps://github.com/Akinori-F-Ebihara/SpecDiff-spoofing-detector.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:03:24 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 07:36:57 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 08:21:52 GMT"}, {"version": "v4", "created": "Mon, 9 Nov 2020 02:37:40 GMT"}, {"version": "v5", "created": "Wed, 16 Dec 2020 01:18:22 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Ebihara", "Akinori F.", ""], ["Sakurai", "Kazuyuki", ""], ["Imaoka", "Hitoshi", ""]]}, {"id": "1907.12411", "submitter": "Tianyi Wu", "authors": "Tianyi Wu, Sheng Tang, Rui Zhang, Guodong Guo, Yongdong Zhang", "title": "Consensus Feature Network for Scene Parsing", "comments": "13 pages, 7 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene parsing is challenging as it aims to assign one of the semantic\ncategories to each pixel in scene images. Thus, pixel-level features are\ndesired for scene parsing. However, classification networks are dominated by\nthe discriminative portion, so directly applying classification networks to\nscene parsing will result in inconsistent parsing predictions within one\ninstance and among instances of the same category. To address this problem, we\npropose two transform units to learn pixel-level consensus features. One is an\nInstance Consensus Transform (ICT) unit to learn the instance-level consensus\nfeatures by aggregating features within the same instance. The other is a\nCategory Consensus Transform (CCT) unit to pursue category-level consensus\nfeatures through keeping the consensus of features among instances of the same\ncategory in scene images. The proposed ICT and CCT units are lightweight,\ndata-driven and end-to-end trainable. The features learned by the two units are\nmore coherent in both instance-level and category-level. Furthermore, we\npresent the Consensus Feature Network (CFNet) based on the proposed ICT and CCT\nunits, and demonstrate the effectiveness of each component in our method by\nperforming extensive ablation experiments. Finally, our proposed CFNet achieves\ncompetitive performance on four datasets, including Cityscapes, Pascal Context,\nCamVid, and COCO Stuff.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:22:30 GMT"}, {"version": "v2", "created": "Sun, 1 Dec 2019 15:26:05 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Wu", "Tianyi", ""], ["Tang", "Sheng", ""], ["Zhang", "Rui", ""], ["Guo", "Guodong", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1907.12425", "submitter": "Amy Tabb", "authors": "Amy Tabb and Khalil M. Ahmad Yousef", "title": "Solving the Robot-World Hand-Eye(s) Calibration Problem with Iterative\n  Methods", "comments": "25 pages, including Erratum", "journal-ref": "Machine Vision and Applications, 2017", "doi": "10.1007/s00138-017-0841-7", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot-world, hand-eye calibration is the problem of determining the\ntransformation between the robot end-effector and a camera, as well as the\ntransformation between the robot base and the world coordinate system. This\nrelationship has been modeled as $\\mathbf{AX}=\\mathbf{ZB}$, where $\\mathbf{X}$\nand $\\mathbf{Z}$ are unknown homogeneous transformation matrices. The\nsuccessful execution of many robot manipulation tasks depends on determining\nthese matrices accurately, and we are particularly interested in the use of\ncalibration for use in vision tasks. In this work, we describe a collection of\nmethods consisting of two cost function classes, three different\nparameterizations of rotation components, and separable versus simultaneous\nformulations. We explore the behavior of this collection of methods on real\ndatasets and simulated datasets, and compare to seven other state-of-the-art\nmethods. Our collection of methods return greater accuracy on many metrics as\ncompared to the state-of-the-art. The collection of methods is extended to the\nproblem of robot-world hand-multiple-eye calibration, and results are shown\nwith two and three cameras mounted on the same robot.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:42:46 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Tabb", "Amy", ""], ["Yousef", "Khalil M. Ahmad", ""]]}, {"id": "1907.12428", "submitter": "Chenfeng Xu", "authors": "Chenfeng Xu, Kai Qiu, Jianlong Fu, Song Bai, Yongchao Xu, Xiang Bai", "title": "Learn to Scale: Generating Multipolar Normalized Density Maps for Crowd\n  Counting", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense crowd counting aims to predict thousands of human instances from an\nimage, by calculating integrals of a density map over image pixels. Existing\napproaches mainly suffer from the extreme density variances. Such density\npattern shift poses challenges even for multi-scale model ensembling. In this\npaper, we propose a simple yet effective approach to tackle this problem.\nFirst, a patch-level density map is extracted by a density estimation model and\nfurther grouped into several density levels which are determined over full\ndatasets. Second, each patch density map is automatically normalized by an\nonline center learning strategy with a multipolar center loss. Such a design\ncan significantly condense the density distribution into several clusters, and\nenable that the density variance can be learned by a single model. Extensive\nexperiments demonstrate the superiority of the proposed method. Our work\noutperforms the state-of-the-art by 4.2%, 14.3%, 27.1% and 20.1% in MAE, on\nShanghaiTech Part A, ShanghaiTech Part B, UCF_CC_50 and UCF-QNRF datasets,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:48:07 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 15:56:05 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Xu", "Chenfeng", ""], ["Qiu", "Kai", ""], ["Fu", "Jianlong", ""], ["Bai", "Song", ""], ["Xu", "Yongchao", ""], ["Bai", "Xiang", ""]]}, {"id": "1907.12436", "submitter": "Steven Frank", "authors": "Steven J. Frank and Andrea M. Frank", "title": "Salient Slices: Improved Neural Network Training and Performance with\n  Image Entropy", "comments": "Final version; article will be published in Neural Computation 32,\n  1222-1237 (June 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a training and analysis strategy for convolutional neural networks (CNNs),\nwe slice images into tiled segments and use, for training and prediction,\nsegments that both satisfy a criterion of information diversity and contain\nsufficient content to support classification. In particular, we utilize image\nentropy as the diversity criterion. This ensures that each tile carries as much\ninformation diversity as the original image, and for many applications serves\nas an indicator of usefulness in classification. To make predictions, a\nprobability aggregation framework is applied to probabilities assigned by the\nCNN to the input image tiles. This technique facilitates the use of large,\nhigh-resolution images that would be impractical to analyze unmodified;\nprovides data augmentation for training, which is particularly valuable when\nimage availability is limited; and the ensemble nature of the input for\nprediction enhances its accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:55:13 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 19:53:57 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2019 18:40:29 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 22:06:11 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Frank", "Steven J.", ""], ["Frank", "Andrea M.", ""]]}, {"id": "1907.12446", "submitter": "Patrick Kn\\\"obelreiter", "authors": "Patrick Kn\\\"obelreiter, Christoph Vogel and Thomas Pock", "title": "Self-Supervised Learning for Stereo Reconstruction on Aerial Images", "comments": "Symposium Prize Paper Award @IGARSS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments established deep learning as an inevitable tool to boost\nthe performance of dense matching and stereo estimation. On the downside,\nlearning these networks requires a substantial amount of training data to be\nsuccessful. Consequently, the application of these models outside of the\nlaboratory is far from straight forward. In this work we propose a\nself-supervised training procedure that allows us to adapt our network to the\nspecific (imaging) characteristics of the dataset at hand, without the\nrequirement of external ground truth data. We instead generate interim training\ndata by running our intermediate network on the whole dataset, followed by\nconservative outlier filtering. Bootstrapped from a pre-trained version of our\nhybrid CNN-CRF model, we alternate the generation of training data and network\ntraining. With this simple concept we are able to lift the completeness and\naccuracy of the pre-trained version significantly. We also show that our final\nmodel compares favorably to other popular stereo estimation algorithms on an\naerial dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 14:10:26 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Kn\u00f6belreiter", "Patrick", ""], ["Vogel", "Christoph", ""], ["Pock", "Thomas", ""]]}, {"id": "1907.12452", "submitter": "Kimberlin Van Wijnen", "authors": "Kimberlin M.H. van Wijnen, Florian Dubost, Pinar Yilmaz, M. Arfan\n  Ikram, Wiro J. Niessen, Hieab Adams, Meike W. Vernooij, Marleen de Bruijne", "title": "Automated Lesion Detection by Regressing Intensity-Based Distance with a\n  Neural Network", "comments": "MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization of focal vascular lesions on brain MRI is an important component\nof research on the etiology of neurological disorders. However, manual\nannotation of lesions can be challenging, time-consuming and subject to\nobserver bias. Automated detection methods often need voxel-wise annotations\nfor training. We propose a novel approach for automated lesion detection that\ncan be trained on scans only annotated with a dot per lesion instead of a full\nsegmentation. From the dot annotations and their corresponding intensity images\nwe compute various distance maps (DMs), indicating the distance to a lesion\nbased on spatial distance, intensity distance, or both. We train a fully\nconvolutional neural network (FCN) to predict these DMs for unseen intensity\nimages. The local optima in the predicted DMs are expected to correspond to\nlesion locations. We show the potential of this approach to detect enlarged\nperivascular spaces in white matter on a large brain MRI dataset with an\nindependent test set of 1000 scans. Our method matches the intra-rater\nperformance of the expert rater that was computed on an independent set. We\ncompare the different types of distance maps, showing that incorporating\nintensity information in the distance maps used to train an FCN greatly\nimproves performance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 14:17:17 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["van Wijnen", "Kimberlin M. H.", ""], ["Dubost", "Florian", ""], ["Yilmaz", "Pinar", ""], ["Ikram", "M. Arfan", ""], ["Niessen", "Wiro J.", ""], ["Adams", "Hieab", ""], ["Vernooij", "Meike W.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1907.12474", "submitter": "Haoran Wei", "authors": "Haoran Wei, Yue Zhang, Bing Wang, Yang Yang, Hao Li, Hongqi Wang", "title": "X-LineNet: Detecting Aircraft in Remote Sensing Images by a pair of\n  Intersecting Line Segments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the development of deep convolution neural networks (DCNNs),\ntremendous progress has been gained in the field of aircraft detection. These\nDCNNs based detectors mainly belong to top-down approaches, which first\nenumerate massive potential locations of objects with the form of rectangular\nregions, and then identify whether they are objects or not. Compared with these\ntop-down approaches, this paper shows that aircraft detection via bottom-up\napproach still performs competitively in the era of deep learning. We present a\nnovel one-stage and anchor-free aircraft detection model in a bottom-up manner,\nwhich formulates the task as detection of two intersecting line segments inside\neach target and grouping of them without any rectangular region classification.\nThis model is named as X-LineNet. With simple post-processing, X-LineNet can\nsimultaneously provide multiple representation forms of the detection result:\nthe horizontal bounding box, the rotating bounding box, and the pentagonal\nmask. The pentagonal mask is a more accurate representation form which has less\nredundancy and can better represent aircraft than that of rectangular box.\nExperiments show that X-LineNet outperforms state-of-the-art one-stage object\ndetectors and is competitive compared with advanced two-stage detectors on both\nUCAS-AOD and NWPU VHR-10 open dataset in the field of aircraft detection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 15:22:27 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 12:00:35 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2019 01:39:48 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Wei", "Haoran", ""], ["Zhang", "Yue", ""], ["Wang", "Bing", ""], ["Yang", "Yang", ""], ["Li", "Hao", ""], ["Wang", "Hongqi", ""]]}, {"id": "1907.12488", "submitter": "Mohammad Saeed Rad", "authors": "Mohammad Saeed Rad, Behzad Bozorgtabar, Claudiu Musat, Urs-Viktor\n  Marti, Max Basler, Hazim Kemal Ekenel and Jean-Philippe Thiran", "title": "Benefiting from Multitask Learning to Improve Single Image\n  Super-Resolution", "comments": "accepted at Neurocomputing (Special Issue on Deep Learning for Image\n  Super-Resolution), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress toward super resolving more realistic images by\ndeeper convolutional neural networks (CNNs), reconstructing fine and natural\ntextures still remains a challenging problem. Recent works on single image\nsuper resolution (SISR) are mostly based on optimizing pixel and content wise\nsimilarity between recovered and high-resolution (HR) images and do not benefit\nfrom recognizability of semantic classes. In this paper, we introduce a novel\napproach using categorical information to tackle the SISR problem; we present a\ndecoder architecture able to extract and use semantic information to\nsuper-resolve a given image by using multitask learning, simultaneously for\nimage super-resolution and semantic segmentation. To explore categorical\ninformation during training, the proposed decoder only employs one shared deep\nnetwork for two task-specific output layers. At run-time only layers resulting\nHR image are used and no segmentation label is required. Extensive perceptual\nexperiments and a user study on images randomly selected from COCO-Stuff\ndataset demonstrate the effectiveness of our proposed method and it outperforms\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 15:37:05 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Rad", "Mohammad Saeed", ""], ["Bozorgtabar", "Behzad", ""], ["Musat", "Claudiu", ""], ["Marti", "Urs-Viktor", ""], ["Basler", "Max", ""], ["Ekenel", "Hazim Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1907.12537", "submitter": "Bappaditya Mandal Dr.", "authors": "Andrew Cook, Bappaditya Mandal, Donna Berry and Matthew Johnson", "title": "Towards Automatic Screening of Typical and Atypical Behaviors in\n  Children With Autism", "comments": "This paper has been withdrawn by the authors due to insufficient or\n  definition error(s) in the ethics approval protocol", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the authors due to insufficient or\ndefinition error(s) in the ethics approval protocol.\n  Autism spectrum disorders (ASD) impact the cognitive, social, communicative\nand behavioral abilities of an individual. The development of new clinical\ndecision support systems is of importance in reducing the delay between\npresentation of symptoms and an accurate diagnosis. In this work, we contribute\na new database consisting of video clips of typical (normal) and atypical (such\nas hand flapping, spinning or rocking) behaviors, displayed in natural\nsettings, which have been collected from the YouTube video website. We propose\na preliminary non-intrusive approach based on skeleton keypoint identification\nusing pretrained deep neural networks on human body video clips to extract\nfeatures and perform body movement analysis that differentiates typical and\natypical behaviors of children. Experimental results on the newly contributed\ndatabase show that our platform performs best with decision tree as the\nclassifier when compared to other popular methodologies and offers a baseline\nagainst which alternate approaches may developed and tested.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 17:18:11 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 14:00:52 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Cook", "Andrew", ""], ["Mandal", "Bappaditya", ""], ["Berry", "Donna", ""], ["Johnson", "Matthew", ""]]}, {"id": "1907.12622", "submitter": "Padraig Boulton", "authors": "Padraig Boulton and Peter Hall", "title": "Artistic Domain Generalisation Methods are Limited by their Deep\n  Representations", "comments": "7 pages, 3 figures, 1 Table, 3 Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-depiction problem refers to the task of recognising visual objects\nregardless of their depictions; whether photographed, painted, sketched, {\\em\netc}. In the past, some researchers considered cross-depiction to be domain\nadaptation (DA). More recent work considers cross-depiction as domain\ngeneralisation (DG), in which algorithms extend recognition from one set of\ndomains (such as photographs and coloured artwork) to another (such as\nsketches). We show that fixing the last layer of AlexNet to random values\nprovides a performance comparable to state of the art DA and DG algorithms,\nwhen tested over the PACS benchmark. With support from background literature,\nour results lead us to conclude that texture alone is insufficient to support\ngeneralisation; rather, higher-order representations such as structure and\nshape are necessary.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 20:06:41 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Boulton", "Padraig", ""], ["Hall", "Peter", ""]]}, {"id": "1907.12629", "submitter": "Hai Phan", "authors": "Hai Phan and Dang Huynh and Yihui He and Marios Savvides and Zhiqiang\n  Shen", "title": "MoBiNet: A Mobile Binary Network for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MobileNet and Binary Neural Networks are two among the most widely used\ntechniques to construct deep learning models for performing a variety of tasks\non mobile and embedded platforms.In this paper, we present a simple yet\nefficient scheme to exploit MobileNet binarization at activation function and\nmodel weights. However, training a binary network from scratch with separable\ndepth-wise and point-wise convolutions in case of MobileNet is not trivial and\nprone to divergence. To tackle this training issue, we propose a novel neural\nnetwork architecture, namely MoBiNet - Mobile Binary Network in which skip\nconnections are manipulated to prevent information loss and vanishing gradient,\nthus facilitate the training process. More importantly, while existing binary\nneural networks often make use of cumbersome backbones such as Alex-Net,\nResNet, VGG-16 with float-type pre-trained weights initialization, our MoBiNet\nfocuses on binarizing the already-compressed neural networks like MobileNet\nwithout the need of a pre-trained model to start with. Therefore, our proposal\nresults in an effectively small model while keeping the accuracy comparable to\nexisting ones. Experiments on ImageNet dataset show the potential of the\nMoBiNet as it achieves 54.40% top-1 accuracy and dramatically reduces the\ncomputational cost with binary operators.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 20:31:15 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 03:38:10 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Phan", "Hai", ""], ["Huynh", "Dang", ""], ["He", "Yihui", ""], ["Savvides", "Marios", ""], ["Shen", "Zhiqiang", ""]]}, {"id": "1907.12635", "submitter": "Anjul Tyagi", "authors": "Ayush Kumar, Anjul Tyagi, Michael Burch, Daniel Weiskopf, Klaus\n  Mueller", "title": "Task Classification Model for Visual Fixation, Exploration, and Search", "comments": "4 pages", "journal-ref": "In proceedings of the 11th ACM Symposium on Eye Tracking Research\n  and Applications, 2019", "doi": "10.1145/3314111.3323073", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yarbus' claim to decode the observer's task from eye movements has received\nmixed reactions. In this paper, we have supported the hypothesis that it is\npossible to decode the task. We conducted an exploratory analysis on the\ndataset by projecting features and data points into a scatter plot to visualize\nthe nuance properties for each task. Following this analysis, we eliminated\nhighly correlated features before training an SVM and Ada Boosting classifier\nto predict the tasks from this filtered eye movements data. We achieve an\naccuracy of 95.4% on this task classification problem and hence, support the\nhypothesis that task classification is possible from a user's eye movement\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 20:50:37 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Kumar", "Ayush", ""], ["Tyagi", "Anjul", ""], ["Burch", "Michael", ""], ["Weiskopf", "Daniel", ""], ["Mueller", "Klaus", ""]]}, {"id": "1907.12646", "submitter": "Ukcheol Shin", "authors": "Ukcheol Shin, Jinsun Park, Gyumin Shim, Francois Rameau and In So\n  Kweon", "title": "Camera Exposure Control for Robust Robot Vision with Noise-Aware Image\n  Quality Assessment", "comments": "8 pages,6 figures, accepted in IROS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a noise-aware exposure control algorithm for robust\nrobot vision. Our method aims to capture the best-exposed image which can boost\nthe performance of various computer vision and robotics tasks. For this\npurpose, we carefully design an image quality metric which captures\ncomplementary quality attributes and ensures light-weight computation.\nSpecifically, our metric consists of a combination of image gradient, entropy,\nand noise metrics. The synergy of these measures allows preserving sharp edge\nand rich texture in the image while maintaining a low noise level. Using this\nnovel metric, we propose a real-time and fully automatic exposure and gain\ncontrol technique based on the Nelder-Mead method. To illustrate the\neffectiveness of our technique, a large set of experimental results\ndemonstrates higher qualitative and quantitative performances when compared\nwith conventional approaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 04:31:59 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Shin", "Ukcheol", ""], ["Park", "Jinsun", ""], ["Shim", "Gyumin", ""], ["Rameau", "Francois", ""], ["Kweon", "In So", ""]]}, {"id": "1907.12647", "submitter": "Arpan Sainju", "authors": "Arpan Sainju and Zhe Jiang", "title": "Mapping road safety features from streetview imagery: A deep learning\n  approach", "comments": "17 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, around 6 million car accidents occur in the U.S. on average. Road\nsafety features (e.g., concrete barriers, metal crash barriers, rumble strips)\nplay an important role in preventing or mitigating vehicle crashes. Accurate\nmaps of road safety features is an important component of safety management\nsystems for federal or state transportation agencies, helping traffic engineers\nidentify locations to invest on safety infrastructure. In current practice,\nmapping road safety features is largely done manually (e.g., observations on\nthe road or visual interpretation of streetview imagery), which is both\nexpensive and time consuming. In this paper, we propose a deep learning\napproach to automatically map road safety features from streetview imagery.\nUnlike existing Convolutional Neural Networks (CNNs) that classify each image\nindividually, we propose to further add Recurrent Neural Network (Long Short\nTerm Memory) to capture geographic context of images (spatial autocorrelation\neffect along linear road network paths). Evaluations on real world streetview\nimagery show that our proposed model outperforms several baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:38:33 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Sainju", "Arpan", ""], ["Jiang", "Zhe", ""]]}, {"id": "1907.12659", "submitter": "Bin Wang", "authors": "Bin Wang, Bing Xue, Mengjie Zhang", "title": "Particle Swarm Optimisation for Evolving Deep Neural Networks for Image\n  Classification by Evolving and Stacking Transferable Blocks", "comments": "To appear in ieee wcci 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have been widely used in image\nclassification tasks, but the process of designing CNN architectures is very\ncomplex, so Neural Architecture Search (NAS), automatically searching for\noptimal CNN architectures, has attracted more and more research interests.\nHowever, the computational cost of NAS is often too high to apply NAS on\nreal-life applications. In this paper, an efficient particle swarm optimisation\nmethod named EPSOCNN is proposed to evolve CNN architectures inspired by the\nidea of transfer learning. EPSOCNN successfully reduces the computation cost by\nminimising the search space to a single block and utilising a small subset of\nthe training set to evaluate CNNs during evolutionary process. Meanwhile,\nEPSOCNN also keeps very competitive classification accuracy by stacking the\nevolved block multiple times to fit the whole dataset. The proposed EPSOCNN\nalgorithm is evaluated on CIFAR-10 dataset and compared with 13 peer\ncompetitors comprised of deep CNNs crafted by hand, learned by reinforcement\nlearning methods and evolved by evolutionary computation approaches, which\nshows very promising results by outperforming all of the peer competitors with\nregard to the classification accuracy, number of parameters and the\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 21:30:36 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 04:25:01 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Wang", "Bin", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""]]}, {"id": "1907.12704", "submitter": "Zhizhong Han", "authors": "Zhizhong Han, Xiyang Wang, Yu-Shen Liu, Matthias Zwicker", "title": "Multi-Angle Point Cloud-VAE: Unsupervised Feature Learning for 3D Point\n  Clouds from Multiple Angles by Joint Self-Reconstruction and Half-to-Half\n  Prediction", "comments": "To appear at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised feature learning for point clouds has been vital for large-scale\npoint cloud understanding. Recent deep learning based methods depend on\nlearning global geometry from self-reconstruction. However, these methods are\nstill suffering from ineffective learning of local geometry, which\nsignificantly limits the discriminability of learned features. To resolve this\nissue, we propose MAP-VAE to enable the learning of global and local geometry\nby jointly leveraging global and local self-supervision. To enable effective\nlocal self-supervision, we introduce multi-angle analysis for point clouds. In\na multi-angle scenario, we first split a point cloud into a front half and a\nback half from each angle, and then, train MAP-VAE to learn to predict a back\nhalf sequence from the corresponding front half sequence. MAP-VAE performs this\nhalf-to-half prediction using RNN to simultaneously learn each local geometry\nand the spatial relationship among them. In addition, MAP-VAE also learns\nglobal geometry via self-reconstruction, where we employ a variational\nconstraint to facilitate novel shape generation. The outperforming results in\nfour shape analysis tasks show that MAP-VAE can learn more discriminative\nglobal or local features than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 02:17:12 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Han", "Zhizhong", ""], ["Wang", "Xiyang", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1907.12720", "submitter": "Luke Oakden-Rayner", "authors": "Luke Oakden-Rayner", "title": "Exploring large scale public medical image datasets", "comments": "9 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rationale and Objectives: Medical artificial intelligence systems are\ndependent on well characterised large scale datasets. Recently released public\ndatasets have been of great interest to the field, but pose specific challenges\ndue to the disconnect they cause between data generation and data usage,\npotentially limiting the utility of these datasets.\n  Materials and Methods: We visually explore two large public datasets, to\ndetermine how accurate the provided labels are and whether other subtle\nproblems exist. The ChestXray14 dataset contains 112,120 frontal chest films,\nand the MURA dataset contains 40,561 upper limb radiographs. A subset of around\n700 images from both datasets was reviewed by a board-certified radiologist,\nand the quality of the original labels was determined.\n  Results: The ChestXray14 labels did not accurately reflect the visual content\nof the images, with positive predictive values mostly between 10% and 30% lower\nthan the values presented in the original documentation. There were other\nsignificant problems, with examples of hidden stratification and label\ndisambiguation failure. The MURA labels were more accurate, but the original\nnormal/abnormal labels were inaccurate for the subset of cases with\ndegenerative joint disease, with a sensitivity of 60% and a specificity of 82%.\n  Conclusion: Visual inspection of images is a necessary component of\nunderstanding large image datasets. We recommend that teams producing public\ndatasets should perform this important quality control procedure and include a\nthorough description of their findings, along with an explanation of the data\ngenerating procedures and labelling rules, in the documentation for their\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 03:09:27 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Oakden-Rayner", "Luke", ""]]}, {"id": "1907.12727", "submitter": "Qingyu Zhao", "authors": "Qingyu Zhao, Ehsan Adeli, Adolf Pfefferbaum, Edith V. Sullivan, Kilian\n  M. Pohl", "title": "Confounder-Aware Visualization of ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances in deep learning, neuroimaging studies increasingly rely\non convolutional networks (ConvNets) to predict diagnosis based on MR images.\nTo gain a better understanding of how a disease impacts the brain, the studies\nvisualize the salience maps of the ConvNet highlighting voxels within the brain\nmajorly contributing to the prediction. However, these salience maps are\ngenerally confounded, i.e., some salient regions are more predictive of\nconfounding variables (such as age) than the diagnosis. To avoid such\nmisinterpretation, we propose in this paper an approach that aims to visualize\nconfounder-free saliency maps that only highlight voxels predictive of the\ndiagnosis. The approach incorporates univariate statistical tests to identify\nconfounding effects within the intermediate features learned by ConvNet. The\ninfluence from the subset of confounded features is then removed by a novel\npartial back-propagation procedure. We use this two-step approach to visualize\nconfounder-free saliency maps extracted from synthetic and two real datasets.\nThese experiments reveal the potential of our visualization in producing\nunbiased model-interpretation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 03:54:08 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 01:06:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhao", "Qingyu", ""], ["Adeli", "Ehsan", ""], ["Pfefferbaum", "Adolf", ""], ["Sullivan", "Edith V.", ""], ["Pohl", "Kilian M.", ""]]}, {"id": "1907.12736", "submitter": "Ho-Deok Jang", "authors": "Ho-Deok Jang, Sanghyun Woo, Philipp Benz, Jinsun Park, In So Kweon", "title": "Propose-and-Attend Single Shot Detector", "comments": "8 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet effective prediction module for a one-stage detector.\nThe main process is conducted in a coarse-to-fine manner. First, the module\nroughly adjusts the default boxes to well capture the extent of target objects\nin an image. Second, given the adjusted boxes, the module aligns the receptive\nfield of the convolution filters accordingly, not requiring any embedding\nlayers. Both steps build a propose-and-attend mechanism, mimicking two-stage\ndetectors in a highly efficient manner. To verify its effectiveness, we apply\nthe proposed module to a basic one-stage detector SSD. Our final model achieves\nan accuracy comparable to that of state-of-the-art detectors while using a\nfraction of their model parameters and computational overheads. Moreover, we\nfound that the proposed module has two strong applications. 1) The module can\nbe successfully integrated into a lightweight backbone, further pushing the\nefficiency of the one-stage detector. 2) The module also allows\ntrain-from-scratch without relying on any sophisticated base networks as\nprevious methods do.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 04:56:25 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Jang", "Ho-Deok", ""], ["Woo", "Sanghyun", ""], ["Benz", "Philipp", ""], ["Park", "Jinsun", ""], ["Kweon", "In So", ""]]}, {"id": "1907.12739", "submitter": "Andrew Shepley Mr", "authors": "Andrew Jason Shepley", "title": "Deep Learning For Face Recognition: A Critical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is a rapidly developing and widely applied aspect of\nbiometric technologies. Its applications are broad, ranging from law\nenforcement to consumer applications, and industry efficiency and monitoring\nsolutions. The recent advent of affordable, powerful GPUs and the creation of\nhuge face databases has drawn research focus primarily on the development of\nincreasingly deep neural networks designed for all aspects of face recognition\ntasks, ranging from detection and preprocessing to feature representation and\nclassification in verification and identification solutions. However, despite\nthese improvements, real-time, accurate face recognition is still a challenge,\nprimarily due to the high computational cost associated with the use of Deep\nConvolutions Neural Networks (DCNN), and the need to balance accuracy\nrequirements with time and resource constraints. Other significant issues\naffecting face recognition relate to occlusion, illumination and pose\ninvariance, which causes a notable decline in accuracy in both traditional\nhandcrafted solutions and deep neural networks. This survey will provide a\ncritical analysis and comparison of modern state of the art methodologies,\ntheir benefits, and their limitations. It provides a comprehensive coverage of\nboth deep and shallow solutions, as they stand today, and highlight areas\nrequiring future development and improvement. This review is aimed at\nfacilitating research into novel approaches, and further development of current\nmethodologies by scientists and engineers, whilst imparting an informative and\nanalytical perspective on currently available solutions to end users in\nindustry, government and consumer contexts.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 22:55:49 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Shepley", "Andrew Jason", ""]]}, {"id": "1907.12740", "submitter": "Gioele Ciaparrone", "authors": "Gioele Ciaparrone, Francisco Luque S\\'anchez, Siham Tabik, Luigi\n  Troiano, Roberto Tagliaferri, Francisco Herrera", "title": "Deep Learning in Video Multi-Object Tracking: A Survey", "comments": "Accepted in Neurocomputing, 2019. New in v4: updated license in\n  compliance with Elsevier policy. Main text: 29 pages, 10 figures, 7 tables.\n  Summary table in appendix at the end of the paper", "journal-ref": null, "doi": "10.1016/j.neucom.2019.11.023", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Multiple Object Tracking (MOT) consists in following the\ntrajectory of different objects in a sequence, usually a video. In recent\nyears, with the rise of Deep Learning, the algorithms that provide a solution\nto this problem have benefited from the representational power of deep models.\nThis paper provides a comprehensive survey on works that employ Deep Learning\nmodels to solve the task of MOT on single-camera videos. Four main steps in MOT\nalgorithms are identified, and an in-depth review of how Deep Learning was\nemployed in each one of these stages is presented. A complete experimental\ncomparison of the presented works on the three MOTChallenge datasets is also\nprovided, identifying a number of similarities among the top-performing methods\nand presenting some possible future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 11:51:26 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 02:11:24 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 11:49:43 GMT"}, {"version": "v4", "created": "Tue, 19 Nov 2019 11:26:20 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Ciaparrone", "Gioele", ""], ["S\u00e1nchez", "Francisco Luque", ""], ["Tabik", "Siham", ""], ["Troiano", "Luigi", ""], ["Tagliaferri", "Roberto", ""], ["Herrera", "Francisco", ""]]}, {"id": "1907.12741", "submitter": "Gautam Srivastava", "authors": "Hamid Jan, Amjad Ali, Shahid Mahmood, and Gautam Srivastava", "title": "Statistical Descriptors-based Automatic Fingerprint Identification:\n  Machine Learning Approaches", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of a person from fingerprints of good quality has been used by\ncommercial applications and law enforcement agencies for many years, however\nidentification of a person from latent fingerprints is very difficult and\nchallenging. A latent fingerprint is a fingerprint left on a surface by\ndeposits of oils and/or perspiration from the finger. It is not usually visible\nto the naked eye but may be detected with special techniques such as dusting\nwith fine powder and then lifting the pattern of powder with transparent tape.\nWe have evaluated the quality of machine learning techniques that has been\nimplemented in automatic fingerprint identification. In this paper, we use\nfingerprints of low quality from database DB1 of Fingerprint Verification\nCompetition (FVC 2002) to conduct our experiments. Fingerprints are processed\nto find its core point using Poincare index and carry out enhancement using\nDiffusion coherence filter whose performance is known to be good in the high\ncurvature regions of fingerprints. Grey-level Co-Occurrence Matrix (GLCM) based\nseven statistical descriptors with four different inter pixel distances are\nthen extracted as features and put forward to train and test REPTree,\nRandomTree, J48, Decision Stump and Random Forest Machine Learning techniques\nfor personal identification. Experiments are conducted on 80 instances and 28\nattributes. Our experiments proved that Random Forests and J48 give good\nresults for latent fingerprints as compared to other machine learning\ntechniques and can help improve the identification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 22:17:13 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Jan", "Hamid", ""], ["Ali", "Amjad", ""], ["Mahmood", "Shahid", ""], ["Srivastava", "Gautam", ""]]}, {"id": "1907.12743", "submitter": "Min-Hung Chen", "authors": "Min-Hung Chen, Zsolt Kira, Ghassan AlRegib, Jaekwon Yoo, Ruxin Chen,\n  Jian Zheng", "title": "Temporal Attentive Alignment for Large-Scale Video Domain Adaptation", "comments": "ICCV 2019 (Oral) camera-ready + supplementary. Code and data:\n  http://github.com/cmhungsteve/TA3N", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although various image-based domain adaptation (DA) techniques have been\nproposed in recent years, domain shift in videos is still not well-explored.\nMost previous works only evaluate performance on small-scale datasets which are\nsaturated. Therefore, we first propose two large-scale video DA datasets with\nmuch larger domain discrepancy: UCF-HMDB_full and Kinetics-Gameplay. Second, we\ninvestigate different DA integration methods for videos, and show that\nsimultaneously aligning and learning temporal dynamics achieves effective\nalignment even without sophisticated DA methods. Finally, we propose Temporal\nAttentive Adversarial Adaptation Network (TA3N), which explicitly attends to\nthe temporal dynamics using domain discrepancy for more effective domain\nalignment, achieving state-of-the-art performance on four video DA datasets\n(e.g. 7.9% accuracy gain over \"Source only\" from 73.9% to 81.8% on \"HMDB -->\nUCF\", and 10.3% gain on \"Kinetics --> Gameplay\"). The code and data are\nreleased at http://github.com/cmhungsteve/TA3N.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 05:43:55 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 16:06:39 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2019 05:17:51 GMT"}, {"version": "v4", "created": "Thu, 8 Aug 2019 05:50:11 GMT"}, {"version": "v5", "created": "Mon, 12 Aug 2019 15:28:47 GMT"}, {"version": "v6", "created": "Sun, 15 Sep 2019 00:48:41 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Chen", "Min-Hung", ""], ["Kira", "Zsolt", ""], ["AlRegib", "Ghassan", ""], ["Yoo", "Jaekwon", ""], ["Chen", "Ruxin", ""], ["Zheng", "Jian", ""]]}, {"id": "1907.12744", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Arnout Van Messem, Wesley De Neve", "title": "Not All Adversarial Examples Require a Complex Defense: Identifying\n  Over-optimized Adversarial Examples with IQR-based Logit Thresholding", "comments": "Accepted for the 2019 International Joint Conference on Neural\n  Networks (IJCNN-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting adversarial examples currently stands as one of the biggest\nchallenges in the field of deep learning. Adversarial attacks, which produce\nadversarial examples, increase the prediction likelihood of a target class for\na particular data point. During this process, the adversarial example can be\nfurther optimized, even when it has already been wrongly classified with 100%\nconfidence, thus making the adversarial example even more difficult to detect.\nFor this kind of adversarial examples, which we refer to as over-optimized\nadversarial examples, we discovered that the logits of the model provide solid\nclues on whether the data point at hand is adversarial or genuine. In this\ncontext, we first discuss the masking effect of the softmax function for the\nprediction made and explain why the logits of the model are more useful in\ndetecting over-optimized adversarial examples. To identify this type of\nadversarial examples in practice, we propose a non-parametric and\ncomputationally efficient method which relies on interquartile range, with this\nmethod becoming more effective as the image resolution increases. We support\nour observations throughout the paper with detailed experiments for different\ndatasets (MNIST, CIFAR-10, and ImageNet) and several architectures.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 05:46:49 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Ozbulak", "Utku", ""], ["Van Messem", "Arnout", ""], ["De Neve", "Wesley", ""]]}, {"id": "1907.12763", "submitter": "Victor Escorcia", "authors": "Victor Escorcia, Mattia Soldan, Josef Sivic, Bernard Ghanem, Bryan\n  Russell", "title": "Temporal Localization of Moments in Video Collections with Natural\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we introduce the task of retrieving relevant video moments\nfrom a large corpus of untrimmed, unsegmented videos given a natural language\nquery. Our task poses unique challenges as a system must efficiently identify\nboth the relevant videos and localize the relevant moments in the videos. This\ntask is in contrast to prior work that localizes relevant moments in a single\nvideo or searches a large collection of already-segmented videos. For our task,\nwe introduce Clip Alignment with Language (CAL), a model that aligns features\nfor a natural language query to a sequence of short video clips that compose a\ncandidate moment in a video. Our approach goes beyond prior work that\naggregates video features over a candidate moment by allowing for finer clip\nalignment. Moreover, our approach is amenable to efficient indexing of the\nresulting clip-level representations, which makes it suitable for moment\nlocalization in large video collections. We evaluate our approach on three\nrecently proposed datasets for temporal localization of moments in video with\nnatural language extended to our video corpus moment retrieval setting: DiDeMo,\nCharades-STA, and ActivityNet-captions. We show that our CAL model outperforms\nthe recently proposed Moment Context Network (MCN) on all criteria across all\ndatasets on our proposed task, obtaining an 8%-85% and 11%-47% boost for\naverage recall and median rank, respectively, and achieves 5x faster retrieval\nand 8x smaller index size with a 500K video corpus.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 07:31:02 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Escorcia", "Victor", ""], ["Soldan", "Mattia", ""], ["Sivic", "Josef", ""], ["Ghanem", "Bernard", ""], ["Russell", "Bryan", ""]]}, {"id": "1907.12766", "submitter": "Min Zhang", "authors": "Min Zhang, Haoxuan You, Pranav Kadam, Shan Liu and C.-C. Jay Kuo", "title": "PointHop: An Explainable Machine Learning Method for Point Cloud\n  Classification", "comments": "13 pages with 9 figures", "journal-ref": null, "doi": "10.1109/TMM.2019.2963592", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An explainable machine learning method for point cloud classification, called\nthe PointHop method, is proposed in this work. The PointHop method consists of\ntwo stages: 1) local-to-global attribute building through iterative one-hop\ninformation exchange, and 2) classification and ensembles. In the attribute\nbuilding stage, we address the problem of unordered point cloud data using a\nspace partitioning procedure and developing a robust descriptor that\ncharacterizes the relationship between a point and its one-hop neighbor in a\nPointHop unit. When we put multiple PointHop units in cascade, the attributes\nof a point will grow by taking its relationship with one-hop neighbor points\ninto account iteratively. Furthermore, to control the rapid dimension growth of\nthe attribute vector associated with a point, we use the Saab transform to\nreduce the attribute dimension in each PointHop unit. In the classification and\nensemble stage, we feed the feature vector obtained from multiple PointHop\nunits to a classifier. We explore ensemble methods to improve the\nclassification performance furthermore. It is shown by experimental results\nthat the PointHop method offers classification performance that is comparable\nwith state-of-the-art methods while demanding much lower training complexity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 07:39:40 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 01:37:28 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zhang", "Min", ""], ["You", "Haoxuan", ""], ["Kadam", "Pranav", ""], ["Liu", "Shan", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1907.12769", "submitter": "Hengkai Guo", "authors": "Hengkai Guo, Wenji Wang, Guanjun Guo, Huaxia Li, Jiachen Liu, Qian He,\n  Xuefeng Xiao", "title": "An Empirical Study of Propagation-based Methods for Video Object\n  Segmentation", "comments": "The 2019 DAVIS Challenge on Video Object Segmentation - CVPR\n  Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While propagation-based approaches have achieved state-of-the-art performance\nfor video object segmentation, the literature lacks a fair comparison of\ndifferent methods using the same settings. In this paper, we carry out an\nempirical study for propagation-based methods. We view these approaches from a\nunified perspective and conduct detailed ablation study for core methods, input\ncues, multi-object combination and training strategies. With careful designs,\nour improved end-to-end memory networks achieve a global mean of 76.1 on DAVIS\n2017 val set.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 08:00:47 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Guo", "Hengkai", ""], ["Wang", "Wenji", ""], ["Guo", "Guanjun", ""], ["Li", "Huaxia", ""], ["Liu", "Jiachen", ""], ["He", "Qian", ""], ["Xiao", "Xuefeng", ""]]}, {"id": "1907.12791", "submitter": "Zhenlong Xu", "authors": "Xu Zhenlong, Zhou shuigeng, Cheng zhanzhan, Bai fan, Niu yi, Pu\n  shiliang", "title": "Towards Pure End-to-End Learning for Recognizing Multiple Text Sequences\n  from an Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we address a challenging problem: recognizing multiple text sequences\nfrom an image by pure end-to-end learning. It is twofold: 1) Multiple text\nsequences recognition. Each image may contain multiple text sequences of\ndifferent content, location and orientation, and we try to recognize all the\ntext sequences contained in the image. 2) Pure end-to-end (PEE) learning.We\nsolve the problem in a pure end-to-end learning way where each training image\nis labeled by only text transcripts of all contained sequences, without any\ngeometric annotations. Most existing works recognize multiple text sequences\nfrom an image in a non-end-to-end (NEE) or quasi-end-to-end (QEE) way, in which\neach image is trained with both text transcripts and text locations.Only\nrecently, a PEE method was proposed to recognize text sequences from an image\nwhere the text sequence was split to several lines in the image. However, it\ncannot be directly applied to recognizing multiple text sequences from an\nimage. So in this paper, we propose a pure end-to-end learning method to\nrecognize multiple text sequences from an image. Our method directly learns\nmultiple sequences of probability distribution conditioned on each input image,\nand outputs multiple text transcripts with a well-designed decoding strategy.To\nevaluate the proposed method, we constructed several datasets mainly based on\nan existing public dataset andtwo real application scenarios. Experimental\nresults show that the proposed method can effectively recognize multiple text\nsequences from images, and outperforms CTC-based and attention-based baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 09:07:28 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Zhenlong", "Xu", ""], ["shuigeng", "Zhou", ""], ["zhanzhan", "Cheng", ""], ["fan", "Bai", ""], ["yi", "Niu", ""], ["shiliang", "Pu", ""]]}, {"id": "1907.12849", "submitter": "Chao Zhang", "authors": "Chao Zhang, Stephan Liwicki, William Smith, Roberto Cipolla", "title": "Orientation-aware Semantic Segmentation on Icosahedron Spheres", "comments": "9 pages, accepted to iccv 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address semantic segmentation on omnidirectional images, to leverage a\nholistic understanding of the surrounding scene for applications like\nautonomous driving systems. For the spherical domain, several methods recently\nadopt an icosahedron mesh, but systems are typically rotation invariant or\nrequire significant memory and parameters, thus enabling execution only at very\nlow resolutions. In our work, we propose an orientation-aware CNN framework for\nthe icosahedron mesh. Our representation allows for fast network operations, as\nour design simplifies to standard network operations of classical CNNs, but\nunder consideration of north-aligned kernel convolutions for features on the\nsphere. We implement our representation and demonstrate its memory efficiency\nup-to a level-8 resolution mesh (equivalent to 640 x 1024 equirectangular\nimages). Finally, since our kernels operate on the tangent of the sphere,\nstandard feature weights, pretrained on perspective data, can be directly\ntransferred with only small need for weight refinement. In our evaluation our\norientation-aware CNN becomes a new state of the art for the recent 2D3DS\ndataset, and our Omni-SYNTHIA version of SYNTHIA. Rotation invariant\nclassification and segmentation tasks are additionally presented for comparison\nto prior art.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 11:59:24 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Zhang", "Chao", ""], ["Liwicki", "Stephan", ""], ["Smith", "William", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1907.12859", "submitter": "Onur Tasar", "authors": "Onur Tasar, S L Happy, Yuliya Tarabalka, Pierre Alliez", "title": "ColorMapGAN: Unsupervised Domain Adaptation for Semantic Segmentation\n  Using Color Mapping Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.2980417", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the various reasons such as atmospheric effects and differences in\nacquisition, it is often the case that there exists a large difference between\nspectral bands of satellite images collected from different geographic\nlocations. The large shift between spectral distributions of training and test\ndata causes the current state of the art supervised learning approaches to\noutput unsatisfactory maps. We present a novel semantic segmentation framework\nthat is robust to such shift. The key component of the proposed framework is\nColor Mapping Generative Adversarial Networks (ColorMapGAN), which can generate\nfake training images that are semantically exactly the same as training images,\nbut whose spectral distribution is similar to the distribution of the test\nimages. We then use the fake images and the ground-truth for the training\nimages to fine-tune the already trained classifier. Contrary to the existing\nGenerative Adversarial Networks (GANs), the generator in ColorMapGAN does not\nhave any convolutional or pooling layers. It learns to transform the colors of\nthe training data to the colors of the test data by performing only one\nelement-wise matrix multiplication and one matrix addition operations. Thanks\nto the architecturally simple but powerful design of ColorMapGAN, the proposed\nframework outperforms the existing approaches with a large margin in terms of\nboth accuracy and computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 12:30:32 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 19:56:56 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Tasar", "Onur", ""], ["Happy", "S L", ""], ["Tarabalka", "Yuliya", ""], ["Alliez", "Pierre", ""]]}, {"id": "1907.12861", "submitter": "Sumit Shekhar", "authors": "Ritwick Chaudhry, Sumit Shekhar, Utkarsh Gupta, Pranav Maneriker,\n  Prann Bansal, Ajay Joshi", "title": "LEAF-QA: Locate, Encode & Attend for Figure Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce LEAF-QA, a comprehensive dataset of $250,000$ densely annotated\nfigures/charts, constructed from real-world open data sources, along with ~2\nmillion question-answer (QA) pairs querying the structure and semantics of\nthese charts. LEAF-QA highlights the problem of multimodal QA, which is notably\ndifferent from conventional visual QA (VQA), and has recently gained interest\nin the community. Furthermore, LEAF-QA is significantly more complex than\nprevious attempts at chart QA, viz. FigureQA and DVQA, which present only\nlimited variations in chart data. LEAF-QA being constructed from real-world\nsources, requires a novel architecture to enable question answering. To this\nend, LEAF-Net, a deep architecture involving chart element localization,\nquestion and answer encoding in terms of chart elements, and an attention\nnetwork is proposed. Different experiments are conducted to demonstrate the\nchallenges of QA on LEAF-QA. The proposed architecture, LEAF-Net also\nconsiderably advances the current state-of-the-art on FigureQA and DVQA.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 12:39:06 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Chaudhry", "Ritwick", ""], ["Shekhar", "Sumit", ""], ["Gupta", "Utkarsh", ""], ["Maneriker", "Pranav", ""], ["Bansal", "Prann", ""], ["Joshi", "Ajay", ""]]}, {"id": "1907.12865", "submitter": "Mian Ahsan Iqbal", "authors": "Pau Panareda Busto, Ahsan Iqbal, Juergen Gall", "title": "Open Set Domain Adaptation for Image and Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since annotating and curating large datasets is very expensive, there is a\nneed to transfer the knowledge from existing annotated datasets to unlabelled\ndata. Data that is relevant for a specific application, however, usually\ndiffers from publicly available datasets since it is sampled from a different\ndomain. While domain adaptation methods compensate for such a domain shift,\nthey assume that all categories in the target domain are known and match the\ncategories in the source domain. Since this assumption is violated under\nreal-world conditions, we propose an approach for open set domain adaptation\nwhere the target domain contains instances of categories that are not present\nin the source domain. The proposed approach achieves state-of-the-art results\non various datasets for image classification and action recognition. Since the\napproach can be used for open set and closed set domain adaptation, as well as\nunsupervised and semi-supervised domain adaptation, it is a versatile tool for\nmany applications.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 12:50:17 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Busto", "Pau Panareda", ""], ["Iqbal", "Ahsan", ""], ["Gall", "Juergen", ""]]}, {"id": "1907.12868", "submitter": "Lars Schmarje", "authors": "Lars Schmarje, Claudius Zelenka, Ulf Geisen, Claus-C. Gl\\\"uer,\n  Reinhard Koch", "title": "2D and 3D Segmentation of uncertain local collagen fiber orientations in\n  SHG microscopy", "comments": null, "journal-ref": "DAGM GCPR 2019", "doi": "10.1007/978-3-030-33676-9_26", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collagen fiber orientations in bones, visible with Second Harmonic Generation\n(SHG) microscopy, represent the inner structure and its alteration due to\ninfluences like cancer. While analyses of these orientations are valuable for\nmedical research, it is not feasible to analyze the needed large amounts of\nlocal orientations manually. Since we have uncertain borders for these local\norientations only rough regions can be segmented instead of a pixel-wise\nsegmentation. We analyze the effect of these uncertain borders on human\nperformance by a user study. Furthermore, we compare a variety of 2D and 3D\nmethods such as classical approaches like Fourier analysis with\nstate-of-the-art deep neural networks for the classification of local fiber\norientations. We present a general way to use pretrained 2D weights in 3D\nneural networks, such as Inception-ResNet-3D a 3D extension of\nInception-ResNet-v2. In a 10 fold cross-validation our two stage segmentation\nbased on Inception-ResNet-3D and transferred 2D ImageNet weights achieves a\nhuman comparable accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 12:56:01 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Schmarje", "Lars", ""], ["Zelenka", "Claudius", ""], ["Geisen", "Ulf", ""], ["Gl\u00fcer", "Claus-C.", ""], ["Koch", "Reinhard", ""]]}, {"id": "1907.12887", "submitter": "Fabian B. Fuchs Mr", "authors": "Fabian B. Fuchs, Adam R. Kosiorek, Li Sun, Oiwi Parker Jones, Ingmar\n  Posner", "title": "End-to-end Recurrent Multi-Object Tracking and Trajectory Prediction\n  with Relational Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of contemporary object-tracking approaches do not model\ninteractions between objects. This contrasts with the fact that objects' paths\nare not independent: a cyclist might abruptly deviate from a previously planned\ntrajectory in order to avoid colliding with a car. Building upon HART, a neural\nclass-agnostic single-object tracker, we introduce a multi-object tracking\nmethod MOHART capable of relational reasoning. Importantly, the entire system,\nincluding the understanding of interactions and relations between objects, is\nclass-agnostic and learned simultaneously in an end-to-end fashion. We explore\na number of relational reasoning architectures and show that\npermutation-invariant models outperform non-permutation-invariant alternatives.\nWe also find that architectures using a single permutation invariant operation\nlike DeepSets, despite, in theory, being universal function approximators, are\nnonetheless outperformed by a more complex architecture based on multi-headed\nattention. The latter better accounts for complex physical interactions in a\nchallenging toy experiment. Further, we find that modelling interactions leads\nto consistent performance gains in tracking as well as future trajectory\nprediction on three real-world datasets (MOTChallenge, UA-DETRAC, and Stanford\nDrone dataset), particularly in the presence of ego-motion, occlusions, crowded\nscenes, and faulty sensor inputs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 22:40:13 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 17:17:49 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 15:44:01 GMT"}, {"version": "v4", "created": "Thu, 30 Apr 2020 21:40:28 GMT"}, {"version": "v5", "created": "Mon, 28 Sep 2020 14:25:23 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Fuchs", "Fabian B.", ""], ["Kosiorek", "Adam R.", ""], ["Sun", "Li", ""], ["Jones", "Oiwi Parker", ""], ["Posner", "Ingmar", ""]]}, {"id": "1907.12888", "submitter": "Ts\\`i-U\\'i \\.Ik", "authors": "Tzu-Han Hsu, Ching-Hsuan Chen, Nyan Ping Ju, Ts\\`i-U\\'i \\.Ik, Wen-Chih\n  Peng, Chih-Chuan Wang, Yu-Shuen Wang, Yuan-Hsiang Lin, Yu-Chee Tseng,\n  Jiun-Long Huang, Yu-Tai Ching", "title": "CoachAI: A Project for Microscopic Badminton Match Data Collection and\n  Tactical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision based object tracking has been used to annotate and augment\nsports video. For sports learning and training, video replay is often used in\npost-match review and training review for tactical analysis and movement\nanalysis. For automatically and systematically competition data collection and\ntactical analysis, a project called CoachAI has been supported by the Ministry\nof Science and Technology, Taiwan. The proposed project also includes research\nof data visualization, connected training auxiliary devices, and data\nwarehouse. Deep learning techniques will be used to develop video-based\nreal-time microscopic competition data collection based on broadcast\ncompetition video. Machine learning techniques will be used to develop a\ntactical analysis. To reveal data in more understandable forms and to help in\npre-match training, AR/VR techniques will be used to visualize data, tactics,\nand so on. In addition, training auxiliary devices including smart badminton\nrackets and connected serving machines will be developed based on the IoT\ntechnology to further utilize competition data and tactical data and boost\ntraining efficiency. Especially, the connected serving machines will be\ndeveloped to perform specified tactics and to interact with players in their\ntraining.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 08:33:00 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Hsu", "Tzu-Han", ""], ["Chen", "Ching-Hsuan", ""], ["Ju", "Nyan Ping", ""], ["\u0130k", "Ts\u00ec-U\u00ed", ""], ["Peng", "Wen-Chih", ""], ["Wang", "Chih-Chuan", ""], ["Wang", "Yu-Shuen", ""], ["Lin", "Yuan-Hsiang", ""], ["Tseng", "Yu-Chee", ""], ["Huang", "Jiun-Long", ""], ["Ching", "Yu-Tai", ""]]}, {"id": "1907.12891", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo", "title": "4X4 Census Transform", "comments": "3 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a 4X4 Census Transform (4X4CT) to encourage further\nresearch in computer vision and visual computing. Unlike the traditional 3X3 CT\nwhich uses a nine pixels kernel, the proposed 4X4CT uses a sixteen pixels\nkernel with four overlapped groups of 3X3 kernel size. In each overlapping\ngroup, a reference input pixel profits from its nearest eight pixels to produce\nan eight bits binary string convertible to a grayscale integer of the 4X4CT's\noutput pixel. Preliminary experiments demonstrated more image textural\ncrispness and contrast than the CT as well as alternativeness to enable\nmeaningful solutions to be achieved.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:30:45 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Rukundo", "Olivier", ""]]}, {"id": "1907.12892", "submitter": "Francis Brochu", "authors": "Francis Brochu", "title": "Increasing Shape Bias in ImageNet-Trained Networks Using Transfer\n  Learning and Domain-Adversarial Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) have become the state-of-the-art method\nto learn from image data. However, recent research shows that they may include\na texture and colour bias in their representation, contrary to the intuition\nthat they learn the shapes of the image content and to human biological\nlearning. Thus, recent works have attempted to increase the shape bias in CNNs\nin order to train more robust and accurate networks on tasks. One such approach\nuses style-transfer in order to remove texture clues from the data. This work\nreproduces this methodology on four image classification datasets, as well as\nextends the method to use domain-adversarial training in order to further\nincrease the shape bias in the learned representation. The results show the\nproposed method increases the robustness and shape bias of the CNNs, while it\ndoes not provide a gain in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:30:46 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Brochu", "Francis", ""]]}, {"id": "1907.12896", "submitter": "Irynei Baran", "authors": "Irynei Baran, Orest Kupyn, Arseny Kravchenko", "title": "Safe Augmentation: Learning Task-Specific Transformations from Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is widely used as a part of the training process applied to\ndeep learning models, especially in the computer vision domain. Currently,\ncommon data augmentation techniques are designed manually. Therefore they\nrequire expert knowledge and time. Moreover, augmentations are\ndataset-specific, and the optimal augmentations set on a specific dataset has\nlimited transferability to others. We present a simple and explainable method\ncalled $\\textbf{Safe Augmentation}$ that can learn task-specific data\naugmentation techniques that do not change the data distribution and improve\nthe generalization of the model. We propose to use safe augmentation in two\nways: for model fine-tuning and along with other augmentation techniques. Our\nmethod is model-agnostic, easy to implement, and achieves better accuracy on\nCIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and Cityscapes datasets comparing to\nbaseline augmentation techniques. The code is available at\n$\\href{https://github.com/Irynei/SafeAugmentation}{https://github.com/Irynei/SafeAugmentation}$.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:32:42 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Baran", "Irynei", ""], ["Kupyn", "Orest", ""], ["Kravchenko", "Arseny", ""]]}, {"id": "1907.12898", "submitter": "Yang Hu", "authors": "Ling Jiang, Yang Hu, Xilin Xia, Qiuhua Liang, Andrea Soltoggio", "title": "A Multi-Scale Mapping Approach Based on a Deep Learning CNN Model for\n  Reconstructing High-Resolution Urban DEMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shortage of high-resolution urban digital elevation model (DEM) datasets\nhas been a challenge for modelling urban flood and managing its risk. A\nsolution is to develop effective approaches to reconstruct high-resolution DEMs\nfrom their low-resolution equivalents that are more widely available. However,\nthe current high-resolution DEM reconstruction approaches mainly focus on\nnatural topography. Few attempts have been made for urban topography which is\ntypically an integration of complex man-made and natural features. This study\nproposes a novel multi-scale mapping approach based on convolutional neural\nnetwork (CNN) to deal with the complex characteristics of urban topography and\nreconstruct high-resolution urban DEMs. The proposed multi-scale CNN model is\nfirstly trained using urban DEMs that contain topographic features at different\nresolutions, and then used to reconstruct the urban DEM at a specified (high)\nresolution from a low-resolution equivalent. A two-level accuracy assessment\napproach is also designed to evaluate the performance of the proposed urban DEM\nreconstruction method, in terms of numerical accuracy and morphological\naccuracy. The proposed DEM reconstruction approach is applied to a 121 km2\nurbanized area in London, UK. Compared with other commonly used methods, the\ncurrent CNN based approach produces superior results, providing a\ncost-effective innovative method to acquire high-resolution DEMs in other\ndata-scarce environments.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 21:20:49 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 09:21:58 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Jiang", "Ling", ""], ["Hu", "Yang", ""], ["Xia", "Xilin", ""], ["Liang", "Qiuhua", ""], ["Soltoggio", "Andrea", ""]]}, {"id": "1907.12900", "submitter": "Yingwei Zhou", "authors": "Yingwei Zhou", "title": "Slot Based Image Augmentation System for Object Detection", "comments": "preprint draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Detection has been a significant topic in computer vision. As the\ncontinuous development of Deep Learning, many advanced academic and industrial\noutcomes are established on localising and classifying the target objects, such\nas instance segmentation, video tracking and robotic vision. As the core\nconcept of Deep Learning, Deep Neural Networks (DNNs) and associated training\nare highly integrated with task-driven modelling, having great effects on\naccurate detection. The main focus of improving detection performance is\nproposing DNNs with extra layers and novel topological connections to extract\nthe desired features from input data. However, training these models can be\ncomputationally expensive and laborious progress as the complicated model\narchitecture and enormous parameters. Besides, the dataset is another reason\ncausing this issue and low detection accuracy, because of insufficient data\nsamples or difficult instances. To address these training difficulties, this\nthesis presents two different approaches to improve the detection performance\nin the relatively light-weight way. As the intrinsic feature of data-driven in\ndeep learning, the first approach is \"slot-based image augmentation\" to enrich\nthe dataset with extra foreground and background combinations. Instead of the\ncommonly used image flipping method, the proposed system achieved similar mAP\nimprovement with less extra images which decrease training time. This proposed\naugmentation system has extra flexibility adapting to various scenarios and the\nperformance-driven analysis provides an alternative aspect of conducting image\naugmentation\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 13:38:12 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Zhou", "Yingwei", ""]]}, {"id": "1907.12902", "submitter": "Matias Valdenegro-Toro", "authors": "Nour Soufi, Matias Valdenegro-Toro", "title": "Data augmentation with Symbolic-to-Real Image Translation GANs for\n  Traffic Sign Recognition", "comments": "6 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic sign recognition is an important component of many advanced driving\nassistance systems, and it is required for full autonomous driving.\nComputational performance is usually the bottleneck in using large scale neural\nnetworks for this purpose. SqueezeNet is a good candidate for efficient image\nclassification of traffic signs, but in our experiments it does not reach high\naccuracy, and we believe this is due to lack of data, requiring data\naugmentation. Generative adversarial networks can learn the high dimensional\ndistribution of empirical data, allowing the generation of new data points. In\nthis paper we apply pix2pix GANs architecture to generate new traffic sign\nimages and evaluate the use of these images in data augmentation. We were\nmotivated to use pix2pix to translate symbolic sign images to real ones due to\nthe mode collapse in Conditional GANs. Through our experiments we found that\ndata augmentation using GAN can increase classification accuracy for circular\ntraffic signs from 92.1% to 94.0%, and for triangular traffic signs from 93.8%\nto 95.3%, producing an overall improvement of 2%. However some traditional\naugmentation techniques can outperform GAN data augmentation, for example\ncontrast variation in circular traffic signs (95.5%) and displacement on\ntriangular traffic signs (96.7 %). Our negative results shows that while GANs\ncan be naively used for data augmentation, they are not always the best choice,\ndepending on the problem and variability in the data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 21:52:01 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Soufi", "Nour", ""], ["Valdenegro-Toro", "Matias", ""]]}, {"id": "1907.12904", "submitter": "Wanjie Sun", "authors": "Wanjie Sun and Zhenzhong Chen", "title": "Learned Image Downscaling for Upscaling using Content Adaptive Resampler", "comments": "15 pages; not the final version", "journal-ref": null, "doi": "10.1109/TIP.2020.2970248", "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network based image super-resolution (SR) models\nhave shown superior performance in recovering the underlying high resolution\n(HR) images from low resolution (LR) images obtained from the predefined\ndownscaling methods. In this paper we propose a learned image downscaling\nmethod based on content adaptive resampler (CAR) with consideration on the\nupscaling process. The proposed resampler network generates content adaptive\nimage resampling kernels that are applied to the original HR input to generate\npixels on the downscaled image. Moreover, a differentiable upscaling (SR)\nmodule is employed to upscale the LR result into its underlying HR counterpart.\nBy back-propagating the reconstruction error down to the original HR input\nacross the entire framework to adjust model parameters, the proposed framework\nachieves a new state-of-the-art SR performance through upscaling guided image\nresamplers which adaptively preserve detailed information that is essential to\nthe upscaling. Experimental results indicate that the quality of the generated\nLR image is comparable to that of the traditional interpolation based method,\nbut the significant SR performance gain is achieved by deep SR models trained\njointly with the CAR model. The code is publicly available on: URL\nhttps://github.com/sunwj/CAR.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 05:35:27 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 13:00:42 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Sun", "Wanjie", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "1907.12905", "submitter": "Xiangxi Shi", "authors": "Xiangxi Shi, Jianfei Cai, Shafiq Joty, Jiuxiang Gu", "title": "Watch It Twice: Video Captioning with a Refocused Video Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of video data and the increasing demands of various\napplications such as intelligent video search and assistance toward\nvisually-impaired people, video captioning task has received a lot of attention\nrecently in computer vision and natural language processing fields. The\nstate-of-the-art video captioning methods focus more on encoding the temporal\ninformation, while lack of effective ways to remove irrelevant temporal\ninformation and also neglecting the spatial details. However, the current RNN\nencoding module in single time order can be influenced by the irrelevant\ntemporal information, especially the irrelevant temporal information is at the\nbeginning of the encoding. In addition, neglecting spatial information will\nlead to the relationship confusion of the words and detailed loss. Therefore,\nin this paper, we propose a novel recurrent video encoding method and a novel\nvisual spatial feature for the video captioning task. The recurrent encoding\nmodule encodes the video twice with the predicted key frame to avoid the\nirrelevant temporal information often occurring at the beginning and the end of\na video. The novel spatial features represent the spatial information in\ndifferent regions of a video and enrich the details of a caption. Experiments\non two benchmark datasets show superior performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 08:53:42 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Shi", "Xiangxi", ""], ["Cai", "Jianfei", ""], ["Joty", "Shafiq", ""], ["Gu", "Jiuxiang", ""]]}, {"id": "1907.12906", "submitter": "Silvia Chiappa", "authors": "Silvia Chiappa and Ulrich Paquet", "title": "Unsupervised Separation of Dynamics from Pixels", "comments": null, "journal-ref": "METRON, Springer, 2019", "doi": "10.1007/s40300-019-00155-4", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to learn the dynamics of multiple objects from image\nsequences in an unsupervised way. We introduce a probabilistic model that first\ngenerate noisy positions for each object through a separate linear state-space\nmodel, and then renders the positions of all objects in the same image through\na highly non-linear process. Such a linear representation of the dynamics\nenables us to propose an inference method that uses exact and efficient\ninference tools and that can be deployed to query the model in different ways\nwithout retraining.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 10:22:14 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Chiappa", "Silvia", ""], ["Paquet", "Ulrich", ""]]}, {"id": "1907.12908", "submitter": "Hossein Zeinali", "authors": "Hossein Zeinali, Themos Stafylakis, Georgia Athanasopoulou, Johan\n  Rohdin, Ioannis Gkinis, Luk\\'a\\v{s} Burget, Jan \"Honza'' \\v{C}ernock\\'y", "title": "Detecting Spoofing Attacks Using VGG and SincNet: BUT-Omilia Submission\n  to ASVspoof 2019 Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the system description of the joint efforts of Brno\nUniversity of Technology (BUT) and Omilia -- Conversational Intelligence for\nthe ASVSpoof2019 Spoofing and Countermeasures Challenge. The primary submission\nfor Physical access (PA) is a fusion of two VGG networks, trained on single and\ntwo-channels features. For Logical access (LA), our primary system is a fusion\nof VGG and the recently introduced SincNet architecture. The results on PA show\nthat the proposed networks yield very competitive performance in all conditions\nand achieved 86\\:\\% relative improvement compared to the official baseline. On\nthe other hand, the results on LA showed that although the proposed\narchitecture and training strategy performs very well on certain spoofing\nattacks, it fails to generalize to certain attacks that are unseen during\ntraining.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 17:27:40 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Zeinali", "Hossein", ""], ["Stafylakis", "Themos", ""], ["Athanasopoulou", "Georgia", ""], ["Rohdin", "Johan", ""], ["Gkinis", "Ioannis", ""], ["Burget", "Luk\u00e1\u0161", ""], ["\u010cernock\u00fd", "Jan \"Honza''", ""]]}, {"id": "1907.12910", "submitter": "Mason Swofford", "authors": "Mason Swofford, John Charles Peruzzi, Nathan Tsoi, Sydney Thompson,\n  Roberto Mart\\'in-Mart\\'in, Silvio Savarese, Marynel V\\'azquez", "title": "Improving Social Awareness Through DANTE: A Deep Affinity Network for\n  Clustering Conversational Interactants", "comments": null, "journal-ref": null, "doi": "10.1145/3392824", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven approach to detect conversational groups by\nidentifying spatial arrangements typical of these focused social encounters.\nOur approach uses a novel Deep Affinity Network (DANTE) to predict the\nlikelihood that two individuals in a scene are part of the same conversational\ngroup, considering their social context. The predicted pair-wise affinities are\nthen used in a graph clustering framework to identify both small (e.g., dyads)\nand large groups. The results from our evaluation on multiple, established\nbenchmarks suggest that combining powerful deep learning methods with classical\nclustering techniques can improve the detection of conversational groups in\ncomparison to prior approaches. Finally, we demonstrate the practicality of our\napproach in a human-robot interaction scenario. Our efforts show that our work\nadvances group detection not only in theory, but also in practice.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:09:03 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 10:41:12 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 21:56:52 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2020 05:03:36 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Swofford", "Mason", ""], ["Peruzzi", "John Charles", ""], ["Tsoi", "Nathan", ""], ["Thompson", "Sydney", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Savarese", "Silvio", ""], ["V\u00e1zquez", "Marynel", ""]]}, {"id": "1907.12914", "submitter": "Farid Ghareh Mohammadi", "authors": "Farid Ghareh Mohammadi, Farzan Shenavarmasouleh, M. Hadi Amini, Hamid\n  R. Arabnia", "title": "Evolutionary Algorithms and Efficient Data Analytics for Image\n  Processing", "comments": "8 pages,5 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steganography algorithms facilitate communication between a source and a\ndestination in a secret manner. This is done by embedding messages/text/data\ninto images without impacting the appearance of the resultant images/videos.\nSteganalysis is the science of determining if an image has secret messages\nembedded/hidden in it. Because there are numerous steganography algorithms, and\nsince each one of them requires a different type of steganalysis, the\nsteganalysis process is extremely challenging. Thus, researchers aim to develop\none universal steganalysis to detect all known and unknown steganography\nalgorithms, ideally in real-time. Universal steganalysis extracts a large\nnumber of features to distinguish stego images from cover images. However, the\nincrease in features leads to the problem of the curse of dimensionality (CoD),\nwhich is considered to be an NP-hard problem. This COD problem additionally\nmakes real-time steganalysis hard. A large number of features generates large\ndatasets for which machine learning cannot generate an optimal model.\nGenerating a machine learning based model also takes a long time which makes\nreal-time processing appear impossible in any optimization for time-intensive\nfields such as visual computing. Possible solutions for CoD are deep learning\nand evolutionary algorithms that overcome the machine learning limitations. In\nthis study, we investigate previously developed evolutionary algorithms for\nboosting real-time image processing and argue that they provide the most\npromising solutions for the CoD problem.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 16:13:53 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 05:14:06 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 16:10:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mohammadi", "Farid Ghareh", ""], ["Shenavarmasouleh", "Farzan", ""], ["Amini", "M. Hadi", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "1907.12915", "submitter": "Gregor Ramien", "authors": "Gregor N. Ramien, Paul F. Jaeger, Simon A. A. Kohl, Klaus H.\n  Maier-Hein", "title": "Reg R-CNN: Lesion Detection and Grading under Noisy Labels", "comments": "9 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the task of concurrently detecting and categorizing objects, the medical\nimaging community commonly adopts methods developed on natural images. Current\nstate-of-the-art object detectors are comprised of two stages: the first stage\ngenerates region proposals, the second stage subsequently categorizes them.\nUnlike in natural images, however, for anatomical structures of interest such\nas tumors, the appearance in the image (e.g., scale or intensity) links to a\nmalignancy grade that lies on a continuous ordinal scale. While classification\nmodels discard this ordinal relation between grades by discretizing the\ncontinuous scale to an unordered bag of categories, regression models are\ntrained with distance metrics, which preserve the relation. This advantage\nbecomes all the more important in the setting of label confusions on ambiguous\ndata sets, which is the usual case with medical images. To this end, we propose\nReg R-CNN, which replaces the second-stage classification model of a current\nobject detector with a regression model. We show the superiority of our\napproach on a public data set with 1026 patients and a series of toy\nexperiments. Code will be available at github.com/MIC-DKFZ/RegRCNN.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 10:16:28 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 12:00:30 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 10:16:05 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ramien", "Gregor N.", ""], ["Jaeger", "Paul F.", ""], ["Kohl", "Simon A. A.", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1907.12917", "submitter": "Dian Ang Yap", "authors": "Vinay Uday Prabhu, Dian Ang Yap, Alexander Wang, John Whaley", "title": "Covering up bias in CelebA-like datasets with Markov blankets: A\n  post-hoc cure for attribute prior avoidance", "comments": "Accepted for presentation at the first workshop on Invertible Neural\n  Networks and Normalizing Flows (ICML 2019), Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attribute prior avoidance entails subconscious or willful non-modeling of\n(meta)attributes that datasets are oft born with, such as the 40 semantic\nfacial attributes associated with the CelebA and CelebA-HQ datasets. The\nconsequences of this infirmity, we discover, are especially stark in\nstate-of-the-art deep generative models learned on these datasets that just\nmodel the pixel-space measurements, resulting in an inter-attribute bias-laden\nlatent space. This viscerally manifests itself when we perform face\nmanipulation experiments based on latent vector interpolations. In this paper,\nwe address this and propose a post-hoc solution that utilizes an Ising\nattribute prior learned in the attribute space and showcase its efficacy via\nqualitative experiments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 00:18:34 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Prabhu", "Vinay Uday", ""], ["Yap", "Dian Ang", ""], ["Wang", "Alexander", ""], ["Whaley", "John", ""]]}, {"id": "1907.12918", "submitter": "Haipeng Zeng", "authors": "Haipeng Zeng, Xingbo Wang, Aoyu Wu, Yong Wang, Quan Li, Alex Endert\n  and Huamin Qu", "title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "comments": "11 pages, 8 figures. Accepted by IEEE VAST 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934656", "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions play a key role in human communication and public presentations.\nHuman emotions are usually expressed through multiple modalities. Therefore,\nexploring multimodal emotions and their coherence is of great value for\nunderstanding emotional expressions in presentations and improving presentation\nskills. However, manually watching and studying presentation videos is often\ntedious and time-consuming. There is a lack of tool support to help conduct an\nefficient and in-depth multi-level analysis. Thus, in this paper, we introduce\nEmoCo, an interactive visual analytics system to facilitate efficient analysis\nof emotion coherence across facial, text, and audio modalities in presentation\nvideos. Our visualization system features a channel coherence view and a\nsentence clustering view that together enable users to obtain a quick overview\nof emotion coherence and its temporal evolution. In addition, a detail view and\nword view enable detailed exploration and comparison from the sentence level\nand word level, respectively. We thoroughly evaluate the proposed system and\nvisualization techniques through two usage scenarios based on TED Talk videos\nand interviews with two domain experts. The results demonstrate the\neffectiveness of our system in gaining insights into emotion coherence in\npresentations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 10:27:42 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 16:46:29 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Zeng", "Haipeng", ""], ["Wang", "Xingbo", ""], ["Wu", "Aoyu", ""], ["Wang", "Yong", ""], ["Li", "Quan", ""], ["Endert", "Alex", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.12919", "submitter": "Jo\\~ao Antunes", "authors": "Jo\\~ao Antunes, Pedro Abreu, Alexandre Bernardino, Asim Smailagic,\n  Daniel Siewiorek", "title": "Attention Filtering for Multi-person Spatiotemporal Action Detection on\n  Deep Two-Stream CNN Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action detection and recognition tasks have been the target of much focus in\nthe computer vision community due to their many applications, namely, security,\nrobotics and recommendation systems. Recently, datasets like AVA, provide\nmulti-person, multi-label, spatiotemporal action detection and recognition\nchallenges. Being unable to discern which portions of the input to use for\nclassification is a limitation of two-stream CNN approaches, once the vision\ntask involves several people with several labels. We address this limitation\nand improve the state-of-the-art performance of two-stream CNNs. In this paper\nwe present four contributions: our fovea attention filtering that highlights\ntargets for classification without discarding background; a generalized binary\nloss function designed for the AVA dataset; miniAVA, a partition of AVA that\nmaintains temporal continuity and class distribution with only one tenth of the\ndataset size; and ablation studies on alternative attention filters. Our\nmethod, using fovea attention filtering and our generalized binary loss,\nachieves a relative video mAP improvement of 20% over the two-stream baseline\nin AVA, and is competitive with the state-of-the-art in the UCF101-24. We also\nshow a relative video mAP improvement of 12.6% when using our generalized\nbinary loss over the standard sum-of-sigmoids.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 16:53:43 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Antunes", "Jo\u00e3o", ""], ["Abreu", "Pedro", ""], ["Bernardino", "Alexandre", ""], ["Smailagic", "Asim", ""], ["Siewiorek", "Daniel", ""]]}, {"id": "1907.12920", "submitter": "Elie Aljalbout", "authors": "Axel Sauer, Elie Aljalbout, Sami Haddadin", "title": "Tracking Holistic Object Representations", "comments": "Accepted for oral presentation at BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in visual tracking are based on siamese feature extractors\nand template matching. For this category of trackers, latest research focuses\non better feature embeddings and similarity measures. In this work, we focus on\nbuilding holistic object representations for tracking. We propose a framework\nthat is designed to be used on top of previous trackers without any need for\nfurther training of the siamese network. The framework leverages the idea of\nobtaining additional object templates during the tracking process. Since the\nnumber of stored templates is limited, our method only keeps the most diverse\nones. We achieve this by providing a new diversity measure in the space of\nsiamese features. The obtained representation contains information beyond the\nground truth object location provided to the system. It is then useful for\ntracking itself but also for further tasks which require a visual understanding\nof objects. Strong empirical results on tracking benchmarks indicate that our\nmethod can improve the performance and robustness of the underlying trackers\nwhile barely reducing their speed. In addition, our method is able to match\ncurrent state-of-the-art results, while using a simpler and older network\narchitecture and running three times faster.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 10:51:21 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 09:27:19 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Sauer", "Axel", ""], ["Aljalbout", "Elie", ""], ["Haddadin", "Sami", ""]]}, {"id": "1907.12921", "submitter": "Dr. B  Thirumala Rao", "authors": "K.Kavitha, B. Thirumala Rao", "title": "Evaluation of Distance Measures for Feature based Image Registration\n  using AlexNet", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2018.091034", "report-no": null, "categories": "cs.CV cs.DC eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image registration is a classic problem of computer vision with several\napplications across areas like defence, remote sensing, medicine etc. Feature\nbased image registration methods traditionally used hand-crafted feature\nextraction algorithms, which detect key points in an image and describe them\nusing a region around the point. Such features are matched using a threshold\neither on distances or ratio of distances computed between the feature\ndescriptors. Evolution of deep learning, in particular convolution neural\nnetworks, has enabled researchers to address several problems of vision such as\nrecognition, tracking, localization etc. Outputs of convolution layers or fully\nconnected layers of CNN which has been trained for applications like visual\nrecognition are proved to be effective when used as features in other\napplications such as retrieval. In this work, a deep CNN, AlexNet, is used in\nthe place of handcrafted features for feature extraction in the first stage of\nimage registration. However, there is a need to identify a suitable distance\nmeasure and a matching method for effective results. Several distance metrics\nhave been evaluated in the framework of nearest neighbour and nearest neighbour\nratio matching methods using benchmark dataset. Evaluation is done by comparing\nmatching and registration performance using metrics computed from ground truth.\n  Keywords: Distance measures; deep learning; feature detection; feature\ndescriptor; image matching\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 09:36:50 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Kavitha", "K.", ""], ["Rao", "B. Thirumala", ""]]}, {"id": "1907.12924", "submitter": "Hamidreza Kasaei", "authors": "S. Hamidreza Kasaei", "title": "Look Further to Recognize Better: Learning Shared Topics and\n  Category-Specific Dictionaries for Open-Ended 3D Object Recognition", "comments": "arXiv admin note: text overlap with arXiv:1902.03057", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service robots are expected to operate effectively in human-centric\nenvironments for long periods of time. In such realistic scenarios,\nfine-grained object categorization is as important as basic-level object\ncategorization. We tackle this problem by proposing an open-ended object\nrecognition approach which concurrently learns both the object categories and\nthe local features for encoding objects. In this work, each object is\nrepresented using a set of general latent visual topics and category-specific\ndictionaries. The general topics encode the common patterns of all categories,\nwhile the category-specific dictionary describes the content of each category\nin details. The proposed approach discovers both sets of general and specific\nrepresentations in an unsupervised fashion and updates them incrementally using\nnew object views. Experimental results show that our approach yields\nsignificant improvements over the previous state-of-the-art approaches\nconcerning scalability and object classification performance. Moreover, our\napproach demonstrates the capability of learning from very few training\nexamples in a real-world setting. Regarding computation time, the best result\nwas obtained with a Bag-of-Words method followed by a variant of the Latent\nDirichlet Allocation approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 08:28:49 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Kasaei", "S. Hamidreza", ""]]}, {"id": "1907.12926", "submitter": "Jayaraman J. Thiagarajan", "authors": "Jayaraman J. Thiagarajan, Satyananda Kashyap and Alexandros Karagyris", "title": "Distill-to-Label: Weakly Supervised Instance Labeling Using Knowledge\n  Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised instance labeling using only image-level labels, in lieu of\nexpensive fine-grained pixel annotations, is crucial in several applications\nincluding medical image analysis. In contrast to conventional instance\nsegmentation scenarios in computer vision, the problems that we consider are\ncharacterized by a small number of training images and non-local patterns that\nlead to the diagnosis. In this paper, we explore the use of multiple instance\nlearning (MIL) to design an instance label generator under this weakly\nsupervised setting. Motivated by the observation that an MIL model can handle\nbags of varying sizes, we propose to repurpose an MIL model originally trained\nfor bag-level classification to produce reliable predictions for single\ninstances, i.e., bags of size $1$. To this end, we introduce a novel\nregularization strategy based on virtual adversarial training for improving MIL\ntraining, and subsequently develop a knowledge distillation technique for\nrepurposing the trained MIL model. Using empirical studies on colon cancer and\nbreast cancer detection from histopathological images, we show that the\nproposed approach produces high-quality instance-level prediction and\nsignificantly outperforms state-of-the MIL methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 04:39:17 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Thiagarajan", "Jayaraman J.", ""], ["Kashyap", "Satyananda", ""], ["Karagyris", "Alexandros", ""]]}, {"id": "1907.12927", "submitter": "Xi Wang", "authors": "Xi Wang, Hao Chen, Luyang Luo, An-ran Ran, Poemen P. Chan, Clement C.\n  Tham, Carol Y. Cheung, Pheng-Ann Heng", "title": "Unifying Structure Analysis and Surrogate-driven Function Regression for\n  Glaucoma OCT Image Screening", "comments": "9 pages, 2 figures, MICCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography (OCT) imaging plays an important role in\nglaucoma diagnosis in clinical practice. Early detection and timely treatment\ncan prevent glaucoma patients from permanent vision loss. However, only a\ndearth of automated methods has been developed based on OCT images for glaucoma\nstudy. In this paper, we present a novel framework to effectively classify\nglaucoma OCT images from normal ones. A semi-supervised learning strategy with\nsmoothness assumption is applied for surrogate assignment of missing function\nregression labels. Besides, the proposed multi-task learning network is capable\nof exploring the structure and function relationship from the OCT image and\nvisual field measurement simultaneously, which contributes to classification\nperformance boosting. Essentially, we are the first to unify the structure\nanalysis and function regression for glaucoma screening. It is also worth\nnoting that we build the largest glaucoma OCT image dataset involving 4877\nvolumes to develop and evaluate the proposed method. Extensive experiments\ndemonstrate that our framework outperforms the baseline methods and two\nglaucoma experts by a large margin, achieving 93.2%, 93.2% and 97.8% on\naccuracy, F1 score and AUC, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 01:45:43 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Wang", "Xi", ""], ["Chen", "Hao", ""], ["Luo", "Luyang", ""], ["Ran", "An-ran", ""], ["Chan", "Poemen P.", ""], ["Tham", "Clement C.", ""], ["Cheung", "Carol Y.", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1907.12928", "submitter": "JunYu Wang", "authors": "Junyu (Jason) Wang, Rong Song", "title": "Improved Super-Resolution Convolution Neural Network for Large Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) is a very popular topic nowadays, which\nhas both research value and practical value. In daily life, we crop a large\nimage into sub-images to do super-resolution and then merge them together.\nAlthough convolution neural network performs very well in the research field,\nif we use it to do super-resolution, we can easily observe cutting lines from\nmerged pictures. To address these problems, in this paper, we propose a refined\narchitecture of SRCNN with 'Symmetric padding', 'Random learning' and 'Residual\nlearning'. Moreover, we have done a lot of experiments to prove our model\nperforms best among a lot of the state-of-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 02:44:07 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Junyu", "", "", "Jason"], ["Wang", "", ""], ["Song", "Rong", ""]]}, {"id": "1907.12929", "submitter": "Lex Fridman", "authors": "Li Ding, Lex Fridman", "title": "Object as Distribution", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a critical part of visual scene understanding. The\nrepresentation of the object in the detection task has important implications\non the efficiency and feasibility of annotation, robustness to occlusion, pose,\nlighting, and other visual sources of semantic uncertainty, and effectiveness\nin real-world applications (e.g., autonomous driving). Popular object\nrepresentations include 2D and 3D bounding boxes, polygons, splines, pixels,\nand voxels. Each have their strengths and weakness. In this work, we propose a\nnew representation of objects based on the bivariate normal distribution. This\ndistribution-based representation has the benefit of robust detection of\nhighly-overlapping objects and the potential for improved downstream tracking\nand instance segmentation tasks due to the statistical representation of object\nedges. We provide qualitative evaluation of this representation for the object\ndetection task and quantitative evaluation of its use in a baseline algorithm\nfor the instance segmentation task.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 23:10:21 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Ding", "Li", ""], ["Fridman", "Lex", ""]]}, {"id": "1907.12930", "submitter": "Huazhu Fu", "authors": "Shihao Zhang, Huazhu Fu, Yuguang Yan, Yubing Zhang, Qingyao Wu, Ming\n  Yang, Mingkui Tan, Yanwu Xu", "title": "Attention Guided Network for Retinal Image Segmentation", "comments": "Accepted to MICCAI 2019. Project page:\n  (https://github.com/HzFu/AGNet)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning structural information is critical for producing an ideal result in\nretinal image segmentation. Recently, convolutional neural networks have shown\na powerful ability to extract effective representations. However, convolutional\nand pooling operations filter out some useful structural information. In this\npaper, we propose an Attention Guided Network (AG-Net) to preserve the\nstructural information and guide the expanding operation. In our AG-Net, the\nguided filter is exploited as a structure sensitive expanding path to transfer\nstructural information from previous feature maps, and an attention block is\nintroduced to exclude the noise and reduce the negative influence of background\nfurther. The extensive experiments on two retinal image segmentation tasks\n(i.e., blood vessel segmentation, optic disc and cup segmentation) demonstrate\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:37:23 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 14:54:44 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 07:48:48 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Zhang", "Shihao", ""], ["Fu", "Huazhu", ""], ["Yan", "Yuguang", ""], ["Zhang", "Yubing", ""], ["Wu", "Qingyao", ""], ["Yang", "Ming", ""], ["Tan", "Mingkui", ""], ["Xu", "Yanwu", ""]]}, {"id": "1907.12934", "submitter": "Soufiane Belharbi", "authors": "Soufiane Belharbi, J\\'er\\^ome Rony, Jose Dolz, Ismail Ben Ayed, Luke\n  McCaffrey, Eric Granger", "title": "Min-max Entropy for Weakly Supervised Pointwise Localization", "comments": "27 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pointwise localization allows more precise localization and accurate\ninterpretability, compared to bounding box, in applications where objects are\nhighly unstructured such as in medical domain. In this work, we focus on weakly\nsupervised localization (WSL) where a model is trained to classify an image and\nlocalize regions of interest at pixel-level using only global image annotation.\nTypical convolutional attentions maps are prune to high false positive regions.\nTo alleviate this issue, we propose a new deep learning method for WSL,\ncomposed of a localizer and a classifier, where the localizer is constrained to\ndetermine relevant and irrelevant regions using conditional entropy (CE) with\nthe aim to reduce false positive regions. Experimental results on a public\nmedical dataset and two natural datasets, using Dice index, show that, compared\nto state of the art WSL methods, our proposal can provide significant\nimprovements in terms of image-level classification and pixel-level\nlocalization (low false positive) with robustness to overfitting. A public\nreproducible PyTorch implementation is provided in:\nhttps://github.com/sbelharbi/wsol-min-max-entropy-interpretability .\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 00:51:18 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 22:00:15 GMT"}, {"version": "v3", "created": "Sat, 31 Aug 2019 14:54:43 GMT"}, {"version": "v4", "created": "Thu, 26 Sep 2019 04:42:50 GMT"}, {"version": "v5", "created": "Thu, 21 Jan 2021 04:08:10 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Belharbi", "Soufiane", ""], ["Rony", "J\u00e9r\u00f4me", ""], ["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""], ["McCaffrey", "Luke", ""], ["Granger", "Eric", ""]]}, {"id": "1907.12935", "submitter": "Davit Soselia", "authors": "Davit Soselia, Shota Amashukeli, Irakli Koberidze, Levan Shugliashvili", "title": "RNN-based Online Handwritten Character Recognition Using Accelerometer\n  and Gyroscope Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This abstract explores an RNN-based approach to online handwritten\nrecognition problem. Our method uses data from an accelerometer and a gyroscope\nmounted on a handheld pen-like device to train and run a character pre-diction\nmodel. We have built a dataset of timestamped gyroscope and accelerometer data\ngathered during the manual process of handwriting Latin characters, labeled\nwith the character being written; in total, the dataset con-sists of 1500\ngyroscope and accelerometer data sequenc-es for 8 characters of the Latin\nalphabet from 6 different people, and 20 characters, each 1500 samples from\nGeorgian alphabet from 5 different people. with each sequence containing the\ngyroscope and accelerometer data captured during the writing of a particular\ncharacter sampled once every 10ms. We train an RNN-based neural network\narchitecture on this dataset to predict the character being written. The model\nis optimized with categorical cross-entropy loss and RMSprop optimizer and\nachieves high accuracy on test data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:44:00 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Soselia", "Davit", ""], ["Amashukeli", "Shota", ""], ["Koberidze", "Irakli", ""], ["Shugliashvili", "Levan", ""]]}, {"id": "1907.12945", "submitter": "Tao Sun", "authors": "Tao Sun, Roberto Barrio, Marcos Rodriguez, Hao Jiang", "title": "Inertial nonconvex alternating minimizations for the image deblurring", "comments": "Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2924339", "report-no": null, "categories": "math.OC cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image processing, Total Variation (TV) regularization models are commonly\nused to recover blurred images. One of the most efficient and popular methods\nto solve the convex TV problem is the Alternating Direction Method of\nMultipliers (ADMM) algorithm, recently extended using the inertial proximal\npoint method. Although all the classical studies focus on only a convex\nformulation, recent articles are paying increasing attention to the nonconvex\nmethodology due to its good numerical performance and properties. In this\npaper, we propose to extend the classical formulation with a novel nonconvex\nAlternating Direction Method of Multipliers with the Inertial technique\n(IADMM). Under certain assumptions on the parameters, we prove the convergence\nof the algorithm with the help of the Kurdyka-{\\L}ojasiewicz property. We also\npresent numerical simulations on classical TV image reconstruction problems to\nillustrate the efficiency of the new algorithm and its behavior compared with\nthe well established ADMM method.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 02:50:45 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Sun", "Tao", ""], ["Barrio", "Roberto", ""], ["Rodriguez", "Marcos", ""], ["Jiang", "Hao", ""]]}, {"id": "1907.12949", "submitter": "Sara Moccia", "authors": "Sara Moccia, Lucia Migliorelli, Rocco Pietrini, Emanuele Frontoni", "title": "Preterm infants' limb-pose estimation from depth images using\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preterm infants' limb-pose estimation is a crucial but challenging task,\nwhich may improve patients' care and facilitate clinicians in infant's\nmovements monitoring. Work in the literature either provides approaches to\nwhole-body segmentation and tracking, which, however, has poor clinical value,\nor retrieve a posteriori limb pose from limb segmentation, increasing\ncomputational costs and introducing inaccuracy sources. In this paper, we\naddress the problem of limb-pose estimation under a different point of view. We\nproposed a 2D fully-convolutional neural network for roughly detecting limb\njoints and joint connections, followed by a regression convolutional neural\nnetwork for accurate joint and joint-connection position estimation. Joints\nfrom the same limb are then connected with a maximum bipartite matching\napproach. Our analysis does not require any prior modeling of infants' body\nstructure, neither any manual interventions. For developing and testing the\nproposed approach, we built a dataset of four videos (video length = 90 s)\nrecorded with a depth sensor in a neonatal intensive care unit (NICU) during\nthe actual clinical practice, achieving median root mean square distance\n[pixels] of 10.790 (right arm), 10.542 (left arm), 8.294 (right leg), 11.270\n(left leg) with respect to the ground-truth limb pose. The idea of estimating\nlimb pose directly from depth images may represent a future paradigm for\naddressing the problem of preterm-infants' movement monitoring and offer all\npossible support to clinicians in NICUs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 10:15:57 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Moccia", "Sara", ""], ["Migliorelli", "Lucia", ""], ["Pietrini", "Rocco", ""], ["Frontoni", "Emanuele", ""]]}, {"id": "1907.12956", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Elham Azimi, Amirali Abdolrashidi", "title": "FingerNet: Pushing The Limits of Fingerprint Recognition Using\n  Convolutional Neural Network", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.09380", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint recognition has been utilized for cellphone authentication,\nairport security and beyond. Many different features and algorithms have been\nproposed to improve fingerprint recognition. In this paper, we propose an\nend-to-end deep learning framework for fingerprint recognition using\nconvolutional neural networks (CNNs) which can jointly learn the feature\nrepresentation and perform recognition. We train our model on a large-scale\nfingerprint recognition dataset, and improve over previous approaches in terms\nof accuracy. Our proposed model is able to achieve a very high recognition\naccuracy on a well-known fingerprint dataset. We believe this framework can be\nwidely used for biometrics recognition tasks, making more scalable and accurate\nsystems possible. We have also used a visualization technique to highlight the\nimportant areas in an input fingerprint image, that mostly impact the\nrecognition results.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 21:00:56 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Minaee", "Shervin", ""], ["Azimi", "Elham", ""], ["Abdolrashidi", "Amirali", ""]]}, {"id": "1907.12975", "submitter": "Florian Kromp Dipl. Ing.", "authors": "Florian Kromp, Lukas Fischer, Eva Bozsaky, Inge Ambros, Wolfgang\n  Doerr, Sabine Taschner-Mandl, Peter Ambros, Allan Hanbury", "title": "Deep Learning architectures for generalized immunofluorescence based\n  nuclear image segmentation", "comments": "10 pages + 3 supplementary pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.TO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Separating and labeling each instance of a nucleus (instance-aware\nsegmentation) is the key challenge in segmenting single cell nuclei on\nfluorescence microscopy images. Deep Neural Networks can learn the implicit\ntransformation of a nuclear image into a probability map indicating the class\nmembership of each pixel (nucleus or background), but the use of\npost-processing steps to turn the probability map into a labeled object mask is\nerror-prone. This especially accounts for nuclear images of tissue sections and\nnuclear images across varying tissue preparations. In this work, we aim to\nevaluate the performance of state-of-the-art deep learning architectures to\nsegment nuclei in fluorescence images of various tissue origins and sample\npreparation types without post-processing. We compare architectures that\noperate on pixel to pixel translation and an architecture that operates on\nobject detection and subsequent locally applied segmentation. In addition, we\npropose a novel strategy to create artificial images to extend the training\nset. We evaluate the influence of ground truth annotation quality, image scale\nand segmentation complexity on segmentation performance. Results show that\nthree out of four deep learning architectures (U-Net, U-Net with ResNet34\nbackbone, Mask R-CNN) can segment fluorescent nuclear images on most of the\nsample preparation types and tissue origins with satisfactory segmentation\nperformance. Mask R-CNN, an architecture designed to address instance aware\nsegmentation tasks, outperforms other architectures. Equal nuclear mean size,\nconsistent nuclear annotations and the use of artificially generated images\nresult in overall acceptable precision and recall across different tissues and\nsample preparation types.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 14:23:29 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Kromp", "Florian", ""], ["Fischer", "Lukas", ""], ["Bozsaky", "Eva", ""], ["Ambros", "Inge", ""], ["Doerr", "Wolfgang", ""], ["Taschner-Mandl", "Sabine", ""], ["Ambros", "Peter", ""], ["Hanbury", "Allan", ""]]}, {"id": "1907.12993", "submitter": "Gautam Pai", "authors": "Gautam Pai, Mor Joseph-Rivlin, Ron Kimmel", "title": "Bilateral Operators for Functional Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A majority of shape correspondence frameworks are based on devising pointwise\nand pairwise constraints on the correspondence map. The functional maps\nframework allows for formulating these constraints in the spectral domain. In\nthis paper, we develop a functional map framework for the shape correspondence\nproblem by constructing pairwise constraints using point-wise descriptors. Our\ncore observation is that, every point-wise descriptor allows for the\nconstruction a pairwise kernel operator whose low frequency eigenfunctions\ndepict regions of similar descriptor values at various scales of frequency. By\naggregating the pairwise information from the descriptor and the intrinsic\ngeometry of the surface encoded in the heat kernel, we construct a hybrid\nkernel and call it the bilateral operator. Analogous to the edge preserving\nbilateral filter in image processing, the action of the bilateral operator on a\nfunction defined over the manifold yields a descriptor dependent local\nsmoothing of that function. By forcing the correspondence map to commute with\nthe Bilateral operator, we show that we can maximally exploit the information\nfrom a given set of pointwise descriptors in a functional map framework.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 14:55:23 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Pai", "Gautam", ""], ["Joseph-Rivlin", "Mor", ""], ["Kimmel", "Ron", ""]]}, {"id": "1907.13020", "submitter": "Dongming Wei", "authors": "Dongming Wei, Sahar Ahmad, Jiayu Huo, Wen Peng, Yunhao Ge, Zhong Xue,\n  Pew-Thian Yap, Wentao Li, Dinggang Shen, Qian Wang", "title": "Synthesis and Inpainting-Based MR-CT Registration for Image-Guided\n  Thermal Ablation of Liver Tumors", "comments": "Accepted in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal ablation is a minimally invasive procedure for treat-ing small or\nunresectable tumors. Although CT is widely used for guiding ablation\nprocedures, the contrast of tumors against surrounding normal tissues in CT\nimages is often poor, aggravating the difficulty in accurate thermal ablation.\nIn this paper, we propose a fast MR-CT image registration method to overlay a\npre-procedural MR (pMR) image onto an intra-procedural CT (iCT) image for\nguiding the thermal ablation of liver tumors. By first using a Cycle-GAN model\nwith mutual information constraint to generate synthesized CT (sCT) image from\nthe cor-responding pMR, pre-procedural MR-CT image registration is carried out\nthrough traditional mono-modality CT-CT image registration. At the\nintra-procedural stage, a partial-convolution-based network is first used to\ninpaint the probe and its artifacts in the iCT image. Then, an unsupervised\nregistration network is used to efficiently align the pre-procedural CT (pCT)\nwith the inpainted iCT (inpCT) image. The final transformation from pMR to iCT\nis obtained by combining the two estimated transformations,i.e., (1) from the\npMR image space to the pCT image space (through sCT) and (2) from the pCT image\nspace to the iCT image space (through inpCT). Experimental results confirm that\nthe proposed method achieves high registration accuracy with a very fast\ncomputational speed.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 15:37:16 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Wei", "Dongming", ""], ["Ahmad", "Sahar", ""], ["Huo", "Jiayu", ""], ["Peng", "Wen", ""], ["Ge", "Yunhao", ""], ["Xue", "Zhong", ""], ["Yap", "Pew-Thian", ""], ["Li", "Wentao", ""], ["Shen", "Dinggang", ""], ["Wang", "Qian", ""]]}, {"id": "1907.13025", "submitter": "Carlos Caetano", "authors": "Carlos Caetano, Jessica Sena, Fran\\c{c}ois Br\\'emond, Jefersson A. dos\n  Santos and William Robson Schwartz", "title": "SkeleMotion: A New Representation of Skeleton Joint Sequences Based on\n  Motion Information for 3D Action Recognition", "comments": "16-th IEEE International Conference on Advanced Video and\n  Signal-based Surveillance (AVSS2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the availability of large-scale skeleton datasets, 3D human action\nrecognition has recently called the attention of computer vision community.\nMany works have focused on encoding skeleton data as skeleton image\nrepresentations based on spatial structure of the skeleton joints, in which the\ntemporal dynamics of the sequence is encoded as variations in columns and the\nspatial structure of each frame is represented as rows of a matrix. To further\nimprove such representations, we introduce a novel skeleton image\nrepresentation to be used as input of Convolutional Neural Networks (CNNs),\nnamed SkeleMotion. The proposed approach encodes the temporal dynamics by\nexplicitly computing the magnitude and orientation values of the skeleton\njoints. Different temporal scales are employed to compute motion values to\naggregate more temporal dynamics to the representation making it able to\ncapture longrange joint interactions involved in actions as well as filtering\nnoisy motion values. Experimental results demonstrate the effectiveness of the\nproposed representation on 3D action recognition outperforming the\nstate-of-the-art on NTU RGB+D 120 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 15:40:07 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Caetano", "Carlos", ""], ["Sena", "Jessica", ""], ["Br\u00e9mond", "Fran\u00e7ois", ""], ["Santos", "Jefersson A. dos", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1907.13033", "submitter": "Jiaxin Cai", "authors": "Jiaxin Cai, Hongfeng Zhu", "title": "Lung image segmentation by generative adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "2019 International Conference on Image and Video Processing, and\n  Artificial Intelligence. Shanghai, China, Aug, 2019", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung image segmentation plays an important role in computer-aid pulmonary\ndiseases diagnosis and treatment. This paper proposed a lung image segmentation\nmethod by generative adversarial networks. We employed a variety of generative\nadversarial networks and use its capability of image translation to perform\nimage segmentation. The generative adversarial networks was employed to\ntranslate the original lung image to the segmented image. The generative\nadversarial networks based segmentation method was test on real lung image data\nset. Experimental results shows that the proposed method is effective and\noutperform state-of-the art method.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 15:47:02 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Cai", "Jiaxin", ""], ["Zhu", "Hongfeng", ""]]}, {"id": "1907.13037", "submitter": "Abulikemu Abuduweili", "authors": "Abulikemu Abuduweili and Xin Wu and Xingchen Tao", "title": "Efficient Method for Categorize Animals in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic species classification in camera traps would greatly help the\nbiodiversity monitoring and species analysis in the earth. In order to\naccelerate the development of automatic species classification task, \"Microsoft\nAI for Earth\" have prepared a challenge in FGVC6 workshop at CVPR 2019, which\ncalled \"iWildCam 2019 competition\". In this work, we propose an efficient\nmethod for categorizing animals in the wild. We transfer the state-of-the-art\nImagaNet pretrained models to the problem. To improve the generalization and\nrobustness of the model, we utilize efficient image augmentation and\nregularization strategies, like cutout, mixup and label-smoothing. Finally, we\nuse ensemble learning to increase the performance of the model. Thanks to\nadvanced regularization strategies and ensemble learning, we got top 7/336\nplaces in the final leaderboard. Source code of this work is available at\nhttps://github.com/Walleclipse/iWildCam_2019_FGVC6\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 15:51:56 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Abuduweili", "Abulikemu", ""], ["Wu", "Xin", ""], ["Tao", "Xingchen", ""]]}, {"id": "1907.13051", "submitter": "Zhengyuan Yang", "authors": "Zhengyuan Yang, Yuncheng Li, Linjie Yang, Ning Zhang, Jiebo Luo", "title": "Weakly Supervised Body Part Segmentation with Pose based Part Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human body part segmentation refers to the task of predicting the semantic\nsegmentation mask for each body part. Fully supervised body part segmentation\nmethods achieve good performances but require an enormous amount of effort to\nannotate part masks for training. In contrast to high annotation costs needed\nfor a limited number of part mask annotations, a large number of weak labels\nsuch as poses and full body masks already exist and contain relevant\ninformation. Motivated by the possibility of using existing weak labels, we\npropose the first weakly supervised body part segmentation framework. The core\nidea is first converting the sparse weak labels such as keypoints to the\ninitial estimate of body part masks, and then iteratively refine the part mask\npredictions. We name the initial part masks estimated from poses the \"part\npriors.\" With sufficient extra weak labels, our weakly supervised framework\nachieves a comparable performance (62.0% mIoU) to the fully supervised method\n(63.6% mIoU) on the Pascal-Person-Part dataset. Furthermore, in the extended\nsemi-supervised setting, the proposed framework outperforms the state-of-art\nmethods. Moreover, we extend our proposed framework to other\nkeypoint-supervised part segmentation tasks such as face parsing.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 16:21:11 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 18:53:06 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yang", "Zhengyuan", ""], ["Li", "Yuncheng", ""], ["Yang", "Linjie", ""], ["Zhang", "Ning", ""], ["Luo", "Jiebo", ""]]}, {"id": "1907.13052", "submitter": "Martin Engelcke", "authors": "Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, Ingmar Posner", "title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric\n  Latent Representations", "comments": "Published at the International Conference on Learning Representations\n  (ICLR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative latent-variable models are emerging as promising tools in robotics\nand reinforcement learning. Yet, even though tasks in these domains typically\ninvolve distinct objects, most state-of-the-art generative models do not\nexplicitly capture the compositional nature of visual scenes. Two recent\nexceptions, MONet and IODINE, decompose scenes into objects in an unsupervised\nfashion. Their underlying generative processes, however, do not account for\ncomponent interactions. Hence, neither of them allows for principled sampling\nof novel scenes. Here we present GENESIS, the first object-centric generative\nmodel of 3D visual scenes capable of both decomposing and generating scenes by\ncapturing relationships between scene components. GENESIS parameterises a\nspatial GMM over images which is decoded from a set of object-centric latent\nvariables that are either inferred sequentially in an amortised fashion or\nsampled from an autoregressive prior. We train GENESIS on several publicly\navailable datasets and evaluate its performance on scene generation,\ndecomposition, and semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 16:22:39 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 20:19:08 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 14:02:16 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2020 10:31:22 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Engelcke", "Martin", ""], ["Kosiorek", "Adam R.", ""], ["Jones", "Oiwi Parker", ""], ["Posner", "Ingmar", ""]]}, {"id": "1907.13054", "submitter": "Volker Fischer", "authors": "Lukas Hoyer, Mauricio Munoz, Prateek Katiyar, Anna Khoreva, Volker\n  Fischer", "title": "Grid Saliency for Context Explanations of Semantic Segmentation", "comments": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a growing interest in developing saliency methods\nthat provide visual explanations of network predictions. Still, the usability\nof existing methods is limited to image classification models. To overcome this\nlimitation, we extend the existing approaches to generate grid saliencies,\nwhich provide spatially coherent visual explanations for (pixel-level) dense\nprediction networks. As the proposed grid saliency allows to spatially\ndisentangle the object and its context, we specifically explore its potential\nto produce context explanations for semantic segmentation networks, discovering\nwhich context most influences the class predictions inside a target object\narea. We investigate the effectiveness of grid saliency on a synthetic dataset\nwith an artificially induced bias between objects and their context as well as\non the real-world Cityscapes dataset using state-of-the-art segmentation\nnetworks. Our results show that grid saliency can be successfully used to\nprovide easily interpretable context explanations and, moreover, can be\nemployed for detecting and localizing contextual biases present in the data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 16:25:52 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 12:29:44 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Hoyer", "Lukas", ""], ["Munoz", "Mauricio", ""], ["Katiyar", "Prateek", ""], ["Khoreva", "Anna", ""], ["Fischer", "Volker", ""]]}, {"id": "1907.13057", "submitter": "Jungkyu Park", "authors": "Jungkyu Park, Jason Phang, Yiqiu Shen, Nan Wu, S. Gene Kim, Linda Moy,\n  Kyunghyun Cho, Krzysztof J. Geras", "title": "Screening Mammogram Classification with Prior Exams", "comments": "MIDL 2019 [arXiv:1907.08612]", "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/HkgCdUaMq4", "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Radiologists typically compare a patient's most recent breast cancer\nscreening exam to their previous ones in making informed diagnoses. To reflect\nthis practice, we propose new neural network models that compare pairs of\nscreening mammograms from the same patient. We train and evaluate our proposed\nmodels on over 665,000 pairs of images (over 166,000 pairs of exams). Our best\nmodel achieves an AUC of 0.866 in predicting malignancy in patients who\nunderwent breast cancer screening, reducing the error rate of the corresponding\nbaseline.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 16:34:53 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Park", "Jungkyu", ""], ["Phang", "Jason", ""], ["Shen", "Yiqiu", ""], ["Wu", "Nan", ""], ["Kim", "S. Gene", ""], ["Moy", "Linda", ""], ["Cho", "Kyunghyun", ""], ["Geras", "Krzysztof J.", ""]]}, {"id": "1907.13075", "submitter": "Pau Rodr\\'iguez L\\'opez", "authors": "Pau Rodr\\'iguez L\\'opez, Diego Velazquez Dorta, Guillem Cucurull\n  Preixens, Josep M. Gonfaus, F. Xavier Roca Marva, Jordi Gonz\\`alez Sabat\\'e", "title": "Pay attention to the activations: a modular attention mechanism for\n  fine-grained image recognition", "comments": "IEEE Transactions on Multimedia, ECCV extension", "journal-ref": null, "doi": "10.1109/TMM.2019.2928494", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image recognition is central to many multimedia tasks such as\nsearch, retrieval and captioning. Unfortunately, these tasks are still\nchallenging since the appearance of samples of the same class can be more\ndifferent than those from different classes. Attention has been typically\nimplemented in neural networks by selecting the most informative regions of the\nimage that improve classification. In contrast, in this paper, attention is not\napplied at the image level but to the convolutional feature activations. In\nessence, with our approach, the neural model learns to attend to lower-level\nfeature activations without requiring part annotations and uses those\nactivations to update and rectify the output likelihood distribution. The\nproposed mechanism is modular, architecture-independent and efficient in terms\nof both parameters and computation required. Experiments demonstrate that\nwell-known networks such as Wide Residual Networks and ResNeXt, when augmented\nwith our approach, systematically improve their classification accuracy and\nbecome more robust to changes in deformation and pose and to the presence of\nclutter. As a result, our proposal reaches state-of-the-art classification\naccuracies in CIFAR-10, the Adience gender recognition task, Stanford Dogs, and\nUEC-Food100 while obtaining competitive performance in ImageNet, CIFAR-100,\nCUB200 Birds, and Stanford Cars. In addition, we analyze the different\ncomponents of our model, showing that the proposed attention modules succeed in\nfinding the most discriminative regions of the image. Finally, as a proof of\nconcept, we demonstrate that with only local predictions, an augmented neural\nnetwork can successfully classify an image before reaching any fully connected\nlayer, thus reducing the computational amount up to 10%.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 17:00:15 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["L\u00f3pez", "Pau Rodr\u00edguez", ""], ["Dorta", "Diego Velazquez", ""], ["Preixens", "Guillem Cucurull", ""], ["Gonfaus", "Josep M.", ""], ["Marva", "F. Xavier Roca", ""], ["Sabat\u00e9", "Jordi Gonz\u00e0lez", ""]]}, {"id": "1907.13079", "submitter": "Yuwen Xiong", "authors": "Yuwen Xiong, Mengye Ren, Renjie Liao, Kelvin Wong, Raquel Urtasun", "title": "Deformable Filter Convolution for Point Cloud Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are the native output of many real-world 3D sensors. To borrow\nthe success of 2D convolutional network architectures, a majority of popular 3D\nperception models voxelize the points, which can result in a loss of local\ngeometric details that cannot be recovered. In this paper, we propose a novel\nlearnable convolution layer for processing 3D point cloud data directly.\nInstead of discretizing points into fixed voxels, we deform our learnable 3D\nfilters to match with the point cloud shape. We propose to combine voxelized\nbackbone networks with our deformable filter layer at 1) the network input\nstream and 2) the output prediction layers to enhance point level reasoning. We\nobtain state-of-the-art results on LiDAR semantic segmentation and producing a\nsignificant gain in performance on LiDAR object detection.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 17:09:12 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Xiong", "Yuwen", ""], ["Ren", "Mengye", ""], ["Liao", "Renjie", ""], ["Wong", "Kelvin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1907.13106", "submitter": "Rajeev Yasarla", "authors": "Rajeev Yasarla (Student Member, IEEE), Federico Perazzi (Member,\n  IEEE), Vishal M. Patel (Senior Member, IEEE)", "title": "Deblurring Face Images using Uncertainty Guided Multi-Stream Semantic\n  Networks", "comments": "Accepted at TIP 2020", "journal-ref": null, "doi": "10.1109/TIP.2020.2990354", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel multi-stream architecture and training methodology that\nexploits semantic labels for facial image deblurring. The proposed Uncertainty\nGuided Multi- Stream Semantic Network (UMSN) processes regions belonging to\neach semantic class independently and learns to combine their outputs into the\nfinal deblurred result. Pixel-wise semantic labels are obtained using a\nsegmentation network. A predicted confidence measure is used during training to\nguide the network towards the challenging regions of the human face such as the\neyes and nose. The entire network is trained in an end- to-end fashion.\nComprehensive experiments on three different face datasets demonstrate that the\nproposed method achieves significant improvements over the recent\nstate-of-the-art face deblurring methods. Code is available at:\nhttps://github.com/ rajeevyasarla/UMSN-Face-Deblurring\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 17:41:41 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 21:08:03 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Yasarla", "Rajeev", "", "Student Member, IEEE"], ["Perazzi", "Federico", "", "Member,\n  IEEE"], ["Patel", "Vishal M.", "", "Senior Member, IEEE"]]}, {"id": "1907.13123", "submitter": "Chen Kong", "authors": "Chen Kong and Simon Lucey", "title": "Deep Non-Rigid Structure from Motion with Missing Data", "comments": "Submission to PAMI", "journal-ref": null, "doi": null, "report-no": "pami00", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Rigid Structure from Motion (NRSfM) refers to the problem of\nreconstructing cameras and the 3D point cloud of a non-rigid object from an\nensemble of images with 2D correspondences. Current NRSfM algorithms are\nlimited from two perspectives: (i) the number of images, and (ii) the type of\nshape variability they can handle. These difficulties stem from the inherent\nconflict between the condition of the system and the degrees of freedom needing\nto be modeled -- which has hampered its practical utility for many applications\nwithin vision. In this paper we propose a novel hierarchical sparse coding\nmodel for NRSFM which can overcome (i) and (ii) to such an extent, that NRSFM\ncan be applied to problems in vision previously thought too ill posed. Our\napproach is realized in practice as the training of an unsupervised deep neural\nnetwork (DNN) auto-encoder with a unique architecture that is able to\ndisentangle pose from 3D structure. Using modern deep learning computational\nplatforms allows us to solve NRSfM problems at an unprecedented scale and shape\ncomplexity. Our approach has no 3D supervision, relying solely on 2D point\ncorrespondences. Further, our approach is also able to handle missing/occluded\n2D points without the need for matrix completion. Extensive experiments\ndemonstrate the impressive performance of our approach where we exhibit\nsuperior precision and robustness against all available state-of-the-art works\nin some instances by an order of magnitude. We further propose a new quality\nmeasure (based on the network weights) which circumvents the need for 3D\nground-truth to ascertain the confidence we have in the reconstructability. We\nbelieve our work to be a significant advance over state-of-the-art in NRSFM.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 00:35:15 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 18:46:07 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Kong", "Chen", ""], ["Lucey", "Simon", ""]]}, {"id": "1907.13124", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Arnout Van Messem, Wesley De Neve", "title": "Impact of Adversarial Examples on Deep Learning Models for Biomedical\n  Image Segmentation", "comments": "Accepted for the 22nd International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models, which are increasingly being used in the field of\nmedical image analysis, come with a major security risk, namely, their\nvulnerability to adversarial examples. Adversarial examples are carefully\ncrafted samples that force machine learning models to make mistakes during\ntesting time. These malicious samples have been shown to be highly effective in\nmisguiding classification tasks. However, research on the influence of\nadversarial examples on segmentation is significantly lacking. Given that a\nlarge portion of medical imaging problems are effectively segmentation\nproblems, we analyze the impact of adversarial examples on deep learning-based\nimage segmentation models. Specifically, we expose the vulnerability of these\nmodels to adversarial examples by proposing the Adaptive Segmentation Mask\nAttack (ASMA). This novel algorithm makes it possible to craft targeted\nadversarial examples that come with (1) high intersection-over-union rates\nbetween the target adversarial mask and the prediction and (2) with\nperturbation that is, for the most part, invisible to the bare eye. We lay out\nexperimental and visual evidence by showing results obtained for the ISIC skin\nlesion segmentation challenge and the problem of glaucoma optic disc\nsegmentation. An implementation of this algorithm and additional examples can\nbe found at https://github.com/utkuozbulak/adaptive-segmentation-mask-attack.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 06:03:57 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Ozbulak", "Utku", ""], ["Van Messem", "Arnout", ""], ["De Neve", "Wesley", ""]]}, {"id": "1907.13185", "submitter": "Quoc-Huy Tran", "authors": "Bingbing Zhuang, Quoc-Huy Tran, Pan Ji, Gim Hee Lee, Loong Fah Cheong,\n  Manmohan Chandraker", "title": "Degeneracy in Self-Calibration Revisited and a Deep Learning Solution\n  for Uncalibrated SLAM", "comments": "To appear at IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-calibration of camera intrinsics and radial distortion has a long\nhistory of research in the computer vision community. However, it remains rare\nto see real applications of such techniques to modern Simultaneous Localization\nAnd Mapping (SLAM) systems, especially in driving scenarios. In this paper, we\nrevisit the geometric approach to this problem, and provide a theoretical proof\nthat explicitly shows the ambiguity between radial distortion and scene depth\nwhen two-view geometry is used to self-calibrate the radial distortion. In view\nof such geometric degeneracy, we propose a learning approach that trains a\nconvolutional neural network (CNN) on a large amount of synthetic data. We\ndemonstrate the utility of our proposed method by applying it as a\ncheckerboard-free calibration tool for SLAM, achieving comparable or superior\nperformance to previous learning and hand-crafted methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 19:07:13 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Zhuang", "Bingbing", ""], ["Tran", "Quoc-Huy", ""], ["Ji", "Pan", ""], ["Lee", "Gim Hee", ""], ["Cheong", "Loong Fah", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1907.13236", "submitter": "Christopher Xie", "authors": "Christopher Xie, Yu Xiang, Arsalan Mousavian, Dieter Fox", "title": "The Best of Both Modes: Separately Leveraging RGB and Depth for Unseen\n  Object Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to function in unstructured environments, robots need the ability to\nrecognize unseen novel objects. We take a step in this direction by tackling\nthe problem of segmenting unseen object instances in tabletop environments.\nHowever, the type of large-scale real-world dataset required for this task\ntypically does not exist for most robotic settings, which motivates the use of\nsynthetic data. We propose a novel method that separately leverages synthetic\nRGB and synthetic depth for unseen object instance segmentation. Our method is\ncomprised of two stages where the first stage operates only on depth to produce\nrough initial masks, and the second stage refines these masks with RGB.\nSurprisingly, our framework is able to learn from synthetic RGB-D data where\nthe RGB is non-photorealistic. To train our method, we introduce a large-scale\nsynthetic dataset of random objects on tabletops. We show that our method,\ntrained on this dataset, can produce sharp and accurate masks, outperforming\nstate-of-the-art methods on unseen object instance segmentation. We also show\nthat our method can segment unseen objects for robot grasping. Code, models and\nvideo can be found at\nhttps://rse-lab.cs.washington.edu/projects/unseen-object-instance-segmentation/.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 21:35:33 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 02:05:00 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Xie", "Christopher", ""], ["Xiang", "Yu", ""], ["Mousavian", "Arsalan", ""], ["Fox", "Dieter", ""]]}, {"id": "1907.13242", "submitter": "Zhenhua Feng", "authors": "Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu and Josef Kittler", "title": "Joint Group Feature Selection and Discriminative Filter Learning for\n  Robust Visual Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Group Feature Selection method for Discriminative\nCorrelation Filters (GFS-DCF) based visual object tracking. The key innovation\nof the proposed method is to perform group feature selection across both\nchannel and spatial dimensions, thus to pinpoint the structural relevance of\nmulti-channel features to the filtering system. In contrast to the widely used\nspatial regularisation or feature selection methods, to the best of our\nknowledge, this is the first time that channel selection has been advocated for\nDCF-based tracking. We demonstrate that our GFS-DCF method is able to\nsignificantly improve the performance of a DCF tracker equipped with deep\nneural network features. In addition, our GFS-DCF enables joint feature\nselection and filter learning, achieving enhanced discrimination and\ninterpretability of the learned filters.\n  To further improve the performance, we adaptively integrate historical\ninformation by constraining filters to be smooth across temporal frames, using\nan efficient low-rank approximation. By design, specific\ntemporal-spatial-channel configurations are dynamically learned in the tracking\nprocess, highlighting the relevant features, and alleviating the performance\ndegrading impact of less discriminative representations and reducing\ninformation redundancy. The experimental results obtained on OTB2013, OTB2015,\nVOT2017, VOT2018 and TrackingNet demonstrate the merits of our GFS-DCF and its\nsuperiority over the state-of-the-art trackers. The code is publicly available\nat https://github.com/XU-TIANYANG/GFS-DCF.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 21:41:59 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 13:38:05 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Xu", "Tianyang", ""], ["Feng", "Zhen-Hua", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1907.13255", "submitter": "Amit Kumar", "authors": "Amit Kumar, Rama Chellappa", "title": "Landmark Detection in Low Resolution Faces with Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Landmark detection algorithms trained on high resolution images perform\npoorly on datasets containing low resolution images. This deters the\nperformance of algorithms relying on quality landmarks, for example, face\nrecognition. To the best of our knowledge, there does not exist any dataset\nconsisting of low resolution face images along with their annotated landmarks,\nmaking supervised training infeasible. In this paper, we present a\nsemi-supervised approach to predict landmarks on low resolution images by\nlearning them from labeled high resolution images. The objective of this work\nis to show that predicting landmarks directly on low resolution images is more\neffective than the current practice of aligning images after rescaling or\nsuperresolution. In a two-step process, the proposed approach first learns to\ngenerate low resolution images by modeling the distribution of target low\nresolution images. In the second stage, the roles of generated images and real\nlow resolution images are switched and the model learns to predict landmarks\nfor real low resolution images from generated low resolution images. With\nextensive experimentation, we study the impact of each of the design choices\nand also show that prediction of landmarks directly on low resolution images\nimproves the performance of important tasks such as face recognition in low\nresolution images.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 23:12:01 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kumar", "Amit", ""], ["Chellappa", "Rama", ""]]}, {"id": "1907.13261", "submitter": "Rodrigo Lobos", "authors": "Rodrigo A. Lobos, W. Scott Hoge, Ahsan Javed, Congyu Liao, Kawin\n  Setsompop, Krishna S. Nayak, Justin P. Haldar", "title": "Robust Autocalibrated Structured Low-Rank EPI Ghost Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: We propose and evaluate a new structured low-rank method for EPI\nghost correction called Robust Autocalibrated LORAKS (RAC-LORAKS). The method\ncan be used to suppress EPI ghosts arising from the differences between\ndifferent readout gradient polarities and/or the differences between different\nshots. It does not require conventional EPI navigator signals, and is robust to\nimperfect autocalibration data.\n  Methods: Autocalibrated LORAKS is a previous structured low-rank method for\nEPI ghost correction that uses GRAPPA-type autocalibration data to enable\nhigh-quality ghost correction. This method works well when the autocalibration\ndata is pristine, but performance degrades substantially when the\nautocalibration information is imperfect. RAC-LORAKS generalizes Autocalibrated\nLORAKS in two ways. First, it does not completely trust the information from\nautocalibration data, and instead considers the autocalibration and EPI data\nsimultaneously when estimating low-rank matrix structure. And second, it uses\ncomplementary information from the autocalibration data to improve EPI\nreconstruction in a multi-contrast joint reconstruction framework. RAC-LORAKS\nis evaluated using simulations and in vivo data, including comparisons to\nstate-of-the-art methods.\n  Results: RAC-LORAKS is demonstrated to have good ghost elimination\nperformance compared to state-of-the-art methods in several complicated EPI\nacquisition scenarios (including gradient-echo brain imaging, diffusion-encoded\nbrain imaging, and cardiac imaging).\n  Conclusion: RAC-LORAKS provides effective suppression of EPI ghosts and is\nrobust to imperfect autocalibration data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 23:40:15 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 23:04:05 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 23:04:24 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Lobos", "Rodrigo A.", ""], ["Hoge", "W. Scott", ""], ["Javed", "Ahsan", ""], ["Liao", "Congyu", ""], ["Setsompop", "Kawin", ""], ["Nayak", "Krishna S.", ""], ["Haldar", "Justin P.", ""]]}, {"id": "1907.13268", "submitter": "Yan Zuo", "authors": "Gil Avraham, Yan Zuo, Thanuja Dharmasiri, Tom Drummond", "title": "EMPNet: Neural Localisation and Mapping Using Embedded Memory Points", "comments": "Accepted at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuously estimating an agent's state space and a representation of its\nsurroundings has proven vital towards full autonomy. A shared common ground\namong systems which successfully achieve this feat is the integration of\npreviously encountered observations into the current state being estimated.\nThis necessitates the use of a memory module for incorporating previously\nvisited states whilst simultaneously offering an internal representation of the\nobserved environment. In this work we develop a memory module which contains\nrigidly aligned point-embeddings that represent a coherent scene structure\nacquired from an RGB-D sequence of observations. The point-embeddings are\nextracted using modern convolutional neural network architectures, and\nalignment is performed by computing a dense correspondence matrix between a new\nobservation and the current embeddings residing in the memory module. The whole\nframework is end-to-end trainable, resulting in a recurrent joint optimisation\nof the point-embeddings contained in the memory. This process amplifies the\nshared information across states, providing increased robustness and accuracy.\nWe show significant improvement of our method across a set of experiments\nperformed on the synthetic VIZDoom environment and a real world Active Vision\nDataset.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 01:04:13 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 02:41:01 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Avraham", "Gil", ""], ["Zuo", "Yan", ""], ["Dharmasiri", "Thanuja", ""], ["Drummond", "Tom", ""]]}, {"id": "1907.13285", "submitter": "Uehwan Kim", "authors": "Ue-Hwan Kim, Sahng-Min Yoo and Jong-Hwan Kim", "title": "I-Keyboard: Fully Imaginary Keyboard on Touch Devices Empowered by Deep\n  Neural Decoder", "comments": "Submitted to IEEE TRANSACTIONS ON CYBERNETICS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-entry aims to provide an effective and efficient pathway for humans to\ndeliver their messages to computers. With the advent of mobile computing, the\nrecent focus of text-entry research has moved from physical keyboards to soft\nkeyboards. Current soft keyboards, however, increase the typo rate due to lack\nof tactile feedback and degrade the usability of mobile devices due to their\nlarge portion on screens. To tackle these limitations, we propose a fully\nimaginary keyboard (I-Keyboard) with a deep neural decoder (DND). The\ninvisibility of I-Keyboard maximizes the usability of mobile devices and DND\nempowered by a deep neural architecture allows users to start typing from any\nposition on the touch screens at any angle. To the best of our knowledge, the\neyes-free ten-finger typing scenario of I-Keyboard which does not necessitate\nboth a calibration step and a predefined region for typing is first explored in\nthis work. For the purpose of training DND, we collected the largest user data\nin the process of developing I-Keyboard. We verified the performance of the\nproposed I-Keyboard and DND by conducting a series of comprehensive simulations\nand experiments under various conditions. I-Keyboard showed 18.95% and 4.06%\nincreases in typing speed (45.57 WPM) and accuracy (95.84%), respectively over\nthe baseline.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 02:22:49 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kim", "Ue-Hwan", ""], ["Yoo", "Sahng-Min", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "1907.13315", "submitter": "Chunhua Shen", "authors": "Xinyu Zhang, Jiewei Cao, Chunhua Shen, Mingyu You", "title": "Self-training with progressive augmentation for unsupervised\n  cross-domain person re-identification", "comments": "Accepted to Proc. Int. Conf. Computer Vision, 2019. Code is available\n  at: https://tinyurl.com/PASTReID", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Person re-identification (Re-ID) has achieved great improvement with deep\nlearning and a large amount of labelled training data. However, it remains a\nchallenging task for adapting a model trained in a source domain of labelled\ndata to a target domain of only unlabelled data available. In this work, we\ndevelop a self-training method with progressive augmentation framework (PAST)\nto promote the model performance progressively on the target dataset.\nSpecially, our PAST framework consists of two stages, namely, conservative\nstage and promoting stage. The conservative stage captures the local structure\nof target-domain data points with triplet-based loss functions, leading to\nimproved feature representations. The promoting stage continuously optimizes\nthe network by appending a changeable classification layer to the last layer of\nthe model, enabling the use of global information about the data distribution.\nImportantly, we propose a new self-training strategy that progressively\naugments the model capability by adopting conservative and promoting stages\nalternately. Furthermore, to improve the reliability of selected triplet\nsamples, we introduce a ranking-based triplet loss in the conservative stage,\nwhich is a label-free objective function basing on the similarities between\ndata pairs. Experiments demonstrate that the proposed method achieves\nstate-of-the-art person Re-ID performance under the unsupervised cross-domain\nsetting. Code is available at: https://tinyurl.com/PASTReID\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 05:51:38 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Zhang", "Xinyu", ""], ["Cao", "Jiewei", ""], ["Shen", "Chunhua", ""], ["You", "Mingyu", ""]]}, {"id": "1907.13322", "submitter": "Inyoung Paik -", "authors": "Inyoung Paik, Sangjun Oh, Tae-Yeong Kwak, Injung Kim", "title": "Overcoming Catastrophic Forgetting by Neuron-level Plasticity Control", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the issue of catastrophic forgetting in neural networks, we\npropose a novel, simple, and effective solution called neuron-level plasticity\ncontrol (NPC). While learning a new task, the proposed method preserves the\nknowledge for the previous tasks by controlling the plasticity of the network\nat the neuron level. NPC estimates the importance value of each neuron and\nconsolidates important \\textit{neurons} by applying lower learning rates,\nrather than restricting individual connection weights to stay close to certain\nvalues. The experimental results on the incremental MNIST (iMNIST) and\nincremental CIFAR100 (iCIFAR100) datasets show that neuron-level consolidation\nis substantially more effective compared to the connection-level consolidation\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 06:22:00 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Paik", "Inyoung", ""], ["Oh", "Sangjun", ""], ["Kwak", "Tae-Yeong", ""], ["Kim", "Injung", ""]]}, {"id": "1907.13327", "submitter": "Inyoung Paik -", "authors": "Inyoung Paik, Taeyeong Kwak, Injung Kim", "title": "Capsule Networks Need an Improved Routing Algorithm", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In capsule networks, the routing algorithm connects capsules in consecutive\nlayers, enabling the upper-level capsules to learn higher-level concepts by\ncombining the concepts of the lower-level capsules. Capsule networks are known\nto have a few advantages over conventional neural networks, including\nrobustness to 3D viewpoint changes and generalization capability. However, some\nstudies have reported negative experimental results. Nevertheless, the reason\nfor this phenomenon has not been analyzed yet. We empirically analyzed the\neffect of five different routing algorithms. The experimental results show that\nthe routing algorithms do not behave as expected and often produce results that\nare worse than simple baseline algorithms that assign the connection strengths\nuniformly or randomly. We also show that, in most cases, the routing algorithms\ndo not change the classification result but polarize the link strengths, and\nthe polarization can be extreme when they continue to repeat without stopping.\nIn order to realize the true potential of the capsule network, it is essential\nto develop an improved routing algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 06:39:58 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Paik", "Inyoung", ""], ["Kwak", "Taeyeong", ""], ["Kim", "Injung", ""]]}, {"id": "1907.13342", "submitter": "April Pyone Maung Maung", "authors": "MaungMaung AprilPyone, Warit Sirichotedumrong and Hitoshi Kiya", "title": "Adversarial Test on Learnable Image Encryption", "comments": "To be appeared in 2019 IEEE 8th Global Conference on Consumer\n  Electronics (GCCE 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data for deep learning should be protected for privacy preserving.\nResearchers have come up with the notion of learnable image encryption to\nsatisfy the requirement. However, existing privacy preserving approaches have\nnever considered the threat of adversarial attacks. In this paper, we ran an\nadversarial test on learnable image encryption in five different scenarios. The\nresults show different behaviors of the network in the variable key scenarios\nand suggest learnable image encryption provides certain level of adversarial\nrobustness.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 07:24:57 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["AprilPyone", "MaungMaung", ""], ["Sirichotedumrong", "Warit", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "1907.13347", "submitter": "Young-Min Song", "authors": "Young-min Song, Kwangjin Yoon, Young-Chul Yoon, Kin-Choong Yow, Moongu\n  Jeon", "title": "Online Multi-Object Tracking Framework with the GMPHD Filter and\n  Occlusion Group Management", "comments": "This paper includes 15 pages and 9 figures, and has been prepared for\n  a journal (not yet submitted anywhere)", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2953276", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient online multi-object tracking framework\nbased on the GMPHD filter and occlusion group management scheme where the GMPHD\nfilter utilizes hierarchical data association to reduce the false negatives\ncaused by miss detection. The hierarchical data association consists of two\nsteps: detection-to-track and track-to-track associations, which can recover\nthe lost tracks and their switched IDs. In addition, the proposed framework is\nequipped with an object grouping management scheme which handles occlusion\nproblems with two main parts. The first part is \"track merging\" which can merge\nthe false positive tracks caused by false positive detections from occlusions,\nwhere the false positive tracks are usually occluded with a measure. The\nmeasure is the occlusion ratio between visual objects,\nsum-of-intersection-over-area (SIOA) we defined instead of the IOU metric. The\nsecond part is \"occlusion group energy minimization (OGEM)\" which prevents the\noccluded true positive tracks from false \"track merging\". We define each group\nof the occluded objects as an energy function and find an optimal hypothesis\nwhich makes the energy minimal. We evaluate the proposed tracker in benchmark\ndatasets such as MOT15 and MOT17 which are built for multi-person tracking. An\nablation study in training dataset shows that not only \"track merging\" and\n\"OGEM\" complement each other but also the proposed tracking method has more\nrobust performance and less sensitive to parameters than baseline methods.\nAlso, SIOA works better than IOU for various sizes of false positives.\nExperimental results show that the proposed tracker efficiently handles\nocclusion situations and achieves competitive performance compared to the\nstate-of-the-art methods. Especially, our method shows the best multi-object\ntracking accuracy among the online and real-time executable methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 07:36:09 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Song", "Young-min", ""], ["Yoon", "Kwangjin", ""], ["Yoon", "Young-Chul", ""], ["Yow", "Kin-Choong", ""], ["Jeon", "Moongu", ""]]}, {"id": "1907.13349", "submitter": "Ke Zhang", "authors": "Ke Zhang, Xinsheng Wang, Yurong Guo, Zhenbing Zhao, Zhanyu Ma, and\n  Tony X. Han", "title": "Competing Ratio Loss for Discriminative Multi-class Image Classification", "comments": "The method proposed in this paper has major technical shortcomings,\n  so we want to withdraw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of deep convolutional neural network architecture is critical\nto the improvement of image classification task performance. A lot of studies\nof image classification based on deep convolutional neural network focus on the\nnetwork structure to improve the image classification performance. Contrary to\nthese studies, we focus on the loss function. Cross-entropy Loss (CEL) is\nwidely used for training a multi-class classification deep convolutional neural\nnetwork. While CEL has been successfully implemented in image classification\ntasks, it only focuses on the posterior probability of correct class when the\nlabels of training images are one-hot. It cannot be discriminated against the\nclasses not belong to correct class (wrong classes) directly. In order to solve\nthe problem of CEL, we propose Competing Ratio Loss (CRL), which calculates the\nposterior probability ratio between the correct class and competing wrong\nclasses to better discriminate the correct class from competing wrong classes,\nincreasing the difference between the negative log likelihood of the correct\nclass and the negative log likelihood of competing wrong classes, widening the\ndifference between the probability of the correct class and the probabilities\nof wrong classes. To demonstrate the effectiveness of our loss function, we\nperform some sets of experiments on different types of image classification\ndatasets, including CIFAR, SVHN, CUB200- 2011, Adience and ImageNet datasets.\nThe experimental results show the effectiveness and robustness of our loss\nfunction on different deep convolutional neural network architectures and\ndifferent image classification tasks, such as fine-grained image\nclassification, hard face age estimation and large-scale image classification.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 07:38:04 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 05:42:45 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Zhang", "Ke", ""], ["Wang", "Xinsheng", ""], ["Guo", "Yurong", ""], ["Zhao", "Zhenbing", ""], ["Ma", "Zhanyu", ""], ["Han", "Tony X.", ""]]}, {"id": "1907.13368", "submitter": "Yihang Lou", "authors": "Yihang Lou, Ling-Yu Duan, Yong Luo, Ziqian Chen, Tongliang Liu, Shiqi\n  Wang, Wen Gao", "title": "Towards Digital Retina in Smart Cities: A Model Generation, Utilization\n  and Communication Paradigm", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digital retina in smart cities is to select what the City Eye tells the\nCity Brain, and convert the acquired visual data from front-end visual sensors\nto features in an intelligent sensing manner. By deploying deep learning and/or\nhandcrafted models in front-end devices, the compact features can be extracted\nand subsequently delivered to back-end cloud for search and advanced analytics.\nIn this context, we propose a model generation, utilization, and communication\nparadigm, aiming to address a set of unique challenges for better artificial\nintelligence services in smart cities. In particular, we present an integrated\nmultiple deep learning models reuse and prediction strategy, which greatly\nincreases the feasibility of the digital retina in processing and analyzing the\nlarge-scale visual data in smart cities. The promise of the proposed paradigm\nis demonstrated through a set of experiments.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 08:50:49 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Lou", "Yihang", ""], ["Duan", "Ling-Yu", ""], ["Luo", "Yong", ""], ["Chen", "Ziqian", ""], ["Liu", "Tongliang", ""], ["Wang", "Shiqi", ""], ["Gao", "Wen", ""]]}, {"id": "1907.13369", "submitter": "Wenhao Wu", "authors": "Wenhao Wu, Dongliang He, Xiao Tan, Shifeng Chen, Shilei Wen", "title": "Multi-Agent Reinforcement Learning Based Frame Sampling for Effective\n  Untrimmed Video Recognition", "comments": "Accepted by ICCV 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Recognition has drawn great research interest and great progress has\nbeen made. A suitable frame sampling strategy can improve the accuracy and\nefficiency of recognition. However, mainstream solutions generally adopt\nhand-crafted frame sampling strategies for recognition. It could degrade the\nperformance, especially in untrimmed videos, due to the variation of\nframe-level saliency. To this end, we concentrate on improving untrimmed video\nclassification via developing a learning-based frame sampling strategy. We\nintuitively formulate the frame sampling procedure as multiple parallel Markov\ndecision processes, each of which aims at picking out a frame/clip by gradually\nadjusting an initial sampling. Then we propose to solve the problems with\nmulti-agent reinforcement learning (MARL). Our MARL framework is composed of a\nnovel RNN-based context-aware observation network which jointly models context\ninformation among nearby agents and historical states of a specific agent, a\npolicy network which generates the probability distribution over a predefined\naction space at each step and a classification network for reward calculation\nas well as final recognition. Extensive experimental results show that our\nMARL-based scheme remarkably outperforms hand-crafted strategies with various\n2D and 3D baseline methods. Our single RGB model achieves a comparable\nperformance of ActivityNet v1.3 champion submission with multi-modal\nmulti-model fusion and new state-of-the-art results on YouTube Birds and\nYouTube Cars.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 08:51:31 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 11:13:13 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Wu", "Wenhao", ""], ["He", "Dongliang", ""], ["Tan", "Xiao", ""], ["Chen", "Shifeng", ""], ["Wen", "Shilei", ""]]}, {"id": "1907.13372", "submitter": "Umberto Michieli", "authors": "Umberto Michieli and Pietro Zanuttigh", "title": "Incremental Learning Techniques for Semantic Segmentation", "comments": "8 pages, 3 figures, 4 tables", "journal-ref": "International Conference on Computer Vision (ICCV), Workshop on\n  Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV) 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning architectures exhibit a critical drop of performance due to\ncatastrophic forgetting when they are required to incrementally learn new\ntasks. Contemporary incremental learning frameworks focus on image\nclassification and object detection while in this work we formally introduce\nthe incremental learning problem for semantic segmentation in which a\npixel-wise labeling is considered. To tackle this task we propose to distill\nthe knowledge of the previous model to retain the information about previously\nlearned classes, whilst updating the current model to learn the new ones. We\npropose various approaches working both on the output logits and on\nintermediate features. In opposition to some recent frameworks, we do not store\nany image from previously learned classes and only the last model is needed to\npreserve high accuracy on these classes. The experimental evaluation on the\nPascal VOC2012 dataset shows the effectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 09:00:15 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 11:59:30 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 12:43:59 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2019 07:33:25 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Michieli", "Umberto", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "1907.13391", "submitter": "Patrick Kn\\\"obelreiter", "authors": "Patrick Kn\\\"obelreiter and Thomas Pock", "title": "Learned Collaborative Stereo Refinement", "comments": "@German Conference on Pattern Recognition 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a learning-based method to denoise and refine\ndisparity maps of a given stereo method. The proposed variational network\narises naturally from unrolling the iterates of a proximal gradient method\napplied to a variational energy defined in a joint disparity, color, and\nconfidence image space. Our method allows to learn a robust collaborative\nregularizer leveraging the joint statistics of the color image, the confidence\nmap and the disparity map. Due to the variational structure of our method, the\nindividual steps can be easily visualized, thus enabling interpretability of\nthe method. We can therefore provide interesting insights into how our method\nrefines and denoises disparity maps. The efficiency of our method is\ndemonstrated by the publicly available stereo benchmarks Middlebury 2014 and\nKitti 2015.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 09:41:36 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kn\u00f6belreiter", "Patrick", ""], ["Pock", "Thomas", ""]]}, {"id": "1907.13394", "submitter": "Shichao Li", "authors": "Yi Zheng, Yifan Zhao, Mengyuan Ren, He Yan, Xiangju Lu, Junhui Liu,\n  Jia Li", "title": "Cartoon Face Recognition: A Benchmark Dataset", "comments": "9 papers, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed increasing attention in cartoon media, powered by\nthe strong demands of industrial applications. As the first step to understand\nthis media, cartoon face recognition is a crucial but less-explored task with\nfew datasets proposed. In this work, we first present a new challenging\nbenchmark dataset, consisting of 389,678 images of 5,013 cartoon characters\nannotated with identity, bounding box, pose, and other auxiliary attributes.\nThe dataset, named iCartoonFace, is currently the largest-scale, high-quality,\nrichannotated, and spanning multiple occurrences in the field of image\nrecognition, including near-duplications, occlusions, and appearance changes.\nIn addition, we provide two types of annotations for cartoon media, i.e., face\nrecognition, and face detection, with the help of a semi-automatic labeling\nalgorithm. To further investigate this challenging dataset, we propose a\nmulti-task domain adaptation approach that jointly utilizes the human and\ncartoon domain knowledge with three discriminative regularizations. We hence\nperform a benchmark analysis of the proposed dataset and verify the superiority\nof the proposed approach in the cartoon face recognition task. We believe this\npublic availability will attract more research attention in broad practical\napplication scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 09:48:41 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 05:31:42 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 03:43:33 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Zheng", "Yi", ""], ["Zhao", "Yifan", ""], ["Ren", "Mengyuan", ""], ["Yan", "He", ""], ["Lu", "Xiangju", ""], ["Liu", "Junhui", ""], ["Li", "Jia", ""]]}, {"id": "1907.13418", "submitter": "Ryutaro Tanno", "authors": "Ryutaro Tanno, Daniel Worrall, Enrico Kaden, Aurobrata Ghosh,\n  Francesco Grussu, Alberto Bizzi, Stamatios N. Sotiropoulos, Antonio\n  Criminisi, Daniel C. Alexander", "title": "Uncertainty Quantification in Deep Learning for Safer Neuroimage\n  Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has shown great potential in medical image enhancement\nproblems, such as super-resolution or image synthesis. However, to date, little\nconsideration has been given to uncertainty quantification over the output\nimage. Here we introduce methods to characterise different components of\nuncertainty in such problems and demonstrate the ideas using diffusion MRI\nsuper-resolution. Specifically, we propose to account for $intrinsic$\nuncertainty through a heteroscedastic noise model and for $parameter$\nuncertainty through approximate Bayesian inference, and integrate the two to\nquantify $predictive$ uncertainty over the output image. Moreover, we introduce\na method to propagate the predictive uncertainty on a multi-channelled image to\nderived scalar parameters, and separately quantify the effects of intrinsic and\nparameter uncertainty therein. The methods are evaluated for super-resolution\nof two different signal representations of diffusion MR images---DTIs and Mean\nApparent Propagator MRI---and their derived quantities such as MD and FA, on\nmultiple datasets of both healthy and pathological human brains. Results\nhighlight three key benefits of uncertainty modelling for improving the safety\nof DL-based image enhancement systems. Firstly, incorporating uncertainty\nimproves the predictive performance even when test data departs from training\ndata. Secondly, the predictive uncertainty highly correlates with errors, and\nis therefore capable of detecting predictive \"failures\". Results demonstrate\nthat such an uncertainty measure enables subject-specific and voxel-wise risk\nassessment of the output images. Thirdly, we show that the method for\ndecomposing predictive uncertainty into its independent sources provides\nhigh-level \"explanations\" for the performance by quantifying how much\nuncertainty arises from the inherent difficulty of the task or the limited\ntraining examples.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 11:17:45 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Tanno", "Ryutaro", ""], ["Worrall", "Daniel", ""], ["Kaden", "Enrico", ""], ["Ghosh", "Aurobrata", ""], ["Grussu", "Francesco", ""], ["Bizzi", "Alberto", ""], ["Sotiropoulos", "Stamatios N.", ""], ["Criminisi", "Antonio", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "1907.13426", "submitter": "Xia Li", "authors": "Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, Hong Liu", "title": "Expectation-Maximization Attention Networks for Semantic Segmentation", "comments": "In Proceedings of International Conference in Computer Vision (ICCV),\n  2019. Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention mechanism has been widely used for various tasks. It is\ndesigned to compute the representation of each position by a weighted sum of\nthe features at all positions. Thus, it can capture long-range relations for\ncomputer vision tasks. However, it is computationally consuming. Since the\nattention maps are computed w.r.t all other positions. In this paper, we\nformulate the attention mechanism into an expectation-maximization manner and\niteratively estimate a much more compact set of bases upon which the attention\nmaps are computed. By a weighted summation upon these bases, the resulting\nrepresentation is low-rank and deprecates noisy information from the input. The\nproposed Expectation-Maximization Attention (EMA) module is robust to the\nvariance of input and is also friendly in memory and computation. Moreover, we\nset up the bases maintenance and normalization methods to stabilize its\ntraining procedure. We conduct extensive experiments on popular semantic\nsegmentation benchmarks including PASCAL VOC, PASCAL Context and COCO Stuff, on\nwhich we set new records.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 11:35:54 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 17:55:50 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Li", "Xia", ""], ["Zhong", "Zhisheng", ""], ["Wu", "Jianlong", ""], ["Yang", "Yibo", ""], ["Lin", "Zhouchen", ""], ["Liu", "Hong", ""]]}, {"id": "1907.13449", "submitter": "Yuriy Anisimov", "authors": "Yuriy Anisimov, Oliver Wasenm\\\"uller, Didier Stricker", "title": "Rapid Light Field Depth Estimation with Semi-Global Matching", "comments": "IEEE 15th International Conference on Intelligent Computer\n  Communication and Processing, Cluj-Napoca, September 5-7, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Running time of the light field depth estimation algorithms is typically\nhigh. This assessment is based on the computational complexity of existing\nmethods and the large amounts of data involved. The aim of our work is to\ndevelop a simple and fast algorithm for accurate depth computation. In this\ncontext, we propose an approach, which involves Semi-Global Matching for the\nprocessing of light field images. It forms on comparison of pixels'\ncorrespondences with different metrics in the substantially bounded light field\nspace. We show that our method is suitable for the fast production of a proper\nresult in a variety of light field configurations\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 12:27:29 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Anisimov", "Yuriy", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "1907.13487", "submitter": "Samuel Albanie", "authors": "Yang Liu and Samuel Albanie and Arsha Nagrani and Andrew Zisserman", "title": "Use What You Have: Video Retrieval Using Representations From\n  Collaborative Experts", "comments": "This update contains a correction to previously reported results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of video on the internet has made searching for video\ncontent using natural language queries a significant challenge. Human-generated\nqueries for video datasets `in the wild' vary a lot in terms of degree of\nspecificity, with some queries describing specific details such as the names of\nfamous identities, content from speech, or text available on the screen. Our\ngoal is to condense the multi-modal, extremely high dimensional information\nfrom videos into a single, compact video representation for the task of video\nretrieval using free-form text queries, where the degree of specificity is\nopen-ended.\n  For this we exploit existing knowledge in the form of pre-trained semantic\nembeddings which include 'general' features such as motion, appearance, and\nscene features from visual content. We also explore the use of more 'specific'\ncues from ASR and OCR which are intermittently available for videos and find\nthat these signals remain challenging to use effectively for retrieval. We\npropose a collaborative experts model to aggregate information from these\ndifferent pre-trained experts and assess our approach empirically on five\nretrieval benchmarks: MSR-VTT, LSMDC, MSVD, DiDeMo, and ActivityNet. Code and\ndata can be found at www.robots.ox.ac.uk/~vgg/research/collaborative-experts/.\nThis paper contains a correction to results reported in the previous version.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 13:19:37 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 04:32:35 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Liu", "Yang", ""], ["Albanie", "Samuel", ""], ["Nagrani", "Arsha", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1907.13494", "submitter": "Alberto Testolin Dr.", "authors": "Alberto Cenzato, Alberto Testolin and Marco Zorzi", "title": "On the difficulty of learning and predicting the long-term dynamics of\n  bouncing objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately predict the surrounding environment is a\nfoundational principle of intelligence in biological and artificial agents. In\nrecent years, a variety of approaches have been proposed for learning to\npredict the physical dynamics of objects interacting in a visual scene. Here we\nconduct a systematic empirical evaluation of several state-of-the-art\nunsupervised deep learning models that are considered capable of learning the\nspatio-temporal structure of a popular dataset composed by synthetic videos of\nbouncing objects. We show that most of the models indeed obtain high accuracy\non the standard benchmark of predicting the next frame of a sequence, and one\nof them even achieves state-of-the-art performance. However, all models fall\nshort when probed with the more challenging task of generating multiple\nsuccessive frames. Our results show that the ability to perform short-term\npredictions does not imply that the model has captured the underlying structure\nand dynamics of the visual environment, thereby calling for a careful\nrethinking of the metrics commonly adopted for evaluating temporal models. We\nalso investigate whether the learning outcome could be affected by the use of\ncurriculum-based teaching.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 13:29:34 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Cenzato", "Alberto", ""], ["Testolin", "Alberto", ""], ["Zorzi", "Marco", ""]]}, {"id": "1907.13524", "submitter": "Julian Krebs", "authors": "Julian Krebs and Tommaso Mansi and Nicholas Ayache and Herv\\'e\n  Delingette", "title": "Probabilistic Motion Modeling from Medical Image Sequences: Application\n  to Cardiac Cine-MRI", "comments": "Probabilistic Motion Model, Motion Tracking, Temporal\n  Super-Resolution, Diffeomorphic Registration, Temporal Variational\n  Autoencoder (Final version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn a probabilistic motion model from a sequence of images.\nBesides spatio-temporal registration, our method offers to predict motion from\na limited number of frames, useful for temporal super-resolution. The model is\nbased on a probabilistic latent space and a novel temporal dropout training\nscheme. This enables simulation and interpolation of realistic motion patterns\ngiven only one or any subset of frames of a sequence. The encoded motion also\nallows to be transported from one subject to another without the need of\ninter-subject registration. An unsupervised generative deformation model is\napplied within a temporal convolutional network which leads to a diffeomorphic\nmotion model, encoded as a low-dimensional motion matrix. Applied to cardiac\ncine-MRI sequences, we show improved registration accuracy and\nspatio-temporally smoother deformations compared to three state-of-the-art\nregistration algorithms. Besides, we demonstrate the model's applicability to\nmotion transport by simulating a pathology in a healthy case. Furthermore, we\nshow an improved motion reconstruction from incomplete sequences compared to\nlinear and cubic interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:25:47 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 12:57:40 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Krebs", "Julian", ""], ["Mansi", "Tommaso", ""], ["Ayache", "Nicholas", ""], ["Delingette", "Herv\u00e9", ""]]}, {"id": "1907.13557", "submitter": "Jozsef Molnar", "authors": "Jozsef Molnar, Peter Horvath", "title": "An Elastic Energy Minimization Framework for Mean Surface Calculation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the continuation of the contour mean calculation - designed for averaging\nthe manual delineations of 3D layer stack images - in this paper, the most\nimportant equations: a) the reparameterization equations to determine the\nminimizing diffeomorphism and b) the proper centroid calculation for the\nsurface mean calculation are presented. The chosen representation space:\nescaled Position by Square root Normal (RPSN) is a real valued vector space,\ninvariant under the action of the reparameterization group and the imposed L2\nmetric (used to define the distance function) has well defined meaning: the sum\nof the central second moments of the coordinate functions. For comparision\npurpose, the reparameterization equations for elastic surface matching, using\nthe Square Root Normal Function (SRNF) are also provided. The\nreparameterization equations for these cases have formal similarity, albeit the\ntargeted applications differ: SRNF representation suitable for shape analysis\npurpose whereas RPSN is more fit for the cases where all contextual information\n- including the relative translation between the constituent surfaces - are to\nbe retained (but the sake of theoretical completeness, the possibility of the\nconsistent relative displacement removal in the RPSN case is also addressed).\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:33:19 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Molnar", "Jozsef", ""], ["Horvath", "Peter", ""]]}, {"id": "1907.13576", "submitter": "Keval Doshi", "authors": "Keval Doshi", "title": "Synthetic Image Augmentation for Improved Classification using\n  Generative Adversarial Networks", "comments": "State Recognition Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection and recognition has been an ongoing research topic for a\nlong time in the field of computer vision. Even in robotics, detecting the\nstate of an object by a robot still remains a challenging task. Also,\ncollecting data for each possible state is also not feasible. In this\nliterature, we use a deep convolutional neural network with SVM as a classifier\nto help with recognizing the state of a cooking object. We also study how a\ngenerative adversarial network can be used for synthetic data augmentation and\nimproving the classification accuracy. The main motivation behind this work is\nto estimate how well a robot could recognize the current state of an object\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 16:05:52 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Doshi", "Keval", ""]]}, {"id": "1907.13580", "submitter": "Saeed Ghorbani", "authors": "Saeed Ghorbani, Ali Etemad, Nikolaus F. Troje", "title": "Auto-labelling of Markers in Optical Motion Capture by Permutation\n  Learning", "comments": null, "journal-ref": "Computer Graphics International Conference, pp. 167-178. Springer,\n  Cham, 2019", "doi": "10.1007/978-3-030-22514-8_14", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical marker-based motion capture is a vital tool in applications such as\nmotion and behavioural analysis, animation, and biomechanics. Labelling, that\nis, assigning optical markers to the pre-defined positions on the body is a\ntime consuming and labour intensive postprocessing part of current motion\ncapture pipelines. The problem can be considered as a ranking process in which\nmarkers shuffled by an unknown permutation matrix are sorted to recover the\ncorrect order. In this paper, we present a framework for automatic marker\nlabelling which first estimates a permutation matrix for each individual frame\nusing a differentiable permutation learning model and then utilizes temporal\nconsistency to identify and correct remaining labelling errors. Experiments\nconducted on the test data show the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 16:25:34 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Ghorbani", "Saeed", ""], ["Etemad", "Ali", ""], ["Troje", "Nikolaus F.", ""]]}, {"id": "1907.13590", "submitter": "Junlin Yang", "authors": "Junlin Yang, Nicha C. Dvornek, Fan Zhang, Julius Chapiro, MingDe Lin,\n  James S. Duncan", "title": "Unsupervised Domain Adaptation via Disentangled Representations:\n  Application to Cross-Modality Liver Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep learning model trained on some labeled data from a certain source\ndomain generally performs poorly on data from different target domains due to\ndomain shifts. Unsupervised domain adaptation methods address this problem by\nalleviating the domain shift between the labeled source data and the unlabeled\ntarget data. In this work, we achieve cross-modality domain adaptation, i.e.\nbetween CT and MRI images, via disentangled representations. Compared to\nlearning a one-to-one mapping as the state-of-art CycleGAN, our model recovers\na many-to-many mapping between domains to capture the complex cross-domain\nrelations. It preserves semantic feature-level information by finding a shared\ncontent space instead of a direct pixelwise style transfer. Domain adaptation\nis achieved in two steps. First, images from each domain are embedded into two\nspaces, a shared domain-invariant content space and a domain-specific style\nspace. Next, the representation in the content space is extracted to perform a\ntask. We validated our method on a cross-modality liver segmentation task, to\ntrain a liver segmentation model on CT images that also performs well on MRI.\nOur method achieved Dice Similarity Coefficient (DSC) of 0.81, outperforming a\nCycleGAN-based method of 0.72. Moreover, our model achieved good generalization\nto joint-domain learning, in which unpaired data from different modalities are\njointly learned to improve the segmentation performance on each individual\nmodality. Lastly, under a multi-modal target domain with significant diversity,\nour approach exhibited the potential for diverse image generation and remained\neffective with DSC of 0.74 on multi-phasic MRI while the CycleGAN-based method\nperformed poorly with a DSC of only 0.52.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 16:45:19 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 02:47:32 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Yang", "Junlin", ""], ["Dvornek", "Nicha C.", ""], ["Zhang", "Fan", ""], ["Chapiro", "Julius", ""], ["Lin", "MingDe", ""], ["Duncan", "James S.", ""]]}, {"id": "1907.13615", "submitter": "Qianli Ma", "authors": "Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard\n  Pons-Moll, Siyu Tang, Michael J. Black", "title": "Learning to Dress 3D People in Generative Clothing", "comments": "CVPR-2020 camera ready. Code and data are available at\n  https://cape.is.tue.mpg.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional human body models are widely used in the analysis of human\npose and motion. Existing models, however, are learned from minimally-clothed\n3D scans and thus do not generalize to the complexity of dressed people in\ncommon images and videos. Additionally, current models lack the expressive\npower needed to represent the complex non-linear geometry of pose-dependent\nclothing shapes. To address this, we learn a generative 3D mesh model of\nclothed people from 3D scans with varying pose and clothing. Specifically, we\ntrain a conditional Mesh-VAE-GAN to learn the clothing deformation from the\nSMPL body model, making clothing an additional term in SMPL. Our model is\nconditioned on both pose and clothing type, giving the ability to draw samples\nof clothing to dress different body shapes in a variety of styles and poses. To\npreserve wrinkle detail, our Mesh-VAE-GAN extends patchwise discriminators to\n3D meshes. Our model, named CAPE, represents global shape and fine local\nstructure, effectively extending the SMPL body model to clothing. To our\nknowledge, this is the first generative model that directly dresses 3D human\nbody meshes and generalizes to different poses. The model, code and data are\navailable for research purposes at https://cape.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:30:54 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 18:50:07 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 17:21:49 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Ma", "Qianli", ""], ["Yang", "Jinlong", ""], ["Ranjan", "Anurag", ""], ["Pujades", "Sergi", ""], ["Pons-Moll", "Gerard", ""], ["Tang", "Siyu", ""], ["Black", "Michael J.", ""]]}, {"id": "1907.13622", "submitter": "Wei-Sheng Lai", "authors": "Wei-Sheng Lai, Orazio Gallo, Jinwei Gu, Deqing Sun, Ming-Hsuan Yang,\n  Jan Kautz", "title": "Video Stitching for Linear Camera Arrays", "comments": "This work is accepted in BMVC 2019. Project website:\n  http://vllab.ucmerced.edu/wlai24/video_stitching/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the long history of image and video stitching research, existing\nacademic and commercial solutions still produce strong artifacts. In this work,\nwe propose a wide-baseline video stitching algorithm for linear camera arrays\nthat is temporally stable and tolerant to strong parallax. Our key insight is\nthat stitching can be cast as a problem of learning a smooth spatial\ninterpolation between the input videos. To solve this problem, inspired by\npushbroom cameras, we introduce a fast pushbroom interpolation layer and\npropose a novel pushbroom stitching network, which learns a dense flow field to\nsmoothly align the multiple input videos for spatial interpolation. Our\napproach outperforms the state-of-the-art by a significant margin, as we show\nwith a user study, and has immediate applications in many areas such as virtual\nreality, immersive telepresence, autonomous driving, and video surveillance.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:42:38 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Lai", "Wei-Sheng", ""], ["Gallo", "Orazio", ""], ["Gu", "Jinwei", ""], ["Sun", "Deqing", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}]